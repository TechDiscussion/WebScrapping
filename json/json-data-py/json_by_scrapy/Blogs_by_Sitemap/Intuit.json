[
{"website": "Intuit", "title": "public cloud cost optimization", "author": ["siddharth ram"], "link": "https://quickbooks-engineering.intuit.com/public-cloud-cost-optimization-b67507f36b35", "abstract": "Companies migrating to the public cloud often make assumptions that often do not pan out. A particular assumption is that moving to public cloud will result in reduced costs. Often, they are shocked by the results — instead of lower cost, they see higher costs. There are several reasons for this and this article covers how you should think about migration to public cloud in the larger context- and why thinking about Cost Optimization should be a fundamental quality attribute of software delivery , just like availability, scalability and other ‘ilities’. is an illustration of the key difference in cost between private and public cloud. Private hosting generally has a cost forecast built out at the beginning of the fiscal year, and a commitment from teams based on which purchases are made. But the model is generally inflexible. If the actual usage is under plan (overcapacity), then money has been wasted: if it is over plan, then the plan is not flexible enough to accommodate it and customers suffer from under-provisioning. Worse, if there is a sudden rise in demand followed by a reduction, you have paid for excess hardware which now needs to be written off because it is not being used (falling demand on the chart) Public Cloud, OTOH, has elasticity as a primary attribute — so there is no over capacity or undercapacity. This is very attractive to organizations. However, this road has pitfalls — some of which are outlined below Public cloud migration comes with some gotchas. Not paying attention to them will result in a phone call from your CFO. Here are some of the common pitfalls: The Double Bubble is a reflection of the fact that there is a period of time where you are still migrating to public cloud. In the meantime, you are paying for two data centers and moving data back and forth. During this time, you will be paying two bills, to the captive data center as well as the public cloud vendor. Depending on the scale and complexity of migration, this can be a multi year double bubble Lesson: Be aware of the double bubble and plan for it. Work with engineers to minimize the double bubble period. Even when the double bubble ends, it takes time for Engineering teams to understand the elastic nature of the cloud, if they are new to public cloud. Resources should be scaled up when demand rises: scaled down during low traffic times. Mature companies actively plot out expected traffic months ahead and automatically scale to meet expected demand (See also 3 below) During the period where companies are relatively immature, a lack of understanding of scalability in the cloud means overprovisioned hardware to meet peak traffic needs, and hence excessive spend Lesson: Ensure that teams understand elasticity prior to cloud migration — not after. A lack of proper infrastructure will get in the way of cost optimization. By infrastructure, I mean allowing engineers to use elasticity without having to design a solution themselves. At Intuit, a large part of the solve is our Kubernetes infrastructure which allows us to scale up and down without product teams being involved based on traffic conditions. Lacking this, teams often have to make decisions — ‘Do I choose the 2x instance or the 16x instance? I am not sure, so let me choose the larger one’ — wasting capacity and money. Lesson: Use infrastructure like Kubernetes (or serverless) which allows transparent scale up and scale down. This includes the use of spot instances for workloads that are suitable for spot. Private hosting is opaque. Costs are opaque, visible to Finance teams, and leaders responsible for hosting. Engineers know nothing about it. This comes from annual forecasts being developed, and everyone having to work in the framework of commitments at the beginning of a fiscal year. By the same token, costs are comprehensively controlled. No change will ever take place without the finance and engineering leaders participation. This model is turned on its head in the cloud world. Engineers can see how many servers are provisioned, what Services are being consumed — and sometimes what the bill is. They have an ability to spin up new instances and use new services with no involvement from leaders or finance. This is empowering for engineers. Unfortunately, it can also have a significant downside: overutilization of public cloud and an unexpectedly large bill. To fix the problem above, Intuit’s approach is the following: Cost optimization is considered a quality attribute. Teams get full visibility to their cloud spend. They work with their leader to determine what their cost should be, and monitor it — no different than how we think about performance, resilience or scalability. There is a tradeoff between cost, availability and performance to be considered. More expensive hardware will result in better performance — but is that the right cost/performance ratio? Deploying in multiple regions can result in higher availability (and regional DR) — but is that worth the cost and complexity? Intuit’s approach is to define the principles for making the decision at a leadership level, but push the responsibility for the decision down to scrum teams. This ensures that every engineer is thinking about costs in designs along with proper availability and other patterns, and choosing the optimal path ensuring that cost is a consideration in designs. Not doing so can lead down wrong paths . Examples of considerations are: Is a service important enough to be in multiple AZ’s? Should this service be multi region? Will the configuration result in proper performance ? To allow teams proper visibility, Intuit offers a flexible cost dashboard that gives visibility from an enterprise level all the way to a particular service: leaders at all levels can view just their expenses and compare with their plans, and make adjustments as needed. Treating cost optimization as a quality attribute has empowered engineers to think about the tradeoffs between availability, performance and costs and make the right calls. In addition, an off the shelf cost optimization tool is integrated into the CI process. It runs loads on various hardware configurations and makes recommendations for a target (cost/performance ratio) set by a team. This allows teams to quickly take advantage newer hardware, or changes in software that allow it to run on different hardware optimally. Public cloud allows teams to address changes in demand quickly while managing costs transparently. Leaders need to think about the top line (‘can I address changing demand quickly?’ as the reason to move to public cloud as opposed to ‘Will it save me money’? At Intuit, Public cloud migration is more about the top line than the bottom line and cost optimization enables efficient long term investment in the top line Cool stuff we are doing and thinking about in engineering… 21 1 Public Cloud Cost Optimization Cloud Migration 21 claps 21 1 Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-30"},
{"website": "Intuit", "title": "enterprise cloud operations and governance", "author": ["Ranadeep Bhuyan"], "link": "https://quickbooks-engineering.intuit.com/enterprise-cloud-operations-and-governance-fdb614919cc5", "abstract": "A case study on cloud security. Many customers start by creating separate cloud (google, aws, azure etc) accounts for each environment (dev, pre-prod, prod etc) for each application. This helps create a strong separation between all resources in each account. However, this approach can create great operational challenges. Let’s imagine an enterprise organisation with 1000+ service teams that are hosted on cloud. A centrally managed organisation with a bunch of tools and automation processes helps cloud ops to centrally manage billing, control access, compliance, security and share resources across many AWS accounts. This blog is going to throw some light on how cloud Ops teams manage all of those thousands of cloud accounts and 100s of resources in each accounts using two simple use cases. Also we will understand, why it is very important for organisation and for their customers. In the end, I will share one learning from a devops standpoint and share a few links to some of the best practices. The “Capital One” Use case: Capital One Data theft of 106M customers : A misconfigured cloud resource (WAF) caused the cloud security failure WAF (web application firewall) is a type of cloud security service to access one’s VPC (virtual private cloud) resources. If any EC2 (elastic cloud compute, a unix/windows box in the cloud) is open to accept incoming traffic from 0.0.0.0/* IP range then it allows internet traffic to the EC2. Generally, WAF rules can be configured prevent 0.0.0.0/* to reach to any resource in the VPC. At a lower level at least a security group can be added to filter incoming traffic. The suspect twitted about the brag. The hacker twitted after she took a dump of all customer data using their S3 and aurora DB. Once inside, she launched an EC2 instance to trick the AWS Metadata Service into trusting it, thereby receiving credentials to access and decrypt data stored in an S3 bucket. This incident created paranoia across industry and suddenly bringing in lots of focus to these cloud services settings and configurations. Security and Compliance is a shared responsibility between cloud infra providers (aws) and the customer (consumer company). It is commonly referred to as Security “of” the Cloud versus Security “in” the Cloud. Example from AWS cloud: for centralised governance, billing, security policies etc. through aws organization. A master account service that gives a centralised control to all of resources that is owned by a company. Security, compliance and governance Policies: Do you see those emails from your cloud operations team? with a message like “Policy Violated — Action needed” Cloud security team notifies regularly about any misconfiguration This is a security framework published by the Center for Internet Security as the foundation of your security policy library, to expedite your time-to-value and achieve consistent configurations across your cloud footprint. AWS Organization A lot of things goes on continuously behind the scene. Fundamental unit of an accounts is called Organizational Unit (OU) . OUs allow the ability to arbitrarily group accounts and other OUs together so that they can be administered as a single unit. A Service Control Policy (SCP) is applied to an OU. These policies can describe the access and resource restrictions for the OU. A root is a top-level parent node in the hierarchy of an organisation that can contain organisational units (OUs) and accounts. A root contains every AWS account in the organisation . Product: Assets which form part of the Product or support the Product in some way. The things you want to build. Core: These assets are perhaps less obvious, but are just as important. They are the supporting cast to your Product. Use of automation to avoid mistakes A governing cloud can be created using CLI Create a security policy - The incident: Customers were unable to view their attached images from a public CDN URLs in their attached documents. It was a controlled traffic therefore the blast radius was very small and spread was slow. Those customers were not able to view their uploaded logos on invoices, and other financial documents as well. Documents such as Invoices are viewed from many different places such as emails, customer’s viewing portal, within also within the main product, HTMLs/PDF etc. For some reason customers logos from cdn go missing in all of those places where they could have viewed an Invoice and other customer docs. We did not have pager or alerts on cdn urls. The care team got to know this first when annoyed customers started sending messages over VOC/emails. By the time we fix the problem it was 10+ hours. Not a happy situation at all. Historically, the micro service was using a single AWS account for development, pre-production and production environments separated via aws tagging. One main reason is being AWS cost optimisation the org wanted to manage lesser accounts. But that’s not the issue. Another caveat, the AWS account was created with a pre-production tagged on it. That implied to — the logo service was running on pre-production account, from the cloud operations perspective. The service creates a reference to a CDN URLs for viewing the logos in the emails/invoices/other places. On that fine day morning a new security policy (WAF rule) was created and applied to block any traffic from outside office intranet for all pre-production accounts including all resources based on the account tag. Best practices By now you may have co-related both use cases and understood both cases were because of not following best practices. Automation to enforce and automatic detection and remediation is the key to reduce the vulnerability window. Regular communication of changes, alerts on production traffics helps minimise friction and failed customer experiences and outages. Every day a new set of security vulnerabilities are uncovered, making development teams aware of those and their remediation helps such outages and preventing any kind of breaching into private networks. All resources in a cloud account needs to tagged properly with an organisational standard policy. Resources Those belongs to prod and development needs to be tagged so that root is aware of what WAF to be applied. A right organisational tagging strategy makes a difference how we govern our accounts and resources in it. In the beginning, we may start with multiple environments from one account such as production database and production database, if the organisation tagging policy is designed right for those requirements and notifying for the same to teams makes a great deal. Reference: I found a bunch of materials among them AWS documentation on account management is one of the best ones. There is a course on governance and compliance — seems good. One best practice blog of separating development and production accounts. Not in AWS? no issues, each of those big players have laid out their best practices. Here’s Google Cloud account management . @ranadeep_bhuyan Cool stuff we are doing and thinking about in engineering… 4 Cloud Security Operations Organization AWS 4 claps 4 Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-10"},
{"website": "Intuit", "title": "root cause analysis an essential systems skill", "author": ["siddharth ram"], "link": "https://quickbooks-engineering.intuit.com/root-cause-analysis-an-essential-systems-skill-be2d0b96d31e", "abstract": "Root Cause Analysis — An Essential Systems skill The April 2019 headlines have been dominated by the multiple Boeing 737 MAX failures which claimed so many lives. Superficial newspaper headlines talked about how there were software failures and how Boeing made a mistake by relying on just one sensor. But why did Boeing do that? To understand how they got there, you really have to dig deep. The excellent analysis by Trevor Sumner and Dave Kammeyer shows how deep rooted cultural and economic issues were behind the failure. It was a culture problem that lead to a cascade of bad decisions and as is often the case with shallow analysis, resulted in a part being blamed. Engineering is supposed to be about precision and exactness. When a defect is uncovered and a fix applied, engineers need to ask ‘what caused this error?’ at a deeper level than ‘fixed a conditional clause’. Thinking deeper about the problem is a must in any engineering system that has more than a passing level of complexity. Not doing deeper analysis is a missed opportunity to prevent the same kind of problem from reoccurring Engineering systems are more than software. Complexity comes from process, culture, team dynamics, market pressures and system designs. The root cause of problems are often related to culture and process. Yet we do not spend enough time thinking about it. Every defect fix has the potential to wipe out a class of defects, if we think about it deeply enough. At Intuit, thinking long and hard about defects — especially those discovered as a production escape — is mandatory. This document shares the background of how we approach RCA’s and why it is critical in complex engineering systems. Drift into failure Sydney Deckers book is a must read for everyone who deals with complex systems, Engineering or not. The more complex a system (and, by extension, the more complex its control structure), the more difficult it can become to map out the reverberations of changes (even carefully considered ones) throughout the rest of the system… Small changes somewhere in the system, or small variations in the initial state of a process, can lead to large consequences elsewhere. Decisions make good local sense given the limited knowledge available to people in that part of the complex system. But invisible and unacknowledged reverberations of those decisions penetrate the complex system. We can try to design a simple system, but complexity arises nevertheless, often due to the interactions of many simple parts. Each part of the organization sees its own simple part: few ever see the entire complex landscape, including market pressures, revenue and growth goals, a company’s attitude towards its customers- all of which drive decisions that result in additional complexity The Humble RCA: A Panacea? If complexity is inevitable, so are failures. When a failure does happen, the worst outcome would be to ‘fix the part’ and ignore the system that allowed the failure to happen. Here is a poorly written outcome from an RCA I reviewed: There was a bug in the script that triggers an automated JVM restart when the AZ fails over which prevented some of the JVM instances from reconnecting promptly to the new RDS instance. It does not address: What allowed the bug to exist? Was there undue pressure on the team to release early? Is the organization accepting of ‘release now, worry later’? Did the engineers not get support from management to delay the release? Was inadequate testing done? If so, why was it inadequate? Answering these questions would help in wiping out a class of defects, rather than just fixing a bug. It does not solve the complexity problem, but helps manage it better by always going back to ‘what was behind what caused this problem”? At Intuit, we have a framework for doing RCA’s. The rest of this document explores how we approach RCA’s. RCA Principles. Be vocally self critical . Every incident is a learning opportunity. No team, process or cultural attribute is above criticism An ounce of prevention is worth a pound of cure . Heroics are disproportionately rewarded. Those who burn the midnight oil to fix production issues are seen as heroes. The real heroes are those who build in resiliency into the system and did not have to burn the midnight oil Broken Systems, not broken components. A broken part is never the answer. What causes the broken part is a flaw in the system that allowed the broken part to manifest itself. Always look for the flaws in the system. The 5 whys the 5 Why’s framework is used to uncover root cause. Each ‘why’ forms the basis of the next question. This framework comes from the Toyota Motor Corp in the development of its manufacturing methodologies. 5 is a suggested level. You may need to go deeper (or maybe stay shallower) to get to root cause. The example above could have used the 5 whys to go deeper: There was a bug in the script that triggers an automated JVM restart when the AZ fails over which prevented some of the JVM instances from reconnecting promptly to the new RDS instance. Why was there a bug in the script that triggered the automated JVM restart? During release 2019.21, a code review was skipped because the release was going out the same day Why was the code review skipped? There was a release going out the same day. If that was missed, there would be a 1 week delay in releasing. Why would there be a 1 week delay? The software process allows releases only once a week Why does the process allow release once a week? (…) — this will result in getting to a root cause — maybe the build system was never a priority for the organization, which it should be. A further check would result in understanding why this was not a priority. To fix the bug in the script, you really had to understand why the build system was not a priority for the organization. Fixing this root cause — and understanding the mindset behind it — would eliminate a class of defects from happening again. Note that often there are multiple underlying causes. This is done well by using Ishikawa diagrams (also called Fishbones) The Swiss Cheese Model & its flaws There are often several contributing factors to things that did or could go wrong. One way to think about them is via the Swiss cheese model. Wikipedia describes the Swiss cheese model as follows: The Swiss cheese model of accident causation illustrates that, although many layers of defense lie between hazards and accidents, there are flaws in each layer that, if aligned, can allow the accident to occur. Our goal is to understand the slices (e.g. systems, software, processes, mindsets, etc), where the holes exist (e.g. software bugs), and how, when, and where new holes can emerge (e.g. software upgrades, hardware failures). Then we can decide how to most effectively add new slices, reinforce existing slices, or plug existing holes with the lowest cost and highest impact. Dekker cautions that the Swiss cheese model is flawed. It does not think about the relationships between the holes and where the holes are or what they consist of, why the holes are there in the first place, why the holes change over time, both in size and location, how the holes get to line up to produce an accident. Postmortems are great for learning from failures. But we can also prevent failures by conducting premortems. Gary Klein describes a premortem in the Harvard Business Review as follows: A premortem is the hypothetical opposite of a postmortem. A postmortem in a medical setting allows health professionals and the family to learn what caused a patient’s death. Everyone benefits except, of course, the patient. A premortem in a business setting comes at the beginning of a project rather than the end, so that the project can be improved rather than autopsied. Unlike a typical critiquing session, in which project team members are asked what might go wrong, the premortem operates on the assumption that the “patient” has died, and so asks what did go wrong. The team members’ task is to generate plausible reasons for the project’s failure. A similar technique is the failure mode and effects analysis (FMEA). Wikipedia describes FMEA as follows: Failure mode and effects analysis ( FMEA ) — also “failure modes” in many publications — was one of the first systematic techniques for failure analysis . It was developed by reliability engineers in the late 1950s to study problems that might arise from malfunctions of military systems. A FMEA is often the first step of a system reliability study. It involves reviewing as many components, assemblies, and subsystems as possible to identify failure modes, and their causes and effects. For projects of significance (e.g. a component is moving to public cloud), a premortem is performed. What are the potential ways a failure could happen? What is our playbook if the failure does happen? A system architecture diagram is useful for premortems. For each interface, think about failures that could happen, and what could cause that to happen, and how the recovery would take place. Mature organizations move from the theoretical to implementation of Chaos Engineering but this is exceedingly rare. RCA’s are for Systems RCA’s at Intuit are not just about software, they are for systems. If any complex tasks — for example, a conference — fails to do as expected, an RCA is often done at Intuit to get to the root cause. These are often reviewed at the Executive level, including by the CEO. Commitment from Senior leadership RCA’s uncover cultural issues, process issues and system architecture issues. For an RCA to be effective, Senior leaders need to be present for the RCA to make decisions on reprioritization, changes in policy and redeclaration of goals. At Intuit, Engineering Directors take responsibility for leading and reviewing RCA’s. In addition, RCA’s of Production incidents are shared with the entire engineering team for review and reflection. In conclusion, RCA’s are an important part of product development and can help teams be more thoughtful, deliver insights and manage tbe complex systems we work in. Acknowledgement Luu Tran (now at Amazon) helped with the thinking around doing RCA’s at Intuit and getting to root cause. Cool stuff we are doing and thinking about in engineering… 24 Engineering Systems Thinking Systems Engineering Software Development Software Architecture 24 claps 24 Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-15"},
{"website": "Intuit", "title": "resiliency two alternatives for fault tolerance to deprecated hystrix", "author": ["Ranadeep Bhuyan"], "link": "https://quickbooks-engineering.intuit.com/resiliency-two-alternatives-for-fault-tolerance-to-deprecated-hystrix-de58870a8c3f", "abstract": "A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. It is the ability of maintaining functionality when portions of a system break down is referred to as graceful degradation. Dealing with unexpected failures is one of the hardest problems to solve, especially in a distributed system. Resiliency is the ability to recover from failures and continue to function. The goal of resiliency is to return the application to a fully functioning state after a failure. Circuitbreaker is the primary design pattern that is followed to solve this problem. Hystrix is (was) a very popular fault tolerance library for server side engineering. It handles resiliency effective in the micro services world that developed and maintained by Netflix. However, it is now in maintenance mode and no one is going to release any updates as of today. Interested community members are requested to reach out hystrixoss@googlegroups.com take ownership of Hystrix for moving back into active mode. However, that gave everyone in the community to explore other available libraries that are being maintained and have a strong back. I happen to study and evaluate a couple of other options viz Sentinel and Resilience4j and going to compare them comprehensively so that the community get benefited by taking faster decision on their choices. Let’s quickly look at the Circuitbreaker pattern and how Hystrix implements it. The circuit (an object) has three states in operation. Closed — Allow the remote Connection through Open — Break the remote connection Half-open — Allow Limited Number of connection to Test (aka Limited Closed ) Hystrix calls to the external world has to be wrapped with HystrixCommand annotation. It do supports spring-boot annotation. As follows -> After that, all are based on the command aspect or in that context with code and configurations. Resilience4j, in contrast provides higher-order functions (decorators) to enhance any functional interface, lambda expression or method reference with a Circuit Breaker, Rate Limiter or Bulkhead. Furthermore, the library provides decorators to retry failed calls or cache call results. That means, we can combine a Bulkhead, RateLimiter and Retry decorator with a CircuitBreaker decorator. The advantage is that we have the choice to select the decorator. Hystrix and other two libraries emit a stream of events that are useful to system operators to monitor metrics about execution outcomes and latency. Semantically, Hystrix has configurations in the code therefore is it tightly coupled with the HystrixCommand callback. One interesting difference in resilience4j is ability to add multiple fault tolerance features into one call as below. At the end of the day it is more configurable and amount of code needs to be written is less with right amount of abstractions. The default Resilience4j Aspects order is the following: Retry ( CircuitBreaker ( RateLimiter ( TimeLimiter ( Bulkhead ( Function ) ) ) ) ) If we need a different order then we can use the functional chaining style instead of the spring annotations style. Sentinel has the same syntax too. The primary difference with Resilience4j is the granularity of control and ability to be integrated with other live monitoring systems. Therefore I would prefer Resilience4j over it. Sentinel has its own dashboard module. It assumes it will run in a distributed multi clustered environment always, therefore it comes with a baggage of dependencies no matter we need it or not. Functional java example Reactive java example Core modules resilience4j-circuitbreaker: Circuit breaking resilience4j-ratelimiter: Rate limiting resilience4j-bulkhead: Bulkheading resilience4j-retry: Automatic retrying (sync and async) resilience4j-cache: Result caching resilience4j-timelimiter: Timeout handling Hystrix is a more matured product and proven through time. Compared to Sentinel, Resilience4j is new however, looks more promising as it tries to culminate some of the goodnesses from both of them. Good news is that HystrixJS is not managed by Netflix. It is one of the very popular npm modules among nodeJS developers. It uses RxJS. The library provides a module HystrixSSEStream to export gathered metrics as a server side events stream. Well, there is a plethora of choices to exactly log and see what is happening to the integration. It took me two days to decide on what my team should learn and use. Resilience4j provides in-built micrometer-core (unlike hystrix added the whole of the library with many dependencies) support with a bunch of useful metrics in it such as State of the circuit, slow call rates, failed rates, retry matrices etc. [ https://resilience4j.readme.io/docs/micrometer ] That’s it! This will register all available metrics to Micrometer event registry. However, there is no way we could view those metrics without doing anything. Couple of easy approaches are mentioned as below — in-order to view those exported metrics. Pull based metric Micrometer -> Prometheus -> Grafana In this case, micrometer.io (spring-boot) library creates logs based on configurable metrics. There are a couple of ways that can expose this data out from the application server. One idea is to run an agent like Splunk or AppDynamics like a side application and push the data to their respective logging services. Event-based metric Micrometer -> Vert.x -> telegraph -> wavefront Below example the circuit breaker events are captured by micrometer and formatted by Prometheus. Cool stuff we are doing and thinking about in engineering… 12 Microservices Alibaba Netflix Fault Tolerance SaaS 12 claps 12 Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-22"},
{"website": "Intuit", "title": "personalization intuit part 1 use cases", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/personalization-intuit-part-1-use-cases-ca0b524ad528", "abstract": "By Anil Madan Platform Engineering, Mayank Gupta Product Management Personalization Our mission at Intuit is to Power Prosperity Around the World for small businesses, self-employed and consumers. We want to generate more money, more time, and more confidence for 50+ million people. By leveraging our data we want to build delightful experiences and unlock the power of many for the prosperity of one. A key capability that powers our mission is Personalization. In this three part series we will walk you through how we solve in QuickBooks Online (QBO) for customer delight through Personalization. In Part 1, we will look at the key user interactions where we want to create delight, in Part 2 we lay some groundwork and provide a functional overview, while in Part 3 we will share a high level overview of our platform. Internally we are maniacal to solve for our customers, and like any experience that we build and release, we first posed the fundamental question back to our customers, how can we best create delight? To answer this we conducted extensive D4D to understand what our customers were looking for. What attracts them to us? What features or products would they be most interested in? The exercise revealed some interesting scenarios in why users come to us. Some were starting a new business. Some wanted to complete simple tasks like send an invoice, sign up for payroll or payments, or looking to improve operational efficiency for their existing business. Some wanted to sign up to try or subscribe to our products. Below we cover each of the scenarios by first identifying the problem , then create our hypothesis and finally defining how we solve them. Scenario 1 Problem Statement — In first scenario where the customer who just started a new business and is using QBO for the first time, some of the issues she usually runs into: How do I send invoice or connect my bank account? How do I get paid, how do I pay my employees? How do I connect to apps that help me run my business more efficiently and save time ? H ypothesis — By personalizing the content for an individual user, we aim to help them derive the most value out of Quickbooks, so that they can streamline their business operations; by predicting the right set of products -Payroll, payments, apps- for the users, we aim to help them grow their business. Solution — Take a look at the prediction below on the left. It is specifically shown to users who are new to the product and who need help in learning Quickbooks, so that they can use it to streamline their business operations. In the other example on the right, we are recommending that the user connects to TSheets, so that they can start tracking time for their employees. This is specifically shown to users who have Employees or Contractors and who need help with tracking their time. Scenario 2 Problem Statement — Small businesses have a hard time to understanding how to use QBO “I can’t find the features I need or know how to use them. I’ve tried to find this information myself, but it’s hard to figure out even where to start. This makes me feel uncertain, worried, and confused when I call Intuit for help.” Hypothesis — By providing proactive self-help to our users we could help them discover what they are looking faster based on their past and current behavior, browsing history and their familiarity with the product. Solution — See the list of the articles below. The articles listed below are personalized for individual users. The articles are recommended based on the context, profile and behavior and what similar users found useful. Scenario 3 Problem Statement — Customers come to our HomePage to either learn more about specific Quickbooks products like Accounting, Payments, Payroll with the an intent to sign up, or they have an immediate poroblem to solve like sending a project estimate, an invoice, pay their employees or accepting payments. It is hard for them to sift through all the possible products to choose what best meets their business needs and feel confused. Hypothesis — By recommending a service or product to the user that meets what he or she is looking for we can quickly help him/her with a seamless sign-up process and solve their immediate problem. Solution — Based on the visitor, what channel they are coming from — Search Engine, Email, or Social channels , what their true intent is, we optimize the Quickbooks Home Page. We also predict what service or product can help grow their business. In the example below we are suggesting that the user sign up for payments to get paid faster. Hope this gives you a good sense of some of the scenarios on how we Personalize to create customer delight for our customers. In Part 2 we will provide a functional overview of Personalization. Cool stuff we are doing and thinking about in engineering… 50 Personalization Relevance Experimentation Intuit Optimization 50 claps 50 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-06"},
{"website": "Intuit", "title": "intuits evolution from content management to content engineering part 1", "author": ["Mawuse Golokuma"], "link": "https://quickbooks-engineering.intuit.com/intuits-evolution-from-content-management-to-content-engineering-part-1-7bb5ad06fcf1", "abstract": "As companies take their products globally, it is important to evolve from traditional Content Management practices to innovative ways that scale to deliver localized customer experiences with speed . Content is a key component of the customer experience — imagine your products without UI content — and Intuit faces the challenge of scaling its content for its global expansion. To address this challenge, Intuit is on an accelerated path to evolve from “Content Management” to “Content Engineering”. This paper describes how this evolution is helping Intuit to reach other markets with speed and scale. In part 1, we examine the scope of Intuit’s Content Ecosystem. In part 2, we describe how Intuit is building technology capabilities to enable scale and speed in that Content Ecosystem. CONTENT AND CONTENT MANAGEMENT Software development is a complex process that we have traditionally built strong industry tools around. The software industry has an array of sophisticated tools to enable software developers to manage the code delivery pipeline from development to testing to deployment and monitoring. When a product is released, engineering is often “done” and we even have explicit done-ness criteria to make sure the product is of high quality. What we often fail to realize is, to deliver a truly amazing customer experience, we have to ensure the associated content is produced to the same high-quality standards, in any language. Let us take QuickBooks as an example. Think about the all-important First Time Use (FTU) experience. The first time a customer interacts with QuickBooks, they will likely see our website that walks them through a few options before they get into the actual product. Soon after, they get into the product user interface (UI) and they might even receive a welcome e-mail from Intuit to provide more detail on what they can expect. All these interactions are fundamental tools we use to build trust with our customers. We build that trust with words and images on the screen. At this stage, the customer has likely not even interacted with our Help system or our Conversational UI. In our complex ecosystem, we also rely on our local partners whom we qualify with training material or collaborate with using legal documents . What we have is a complex content ecosystem . These different content sources are at the heart of how we build trust with our global customers and partners on the web, iOS and Android platforms. In a unilingual, single-locale deployment, it is common for companies to succeed using rudimentary tools and traditional Content Management Systems (CMS) to manage all of its content needs. As soon as a company decides to go global, it is no longer uni-lingual or single-locale deployment, and you start finding traditional CMSs not scaling out of the box to address the required localization and translation needs. The challenge multiplies when you include the need to address even more variability such as different SKUs and point-in-time-based variables such as the current year. What begins to emerge is the ugly pattern of manual CPLT ( C opy-> P aste-> L ocalize-> T ranslate) efforts to deliver the same delight across locales that the base American English language delivers. To the extent that companies are able to sustain this ugly pattern with brute force, the customer’s online experience becomes sensory-overloaded as they sift through a lot of online content to determine what content applies to their locale, SKU, etc. Furthermore, there is the additional challenge of synchronizing common terminology across all the content types and locales to ensure a customer experience that feels 100% local. The pain is not only limited to the customer experience. Our employees who maintain, localize and translate the content also live through the challenge of inefficiently maintaining content that is processed through the manual CPLT pattern. Let us take a closer look at some sample content types Intuit has to deal with: As you can see from the above examples, content comes in various formats and structures to be localized and translated. In Part 2 , we’ll describe how Intuit has leveraged technology to address the scale and speed challenge that comes with handling all this content across the enterprise — from CMS to Content Engineering . I have the privilege of leading an amazing group of individuals whose work has shaped Intuit’s content globalization platform — Lucio Gutierrez for his vision in developing the core technologies that power content globalization, Tracy Macdonald for her deep knowledge of content localization, Katia Belanger and Marina Raphael-Mayans for contributing their Translation industry experience and Sandi Sinticolaas, Miles Sabin, Kathleen Lai Parisa Mohebbi, Colton Begert and Tahar Nasri for their countless hours of reviewing the documents. Cool stuff we are doing and thinking about in engineering… 35 2 Content Strategy Globalization Intuit Content Management 35 claps 35 2 Written by Content Globalization Leader @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Content Globalization Leader @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-22"},
{"website": "Intuit", "title": "personalization intuit part 2 functional overview", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/personalization-intuit-part-2-functional-overview-f6820bf1365c", "abstract": "In Part 1 we looked at some of the scenarios on how we Personalize to create customer delight in their interactions with us. In Part 2 we will look at a functional framework in how we go about solving for those scenarios. Let’s start with the basic definition of Personalization. Personalization enables suggestion of message or content that is relevant to the individual user, based on user’s implicit behavior ( Optimization ) and/or explicitly preferences ( Customization ). Optimization uses implicit interests and learns what you like from your actions. Customization on the other hand is driven by explicit preferences. Optimization usually is at the message/content served level while Customization is at a “feature” level, where you see/hide etc. what you want explicitly through QuickBooks Account Preferences. There are several distinct facets to Personalization Who is the customer/prospect (and how much do we know about him)? Identity — information about each individual customer’s identity — user, small business entity, visitor (web/mobile), social & email Id. Profile — Demographics, activity, profile, transaction history, and so forth. Behavior — information about customer’s interactions. What gets personalized? Message — optimize the message are you trying to convey. Content– optimize the content — which can be for the entire page,a module or a component. Where is the interaction happening — These are the touch points across the entire QuickBooks online ecosystem. When is the personalization decisions made? Before — this usually is based on past behavior, actions and analytical segments. During — generally higher payoff as it leverages contextual data to optimize. How is the personalized message selected? — There are two different approaches to how the personalization decision is made Deterministic — generally through static data . While this approach does give more control over the segment definition and makes it easy to anticipate success, it has limitation as data may quickly become stale or irrelevant. Predictive — uses machine learning algorithms to predict the best course of action. The models generally are adaptive, iterative and self-improving. Why is real-time personalization important? The ability to connect with customers on their own terms, during their interactions is important and can dramatically improve the effectiveness of engagement efforts. With this background, Personalization can be broadly defined as optimizing for an end goal or an Objective that leverages characteristics like segments, user preferences, locale and context. Objective — The end goal that we want the user to accomplish or the function on what we want to personalize upon. Segment — A collection of attributes that group users. Context –Account, Transaction, Activity or User Session. User Preferences — explicit preferences or settings. Locale — product for the local market (intersects with Globalization). Personalization builds upon Globalization and Experimentation. The following section provides a conceptual framework on how you can coherently integrate these loosely coupled systems to tailor your products for your customers. Globalization (G11n) is about building internationalized (I18n) products & services for the global market and then localizing (L10n) the product capabilities per locale — menus, sitemap, currency, language, country, user registration rules. Personalization builds on top of a compliant product features & capabilities which are available for a market. Personalization vs. Experimentation Experimentation is the process of testing & learning on visitors/users by serving two (A/B) or more distinct experiences on a population — i.e. multiple users simultaneously. For e.g. an experiment may have two factors each with two levels — Layout (top or left) & Creative (creative 1 or 2) served to distinct random populations. Experiments are a function of factors served to populations with the objective to measure a statistical significant lift. Personalization on the other hand is about personalizing the experience of a specific user. It is similar to Experimentation as the building block is factors , however unlike Experimentation, has coefficients or weights that are evaluated on a particular user (and not on a random population). While both Personalization and Experimentation has the lowest common denominator as factors, each factor is mutually exclusive for a given scope (product flow, page, module, component,etc). To illustrate, hypothetically, if page layout is a factor on the page you are experimenting, you would not experiment and personalize on l ayout simultaneously. Assuming you are personalizing on layout on the page, you may subsequently perturb on the creative diffrent content and serve say creative 1 or 2. The deviations to a factor ( creative 1 or 2 in this example) is often called Perturbation — as you are continuously experimenting to refine the model. The interaction of Personalization with Experimentation is shown below. To conclude Part 2 , Personalization is a message or content that is relevant to the individual user, build on top of Globalization and interacts closely with Experimentation. In Part 3 , with this background and foundation, we will look at how we have built out a Personalization platform that scalably solves for customer delight. Interested in working on these areas to power prosperity for Small Businesses? We are hiring… Cool stuff we are doing and thinking about in engineering… 11 Personalization Optimization Experimentation Machine Learning Data Science 11 claps 11 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-17"},
{"website": "Intuit", "title": "https medium com krvinod1 build to scale messaging based job server", "author": ["Vinod Kumar"], "link": "https://quickbooks-engineering.intuit.com/https-medium-com-krvinod1-build-to-scale-messaging-based-job-server-4e0f4f3adab2", "abstract": "Intuit’s integration platform is used for data-in integrations between strategic e-commerce leaders like PayPal, Square, Google calendar, GoCardless, and Stripe into customer’s Accounting books. Saving time for the subscribers of these integrations. This no-coding platform is deployed as a service and managed by Inuit’s integration platform team. It supports four kinds of integration patterns Data Sync — Running background jobs to sync data Interactive application Webhooks Trigger action workflow Intuit integration platform at a glance: Performance and scalability journey: The Integration platform is composed of Connectors, Jobs, UI, and API. These components can be scaled vertically, but are bottlenecked by a single database server, limiting throughput. Data sync integration is the primary form of integrations. A typical data sync integration subscription will run a job multiple times a day to ingest data. A job and its runtime state are completely managed in the database, this is one of the key contributors to the database load resulting in processing delays. The journey started about two years back with initial focus on low hanging fruit to relieve immediate pressure on data processing delays. This ended up with a 350% improvement in the job server: Introduce Caching layer — to reduce database lookups Migrated few tables to DynamoDB — for faster writes De-normalized few tables and create table index Optimized application logic Archive obsolete old data Although the initial results gave us a runway for the next year, the integrations platform had to reach the state where it could scale horizontally to support the projected growth. The job server relies heavily on the database server for work items and their run time state. It was initially architected to operate in a single instance mode but when the platform started hitting throughput caps, multi-node support was introduced by way of a distributed lock acquisition strategy. Job server capacity improved but a new ceiling quickly emerged as there was contention for acquiring locks on work items. A classical legacy architecture problem. Following is the simplified view of the multi-node architecture of the job server. Messaging based job framework: The above architecture violates the famous single responsibility principle of SOLID design . The existing job server has two responsibilities, job scheduler, and job runner. In fact, if one component had more than one responsibility it resulted in a complex and unmanageable code. That leads us to rethink the existing job server architecture and design. We decided to re-architect the job server framework based on the event-driven architecture. The new architecture will have two main components as shown in the following picture (simplified version). Job producer: Queue candidate jobs Job consumer: Execute a candidate job The current design allows us to use any messaging platform in the future, but we are currently using AWS SQS as our messaging platform, which enables us to scale without managing the other system. We deployed the new system (partially implemented) in production and the result has been very encouraging. The following graphs provide a glimpse of improvements Key performance highlights: Jobs processing increased from 1.4 million to 12.5 million per day, which is a 793% improvement from 2016 and 268% from 2017 No more delay of Priority jobs. All priority jobs are executing in real-time No jobs waiting more than 60 minutes. 15–60 minutes delay improved by 1100% from 2016 and 520% from 2017 Job delayed time reduced to 3 minutes from 90 minutes in the best case scenario, which is 2900% improvement. Average job delay reduced to 16 minutes from 99 minutes, which is 520% improvement In Summary, Be bold, do not hesitate to throw away what is working now, use loose coupling, follow single responsibility design principle, be tactical to buy time for bigger changes, and finally track everything. Cool stuff we are doing and thinking about in engineering… 13 Intuit Messaging Scaling Integration 13 claps 13 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-15"},
{"website": "Intuit", "title": "quickdata costbuddy taming aws cost proactively", "author": ["Giriraj Bagdi"], "link": "https://quickbooks-engineering.intuit.com/quickdata-costbuddy-taming-aws-cost-proactively-e265e7147f1", "abstract": "As organizations move to the cloud, budgeting, tracking, and optimizing dollar spending in the cloud is becoming a critical capability. This is universally true for all teams, and especially exemplified in Data Platform teams supporting multiple Analysts and Data Scientists as tenants. This blog post describes our challenges with cost accountability and budgeting as we transitioned to operate 100% in AWS. We developed a methodical mechanism we have implements to manage cost. Single View that provides AWS Cost Details for multiple accounts (Dev/Prod) to Management so that we can proactively forecast and manage costs Trend of AWS Cost — Forecasts/Actuals Provides AWS cost mapped to accountable tenant leaders (either owning accounts or tenants within the account) Provides a cost view for accounts Provides Alerting sent to individual leaders based on their spend-to-budget ratio and daily trajectory Provides cost roll up of managed services like AWS EMR, Athena based on the accountable analyst Tracks untagged & underutilized resources There were several alternatives that we explored and to address all the needs for the QuickData Platform we e have implemented QuickData Cost Buddy CostBuddy gathers the raw data collected from AWS for all the account spends. The data is stored in Redshift. The data represents the dollar spend of each account along with the tags associated with the resources CostBuddy combines this data with budgets and leader org structures. It tracks alerts w.r.t. spending anomalies based on spend-to-budget ratio as well as several other patterns. The AWS cost spend is further broken down by tags. Each leader can further diagnose spends on AWS resources. In a multi-tenant environment, alerts can be defined for individual deployments (such as AWS EMR cluster for an individual team, AWS Athena Cost spend by an individual Team). Each accountable leader can look at the cost for an AWS Account, Group of Accounts, tenants, Tags to manage the cost of their organization, henceforth providing the high level or drill down of the infrastructure cost being incurred in their respective organization. AWS Trusted Advisor data about resource usage data is collected for the accounts based on the accountable leader/tenant and data is displayed on the Dashboard per accountable leaders to manage resources and take appropriate action on the underutilized resources. Similar pattern will be leveraged as QuickData Platform Teams onboard other services like AWS QuickSight etc to support Analyst Community. Consolidate/close unneeded accounts like Logging, CloudTrail, AWS Support Cost) In case of any drastic increase in cost the alerts will enable leaders so that proactively measures can be taken. Figure 1: Aggregating AWS spend across accounts The traditional way of managing dashboards through the UI never worked for us when we started to scale because of following reasons :- Consistency: Traditional ways of managing dashboards through the UI have consistency issues. Automation: Inability to automate the creation/modification of Dashboards and alerts. History: Rollback of unintended change(s) was impossible. However, managing and reviewing changes in huge json files were near to impossible. We wanted a solution that does some basic validations and not just push a faulty json, should manage a state of the current system, easily adaptable and flexible, should support rollbacks. Hence we decided to move on with terraform. With terraform wavefront provider, we were able to achieve all the goals without any tech debt. Terraform with git takes care of the consistency. Declarative nature of terraform gives us more power. We always define the desired state. Terraform takes care of how to achieve it. Easy to maintain test and production environment with Terraform Workspace. Code is more readable than nested JSON. This helped review changes easier. Rollbacks become easier with terraform and git. We have few educational series blogs on how we created Dashboards as code. Blog series Part I , Part II and Part III We use Wavefront for our Dashboard, hence we were using terraform-wavefront-provider , where we have also made contributions based on our learning towards Dashboard as code. Similar providers can be found for other vendors as well including Grafana. In summary, AWS CostBuddy provides a one-stop for teams to get visibility on the spend as well as track daily spend w.r.t. allocated budget. Moving forward, CostBuddy will be integrated with alerting solutions such as PagerDuty, etc. This will allow account leaders as well as tenant leaders to get notified for different types of alerts. Cool stuff we are doing and thinking about in engineering… 14 AWS Wavefront Terraform Aws Cost Management 14 claps 14 Written by Group Engineering Manager in QuickData Platform. Small Business Group at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Group Engineering Manager in QuickData Platform. Small Business Group at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-15"},
{"website": "Intuit", "title": "intuits evolution from content management to content engineering part 2", "author": ["Mawuse Golokuma"], "link": "https://quickbooks-engineering.intuit.com/intuits-evolution-from-content-management-to-content-engineering-part-2-528cbf1fab83", "abstract": "In Part 1 , we described the scope of Intuit’s content ecosystem and the challenge of scaling that content globally. In this section, we examine how Intuit is innovating with technology to automate the delivery content globalization at scale. The Globalization Team at Intuit has developed a technology called Stackable Filters (SF) to efficiently manage all content types and localization efforts. SF takes any structured content (json, html, xml, props, css, etc., and infinitely nested forms of these with variables and conditions) and resolves it into a universal content type that we call UCC (Universal Content Container). SF converts a file with multiple layers of content into a set of unified components (UCC file). These components have the same properties and behaviours that enable for a holistic and efficient manipulation and localization of content by machines and humans. After components have been treated, the SF Engine assembles the pieces back together into the original set of layers, with translated versions as applicable. Every content type we create in our ecosystem is already a candidate for this kind of processing. STACKABLE FILTERS IN DETAIL The three main classes of text processing objects are Files , Filters , and Atoms . A File contains one Filter ; a Filter contains a list of Atoms (possibly of varying types). FILE A File object understands encodings (e.g. UTF-8, UTF-16, etc.) and line endings (e.g. Linux/Unix = LF (Line Feed), Mac OS = CR (Carriage Return), Windows = CR+LF). It knows the actual path to the file in question and is responsible for all reading and writing to the filesystem. A Filter object is a text processor, and it parses a text stream into Atoms . The structure of the filter dictates that it will break up into n categories of information, each of which can be passed a sub-filter to operate on. For example, a .properties file would be broken down into atoms of the type key/value , and atoms of the type comment . Therefore there are three categories of information in a .properties file: key, value, and comment. You will note that the categories do not necessarily correspond to Atoms . An Atom object is the largest repeating set of content that makes sense. For example, in iOS .strings files, there is guaranteed to be a comment right above every key/value pair. Therefore, in the iOS world, it makes sense to have a comment/key/value triplet. If comments were optional and appeared only sporadically, then you would have to have a key/value atom and a comment atom both. FILTER Each filter has a certain set of information categories that pertain to it. For example, HTML filters will have Tags and Text . A JSP filter would have JSP and HTML . Our dictionary-type files, like .properties filters or iOS .strings filters, will have Comment , Key , and Value . These need to be passed as a 2-tuple of class and a dictionary which is keyed on the name of the category tied to the tuple which describes what filter comes next for that type of content. Example syntax call: Sample File: Results: IOSFilter creates one atom consisting of: comment : “Sample string” key : “TEST.KEY” value : “<? taxdict …….. </div>” The comment is passed to AnnotateFilter , which is a leaf node, and will do something. Let’s say it regards all text as actionable. The value is passed to JSPFilter , which creates two atoms: JSPAtom : “<? tax…}; ?>” HTMLAtom : “<div>…</div>” The JSP text is passed to JSPTaxDictFilter , which creates three atoms consisting of: codeAtom : “taxdict = {” taxkeyvalAtom , key : “tax.thing1” taxkeyvalAtom , value : “User-facing popup” code atom : “}; ” but we don’t care because it is a leaf node, and just knows what to do with taxkeyval.value as actionable, and nothing else. The HTML text is passed to HTMLFilter , which creates a set of Tag and Text atoms. The HTMLFilter passes its Tags to HTMLAttributesFilter , which is a leaf node that knows to expose the text of alt and title attributes to user-facing text manipulation, but nothing else. The HTMLFilter passes its Text to MailMergeFilter , which creates a set of Text and MergeVar atoms. But we don’t care because it’s a leaf node and it knows that Text is user-facing, and MergeVars are not. A Filter needs to be an iterable object, which will move through a list of Atoms , or possibly other Filters , which internally are treated the same. Required methods for all Filters: __iter__() all filters are iterable objects __len__() all filters are iterable objects load( text ) can pass the defining text to the object __init__, but can also load and parse the text after object creation using .load() print() this will re-create the text stream in its own format, so by itself (with no transformative functions run on it) this should return an exact copy of the text that was used to construct the object. transform( func, dict ) this will execute the function func on the filter. If it’s a text transform, it will operate on the pieces the filter knows to be user-facing text. The filter iterates over its list of atoms and forwards the function that it wants to apply over the user-facing text the returned value will be treated differently depending on the filter that invokes it string() synonym for print(). Otherwise identical. Any functions that need to appear in both a Filter and an Atom — most obviously .escape() and .unescape() — are implemented as Helper functions. Functions that we want to be applied to the filter tree can be passed with the .transform() function, which passes the function and a dictionary for its parameters. .transform() will be a Helper function, as it can be applied to both a Filter and an Atom. Transform functions can alter user-facing text or can include or exclude an atom (for conditionalization). Note that it is up to the filter (and the filter’s atoms) to know exactly what bits of text are operable and which are not. So if we ask it to run a text transform, it should know which bits of text are available for that. ATOM Each atom will contain one or more of the information categories and some helper functions. It’s a consistent wrapper to each node of information in the filter file. Atoms for dictionary objects are atomic to a key, and not an entry. There are two syntaxes which seem to be incompatible with each other: one lends itself to atoms which are one key and all permutations; one wants atoms which are a key/locale/value triple and can have multiple atoms per key. In the first case, a localized function can be run on the atom and return a complete answer for that key. In the second case, it cannot. We’ve chosen the first to be our standard, so the eventual implementation of our “overridable” properties files will have to adjust. One atom per key, which includes all information about that key. The base Atom class contains a public .text variable, as well as other metadata (like .type, also a string). Other Atom specializations derive from it, and can be made up of multiple Atom references. Required methods for all Atoms: getValue() for the root Atom class this is synonymous with .text, but for more complicated Atoms it won’t be. print() this will re-create the text stream in its own format, so by itself (with no transformative functions run on it) this should return an exact copy of the text that was used to construct the object. transform( func, dict ) this will execute the function func on the atom. If it’s a text transform, it will operate on the pieces the atom knows to be user-facing text this function is a reusable interface that allows us to apply different operations over text since the transform method operates over text, this function will return back a new String object different atoms are expecting different return values. The transform method will forward the response obtained from the function that is applied over the text by every atom CLASS DIAGRAM Filters and Atoms are the same in terms of in-memory objects. They both are treated the same for text processing. At the abstract level, a Filter is a more specialized instance of an Atom, which allows to abstract behaviours that are specific to Filters. Atoms contain all the attributes and behaviours for text processing. Both Filters and Atoms are derived from a Scanner engine that uses regular expressions to identify pieces of content in a file. When a Scanner is done generating Tokens, these will then be transformed in Atoms and Filters (in-memory objects in a tree). When Filters and Atoms have been created, these get passed downstream for further processing. At the end of the text processing workflow, all Filters and Atoms know how to integrate the pieces of a file back together for outputting to a file object. As you can imagine, technology (Stackable Filters) alone will not deliver the capabilities we desire with scale and speed. For example, as part of our globalization playbook, we must be capable of supporting new languages in our entire ecosystem within a day of making that decision. Also, especially because Intuit is in the Financial space, our terminology must be 100% locally accurate to provide confidence in compliance with local regulations. Furthermore, in this day of social media, we must be able to update our UI within minutes of an issue being identified. We have therefore built capabilities around SF and the UCC to help us process all types of content at scale and with speed. The diagram below describes where in our localization and translation process we apply this technology: The process we’ve developed around this technology is SASSL — Structured Authoring, Single-Source-Localization. Any content is authored once and then processed into a structured format (UCC) which can then be localized in one place and then made available for translation. The UCC is then processed to ensure that Translators are not seeing all the encoding in the content. After translation, the resulting file is then reconstructed with the original encoding to be published. In summary, our process is highly automated to process all content types as follows: Content is created by Authors. Apply local terminology across all content types to ensure a great customer experience. Apply regional segmentation rules to set the stage for geo-fencing. Generate regional derivatives of content to support search engine optimization. Apply machine or human translation as applicable. Store all content to the globalization one database. Store all content back to their respective repositories for publishing. We believe we’ve addressed the content localization challenge through elegant engineering. Even more importantly, we’ve solved it in a generic way that applies to any software enterprise that requires content to scale globally with speed to enable amazing customer experiences that are personalized to build trust. I have the privilege of leading an amazing group of individuals whose work has shaped Intuit’s content globalization platform — Lucio Gutierrez for his vision in developing the core technologies that power content globalization, Tracy Macdonald for her deep knowledge of content localization, Katia Belanger and Marina Raphael-Mayans for contributing their Translation industry experience and Sandi Sinticolaas, Miles Sabin, Kathleen Lai Parisa Mohebbi, Colton Begert and Tahar Nasri for their countless hours of reviewing the documents. Cool stuff we are doing and thinking about in engineering… 24 1 Content Management Globalization Content Strategy Quickbooks 24 claps 24 1 Written by Content Globalization Leader @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Content Globalization Leader @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-22"},
{"website": "Intuit", "title": "personalization intuit part 3 platform", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/personalization-intuit-part-3-platform-7cf80f6475f1", "abstract": "In Part 2 we looked at how Personalization is a message or content that is relevant to the individual user, built on top of Globalization and interacts closely with Experimentation. In Part 3, we will look at how we have built out a Personalization platform that scalably solves for the scenarios we covered in Part 1 . There are 5 foundational blocks to Personalization namely Globalization, Experimentation , ML , Profile and Tracking & Instrumentation. Globalization and Experimentation are covered in detail in prior blogs, ML and Profile are below, while Tracking will be covered in a subsequent blog. ML is to critical component for our personalization strategy. Background — A ML algorithm uses example (training) data to create a generalized solution (a model ) that addresses the business problem that needs to be solved. After you create a model, you can use it to answer the same business question for a new set of data. This is also referred to as obtaining inferences. A typical ML model cycle is shown below. Build — this is the phase where our Data scientists discover,explore and prepare the relevant data needed in our Data Lake. Additionally they narrow down on the right algorithm needed for a given problem. The algorithm depends on a number of features — relevant attributes or factors that are needed to develop and train the models. The process of feature engineering is to use the domain knowledge to extract the features. Train — After deciding on the approach, we need to teach the model how to make predictions by training. This involves providing the ML algorithm with training data to learn from. The features are selected from the training data. This phase can optionally also involve hyperparameter optimization. Hyperparameters unlike features are decided before fitting the model because they can’t be learned from the data. Deploy — after the model is trained it is deployed for inferences. The model performance is evaluated using different techniques . The model evolution is a continuous cycle. After deploying a model, you monitor the inferences, collect performance data and evaluate the model to identify drift. You then increase the accuracy of your inferences by updating your training data to include the newly collected performance data, by retraining the model with the new dataset. As more and more example data becomes available, you continue retraining your model to increase accuracy. A core part of the lifecycle is the ability to manage all the artifacts associated with a model. This includes the source code, environment metadata, feature-sets, training sets and trained models. This allows us to tie the various components together into a seamless platform and support end to end automation of the model lifecycle. A model can predict an outcome during a user interaction in one of the following ways. A combination of features along with their respective coefficients are evaluated online for a given context to produce a score. A combination of features along with their respective coefficients are evaluated offline to produce a score. The algorithm(code) is evaluated online for a given context. It can optionally look up a combination of features and/or scores. A user profile needs to encapsulate the view of customer with all profile attributes such as behavioral, social, mobile, demographics, transactional, contextual and location data, with the ability to aggregate & cross link 1st party (mobile, web) 2nd party & 3rd party data sets seamlessly across distinct data sources. The data model needs to be flexible to support extensibility where new attributes can be added dynamically. The profile service can Personalize either on a strong identity like logged in user, phone, email or a weak identity like cookies, mobile identifiers or social identifier. The profile backend needs to supports highly parallelized ingestion of new identities and profile maps with inbuilt data security, privacy and compliance. Any Personalization technique has serious consequences to privacy and needs to be carefully evaluated around website’s data collection, data usage and sharing policies. It needs to be either disclosed in T&C or driven by explicit user consent. The profile service needs to support both deterministic segments or predictive scores. With all the concepts defined, a simplified architecture for serving of Personalization content is shown below. Users interact with QuickBooks Products across different touch-points. The Products as part of displaying the user experience makes a call to the Personalization Service, passing the context — identity, session and scope and what model to use to personalize. Personalization Service calls the Globalization Service to check what product capabilities are available for a given locale (country, region, language). This is followed by a call to Experimentation Service to check for eligibility to determine if there are active experiments running for a particular scope (component, page, product flow). If yes the Experiments triumph over any Personalization as we cannot experiment and personalize at the same time. This is followed by a process of model evaluation. The model could be i) a set of features with associated coefficients sitting behind the Feature Service ii) a set of predictive scores sitting behind the the Profile Service iii) or an the algorithm deployed behind the Model Inference Service or a combination of i) and ii) passed as an input to iii). Each of the steps between 2 to 4 above write the appropriate server side instrumentation using the Tracking Service that is used for further optimization. The Personalization Context and Data is returned back to the to the product applications to display the experience. Finally there is client side instrumentation through the Tracking Service that helps close the loop on the user interaction. To conclude, at QuickBooks we leverage Personalization to create user delight . There are 5 essential components to a successful Personalization strategy namely Globalization , Experimentation , ML , Profile and Tracking. Interested in working on these areas to power prosperity for Small Businesses? We are hiring… Cool stuff we are doing and thinking about in engineering… 68 Machine Learning Personalization Optimization Globalization Data Science 68 claps 68 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-17"},
{"website": "Intuit", "title": "automating data sources discovery governance with virtual steward", "author": ["Sandeep Uttamchandani"], "link": "https://quickbooks-engineering.intuit.com/automating-data-sources-discovery-governance-with-virtual-steward-91bbbea25bed", "abstract": "Data is the new Oil i.e., a key source of differentiator for organizations across every vertical sector. There are multiple sources of data divided into three broad categories: SaaS application data, Behavioral data, and Third-party external data including social feeds. In this post, we focus on the use of SaaS application data for extracting analytical and ML insights. In the context of Intuit’s Small Business & Self Employed Group, the SaaS application is the QuickBooks Online offering used by millions of users. Modern SaaS is built using micro-services; each micro-service manages its own data using relational or NoSQL data models. The structure of the data is constantly evolving as micro-services get split, modified, re-designed with new features implemented within the SaaS. A major pain-point today is in keeping up with these data changes to ensure downstream analytics and ML are leveraging these data changes correctly and in a timely fashion. Micro-services teams (often 2 pizza teams) are moving fast to launch new features often with limited or no communication to the downstream data analytics teams about new SaaS features or changes. Often, given the execution agility, teams are unable to coordinate details of new SaaS features and the corresponding data with the downstream teams till fairly late in the launch process. To summarize, the key pain-points today are: - Keeping up with data sources within the SaaS application - Proactively tracking changes in data across 10s of micro-services teams - Monitoring data quality parity between the micro-services sources and the Data Lake In our experience, these pain-points have led to several instances where data was not leveraged in a timely fashion for business insights. Also, there are instances where changes are discovered downstream after-the-fact, and require to be driven as fire-fighting incidents. The tribal knowledge approach of discovering data impacts the confidence of Data Analysts and Data Scientists on the data availability in the Lake. The onus of coordination was traditionally done by Data Stewards i.e., data experts within the SaaS teams. Given the pace of change across micro-services, this model is not sustainable. An alternative approach is to apply analytics on the business events being shared between micro-services. Data-as-events has the benefit of eliminating an extra coordination between micro-service teams and downstream data analytics, but requires significant re-thinking of 1000s of existing analytics and ML jobs fueling business processes. Ideally, data sources and their changes should be automatically discovered, applied data compliance policies, and made available within the Data Lake. Frameworks such as AWS Glue can find changes within existing data sources, but lack the ability to find new sources as well as provide the compliance workflow before data is made available within the Lake. To address these gaps, we have implemented a tool called Virtual_Steward . As the name suggests, the goal is to automate the processes traditionally done by Data Stewards. In particular, Virtual_Steward implements four key capabilities: Automatically discover sources : Virtual_Steward continuously scans AWS accounts used by micro-service teams. The scanning discovers databases deployed within the account. Depending on whether the accounts are prod versus test, the discovered sources are added to the source inventory. These sources are compared with those available within the Data Catalog (we are using Alation ). If these are new sources, the appropriate workflow is initiated working with the micro-service teams to add the source within Alation. The credentials to connect to the source are stored within Alation, and becomes a one-time activity for the micro-services owners. Track sources for changes : The data sources are continuously tracked for schema changes. Multiple different approaches were explored to track these changes. Currently, the source schema are extracted from Alation, and diff’ed periodically to discover changes. Workflow for ingesting data source changes : When changes are discovered, we have multiple steps and approval required across data source owners, governance team, and the ingestion team. This is orchestrated via a workflow with progress tracking. Parity checks on the data : The quality of the data is continuously compared across the source and Data Lake. Moving forward, we continue to extend Virtual_Steward in our vision to develop a world-class data platform. In our overall scorecard of data platform KPIs , Virtual_Steward plays a key role in minimizing Time-to-Find, Time-to-Evolve, and Time-to-Compliance. This is a team work driven by Giriraj Bagdi, Khilawar Verma, Rajesh Saluja. Cool stuff we are doing and thinking about in engineering… 174 1 Big Data Data Science Data 174 claps 174 1 Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-18"},
{"website": "Intuit", "title": "demystifying complex data pipeline lineage with superglue", "author": ["Sandeep Uttamchandani"], "link": "https://quickbooks-engineering.intuit.com/demystifying-complex-data-pipeline-lineage-with-superglue-d5b4014b1482", "abstract": "Data pipelines in production are complex involving hundreds of tables, transformation jobs, and scripts. Before consuming business metrics, Analysts and Data Scientists often need to understand the ETL logic and the sources used to generate the given metric. Also, in production, debugging metric value anomalies today is a nightmare — pinpointing the root cause can take days, weeks or even months! Also, in writing new ETLs, it is critical to correctly specify the job dependencies in the scheduler. The complexities in Data Pipeline lineage directly impacts the productivity of Data Analyst and Data Scientists. The key productivity metric that we track are Time-to-Iterate representing the ability to understand, monitor, debug existing pipelines, and creating new ones. An ideal tool should be able to automatically extract lineage by parsing the data pipeline ETL scripts written in heterogeneous languages namely Python, SQL, Hive, etc. At Intuit, we have developed SuperGlue — a tool that seamlessly tracks lineage of complex production pipelines making it self-serve for Analysts, Data Scientists, Engineers, to interpret, debug, and iterate on data pipelines. Users start-off by logging into the SuperGlue portal and can search for any job, table, or QlikView report. As an output, SuperGlue provides a single pane holistic view combining the pipeline lineage with runtime execution stats including scheduler timings, data quality, change tracking in the scripts. Given the job/table/report name, SuperGlue provides the following views: Lineage View : Shows backward lineage of the specified table, job or report. Execution View : This shows the runtime details associated with Jobs and Tables. Users can highlight any element in the interactive lineage view and get the execution, data quality issues, and change tracking views. Under-the-covers, SuperGlue tracks data lineage by analyzing the jobs associated with the pipeline. Specifically, we define a pipeline to be composed of jobs; each job is composed of one or more scripts; each script consists of one or more SQL statements. A SQL query is analyzed for input and output tables. The lineage of a pipeline is defined as an array of triplets <Job Name, Input Tables, Output Tables> . This analysis is not a one-time activity. It is continuously evolving. Each script consists of one or more queries in different languages: SQL with some of Hive. These are then glued together with the output of one becomes the input for the next job. After extracting lineage, SuperGlue joins the dependencies with execution profiling. It integrates two categories of profiling: Operational Profiling : The focus is on Job health and Data Fabric health. Job health involves tracking execution related stats such as completion time, start-time, etc. Data Fabric health focuses on tracking events and stats from system components namely source databases, ingestion tools, scheduling frameworks, analytical engines, serving databases, publishing frameworks (such as Tableau, QlikView, SageMaker, etc.) Data Profiling : The focus in this bucket is on analyzing the data-related patterns. This is a fairly broad topic and a topic for a future blog. SuperGlue has been significantly helpful in improving productivity of our data platform. We have been in internal beta, and now releasing broadly within Intuit. This is a team initiative led by Sunil Goplani with the technical leadership from Anand Elluru and Shradha Ambekar. The team included Shrushti Patel, Shikha Singhal & Sooji Son. Cool stuff we are doing and thinking about in engineering… 60 2 Big Data Data Sc Machine Learning 60 claps 60 2 Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-04"},
{"website": "Intuit", "title": "managing data issues as incidents", "author": ["Sandeep Uttamchandani"], "link": "https://quickbooks-engineering.intuit.com/managing-data-issues-as-incidents-226f5f1c9e72", "abstract": "Consider the scenario where Users are unable to login into the Quickbooks SaaS application. The issue is managed as an incident with all the relevant teams paged for investigating and addressing the root-cause asap. Now, consider a data issue caused by broken ingestion to the Data Lake. Data issues today are not handled with the urgency and rigor of application incidents. Instead, data issues are handled in an ad-hoc fashion using to-and-fro Jira tickets leading hours and days to resolve. As Data becomes critical for both in-product features as well as data-driven business decisions, this has to change! At Intuit, we have transitioned in managing data-related issues as first-class citizens similar to application issues. This post describes the process, cultural mind shift, and learnings in managing 10s of data incidents over the last few months. Motivation: Why Data as a First-Class Citizen? The correctness of data pipelines for analytics & Machine Learning is critical for in-product customer experiences (such as ML-based forecasts, personalization, etc) as well as for data-driven business decisions (such as deciding product features, customer support models, marketing campaigns, etc). The correctness issues can arise from a wide range of possible root-causes: a) Issues in the source databases; b) Bugs in the ingestion frameworks impacting data availability or quality; c) Dirty data violating domain validation rules; d) Missed SLAs in completion of data transformation; d) Errors in underlying fabric components; and several others. Different phases of the data pipeline (namely collect, store, process, publish) are typically owned by different cross-functional teams complicating issue resolution. Typically, an issue is discovered either by Analysts and Data Scientists when using the data or by data engineers when they monitor anomalies in the job stats or data profiling. The issue is tracked as a Jira ticket with a deluge of comments added to the ticket during the triaging of the issue. In addition to taking longer to resolve, similar problem patterns keep re-surfacing due to a lack of rigorous Root Cause Analysis (RCA) and follow-up resolution. Issue resolution often gets divided into short- and long-term fixes, missing the appropriate prioritization rigor. Ideally, data issues should be handled with the same urgency and rigor as an application outage; they should be resolved in hours and with deep RCA (Root Cause Analysis) to ensure permanent resolution. The Process: Data Issues as Incidents We have established a process that treats data issues as a first-class citizen and at par with application availability outages. Data issues are handled as incidents using the existing incident management process as well as by the existing incident management team. Each data incident has multiple phases similar to popular Incident Management best practices (Figure 1): Identity, Analyze, Resolve, Prevent. A key phase to highlight is the Incident Closure or Prevent phase to ensure future occurrences of similar issues are prevented. Each incident has a priority rating (similar to Jira issues) that dictates the SLA time-window to resolve the issue. It also defines the level of executive visibility for the incident. For instance, a P1 incident (which is our highest level for incident prioritization) alerts all the key executives and requires the on-call engineers to be in the incident war room till the issue is resolved. In contrast, a P3 incident will have alerts going mainly to the engineers and group leaders with a longer time-window for teams to identify and resolve the incident. So, how should data incidents be prioritized especially using the same priority scale as application outages? In our experience, this has been the most challenging question to answer! Engineers working on these incidents need clarity to appropriately prioritize resolving these incidents especially when they are faced with an application outage and data incident concurrently. We have explored several different approaches such as creating a parallel incident track for Data or having all data incidents start with P3 priority or lower. None of these approaches addressed the need to create clarity as well as capture the ground reality. Our current prioritization continues to evolve and does a good balancing act between data stakeholders (who would want the highest priority for their issue) and on-call engineers (who have a wide range of incidents to resolve beyond the data incidents) : P1 Priority: Data issues that have the potential to directly impact customer experience . This bucket includes issues related to correctness such as billing, accuracy of ML model predictions, etc. Also, issues related to performance and availability of the data insights. P2 Priority: Data issues that indirectly impact customer experience . This bucket includes issues which will not disrupt the functioning of the product but are sub-optimal such as customization models, offline model re-training, etc. P3 Priority: Data issues that impact internal business metrics reporting . This bucket includes internal stakeholders that rely on data for business decisions such as product features, marketing, customer support, etc. To reiterate, issues in each of the priority buckets are immediately triaged — they vary with with SLA to resolve and the executive visibility. Cultural Mind Shift In addition to having a process, there is a cultural mind shift required for managing Data Issues as Incidents. The following table summarizes the key shifts: In summary, handling Data issues as incidents is both a process change as well as a cultural change. One of the biggest challenge has been making prioritization of data issues consistent across application outage incidents. In contrast to ad-hoc Jira-based approach for handling data issues, Data incidents have radically improved issue triage time, resolution time, and rigor of fixes w.r.t. preventing repeating similar patterns. This initiative is a team effort driven by Giriraj Bagdi, Sunil Goplani, Anil Madan, Rajesh Saluja, Sandeep Uttamchandani, and several other cross-functional team members. Cool stuff we are doing and thinking about in engineering… 129 Big Data 129 claps 129 Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-04"},
{"website": "Intuit", "title": "updates from intuit kubecon cloudnativecon north america 2018", "author": ["Mukulika Kapas"], "link": "https://quickbooks-engineering.intuit.com/updates-from-intuit-kubecon-cloudnativecon-north-america-2018-1c761c10e9eb", "abstract": "KubeCon + CloudNativeCon North America 2018 in Seattle turned out to be one of the most successful open source conference with 8000 developers attending. It showed exponential growth in adoption of Kubernetes and container technologies. 2018 was Intuit’s first major participation at Kubecon as a Silver end user sponsor. The number of companies joining CNCF grew exponentially and there is increased M&A activity in this space including Intuit’s acquisition of Applatix’s. Liz Rice, technology evangelist at Aqua Security, headed up the keynote proceedings that were dominated by updates on some of the now 32 CNCF projects . These included Prometheus, Containerd, Helm, and Envoy . Etcd joined the list as an incubating project. Serverless, Service Mesh, Security and DevOps dominated the session themes. Kelsey Hightower’s keynote connecting @HiddenFigures (best tech movie ever!) with the Path to running Fortran on serverless was one of the most tweeted sessions :) You can find all the sessions recordings here . 2018 KubeCon had big cloud and infrastructure vendors like AWS, Google Cloud, IBM, Red Hat, VMWare, and Nutanix announcing their latest Kubernetes services. But, the sessions and keynotes focused on Kubernetes early adopters like AirBnB, Alibaba, Capital One, Intuit, Lyft, Uber showcasing how large enterprises are not only adopting Kubernetes and associated open source software but also contributing back to the community. Intuit presented multiple sessions as listed here . Kafka on Kubernetes by Shrinand Javadekar was packed with over 500+ attendees! In the Intro: Open Policy Agent session, Todd Ekenstam talked about how Intuit is using OPA to enforce security and networking policies in Intuit’s Modern SaaS Platform. In the CI/CD in Light Speed with K8s and Argo CD session, Billy Yuen and Parin Shah talked about Intuit’s use of Kubernetes and the CI/CD processes. Intuit also presented at its booth how we are running Intuit’s Modern SaaS infrastructure on Kubernetes in public cloud at scale and had a great time collaborating with other attendees and community contributors. But the best part of the conference was the huge adoption and mention of Intuit’s open source project Argo in multiple sessions by BlackRock, Google, IBM, and Red Hat. Argo workflows has been included in the Linux Foundation Deep Learning Landscape ! It was demoed by AWS in their booth as a part of EKS workshop . BlackRock described how they are automating Data Science using Argo workflows and Argo Events . Full presentation is here . Google showed how Kubeflow pipelines is powered by Argo for ML. Argo CD is only a few months old, but has already been adopted by a number of major companies to enable GitOps and Continuous Delivery! Intuit is deploying hundreds of apps following GitOps with Argo CD. Billy Yuen and Parin Shah of Intuit presented how we have implemented “CI/CD at Lightspeed with k8s and Argo CD” . Google presented how Argo CD can be used to deploy ML components. And finally here is the closing recap on KubeCon 2018 with some of the best moments as tweets :) Already looking forward to KubeCon 2019! Happy Holidays Everyone and don’t forget Barcelona CFP for Kubecon 2019 is already open ! Originally published at blog.argoproj.io on December 17, 2018. Cool stuff we are doing and thinking about in engineering… 52 Kubernetes Kubecon Continuous Delivery Machine Learning Docker 52 claps 52 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-19"},
{"website": "Intuit", "title": "https medium com shivangshah quickbooks search platform our journey to aws", "author": ["Shivang Shah"], "link": "https://quickbooks-engineering.intuit.com/https-medium-com-shivangshah-quickbooks-search-platform-our-journey-to-aws-7e45fe5efd23", "abstract": "“The journey is what brings us happiness, not the destination” — Dan Millman, Way of the Peaceful Warrior In all honesty, reaching our destination to be fully in the cloud made us just as happy as the journey itself, probably more so 😃. So what is this journey I so fondly talk about? After a lot of experiments and “Follow Me Homes” with our customers, we launched QuickBooks Universal Search , with only one goal in mind — to improve our search functionality and power type-ahead search across the Product. This was by no means a small feat — We had to design a scalable and distributed search platform that would not crumble under peak loads of incoming requests at every character typed by the user (debounced of course!). Nor should it require any major upgrades in the near future because being deployed in our own datacenters had limitations on how fast we could scale our hardware. So, we had to prepare for the future. A future where we can support this platform, while we journey towards AWS. Now Pay Attention. This is where I dump all the gruesome, gory details about how complex our Application Architecture was and how intertwined our usage of technologies was just so we could scale. Sounds about right? “Simplicity is the soul of Efficiency” — Austin Freeman (in The Eye of Osiris) No, actually it does not. Architectures are supposed to be as simplistic as possible. Simplicity of our architecture is what allowed us to achieve maximum Developer Productivity and with that comes faster (and more frequent) releases. Below is the high level design of what our architecture looked like when we were in our own datacenters (from here on, we will refer to datacenters as IHP — Intuit Hosting Platform — consisting of 2 datacenters). At any point in time, Quickbooks would be active in one datacenter, thus publishing events on the message bus. The messaging cluster would forward the messages to 2 queues, one per each search cluster in each datacenter. Point to note — we had to do this because Elasticsearch did not provide Data Center Replication out-of-the-box. We basically used Eventing Architecture to feed both our Search Clusters in realtime, essentially being capable of Active-Active DR strategies. Pretty Standard Stuff. We chose Elasticsearch as our weapon of choice to power type-ahead search queries. This Lucene backed search engine is quite powerful and is really the de-facto industry standard for enterprise search. Plus it scales quite well if proper design patterns are implemented based on the nature of the application (Caution: I cannot stress enough — Do your due diligence upfront on how you plan to index, store & query your data because as your business grows, your original designs (to some extent) will determine how well you scale. If you don’t plan well, you will keep reindexing your data and rebalancing your cluster all day long!). Without going too deep into Elasticsearch design, I would just like to share a brief overview about how the data is organized in indices/shards. This is important to discuss because this is what allowed us to scale at Terabyte scale with sub-second latencies in our queries. www.elastic.co Our Elasticsearch Cluster has 100 indices with the naming conventions of “search-XX” where XX is just a representation of our bucketing strategy. For example, company with realmId 12343564 will belong to index “search-35”. Because our queries are always constrained within a company, there will never be a cross-index query. This already gave us a huge boost in our performance. Each index has 15 primary shards with 1 replication. To further tune and boost our performance we used Elasticsearch Routing to index all documents specific to a company in a specific shard within an index, using realmId as the routing value. With the understanding that our queries are strictly bounded within the context of a company, this bolstered the idea of each query specific to a company will always route the same shard, further enhancing our performance “Your most unhappy customers are your greatest source of learning” — Bill Gates What’s the take-away from this? — Know Your Customers, their needs and their business! This is where “Follow Me Homes” with our customers really paid off. We spent some serious time learning how our customers search their data, what specific data do they really care for and what kind of search capabilities do they expect on their fingertips with a Modern SaaS based product such as QuickBooks. Technologies are flexible enough to tune at will. It all boils down to how you tune them to your customers’ delight! Talking about Scale — With this simplistic architecture in place, we currently ingest ~50 million business transactions and handle ~2 million queries per day. Our Application topology is quite simplistic too. We have 10 stateless application servers fronted by a Load Balancer (round-robin routing), which eventually is fronted by Gateway . The application servers connect to Elasticsearch Cluster for feeding-in and querying-out the data. When we evaluated moving to the cloud, a lot of thought was put into how we should arrive on a decision. Here are the guiding principles that we agreed on to determine our choice. Migration Simplicity Security We rather not manage the cluster if it can be done for us (Upgrades, Scaling etc.) Maximize Developer Productivity Blazing fast release cycles AWS Elasticsearch Service came out on top — That whole “we will securely manage it for you” vibe was a perfect sell. We as a team would rather concentrate on solving customer problems. Not to mention, all of the other solutions that AWS provides to manage our application lifecycles, was just a cherry-on-top. The whole AWS ecosystem is quite a well oiled machine and there was no reason for us to shy away from it, but rather just embrace it all. That being said, we did have to make a trade off on one of our guiding principles — Migration Simplicity. In the 6 months we have been supporting our usecases from those clusters, there were major upgrades to Elasticsearch and as you can imagine, with major version upgrades (6.x.x) came even major breaking changes. Some of these breaking changes were important enough to address. To be fair, we knew this was coming for some time now. So, even though 5.x.x supported multiple mapping types, and we did use it heavily, the actual mappings were kept flexible enough for us to easily port to the new world (6.x.x). But change in mappings means fully re-indexing your data. We have close to 10 TB of data with about 8 billion documents. Re-indexing wasn’t going to be easy. It needed to be thought through. Another major huddle for us to overcome was moving away from the Elasticsearch Transport Client and embrace the Elasticsearch Java REST Client . This was partly because with AWS Elasticsearch Service, we cannot use the transport client as AWS only supports HTTP on port 80, but does not support TCP Transport (which is what Transport Client requires). The team have been going back and forth on how we can support Search in IHP and Search in AWS while having the same codebase. On one hand we didn’t want to upgrade our IHP infrastructure to 6.x considering it will be a waste of time. On the other hand, we didn’t want to move to AWS with 5.x which was already an older version and we will have to upgrade anyways. Whichever route we choose, there was code change regardless. We needed to choose the path of least resistance. I remember having a distinct phone call with our team lead one fine evening while crawling through peak traffic of Bay Area. After discussing it for way too long, he said (and I am paraphrasing) — “Let’s just do it, man. Let’s keep it simple ” The essence of the discussion was quite simple, yet beautiful. We will deprecate what we have in IHP, add no new features. At the same time, we will port over all the code to a new repository, experiment with some new CICD Frameworks and accelerate our journey to Modern SaaS (more in another blog). We even simplified our data migration strategies. After discussing a lot with AWS Solution Architects and working through the problem, we realized that we didn’t want to invest in writing one off tools (AWS Lambdas, Kinesis Firehose). We improvised yet again and kept things simple . We decided to use the tools that we already had at our disposal, without writing any one-off tools. 2 things are for certain — 1) We needed new indices in AWS anyways because with 6.2 we cannot use mapping types. 2) Because these will be new indices with new mappings, we will have to re-index the data. So here’s how we planned our data migration We will deploy a fresh new Elasticsearch Cluster in AWS with 6.2. We will also deploy application code compatible with Elasticsearch 6.2. We will open our flood gates of active incoming messages and start indexing new data in new indices (search-new-XX). In the meantime, we will get the latest data snapshot backed-up from IHP to S3 with all indices (search-XX). Thanks to the ability of restoring snapshot from S3 (and 5.3 snapshot restore compatible with 6.2 cluster), all the old indices (search-XX) are now restored in the new AWS Elasticsearch Service Cluster. The best part? the old and new indices now co-exist in the same cluster! With this in place, now we just use the Reindex API to reindex all the documents from old indices (search-XX) to new indices (search-new-XX). We also use entity version type as external so that newer versions of documents (within the time delta of us reindexing and new data coming in through messaging) are not overwritten. And just like that, Universal Search in AWS was open for business. And now it was time for experimentation & testing. We excelled at all the possible OpMechs in our new DevOps World. We have all-round monitoring, alerting, CI/CD and Testing Automations (One more Blog for another time). Our checkin → production window has reduced to under 1 hour, with all the right guard rails. I’d continue on how we marveled at our own magnificence , but I guess it’s time to move on 😏. We aren’t done yet. Being deployed in AWS has opened up a lot more options for us in terms of scalability and we would like to seize this opportunity to experiment more and fail fast if need be. We will continue to monitor Voice of Customers and User Voice channels for any feedback on how to improve the quality of universal search Just like all other products, Universal Search had it’s ebb and flow. But the team’s Customer Obsessed culture, backed by intense desire to learn, simplify and innovate, is what really made it successful. I am extremely proud of all the team members who poured their hearts out getting this feature into Customers’ hands. It was one of my most memorable journeys of delivering a widely used feature with such an Awesome Team. Kudos & Thank you! We have achieved a huge milestone, but this is just the beginning. Stay tuned for more ! The QBO Search team in the Small Business unit was extremely instrumental for the success of this journey. Special thanks to Akbar Rangara who reviewed and added content to the document. Cool stuff we are doing and thinking about in engineering… 75 Elasticsearch AWS Intuit Quickbooks Accounting 75 claps 75 Written by Architect, QuickBooks Platform Cool stuff we are doing and thinking about in engineering at Intuit Written by Architect, QuickBooks Platform Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-22"},
{"website": "Intuit", "title": "experimentation intuit part 3 execution engine", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/experimentation-intuit-part-3-execution-engine-a66efd3130e1", "abstract": "In part 2 we covered the design and setup of experiments to support thousands of concurrent experiments. In Part 3 we will look into the execution engine that serves experiments. Let’s look at the logical architecture — steps 1 to 3 are design time, while steps 4–10 are runtime, when the user interacts with the product. The experimenter goes into the tool and sets up an Experiment, defines treatments and optionally factors (for factor based experiments) — this is what we covered in part 2. The configuration is stored in the Experiment Config database. It is pushed out to an Experimentation Execution Service using messaging. Later customers login the product and access the functionality by being redirected to the appropriate QuickBooks Application Servers. The Application server has the experimentation client SDK integrated which in turn knows how to call the Experimentation Execution Service. Instrumentation is written to the logs. Event data flows both in our big data store for offline processing and Real-time systems for online processing. The basic experimentation, treatment and factors based measures are auto-generated. Experimentation results are delivered in the tool to the experimenter. At run time Customer logins into QuickBooks. Our AppServers identify the user. Experimentation service runtime determines what treatment should be served based on user id and experiments that are active in the system. Applications serve the correct experience to the user based on the treatment selected.The experience is logged for analysis. Each application requires a thin service client — mobile and web — to call the experimentation execution service to determine the qualified experiments and treatments. The Experimentation Execution Service provides a REST endpoint to determine the qualified experiments, treatments and their factors. It is capable of executing experiments remotely and in querying the profile store . A simple experiment that does not involve segmentation can be executed locally at the application layer (as the experiments are eventually cached in the Appserver cache) . However, experiments that need to query attributes for segmentation do require a remote call There is rich instrumentation that feeds both producing self-service analytics and troubleshooting. The platform logs which qualified experiments (Qe), treatments (Qt) and pages (Qp) the user is expected to experience. Additionally the application logs what was truly experienced — experiments (Xe) and treatments (Xt). The reason for two sets of tags is there could be valid reasons for the user to not see the experience due to opt-outs. The other reason being to quickly troubleshoot application vs platform issues. In part 3 we covered the execution engine that supports serving hundreds of concurrent experiments, across thousands of infrastructure servers to millions of users. In Part 4 we will look into the analytics that we are working on to enable self-service insights. Cool stuff we are doing and thinking about in engineering… 29 Big Data Experimentation A B Testing 29 claps 29 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-17"},
{"website": "Intuit", "title": "experimentation intuit part 1 culture", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/experimentation-intuit-part-1-culture-51889a3fb30b", "abstract": "This is a 4 part series on experimentation at Intuit. Part 1 covers the culture of experimentation @Intuit while Parts 2 to 4 cover how we have built a powerful and scalable experimentation platform to power data driven decisioning. Our mission at Intuit is to Power Prosperity Around the World for small businesses, the self-employed and consumers. To achieve this we use design thinking to deeply understand our customers and solve their problems through Design 4 Delight (D4D). D4D is ultimately about evoking positive emotion throughout the customer journey by going beyond customer expectations in delivering awesome product experiences. There are 3 components to D4D Deep Customer Empathy We foster internally a culture of understanding the customer by conducting “Follow Me Homes” to observe customers using our products in different environments, such as the home, office, or wherever they do business. Asking customers questions as they go throughout their day while they are in their physical and mental work space allows us to get profound insight into their thoughts, feelings and habits. This ensures that when we design solutions, we are honing in on the most pressing problems for our customers — even problems they cannot articulate for themselves. 2. Go Broad to Narrow The “Go Broad to Go Narrow” stage is where creativity happens. This is when the scrum teams quickly start coming up with many different ideas. We do that through sticky notes everywhere covered with brilliant ideas. Brainstorming and idea generation is encouraged to be as crazy and creative as possible — ‘you need to kiss a lot of frogs to find a prince’ . We choose what’s most delightful, not what’s easiest… or cheapest. Often a crazy idea spurs a new line of thought. For example, an idea of giving away the product for free may sound crazy. — but may lead to new ideas and business models, such as giving users a 30 day free trial to experience the product decide if they want to subscribe to QuickBooks. 3. Rapid Experiments with Customers Once there is a sufficient number of ideas, they are prioritized to be tested through rapid experimentation with customers. To make the idea more precise, the idea is fleshed out into a prototype or at least a storyboard. A storyboard can be drawn in a few minutes on paper for instant feedback. This allows the team to rapidly complete several iterations with customers. There is an evolution of Intuit’s culture that started with qualitative ‘Follow me Home’ and has expanded to bring in a quantitative mindset; tying the best of customer empathy and data together. At the heart of this transformation is a powerful in-house experimentation platform that is helping us make faster data-driven decisions. Similar to our industry peers like Google , Amazon , Microsoft , Facebook , LinkedIn , Netflix , Booking.com , Uber and AirBnb we are starting to run thousands of online controlled experiments to guide data driven decisioning. Recently through internal open source, we revamped the entire platform to build powerful new capabilities. In Part 2 , we will deep dive into the capabilities. Cool stuff we are doing and thinking about in engineering… 12 Design Thinking Experimentation A B Testing Intuit Quickbooks 12 claps 12 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-03"},
{"website": "Intuit", "title": "experimentation intuit part 2 design of experiments", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/experimentation-intuit-part-2-design-of-experiments-65457c7dde99", "abstract": "In Part 1 we looked at how Intuit’s culture of design thinking has evolved to embrace rapid online experimentation. In Part 2 we will look into Design of Experiments. Any experiment is a continuous process of design, execute and analysis. Let’s take a closer look at each. There are some basic concepts in experimentation Controlled Experiments — A controlled experiment is an experiment done under controlled conditions. E.g. a prospective user is shown an alternative experience to increase sign-ups. Factor (or variable) — A variable that can changed independently (cause) to create a dependent response (effect) . Factors generally have assigned values, sometimes called levels . E.g. you can change the background color of a web page or a label of a button to measure click through rates. Background color and label of a button in this example are factors while factor values are ‘Free Trial’ or ‘Buy Now’. Treatment (or variant) — The control, is usually the current system and considered the “champion” while the treatment, is a modification that attempts to improve something — the “challenger”. A treatment is characterized by changing the level(s) in one or more factors. Experimental Unit — The experimental unit is the physical entity which can be assigned, at random, to a treatment. Commonly it is a visitor or a signed in user. It is an entity on whom the experimentation or analysis is done e.g. visitor, user, customer, etc. In the digital world (web, mobile, etc.) the user is the most common experimentation unit, although some experiments may be done on prospects or page views. E.g. the following two are treatments on the QuickBooks home page. Sample — A group of users who are served the same treatment. Overall Evaluation Metric or Criteria — refers to what measure , objective or goal we are trying to achieve. It is a metric used to compare the response to different treatments. Single-factor testing –A form of testing in which treatments corresponding to values of a single-factor e.g. is there a free trial button on the page ? Yes or No? Multi-factorial testing (MVT) –A method of testing in which treatments corresponding to multiple-values of multiple-factors are compared — e.g. is there a free trial button on the page ? Yes or No? And where is the location of the button ? Top or Right Rail? With the basic concepts defined , we conduct experiments at several distinct stages in the product lifecycle with the goal to continuously learn and iterate. The experimentation funnel starts with simulations in our big data systems. This generally helps us validate a few hypotheses quickly and more importantly discard the ones that don’t make sense. Once a minimum viable product is developed we launch an Alpha. Alphas are playgrounds launched internally to our employees to get feedback with a goal to gather qualitative data. Post the alpha phase we launch Betas that are opt-ins (in our QuickBooks labs) again with the goal to gather qualitative data. Finally A/B or Multivariate Tests are online controlled experiments that we conduct to get quantitative data and tests for statistical significance. These are full blown experiments. Our experimentation tool has a simple self service interface to create an experiment by specifying the name , duration — start date and end date — and region and setup treatments by specifying the allocations (or what % of traffic would be directed to it). The treatment screen captures the name , optionally the factors and the allocation range — allocation range is used to determine if the user experiences the treatment. Technically if userId modulous 100 falls in this range than the user experiences this treatment. A simple experiment on Sign-Up allocates 5% each on control and treatment. A user with userid modulus 100 = 25 maps to the control group and will be shown the default Sign-Up experience. Another experiment on Home Page allocates 10% each to control and treatment. Another user with userid mod 100 = 45 maps to the Home Page treatment. In the diagram above Sign-Up and Home Page are setup as two mutually exclusive experiments — i.e. at any given time a user is in one of the two experiments. While the exclusive segments is a good way to separate traffic and avoid collisions it does not scale to the growing need of experimentation within Intuit . Since at any given time we need to run thousands of experiments we need a way to create overlapping segments. As such we need to think of experiments that can run orthogonal to each other with uniform random collisions. In the figure above you have 5 groups of experiments that show the power of exclusive and orthogonal spectrums. Group A — Home Page and Profile are setup orthogonal to each other. Group B — Login, Sign-Up and Settings are set are setup orthogonal to each other. Group C — is reserved to run A/A test to validate the platform itself. We monitor the statistical framework used for decision making by maintaining a pool of AAs —( experiments whose treatment does not introduce any sort of change) which allow to validate its theoretical properties (e.g. false positive rates or the statistical distribution of metrics). Group D — is exclusively reserved to run an experiment on Reports and Insights. Group E — is used to ramp-up the winning treatment of an experiment that was originally proven to be successful. The groups A,B,C and D use an exclusive spectrum - the experiments across them don’t statistically collide with each other as that can pollute the results. Inside each group there may be overlapping orthogonal experiments that collide uniformly across control and treatments. The Hashing Constant (hashId) (in Fig. 5) serves as an input to an MD5 based algorithm used to uniquely define the orthogonal plane. As we want to run thousands of concurrent experiments, different hashIDs imply that the randomizations between active experiments are orthogonal — Google , Microsoft Bing and LinkedIn use similar approaches. We also allow to further segment users to target different sub-populations. Built in Attributes — The platform has access to more than 300+ built-in customer attributes for experimenters. They range from static attributes such as user subscription status to dynamic attributes such as a member’s last login date. These attributes are either computed daily as part of our data pipelines or stored real time in our profile infrastructure. Contextual Attributes — These attributes are only available at runtime, such as the browser type , user country or mobile device . For example, to target only requests coming from iPhone9, one just needs to inform the platform that an attribute called “deviceType” is to be evaluated at runtime, and target only those with the value equal to “iPhone9”. In part 2 hopefully you got a good sense of how we design and setup experiments and how the platform is scaled to support thousands of concurrent experiments. In Part 3 we will look into the execution engine that serves experiments. Cool stuff we are doing and thinking about in engineering… 35 Startup Experimentation A B Testing Intuit Quickbooks Big Data 35 claps 35 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-17"},
{"website": "Intuit", "title": "experimentation intuit part 4 analysis of experiments", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/experimentation-intuit-part-4-analysis-of-experiments-1a17a3762107", "abstract": "In Part 3 we covered the execution engine that supports serving thousands of concurrent experiments to millions of users. In Part 4 we will look into the basics of experimentation analytics. If you recall from Part 2, OEC refers to what measure, objective or goal we trying to achieve. It is a metric used to compare the response to different treatments. For QuickBooks, the primary business metrics are: Acquisition — conversion of a prospect to a subscriber. Engagement — how often is the customer using the product and what is the time spent in each visit. Retention — total number of subscribers and lifetime value of each customer. As some of these metrics take a while to bake and reach steady state we frequently also use secondary metrics to get an early-read to confirm or deny that an experiment is trending well. These are based on leading indicators like # of sign-ups, # of logins, # of invoices sent. Besides the business metrics, we also look at operational metrics such as availability, page performance, experiential or experimental bugs and call volume from our customers to ensure we don’t have system issues. Sample size is a critical piece where the duration — start and end date in the tool — are determined based on how much traffic we need to drive statistical significance. We look at historical traffic on the page or workflow and then extrapolate the duration based on the statistical significance that we want to drive. For example assume that 5% of QBO users buy payroll. Let’s assume that the lifetime value of a QuickBooks customer is $100. The average user therefore has a lifetime value of $5. Assume the standard deviation is $40. If we want to detect a 5% change to revenue , we will need over 1.6M users. If you get approximately 200K unique users a day , that means the experiment needs to run for 8 days. 𝑛 = (4r𝜎/Δ)² = (4 * 2* 40 / (0.05 * 5))² = 1.6M (see addendum if you want to understand the statistics) Sample size calculator here with supporting b log post We conduct an A/B experiment when iterating on a product feature. We use MVF (partial as opposed to full) testing when several factors are suspected to interact strongly. We remove certain combinations of factors through simulations — remember each combination adds to the total sample size. I would close the 4 part series with an excerpt from Jeff Bezos’s letter to the shareholders — “One area where I think we are especially distinctive is failure. I believe we are the best place in the world to fail (we have plenty of practice!), and failure and invention are inseparable twins. To invent you have to experiment, and if you know in advance that it’s going to work, it’s not an experiment. Most large organizations embrace the idea of invention, but are not willing to suffer the string of failed experiments necessary to get there.” Intuit’s qualitative deep roots of ‘Follow me Home” have expanded with a quantitative mindset as well — tying the best of customer empathy and data together. At the heart of this evolution is the Experimentation Platform that is driving a cultural change to Power Prosperity Around the World. — — — — — — — — — — — — — — — — — — — — — — — — — — — — — A quick refresher on some of the fundamental concepts in statistics to inform how we apply statistical theory to evaluating our experiments. Normal distribution , sometimes called the bell curve, is a common way to describe a continuous distribution in probability theory and statistics. Lots of natural phenomena in the real world approximate to normal distribution, near enough that we can make use of it as a model, phenomena that emerge from a large number of uncorrelated, random events will usually approximate a normal distribution. Some examples that follow normal distribution would be height and weight of humans. Some properties of Normal distribution are The mean, mode and median are all equal. The curve is bilaterally symmetric at the center (i.e. around the mean, μ). The tails are asymptotic to the x axis, meaning they come closer and closer but never actually touch it. Exactly half of the values are to the left of center and exactly half the values are to the right. The total area under the curve is 1. μ refers to the mean of the population while standard deviation represented by σ controls the spread of the distribution. A smaller standard deviation indicates that the data is tightly clustered around the mean; the normal distribution will be taller. A larger standard deviation indicates that the data is spread out around the mean; the normal distribution will be flatter and wider. Empirically for normal distribution virtually all of the scores fall within three standard deviations from the mean. Any value is likely to be within 1 standard deviation of the mean , very likely to be within 2 standard deviations and almost certainly within 3 standard deviations. 68% of the data falls within one standard-deviation of the mean. 95% of the data falls within two one standard-deviations of the mean. 99.7% of the data falls within three standard-deviations of the mean. While a family of bell-shaped curves can be defined for the same combination of μ, s, only one is the normal curve. The standard normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. The standard normal (Z) distribution serves as a standard by which all other normal distributions are measured. When a frequency distribution is normally distributed, we can find out the probability of a score occurring by standardizing the scores, known as standard scores (or z scores). The z-scores or “standard normal deviates, presents data in a standard form that can be easily compared to other distributions z = (X — μ) / σ where z is the z-score, X is the value of the element, μ is the population mean, and σ is the standard deviation. A z-score less than 0 represents an element less than the meanwhile a z-score equal to 1 represents an element that is 1 standard deviation greater than the mean; a z-score equal to 2, 2 standard deviations greater than the mean; etc. The area under the curve is directly proportional to the relative frequency of observations . All normal distributions can be converted into the standard normal curve by subtracting the mean and dividing by the standard deviation. Z-scores often summarized in table form as a CDF (cumulative density function). Let’s see this with an example. Assume a random normal variable follows a normal distribution with a mean of 3.00 and a standard deviation of 1.0. What if we have to find the probability that this random variable is greater than 5.0. 1) The first step is to standardize the given value of 5.0 into a Z value (aka, Z score): Z = (5.0–3.0)/1.0 = 2.00. The Z value of 2.00 means “The value of 5.0 is 2.00 standard deviations above the mean of 3.00.” 2) Then we can use the common Z table to retrieve the associated probability. We go to row 2.0 and then go to column 0.00 to arrive at 0.97725. We see here that for Z = 2.00, the probability is 0.97725 or 97.73%. Pr[X ≤ 5.0 | µ(X) = 3.00 and σ(X) = 1.0] = Pr(Z ≤ 2.000) = 97.73%. You can also use an online calculator to compute this. A hypothesis is an educated guess about something in the world around you. It should be testable, either by experimentation or observation. In hypothesis testing before you can even perform a test, you have to know what your null hypothesis is, and what you are testing is an alternate hypothesis . For instance, in the above example where we want to test the idea of improved value proposition for the customer by having all products in one place, here’s how the null hypothesis (Ho) and alternative hypothesis (Ha) are formulated Ho: Having all products in one place has no effect on customer’s perceived value of QuickBooks Ha: Having all products in one place improves the perceived value of QuickBooks ecosystem thereby increasing our subscription rateFor example, we believe that by having All the Products in one place we can showcase the power of QuickBooks ecosystem to small businesses to manage their business. Let’s look at all the combinations between the experimenter’s hypothesis and the actual sitation Type I and Type II errors are better explained by an example of the fable of the boy and the wolf. When the boy first pretends there is a wolf and the villagers believe him while the wolf did not come, it’s called a Type I error. When he claims there is a wolf again, but no one takes him seriously, though it is true, it’s a Type II error. The villagers can avoid Type I errors by never believing the boy, but that will always cause a Type II error, when there is a wolf around. Similarly, they can always believe him and never make a Type II, but that will cause lots of false alarms. Null hypothesis (H0): there is no wolf Alternate hypothesis (Ha): there is a wolf Type I error (α): we incorrectly reject the null hypothesis, that there isn’t a wolf (i.e., we believe there is a wolf), even though the null hypothesis is true (there is no wolf). Type II error (β): we incorrectly accept (or “fail to reject”) the null hypothesis (there is no wolf) even though the alternative hypothesis is true (there is a wolf). False Positives — A Type I error occurs when we reject the null hypothesis while it is true. The probability of committing a Type I error is denoted by a Greek letter α (alpha). The probability of committing a Type I error is also w.r.t to a significance level α. The reciprocal of Type I error is called the Confidence Level , defined as 1 − α. The level of acceptability for Type I error is conventionally set at 0.05. Setting α at 0.05 means that we accept a 5% probability of Type I error. To put it another way, we understand when setting the α level at 0.05 that in our study we have a 5% chance of rejecting the null hypothesis when we should fail to reject it. False Negatives — A Type II error occurs when we fail to reject the null hypothesis while it should be rejected.The probability of committing a Type II error is denoted by Greek letter β (Beta).The probability of not committing a Type II error (1-β) is called the power of the experiment. The Power of a test is one of the most important factors in hypothesis experimentation. The power essentially tells you the chance of rejecting the null hypothesis when it should be rejected. Conventional levels of acceptability for Type II error are β = 0.1 or β = 0.2. If β = 0.1, that means the study has a 10% probability of a Type II error; that is, there is a 10% chance that the null hypothesis will be false but will fail to be rejected in the study. To put it another way, it means that in a study that should return significant results based on the true state of the population, there is a 10% chance that the results of the study will not be significant. The reciprocal of Type II error is power , defined as 1 − β. Assuming we have the two treatments we can perform a sample size calculation with the intent of comparing means μ1 and μ2 from 2 groups of user population groups that will experience the two treatments — we assume normal distributions and homogeneous variances σ1 = σ2 and equal sample sizes n 1 = n 2 . The false positive rate is set to α and the power for detecting a difference Δ=|μ1 –μ2 | is set to1–β, where β is the false negative rate. Using conventional values of α = 0.05 and β = 0.20, the sample size calculation then can be computed as or 𝑛 = (4r𝜎/Δ)² where r is the no of treatments. For α = 0.05, the numerators corresponding to 1 — β = 0.90 and 0.95, respectively, are 21 and 26 (deep dive here ). Few salient points that ties sample size, power and effect. If the evaluation metric has less variance, we would need a smaller sample size to conduct an experiment. 2. If the desired effect aka minimum detectable effect is higher, we need a smaller sample. Likewise, a smaller effect needs much bigger sample to conclusively attribute such effect to the test. 3. Increasing the desired power of the test requires larger sample as it offers less flexibility with making type II errors. 4. Increasing confidence level (in other words smaller α) requires larger sample as it offers less flexibility with making type I errors 5. An experiment with 99%/1% treatment-control split will have to run about significantly longer than if it ran at 50%/50% split. Cool stuff we are doing and thinking about in engineering… 11 Analytics Data Science Experimentation A B Testing Intuit Quickbooks 11 claps 11 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-29"},
{"website": "Intuit", "title": "the eagle has landed", "author": ["Tuan Le"], "link": "https://quickbooks-engineering.intuit.com/the-eagle-has-landed-b7e073dfdbfd", "abstract": "“We choose to go to the moon. We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win, and the others, too.” - John F. Kennedy On Thursday, December 17th, 2015, Luu Tran, then-SBG Chief Platform Architect, sent out an email to a broad audience declaring the start of the mission to take SBG hosted assets in IHP to AWS. Codenamed “Voyager” after the NASA Voyager interstellar mission, the name was meant to convey humanity’s bold spirit in leaving home and venturing out into the vast unknown reaches of space. Luu’s email contained the above quotation from John F. Kennedy’s famous speech in 1962. The passage served to inspire and galvanize the mission team to reach for the top of the tree and choose to do the hard things first. Fast forward 3 years, and on Monday, September 3rd, 2018 at 8 AM PT, we migrated our last QBO production cluster to the us-west-2 region of AWS. The QBO Accounting Voyager mission was accomplished! The Eagle has landed. We survived 3 QBO peak seasons, 2 QB Connect conferences, numerous network outages (both in IHP and AWS), an SSL cert expiration (facepalm.jpg), various infrastructure failures, and also fixed many code bugs that had built-in assumptions of being executed only in IHP. We exposed capacity flaws in our dependencies, uncovered faults in our own architecture, and worked to rectify both. We added and drove the need for additional capabilities — Consul, Olympus, Spinnaker, Fast Rollback, multi-master replication, huge RDS sizes, transparent application encryption, and SWF to name a few. In doing so, we made our application more {secure, performant, available, scalable, resilient} in our journey to the cloud. Through multiple obstacles and challenges, the team persevered to the end. I joked early on that we should have named the project “Odyssey” for the 10-year trek Odysseus endured to get home from the Trojan War. Little did I know there would be a ring of truth to that. :) As I sit down to pen this, I couldn’t be more proud of the team and what we did. We have all given a significant portion of our time and energy for 3 years to this effort, and it’s immensely satisfying to see the conclusion. We learned and re-affirmed many lessons on this large, sprawling, and complicated project to break free of IHP. I will share some of the key ones below. Having Tayloe Stansbury and Laura Fennell as sponsors were critical to get the project started and keep the momentum going when we hit the inevitable roadblocks. The monthly meetings with the senior executives enabled us to share progress of the overall Voyager program and ask for help as needed. Contrary to popular belief, meeting executives regularly for status updates isn’t as painful as it sounds, and we get a peek into how they think from both a business and a technology perspective. This one may seem obvious, but a 100% full-time team drawn across the necessary functions is needed. We had committed resources from SBG Operations, SBG Data, and PD all working towards a common goal. Here are some pictures from the C34 1-hr tiptoe of the extended team spanning two continents. This was the first time we took a production cluster with real customers into AWS. I must admit, it was both exciting and a little scary at the same time In a long project such as this one, many decisions are made. It’s important to document all of them with the reasoning behind such decisions. Invariably, some decisions are revisited in light of new information, and the proper documentation helps to reset the context for each decision and get the team up to speed quickly to make the right call. One cue we took from Amazon was the concept of two-way doors. Here’s an excerpt from Jeff Bezos’ 2015 letter to Amazon shareholders covering the topic: “Some decisions are consequential and irreversible or nearly irreversible — one-way doors — and these decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don’t like what you see on the other side, you can’t get back to where you were before. We can call these Type 1 decisions. But most decisions aren’t like that — they are changeable, reversible — they’re two-way doors. If you’ve made a suboptimal Type 2 decision, you don’t have to live with the consequences for that long. You can reopen the door and go back through. Type 2 decisions can and should be made quickly by high judgment individuals or small groups. As organizations get larger, there seems to be a tendency to use the heavy-weight Type 1 decision-making process on most decisions, including many Type 2 decisions. The end result of this is slowness, unthoughtful risk aversion, failure to experiment sufficiently, and consequently diminished invention. We’ll have to figure out how to fight that tendency.” We learned this lesson the hard way. Early on, the Voyager team was in deep discussion with the central Security team on many issues regarding architecture, deployment, and infrastructure. On each topic, both sides had reasonable arguments with their own merits, but we just couldn’t see eye-to-eye. Three months were lost. Eventually, Tayloe told us to get in a room with AWS and JFDI (I’m paraphrasing :) ). Immediately after that, several folks from SBG, PI, and Security flew to Virginia in May 2016 to meet with AWS Security. They hammered out the details over a weekend. Out of that was born the QBO Accounting Voyager clean sheet implementation. Highlights included: One account and one VPC for the QBO Accounting application in AWS RDS Oracle in AWS with GoldenGate replication over VPN back to IHP Olympus for access control to AWS accounts If you’ve ever learned how to drive a stick shift with a demanding parent in the passenger seat, you’ll relate to this! While practicing shifting, I stalled the car so many times driving around the parking lot — with my dad glaring at me — that I was cringing in nervous anticipation before every restart. Just like those first few stops-and-starts in the empty high school lot, our early attempts at switchover tests on the production test cluster C92 were done with a mixture of thrill and trepidation. Will the switchover scripts work? Can we log in after switching? What about mobile? It was a bit of a guessing game at first despite our best attempts at preparation. By the 4th switchover attempt on C92, the team had gotten the process and protocol down. Everyone knew their role and executed to perfection. I could see the team gain more confidence on every attempt as we got further and further each time. What had been hard and intimidating was replaced with competence and proficiency through repeated efforts . For tracking purposes and for the team to see and feel the measurable progress, we cataloged the find/fix ratio for each switchover attempt. After 9 official attempts (and more unofficial ones) we were down to 0 newly found bugs and 14 existing, non-critical issues. We were ready for the C34 1-hour tiptoe into uncharted AWS waters! Similarly, after stalling time and time again in that old Toyota Corolla, the motor patterns started to be ingrained, and shifting got easier. I drove the car home from the parking lot smoothly without stalling once! And my dad was no longer giving me the evil eye. So where are we heading next in the voyage? ‘Fork-lift over face-lift’ was one of our primary operating principles. Amazon calls it ‘lift-and-shift’. We wanted to minimize the amount of changes to the application in migrating it to the cloud. Of course, “minimize” is relative as it still took massive changes at all levels of infrastructure and application code. In Phase 1 of our journey, we broke free of the gravitational pull of IHP. The next destination in our journey is going beyond the local planets and venturing into interstellar space. What does that mean for QBO Accounting? It means unlocking developer productivity by faster provisioning. It means truly learning how to efficiently operate in AWS. Optimizing resource consumption. Containerization. Going global to be in the same solar system as our customers (finally!) Leveraging AWS services as Gravity Assists to propel us farther and faster than we have ever been. The QBO Accounting Voyager mission is done, but we won’t be resting on our laurels. Cool stuff we are doing and thinking about in engineering… 24 1 AWS Cloud Cloud Computing Quickbooks Online Intuit 24 claps 24 1 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-10"},
{"website": "Intuit", "title": "bower to npm yarn migration at scale", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/bower-to-npm-yarn-migration-at-scale-384fd8fc24f1", "abstract": "As the web progresses, old technologies need to be traded out for newer ones. It is the fun/unfortunate nature of the business we work in. In the last three years or so, the web has seen a large shift to faster/safer tooling. NPM 3+ made huge strides in dependency resolution from NPM 2/Bower, and Yarn changed how fast and secure dependency resolutions could be. It seems like these changes would be welcomed, and flocked to for most developers. Unfortunately, at scale, sometimes priorities come between you and the shiny new way of doing your work. It wasn’t until this past year that our team was able to fully abolish Bower from our workflows. Moving from one tool to another within the same ecosystem of development should be fairly straight forward. Yet, at scale, the unexpected happens. Moving from Bower to Yarn is not the hard part, it is the execution of the move that is difficult. To paint a picture, here are some things we had to consider: Were all dependencies properly published to NPM (this goes for both internal and external dependencies)? How do we handle scoped modules that have @scope/ or similar in front of the name when we weren’t before? If we have mappings for the browser, how do we ensure we don’t break those 💥? How do we continue to ensure the versions we care about are getting where they need to be (CDN etc) with proper naming? How can we avoid breaking developer productivity in the process We have a number of advantages (semantic versioning, git history, and other internal mechanisms) to help us avoid catastrophy. None the less, it is good to lay out how we intend to handle certain cases. We needed to be able to make changes without end users noticing. It’s pretty much a huge magic trick. Depending on your business, and the scale of your development cycle, you can approach the solution different ways. For us, there were some key flows we needed to always follow. Solve for one depdendency at a time. You never know when changing too much will cause unexpected issues. For each dependency, we ran through our checklist of problems above and made an attempt to address them. Pick either NPM or Yarn. The reason for this is because they have different resolution structures under the hood. Mixing the two can cause unexpected consequences. It is good to also enforce this via your .gitignore and through comms to your developers. Thoroughly test against a handful of downstream dependents (if possible) so you can try and catch any unexpecting issues. In most cases, you may see nothing. Vet that if the module only existed in bower before for a specific version , that the next closest version in NPM doesn’t introduce unexpected functionality (this can happen, maintainers are only human). In certain cases, we were left to put certain modules in an interim folder during the transition and publish to NPM. While it did increase resolution times, it did keep parity during the transition. Can’t say it was pretty, but it worked. Sometimes, that’s what has to be done 🤷‍♂. Once you have vetted the changes, you may need to run around to a bunch of repos and open PRs. Your PR should contain the NPM modules added to the package.json , and the deletion of the bower.json file (assuming nothing depends on that repo via bower!). Otherwise, you may need to send comms out to your consuming teams with instructions on migrating. We were lucky in that a large majority of our internal development had already moved to NPM. Just some core modules were still on Bower. There are a number of packages out there that claim to migrate from Bower to Yarn for you. These tend to work well on small projects, but may not apply at scale. There are too many variables in play in an enterprise environment that compound and break the assumptions of these tools. At the end of the day, you are best doing it by hand on your own. If you are reading this article, you probably already know the benefits. For us, we saw huge improvements throughout our development → deployment cycles, specifically in time spent resolving (usually saved upwards of 50% overall). This is mainly because we were avoiding double resolutions (if we had NPM and Bower coexisting). It also stems from the fact that some of our Bower dependencies were git repos. By packaging and publishing them, we save on download time since we aren’t cloning whole repos + git history. While it is understandable for larger companies to wait and see on most technologies, there are a select few when your team will be faced with a decision that requires quicker action. Bower → NPM is one of those decisions. In addition, the broader community aligns with these sorts of trends. If your company/org/team is unwilling to move, they will start to see friction as newer technologies mainly rely on bleeding edge technologies. We like to say we want to “stay with the grain of open source”. Though we aren’t perfect, keeping to this mantra has helped to alleviate a number of headaches throughout our local development cycles and CI recently. Bower → NPM was another box to check in the roadmap set by that mantra. Cool stuff we are doing and thinking about in engineering… 30 JavaScript NPM Tooling Continuous Integration 30 claps 30 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-20"},
{"website": "Intuit", "title": "react 16 upgrade for quickbooks the headache and zero production issues", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/react-16-upgrade-for-quickbooks-the-headache-and-zero-production-issues-e1b619a41b16", "abstract": "It’s true… we upgraded QuickBooks Online to React 16 last month. It was by no means an easy process, but we did it and without a single production issue. How were we able to pull this off? What steps did we take to move such a massive platform to the latest and greatest? Simple: we failed multiple times internally . As with any major upgrade, we hoped for the best, but expected the worst. We have hundreds of developers working on our product, and we can’t expect all of them to use the tools provided exactly as the docs specify. At some point, someone will inevitably do something hacky under the guise of “shipping a working feature.”. Major version upgrades tend to reveal these sorts of things. In our case, we were led to believe that if our codebase was up to snuff for React 15.6.x, then we could easily move forward as per Facebook’s messaging. Well, we were a little off the mark on that one. Approximately two and a half months off the mark to be exact. Since we have so many moving parts, we had to look into giving devs a way to test drive the changes to see what all broke. This meant: Setup a testing environment for developers that they could log into and test their changes Open a means of communication (Slack channel in our case) where engineers could notify us of breakages Create a spreadsheet and gather data on who is compliant and who isn’t While that last point was pretty painful, it allowed us to follow up with different teams within our org and ensure that everyone was compliant. These ideas may seem obvious, but in a world where many companies are pushing for shipping straight to production, it is important to note that not everyone in the game can do that. It may not be “cool” to vet like this, but it sure is better than shipping something that breaks your entire environment for your customers. Going through this sort of excercise is not something we enjoy doing. It was quite the opposite. There were many times we fell short between dependencies being on the wrong versions, developers using methods in ways that didn’t match the documentation, and general fragility in our environment based on prior assumptions that are no longer true. Through all the headache, there were some major takeaways: Ensure mechanisms exist for testing your downstream consumers at scale . In open source, you kind of have the luxury of making your code however you see fit, and then say “here you deal with it” and not caring how others consume it. In an enterprise environment you don’t get that luxury. If you make things for other developers, you have to take samples and test downstream dependencies so as to ensure they don’t break when you make major changes like this. Give a why for the move. Most developers have other priorities to track that aren’t dependencies (sad truth). If you are changing a dependency, and it requires their time and attention, you need to give a pretty good reason. Automate manual tasks. If there is a need for a spreadsheet, that means you have manual tasks that need automating. That sort of data aggregation needs to exist for you to be able to make high level decisions that impact large developer and user bases. Cool stuff we are doing and thinking about in engineering… 2 Thanks to Piper . JavaScript React Enterprise Intuit 2 claps 2 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-09"},
{"website": "Intuit", "title": "all hail the rtb champion", "author": ["Christopher Faria"], "link": "https://quickbooks-engineering.intuit.com/all-hail-the-rtb-champion-ade8c0bdb335", "abstract": "Hear Ye, Hear Ye! Allow me to regale you with the tale of the RTB Champion. Our hero keeps the dreaded RTB at bay and selflessly sacrifices themselves for the betterment of the team. Gather around and listen to their story. Seriously, the run the business (RTB) champion role has been a key factor to the success of our engineering team. As a long running team, we’ve had to fight the age-old battle of balancing developing new features/functionality with keeping on top of bugs, tech debt, customer support and other RTB tasks. In a perfect world we wouldn’t generate bugs or leave behind any tech debt, but it’s simply not natural in a complex, evolving system. There will be bugs. There will be tech debt. There will be customer support. Our goal is to reduce the amount and manage it, learning from each piece and evolving our product and our processes for the betterment of our customer and humankind — well — the team. We have sworn that we will dedicate 20% of our time to RTB with the rest allocated for new features and technical tasks. Previously we had tried to scope in RTB as part of our sprint planning with the execution of the RTB tasks being a free-for-all where any member of the team that was available would pick up and clear the tasks as they came in. Whilst this approach works — the team was able to tackle sprint work and RTB that came in — we found that there was a price: the cost of context-switching. Let’s imagine: A team member is working on a task which is part of the sprint goal for feature X. They finish the task and pick up a ticket for an alert that we received. They investigate and resolve the issue and then they need to familiarize themselves once more with the sprint goal and pick up the next task. There are two context switches that occur here, one from the sprint work to the RTB, and then another back from RTB to the sprint work. There’s a big difference between the two types of work. Sprint work requires more context and focus whereas the RTB is usually driven by following runbooks or doing investigations. They demand of the engineer two different mindsets entirely and we didn’t realize it at first but it was a time expense for us to have this context switching occurring. We began to see a trend that when one team member switched to focus on RTB and took a string of RTB tasks in a row that they would actually process the tasks quicker than someone who was constantly having to context switch. This surfaced itself in a retrospective, the team discussed and we decided to experiment: what if we put one person 100% focused on RTB? We tried it and observed that it paid off. The team members that were focused on sprint work found their productivity boosted due to not having to switch contexts. The team member that was in RTB mode for the week was able to move through tasks as they came in, always in the right context and benefiting from working through similar processes. We were able to tackle RTB at a higher capacity. It also had some side benefits. The new RTB champion role led to some ‘discovery’ and ‘innovation’ as some tasks had been found to be repetitive, but we hadn’t realized before because different people were taking them. We automated some processes for gathering data, improved our reporting and have made significant leaps forward in our managing of RTB. The definition of the new role also meant our team had a key point of contact for other teams when they brought RTB type work to our attention. It was still possible, as it had always been, that they could come to any member of the team and raise the issue. However, we observed that in having an RTB champion and making it clear who that champion was for any particular week (through setting it as the topic in our slack chat), that other teams would go directly to the RTB champion with their concerns. This prevented further context switching and has helped the team to remain focused and productive. Having someone with the full view/context of all the RTB also helped in reporting of the RTB. We have a weekly operations review where we review our operational excellence and having someone in the team who knows everything that occurred during the week helps to facilitate the conversations and communicate effectively the goings-on of the week. It has helped to spot trends, find points of focus and shape our technical roadmap. We strive to reduce RTB as much as possible either through elimination or mitigation. We’ve been practicing and iterating over the role for a few months now. The RTB champion role has evolved with the mandate to experiment more on trickier issues, or to take on technical debt tasks when the RTB volume was low. And that is the tale of the RTB champion. A hero to our team and a change which has brought much success. If you have something similar in your workplace I would love to hear about it and perhaps learn from it. If you haven’t then I would urge you to consider experimenting with an RTB champion rotation. It may not work for you, but then again, it just might. Cool stuff we are doing and thinking about in engineering… 15 Agile Software Engineering Process 15 claps 15 Written by Software Engineer working for the Global Payroll team at Intuit in London Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer working for the Global Payroll team at Intuit in London Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-06"},
{"website": "Intuit", "title": "semantic release making enterprise repo management way easier", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/semantic-release-making-enterprise-repo-management-way-easier-b78744475763", "abstract": "Ever wish your repositories could just release themselves? We here at Intuit did. Who has time to figure out versioning for every commit that goes in? Plus, manual releases can cause issues for people who haven’t done it before. Missing git tags because --follow-tags was not added to your push command. The version pushed is out of sync with master because you didn’t pull before you released. You may release a dirty workspace with a bad version of your code. Some repos have some sort of structure, others may not. What are the nuances between repos? You get the idea, there is a ton of room for error. Not only that, but you don’t get GitHub’s Release Notes when you release by hand using something like NPM or Yarn . Semantic Release is a tool that aims to ease all of these problems and role them into a single node module. TL;DR: You can jump to Implementing Before we get into Semantic Release and setting it up, let’s cover some core issues. What does it mean to “semantically release” your repo? Our goal is to make determining a version much easier than leaving it up to humans. Instead of looking a bunch of commits and saying “I feel like that is version x.y.z”, we actually look at what got changed and make a concrete decision. It is very easy to say “Breaking is clearly a major version, something that may break or introduces a new feature is a minor version, and anything that is 💯 backwards compatible is a patch” But, try doing that when there are a dozen changes in your repo, half of which add features for other teams, and half of which are bug fixes to core functions. Now you tell us, what semver do you want to tag your release with? It’s the difference between guess the price of an item at checkout, and being able to tell your consumers “Yes, that will cost $3.50”. So the problem is analyzing commits over a period of time. Easy right? Well, so long as you lint your commit messages and enforce a style sure. Maybe that isn’t the case yet for your team though. You probably seen the issue of people coming from other teams and committing however they decide works for them. Things like: “fixed some bugs” “fixed typo in method” “changed stuff” ← 💀 The Angular community has attempted to address this issue. The community created a set of commit guidelines for contributors to follow. Contributors can still commit like they do above, but they need to add a little preface to the commit to make parsing what the commit is easier. fix: some bugs fix: typo in method We can ignore that last one, just call it invalid because it says nothing about the commit. But, this idea is really quite useful! Now, even if I, or some other team member, have no idea what was changed, we both know that we had two fixes go into the codebase. With this small change, a number of doors are opened around automating and analyzing these tags. Semantic Release aims to standardize around these tags and make releasing easy. Now, for example, a change that says: docs: updated README with new badges Is not even considered for release. Semantic Release only cares about fix es, feat ures, and BREAKING changes. More information around what triggers what can be found in their README . We also get a whole slew more though! Semantic Release is plugin based, so it is quite extensible. By default, we get a number of niceties out of the box . Since it is plugin based, what it can do is limitless. You can grab from the community, or write your own if there is one that doesn’t fit your use case yet. To get started, we first need a few things for our node modules to release automagically. While Semantic Release provides a nice CLI tool, it doesn’t cover the use cases for certain enterprise customers (let’s be honest, enterprise setups can be very weird). Pre-Commit Hooks 🐶 yarn add @commitlint/cli @commitlint/config-conventional husky --dev --exact This will provide us a way to lint our commits by passing them through a nifty little git/hooks program known as Husky . Add the following section to your package.json so as to trigger the linting on commit Semantic Release 🚀 yarn add semantic-release @semantic-release/npm --dev --exact — We’ve started using exact in our orgs because we have Renovate handling our version uptakes of dependencies. Add the following section to your package.json : This will ensure your package.json version field gets updated with each release. 3. Ensure you have an .npmrc file in your repo with your registry properly set. 4. Add the following as environment variables on your CI (we use CircleCI on our team) GITHUB_TOKEN — This should be a service token generated by either a user or a non user account (preferrably a non user account). NPM_TOKEN — Generated from your enterprise NPM account GITHUB_URL — The api/v3 endpoint for your GitHub Enterprise instance Semantic Release will look for these variables when running so that it can interact with your GitHub Enterprise Server as well as your NPM Registry. 5. Finally, update your build configuration, in our case it is our .circleci/config.yml to run Semantic Release If you are having trouble, you can always test locally by exporting the environment variables and then running yarn run semantic-release -d . This will run it in dry-run mode so that you don’t accidentally release. For one commits worth of time, and maybe a little elbow grease if necessary, you can have a very basic automated deployment system. While it is still important to document what this does (feel free to link your team to this article), it abstracts away one more step from the release process. This is useful when teams have high turnover or new members frequently. It’s also useful for those one off repos you may not touch as often since now all your team has to do is merge pull requests. CI takes care of the rest! Cool stuff we are doing and thinking about in engineering… 23 JavaScript Automation Intuit Continuous Integration 23 claps 23 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-21"},
{"website": "Intuit", "title": "web performance what you dont know will hurt your customers", "author": ["siddharth ram"], "link": "https://quickbooks-engineering.intuit.com/web-performance-what-you-dont-know-will-hurt-your-customers-fd6b19b7da07", "abstract": "The impact of poor performance on business outcomes is startlingly consistent. Famously, Google increased the number of results of search on each page from 10 to 30 [1]. Traffic and revenue dropped by 20%. Amazon estimated that a 100ms slowdown in performance causes a 1% revenue impact. If you are running an e-commerce site, after 3 seconds, 40% of customers will abandon the site². How do you ensure that you have the right performance built in to your product? There are plenty of papers on the web which tell you the engineering aspects of performance. What is missed out is that performance thinking starts before the first line of code is written . This post covered key principles in end to end story of great website performance. Designers often don’t think about performance in their designs — that is just an engineering concern, so they think. It is most certainly not. A well thought out design takes performance considerations into account. Your customers may be around the world, with varying network latencies and CPU’s. There is a strong correlation between JavaScript and the customer’s CPU — a large dose of JS will work fine on the standard developer machines but will suck on a CPU that is 5 years old — which is likely representative of many of your customers. A great design team will take this into account and work with engineers to propose ‘light’ designs for network and CPU constrained customers, with an option to upgrade to the full experience. Recommendation: Develop designs that will work for all customers. A key way to develop this is shared later — adaptive designs that account for network and CPU latencies that your customers are likely to encounter. Flex your functionality, fix your performance budget, not the other way around. Companies often measure the 50th and 90th percentile performance. The 50th percentile (median performance) is useless as a performance metric. Looking at median performance is equivalent to saying ‘we don’t really care about the bottom 50% of our queries/customers’. At 90th percentile, you still have a problem. Let’s assume that you have 10 million requests per day. The 90th percentile gives you no insight into 1Mrequests. How is that ok? Performance tends to have an interesting characteristic. From the 50th percentile to the 90th percentile, it is typically easy to extrapolate: there is a linear relationship. The linearity starts breaking up beyond the 90th percentile. By the time it is at 99th percentile, numbers have spiked significantly. By the 99.9 percentile, you are seeing a hockey stick of terror. The really bad news is that: This is quite counter-intuitive. TP99 refers to the 99th percentile of performance. So how can it happen all the time? This is because what I refer to as ‘The Tyranny of Microservices’. Even if underlying micro-services honor a single TP99 standard, they will collectively be unable to honor the SLA. Let’s assume that user response is dependent on a single service (a monolith, for instance) and it has a 1% probability of responding outside your SLA. Then 1 in 100 requests is going to be impacted by poor system performance. What happens if a page being rendered depends on 10 services with a 1% probability of responding outside SLA? 1 — (.99)¹⁰ = 9.5% of your requests will not respond in time. Well, what if you actually were making 100 calls to services with 1% probability of violating SLA at TP99? 1 — (.99)¹⁰⁰ = 63% of your requests will violate your SLA. At 160 calls, there is an 80% probability that a given request encountered a 99th percentile response. Recommendation: Be wary of going too far with your microservices. Fine grained services sound fantastic from a velocity and independence perspective but can seriously hurt your performance. Think about the granularity of your calls. Making repeated calls for small amounts of data is bad from a resiliency and performance perspective. In addition, World class companies do not stop at TP99 — they also monitor TP99.9, TP99.99, TP99.999. To solve for performance beyond the 99th percentile, you need to think out of the box. ‘The Tail at scale’³ is an excellent read for patterns that will help solve for outliers Unexpected attributes of the customer environment can cause inconsistent behavior. Networks that your customer have can be inconsistent, especially on shared bandwidth like DSL. Overly aggressive microservices can result in TP99 behavior often, as explained in the previous section. Browser cached assets can be slower than fetching from the network (yes, you read that right) Customers work on multiple application in parallel, typically. This means that the OS time shares across everything running locally, including the browser. We have learnt the hard way that cached assets in the browser can be abnormally slow to load due to the underlying scheduling algorithms in the OS, virus scanners and other resource intensive applications running in parallel. Actual faults in the underlying hardware can slow down disk accesses — something that happens intermittently⁴ Network connectivity poses another problem. Users can face as much as a 5% packet loss within a two minute window on DSL connections, in our experience. This is more common when the user is accessing web applications while being mobile. All of these lead to underlying retries and sub optimal network performance. Recommendation: A search for “chrome waiting for cache problem” will show a range of solutions that are suggested including running the good old chkdsk. As application developers, implementing service workers that support network cache race strategy can alleviate the issue significantly⁵. For the Network connectivity, something you have no control over, a few things can be done: Pushing critical assets to users ahead of the time of need via service workers, push notifications⁶ Enable Brotli compression to reduce the time spent on the network⁷ Many of the problems above can be warded off by using hardware representative of what customers actually will be using. If you work with millions of customers, a substantial portion of your customers are likely not to have the screaming hot performance of your developer machine — Instead, they will have PC’s running older OS’s and hardware. The performance of a poorly designed page is likely to be significantly worse on a real world computer. Recommendation: At Intuit, we have set up a Real world performance lab, consisting of machines scavenged off ebay. These are reflective of the actual machines used by customers across the world. This has helped us develop a sense of empathy in addition to understanding how code will actually perform. The chrome developer tools⁸ are very useful in both simulating slow CPU/network conditions and understanding performance.. Intuit is also a big users of WebPageTest⁹ that help us test in varying conditions, which is the next point below. The best time to test for performance bugs is every time a PR is made. Every code checkin — front and backend — results in a performance test job using our CI/CD pipeline. When the code change shows a degradation in performance, the changes are quarantined till the results can be examined. This has resulted in us detecting many regressions and ensuring that customers have never seen them. We make extensive use of WebPageTest for this validation. We could not recommend it strongly enough. Not everything is under your control. Your Javascript is parsed on your customers machine. The specs of the customer’s machine has an impact on your performance. Much of this can be measured. You have good insight in the customer’s hardware via chrome API’s (crack open your developer console and check out navigator.hardwareconcurrency¹⁰, navigator.deviceMemory¹¹, Network Information API¹²). This will not always be available for customers using different browsers given the current experimental status — but the data can be extrapolated based on known measurements. Recommendation: Measure information about your customers environment where possible. Over time, you can use this data to have a deep understanding of your customers machines and be able to generalize it. For example you may choose to have a different product experience for customers in India who often have high latency — and the default experience for customers in India can be different It is critical to start by measuring the customer experience — not what your servers are seeing. In between you and the customer are lots of devices — CPU’s, networks, storage, all of which could cause the customer to have a poor experience. Real User Monitoring — measuring from the browser — is a key metric. This can be measured in several ways. There are a number of 3rd party tools that allow this measurement. Intuit measures it with a custom library that allows greater flexibility in filtering and changing metrics. Measurements does not mean ‘what is my software performance’. It needs to be end to end. That means you understand DNS performance, TCP, SSL and request/response processing. This W3C image¹³ shows what you need to think about: Intuit measures this data starting at fetchStart. The end event varies depending on the nature of the page). For some pages, it is loadEndEvent. For others it is domInteractive. Recommendation: Understand your performance budget end to end. Measure it on all browsers. Understand what your budget it prior to the first byte being fetched. You might be surprised of how much of your budget is taken up by your CDN, DNS, TCP, SSL (and radio setup, if you are on mobile). It is easy to plan for asset size. A simple spreadsheet will allow you to type in your latency, bandwidth and a few other numbers and it will tell you what your asset size should be to meet your goal. Expressed differently: If you have stateful services, you will not be able to scale. If you are unable to scale, you will not have a good time. Statelessness is a key component to great performance at scale. Statelessness means that you scale infinitely to handle more traffic. If you have a stateful service, you will face bottlenecks in performance during peak times. Your 99th percentile performance is tied to your behavior under stress. Parsing Javascript is expensive, and is very much a function of the CPU. The chart below (from Addy Osmani’s presentation at Chrome Dev Summit¹⁴) shows the startling difference between a iphone 8 processor at the top and an Asus Zenfone 5 at the bottom. Recommendation: Think hard about what the right JS size is to get the right user experience. It is surprising to see how often fundamentals are not taken care of. Are all your assets in a CDN? If you are serving them out of your web tier, you are hurting performance. Do you share consistent CSS in the organization? When customers move from page to page, they should be able to reuse the same CSS. Do you have OCSP stapling enabled¹⁵? That will help save round trips from the client to the Certificate Authority. If you are using a cloud provider, do you have location affinity for customers? Performance is a fundamental quality attribute of your system. In a distributed system, performance needs to be thought through carefully. Microservices give scale characteristics but requires more careful thinking around performance. The performance tiger team in the Small Business unit helped both validate the content but more importantly helped put improvements in front of customers. Tapasvi Moturu reviewed and added content to the document. http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html 2. http://www.mcrinc.com/Documents/Newsletters/201110_why_web_performance_matters.pdf 3. https://developers.google.com/web/tools/chrome-devtools/ 4. http://bitvisuals.com/2017/05/19/fixed-chrome-waiting-cache-problem). 5. https://developers.google.com/web/fundamentals/instant-and-offline/offline-cookbook/#cache-and-network-race) 6. https://24ways.org/2016/http2-server-push-and-service-workers/ 7. https://opensource.googleblog.com/2015/09/introducing-brotli-new-compression.html 8. https://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/abstract 9. https://www.webpagetest.org/ 10. https://www.chromestatus.com/feature/6248386202173440 11. https://developer.mozilla.org/en-US/docs/Web/API/Navigator/deviceMemory 12. https://developer.mozilla.org/en-US/docs/Web/API/Network_Information_API 13. https://www.w3.org/TR/navigation-timing/ 14. https://www.youtube.com/watch?v=_srJ7eHS3IM 15. https://en.wikipedia.org/wiki/OCSP_stapling Cool stuff we are doing and thinking about in engineering… 88 3 Web Development Web Performance Performance Performance Testing Performance Art 88 claps 88 3 Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Written by @_siddharth_ram; CTO @Inflection Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-09"},
{"website": "Intuit", "title": "perf matters 17 a brief recap", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/perf-matters-17-a-brief-recap-9b5b137ff86e", "abstract": "This year was my first year at Perf Matters, a conference dedicated to speed and performance in the web browser. While there were many good talks, there were a couple that really stood out to me and that I believe are worth sharing. At the time of writing this, there doesn’t seem to be any video recaps of the conference, so I am going mainly off of my notes that I took during the conference (thus, please pardon any errors on my part). Note: While I only cover a few (two) talks here, there were many more I did not cover to keep this short and sweet. I encourage those interested to reach out to the speakers and ask for more info on their topics of discussion. Pinterest gave a pretty interesting talk about how they have worked over the last 9 months to improve the load time for their top three workflows. Before going into detail, they noted a few high level stats they had observed: Average expected load time for a page is around 2s Slower sites have a 28% abandonment rate 80–90% of bottlenecks reside client side After asserting their points above, they started to talk about things they needed before they could approach this performance problem. Mainly regression testing and a mentallity shift. Performance cannot be something that just a few people own in a company. It is every developers job to own performance. They handle these areas in a slew of different ways, but at all levels tried to make sure their metrics were accurate and precise. Regression Testing → the team created a regression tool (more below) that would back up their assertions and allow them to find bad commits Mentallity Shift → the team ran a PR campaign in house to evangelize performance as a habit Regression testing was really the meat of this talk. The team took steps to create a baseline measurement of performance at the onset. Then, they layered a CI job ontop of that baseline that runs 100 times upon every couple of commits to master. So the workflow looked like: PR → Merge to Master → Every X commits kick of CI What this allowed them to do was to cherry pick a set of commits and run them against a test environment and assert that the average baseline speed was the same or lower than the prior run. They ran the test suite 100 times to be statistically significant, as well as attempt to weed out network anomolies. Should a PR fail due to a regression, there was a algorithm that they described as “similar to binary search” that would single out the possible “offending” commit, and notify the contributor of the regression they introduced. In addition, it would create a revert PR if possible. This type of regression automation is a great way to help ensure that the progress made is not accidentally being undone over time via creep. In addition to the constant regression testing, the team went a step further and ensured that any regression was tied to an on call alert like PagerDuty. This was to ensure that regessions would not go unnoticed. Treating performance like a production incident is a great way to push teams to ensure it isn’t poor. For the PR campaign, the team spent time focusing on doing company wide demos, as well as being a resource for others. The offered office hours and working sessions to empower their developers as they brought changes to the development life cycle. One of the most important things, with any cultural shift in a company, is to ensure people feel like they can be a part of it and understand it . The takeaways are many for the Pinterest team, but the few they selected to highlight were scallability, ownership, and strategy. Once they started down this route, they have to continue to own what they started, and think about strategies to improve and scale. On top of this, they used their feedback loop to ensure that their tools were developer friendly, and that they improved on their efforts based on what worked. While what they shared has only been about 9 months of progress, there is still so much more they plan to do (none of which they disclosed). Yoav Weiss gave a great talk about the problems facing content delivery today on the web. Over the passed decade, websites have tried to deliver more and more (while still being performant), but under two decade old constraints: latency and file size. At a high level: Latency has a huge impact on performance. Though it has shrunk, not everything has gotten better with it. Server and client still must perform a handshake → this is latency bound. Servers still have “ slow start ”, a preventative measure that makes a server gradually ramp up to filling the connection pipe so as not to cause a disconnect. Discovery still blocks rendering — HTML must be read and parsed. That means DOM blocking scripts, CSS Object Model compilation, and plain old javascript. All of prevents the page from painting. In addition to that, developers are left with basic tradoffs and guessing. There is only so much that can be delivered on first page load, and we as developers aim to deliver only the necessities. However, we always run into: Guessing → using data to structure the page load time to only load what we think is necessary (could be wrong) Contention → Of the things we do load, we have a limited number of resources per round trip (HTTP 1.1 being a good example of this) Bloat → Just plain software bloat that slows us down. Think of how many libraries and files you load upfront. Do you need all of them right away? It may seem like an impossible challenge, but there were some key takeaways for developers operating in the space today. H2/HTTP2 is the biggest improvement. H2 allows requests to the same endpoint to be prioritized by the server. Sounds like a silver bullet right? Well… sort of. The words “same endpoint” are very important there. If you are making network requests to assets.company.com and scripts.company.com , those endpoints will prioritize within themselves, but not amongst each other. Careful with that. The recommended solution was to unshard your endpoints. Also, something to note is that while performance should get better, it isn’t guaranteed. Packet loss with H2 can be worse in some cases than with H1 (HTTP 1.1). The other protocol, which is still in development, is QUIC by Google. However, there are two things moving against this. It isn’t a standard It only exists in Chrome and for some servers as of now While Google is still working with standards bodies to have QUIC be a standard, it isn’t one currently. Careful adopting as it isn’t across all browsers and skews your performance to one platform. There are also many proposals being made to help improve bundling! Server Side Process → H2 push, ramp up faster HTML Discovery Improvements → this would be things such as link rel=preload or as=script . It would add priority based hints to HTML. Improving Handshake Connection → Things like connection coalescing, TCP fast open, TCP 1.3, etc. There was a ton of information in this talk, but to sum it up: Use H2/HTTP2 Consider other compression algorithms (such as Brotli) over Gzip. Weight the options before blinding jumping in. Size gains are substantial, but could require more processing time server side. Leverage Push/Preload/Preconnect Unshard your domains Coelesce your connections These were only two of the many great talks given at this conference. Other things that were notable were Mozilla’s talk on Firefox’s use of Rust for parallelism in the browser (thanks Lin Clark ), and Netflix’s talk about tradeoffs (development vs performance) and the fine balance of these tradeoffs (thanks Jem Young ). The biggest takeaway is that there are many improvements teams can make to easily speed up their performance, but at the end of the day performance must be a priority for your company as a whole. It doesn’t happen overnight. Many aspects of the web are doing their part to improve the web, and your team should focus on how they can contribute to the process or leverage existing technologies to do so. Cool stuff we are doing and thinking about in engineering… 2 JavaScript Performance 2 claps 2 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-16"},
{"website": "Intuit", "title": "operationalizing scikit learn machine learning model under apache spark", "author": ["Raymond Chapman"], "link": "https://quickbooks-engineering.intuit.com/operationalizing-scikit-learn-machine-learning-model-under-apache-spark-b009fb6b6c45", "abstract": "At Intuit, I work as a member of the data team for small business and self-employed group where we collaborate with data science and data analytics teams to deliver ML models to production. One recent project involved taking an offline Python model developed with scikit-learn library, popular amongst data scientists and ML enthusiasts alike, and deploying it to a hosted Apache Spark cluster. Details of effort to run model code using PySpark, Spark Python API, plus various improvements in overall execution time and model performance are shared here. The ML use case involves a supervised learning model run daily to generate product recommendations from customer features. The code combines model training and prediction generation. Model training is accomplished with RandomForestRegressor algorithm and GridSearchCV functionality to perform hyperparameter tuning . Basically, we train multiple models in parallel varying algorithm input parameter values. Each model’s performance is evaluated with a score. The best scoring model is chosen for prediction phase. Input data passed to train and predict steps is processed as pandas dataframes. Steps within Python model code are: When entire job ran on a single machine (16 GB memory, 4 2.80 GHz CPU cores), it takes a total 8+ hours to complete. The job executing this code was run manually, required download of raw feature data from data lake, and upload of generated recommendations to consuming application. Apache Spark allows us to overcome shortcomings of single server operating environment. The cluster has direct access to data lake for immediate retrieval of input features and final storage of generated recommendations. The availability of multiple server nodes in cluster has the promise for distributing tasks within the job. In addition, the job can be injected into a schedule for automated daily execution. Here’s an overview of how the job is transformed to run under Spark: It is no surprise that most time-consuming aspect of original job run is model training: The parameter grid supplied to GridSearchCV through parameters has 8 total combinations of input value. Each combination represents a candidate model and is evaluated using cross-validation k-fold value of 5, i.e. cv=5 , where original sample data is randomly partitioned into k equal size subsamples. This results in training a total of 40 separate model instances. While GridSearchCV attempts to run each instance concurrently based on n_jobs input ( n_jobs=-1 asks to run against all available CPUs), training is bounded by availability of CPUs on host system for competing processes. Leveraging DataBricks scikit-learn integration package for PySpark, spark_sklearn , we can substitute a Spark friendly implementation of GridSearchCV to distribute execution of each model training run instance against training data across Spark executors. Each instance runs in parallel across 40 Spark tasks: It reduces overall time for job run to 2 hours 30 minutes. The Spark cluster needs to have spark_sklearn package available across all nodes. In our case, it is available through Anaconda parcel for Cloudera from Continuum Analytics : This solution works for smaller input datasets to training phase in order to fit into available memory on a single node. Larger datasets would require switching to use Spark model implementations . Code changes: When running updated code in Spark, spark_sklearn GridSearchCV has a separate task for each model instance. Looking at the stderr log output for an Spark executor running one of those tasks, the parameter combination trained, its score and how long it takes are displayed: Final score results is accessible from spark_sklearn GridSearchCV object and displayed following completion of training: produces output in Spark driver stdout: The elapsed training time is a reflection of longest running parameter combination. With logging of GridSearchCV in place we can examine scoring values and elapsed times to compare training result for each parameter combination. What we find is that the following parameter combination takes a significantly higher time, and produces an under-achieving final score far from ever being the best model. The parameter combination with consistent best score is: Adjusting the parameter grid to: to replace max_depth=None with additional upper and lower parameters close to max_depth value appearing in best model, we increase number of model instances to 60, but reduce overall elapsed time of model training to 60 minutes. This actually improved the model performance too by yielding a new best model parameter combination for one of the new max_depth values: The scikit-learn Python model takes input data as a pandas dataframe format for both training and prediction phases. Under Apache Spark input data is read as a Spark dataframe and subsequently converted into a local pandas object. Depending on size of pandas objects it can add significant memory overhead to the Spark driver process (where all data is collected). In our use case, training data has 141K records each with 500 features, whereas prediction input data has 2.4 million records. To offset memory demands on Spark driver process we can: Reduce original input data footprint by preprocessing raw features, such as dropping features with no variance in values, in a distributed manner while still in a Spark dataframe. Essentially, swap out single node preprocessing using pandas with Spark dataframe transformations. To mitigate possibility of job failing due to OutOfMemory errors when generating predictions from a large input pandas object, we can process in chunks by splitting it up into smaller pandas objects. Each object is run against the model, iteratively, to produce a pandas object with corresponding recommendations before being converted to a Spark dataframe and written out as CSV file in HDFS. While the final outcome is favorable, a number of challenges were encountered in porting model code to Apache Spark utilizing spark-sklearn GridSearchCV: High CPU Utilization by PySpark Python Processes As mentioned spark-sklearn GridSearchCV distributes each candidate fit (or model instance) execution as a Spark task amongst available Spark executors. Originally, each executor was assigned 4 CPUs (spark.executor.cores=4). This meant it was possible for some executors to have up to 4 concurrent candidate fits running. You would think this should not be a problem as each node has at minimum 4 CPUs available, but the architecture behind PySpark results in a dedicated Python process spawned for each task by parent JVM executor process. And, each Python process is not limited to same CPU cores as executor. Therefore, when a RandomForestRegressor object is created with n_jobs=-1 (as in original model code), as many concurrent threads as required by candidate fit are used in training the model. This translates into Python process for computational intensive candidate fits using most if not all CPU cores on machine where executor resides. It results in high contention for CPU resources from not just each of the 4 possible tasks running for the executor, but other executors for our Spark job and any other application assigned by YARN (the resource manager at play here) to execute on same host. Consequently, we need to set n_jobs to a finite number in model code, we set it to 20. In addition, the Spark job must be configured to have enough executors for each candidate fit while keeping a single task per executor: This does not completely eliminate possibility of having multiple executors running on same host — controlled by YARN. It is still possible to end up a situation with high CPU utilization and contention impacting elapsed time for model training. Below we see 2 executors running on same node each with a Python process running a model fit instance: Executor Keep Alive To ensure enough executors were available for model training (following loading of training data), the executorIdleTimeout property needed to be configured with allowance for time spent extracting features from training data on driver. Without this, a number of executors are killed during features processing and when model training begins GridSearchCV tasks are distributed amongst a few remaining executors resulting in tasks being backlogged. Collecting Data To Driver When the driver collects Spark dataframe containing user data into local Pandas dataframe, some default configuration properties need to be adjusted to prevent failures: Property controlling limit for data collected by driver from a Spark dataframe, spark.driver.maxResultSize , needs to be increased to accommodate input data size. Property controlling limit for Kyro serializer buffer, spark.kryoserializer.buffer.max , has to be set to maximum allowed value for job to succeed. Following partial distribution of model training and prediction code in first attempt to execute code in PySpark, attention was switched to remaining time-consuming parts of the model process. Areas explored include: Distribute model prediction code Distribute preprocessing of input data for model prediction The first version of Spark code does not scale well with increasing volume of input user data for model prediction. Collecting such a large set of user data into Spark driver process as a Pandas dataframe is subject to memory and serialization failures. Entire process of extracting features from user data and retrieving model predictions needs to be distributed to avoid these potential issues. Spark allows us to process records in the underlying distributed partitions of a dataframe RDD in parallel, using mapPartitions( func ) function. By enclosing input function func in a Python object containing all required objects necessary to extract features, retrieve predictions and assemble recommendations per record key, all that remains from Spark driver perspective is to wrap result from mapPartitions( func ) as a Spark dataframe and write out to HDFS as a CSV file (or collection of files). The Spark driver performs pandas feature extraction logic from large input dataset in prediction phase of the job. Equivalent Spark dataframe transformation functions can be used instead of pandas transformations to distribute work to Spark executors. In this way we eliminate resource pressure on driver and reduce overall processing time. A set of PySpark functions were developed for each input data preprocessing step. Tasks now performed against Spark dataframe instead of pandas object include: Update empty string column values with ‘unknown’ Drop unused columns and columns identified as excluded in training phase Replace null data across a number of columns Drop duplicate rows The transformed Spark dataframe has mapPartitions( func ) function applied, as described in previous section. Each partition of the dataframe is extracted as a pandas dataframe since model predict function expects a pandas object. Additional processing of the features is applied before passing to model predict function. It aligns with feature processing performed on input pandas object during model training. In obtaining predictions in a distributed manner from a spark-sklearn wrapped scikit-learn model, some important lessons were learned: Include all dependencies Ensure all dependent objects required for processing user data is available to input function func of mapPartitions( func ) . In our model use case it includes: Trained model object returned by spark-sklearn GridSearchCV implementation List of columns defined in input user data Spark dataframe. It needs to be passed explicitly as they are not available while iterating partition rows) Output columns (y_cols) extracted during model training. Feature specific columns (x_cols) extracted during model training Static list of categorical feature columns (g_categorical_columns) Static list of columns to be excluded (g_excluded_columns) Static dictionary of categorical and non-categorical feature columns with default value assigned (g_users_columns) List of derived columns from input user data Spark dataframe not excluded or not a categorical column (non_categorical_columns) Boolean dictating number of recommendations to extract (day_one) Exclude Spark Context It is illegal to pass Spark context in content distributed to process each partition using mapPartitions( func ) . Will result in error: The spark-sklearn wrapped model object, passed to mapPartitions( func ) has a reference to Spark context (in order to perform distributed GridSearchCV functionality during training). We need to null it out as it is no longer needed for predictions: Apply input data columns as index to Pandas dataframe Mentioned earlier in set of dependencies required by mapPartitions( func ) is the list of input Spark dataframe columns. These are required in order to continue to preparation of model input features. Why? Well these column names are not available in Python iterable object, representing each partition row. When partition is converted to a pandas dataframe, a pandas.indexes.range.RangeIndex is constructed to label each column as an integer, but this is not sufficient. What we do is re-index the pandas dataframe with supplied list of input Spark dataframe column names: Include missing features due to absence of categorical data in partition data One of the final steps in preparing input feature vector to model, in both training and prediction phases, is encoding of categorical data. Encoding a column named ‘Color’ can produce binary features like ‘Color_Red’, ‘Color_Blue’, etc. When processing a subset of all input data, as is the case when operating on a single partition of input Spark dataframe during prediction phase, categorical column values may not include full range of possible values. As a result, some feature columns will be absent after encoding step. For instance, if no value ‘Blue’ was found in set of values for column ‘Color’, the feature ‘Color_Blue’ is excluded from final set of feature columns. Model prediction will fail since input feature vector does not match what was produced in training. To address this problem we need code to identify missing columns, and inject them with default values: The final Spark job now takes just over 50 minutes to complete, compared to initial 8+ hours. Here’s a view showing extent of distribution applied: We’ve covered quite a bit of ground here. There’s significant value of using the distribution power of Apache Spark to operationalize an existing offline scikit-learn model. In addition it can be helpful during development/training of a new scikit-learn models to quickly identify best model from hyperparameter tuning. Cool stuff we are doing and thinking about in engineering… 133 Machine Learning Scikit Learn Apache Spark 133 claps 133 Written by Machine Learning Engineer Cool stuff we are doing and thinking about in engineering at Intuit Written by Machine Learning Engineer Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-03"},
{"website": "Intuit", "title": "lessons learnt from netflix keystone pipeline with trillions of daily messages", "author": ["Sandeep Uttamchandani"], "link": "https://quickbooks-engineering.intuit.com/lessons-learnt-from-netflix-keystone-pipeline-with-trillions-of-daily-messages-64cc91b3c8ea", "abstract": ". Netflix streaming pipeline (called Keystone) processes 2 Trillion messages/day at the peak! It handles 3 PB/day of incoming data, and 7PB/day of outgoing data! Built 100% on AWS, Keystone is built as a self-serve platform allowing multiple teams to publish, process, and consume events. There are lessons from Netflix’s operational experience that every enterprise can leverage . Monal Daxini (Stream Processing Tech Lead) at Netflix recently gave a talk at Intuit and spent time afterward talking to the team. Following is a summary of key operational lessons from the discussion. In the context of CAP theorem , Netflix has use-cases that are both AP (Availability over Consistency) and CP (Consistency over Availability). Hierarchical design of the Message Bus: Netflix uses Kafka as its technology for the message bus. Similar to most deployments, they have orders of magnitude more consumers than producers. Their approach is to have a Fronting Kafka cluster, that feeds multiple Consumer clusters. The Frontend cluster provides bulkhead isolation from errors in Consumer clusters. The Frontend clusters are non-keyed for better availability, with a topic retention of 8–24 hours. Consistency Guarantees at Scale: Keystone is configured with At-least once, ut-of-order delivery (with timestamp-based sequencing handled by the app layer). At scale, Keystone has less than 0.01% drop rate per day — Prefer dropping data rather than impacting a user-facing service application that is producing events. Event producer library creates a buffer whose size is dynamically adjusted within bounds as a best effort to prevent drops. A significant increase in event traffic (multiples of provisioned traffic) for a topic, is usually indicative of either an unplanned increase or an application bug related to producing events. Nodes per Kafka cluster : Through empirical iteration over years with various cluster sizes in AWS, the team follows the best practice of a max of 200 nodes (VMs) per Kafka cluster. This could lead to a dedicated cluster for topics with large even traffic. Partitions per topic : To decide the number of partitions per topic, the rule of thumb used is 0.5–1MBps per partition. For instance, if the expected topic throughput is 10 MBps, there will be 10 partitions allocated to the topic. ZooKeeper per cluster : Each Kafka cluster is deployed with a separate ZooKeeper cluster Durability profiles : Depending on the nature of data, the Kafka clusters are configured with 2 or 3 replicas. Also, for AP clusters (Availability over consistency), they use the option to restart with Unclean Leader election Kafka message Metadata : The Kafka client library is wrapped in a Netflix ecosystem friendly wrapper to apply producer best practices, and transparently add metadata attributes to each message: GUID, Timestamp, App Name, and Host. Handling Config Changes as a simulated failure : Changes to the Kafka cluster (code upgrades, topic scale up or scale down, moving partitions) are handled by failover to a new Kafka cluster. Monitoring the cluster : Netflix has implemented a Kafka Auditor to track the lag between producer and consumer (similar to other audit tools such as the one from Uber ). For a detailed treatise on Keystone and the related stream processing platform that Netflix is building, Monal has an interesting AWS re-invent talk . To summarize, as real-time streaming analytics becomes the de-facto, frameworks such as Keystone are going to become a critical building block for every modern SaaS Data Platform team. Cool stuff we are doing and thinking about in engineering… 77 Big Data Streaming Analytics Kafka AWS 77 claps 77 Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Written by Democratize Data + AI/ML — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author|DataForHumanity.org Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-26"},
{"website": "Intuit", "title": "moving to codecov on ci cd", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/moving-to-codecov-on-ci-cd-82050269cf9a", "abstract": "With the push for seamless CI/CD integrations, we’ve focused on refining both our user experience and seamlessness for our devs. While testing isn’t always the be-all and end-all of preventing bad code from being merged, it is a way to gauge from a high level that you are shipping code that does what you expect. Teams have widely used grunt-code-coverage-enforcer to prevent bad code from going in. While this tool does have its merits, there were a few main drawbacks: This analyzed code coverage, but didn’t collect it and make it visually clear what the main issues were during failures Teams couldn’t prevent people from adding exceptions or bending the thresholds While this is a tool we developed for our CI/CD, things like Codecov are more user-friendly and straightforward. Teams within Intuit have started to adopt Codecov’s Enterprise version to allow users to use the fine-grain options it provides. As we upgrade our CI/CD, we want tools that integrate well and are easy to use. There was so much more that we can get out of our CI/CD system: Integration with GitHub PR-level granularity on changesets Better visual overviews of how we test our code While aggregating coverage is good for upper management, it doesn’t always tell the full story. With our recent introduction of CircleCI , we’ve introduced a seamless way for users to easily add steps to their CI pipeline. Codecov provides many of the features above. Even better, the integration is a one-line bash script and a global key. Let’s take a brief second to look at what Codecov offers: Out-of-box GitHub integration (using GitHub OAuth) PR status hooks, as well as a bot that will post change diffs Granular overview of coverage (with badges!) Bonus : The ability to lock coverage thresholds to a specific branch Integrating with Codecov is straightforward. By default, you don’t need a config for it to work. This is convenient when you are first starting out and just want the basics. Start by onboarding the repo of your choosing in your Codecov Enterprise instance. You will be presented with a screen similar to what is shown below: Once that’s done, you have a simple one-liner to add to your .circleci/config (or equivalent if using another provider). Our link is slightly different as we have an Enterprise instance, but the command is the same. Just swap out the URL for whatever instance your company uses internally. Once you have saved that, head over to CircleCI and edit the settings for your project. You can now set an environment variable called CODECOV_TOKEN , which is the token we took from above when we added the repo to Codecov. And that’s it! Once you push, your code coverage results and all the other goodies will be activated automatically! This is just another gear in the CI/CD mechanism that makes life easier and more pleasant for developers. The less friction between all of these steps, the better. Remember: happy developers write better code. Software should enable developers to do their best work. While still in the early stages here at Intuit, our team believes the combination of CircleCI and Codecov is the future for CI/CD when it comes to testing on CI. Codecov provides fluid integration with GitHub, enabling developers to work instead of getting in their way. We are excited to see our developers adopt new tooling that allows them to write code without overhead or friction. Cool stuff we are doing and thinking about in engineering… 8 Thanks to Juanita Dickhaus . Continuous Integration Circleci Codecov Testing Intuit 8 claps 8 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-05"},
{"website": "Intuit", "title": "circleci helping intuit teams save 40 or more on ci cd", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/circleci-helping-intuit-teams-save-40-or-more-on-ci-cd-42eb23bf7313", "abstract": "CircleCI has an amazing set of features in a simple package that allows our developers to move faster than ever before. Coming from our old build system, we wanted to push Circle to the limits such that we could squeeze as much speed out of it as possible. Be as small as possible Be reliable/reproducible Be fault tolerant From the outset, we wanted to make sure that we could build at scale with the least amount of friction possible. That meant embracing open source, and interweaving it our internal solutions. At each step of our build process, we focused on these things. CircleCI leverages Docker containers to build and run the repositories being resolved. This is great because it allows us the flexibility to create containers that are unique to our organization, or to use those from the outside world. Leveraging Alpine containers with basic things baked in like Node, Bash, Yarn etc. provided a nice small start. Unfortunately, these didn’t scale well because it meant having to keep these images updated and add to them for each dependency needed. Some projects require tools like node-sass, which requires gcc to compile. This would mean more images to make and maintain. Eventually, we decided to use CircleCI’s Public Docker Images . Not only do we not have to make/maintain them, but they also include some nifty hacks baked (e.g. supporting Chrome and Firefox headless on CI when you don’t have a display to output to). Once we settled on our containers, CircleCI does the rest. Each build attempts to check if the current builder box has the image needed. As we scale, projects will overlap with image requirements. That means that each box will only have to download the image once , and then fetch it when needed. A build only spends time downloading an image when it doesn’t exist. In the worst case, we spend 22s downloading images per build . In the best case, we spend 0s downloading images per build . On a scale of hundreds to thousands of builds an hour using the same images, we avoid spending time downloading and setting up our environment over and over again. Many of our builds are NPM modules. This means huge node_modules folders. We are talking 500MBs large. Each build needs to redownload all of those and construct the file hierarchy. Thankfully, updating dependencies is not something that happens often. As such, we can avoid this all together. CircleCI allows us to specify caching of folders and files.Utilizing this is important as it prevents the need to do repetitive tasks. At the top of each .circleci/config.yml file, we provide the following: And we restore using: This allows us to always have a cache key. This is important because it means we will never have to download the entire node_modules directory at any point in the build cycle after the initial build of the project. CircleCI will attempt to match the key in descending order starting at the top. Thus, we provide most specific to least specific. This is useful when having PRs from forks or branches because it means the branch will always have a base to work off of. At any point, we will have some node_modules already downloaded. The diff for Yarn or NPM will be negligible, if one is needed. When we save, we use the following: This provides us with both the node_modules we need, and the Yarn cache for faster resolutions should we need to download new dependencies. In the worst case, this costs us 2 minutes and 18 seconds to download everything fresh via Yarn. When we have a cache, it only takes us 24 seconds to download our entire directory, and 0 seconds to resolve via Yarn or NPM. As an added bonus, if your internal registry ever goes down, builds can proceed because you cached your dependencies . CircleCI turned out to be more performant in general than our prior systems. One of the main areas we saved time on was queueing builds. Our prior system required the provisioning of a builder node. This node could take 1 minute and 30 seconds or longer to provision, and that is before any container was resolved. With CircleCI, we don’t experience this. Builds are immediate thanks to their use of Docker and other technologies. Again, having this on multiple pull requests and builds over the course of the day saves hours of dev time, if not days . How much time did we end up saving? On our prior system, our average build time for the repositories we were measuring was 6 minutes and 34 seconds . CircleCI, with caching and all the goodies above, brought that time down to 3 minutes and 33 seconds . By having local Docker images cached and ready, we shaved 1 minute and 30 seconds off each build. Caching node_modules decreased resolution time from 2 minutes and 24 seconds to just 24 seconds, saving 2 minutes on each build. By sharing Docker images, we removed the need to redownload and set up the environment every time. The ability to save/restore/propagate artifacts between builds is an enormous time save. While we are just scratching the surface, these initial changes have helped to accelerate our development speeds hand over fist. However, there are still more improvements to be made. Looking to the future, we will start to leverage Docker locally to vet builds before they go to CI. While limited in scope, CircleCI allows you to do this today for building locally using their CLI tool . We found this to be a bit limiting since it was only building, and not testing. One of our engineers ended up making a .circleci/config-local.yml to pass into the CLI tool. This local .yml contained the test flow, but named under “build” so the tool would run it. Using npm-scripts and Husky , the engineer was able to get tests to run in Docker with the same container that would be on CI via a prepush hook. This will allow contributors to know that their changes will work when they get to CI. In addition, some folks have been asking about shallow clones for larger repositories. While CircleCI doesn’t natively support this out of box, we have hacked together a solution (based off of their checkout script) that allowed us to do shallow clones. For reference, we were able to decrease a 1.5 GB clone that took 7 minutes to just a few megabytes that took 10 seconds. You can see (and give feedback on) the changes on GitHub . Cool stuff we are doing and thinking about in engineering… 130 Thanks to Piper and Juanita Dickhaus . Docker Circleci Intuit Continuous Integration Continuous Deployment 130 claps 130 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-19"},
{"website": "Intuit", "title": "solution for challenges rendering rich contents on email clients across devices", "author": ["Ranadeep Bhuyan"], "link": "https://quickbooks-engineering.intuit.com/solution-for-challenges-rendering-rich-contents-on-email-clients-across-devices-433255033a96", "abstract": "Quickbooks has started generating Responsive email Invoices for it’s Small business’s customers. Responsive email templates (templates that adjust to the size of your screen) are the cream of the crop of email marketing. When I open them on mobile the text is bigger and easier to read and the images have resized. This is the standards of email marketing in the modern digital world. I am going to outline our leanings that helps identify the culprit sabotaging any email content generation (programmatically) project — along with tips to make coding jobs a success. Modern devices like iPhone/iPad android devices supports HTML5, advance CSS and media queries same as like desktop browsers chrome, safari and Firefox. Email HTML can use those to create beautiful looking emails. However Outlook and office 365 are among the most dumb email clients when it comes to render smart and rich contents. When we look back few years from now, HTML email was not considered as necessity. There is still a lot of issues with html emails that are exists. Coding HTML email can be tough and it is even tougher when we generate rich HTML contents, knowing nothing about the type of email application that will render my HTML. If you’ve included video, Flash, rollovers, JavaScript or even a simple picture or text — that’s it, chances are it won’t show up the way you wanted it to. Even with plain text, things can go wrong with spacing and alignments in various email clients. These bellow are the most common issues faced by most of email creation codes if the HTML contains CSS and images. javascript? no! it will not execute at all. Large number of common HTML tags are not recognized the way they work in browsers. Syntax issues: Improperly nested tables or missing tags are easily avoidable easily. Make sure all tags are closed properly. 1.1 Use fallback : We found a few supported stuffs for the HTML contents — semantic building blocks for HTML email. Here is a list — medium.com We have a surprisingly smaller set of resources to build the email HTML that works across devices and email clients. Design for both the best experience and the worst experience as well keeping a baseline. 1.2 Choosing your Rendering library — if you are using react JS for server side rendering, upgrading to 16.2 . With react 15.x, non-standard HTML5 attributes were not supported, but many email clients still support them, like table align, valign etc. Another option is Apache velocity templates. Mainly the rendering engine needs to support basic HTML, CSS and manipulation of media queries during the content generation time. 2. Images are not showing up — there are a bunch of reasons why images does not display as expected. 2.1 One way to ask subscribers adds the sender email to their contacts. 2.2 Text of top of an image — background images are not supported on Outlook later 2007. Background color can be used as a fallback property. 2.3 Links and href — never use relative paths, instead use absolute paths. 2.4 Colored borders around images (blue by default) 2.5 White space bellow images — In a table based design 2.6 Image file format — Some email client does not support Tif, bmp, png 3 Fonts are not displaying correctly has couple of reasons. a) fonts are not installed and b) fonts specified in the email were striped out by the email server or the client. Override default settings with “0” — some email clients has their own logic of rendering defaults. Include values for cellpadding, cellspacing, image borders etc. Try, explicitly setting values for font colors, size and faces are also ways to override defaults. Issues with cells like, column/row not appearing in table add a border for troubleshooting Issues with off layouts and alignments — Double check you math. Include properties like valign, align, in <td> and <p> tags Isolate the problem — Copy your generate HTML code to browser debugger like Firefox and try using features like hide background, images, show ALT. Use W3C HTML validator to find out syntax issues. We had all in monolith —the build used to take 25mins in local and 30mins to test from the code. We moved out whole of it to AWS and started generating the HTML email from outside of monolith. In addition to that, we've created just a dummy page (simutaled) as a smart development environment. Now the HTML can be created where a real time changes can be visible on just a button click. Testing in as many email clients across devices, browsers and operating systems available is a challenge. There are quite a few online tools out their come handy. We subscribed a license for mailchimp. That can show the simulated HTML on a good number of email clients. It also allows to sent email as test. For any technical query on email creation issues — my twitter handle is @ranadeep_bhuyan Happy designing!!! Cool stuff we are doing and thinking about in engineering… 5 Email Marketing React Responsive Design HTML Email 5 claps 5 Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Developer @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-13"},
{"website": "Intuit", "title": "bringing invoicing to gmail", "author": ["Mukesh Aggarwal"], "link": "https://quickbooks-engineering.intuit.com/bringing-invoicing-to-gmail-846ea546e9b7", "abstract": "In United States there are millions of small business owners who juggle between spreadsheets (to keep track of sales/money), word/one off invoicing solutions (to send invoices) and email (to communicate with customers). They spend most of the time in their email app communicating with their customers and trying to stay top of invoices and payments while juggling between a medley of apps. Currently there is no one product which provides them a single interface which saves them time/money and effort. 55% of small business owners face ‘ time to get things done ’ as biggest challenge. What if they could just invoice/manage and communicate with their customers from single app, and all from within their familiar email interface? And what if that app worked on their computer and mobile phone at same time, saving tons of effort and time Intuit is opening up it’s platform to developers and integrating with several other platforms (Amex, Square, Paypal, Stripe among others). This was another opportunity to partner with best in industry to save time/effort and money for small business customers. So we partnered with largest email provider, Google, to bring a very simple invoicing interface from within Gmail. Half of small business customers using Quickbooks online are already use Gmail as their email provider. Goal was to allow merchants to manage all communication including sending invoices, checking status of payment on those invoices and communicate with their customers without leaving familiar Gmail interface on their desktop browsers AND their mobile devices. Since Google was already working on adding enhanced add-on capabilities within Gmail , it was a win-win-win for Small Business Merchants, Intuit and Google. And ‘Quickbooks Invoicing For Gmail’ project was born. Google’s choice of architecture for Gmail is an extension of already existing ‘Google App Script’ framework. Google app script is similar to ECMA 3 javascript specification (more on that later). Polished Interface with well defined sections/cards/buttons. Based on javascript, so almost no learning curve. Our first proof of concept which involved making a rest call to Intuit ecosystem and show data in Gmail interface was ready in just few hours. App script is automatically transpiled to client and server code. So all browser+server communication boilerplate code is fully eliminated ! A set of basic widget library is already provided (textboxes, check boxes, dropdowns, labels, buttons etc). That library is continually getting enhanced with frequent releases with more widgets and functionality. Make it possible to truly write-once-and-run-everyone app ! Just write code once and same code works on browser AND iOS+Android native Gmail apps ! No need to worry about different form factors/OS version/resolution. Provides a very good implementation of OAuth library with dedicated storage of oauth tokens. Just configure and use, eliminates writing boilerplate code for oauth. Has direct access to other Google functionality (ability to send email, store on drive, access GCP etc). Provides a browser based sandboxed environment/IDE with code complete for development. Provides fast storage/cache to store app level and session level parameters (useful for securely persisting tokens, settings, state). Easy to release new version which automatically gets propagated to all users. Google App script gets transpiled to server/client code so developers are limited UI controls provisioned by Google. That additionally means user can’t directly manipulate DOM elements or even listen to or emit browser events. All of user interaction is through preset callbacks. That means, as developer we can’t dynamically change the UI and provide pure client side validations/updates. Most of interactions require server trip. Google App script doesn’t have ability to fire async HTTP requests to any external api. All http calls are blocking and synchronous. That has big impact on code design. Unlike regular javascript, developers can’t just fire-off an xhr event and update results on callback. All UI transitions stop until results are received. All functions defined in Google app script package are global and there is no name spacing Since it is not pure javascript, it is not possible to reuse an existing javascript library as is. Reuse — At Intuit, we have several libraries handling complex business logic which we reuse across projects. And we wanted to use some of them in this project too. However all of those libraries are based on ES5/6 and by default not compatible with Google App Script. To avoid rewrite and constant catch up whenever a new version of those libraries are released, team came up with a innovative way to solve the problem. As first step these libraries are run through babel to convert to ES3 and then through rollup to export all functions to global namespace. This made it super easy for us to directly reuse those libraries. Source Control/Quick deployment — Google provides a browser based sandboxed environment for development. Changes are instantaneous as code is changed. However there is no predefined way to develop locally. No local development means no benefits of full source control (branching, merging, build hooks, pull requests, reviews etc) To work around this issue, we decided to develop using locally on IDE’s and use Intuit github repo. However that still meant we would need to manually copy paste code in Google sandbox to be able to run it and check behaviour. Now think about all different environments (qa, e2e, stage) and multiply that manual effort that many times. Doesn’t seem like good use of productive hours. So another perfect opportunity to automate. Team found a node package which deploys code on Google App Script servers and integrate as a npm script. Now team was free to develop on local machine, run tests and directly deploy to Google servers with simple npm commands in different environments. To make job even easier (why waste few seconds running npm, if we don’t have to) , we added a file watcher, which triggered deploy script as soon as any file in source folder was saved. Same npm script were used for deploying through jenkins. Unit testing — Since Google App Script loads all javascript functions in single public namespace (which is incompatible with node.js approach where a module is strictly single file and content is private by default), we needed a new way to write unit tests while using node framework. For that team came across gas-local which loads all functions in single context. This library also provided mocking ability. Automation — For automating our deployment process, we wanted to follow industry standard practices of running unit tests and only deploy to Google servers if they pass. Using npm scripts made it super easy. Before deployment kicks in, our npm script fires off unit test scripts first and push to Google servers if they pass. Error Logging — At Intuit, we are always ahead of errors. We want to be able to fix issues before customers report a problem. However, there was no direct way to get logs to Intuit servers. By default Google App script only provides last session’s logs. We could use HTTP endpoint of splunk and post logs using HTTP but would have kill app performance (remember in Google App script all HTTP calls are synchronous ) To solve this problem, initially we thought of storing logs in session storage and use a timer (which runs async) to read and post the logs to Intuit servers. However, even that option was not optimal, since there was a limit on number of characters on session storage and timer api was scheduled to be deprecated. Finally, we were able to establish a path from Google App script to Stackdriver (Google’s cloud logging platform) and from there to Google pub/sub servers and pull the logs to our splunk servers from Pub/Sub queues. It was a long path to get logs to our server, but it solved our problem of sending logs asynchronously to Intuit servers. Plus added bonus was we could aggregate logs from multiple prepod accounts into one pub/sub queue and establishing only one pipe between Intuit splunk servers and Google pub sub. Usage tracking — Not just errors, but we are fanatic about collecting aggregate metrics on usage of our app and tracking potential drop off points or steps where user get stuck. Again there was no direct way to send that data over from app script to our analytics servers. For this we followed the log pattern. Our app sends a specially formatted message to stackdriver which funnels it to a separate pub/sub queue. We then deployed a custom Google cloud function which listens on the queue and posts it to our analytics servers. This project threw in unique challanges where our tranditional enterprise CI/CD tools were not enough. Not only team needed to build the product but also build the tooling to build and deploy the product. In a very short time, team put together a bunch of tools to add missing functionality and tripled developer productivity. I hope that this blog post is helpful to any team starting development on Google App script. Would love to hear if you hit similar problems and how you solved them. If you are curious on how the app looks, QuickBooks Invoicing app is already live in Google app marketplace . Give it a spin. With help from: Jon Callahan Cool stuff we are doing and thinking about in engineering… 115 1 JavaScript Google App Script Invoice App Small Business Gmail 115 claps 115 1 Written by Tech Lead at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Tech Lead at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-13"},
{"website": "Intuit", "title": "doing data migrations without risk", "author": ["Merrin Kurian"], "link": "https://quickbooks-engineering.intuit.com/doing-data-migrations-without-risk-13565a4f7646", "abstract": "As part of enhancing security of QuickBooks Accounting Platform, we review our encryption policies and periodically refresh the encryption scheme so that our systems are compliant with the latest industry standards and ensure that the sensitive data our customers entrust with us are protected with the highest level of security. Doing such large scale changes for our 2.5+ million customers also means that there is data to be migrated. In this post, I outline the approach we followed to ensure that data migration was done without any downtime allowing room to rollback to a prior state without affecting customers in case an issue is detected. I will also describe the approaches we followed in terms of verification and monitoring in order to ensure zero data loss and zero data corruption. We rollout changes gradually to customers, so we can test the approach on low risk segments, improve our confidence, correct course if we need to and then rollout the changes to all. This ability is foundational to all our rollouts and has played a critical role in doing data migration as well. We had to ensure absolutely zero data loss and zero data corruption and entire dataset had to be migrated without leaving anyone behind and zero downtime . There were special cases to be handled to ensure that no data is left behind . For example, QuickBooks companies which were no longer active, sample companies, test companies; we had a plan to migrate every single company. We had to also ensure that every step can be rolled back so that we can restart the process in case we run into issues. We followed the common 4 phase approach for doing data migration but included specific steps of monitoring and verification which enabled us to ensure the highest level of service to our customers while the migration was in progress. Phase 0: Write to source, Read from source(default initial state) Phase 1: Dual writes to source and target, Read from source Phase 2: Dual writes to source and target, Read from target Phase 3: Write to target, Read from target(desired final state) Phase 4: Delete source First of all we needed to build the logic that would do reads and writes from/to both source and target and the support for toggling among the 4 configurations - one for each phase. In addition to these, we implemented a Migration job that systematically migrated existing data from source to target. Our data migrations are versioned . So if the same dataset has been migrated more than once, the versioning would indicate that. Suppose we detected a bug in version 1 of migration and it was fixed in version 2 after we had started the migration, we’d now need to migrate all datasets using version 2 of the migration job. We also wrote Verification jobs that compare data from source and target and report discrepancies. Quality of Verification jobs is key here as it is the primary lever that determines the confidence to proceed to the next phase. We also built dashboards to monitor progress and detect discrepancies from both these jobs. These dashboards played a critical role in providing us the confidence to proceed from one phase to the next or the necessary insights for us in case we needed to rollback to a previous phase due to an issue. Also we implemented checks and balances to prevent any accidental data corruption . Some of the examples include: Migration job should not run without turning on dual writes. Finally, we had automated unit tests to evaluate phase changes and validate expected behavior at each phase change with respect to reads and writes. We’ll see details of the phased approach of data migration in the following sections. Phase 0 Write to source, Read from source: This is the default initial phase for any data migration process. Phase 1 Dual Writes, Read From Source: This phase is rolled out to a few customers. The application does dual writes, but continues to read only from the source, so customers always see the latest version of the data. Data migration Then we turned on Migration job in this phase which does data migration from source to target for all existing data of the selected customer segment. Please note that dual writes will only capture the delta when the customers update the data, but we had to ensure that all of existing data needed to be migrated in a systematic manner which is achieved using this job. Verification We ran Verification jobs daily to compare source and target and ensure that there is no data loss / data corruption. This was our TDD approach to data migration. In this phase, until all data is migrated Verification jobs would log discrepancies as some data in target would not match data as in source or could be missing altogether, and dashboards will appear in Red. Only when all data is migrated, would the dashboards turn Green and indicate that we can proceed to the next phase. One important aspect is to ensure that Verification jobs do not have any bugs that may prevent it from reporting discrepancies. In this phase, due to the discrepancies reported by the Verification jobs, we learnt a lot about some of the legacy issues in the system and had to make adjustments to factor in those. Rollback support This phase can be rolled back. Let us say there were bugs in dual writes or in migration job. The application can be rolled back to the previous phase by toggling the configuration to original state(Phase 0: Write to source, Read from source), bugs fixed and rolled over again to this phase. Prerequisite to proceed to Phase 2: Ensure that Verification dashboards are green and all discrepancies if any are addressed through bug fixes and Migration job rerun on those datasets. Phase 2 Dual Writes, Read From Target: We proceed to this phase after all migration has been done and Verification job denotes no discrepancies. The application configuration now ensures that it continues to do dual writes, at the same time reads from the target data store. We continue to run Verification jobs and ensure that everything looks good. If the Verification dashboard shows any discrepancy, we have the ability to rollback the configuration to the previous phase. Rollback support Let us say, the application had a bug in Reading from Target data store, it can rollback to the previous phase(Phase 1) where data is Read from Source, fix the bug and roll forward to this phase. In case a data corruption was detected, rollback to Phase 0 and start again. Verification dashboard will appear in Red until all data is systematically migrated to target. Remember that we also have the ability to version our data migration. So our monitoring in this case would need to ensure that all data migration has happened to the latest version. Prerequisite to proceed to Phase 3: Ensure that Verification dashboards are green and all discrepancies if any are addressed through bug fixes and Migration job rerun on those datasets. Next phase is the target final state and cannot be rolled back. Phase 3 Write to Target, Read from Target: After Phase 2 has been turned on for a while and Verification dashboard remained Green for the whole time, we are confident that all data has been migrated and there is no data loss and no data corruption. Rollback support In this phase, we no longer use source data for reads or writes. So newer data is no longer present in source and this phase cannot be rolled back. While this confidence in proceeding to the next phase, what we should ensure before proceeding to this phase there is no need to rollback once we are in this phase. Phase 4 Delete from Source: Now that all phases of migration are successfully completed and verified with data, we can go ahead and delete the data from source, since it is no longer needed or used. Rather, source data is stale from Phase 3 as it no longer receives the latest writes. After we have successfully migrated a few customers and verified that everything worked as expected, it was just a repetition of the phased approach to another set of customers and so on until all customer segments were successfully migrated. We spent a lot of time in the initial stages reviewing our design with multiple stakeholders and discussed pros and cons of the various approaches before settling in on this path. All the tooling and planning we did upfront ensured that the data migration was a smooth predictable affair. I’d recommend anyone who undertakes data migration projects to plan and test as much as possible upfront so that the application state can be rolled back and forth without impacting customers or corrupting their data. Then we can indeed sleep well when large scale migration is in progress without having to worry about it knowing that we have practised enough. Good luck! What is not part of this writeup are the set of challenges outside the engineering work. We had to address concerns such as making these phased rollouts part of ongoing releases, working closely with other teams delivering new features on data being migrated, co-ordinating testing across the ecosystem of teams whose features depend on the data being migrated for each phase in pre-prod and planning for it much ahead of the prod rollout schedule, building an actual rollout schedule for the various segments of customers worldwide and proactively informing stakeholders(global engineering teams, customer care), working around freeze dates of important events such as peak revenue traffic, QBConnect conference... Engineering effort was only a fraction of the work needed to successfully pull this off and that is something I’d recommend anyone to watch out for :-) Cool stuff we are doing and thinking about in engineering… 40 Data Migration Distributed Systems Data Integration 40 claps 40 Written by Tinkers with software, data, camera, cuisine Cool stuff we are doing and thinking about in engineering at Intuit Written by Tinkers with software, data, camera, cuisine Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-26"},
{"website": "Intuit", "title": "5 things every developer needs to know to get started with accessibility", "author": ["Anil Suryanarayana"], "link": "https://quickbooks-engineering.intuit.com/5-things-every-developer-needs-to-know-to-get-started-with-accessibility-1c6dc8cb128c", "abstract": "5 Things Every Developer Needs to Know To Get Started with Accessibility Your guide to building products with Accessibility in mind! In this series of articles, I want to share ideas, technology trends and innovations in the field of web accessibility and how technology can empower people with disability. Here we go, 5 things every developer needs to know to get started with accessibility… Larger Total Addressable Market (TAM) — Building Accessible products increases audience and customer base. In simple terms, if more customers tend to be actively engaged and interested in a product, it is possible to sell more of that product. It costs less to build a product that’s accessible to start with than to make one compliant. Organizations that embed accessibility early in the product development cycle will significantly benefit as and when the product and customer base matures. Checkout Slack’s accessibility journey (3) as they grew exponentially in the past year or so to be THE platform for workplace communication. Inclusion spurs Innovation — Apart from legal and compliance reasons, building accessible product fosters a culture of inclusion within organizations. Developers with accessibility expertise can build products that improve usability for everyone, not just for people with disability. Intuit Accessibility team is doing outstanding work in this space with its innovative ways to make Turbo Tax and QuickBooks accessible to people with disability ( that’s changing someone’s financial life ). Accessibility initiatives have fostered ‘customer-experience-focused’ environment within Intuit where engineers are innovating and creatively solving for end users. Learn more about Intuit Accessibility here . As George Zamfir, Slack’s Accessibility Product Manager, puts it — “Accessibility touches every facet of a product”. It infers accessibility has to be built deep into the product to allow assistive technology to interpret web pages (voice over, screen readers and many more). To do so, developers need to be proficient with accessibility guidelines defined by WCAG 2.0 & WAI-ARIA You could read all about how to meet WCAG requirements using this quick reference guide — WCAG 2.0 Quick Ref For those who need a refresh, here’s a quick glance at WCAG 2.0 principles(4): P erceivable — Provide text alternatives for non-text content; Provide captions and other alternatives for multimedia; Create content that can be presented in different ways , including by assistive technologies, without losing meaning; Make it easier for users to see and hear content . O perable — Make all functionality available from a keyboard ; Give users enough time to read and use content; Do not use content that causes seizures ; Help users navigate and find content . U nderstandable — Make text readable and understandable ; Make content appear and operate in predictable ways; Help users avoid and correct mistakes . R obust — Maximize compatibility with current and future user tools. WAI-ARIA — the Accessible Rich Internet Applications Suite, is a framework to make Web content and Web applications more accessible to people with disabilities. It especially helps with dynamic content and advanced user interface controls developed with Ajax, HTML, JavaScript, and related technologies. More about AIRA(5) here Accessibility developers all around the world have open sourced some of the best tools and frameworks to bring awareness early in development process. Here’s a small set of tools that’ll come in handy during development: HTML Code Sniffer is an excellent chrome extension that comes with accessibility standards to enforce three conformance levels of the Web Content Accessibility Guidelines (WCAG) 2.0 , and the web-related components of the U.S.”Section 508\" legislation . Get the Accessibility Auditor Bookmarklet here (it has identified close to 50million accessibility issues!) : HTML Code Sniffer Google has a suite of tools to develop accessible products and apps: For Android applications, ‘ Accessibility test framework ’ provides libraries for automated accessibility checks — For a given view, libraries run all accessibility checks on all views in the hierarchy rooted at that view and throws exception if errors are found. Accessibility Test Framework for Android Lighthouse for Chrome — You give Lighthouse a URL to audit, it runs a series of accessibility audits against the page, and generates a report on how well the page performed. Lighthouse for Chrome WAVE — Web Accessibility Evaluation tool with chrome and firefox extension is developed by WebAIM.org. It provides visual feedback about the accessibility of your web content by injecting icons and indicators into your page. Web Accessibility Evaluation tool Lastly, for those looking for web accessibility best practices, Manuel Matuzovic ’s has excellent tips for writing HTML and CSS with Accessibility in mind. Read all about it here: Writing HTML with Accessibility in mind Writing CSS with Accessibility in mind Continuous integration and delivery is key to faster product cycles and embedding accessibility compliance into “build-test-deploy” pipeline could provide the right level of gating for code pushed to end users. Here are few things to consider for automated accessibility auditing: For ReactJS users, react-a11y is an excellent runtime analysis tool to identify accessibility issues in react.js elements: react-a11y For progressive web applications, lighthouse could be also used as a node module to run accessibility audits. With the right usage, lighthouse could be configured to enforce test level assertions for accessibility compliance checks: Using Lighthouse Programmatically Eslint-plugin-jsx-a11y — For teams that are already using linting in their projects, accessibility standards could be baked into the application in real-time with this plugin: Eslint-plugin-jsx-a11y Axe-Webpack-Plugin — Axe has published webpack plugin to be integrated into development server. Each time webpack finishes compilation, the aXe accessibility ruleset will be run against the app Acop — For Java and ruby users on selenium webdriver, Intuit engineers have open sourced accessibility jar and ruby gem to enforce webpage accessibility compliance during automated deployment cycles. Web Accessibility community is a vibrant cohort of like minded people with diverse background and skills who come together to build better products. There’s plethora of opportunities to network, share ideas and learn to create products that are accessible to and inclusive of users with different disabilities. Global events such as Global Accessibility Awareness Day and World Usability Day have events lined up all around the world to get everyone talking, thinking and learning about digital (web, software, mobile, etc.) access/inclusion and people with different disabilities. You could follow these events at # gbla11yday and #WorldUsabilityDay A11y meetups have accessibility centric events in several geographical locations so look for an event and join the community. CSUN Assistive Technology Conference is coming up in March 2018 and its the single largest assistive technology conference in the world with more than 5000 attendees. You could follow all the events at # csunatc18 If you came this far then then thank you for reading! It’s obvious there’s more than 5 things that you need to know about Accessibility. Hopefully this article will motivate you to learn about accessibility, develop empathy and contribute to the accessibility community. (1) https://drmarkcamilleri.com/2014/11/06/re-conceiving-corporate-sustainability-and-responsibility-for-education (2) https://w3reign.com/what-are-web-content-accessibility-guidelines-wcag/ (3) / https://slackhq.com/designing-slack-for-everyone-456002920bf2 (4) https://www.w3.org/WAI/WCAG20/quickref/ (5) https://www.w3.org/WAI/intro/aria (6) http://squizlabs.github.io/HTML_CodeSniffer (7) / https://webaim.org / (8) https://developers.google.com/web/tools/lighthouse/ Cool stuff we are doing and thinking about in engineering… 26 Accessibility React Web Development Developer Automation 26 claps 26 Written by Staff Engineer @ Intuit’s Small Business and Self Employed Group / Accessibility Enthusiast / Tools and Automation freak Cool stuff we are doing and thinking about in engineering at Intuit Written by Staff Engineer @ Intuit’s Small Business and Self Employed Group / Accessibility Enthusiast / Tools and Automation freak Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-05"},
{"website": "Intuit", "title": "intuit gets groovy", "author": ["Santos Solorzano"], "link": "https://quickbooks-engineering.intuit.com/intuit-gets-groovy-4c1f85f9ab4a", "abstract": "Most of the code written at Intuit is in Java. During my internship last summer in Mountain View, I wrote a module to export data from Splunk to Amazon S3 using Java SDKs. Although we currently have projects written in Java, the engineers in Reno do things a little different. My team works on and enhances Online Payroll for small businesses. We are responsible for maintaining and developing critical back office systems that process millions of transactions a day. To do this, we have taken advantage of Grails, a framework used to build web applications with the Apache Groovy programming language. Groovy is a dynamic object-oriented programming language for the Java platform. The aim of this post is to cover some of Groovy’s powerful features and my experience with the language so far. Concise, readable and expressive syntax, easy to learn for Java developers — The Apache Groovy programming language One of the goals of Groovy is to improve developer productivity. For example, take a look at the following code: In Groovy, there exists the Ternary operator (which also exists in Java ) that allows us to shorten this statement into: Coming from a Java background, I was able to get acquainted with the codebase because of the simple syntax and with some help from IntelliJ. I was writing production code and integration tests in Groovy by the end of my first week. Getting acquainted with the Payroll domain, terminology, etc. was a whole other ball game though. Great for writing concise and maintainable tests, and for all your build and automation tasks — The Apache Groovy programming language My team has taken advantage of Groovy’s extensive capabilities such as scripting. In the next section I will talk about how we used JSch to support Secure File Transfer Protocol (SFTP). For now, let’s see how we would include JSch in our build specification using Maven and Gradle/Grails: As you can imagine, the more dependancies you add to your Maven project, the more complex your XML file becomes. I still have nightmares about my internship and the XML file I worked with. The Groovy file containing our Grails project dependancies are much shorter and clearer ! Unrelated to scripting, we also make use of Groovy closures and Spock , a testing and specification framework for Java and Groovy applications. Seamlessly and transparently integrates and interoperates with Java and any third-party libraries — The Apache Groovy programming language One of our services handles sending reports that includes monthly customer activity. The only delivery method in production right now is via email. We recently began work to support SFTP so we can connect to and upload files to a remote host. There is no native support for SFTP in Java or Groovy, so I did some research on any libraries we could use. I landed on two options: JSch and sshj . Both libraries are Java implementations of SSH2. For security reasons, Intuit applications cannot download dependencies from the Internet. There is a repository that Intuit applications can download approved dependencies from. JSch was the only permitted SFTP library at the time so we went with that. After scouring the Internet for more information on JSch, I stumbled upon Sandra Parsick ’s JSchClient and took the bits and pieces that I needed. We instantiate a session to the SSH server we’d like to connect to and open an SFTP channel to upload files. In this example, I was able to upload a file from my desktop to another location on my computer. Let’s dive into the code! Note: I have created a gist of the entire program on GitHub for reference. We can establish an SSH connection without providing a password using public key authentication. We check for the public key data of known server hosts and instantiate the Session object with a username, host and port. We can establish a TCP connection to our host by calling session.connect() : We then open a new channel over this connection by calling session.openChannel(“sftp”) . The string sftp identifies the channel type to open. Other available types include shell , exec , direct-tcpip , and subsystem : After uploading our file, we close the channel and our connection to the SSH server: We can run the program with the following command: Had this been a Java program, we would run the program like so: Java is predominantly used throughout Intuit from the Mountain View office to the India Development Centre. Groovy has found a place at Intuit in scripts that request bank account information and services that run Payroll for small businesses. I hope you enjoyed this post and got a taste of the Groovy programming language! You can get more information on the major differences between Java and Groovy here . Check out this post on Stack Overflow if you get a similar error message when running the program: You may also encounter the following error message when running the program: You may have to create an authorized_keys file if it doesn’t exist in the ~/.ssh/ directory on the remote host. After creating it, make this file non-readable to other users/groups and add your public key to it. This can be done with the following commands: P.S. Semicolons are optional in Groovy 👍 Cool stuff we are doing and thinking about in engineering… 5 5 claps 5 Written by Software Engineer at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-18"},
{"website": "Intuit", "title": "graphql on the edge", "author": ["Mike Fix"], "link": "https://quickbooks-engineering.intuit.com/graphql-on-the-edge-12b6d60064b0", "abstract": "Note: this article is an adaptation of my slides for my talk at GraphQL Summit 2017 . You can also checkout the video here . I know this article is titled “ GraphQL on the Edge ,” but don’t worry, I am not talking about cutting-edge GraphQL. Rather, I am referring to how placing GraphQL at the very edge of your application provides a protective shield around your app. GraphQL protects the data coming in and out of your app, no matter where the data is coming from. As you will see later, this common interface also makes testing your APIs super easy while providing a unified method to access your data. No matter where the users’ data is coming from, GraphQL can normalize common data types, sanitize against bad inputs, and verify those values meet certain criteria. For example, if there are phone numbers entering your app from a bunch of different places and you want to use these phone numbers as a database key, you are going to need a common format. GraphQL can normalize every number to a single format across your entire app very easily with a custom scalar type . You can also use these custom scalars to sanitize inputs, say, to protect against XSS attacks, or maybe validate that a number is less than 100 or that an integer is odd. We as a community should be tackling these data problems with custom scalar type, and reusing them not only across our apps, but among many distinct applications. The examples I discussed are simple, but you can imagine many, many more possibilities: I know that testing isn’t always the most fun or rewarding part of creating software, but it is more than critical. My favorite part about GraphQL is that it makes testing a cinch. When we test our APIs, we are not testing the database or the code, we are testing how someone is going to use our application; testing against the intended behavior. As our friend Dan Abramov (from React’s core team) says: and this statement holds true for more than just libraries. It is crucial for public APIs as well. The good news is GraphQL makes this task simple and declarative. GraphQL has its benefits at every stage of testing: unit, integration, deployment, regression . . . you name it. For a past project, my team even wrote unit tests against our public API with no mocks and no stateful data , as you can see in this example: This test simply builds a “random” user, and makes sure that querying for \"me\" returns that user. It also uses chai-graphq l which my friend Steve wrote. You should check it out 👍. For those of you using using continuous deployment pipelines, you can use these same queries in a “cURL-like” command to test each deployment, even if just for a health-check. Which brings me to one of the biggest testing advantages that GraphQL brings: reusability . Simply store your common queries (bonus points if they are the same ones your client uses) in a .graphql file, import them as strings in your unit tests, and reuse them for integration, deployment, and regression testing. At Intuit, we enhanced our very own Karate framework to declaratively test our GraphQL queries. We reuse static queries to test a ton of different use cases. This lets us reuse a lot of “code,” and simply make our tests a function of data. Whenever we need to test new capabilities, we just have to add the related fields to our queries, and add sample data to our payloads. We even randomly generate a lot of this data to fuzz-test our APIs. Type safety, universal data access, data-driven testing — these are just a few of the amazing benefits GraphQL will give you. If you want to chat about GraphQL, please email me , tweet me , or hit me up on the Apollo GraphQL Slack (username @fix )! You might be asking, how did you create these awesome looking code screenshots? The answer is Carbon , a little web-app a couple of friends and I made. You should check it out 🙃 Cool stuff we are doing and thinking about in engineering… 34 Thanks to Emily Hansel . GraphQL Programming Testing 34 claps 34 Written by “Life is either an incredible adventure . . . or it is nothing at all.” Cool stuff we are doing and thinking about in engineering at Intuit Written by “Life is either an incredible adventure . . . or it is nothing at all.” Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-20"},
{"website": "Intuit", "title": "service decomposition at scale", "author": ["Jonah Back"], "link": "https://quickbooks-engineering.intuit.com/service-decomposition-at-scale-70405ac2f637", "abstract": "The problem : QuickBooks Self-Employed has seen some massive growth over the last two years. We’ve gone from 1 subscriber in mid-2014 to over 390,000 subscribers today. With this user growth, we’ve seen some of MySQL tables grow really quick as well. In this particular case, the tables related to our mileage tracking feature have grown to over 100 million records. As we saw during our peak season last year, this led to some fairly large I/O growth on our MySQL servers. After peak season, we decided that it was time to have the mileage tracking data say goodbye to MySQL servers 👋. First steps : So, we were ditching MySQL for a few tables— but what would replace it? We had a few important things we wanted out of our new database: It needed to be scale to handle our future needs — we needed a database solution that was more suited to the transactional data we were dealing with. It needed to be resilient — if possible, we wanted to avoid having a single point of failure in our stack. Initially, we need to be able to tolerate an AWS AZ outage, with the ability to change that to regional outage tolerance later on. After some investigations into various databases, we decided to use the DataStax Enterprise (DSE) platform. DSE’s combination of Apache Cassandra for data storage and Apache Solr for searching/querying. The implementation: Once we decided on Cassandra, we got up and running in no time. By using our internal AWS “Blaster appliance”(an automated tool for setting up a DSE Cassandra Ring), along with some open source Spring Boot components and some internal SDKs, we were able to standup our new service pretty quickly. We were able to create new trips without issue. Great! Ready to ship! …… except for those 100 million records that still need to be migrated 😧 Data migration is never easy. Throw in some legacy data bugs, various timezone issues, and a change from long-based IDs to UUIDs — and it’s suddenly a lot harder than you anticipated. Since our data migration consisted of incremental CSV dumps from our MySQL database into our Cassandra ring, we didn’t have an easy way to validate that all of our data was correct in both places. To make sure we migrated all of our data with 100% accuracy, we built a data verifier into our product to ensure that the migration (and all of the code that maps from our new API to the old one) was correct. This verifier basically took the API objects for trips from the MySQL stack and compared them to the API objects that were mapped from the new Cassandra stack. The verifier quickly turned out to be worth the time invested — we caught at least 5 or 6 critical migration bugs in our first few runs. It’s given us the confidence that we need to say that the data was migrated successfully onto the new stack. Besides data migration, the other main challenge we ran into was certain workflows where we had assumed (correctly, until this decomposition) that certain service calls were quick. We found a few cases where we would loop through a list of trips and save each trip one by one, which was (relatively) performant on a co-hosted MySQL server, but blew up when each save had a network hop associated with it. In one case, we had an API that normally took on the order of ~100ms grow to 2 minutes . To fix these painful experiences, we changed all of these workflows to do a batch save, using 1 service call for the entire collection of trips. Results and lessons learned We’re still working through the last steps of the migration, but we’re serving live production traffic now 🚀! Looking back at the process, there are a few things we could have done better / earlier: Have a plan for data migration up front. When we first set out to design our new service, data migration was something of an afterthought. As we got into the details, it became pretty quickly apparent that most of our time was going to spent there. Looking forward, we’re going make sure that we build + plan for data migration upfront. Validate the database setup with experts early and often. As we got closer to shipping our service, some consultation with experts from DataStax found some issues that caused us to do full rebuilds on our environments. While this is pretty much automated, it caused us to run our lengthy migration jobs quite a few times. Build data-integrity checks into the product to validate data migration. Data migration is hard, and there will be issues. Cool stuff we are doing and thinking about in engineering… 179 Thanks to Shrisha Radhakrishna and Tim . Programming Cassandra Solr Microservices Intuit 179 claps 179 Written by Staff Software Engineer @Intuit . Founder @Revolution_UC. Cool stuff we are doing and thinking about in engineering at Intuit Written by Staff Software Engineer @Intuit . Founder @Revolution_UC. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-10"},
{"website": "Intuit", "title": "quickbooks a platform transformation part 2", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/quickbooks-a-platform-transformation-part-2-926580a565f0", "abstract": "In Part1 we looked at how we are moving away from a monolithic implementation, into a distributed, decentralized and open Platform that is powering multiple Products. It is important to think of an API Platform as a Product itself that solves not only for supporting the existing customer base and products (representing linear growth) but that also inspires and enables innovation to serve new customers through yet undelivered solutions (representing non-linear growth). In Part 2 we will look at the ‘What’ — the functional capabilities — are we are leveraging to drive a successful transition to an open Platform. Before we proceed, it’s important to familiarize yourself with some background on Microservices and Platforms. Some introductory materials that are useful for Microservices is Martin Fowler seminal work , Chris Richardson’s 7 part series , Vivek Juneja’s architectural strategy and “ Building Microservices ” by Sam Newman. A detailed overview on Platforms can be found at Platform revolution and Platform Manifesto . Approach There are multiple approaches to breaking a Monolith into Microservices. The goal is to take a tightly coupled application and break it down into smaller services, each running in its own process and communicating with lightweight mechanisms. The big idea is to architect a large, complex and long-lived system, as a set of cohesive services that evolve over time. These services are built around business capabilities and deployed independently. We embarked on this journey by re-architecting our monolith into four layers User Interface or Presentation Layer - responsible for interfacing with the user. All our presentation tier is built on React. We have spent time in decomposing our experiences into Jobs powered by widgets. Application Layer - orchestrates application activity and holds state of the application task progress. We have been re-architecting our application layer to SPA (Node/Express). The data exchange is through Relay and GraphQL Domain Layer - contains the key business logic. We have been re-architecting our domains into Mircoservices using Java/Springboot. Infrastructure Layer - our infrastructure is actively being migrated to AWS. We are sequencing the decomposition as follows. Repackaging the Monolith — by applying the service per container pattern . Migrating our infrastructure to AWS has been internally a critical priority and hence we started our decomposition journey by treating the monolith as a single service and moving it to a container based deployment. Shimming — next we are splitting the presentation layer (React.Js) from the application and domain layers. The application and domain layers are fronted by a new GraphQL API endpoint and legacy REST-based APIs. This work is completed when all interactions between the presentation and application/domain layers are through these APIs. Refactoring — next we are reorganizing the monolith code into logical packages and modules that are aligned with their future Microservice break-outs — e.g. Company, Billing, Taxes and Reporting. In order to cleanly break the dependencies between modules, each module has a well defined API interface and all communication between modules is routed through their APIs. It is helping us practice weekly releases in AWS. Decomposition — finally we are moving towards extracting modules and creating isolated, standalone Microservices — each with their own data store. Supporting decomposition will be initial investment in cross-cutting functionality (i.e. Configuration, Logging, etc) to remove boilerplate code and improve engineering velocity; see — see Microservice Chassis framework . Domain Driven Design One of the first steps we leveraged in our decomposition is a domain design . This involved mapping the entire set of QuickBooks ecosystem capabilities into 14 distinct domains. Each domain has a defined scope and associated boundaries. A Bounded Context provides the logical frame inside of which the model evolves. We have also used the context as the organizing principle for our scrum teams; each context is owned by an independent scrum team. Scalability We have been investing in scaling using the scale cube techniques. In this model, X-axis service scaling is achieved by running multiple identical copies of the application behind a load balancer. Y-axis scaling is achieved by decomposing a monolithic application into a set of Microservices. Finally, the Z-Axis scaling is about data partitioning using database sharding. We have swimlaned QuickBooks with application clusters corresponding to global regions - this enables not only scalability but is foundational to supporting Data Sovereignty. Having looked at the approach, let’s now try and understand how we are building our Platform. API Gateway As we started breaking our monolith into several Micorservices each exposing a set of fine grained endpoints it became obvious that it would become extremely complex for clients to deal with each of these services separately. Additionally, each service in turn needed to deal with common concerns of authentication, authorization, routing, load balancing, transport, serialization data formats and protocol bindings (REST, SOAP, etc.). Consequently, we have invested in an API Gateway that acts as a façade to present a single entry point to external clients and to provide functional implementations. The gateway provides a single endpoint which has the intelligence of routing to the appropriate Microservice. It encapsulates orchestration complexity from the clients. Later in this article, we will understand why this can be so powerful in unifying different protocols and channel interactions (web, mobile). A few examples of API gateway are Netflix , Amazon AWS and Ngnix . Service Discovery Clients need to know service endpoints and should be able to easily discover them. The two common patterns used are client side and server side discovery. In client side discovery, a client queries a service registry which in turn maintains the location of service instances and load balances requests across them. Netflix’s Ribbon is a great example of client side discovery pattern. On the other hand, AWS ELB is a good example of server side discovery pattern. We have invested in a Service registry (similar to Netflix Eureka ) that acts as a repository to hold the mapping between network locations and service provider endpoints. Rest vs Graphs REST has been a standard protocol for developing APIs for over a decade. Its beauty has been its simplicity especially in how it works over basic http. However, it also has certain limitations. In REST the server determines what data is sent down to the client and the interaction is resource based . The client does not have complete control over what aspects it will get. However, with the advent of modern application presentation frameworks, single-page applications and clients, that implement complex logic are the new reality. Technologies like GraphQL inverts the model where the client determines what fields and relationships it wants to get back, and the service is responsible for constructing a response tailored for this particular request. Interaction is experience based . The other difference is that with the traditional REST you write imperative code while in case of GraphQL it is primarily declarative code and hence decouples the client and server. Last but not the least, by dealing in terms of graphs as opposed to linear entities, you reduce chattiness , especially important in mobile. For the built out of QuickBooks experiences, we are migrating to GraphQL entities. REST will be the standard for 3rd parties. Administration We are investing in Administration to programmatically onboard to our internal service portal, service registry and monitoring tools. Versioning The API schema needs to support a versioning scheme where each property can be associated with metadata to express version information on when it was introduced & deprecated . The dates can be either be associated with an entity or with any of the attributes contained within an entity. In the e.g. above, Customer could have a version identifier (Entity versioning) or at a property (e.g. dateofBirth). Lack of any versioning identifier means entity & its properties are available in all versions. While there are several techniques to do versioning — version ids, dates, etc we have invested in a scheme that allows a client to bind to a specific version number to ensure that there is backward compatibility between distinct versions. We are also looking at Reactive Microservices. As we decompose our monolith into a set of isolated services, each one with a single area of responsibility, data access becomes much more complex. That is because the data owned by each Microservice is private to it and can only be accessed via its API — each Microservice takes sole responsibility for their own state and persistence. Modeling each service as a Bounded Context is useful since each service usually defines its own domain. This gives each service the freedom to represent its state in any way it wants, and store it in the format and medium that is most suitable — Polyglot Persistence . Some services might choose a traditional Relational Database Management System (RDBMS) (like Oracle, MySQL and Postgres) and some a NoSQL database (for example Cassandra, Mongo DB). State changes create Event to be stored in the Event Log using Event Sourcing . Event sourcing achieves atomicity by using a event- centric approach. Rather than store the current state of an entity, the application stores a sequence of state-changing events and reconstructs an entity’s current state by replaying the events. Event sourcing also has some drawbacks. The event store only directly supports the lookup of business entities by primary key, while you need to implement command query responsibility separation ( CQRS ) to implement queries. Communication between Microservices needs to be based on asynchronous message passing using asynchronous and non-blocking execution and IO. We have been looking at Reactive Streams specification to enable asynchronous stream processing- and leverage Akka and RxJava . To conclude, in Part 2 we looked at what functional capabilities are powering our open platform. In Part 3 we will look at how schema driven API production & consumption helps us unify different interaction models between internal and external developers. Cool stuff we are doing and thinking about in engineering… 10 Microservices 10 claps 10 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-15"},
{"website": "Intuit", "title": "quickbooks a platform transformation part 3", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/quickbooks-a-platform-transformation-part-3-9bea35bc2a56", "abstract": "In Part 2 , we looked the key capabilities required to build a Platform. In Part 3 we will look at how we are building a unified set of APIs that are available for 1st (Internal),2nd (Strategic Partners) and 3rd Party (External) developers. To solve seamlessly for internal developers who generally are building experiences for mobile first products — and need to optimize chattiness — and for external developers who are familiar with REST (.NET, Java, Python, etc.) we are taking a schema driven approach, supporting multiple interaction models — once we author the specification, the provider, consumer and administrative aspects are programmatically generated. API Schema Schemas define the structure of data in the resource model. Services expose a public interface to consumers as described by a resource model. A resource model includes a URL path, e.g. /Customer, a set of interactions ( read , write, react and master ), and a set of properties, e.g. last name. The read and write interactions translate to the HTTP GET and POST methods. The react interaction indicates that the service is interested in changes to the specified resource. The master interaction indicates that the service has authority over modifications to the specified resource. Internally, the services persist data using a data model. A data model includes entities, their attributes, and their relationships with other entities. We are leveraging Open API Specification — YAML with JSON specification — to author a resource model. It includes a strong type system and a good way to model entities, properties and constraints. It captures metadata for entities, relationships to other entities, service routing, properties. Properties in turn has a type, name, version and scope (public, private, etc.) and constraints. Finally, rich syntactic constraints are modeled using JSON Schema Validation Specification . . Providers — On the provider side, we generate basic service interfaces in stubs with default implementation — this uses the container and libraries of choice from Springboot — or any of the JAX RS 2.0 specification frameworks like Apache CXF, Jersey, Resteasy, etc. The service developer then simply implements the domain specific logic. The Mock serve r porotypes the API and starts exposing it to clients for both feedback and consumption. The service tests are generated through Karate . Finally, we are investing in building a scaffolding for services ( a hello QBO service ) that will enable developers to be up and running in <15 mins, and continuously deploy code in AWS using Kubernetes. Developers focus on their “app logic” while the “ hello QBO ” service comes pre-built with all common code and dependencies needed to be deployed in AWS with complete CI/CD. Consumers — Consumption happens through code generation of SDK, API Docs and Samples. We have built a meta SDK code generator that does lexical analysis of the schema to build an abstract syntax tree (AST) to dynamically generate SDKs for each language — Java, .NET and PHP. The SDK abstracts the details of creating a context, calling the API Gateway and exposing the response back to the application for consumption. We have been leveraging Variability engineering as a standard way on how we are architecting our products and that is helping us penetrate new markets quickly. Variability is the ability of a APIs to be efficiently extended, changed, customized, or configured for use in a context. This usually consists of identification and analysis of common and varying parts in a domain model. Customization is a mechanism where a variant feature is implemented via code and configuration is a variation mechanism where a variant feature is implemented through data changes. We are spending time in modeling API variability so that we can reuse strategies based on context and behavior. Variability of the schema is injected at runtime based of Country or geographic region — e.g. what information is required to onboard a new Employee for Payroll varies in each of the 50 US states. By modeling this as configuration we can dynamically drive the end to end experience. Type of the customer — we can provide differentiated treatment to our high value customers. To unify the needs for both internal and external developers there are some key considerations which we are using to architect our APIs Ability to deal with linear entities and nested graphs. Ability to deal with interactive experiences. Reduce chattiness in mobile experiences through batch interaction requests. Ability to support standard REST protocols familiar to 3rd party developers and GraphQL for internal developers. Generalization of Batch, GraphQL and REST results in a Batch interaction model between the API Gateway and the Domain Services. The Gateway has intelligence to route the graph request to different entities. In the example above Customer and Reporting may get routed to two different domain services. To conclude , our schema driven approach helps us programmatically generate API stubs, mock tests , service tests and SDKs . Our novel approach unifies the needs for both cross device consumption in building modern experiences while expose a coherent set of APIs to external developers. In Part 4 we will look at what are key operational measures for success. Cool stuff we are doing and thinking about in engineering… 7 API 7 claps 7 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-13"},
{"website": "Intuit", "title": "quickbooks a platform transformation", "author": ["Anil Madan"], "link": "https://quickbooks-engineering.intuit.com/quickbooks-a-platform-transformation-1bd98cc70aa2", "abstract": "In November, Intuit was recognized as #8 in Fortune’s inaugural ranking of “the future 50” article — a list of companies best positioned for breakout growth. As the article outlines, the key to our success has been self-disruption. We have been constantly reinventing ourselves before a competitor does. The foundation to this is our investment in an open Platform. A Platform that acts as the operating system for small businesses around the world and fuels their growth. A Platform that enables 10x more efficiency over the next 5 years to continually increase the velocity at which we bring value and innovation to our customers. A Platform that can support 10M small businesses across regions and industries, accountants with 100s of clients and 100s of integrated apps. A platform that has inherent triangular network effects connecting Small Business Owners, Accountants and Developers. This is the first part of a 5-part series which will focus on ‘Why’ we needed a cultural and mindset change to lead a Platform transformation. The 2nd and 3rd parts will provide an overview of ‘What’ capabilities — tools, technologies and frameworks that we are leveraging to successfully lead the change, while the 4th and 5th parts will cover the ‘How’ — execution strategies & approaches that we are finding useful in this journey. Background With over 15+ years of development, our codebase had become unsustainable due to deferred tech debt that directly impacted both our velocity and quality. The code had high friction/collision (12,000 cyclic references), little ownership (~10M+ log exceptions/day, 600+ out of SLA defects), and was complex to test (indeterminate functional test coverage, <5% code coverage). It would take us 30 days to make an experience change and 6 months to penetrate a new market. There were no durable APIs with little to no automation (>600 custom built internal APIs, <4000 functional tests, >50% of which failed regularly). This resulted in about ~150 defects escapes/week to production. Meanwhile we were seeing hyper growth in our business. Recognizing this was unsustainable, in the last year we have been on a journey to transform our Monolith into Microservices. This transformation has a few distinct facets: Plugins & Widgets — Modularizing our frontend stack through widgets and plugins which are used to build QuickBooks and third-party experiences. APIs & Services — Modularizing our backend through our next generation API Platform called V4 which exposes a coherent set of APIs realized by isolated and decomposed services. Variability — Build technology with Variability engineering across industry, region, business size, user roles and business entity, through configuration instead of code. Data — Creating a Data Platform that enables seamless data flow between QuickBooks and all other products in the ecosystem to power personalized experiences leveraging the advantages of machine learning. Productivity Tools — Automated Testing and code to deploy using CI/CD (prod and pre-prod) practices in the public cloud (AWS). Journey In the last year, we have made tremendous progress on decomposing the front-end. We invested in harmonizing and modularizing our experiences which enabled us to decompose the front-end into ~70 loosely coupled and isolated plugins that can be built, tested and deployed independently. As a result, we are seeing +10x velocity and releasing ~400 times per month instead of monthly monolithic releases. Teams are now autonomous and follow a DevOps model. Developer NPS has increased from -41 to +30 and developers can push code to production in <1 hr., and rollback < 15 mins in case of issues. Creating and integrating a new plugin requires less than 10 minutes and it is +2x more productive than before. We are now driving the same transformation for the back-end monolith by decomposing it into Microservices. Transformation The transformation is being driven by a strong cultural change. Platform vs Products There has been a cultural mindset shift to think in terms of building a Platform as opposed to Products . There is realization that Platforms are enablers that allow multiple products to be built by investing simultaneously in both business and technology frameworks. A Platform can not only enable Products built by the enterprise directly for their customers, but also unlocks new opportunities for contributors — both partners and external developers. The ecosystem flywheel is leveraged by external developers to market and sell their apps. Delivering a world class ecosystem required exposing the core business capabilities through a unified and coherent set of APIs (Application Programming Interface) — as opposed to slapping APIs in front of existing Products that simply expose product features and hence myopically limit themselves to a subset of domain/capabilities of the Product. Customer Benefit Secondly, everything we do is backed by a customer benefit. We have tied the evolution of the Platform to clear customer benefits and that has led to strong alignment across Product, Technology and Design teams. Organizational Design Last but not least, Organizational Design plays a big part in a successful transformation. Conway’s Law summarizes this well that ‘organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.’ For decomposition we have organized ourselves in small two pizza cross functional teams. Netflix’s lessons provide great insights into the design. Create high freedom, high responsibility culture with less process. By organizing around Product & Platform teams, we have established a culture of continuous delivery. Approach At the heart of a successful Platform is an API that exposes the capabilities through a coherent set of interfaces that allow consumers and contributors to generate value. These APIs are adhering to certain principles: Open — we have been building a unified set of APIs that are available for 1st (Internal),2nd (Strategic Partners) and 3rd Party (External) developers. Dogfooding — Dog-Fooding is helping us remove kinks before our partners and external developers consume our APIs. It also helps us build empathy for our external developers and the experience they go through when they use our APIs. Standards — We are building APIs in adherence to industry standards, e.g. OAI Initiative and OAuth2 (currently being rolled across our APIs). Developers are used to these standards and it is important to employ tools that interoperate seamlessly with these standards. Governance — we have decentralized governance with schema driven development; including a scalable approach to monitoring and auditing them ; APIs are evolved through automation of schema contracts and interfaces instead of bureaucratic committees (more in Part 2). Maturity Model — an internal maturity model that helps evolve the service decomposition. Basic — exposes a basic API with standards, fully tested. Self-served — complete developer experience, available on a services portal with complete documentation and samples. Performant — Complies with Service Level Objectives — response-time and availability. This should be table stakes before APIs are exposed to 2nd and 3rd parties. Isolated — fully isolated, with code and data encapsulated. Portable –portable units built around a container technology that can be independently deployed ideally in a cloud. 6. Non-Functional Requirements — Availability, Scalability, Performance, Maintainability, Portability and Quality to name a few (Part 4 will cover them in detail). These are implied expectations from the consumers. Success Metrics Decomposition largely involves defining the Platform as a set of capabilities which in turn are broken further into domains . A first step for us was to break down the entire QuickBooks ecosystem into 14 domains. The second step was to start measuring both the Production (Development) of the APIs & Services and the Consumption (Adoption) by internal developers and usage by partners & external developers. Success measures on Production that we are driving Having a durable contract of the platform with V4 APIs and comprehensive API test automation with >85% code and functional coverage. Ability for developers to independently build, test, and deploy in <45mins. Isolating failures and faults. Isolation will ensure parts fail or recover, instead of the whole system. We will be resilient and critical customer experiences will NEVER fail (even if systems fail). A test platform that will auto-generate tests to reduce the effort of writing tests by 70%, will make it easy to write tests by +10x, will test the product based on real customer usage, and will run millions of tests in parallel to certify changed code and push to production in <45 mins. Success measures on Consumption are % of customers driven through Domain APIs and Services Volume of traffic driven through Domain APIs and Services. To summarize, in Part 1 we looked at how we are moving away from a monolithic implementation into a distributed, decentralized, open Platform that is powering multiple Products. The Platform culture embraces agility and evolution as the enterprise transforms. The success in the journey requires an organizational design that promotes autonomy coupled with accountability. In Part 2 we will look at the functional capabilities that are required in the journey. Cool stuff we are doing and thinking about in engineering… 13 Microservices Intuit Intuit Quickbooks 13 claps 13 Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Written by Vice President, Data Platforms Walmart Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-14"},
{"website": "Intuit", "title": "road to continuous deployment", "author": ["Christopher Faria"], "link": "https://quickbooks-engineering.intuit.com/road-to-continuous-deployment-65bd03b985fc", "abstract": "The road to continuous deployment is not always easy, especially when it involves older systems. Here in the Global Payroll Platform at Intuit UK we began a journey towards continuous deployment. We started out as a monolith. Payroll is a complex domain and we were working with an aging beast of an application. The system was not always built with the best principles in mind and certains parts showed signs of age and wear. As is often the case with older systems, things were not optimized, processes were slow and things happened in a particular way just because ‘that’s the way we always did it’. One particular process was our release process. We were releasing each month and it took almost an entire day of engineering time to go through verifying the release, deploying (which would likely fail) and then ensuring each environment was correct. This was a heavily manual process: the verification was manually done, the deployment was manually launched and the deployment itself involved several manual steps. We couldn’t guarantee that each environment was the same. Sometimes something would go wrong and we would have to go back and fix customer data manually or rush to get a hotfix out. Oh and we were using Subversion. This state we will mark as the beginning of journey. Why did we start on this journey? The team here has a hunger to improve and we were not satisfied with the system that we had. It needed to be better. It was causing pain and we didn’t like pain. We wanted to increase our release cadence to ease our pain. Why do we want to release often? Well, when you release often it generally means releases are smaller. Following Kniberg’s train of thought, the more often we release, the more often we release. It’s a cycle that continuously gathers momentum, each release being easier than the next. We needed to identify our key pain points and solve them, easing ourselves into this cycle first rigidly, forcibly and then smoothly going through the motions as we gathered pace. if something is painful do it more often and you will find a way to reduce the pain This should apply to the team’s release process. This is why the cycle is great to be a part of and why it works. We went from monthly releases full of pain to daily releases that happened seamlessly whilst we worked on the things that matter. In order to make changes we needed to be confident in our code. It starts with our application code: we need to be confident that each change can go to production without breaking anything. Having this confidence allows us to deploy without manual testing or verification. At the point where we started our journey our test coverage was not great, around 50%. Better than some projects but nowhere near a level where we could be confident in what we were shipping. We would routinely find problems in production and have to scramble to fix it. We needed to be better. Improving test coverage and working our way towards a continuous integration process was a key step on the road to continuous delivery. It meant being rigorous about quality. There were mindset changes that had to occur. We had to drive for quality, and we took the principle, “quality is non-negotiable”. We built quality into everything that we do, not just striving for a high code coverage but also making sure we had the right tests in place across all levels. Our coverage was not based just on our application code, but also on the infrastructure code we eventually wrote in service of our automation goals. Everything is tested and ensured to be production ready, from the application to the tooling and infrastructure. At this moment we stand at 90% coverage in our monolith, a far cry from where we once were. We have confidence in our code and that the changes we make have a safety net to ensure that we are not breaking any of the existing functionality. To get to this we had to tackle pain points in our continuous integration process. We had suffered problems using Subversion as trunk was not always stable. (Actually, it was called trunk2, but that’s a different story.) To facilitate this we migrated from Subversion to Git and now each piece of code is tested in isolation before it can be merged. The migration eased our development process, so that we could adopt a branching and merging strategy that is natural to Git. This eliminates the chance of user error and ensures an always stable master branch (We call it master, not master2 thankfully.) The migration also helped to facilitate a new way of working for the team and we had achieved not only continuous integration, but continuous delivery . We run millions of tests every week as part of this rigorous quality assurance process. Every piece of code merged to master was guaranteed production ready and could be deployed. We just weren’t doing so. With our new-found confidence in our test coverage it was time to begin automating our release process. The reason to automate was that there was a need to eliminate human error in the process. With manual steps it’s hard to ensure accuracy, unlike with automated steps. We are able to validate that we executing in a specific way and replicate behaviour across environments. Automating each step of the release would allow us to actively spend less time on releases in the future whilst increasing the efficiency and success rate of our release process, a huge benefit. We identified manual steps that we were doing and needed to automate. For example, we had to execute SQL scripts as part of the release process to modify the database schema. We wanted to improve our release process by cutting people out of it. So we invested some time in looking at solutions for our problem and we settled on using Liquibase to evolve the database schema and data and execute them automatically as part of our release. This cut out a lengthy manual step of our process. Now some poor guy doesn’t have go through all environments and run a bunch of SQL scripts (which is extremely error prone, I might add). Now the engineer includes the script in a ‘version’ folder and it’s executed during the release. Great! The change itself wasn’t that huge but the impact was far-reaching. We were no longer manually executing scripts. Our testing mindset meant we wanted everything that we committed and merged to master to be tested, so we also wanted to test these scripts. We included in our build process an execution of the liquibase changesets. This ensured that before any script could be merged into master that it was not going to break the schema. It was time for us to embark on the next road, one that required a huge mindset change. The next problem that we had set out to solve was that in doing a release it meant that the server would be down for the duration of the deployment, which was around 30 minutes. This caused releases to be a pain as they have to be planned around being convenient for our customers. It usually meant at night outside of work hours, and someone would have to stay around to support it. Moving to zero downtime releases would lead us to have more control over when we release. It would give us the freedom to release at any time and know that we would not be breaking anyone. Implementing zero downtimes releases was not an easy feat. It required a huge change in mindset from the team. We had to get into the habit of thinking about how every change we made could interact with an environment that could simultaneously be working with two versions of the code. We follow the approach of Blue-Green deployments . This means that at any point we could have two different versions of the application, the current version and the previous version, running at the same time. Our commitment is to be database compatible with the previous version of the code always. We can’t break every user whilst the new instance is spinning up because we removed a field that we were previously using. It means re-thinking how schema changes work. A change that previously involved just a tweak in the code and a quick script to update the schema now implies several releases and a timed migration. Getting backwards compatibility as a mindset takes some time. There will be mistakes. So it’s important that we can catch them before they affect a user. We definitely don’t want that to happen. As part of our CI we run the previous and current version of the code with the same database. A series of tests is executed against both instances and, voila! We ensure that when we release to production we won’t break the application for the users that are currently using it. Backwards compatibility also applies to code changes. We had to explore the topic of feature flags, or feature toggles . We needed to be able to ship code that wasn’t feature complete and know that we would not break customers. This was another conscious change the team had to make. We had to think about code changes in a backwards compatible way and evaluate where we needed to wrap the changes behind a feature toggle. The result is that anything that is not complete or ready to be consumed is wrapped with a feature toggle so we have peace of mind that all code in the master branch is safe to go to production. This enabled us to start releasing whenever we wanted, and it meant we had the building blocks in place to be able to release more often and ultimately automate our releases. The best way to achieve anything, is to break it down into smaller milestones, sometimes referred to as Divide and Conquer . Our first target was to get to biweekly releases. We had to think about our current process. What could we improve? What were we doing horribly wrong? We had made some significant changes so far in the process. We had nailed backwards compatibility at the database level, automated our script execution and improved our development process. Master is always stable and sealed with the approval of a barrage of CI tests. This first goal of getting from monthly releases to biweekly releases was our first small step towards continuous deployment. We achieved this along our journey of achieving zero downtime releases, but actually we got through to weekly releases. It had become much much easier to release. The team was identifying pain points and automating where possible. We had entered into the release cycle, where each release makes the next easier. Frequent releases made backwards compatibility changes easier because a chain of several releases becomes several weeks as opposed to several months. The next target was daily releases. It took further changes in the team mindset and our release and development process to get there. First we looked at our release process which at this point was taking around 5–6 hours. It is not great but it’s better than an entire day, for sure. We had a release sheriff who was appointed every week (they even had a hat, badge and gun). This individual was responsible for creating a release task in JIRA; Identifying the issues included in the release; hitting a button to promote the release candidate; waiting a couple of hours; and then hitting a separate button to release the candidate out. Already it’s obvious some things that we can automate here. Why does the guy have to hit two buttons? He doesn’t, so we made it one button: promote and release. Next we created a CI bot so that when a PR is merged to master in git it gets tagged with the current release version in JIRA. Great, another manual step gone! This CI bot also does some other awesome things, but that’s a topic for another blog post. At this point all our sheriff has to do is create the release task in JIRA and then click a button. The process was around 4 hours. A minor improvement but still an improvement. We began looking at the actual Jenkins job that made up the automated release process. There had to be a way to cut down the time from button click to production deployment. Our test execution takes around 45m, the deploy process itself only around 20. There had to be some extra fat we could trim. We identified that we were running tests as part of the master build, taking the candidate and then running the same tests against the candidate to promote it before deploying it to our pre-production instance. After this we would deploy our staging environment, run regression tests against a copy of the production database and finally deploy our production environment. It’s immediately obvious that there’s some fat there. We run the tests when building the candidate, then we run them again before promoting the candidate. It was unnecessary. So the first thing to do was to just take the candidate that already passed the master build and promote it directly without running any further tests. That’s 45m minutes saved already. Next question is why are we deploying e2e and then kicking off the production release afterwards? This could definitely be done in parallel because we could guarantee that the code was shippable. With some fat trimming we managed to get the button click down to around 1.5 hours. The new process was to take the last green master build, promote it generating a tag in git, deploy it to stage and e2e simultaneously, launch the regression tests against stage and then deploy production. We’re keeping the same quality assurance but saving a lot of time. As part of these improvements we also automated tagging of versions in JIRA, marking versions as released automatically and sending release notes out to stakeholders automatically once release is finished. It eventually got to the point where clicking the button for a release was cumbersome. So much effort spending those five seconds to open a Jenkins job and build it as opposed to the hours we spent before with manual work. It is amazing how things change. So we now have our release train scheduled to go at 3 every day. No involvement by individuals whatsoever. Releases are just something that happens and thinking in a backwards compatible way is just part of the team’s DNA. We still sometimes click the button for ad-hoc releases when needed. We recently released UK Payroll to general availability here in the UK and in the build-up to the our delivery date we were releasing around 3 or 4 times a day — a huge step forward from where we were. Our biggest gain from everything that we have achieved so far on this journey is confidence. We believe in our tools, people and processes. We believe that we can produce code without causing fatal defects in production because we have a battery of automation in place to keep us in check. We have achieved a mindset in the team that enables our success. I opened by stating that getting to continuous deployment is not an easy process, but I would amend that to say that getting to continuous delivery was the hardest part of the journey. It was the culmination of a lot of work from the entire team on our process and a mindset change that has proven itself to be for the better. We’re at a great point. So what is next? Well, we are aiming for continuous deployment and we’ve still got a ways to go to get there. Having the releases automated and going without individual intervention is a big step in that direction but we still have some processes to define and further pain to solve on how we get to a point where every pull request merged triggers a release. I hope you’ve enjoyed hearing about our journey towards continuous deployment. Cool stuff we are doing and thinking about in engineering… 41 Development Agile Continuous Deployment Technology 41 claps 41 Written by Software Engineer working for the Global Payroll team at Intuit in London Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer working for the Global Payroll team at Intuit in London Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-03"},
{"website": "Intuit", "title": "performance at pull request level", "author": ["Vanya Sehgal"], "link": "https://quickbooks-engineering.intuit.com/performance-at-pull-request-level-d8a7507b2c12", "abstract": "We are taught to perform our best, be it in studies, sports or other extra curricular activities. And I heard the same ideology on the first day at work, “We need to plan to improve our TP90 and TP50 numbers”. Hence, I learnt besides performing at work, we need to make sure our web application performs well too (which totally makes sense). Why performance is important? We live in the age where everybody wants a quick result, no one is ready wait. If they have to wait, they simply move on to try the second best solution available in the market (and we cannot let that happen). Speed and Time is Money. The plan of action we decided to move forward with was detecting any discrepancies in performance before pushing out code changes to pipeline. Thus, comes the concept of evaluating Performance at P.R. (Pull Request) level. Page loading time is obviously an important part of any website’s user experience. Many times we let page load time slide to accommodate either a better appealing design, a new well-designed functionality or to add more content to web pages. Unfortunately, website visitors tend to care more about speed than all the bells and whistles we want to add to our websites. Performance Impacts Conversion Performance Impacts User Engagement Performance Impacts OpEx and Revenue Performance Impacts Usability A bunch of smart engineers modify the code and submit a PR for a specific plugin repo. GIT receives the Pull Request and informs the Jenkins jobs that it needs to get to work on validating those Jenkins Jobs (template based jobs) are than kicked-off to validate the PR’s The Jenkins job, thus kicked-off performs the below steps: a. Build & Unit Test Phase — Git Clone the repo — npm install the dependencies — Runs the Unit Tests (leveraging docker) — Generates the Code Coverage — Builds and Dist the plugin code b. Deploy Phase — Deploys the Dist to CDN/S3 with a unique version Since the code change is ready to be consumed, a functional test suite is kicked-off Run a basic set of functional tests to ensure the PR has not introduced any regression (leveraging docker) Grunt Task for the functional test x-browser/x-platform would be covered at this stage. Test suites would run in parallel and cover the broader combinations to provide faster feedback to the team. The objective of this phase is to add Performance Testing along with the chain of Build Unit Tests, Deploy Tests and Function Test before pushing the P.R. to the master pipeline. In the current scenario, the P.R.’s are pushed after passing all the above tests except the performance phase. The Performance Testing comes into the scenario after all the modifications/updates have been done, right before the release to measure whether any new functionality has altered the performance or not. The inclusion of Performance Testing for every pull request will decrease the overhead faced by the engineers during the release time. This also provides a mechanism to measure and experience the real world scenario. Once a PR version of the module is available in CDN/S3, we leverage open source tool WebPageTest to test the performance of the shell. We would simulate a request from different geographical locations such as (US-East, US-West, Australia, France & Canada etc) with variance in bandwidths such as DSL & Cable to hit PROD/Pre-PROD version of our application. Prior to launching the application and logging in, we overwrite the cookie which would switch the module version from what’s in prod to our desired PR version in CDN/S3 At this point a simple launch, login would be performed and a detailed result summary is generated via WebPageTest. We can define thresholds on the generated results to ensure we are not degrading/deviating from our base-benchmarks. This will help in identifying if there is in any change in performance. We need the ability to assess and understand the performance characteristics of our application. With the help of custom markers, we are able to expose a high precision timestamp for better measurement of the performance of our application. Gone are those days when performance testing is done when we are almost ready to ship. The pace at which we are currently building, performance testing is done with each Pull Request (PR). The current mechanism would validate each PR and update the same with the impact in terms of performance. Once the test is performed with DSL/Cable bandwidth(s), the same PR is published with the links to the detailed reports. Engineers would be notified on the status of the performance. The goal is to be proactive to identify the performance bottlenecks. Authors: Raj Vasikarla Vanya Sehgal Cool stuff we are doing and thinking about in engineering… 24 Continuous Integration Performance Testing Continuous Delivery Continuous Improvement 24 claps 24 Written by Software Engineer at @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at @Intuit Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-26"},
{"website": "Intuit", "title": "code coverage with functional automation react istanbul webdriverio", "author": ["Raj Vasikarla"], "link": "https://quickbooks-engineering.intuit.com/code-coverage-with-functional-automation-react-istanbul-webdriverio-33ba364d0a69", "abstract": "UI Automation is costly in terms of building and maintenance. A right measure of these suites would ensure we are not ending-up with heavy maintenance state without knowing the actual ROI. A perfect state would be to know the value what these tests provide and quantify them to understand the actual ROI. Here’s what we could do to avoid the routine and measure the Functional Automation tests. We( Raj Vasikarla & Suresh ) followed the below approach to achieve the same: Build with Istanbul Instrumentation Run your end-to-end UI Automation Suite Generate the Coverage Build & Serve with Istanbul Instrumentation: This is the first step in getting your code prepared for providing useful insights on the test automation code you write for your feature. There are many ways to instrument your react code, I am providing one such method below to instrument your code with istanbul. For the below example we assume that there is already a grunt task that will build your react code and for the sake of simplicity we will update the same grunt task to do the necessary instrumentation. Add the loader for instrumenting the code with istanbul below, using the babel-plugin-istanbul 2. Define the filter for the source file including/excluding the folders which needs to be instrumented. 3. Update the loader This can be verified by checking for “__coverage__” variable in the dev tools console window. If you do see that variable then your instrumentation is successful else it is not successful in which case you need to go back and check the instrumentation. Run end-to-end UI Automation : Once the build is instrumented and started we can run UI Automation suites. As there are many UI Automation frameworks available in the open source world, we have decided to use webdriverio for various reasons. WebdriverIO has a bunch of features that allows automation engineers to setup and build an automation suite in no time. Below is an example of how a typical UI Automation test built with webdriverio would like : More examples on how to use webdriverio could be found here . Once we have a suite of tests to run against the instrumented code, we need to generate the coverage and represent that in a readable format. Generate the Coverage : In the above snippet we enhance the webdriverio config file to do the below : Generate coverage after each test . We leverage the afterTest() hook from webdriverio and capture the __coverage__ object once every test is completed. Coverage object thus captured is added to an array of coverageObjects at this step. Merge the Overall Coverage : At the end of the entire suite, we merge all the coverage information from the coverageObjects and instruct istanbul to generate readable reports. Istanbul provide various formats of reports generation. We have opted “html” in the above snippet. Below is a screenshot of how a sample report would look like : Cool stuff we are doing and thinking about in engineering… 39 1 JavaScript Webdriverio Selenium React Istanbul 39 claps 39 1 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-26"},
{"website": "Intuit", "title": "setup circleci in an enterprise environment", "author": ["David Aghassi"], "link": "https://quickbooks-engineering.intuit.com/setup-circleci-in-an-enterprise-environment-59f4e3e89ef3", "abstract": "Over the past couple of weeks, we have been working to deploy CircleCI 2.0 in our enterprise AWS environment so we can reap the benefits that 2.0 brings. The purpose of this article is to clarify some points that aren’t super clear from CircleCI’s documentation . First off, props to the team at Circle for open sourcing this. It’s really awesome to see a tool like this publicly available and open to feedback. git clone https://github.com/circleci/enterprise-setup cd enterprise-setup make init This will create a terraform.tfvars file for you to use. Terraform is a technology that allows us to orchestrate AWS deployments automatically so we don’t have to do each step by hand. It is quite nifty, and elegant in execution. Once we have that terraform file, I’d recommend making a branch and committing so that your company can make changes and not work on the master branch. Let’s take a look at the terraform file. Most of this is pretty straight forward, but let’s hone in on some things that you may not need or know up front. circle_secret_passphrase is optional. Make it an empty string if you don’t want it. nomad_client_instance_type has a default value if you aren’t sure prefix is super useful because you can append a constant string to the front of the instance names using this You can refer to the Circle GitHub page above for an overview of options you can use, as well as their default values. One last thing. If your company doesn’t want to have your CI on a public IP address, you will want to disable that functionality. Open up the circleci.tf file and set the value associate_public_ip_address to false . There is currently a PR open for this to be part of the default tfvars files. Once you have filled in the above file, you may need to run terraform init in the repo. This is so terraform can keep state of your project. First, before deploying, you should run terraform plan . This allows you to see what will be changed in AWS prior to it actually being changed. Once you have done that, and verified that it has all the changes you are expecting, you can run terraform apply to generate your new AWS instances. This may take a while, so go get a cup of your favorite drink of choice while this executes. If you work at a large scale enterprise company, there is a good chance you have strict security restrictions around inbound and outbound traffic on your network. Unfortunately, at the time of this writing, CircleCI doesn’t take proxies into account during the setup phase. We will manually have to go into our AWS instances and do the rest by hand. Service Box ✍️ Log into AWS and go to your EC2 instances. In the filter box type the prefix you entered early in your terraform file (mine has “intuit” in it, so that’s what I used) In the results find the instance ending in the word “service”. Once you have the service instance, grab the IP. You will need to SSH into the box. If you click “Connect” in AWS, it will give you instructions on how to do that. I won’t cover that here. Once you have SSH’ed into the box, it is time to use our proxy. Make sure to replace the above with your actual proxies . Once you have done that, kick off the setup script for the service box by running the code below. Note: If you run into an issue regarding “unable to obtain lock”, reboot the instance and on start repeat the above export commands followed by sudo -E apt-get update . This will guarantee you are updated and not fighting for a lock. What is important here is the -E command. It passes the environment variables down to the script. In this case, we are setting the proxy so that any network requests made can reach outside servers. Builders (2.0) 👷 Before starting, there is a box for 1.0 builders and a box for 2.0 builders. This section only covers the 2.0 builders. We haven’t attempted to get 1.0 builders running yet, but I’m sure Circle is aware of it after working with us. As with the Service Box, the Builder Box also needs to make a call when it starts to standup the service. Again, we need to expose the proxy we want to use so that this script can run properly. In the enterprise-setup repo we checked out at the beginning, open up the file nomad_user_data.tpl and at the top add the following: You will also want to edit the area where the script creates the config for nomad . Search for /etc/nomad/config.hcl . You will want to edit it to include the following advertise block: Finally, we want to make sure our Docker containers have access to the outside world. Find the section in the script that says “Installing Docker” and add the following right before the: Once you have done that, chmod +x the script and run via sudo ./init.sh . Make sure it runs completely. If it fails at any point, just run different sections by hand. For safe measure, restart both nomad and docker via sudo service _____ restart . Once you are all set with the above, head to the IP of your service box on port 8800. You can finish the setup of your new instance by walking through the steps presented to you. I’d like to give a huge thank you to the CircleCI team for taking hours of their time to sit with me to step through these problems. Together, we worked to find the path to this state. There is currently a PR open on the enterprise-setup repo with the fix for the http-proxy issues. I encourage you all to go contribute and be part of the discussion. This is just some of the cool things that we work on here at Intuit. Got a cool thing you or your company is working on? Share below in the comments and let us know! Cool stuff we are doing and thinking about in engineering… 201 2 DevOps Circleci Infrastructure Continuous Integration 201 claps 201 2 Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Written by Software Engineer at Intuit, constantly breaking things to improve future software. Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-19"},
{"website": "Intuit", "title": "the one with the chatbot using api ai fb messenger", "author": ["Shashank Kaushik"], "link": "https://quickbooks-engineering.intuit.com/the-one-with-the-chatbot-using-api-ai-fb-messenger-c4c80ae9a07e", "abstract": "By following this tutorial you will learn: How to create a bot for a Facebook page that can automatically reply to user queries based on what it knows and what you can teach/train it to do. What tech does it require For part-1 we will only use FB developer portal and Api.ai’s GUI (graphical user interface) but part-2 is where it gets interesting, we will use Nodejs to write our own server which can call a variety of api’s to complete an array of tasks (For our case tells horoscope) and also deploy the server using hosting platform known as heroku. Is there any pre-requisite for this series. Yes, but very basic. You should know a little bit of JavaScript But But even if you don’t follow through. The tutorial is created keeping in mind the most basic of users and provides screenshots and videos for every step. Let’s get started… Login in to your fb account and create a new page. (go to https://www.facebook.com/pages/create/ ) Select Brand or Product and from the dropdown select website and in the input box type My Test Horoscope bot and then wait a bit. Your page will be created and looks something like this… Go to https://developers.facebook.com/ Click on Get Started, complete all the steps (agree to services agreement, authenticate yourself by credit card email or phone), fill “my test horoscope bot” as the app name. (you will land on a page that looks like…) Now lets go and create and account on Api.ai. Go to https://api.ai/ click on Sign up for free (complete the process with your gmail account) after which you will land on the homepage… A little about api.ai API.AI is a natural language understanding platform. So what does that mean? It means that if you pass a sentence to api.ai it will give you what the sentence means, as in what the user is asking for and what does he mean by that. Once you know what the user wants you can provide the same by writing the functionality. But how does api.ai understand what the user is saying and will it understand everything the user is trying to tell, even this “aklsdhkadjkabjkbjzcadiw” or emojis? For api.ai to understand anything you have to train it. Just like you teach a child to read and understand. The more you train the more your bot will understand. If you train it to understand aklsdhkadjkabjkbjzcadiw and emojis it will. How do i do that? Well before going to that let us understand how api.ai understands these things. For that we have to understand the language api.ai understands. To train your bot you need to understand a few terms first. Agents: Agents are NLU (Natural Language Understanding) modules. Their job is to convert speech or text passed to actionable data (Basically understanding what the user means). Entity: An Entity represents concept. Entity is the main subject of the whole sentence example: Which toppings do you want on your pizza? Here toppings and pizza are entities (what you want to do with is the Intent). Now we understand you want to do something with pizza and it’s toppings and what you want to do is Intent. Intent: An intent represents a mapping between what a user says and what action should be taken. So in above example we want to know what toppings the user wants on his/her pizza and now we can take the appropriate action. So let’s go back to the api.ai and start by create a new agent/BOT. Click on create agent (if a pop up opens stating don’t have correct permissions just Authorize). In the Description fill any description. Select correct timezone and language (I filled Horoscope Bot, GMT+530 and English) then click save. You will see that a lot of different options come on the left Nav and a Default Welcome Intent shows up. You can read about these in detail through api.ai docs . So we have a basic setup of Api.ai and we have created our FB developer account and app. Now let’s go and integrate them. Go to Integrations tab on the left nav of Api.ai account and you will see a bunch of one click integrations. Go ahead and select FB messenger integration. In a new tab open your facebook developer page and go to your app. On the left Nav you will see a section Products and under which there is an option to add product. Click +Add Product and search for Messenger . Once found click on Get Started Select the page you want the bot to be integrated on and you will get a Page Access token. Copy the Page Access Token and switch to api.ai and paste the token in Api.ai portal. Add a random string in the Verify Token field (I added my-chat-bot) and click start.(On successful start you will see a message in the bottom right of screen with the message “BOT started successfully”) Go back to FB developer portal again and add another product . This time you need to add Webhooks . Press Get Started on Webhooks. You will see this… Select Page from dropdown and then click on Subscribe to this topic. A modal will pop up in Callback Url fill the callback url that is there in api.ai and give the same verify token that you entered a few steps back in api.ai. Click on Verify and Save on the modal. It will make a call to your api.ai account behind the scene and verify (using webhooks mechanism) the account (If you get any error if double check the callback url and the verify token). Go back to Messenger (from left Nav on developer account under products section). Scroll down to the area where you generated the Token there you will see a section called Webhooks, from the drop down select the page and click on subscribe. Now see the video below to make some final changes and talk to your bot. With this we come to the end of this tutorial. The next part of the series can be found here and talks about creating a custom server and deployment of the same for a much powerful control of the bot and integrates with an horoscope api to you provide one with actual horoscope. Cool stuff we are doing and thinking about in engineering… 11 Bots Facebook Messenger 11 claps 11 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-27"},
{"website": "Intuit", "title": "the one with the chatbot part 2 creating a custom server for the bot", "author": ["Shashank Kaushik"], "link": "https://quickbooks-engineering.intuit.com/the-one-with-the-chatbot-part-2-creating-a-custom-server-for-the-bot-853c02f4de2c", "abstract": "Click this to go to part one of the series. Let’s get started For this part of the tutorial we will be using Expressjs over Node to create a simple server. (Please note there is a ton of material available out there on Node and Express, hence i will not be going into that. You can read Express docs for clarity. Personally i find the docs very rich in content and very crisp in explanation) The code for this tutorial can be found here or you can follow through and write it as you go. Create a folder chatbot and go inside the folder. Once inside run the following command on your terminal. (Upon running it you will get a prompt to fill some fields. Just “PRESS ENTER” in all and “ yes ” in the end). A file named package.json is now in your folder. If you open you’ll see that it contains keys with some value that you were prompted for earlier. Now run the following commands. Express: It is a framework over Node. body-parser: It parses the request body. Superagent: It is a light weight wrapper over AJAX. Create a file index.js inside the folder chatbot. This is the folder structure. (I have created one more file .gitignore and put node_modules inside it). Let’s start with a simple hello world example. In the terminal run node index.js and you check to see that a server on localhost:3000 has started and prints hello world on the screen when you hit the route http://localhost:3000. If it does that means everything is working fine. Next we will add a line in our package.json. Under scripts add a key:value pair as below Your package.json looks something like this (Please ignore other info and look at the scripts key only). Now we will use Heroku (a super simple hosting platform) to deploy our server. Go to this link . This is the getting started page for deploying Nodejs application on heroku. Follow the steps given. You can see the app that was created if you go here . You can also run “ heroku logs ” command in your terminal to see the logs. Open index.js and add another GET route to it This route will be used by FB webhook mechanism to verify your servers identity. Go to developers page on FB and in your account open webhooks section under product. Click on Edit Subscription. In callback url put https://your-app-name.herokuapp.com/webhook (for me it becomes https://intense-dawn-76396.herokuapp.com/webhook ) In Verify token put my-chat-bot (if you want to put anything else then put the same thing in the if condition of the new route). Push the new code again to heroku so that it gets the newly added route. In the terminal run. In a separate terminal window keep the logs running (heroku logs -t). Click on verify and save and check that it hits your sever by checking the logs. Hardcoding the verify token in the server is not a good practice as this will go into your code once you push it and can be used by anyone who has access to it… so log back into heroku and go to your dashboard . Select your app and go to settings. Once there click on Reveal Config Vars and add a new key VERIFY_TOKEN and value as your verify token. In the code (in index.js) replace your hardcoded token by process.env.VERIFY_TOKEN Go back to api.ai and follow the video below to create entity for Sun Signs so our bot can understand them. Here is one API endpoint i found while searching the net. We will be using the same to get the horoscope (You can choose others if you want). So we have integrated our FB messenger with a server. Deployed the server. Next we will connect the server to api.ai and then call the horoscope api to get the horoscope. For connecting to api.ai we need to have the right authorization. Go to your api.ai homepage and click on the Gear icon next to your agent/bot name. There you will see two tokens Client and Developer. We will be using Client token (The difference between these tokens is that developer token can be used to create new intents and entities while the client token can only read them). So copy the token and go to heroku’s app settings again and add a new config variable key API_CLIENT_TOKEN with value as your Client Token. Let us create some user intents in api.ai Now that facebook is pointing to your server, it will send user messages to it using webooks and it will be POST call. The structure of the call can be found here and looks like as shown in the image below. To capture the message we shall add another route to our server, this time a POST route. Newly added route above makes the appropriate checks against the common format to extract the event object which has all the necessary information required. If there is message object in the intent then we will make a call to api.ai to get information out of the message (sun sign and date). For making any call we will be using Superagent it is super simple to use wrapper over AJAX. You can find examples of it various functionalities here We have added a new function called processMessage and it makes a call to api.ai. Most of the code is self explanatory just note version session and language are required query parameters for the API call. Now that we have the sign and date let’s make a call to the API to get the horoscope. Final step is to now send the data back to the messenger. For that we need one more token for Authenticating the API i.e. the page access token which you can get from the api.ai integration page with FB messenger. Like before copy the token (You will need to stop the server to copy it and then start it again) and go to heroku and make a new config variable with PAGE_ACCESS_TOKEN and value the token. Now we will add the final function to send the data back to Facebook. Let’s push all these changes onto heroku. Run these command again from the root folder inside the terminal Wait for it to compile and check logs meanwhile (heroku logs -t). After it is done go to your page on FB and test the button. Type the strings that you gave in api.ai as intents. example: How does today look like for cancer. This brings us to the end of this tutorial. (Note that you can make your bot more powerful by putting in other common intents and maybe integrating to an API with more functionalities) In the next and last part of the tutorial series we will look at having multiple sessions and how to subscribe for auto notifications. Cool stuff we are doing and thinking about in engineering… 7 1 Nodejs Messenger Bots Chatbots 7 claps 7 1 Written by Cool stuff we are doing and thinking about in engineering at Intuit Written by Cool stuff we are doing and thinking about in engineering at Intuit Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-18"}
]