[
{"website": "HBCTech", "title": "A Non-Technical Guide to Posting to the Tech Blog", "author": "Unknown", "link": "https://tech.hbc.com/2018-04-12-non-technical-guide-to-posting-to-the-blog.html", "abstract": "Our tech organization is obviously made up of more than just developers.  Non-developers have great insight from their work too! This post is a nudge to all my fellow colleagues who are also doing amazing work and would like to share, but feel intimitated by the pull request process. Are you a techie? We have a guide for developers here , for everyone else, read on. Step 1: Write Your Post Writing is probably the hardest part of this whole thing. All you need to do is open a new Google Doc and type one letter after the other. Organize your thoughts by utilizing built in text headings; ie: bulleted lists, and text styles already available in Google Docs. As an example, take a look at the doc used to create this post . Step 2: Add Some Images People like pictures! Illustrate your ideas with a few images, screen captures, or even cat GIFs if appropriate. Insert your images via the toolbar in Google Docs, but keep the original assets handy, we’ll need to upload them to Github and adjust the image paths later. Don’t worry about the technical stuff yet, just keep writing. Step 3: Convert Your Google Doc to Markdown The Markdown stuff helps some web applications format text, create links, and embed images. Don’t be scared, most of the conversion can be automated for you. There is a Google Docs Add-on called, you guessed it, Google Docs To Markdown, or GD2md for short. Follow this link to add it to your Google Drive. When you’re happy with what you’ve written, follow the steps below: From the Google Docs Add-ons menu, select GD2md-html > Convert . The sidebar window opens. Use the Markdown button in the sidebar window to convert your document to Markdown. If you select part of the document, GD2md-html will convert only the selection. Otherwise it will convert the entire document. Click the Docs link for more information. Preview your post by copying and pasting the text from the sidebar into an online editor like Stack Edit . You’ll see there are a few extra lines of text added during the conversion. These can safely be removed, as they’re just warnings about image paths needing to be corrected. We’ll tackle these changes later. Step 4: Add Some YAML We use YAML formatting to store information about each post ie: title, post date, category, etc. Copy the snippet below and change the values according to your post. (make sure to copy the 3 dashes at the beginning and ending) ---\ntitle: A Non-Technical Guide to Posting to the Tech Blog.\ndescription: Step by step instructions explaining how to post to the tech blog without requiring a degree in computer science.\nauthor: Jaret Stezelberger\ndate: 2018-04-12\ncategories:\n- Culture\ntags:\n- Tech Blog\n- How To\n- Culture\n--- Step 5: Upload Everything To Github If you don’t already have GitHub account, get one here . It’s the modern library card! Once you’re signed into your Github Account, you’re just a few simple steps away from posting. Copy (Fork) HBC Tech Blog To Your GitHub Account Create A Copy For Your New Work (Create a New Branch) Add Your Post File and Then Save (Commit) it If Needed, Upload A Folder of Assets (Another Commit) Let The Team Know You’re Ready to Publish (Create A New Pull Request in GitHub) Here’s a closer look at those steps one by one: Step 5.1 Fork It Step 5.2 Create A Branch For Your New Post Step 5.3 Navigate to the Posts Folder Step 5.4 Create Your Post File Step 5.5 Save Your Changes Step 5.6 Add Your Images Step 5.7 Upload your images Step 5.8 Update Image Your Paths In the previous step, converting your Google Doc to Markdown, image paths were set with placeholders. You’ll need to change these to correctly match the folder and file names you’ve uploaded to GitHub. In our example above they would change from: ![alt_text](images/blog-post-how-to0.png \"image_tooltip\") to ![Fork The Blog Repo](./assets/images/blog-post-how-to/01-fork-button.png \"Fork the Blog Repo\") Step 5.9 Create a New Pull Request Summary Hopefully, after reading this post, the tech blog has been demystified and the few technical things around markdown conversion and fixing image paths won’t scare you away. If you get stuck, try looking for your answer in the documentation here . If you need help with Markdown, there are a ton of resources online. Stack Edit is your friend.", "date": "2018-04-12"},
{"website": "HBCTech", "title": "Brand Alerts V2: Breaking Down a Monolith with AWS", "author": ["Kinshuk Varshney"], "link": "https://tech.hbc.com/2018-04-02-brand-alerts-v2-breaking-down-a-monolith-with-aws.html", "abstract": "Recently, team T-Rex worked on the re-architecture of the\nBrand Alerts back end system, a project which we named Brand Alerts V2.\nIn this blog, I share our new architecture detailing some of the new AWS\npowered components. Introduction Brand Alerts V2 breaks down the monolithic architecture of Brand Alerts\nV1, into small, independent micro services. It enhances flexibility and\nextensibility of the system, not to forget, facilitates agile development,\ncontinuous integration and delivery. It also addresses some of our recent\npain points- database and thread scheduling issues, resulting at times\nin non delivery of brand alert emails. Our intent was to delegate much\nof the boilerplate work to AWS tools and services, which are built to\nhandle these use cases in tried and tested fashion and at scale. What Are Brand Alerts? Before I go any further, let me describe what Brand Alerts are. Brand Alerts are emails sent to Gilt members notifying them of an upcoming\nsale on a brand of their liking. A brand alert email is the ideal way to\nbeat the crowd and avail deep discounts on high-end fashion\nbrands. A typical brand alert email is sent in the evening (usually\naround 6 PM) a day prior to when the brand goes on sale. It should also be noted that Brand Alert emails have one of the highest\nclick-through rates (among all our triggered emails) and so business is\nalways looking to enhance and experiment, in order to squeeze more out\nof these emails. High Level Technical Components A good first step in system design is to chalk out the various building\nblocks. Here are the high level technical components that constitute the\nBrand Alerts system- Front End- for Marketing team to create, visualize brand alert email Database- to store a brand alert record (with brands, sales and their relevant info) Data access layer for the database Brand Alerts API- that other components can call to manage or get information on brand alerts Generate audience (business logic component) Render HTML (business logic component) Send emails (business logic component) Job scheduler for components 5-7 Brand Alerts V1 Before I introduce the V2 architecture, let me establish a baseline by\ntalking about Brand Alerts V1. Here is the architecture diagram (with\nthe distribution of aforementioned eight components)- The service ( svc-email-brand-alert ) is a Scala app based on standard\ncore-server-client model. As is evident from the diagram, this\ncomponent is doing a lot of heavy lifting. In addition to holding all the\nbusiness logic on how to generate audience and render HTML, it is also a\ndata access layer, an API (client module) and job scheduler (through a\nseparate Scheduler thread using Java’s ExecutionService ). Although most of the codebase is located inside one repo, this monolithic\narchitecture faces several disadvantages. It is complex, difficult to\nmaintain and troubleshoot by new team members, difficult to test in smaller\npieces and has multiple single points of failure that can bring down the\nwhole system. Brand Alerts V2 Allow me to introduce Brand Alerts V2 with the following architecture\ndiagram (again, with markings for the eight technical components)- At first glance, one notices a lot more boxes and arrows. I can assure\nyou that this is a good thing. The monolithic service piece of V1 has now\nbeen broken down into several smaller pieces, each of which is doing one\nor maximum two things and is doing it rather well. These are small, yet\npowerful components with baked in scalability and cohesiveness, being part\nof the AWS family. New Components Next, let me elaborate on some of the newly born, super-charged components. Brand Alerts Job The core component of the brand alerts process is trex-job-brand-alerts .\nThe job is written in Scala and is scheduled via AWS Batch (Sundial) to\nrun at a regular interval. No more maintaining our own Scheduler thread! When the job starts, a two step process is triggered. First, an audience\nis generated for each sale/brand in the brand alert. A call is made to\nAffinity API to get users who have an ‘affinity for the brand’. This set\nof users is then filtered based on the user’s subscription to the brand\nalert email and also to preserve his/her brand alert email receive limit\n(cadence check). In the second step, a brand alert event message\nis constructed corresponding to each user in the filtered\nset and is sent to the consumer-email-brand-alert SQS queue. As previously stated, the Brand Alerts job is configured to run via Sundial . Sundial, created by Gilt\nengineers, is an open source tool build on top\nof AWS Batch. This tool enhances AWS Batch capabilities by providing\nfeatures such as job scheduling, multi job dependency management,\njob dashboard, etc. All job settings are configurable and reside in bin/job-config.template.json . These include maximum runtime, number of\nretries, PagerDuty alerts (in case of job failure), to name a few. Brand Alerts API The API for Brand Alerts V2 ( api-brand-alert ) is written in Python and\nis an AWS Lambda, built using Chalice (which also provides a complimentary AWS API Gateway layering in front\nof AWS Lambda). AWS Lambda provides a lightweight, “server-less” environment to host the\nAPI’s endpoints, which are used to manage the lifecycle of a brand alert,\nfrom creation to deletion. API Gateway, on the other hand, manages the\nlifecycle of the API itself and provides valuable features such as access\ncontrol and traffic monitoring. In the root folder of the project, an api.json file details resources\nand data models of the API and is uploaded to API Builder for versioning and client generation. This component also acts as the Data Access Layer to the DynamoDB\ndatabase. This ensures that no other component has direct access to the\ndatabase, and that any component that needs to interact with the\ndatabase must go through the API. Boto3 library is used to obtain a Python client for DynamoDB. Brand Alerts Consumer The brand alert event messages sent to consumer-email-brand-alert queue\nare picked up by BrandAlertConsumer (in consumer-email ). The consumer\npolls for messages at a configurable interval. On finding a message in\nthe queue, the consumer first renders an HTML body from the information\ncontained in the message. Subsequently, it creates an object wrapper for\nthe HTML body and sends it to email-gateway (which then sends it to\nour Email Service Provider (ESP), SparkPost, for delivery). The consumer-email-brand-alert queue is an Amazon SQS queue and has an\nassociated Dead Letter Queue (DLQ) for message persistence in case of\nfailure. This lightweight component is fast, secure, reliable and can\nscale elastically. Conclusion Since its Production launch in early February, Brand Alerts V2 has been\nrunning smoothly, without fail, every day, in a timely, reliable manner.\nThe new architecture has emboldened Business, Product and Tech to\noutline future iterations, packing more features than ever before. For the Tech team, this effort was a win on many levels. We were able to\nenvision and bring to fruition a micro services architecture, which\npromises to be a step up from its previous iteration in all aspects.\nIn the process, through our research and experimentation, we acquired\nknowledge of various AWS tools and other third party libraries. Finally, AWS, with its vast set of tools and services, is instrumental\nin building powerful, resilient software. We should invest time and\nresources in finding the right tools for the job; I promise you it is\nwell worth the effort.", "date": "2018-04-02"},
{"website": "HBCTech", "title": "Negative Sampling (in Numpy)", "author": ["Jason Tam"], "link": "https://tech.hbc.com/2018-03-23-negative-sampling-in-numpy.html", "abstract": "Alright, time to have some fun exploring efficient negative sampling implementations in NumPy… Negative sampling is a technique used to train machine learning models that generally have several order of magnitudes more negative observations compared to positive ones. And in most cases, these negative observations are not given to us explicitly and instead, must be generated somehow. Today, I think the most prevalent usages of negative sampling is in training Word2Vec (or similar) and in training implicit recommendation systems (BPR). In this post, I’m going to frame the problem under the recommendation system setting — sorry NLP fans. Problem For a given user, we have the indices of positive items corresponding to that user. These are items that the user has consumed in the past. We also know the fixed size of the entire item catalog. Oh, we will also assume that the given positive indices are ordered. This is quite a reasonable assumption because positive items are often stored in CSR interaction matrices (err… at least in the world of recommender systems). And from this information, we would like to sample from the other (non-positive) items with equal probability. n_items = 10 pos_inds = [ 3 , 7 ] item_ind Probability 0 1/8 1 1/8 2 1/8 3 0 4 1/8 5 1/8 6 1/8 7 0 8 1/8 9 1/8 Bad Ideas We could enumerate all the possible choices of negative items and then use np.random.choice (or similar). However, as there are usually orders of magnitude more negative items than positive items, this is not memory friendly. Incremental Guess and Check As a trivial (but feasible) solution, we are going to continually sample a random item from our catalog, and keep items if they are not positive. This will continue until we have enough negative samples. def negsamp_incr ( pos_check , pos_inds , n_items , n_samp = 32 ): \"\"\" Guess and check with arbitrary positivity check\n    \"\"\" neg_inds = [] while len ( neg_inds ) < n_samp : raw_samp = np . random . randint ( 0 , n_items ) if not pos_check ( raw_samp , pos_inds ): neg_inds . append ( raw_samp ) return neg_inds A major downside here is that we are sampling a single value many times — rather than sampling many values once. And although it will be infrequent, we have to re-sample if we get unlucky and randomly choose a positive item. This family of strategies will pretty much only differ by how item positivity is checked. We will go through a couple of ways to tinker with the complexity of the positivity check, but keep in mind that the number of positive items is generally small, so these modifications are actually not super-duper important. Using in operator on the raw list: With a list , the item positivity check is O(n) as it checks every element of the list. def negsamp_incr_naive ( pos_inds , n_items , n_samp = 32 ): \"\"\" Guess and check with list membership\n    \"\"\" pos_check = lambda raw_samp , pos_inds : raw_samp in pos_inds return negsamp_incr ( pos_check , pos_inds , n_items , n_samp ) Using in operator on a set created from the list: Here, we’re going to first convert our list into a python set which is implemented as a hashtable. Insertion is O(1), so the conversion itself is O(n). However, once the set is created, our item positivity check (set membership) will be O(1) thereon after. So we can expect this to be a nicer strategy if n_samp is large. def negsamp_incr_set ( pos_inds , n_items , n_samp = 32 ): \"\"\" Guess and check with hashtable membership\n    \"\"\" pos_inds = set ( pos_inds ) pos_check = lambda raw_samp , pos_inds : raw_samp in pos_inds return negsamp_incr ( pos_check , pos_inds , n_items , n_samp ) Using a binary search on the list (assuming it’s sorted): One of best things you can do exploit the sortedness of a list is to use binary search. All this does is change our item positivity check to O(log(n)). from bisect import bisect_left def bsearch_in ( search_val , val_arr ): i = bisect_left ( val_arr , search_val ) return i != len ( val_arr ) and val_arr [ i ] == search_val def negsamp_incr_bsearch ( pos_inds , n_items , n_samp = 32 ): \"\"\" Guess and check with binary search\n    `pos_inds` is assumed to be ordered\n    \"\"\" pos_check = bsearch_in return negsamp_incr ( pos_check , pos_inds , n_items , n_samp ) (Aside: LightFM, a popular recommendation system implements this in Cython. They also have a good reason to implement this in a sequential fashion — but we won’t go into that.) Vectorized Binary Search Here we are going to address the issue of incremental generation. All random samples will now be generated and verified in vectorized manners. The upside here is that we will reap the benefits of NumPy’s underlying optimized vector processing. Any positives found during this check will then be masked off. A new problem arises in that if we hit any positives, we will end up returning less samples than prescribed by the n_samp parameter. Yeah, we could fill in the holes with the previously discussed strategies, but let’s just leave it at that. def negsamp_vectorized_bsearch ( pos_inds , n_items , n_samp = 32 ): \"\"\" Guess and check vectorized\n    Assumes that we are allowed to potentially \n    return less than n_samp samples\n    \"\"\" raw_samps = np . random . randint ( 0 , n_items , size = n_samp ) ss = np . searchsorted ( pos_inds , raw_samps ) pos_mask = raw_samps == np . take ( pos_inds , ss , mode = 'clip' ) neg_inds = raw_samps [ ~ pos_mask ] return neg_inds Vectorized Pre-verified Binary Search Finally, we are going to address both main pitfalls of the guess-and-check strategies. Vectorize: generate all our random samples at once\nPre-verify: no need for an item positivity check\nWe know how many negative items are available to be sampled since we have the size of our item catalog, and the number of positive items ( len(pos_inds) is just O(1) ) to subtract off. So let’s sample uniformly over a range of imaginary negative indices with 1–1 correspondence with our negative items. This gives us the correct distribution since we have the correct number of negative item slots to sample from; however, the indices now need to be adjusted. To fix our imaginary index, we must add the number of positive items that precede each position. Assuming our positive indices are sorted, this is just a binary search (compliments of np.searchsorted). But keep in mind that in our search, for each positive index, we also need to subtract the number of positive items that precede each position. def negsamp_vectorized_bsearch ( pos_inds , n_items , n_samp = 32 ): \"\"\" Pre-verified with binary search\n    `pos_inds` is assumed to be ordered\n    \"\"\" raw_samp = np . random . randint ( 0 , n_items - len ( pos_inds ), size = n_samp ) pos_inds_adj = pos_inds - np . arange ( len ( pos_inds )) ss = np . searchsorted ( pos_inds_adj , raw_samp , side = 'right' ) neg_inds = raw_samp + ss return neg_inds Briefly, let’s look at how this works for all possible raw sampled values. n_items = 10 pos_inds = [ 3 , 7 ] # raw_samp = np.random.randint(0, n_items - len(pos_inds), size=n_samp)\n# Instead of sampling, see what happens to each possible sampled value raw_samp = np . arange ( 0 , n_items - len ( pos_inds )) raw_samp : array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) # Subtract the number of positive items preceding pos_inds_adj = pos_inds - np . arange ( len ( pos_inds )) pos_inds_adj : array ([ 3 , 6 ]) # Find where each raw sample fits in our adjusted positive indices ss = np . searchsorted ( pos_inds_adj , raw_samp , side = 'right' ) ss : array ([ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 2 ]) # Adjust our raw samples neg_inds = raw_samp + ss neg_inds : array ([ 0 , 1 , 2 , 4 , 5 , 6 , 8 , 9 ]) As desired, each of our sampled values has a 1–1 mapping to a negative item. Summary Notebook with Results The notebook linked below compares the implementations discussed in this post in some example scenarios. The previously discussed “Vectorized Pre-verified Binary Search” strategy seems to be the most performant except in the edge case where n_samp=1 where vectorization no longer pays off (in that case, all strategies are very close). Notebook with results Concluding Remarks In models that require negative sample, the sample stage is often a bottleneck in the training process. So even little optimizations like this are pretty helpful. Some further thinking: how to efficiently sample for many users at a time (variable length number of positive items) at what point (sparsity of our interaction matrix) does our assumption that n_neg_items >> n_pos_items wreck each implementation how easy is it to modify each implementation to accommodate for custom probability distributions — if we wanted to take item frequency or expose into account", "date": "2018-03-23"},
{"website": "HBCTech", "title": "Airing Out A New Job System", "author": ["Terry McCartan"], "link": "https://tech.hbc.com/2018-03-05-airing-out-a-new-job-system.html", "abstract": "In this article I’ll be sharing some of the knowledge the Data team at HBC Tech picked up in replacing our old job system with Apache Airflow.\nWe undertook the decision to overhaul our job orchestration system a few months ago due to a number of reasons but have now successfully migrated all our data ingestion jobs to the new system. Firstly a little bit about our team. The Data Team at HBC Tech is responsible for constructing, delivering, and supporting all systems and services which enable self-service analytics\nand data science across all banners and all functions at Hudson’s Bay Corp (HBC).\nThe scope of responsibilities begins with data ingestion, and ends with supporting our user community of BI tool uses and data scientists.\nThese varieties of sources and types of processing lead us to review our job orchestration and I’ll be sharing the challenges involved, attempted solutions and lessons learned. In the beginning… The Data team has various jobs that handle the ingestion of data from multiple sources in a variety of formats.\nEach of these sources have either strict integration guidelines regarding time of ingestion or require coordination between various jobs to ensure correct ingestion.\nThis leads to numerous issues surrounding job scheduling, coordination and success criteria. We solved these issues with using our internally built job system Sundial As times changed, we wanted to take a fresh approach to how jobs were provisioned from a AWS point of view.\nThe jobs we use are lightweight and having them running on ECS instances around the clock when they were idle was deemed as a waste of resources.\nIt made sense to us to move the jobs to AWS batch instances to limit the cost of running jobs while still maintaining all the benefits of ECS.\nThis lead to some problems with our Sundial job system due to the fact that at the time it didn’t support AWS Batch.\nThis has changed since then with some great work by our personalisation team which you can read more about here . Time for something new… The team decided it was time to take a fresh approach to how we ran our jobs, which kicked off the investigation of a new system.\nThe investigation took place over a few weeks and spanned across a number of open source solutions.\nWe outlined that a new job system should at minimum support a number of features It should be able to integrate with AWS Batch Have a rich feature list inline with what was available in Sundial Have the ability to contribute new features and expand existing functionality Have the ability to have rich visualization of jobs and their dependencies We were able to whittle down the numerous possibilities to three possible solutions.\nThe first being Spotify’s Luigi system, available here which is a really great solution and was ticking most of our boxes.\nWe decided it was worthwhile to generate a proof of concept approach to really trial the solution. Luigi has been around for a long time and has rich user base which was a positive factor for us when considering it as our solution.\nIt was a really close call between Luigi and the solution that we picked mainly because Luigi provides a lot of the features we are looking for. The second solution we investigated was LinkedIn’s Azkaban workflow manager, available here .\nBeing based in java was probably closer to our comfort zone which was a plus for this project. What we found out was that although it satisfied our needs with regards rich visualization of the jobs and their dependencies there was some drawbacks to the solution.\nIt seems to be solely focused on orchestrating hadoop based jobs while we require a solution that allows us to interact with a number of possible executors, namely AWS Batch and EMR.\nIt’s feature list is expansive and could prove a perfect solution for those interested in running only hadoop based jobs. The final solution was AirBnB’s Airflow solution which at the time was just picked up by the Apache Foundation .\nAirflow was known to some of the people here in HBC but when they investigated it, it was still in its infancy and was missing a lot of features.\nWe decided to see what progress was made since the last time it was looked at and we were pleased with the improvements.\nIt was ticking all the boxes and after one of the engineers here did a proof of concept we decided it was the way forward for us. For a quick reference between Luigi vs Airflow, this is a great link The first attempt… Implementing the Airflow solution was a slight bit tricky for us at the beginning.\nA lot of the team’s expertise was based in Scala, so implementing a python based solution created a great opportunity to learn about the language. Early into the project one of our interests was to figure out how to deploy Airflow to our AWS account and how do we then deploy the DAGs to the instance.\nWe addressed the first by standing up an ECS cluster with Airflow installed on it.\nFor the second we setup  the ECS instances with a cron job that would pull down any changes that was pushed to the S3 bucket. We were able to successfully migrate all our existing jobs into DAGs and with some of the additions we made to the code allowed us to integrate nicely with our AWS tools such as Batch,\nSNS and Lambda In this attempt, we decided to fork the master branch of Airflow and use that as a source for us to deploy to ECS.\nThis gave us some benefits such as customizing some of Airflows base code to provide us with some extra functionality.\nAfter some soul searching however, we decided that this perhaps was not the best approach. Maintaining the branch and our additional code could create a maintenance issue for us in the future,\nfor example an incompatible change with our additional code. We decided that there must be a better way. The refinement… In the first attempt we figured out lot of the early problems with Airflow but we decided that maintaining the forked version was going to cause trouble down the road.\nTo try fix this, we decided to get a vanilla based Airflow instance up and running. This was partly to help with maintaining the system going forward since we didn’t want to have to constantly merge changes from the master branch into our fork.\nThis decision coupled with our earlier decision to change some of Airflows base code caused some problems for us. We decided to go with the latest release of Airflow (1.9) and create a fully dockerised version of Airflow with our DAG’s.\nLuckily there are people already working in this space and we were able to source a lot of the work required from this repository . To migrate the base code that we changed, we found out that Airflow had already solved this problems via it’s Plugin system . The plugin system gives us a nice way of expanding our functionality and hopefully releasing our plugins as an open sourced in the future. This process had its challenges but there is a great community of Airflow users and we were able to get a lot of help. Some of the sources we used were Stack Overflow and the ever busy Apache Airflow gitter. Next steps Now that we have our Airflow setup and jobs migrated we are experiencing the full benefits of what airflow has to offer.\nWithin a day we were able to create plugins that allow us to integrate with AWS EMR. This allowed the creation of DAG’s that will in the future support the migration of our ELT process to ETL using Spark, a big 2018 and 2019 initiative here at HBC Tech.\nWe are eagerly awaiting the improved DAG deployment system thats coming to Airflow to improve our deployment process but we feel we are in a good place with it at the moment. The Data teams roadmap has multiple exciting challenges to solve from ingestion, transformation to loading.\nHaving our Airflow setup, we feel we are in a good position now to tackle these problems. A big callout to Daniel Mateus Pires who acted as our go to person in all things Airflow. Over the new month or so we are really going to ramp up and if you’re interested in helping us solve these problems, take a look at our careers page here and get in touch.", "date": "2018-03-05"},
{"website": "HBCTech", "title": "AWS SDK for Java, version 2.0", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2018-01-26-aws-sdk-for-java.html", "abstract": "The Capital Region AWS User Group met on January 18th at the Nanotech Complex in Albany New York. CommerceHub hosted the meeting at their main office. The topic of this month’s meetup was the AWS SDK for Java. At HBC, our development teams use the SDK to access AWS services such as DynamoDB, S3, CloudWatch, and SNS. The v1 SDK has been a core building block at HBC since 2014. In June 2017, Amazon released a new implementation of the SDK for Java. The version 2.0 SDK is available as a developer preview. HBC is evaluating the new SDK and we look forward to using it in production later this year. Our engineering team has already started incorporating the v2 SDK into our helper libraries: gfc-aws-cloudwatch – pull request gfc-s3-cache – pull request The v2 API uses java.util.concurrent.CompletableFuture to encapsulate the result of an AWS service call. HBC’s Scala libraries will use FutureConverters to convert Java CompletableFuture objects into Scala Future objects. If you want to learn more about the v2 SDK, review my slidedeck or watch Kyle Thomson’s re:invent 2017 presentation.", "date": "2018-01-26"},
{"website": "HBCTech", "title": "Sundial AWS EMR Integration", "author": "Unknown", "link": "https://tech.hbc.com/2018-01-16-sundial-aws-emr-integration.html", "abstract": "AWS Elastic Map Reduce on Sundial Today I want to talk about a recent improvement we implemented in Sundial , an Open Source product launched by Gilt in early 2016. With Sundial 2.0.0 it’s now possible to schedule AWS Elastic Map Reduce jobs. For those of you who are not familiar with it, Sundial is a batch job scheduler, developed by the Gilt Personalization Team, that works with Amazon ECS and Amazon Batch. Before jumping into the nitty gritty details, it’s worth taking a deeper dive into the current batch job processing setup in Gilt and the challenges we have recently started to face. We will quickly cover the following areas: the current batch jobs setup batch job scalability Batch processing today Every night, the Gilt Aster data warehouse (DW) is locked down in order to update it with the latest data coming from the relevant area of the business. During this lock, Extract-Transform-Load ( ETL ) suites, or ELT as we prefer to call it , are\nrun. \nWhen all the jobs complete, the DW gets unlocked and the normal access to Aster is resumed. There are a number of client systems relying on the DW, most relevant are BI tools, i.e Looker , and Sundial.\nSundial in particular is used in personalization for scheduling additional jobs and to build Machine Learning models. Since there is no synchronization between Aster and Sundial, occasionally when Aster takes longer to complete, Sundial jobs would fail because of the DW being still locked down or data being stale. Performance degradation Because Aster is a shared resource, and the number of jobs relying on it is increasing day by day, in the past few weeks we’ve experienced significant performance degradation.\nThis issue is particularly amplified at a specific time of the week, when BI reports are generated. The result is that batch jobs and reports are taking longer and longer to complete. \nThis of course affects developers experience and productivity. EMR adoption Because of all the nuisances above, there is additional operational time spent to restart failed jobs. Furthermore, when developing a new model, \nmost of the time is spent extracting and massaging data, rather than focusing on the actual job logic. It’s easy to understand that Aster wasn’t a good candidate anymore for us and that we needed to migrate to a better and more elastic platform. The solution we were looking for should: work with multiple data formats be scalable be owned by the team be easy to integrate with our scheduling solution We didn’t have to look far to find a great candidate to solve our problems: Spark running on AWS EMR (Elastic Map Reduce) . Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. \nYou can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB. A complete list of open source applications (or components) running on AWS ERM can be found here . AWS EMR also offers a nice SDK to spin a new dynamic EMR cluster, run a job and tear down resources on the fly and a cost per second billing system so to make the whole platform very cost efficient. The last two perks of using AWS EMR are: AWS Spot Instances : running hardware at a discounted price Large variety of hardware : most of ELT jobs run on commodity hardware, some ML require intensive GPU computation and EMR offers hardware solutions for all of our use cases. The Sundial EMR Integration Since we were already using Sundial for most of our ETL and ML heavy lifting, we decided to extend the Sundial task_definition and add a new executable : the emr_command . Features we’ve implemented are: running a Spark EMR job on a pre-existing cluster running a Spark EMR job on a new created-on-the-fly cluster (and automatic tear down of resources) choose between on_demand vs spot instances live logs In the next two paragraphs I will go through two Sundial EMR task definition examples: the first is a Spark EMR job running on a pre-existing cluster, the second is the same job but running on a dynamically created cluster instead. Running a job on a pre-existing EMR Cluster Launching an EMR job on a pre-existing cluster is really simple, all that you need are some job details and the cluster_id where you want the job to run. \"executable\" :{ \"emr_command\" :{ \"emr_cluster\" :{ \"existing_emr_cluster\" :{ \"cluster_id\" : \"j-123ABC456DEF9\" } }, \"job_name\" : \"MyJobName1\" , \"region\" : \"us-east-1\" , \"class\" : \"com.company.job.spark.core.MainClass\" , \"s3_jar_path\" : \"s3://my-spark-job-release-bucket/my-job-spark-v1-0-0.jar\" , \"spark_conf\" :[ \"spark.driver.extraJavaOptions=-Denvironment=production\" ], \"args\" :[ \"arg1\" , \"arg2\" ], \"s3_log_details\" :{ \"log_group_name\" : \"spark-emr-log-group\" , \"log_stream_name\" : \"spark-emr-log-stream\" } } } The other properties are: class : the fully qualified main class of the job, e.g. “com.company.job.spark.core.MainClass” s3_jar_path : the s3 path to the job jar file e.g “s3://my-spark-job-release-bucket/my-job-spark-v1-0-0.jar” spark_conf : this is a list of attributes that you can pass to the spark driver, like memory or Java Opts (as per above example) args : another list of params that will be passed to the MainClass as arguments (as per above example) s3_log_details : Cloudwatch Log Group and Stream names for your job. See EMR Logs paragraph EMR Logs One nice feature of Sundial is the possibility of viewing jobs’ live logs. While AWS Elastic Container Service (ECS) and Batch natively offer \na way to access live logs, EMR updates logs only every five minutes on S3 and it cannot be used as feed for live logs. Since there isn’t a straightforward way of fixing this, it is developer’s \nresponsibility to implement the code that streams job’s log to AWS Cloudwatch Logs . One way of achieving this is via the log4j-cloudwatch-appender . The downside of having jobs running on static AWS EMR clusters is that you will be paying for it even if no jobs are running. For this reason it would be ideal if we could spin up an EMR cluster on-the-fly , run a Spark job and then dispose all the resources. If you want to know more, well, keep reading! Running a job on a dynamic EMR Cluster The Sundial Task definition that uses a dynamic cluster is fairly more complex and gives you some fine grained control when provisioning your cluster. \nAt the same time though, if your jobs don’t require very specific configurations (e.g. permissions, aws market type), sensible default options have been provided so to simplify the \nTask Definition where possible. Let’s dig into the different sections of the json template. \"emr_cluster\" :{ \"new_emr_cluster\" :{ \"name\" : \"My Cluster Name\" , \"release_label\" : \"emr-5.11.0\" , \"applications\" :[ \"Spark\" ], \"s3_log_uri\" : \"s3://cluster-log-bucket\" , \"master_instance\" :{ \"emr_instance_type\" : \"m4.large\" , \"instance_count\" : 1 , \"aws_market\" :{ \"on_demand\" : \"on_demand\" } }, \"core_instance\" :{ \"emr_instance_type\" : \"m4.xlarge\" , \"instance_count\" : 2 , \"aws_market\" :{ \"on_demand\" : \"on_demand\" } }, \"emr_service_role\" :{ \"default_emr_service_role\" : \"EMR_DefaultRole\" }, \"emr_job_flow_role\" : { \"default_emr_job_flow_role\" : \"EMR_EC2_DefaultRole\" }, \"ec2_subnet\" : \"subnet-a123456b\" , \"visible_to_all_users\" : true } } The json object name for a dynamic emr cluster is new_emr_cluster . It is composed by the following attributes: name : The name that will appear on the AWS EMR console release_label : The EMR version of the cluster to create. Each EMR version maps to specific version of the applications that can run in the EMR cluster. Additional details are available on the AWS EMR components page applications : The list of applications to launch on the cluster. For a comprehensive list of available applications, visit the AWS EMR components page s3_log_uri : The s3 bucket where the EMR cluster put their log files. These are both cluster logs as well as stdout and stderr of the EMR job master_instance : The master node hardware details (see below for more details.) core_instance : The core node hardware details (see below for more details.) task_instance : The task node hardware details (see below for more details.) emr_service_role : The IAM role that Amazon EMR assumes to access AWS resources on your behalf. For more information, see Configure IAM Roles for Amazon EMR emr_job_flow_role : (Also called instance profile and EC2 role.) Accepts an instance profile that’s associated with the role that you want to use. All EC2 instances in the cluster assume this role. For more information, see Create and Use IAM Roles for Amazon EMR in the Amazon EMR Management Guide ec2_subnet : The subnet where to spin the EMR cluster. (Optional if the account has only the standard VPC) visible_to_all_users : Indicates whether the instances in the cluster are visible to all IAM users in the AWS account. If you specify true, all IAM users can view and (if they have permissions) manage the instances. If you specify false, only the IAM user that created the cluster can view and manage it Master, core and task instances An EMR cluster is composed by exactly one master instance, at least one core instance and any number of tasks instances. A detailed explanation of the different instance types is available in the AWS EMR plan instances page . For simplicity I’ll paste a snippet of the AWS official documentation: master node: The master node manages the cluster and typically runs master components of distributed applications. For example, the master node runs the YARN ResourceManager service to manage resources for applications, as well as the HDFS NameNode service. It also tracks the status of jobs submitted to the cluster and monitors the health of the instance groups. Because there is only one master node, the instance group or instance fleet consists of a single EC2 instance. core node: Core nodes are managed by the master node. Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). They also run the Task Tracker daemon and perform other parallel computation tasks on data that installed applications require. task node: Task nodes are optional. You can use them to add power to perform parallel computation tasks on data, such as Hadoop MapReduce tasks and Spark executors. Task nodes don’t run the Data Node daemon, nor do they store data in HDFS. The json below describes configuration details of an EMR master instance: \"master_instance\" :{ \"emr_instance_type\" : \"m4.large\" , \"instance_count\" : 1 , \"aws_market\" :{ \"on_demand\" : \"on_demand\" } } , Please note that there can only be exactly one master node, if a different values is specified in the instance_count , it is ignored. For other instance group types the \nvalue instance_count represents, as the name says, the number of EC2 instances to launch for that instance type. Other attributes are: emr_instance_type : the EC2 instance type to use when launching the EMR instance aws_market : the marketplace to provision instances for this group. It can be either on_demand or spot An example of a EMR instance using spot is: \"aws_market\" : { \"spot\" : { \"bid_price\" : 0.07 } } Where bid_price is the Spot bid price in dollars. Limitations Because of some AWS EMR implementation details, Sundial has two major limitations when it comes to EMR job scheduling. The first limitation is that Sundial is not able to stop EMR jobs running on pre-existing clusters. Since jobs on the EMR cluster are scheduled via yarn and since \nAWS did not build any api on top of it, once a job is scheduled on an existing EMR cluster, in order to kill it, it would be required to ssh on the EC2 instance where the master node is running, query yarn so to find out the\ncorrect application id and issue a yarn kill command. We decided to not implement this feature because it would have greatly over complicated the job definition.\nJobs running on dynamic cluster are affected by the same issue. We’ve managed to still implement this feature by simply killing the whole EMR cluster. The second limitation is about live logs. As previously mentioned live logs are not implemented out of the box. Developers require to stream logs to Cloudwatch Logs and set log group and log name in the task definition.", "date": "2018-01-16"},
{"website": "HBCTech", "title": "Presentations we love: 2017", "author": "Unknown", "link": "https://tech.hbc.com/2017-12-30-presentations-we-love.html", "abstract": "2017 was a year of growth and learning at HBC Tech. Our organization embraced new technologies and new ways of building application software. As the year comes to an end, let’s recognize some notable technical presentations from 2017. Kubernetes Project update Kelsey Hightower ( @kelseyhightower ) at KubeCon 2017 Production: Designing for testability Mike Bryzek ( @mbryzek ) at QCon New York 2017 Streaming Microservices: Contracts & Compatibility Gwen Shapira ( @gwenshap ) at QCon New York 2017 Spinnaker and the Culture Behind the Tech Dianne Marsh ( @dmarsh ) at KubeCon 2017 Embracing Change without breaking the world Jim Flanagan and Kyle Thomson at AWS re:invent 2017 Developing Applications on AWS in the JVM Kyle Thomson ( @kiiadi ) at AWS re:invent 2017 Chaos Engineering at Netflix Nora Jones ( @nora_js ) at AWS re:invent 2017 apibuilder Sean Sullivan ( @tinyrobots ) at Scala Up North 2017 Managing Data in Microservices Randy Shoup ( @randyshoup ) at QCon New York 2017 Crushing Tech Debt Through Automation at Coinbase Rob Witoff  ( @rwitoff ) at QCon London 2017 Gilt’s iOS codebase evolution Evan Maloney ( @_emaloney_ ) at the Brooklyn Swift Developers Meetup Apache Struts and the Equifax Data Breach Sean Sullivan ( @tinyrobots ) at the Portland Java User Group Promcon 2017 Giovanni Gargiulo ( @giannigar ) at Promcon 2017 (Munich) The Paved PaaS to Microservices at Netflix Yunong Xiao ( @yunongx ) at QCon New York 2017 Productivity Engineering at Netflix Sangeeta Narayanan ( @sangeetan ) and Mike McGarr ( @sonofgarr ) at Productivity Engineering Silicon Valley Meetup - December 2017 Distroless Docker: Containerizing Apps, not VMs Matthew Moore ( @mattomata ) at JFrog SwampUp 2017 Shopify’s Architecture to handle 80K RPS Celebrity Sales Simon Eskildsen ( @sirupsen ) at GOTO Copenhagen 2017 Simplifying Omni-Channel Retail at Scale Aaron Strey ( @strey203 ) at Kafka Summit NYC 2017", "date": "2017-12-30"},
{"website": "HBCTech", "title": "Rethinking Retail: 4 Opportunities to Upgrade the Associate Experience", "author": ["Elizabeth Pizzuti"], "link": "https://tech.hbc.com/2017-11-24-rethinking-retail-4-oportunities.html", "abstract": "At HBC Tech, one of the company’s current initiatives is to bridge the divide between the physical and digital experience. Saks Fifth Avenue has a considerable network of retail locations throughout the US, and our goal is to use technology to entice more people into these stores. More specifically, one initiative focuses on connecting new and existing Saks customers with our style professionals on the retail floor. How does the design team go about tackling this challenge? The first thing we do is map out the full customer journey from beginning to end — device-agnostic, channel-agnostic. We create a view of the distinct elements that make up the customer journey, in order to devise an approach to provide a better experience for each touchpoint. Research helps to inform the customer journey, and for this initiative we’ve conducted many customer and associate interviews in order to get to the heart of their challenges and how we can help. We have two types of customers to design for in this case — the associate who works in the retail store and provides styling advice to the customer, and the Saks customer themselves. There are a few points in the journey where these two users intersect.\nCustomers may have an event coming up that they need to dress for, or they might be looking for that impossible-to-find Fendi backpack. They might deep down really want a new fashionable friend that they can get advice from on the daily. In these moments, they will be more likely to reach out to a retail associate for help. Associates are style professionals who thoughtfully build their client book month after month and meticulously style and assist their regular clients. They want to provide the best possible service to Saks customers and drive sales. There are some common pain points that came up again and again in the interviews, and design can help to solve some of them.\nThese are four core opportunities to make the retail associate workflow more seamless, structured and analytical. 1. One view of the customer Visibility into the customer’s previous orders, style affinities, brands they follow, sizes and online activity will give associates the information they need to provide the best advice to customers. Most importantly, associates need to be able to access this information on their personal devices as customer requests don’t stop after business hours. 2. Client acquisition Associates need to have the ability to accept digital leads in one system — the same system that they add new clients to that they meet in the store. We want to provide as much information about the customer up front to allow them to successfully work with the customer.\nWe’re launching a new way for customers to onboard with a stylist in the Saks app soon, however to scale the styling service, we’re also talking about automation. Some companies are turning to chatbots to help customers narrow down their requests before connecting with a styling professional. This boosts the efficiency of customer service for companies, while also potentially leading to a more accurate match with a stylist. 3. Inventory Visibility into online and in-store inventory is a must. It should be clear to Associates what they can pull from the store racks vs what they need to send a link to online. This visibility should extend to other store locations and beyond. What if there were even further transparency of global inventory levels of a product? This would benefit customers as they could find anything they’re looking for through Saks stores, and it would benefit associates by boosting sales. 4. Automation Store associates don’t have time to sift through inventory to provide personalized recommendations to each of their clients, or to personally get to know each new customer coming through the doors. They need insights regularly delivered to them that they can take immediate action on. For example, suggestions for follow up, new arrivals, upcoming events, or personal notes that can be searched and added to tasks. Machine learning can be implemented with product recommendations at the customer level as well as at the group level of customers with similar affinities.\nPerhaps most importantly, ideally all of these activities would exist in a unified stylist tool. We should provide a seamless integration of their CRM tool and point of sale so that associates can checkout a customer and add notes to follow up efficiently without switching between systems.\nAfter more than 30 interviews with our Saks Fifth Avenue retail associates, these opportunities to improve their experience became quite clear. Some of these challenges are beyond design, but through workshops, research, prototyping and iteration, we can assist in identifying the priorities for business, product and all other teams involved.\nAssociates are our customers too. They are the thread that ties together all of the services offered by Saks, and they represent the brand. We should be doing everything in our power to make their lives easier and to nurture their client relationships.", "date": "2017-11-24"},
{"website": "HBCTech", "title": "Dublin Scala Spree", "author": ["Gregor Heine"], "link": "https://tech.hbc.com/2017-09-11-dublin-scala-spree.html", "abstract": "This Friday the HBC Tech Dublin office will be hosting the first ever Dublin Scala Spree, a day-long Scala Open Source Hackathon.\nThe event is organized by the Dublin Scala Usergroup in cooperation with Dublin Functional Kubs and the Scala Center at EPFL in Lausanne, Switzerland. Date & Time: Friday, 15th September, 10am - 4pm Location: HBC Tech Office, Shelbourne Rd., Dublin 4, Ireland Sign-Up: Please register for the event via the Dublin Scala Users Group Organizers: Dublin Scala Meetup and Dublin Functional Kubs in cooperation with the Scala Center @ EPFL in Lausanne What is a Scala Spree? Scala Spree is a free community event aiming to popularize Open Source Software. It brings together Open Source authors,\nmaintainers and software engineers willing to contribute to OSS projects. Under the guidance of seasoned experts, newcomers learn about the inner\nworking of some popular tools and Scala libraries, and contribute to make them even better. For library authors, it’s an opportunity to improve\ntheir tools and get fresh feedback. For attendees it is a unique opportunity to lean more about Scala, contribute to Open Source Software and\nexpand their skills. And for everyone it’s a great opportunity to meet and have fun! Featured Projects For this week’s Spree we have the following special guests and their OSS projects: sbt and zinc - Jorge Vicente Cantero @jvican ( Scala Center ) API Builder - Michael Bryzek @mbryzek ( Flow.io ) HBC Foundation Classes - Gregor Heine @greheine ( HBC Tech ) If you have a Scala open source project that you would like to feature at the Spree, please get in touch with the Dublin Scala Users Group organizers . Like all Dublin Scala Community events, Scala Spree is free of charge and the only real requirement is an open mind and the will to contribute!\n– Apart from bringing your own computer to use, but chances are you figured that out already. Duration and pace To begin with, maintainers gather together in front of all the contributors to briefly explain their projects and tickets in one minute.\nThe idea is to give a good high-level explanation to motivate participants without going into too much detail. When they are done, participants\napproach the projects they are most interested in and get it contact with the maintainers. At this point, maintainers usually listen to the\nparticipants’ experience and provide personal guidance on tickets that would suit them.\nThen, the fun begins! Participants start hacking on their projects and maintainers review PRs as they come, assisting participants when they ask\nfor help. We encourage maintainers to merge as many PRs as possible in the place, for two reasons:\nParticipants get a small token of appreciation from the Scala Center. It increases the motivation of the participants. If participants get the\nfirst PR merged, they are invited to continue solving issues until they are happy with their work!\nAt the middle of the spree, we will provide free lunch and refreshments.\nParticipants can leave the event at any time they want. When the time approaches the end, everyone starts to wrap up: participants finish their\nPRs while maintainers finish their reviews, and organizers of the spree give away swag. Places will be strictly limited and will be allocated on a first come first served basis.\nRegistration through the Dublin Scala Users Group is required and only successfull RSVPs can attend.", "date": "2017-09-11"},
{"website": "HBCTech", "title": "Team Rookie 2017", "author": ["Team Rookie"], "link": "https://tech.hbc.com/2017-08-30-team-rookie.html", "abstract": "Who We Are Team-Rookie-2017, as we pride ourselves with being the most awesome team ever, has spent the summer improving the browsing experience for Gilt users as well as to collect data for our personalization team. The end result of our project included the crafted front-end user experience and a back-end service for data processing. Project Ideation The final project idea rose to the top through countless meetings and discussions with various teams in the organization. With the initially decided problem-solution proven to be unexecutable, our team, along with all of our mentors, took efforts to come up with a new solution to solve the given problem with the limited resources we had. This immersive process, in the very beginning of the program, ensured the understanding of the engineering problem and established the success of our project. To arrive at the best possible solution, we spent time learning the technology stack end-to-end. We went through many tutorials and labs with our mentors on the technologies we were going to eventually use, namely Scala, Android, and the Play framework. As we gained familiarities with these tools and technologies daily, we were quickly able to finalize on our ideas and the project has finally taken off. Problem Space: So let’s talk about the problem. With a growing user base, the Gilt platform needs to better understand what the users’ interests are in order to tailor unique shopping experiences to different user groups. Currently, users are able to “shop-the-look.” This feature allows a user to browse a completed set of apparels, such as the combination of a shirt, a pair of jeans, and shoes. It rids the hassle of a lot of users having to discover these items separately, they are able to find them all at once and make one single purchase. At the moment, these completed looks are selected by stylists who understand them. While stylists may provide the highest quality pairings, we are unable to scale human labor to the entire catalog. As fashion trends change, we need to update our pairings accordingly. Therefore, we aim to continuously collect user opinions on possible pairings. With these we can develop machine learning models to infer item compatibility. This is an ambitious goal, but not unachievable. We just need a steady supply of data. Solution: To tackle this problem, we proposed to create a fun and engaging experience for the users while they are shopping: completing their own outfits. One key requirement for this experience is that it can not interfere with the current purchase flow, meaning that if a user is closing in on a purchase, that process should not be interrupted. Therefore, rather than inserting the experience within the current workflow, we’ve decided to include the feature on the search page where users are able to favorite items they like. This is shown in the figure below. For our experience, to minimize disruption to the current workflow, we’ve added an additional hover link on the favorite button, and this will direct the users to our experience. We provide the users with additional items that can potentially be paired with the initial favorited item to form completed looks. These products, limited by category and price based on the favorited items, will be presented to the users for individual selections. The users can let their imaginations go wild and pick what they think are the best combinations. During this process, we will collected this data and persist it through our back-end API to the database. Finally, in order to complete the experience and make it as engaging as possible, we’ve decided to allow the users to immediately purchase the selected items if they wish. Since these items are what they specifically picked out from a pool of products, they will have a greater likelihood for conversion. So in a nutshell, this is the completed project of the 10 week internship filled with hard work, grind, sweat (mostly from our daily trips to equinox right down stairs), and a whole lot of fun. Intern Activities While we were not busy being awesome engineers, team-rookie spent most of our leisure time exploring New York and staying cool. Here are some of the highlights. Mentorship Team Rookie would like to give out a huge shout out to all of our mentors that helped us along they way and made this project possible (you know who you are)! With a special thanks to Doochan and Mike, who led the intern committee through all of our battles and came out on the other end with a solid victory. The complete-the-look experience would not have been possible without you guys.", "date": "2017-08-30"},
{"website": "HBCTech", "title": "HBC Tech Talks: February 2017 through July 2017", "author": "Unknown", "link": "https://tech.hbc.com/2017-08-10-midyear-recap.html", "abstract": "We’ve had a busy 2017 at HBC. The great work of our teams has created opportunities to share what we’ve learned with audiences around the world. This year our folks have been on stage in Austin, Sydney, Portland, Seattle, San Diego, Boston, London, Israel and on our home turf in NYC and Dublin. The talks have covered deep learning, design thinking, data streaming and developer experience to name just a few. Lucky for you, if you haven’t been able to check out our talks in person, we’ve compiled the decks and videos from a bunch of our talks right here. Enjoy! February Sean Sullivan spoke at the Portland Java User Group about Gilt’s E-Commerce Platform . Mikhail Girkin spoke at the Reactive Systems Meetup in Dublin about Streaming Data to s3 Using Akka Streams . March Ryan Martin spoke at AWS Architecture Week in NYC. Ugo Mantrangalo presented at the Microservices Meetup in Dublin about Writing Microservices in Go . Dana Pylayeva lead a workshop entitled Growing by Sharing: Transitioning a Group to a Self-Directed Model at the NYC Scrum User Group in March and the Scrum Gathering in San Diego in April. Jose Martinez and Ed Perry spoke at the Elastic NYC User Group Meetup. April Adrian Trenaman spoke at the AWS Dublin User Group about Serverless Architectures . Dana Pylayeva lead a workshop on Gamifying DevOps with Chocolate and Legos at the Agile Alliance Technical Conference in Boston. May Sean Sullivan spoke at the Portland Java User Group about Payment Processing at Gilt . Evan Maloney spoke at the Brooklyn Swift Meetup about Gilt’s 8 Year Codebase Evolution . June Mike Hansen spoke at QCon about Streaming Architectures . Adrian Trenaman spoke at QCon about Removing Friction in the Developer Experience . Gregor Heine spoke at the Dublin Microservices Meetup about Making Microservice deployments to AWS a breeze with Nova . Pau Carré Cardona spoke a the O’Reilly AI Conference in NYC and the Deep Learning in Retail Summit in London about Deep Learning in the Fashion Industry . July Sean Sullivan spoke at Scala Up North and the Portland Java User Group about ApiBuilder. Sophie Huang spoke at the Customer Love Summit in Seattle. Kyla Robinson gave a keynote on Key to Success: Creating A Mobile–First Mentality. Sera Chin and Yi Cao spoke at the NYC Scrum User Group about HBC’s Design Sprints.", "date": "2017-08-10"},
{"website": "HBCTech", "title": "Sundial or AWS Batch, Why not both?", "author": ["Kevin O'Riordan"], "link": "https://tech.hbc.com/2017-08-04-sundial-batch.html", "abstract": "About a year ago, we (the HBC Tech personalization team) open sourced Sundial , a batch job orchestration system leveraging Amazon EC2 Container Service . We built Sundial to provide the following features on top of the standard ECS setup: Streaming Logs (to Cloudwatch and S3 and live in Sundial UI) Metadata collection (through Graphite and displayed live in Sundial UI) Dependency management between jobs Retry strategies for failed jobs Cron style scheduling for jobs Email status reporting for jobs Pagerduty integration for notifying team members about failing critical jobs Sundial DAG Other solutions available at the time didn’t suit our needs. Solutions we considered included Chronos which lacked the features we needed and required a Mesos cluster, Spotify Luigi and Airbnb Airflow , which was immature at the time. At the time, we chose ECS because we hoped to take advantages of AWS features such as autoscaling in order to save costs\n by scaling the cluster up and down by demand. In practice, this required too much manual effort and moving parts so we lived with a long running cluster\n scaled to handle peak load. Since then, our needs have grown and we have jobs ranging in size from a couple of hundred MB of memory to 60GB of memory. Having a cluster scaled\n to handle peak load with all these job sizes had become too expensive. Most job failure noise has been due to cluster resources not being available or smaller jobs taking up space on instances meant to be dedicated to bigger jobs. (ECS is weak when it comes to task placement strategies). Thankfully AWS have come along with their own enhancements on top of ECS in the form of AWS Batch . What we love about Batch Managed compute environment. This means AWS handles scaling up and down the cluster in response to workload. Heterogenous instance types (useful when we have outlier jobs taking large amounts of CPU/memory resources) Spot instances (save over half on on-demand instance costs) Easy integration with Cloudwatch Logs (stdout and stderr captured automatically) What sucks Not being able to run “linked” containers (We relied on this for metadata service and log upload to S3) Needing a custom AMI to configure extra disk space on the instances. What we’d love for Batch to do better Make disk space on managed instances configurable.\n   Currently the workaround is to create a custom AMI with the disk space you need if you have jobs that store a lot of data on disk (Not uncommon in a data processing environment).\n   HBC Tech has a feature request open with Amazon on this issue. Why not dump Sundial in favour of using Batch directly? Sundial still provides features that Batch doesn’t provide: Email reporting Pagerduty integration Easy transition, processes can be a mixed workload of jobs running on ECS and Batch. Configurable backoff strategy for job retries. Time limits for jobs. If a job hangs, we can kill and retry after a certain period of time Nice dashboard of processes (At a glance see what’s green and what’s red) Sure enough, some of the above can be configured through hooking up lambdas/SNS messages etc. but Sundial gives it to you out of the box. What next? Sundial with AWS Batch backend now works great for the use cases we encounter doing personalization. We may consider enhancements such as Prometheus push gateway integration (to replace the Graphite service we had with ECS and to keep track of metrics over time) and UI enhancements to Sundial. In the long term we may consider other open source solutions as maintaining a job system counts as technical debt that\n is a distraction from product focused tasks. The HBC data team, who have very similar requirements to us, have started adopting Airflow (by Airbnb). As part of their adoption, they have contributed to an open source effort to make Airflow support Batch as a backend: https://github.com/gilt/incubator-airflow/tree/aws_batch . If it works well, this is a solution we may adopt in the future.", "date": "2017-08-04"},
{"website": "HBCTech", "title": "Visually Similar Recommendations", "author": ["Chris Curro"], "link": "https://tech.hbc.com/2017-07-31-tiefvision-2.html", "abstract": "Previously we’ve written about about Tiefvision ,\na technical demo showcasing the ability to automatically find similar\ndresses to a particular one of interest. For example: Since then, we’ve worked on taking the ideas at play in Tiefvision, and\nmaking them usable in a production scalable way, that allows us to\nroll out to new product categories besides dresses quickly and\nefficiently. Today, we’re excited to announce that we’ve rolled out\nvisually similar recommendations on Gilt for all dresses, t-shirts, and handbags,\nas well as to women’s shoes, women’s denim, women’s pants, and men’s\nouterwear. Let’s start with a brief overview. Consider the general task at\nhand. We have a landing page for every product on our online\nstores. For the Gilt store, we refer to this as the product detail\npage (PDP). On the PDP we would like to offer the user a variety of\nalternatives to the product they are looking at, so that they can\nbest make a purchasing decision. There exist a variety of approaches\nto selecting other products to display as alternatives; a particularly\npopular approach is called collaborative filtering which leverages\npurchase history across users to make recommendations. However this\napproach is what we call content-agnostic – it has no knowledge of\nwhat a particular garment looks like. Instead, we’d like to look at the\nphotographs of garments and recommend similar looking garments within\nthe same category. Narrowing our focus a little bit, our task is to take a photograph of a\ngarment and find similar looking photographs. First, we need to come up\nwith some similarity measure for photographs, then we will need to be able\nto quickly query for the most similar photographs from our large\ncatalog. This is something we need to do numerically. Recall that we can\nrepresent a photograph as some tensor \\(P \\in [0,1]^{H \\times W \\times 3}\\) (in other words a three dimensional array with entries in between 0\nand 1). Given that we have a numerical representation for an photograph, you\nmight think we could so something simple to the measure the similarity\nbetween two photographs. Consider: \\[\\text{sim} (P, P^\\prime) =\n\\sqrt{\\sum\\limits_{h=0}^{H-1}\\sum\\limits_{w=0}^{W-1}\\sum\\limits_{c=0}^{2}\n\\left(P_{h,w,c} - P^\\prime_{h,w,c}\\right)^2}\\] which we’d refer to as the Frobenius norm of the difference between the\ntwo photographs. The problem with this, although it is simple, is that\nwe’re not measuring the difference between semantically meaningful\nfeatures. Consider these three dresses: a red floral print, pink stripes, and\na blue floral print. With this “pixel-space” approach the red floral print and the pink stripes are\nmore likely to be recognized as similar than the red floral print and the\nblue floral print, because they have pixels of similar colors at similar\nlocations. The “pixel-space” approach ignores locality and global\nreasoning, and has no insight into semantic concepts. What we’d like to do is find some function \\(\\phi(\\cdot)\\) that extracts\nsemantically meaningful features. We can then compute our similarity\nmetric in the feature-space rather than the pixel-space. Where do we\nget this \\(\\phi(\\cdot)\\)? In our case, we leverage deep neural networks\n(deep learning) for this function. Neural networks are hierarchical\nfunctions composed of typically sequential connections of simple\nbuilding blocks. This structure allows us take a neural network\ntrained for a specific task, like arbitrary object recognition and\npull from some intermediate point in the network. For example say we\ntake a network, trained to recognize objects in the ImageNet dataset,\ncomposed of building blocks \\(f_1, f_2, \\dots, f_M\\): \\[f\\left(P\\right) = f_M(f_{M-1}(\\cdots f_2(f_1(P))\\cdots)\\] We might take the output of \\(f_3\\) and call those our features: \\[\\phi(P) = f_3(f_2(f_1(P)))\\] In the case of convolutional networks like the VGG, Inception, or\nResnet families our output features would lie in some\nvector space \\(\\mathbb{R}^{H^\\prime \\times W^\\prime \\times C}\\). The\nfirst two dimensions correspond to the original spatial dimensions (at\nsome reduced resolution) while the\nthird dimension corresponds to some set of \\(C\\) feature types. So in\nother words, if one of our \\(C\\) feature types detects a human face,\nwe might see a high numerical value in spatial position near where a\nperson’s face is in the photograph. In our use cases, we’ve determined that\nthis spatial information isn’t nearly as important as the feature types\nthat we detect, so at this point we aggregate over the spatial\ndimensions to get a vector in \\(\\mathbb{R}^C\\). A simple way to do\nthis aggregation is with a simple arithmetic mean but other methods\nwork as well. From there we could build up some matrix \\(\\Phi \\in \\mathbb{R}^{N\n\\times C}\\) where \\(N\\) is the number of items in a category of\ninterest. We could then construct an \\(N \\times N\\) similarity matrix\n\\(S\\) \\[S_{ij} = \\sqrt{\\sum\\limits_{k=0}^{C-1} \\left(\\Phi_{ik} - \\Phi_{jk}\\right)^2}\\] Then to find the most similar items to a query \\(i\\), we look at the\nlocations of the highest values in row \\(i\\) of the matrix. This approach is infeasible as \\(N\\) becomes large, as it has computational complexity\n\\(O(N^2C)\\) and space complexity \\(O(N^2)\\). To alleviate this issue,\nwe can leverage a variety of approximate nearest neighbor methods. We\nempirically find that approximate neighbors are sufficient. Also\nwhen we consider that our feature space represents some arbitrary\nembedding with no guarantees of any particular notion of optimality,\nit becomes clear there’s no grounded reason to warrant exact nearest\nneighbor searches. How do we do it? We leverage several open source technologies, as well as established\nresults from published research to serve visually similar garments. As\nfar as open source technology is concerned, we use Tensorflow , and (our\nvery own) Sundial . Below you can\nsee a block diagram of our implementation: Let’s walk through this process. First, we have a Sundial job that\naccomplishes two tasks. We check for new products, and then we compute\nembeddings using Tensorflow and a pretrained network of a particular type for\nparticular categories of products. We persist the embeddings on AWS\nS3. Second, we have another Sundial job, again with two tasks. This job\nfilters the set of products to ones of some particular interest and\ngenerates a nearest neighbors index for fast nearest neighbor look-ups. The job\ncompletes, persisting the index on AWS S3. Finally, we wrap a\ncluster of servers in a load balancer. Our product recommendation\nservice can query these nodes to get visually similar recommendations\nas desired. Now, we can take a bit of a deeper dive into the thought process\nbehind some of the decisions we make as we roll out to new\ncategories. First, and perhaps the most important, is what network\ntype and where to tap it off so that we can compute embeddings. If we\nrecall that neural networks produce hierarchical representations, we\ncan deduce (and notice empirically) that deeper tap-points (more steps\nremoved from the input) produce embeddings that pick up on “higher\nlevel” concepts rather than “low level” textures. So, for example, if\nwe wish to pick up on basic fabric textures we might pull from near\nthe input, and if we wish to pick up something higher level like\nsilhouette type we might pull from deeper in the network. The filtering step before we generate a index is also critically\nimportant. At this point we can narrow down our products to only come\nfrom one particular category, or even some further sub-categorization\nto leverage the deep knowledge of fashion present at HBC. Finally, we must select the parameters for the index generation,\nwhich control the error rate and performance trade-off in the approximate nearest\nneighbors search. We can select these parameters empirically. We\nutilize our knowledge of fashion, once again, to determine a good\noperation point. What’s next? We’ll be working to roll out to more and more categories, and even do\nsome cross category elements, perhaps completing outfits based on\ntheir visual compatibility.", "date": "2017-07-31"},
{"website": "HBCTech", "title": "How Large Is YOUR Retrospective?", "author": "Unknown", "link": "https://tech.hbc.com/2017-07-27-large-scale-retro.html", "abstract": "Can you recall the size and length of your typical retrospective?\nIf your team operates by The Scrum Guide , your retrospectives likely have less than ten people in one room and last about an hour for a two-weeks Sprint. What if your current team is larger than a typical Scrum team and a retrospective period is longer than a month? What if the team members are distributed across locations, countries, time zones and multiple third party vendors? Is this retrospective doomed to fail? Not quite.\nThese factors just add an additional complexity and call for a different facilitation approach. Last month at HBC we facilitated a large-scale mid-project retrospective for a 60 people-project team. While this project certainly didn’t start as an agile project, bringing in an agile retrospective practice helped identify significant improvements. \nHere is how we did it. From Inquiry to Buy-in This all started with one of the project sub-teams reaching out with an inquiry: “Can you facilitate a retrospective for us?”\nThat didn’t sound like anything major. We’ve been advocating for and facilitating retrospectives on various occasions at HBC: regular Sprint retrospectives, process retrospectives, new hire onboarding retrospectives etc. Further digging into a list of participants revealed that this retro would be unlike any others. We were about to pull together a group of 60 people from HBC and five consulting companies(!) In spite of working on the same project for a long time, these people never had a chance to step back and reflect on how they could work together differently. In order to make it successful, we needed buy-in from the leadership team to bring the entire team (including consultants) into the retrospective. Our first intent was to bring everyone into the same space (physical and virtual) and facilitate a retrospective with Open Space Technology .\nInitial response wasn’t promising: “We have another problem with this retro\n[…] is concerned that it is all day and that the cost of doing this meeting is like $25K-$50K” We had to go back and re-think the retrospective approach. How can we reduce the cost of this event without affecting the depth and breadth of the insights? Options we considered Thanks to the well-documented large retrospectives experiments by other agile practitioners, there was a number of options to evaluate: 1) Full project team, full day, face-to-face, Open Space-style retro 2) Decentralized, themes-based retros with learnings collected over a period of time and shared with the group 3) Decentralized retrospectives using Innovation Games Online platform 4) Overall retrospective (LeSS framework) Around the same time, I was fortunate to join a Retrospective Facilitator’s Gathering (RFG2017) - an annual event that brought together the most experienced retrospective facilitators from around the World. Learning from their experience as well as brainstorming together on the possible format was really helpful. Thank you Tobias Baier, Allan Jepsen, Joanne Perold, George Dinwiddie and many others for sharing your insights! I was especially grateful for the in-depth conversation with Diana Larsen in which she pointed out to “Clarify the goal and commitment of the key stakeholders before you start designing how to run the retrospective.” Back to the drawing board again! More conversations, clarifications and convincing… \nWith some modifications and adjustments, we finally were able to get the buy-in and moved forward with the retrospective. What worked for us – a tiered format. Individual team-level retrospectives We had a mix of co-located and distributed sub-teams on this project and chose to enlist some help from multiple facilitators. To simplify data consolidation, each facilitator received a data gathering format along with a sample retrospective facilitation plan. Each individual sub-team was asked to identify two types of action items: ones that they felt were in their power to address and others that required a system-level thinking and the support from the larger project community. The former were selected by the sub-teams and put in motion by their respective owners. The latter were passed to the main facilitator for analysis and aggregation to serve as a starting point for the final retrospective. Final retrospective For the final retrospective we brought together two types of participants: 1) Leads and delegates from individual sub-teams who participated actively at all times.\n2) Senior leaders of the organization who joined in the last hour to review and support team’s recommendations. The goal of this workshop was to review the ideas from sub-teams, explore system level improvements and get the support from senior leadership to put the system-level changes into motion. Retrospective plans Each retrospective was structured according to the classic five-steps framework and included a number of activities selected from Retromat . Example of an in-room sub-team retrospective (1 - 1.5 hours) Set the Stage We used a happiness histogram to get things started and get a sense for how the people felt about the overall project. Instead of reading the Prime Directive once at the beginning with the team, we opted for displaying it in the room on a large poster as a visible reminder throughout the retrospective. Gather Data Everyone was instructed to think about the things they liked about the project ( What worked well? ) and the ones that could’ve been better ( What didn’t work so well? ). In a short time-boxed silent brainstorming each team member had to come up with at least two items in each category. Next we facilitated a pair-share activity in a “speed dating” format. Forming two lines, we asked participants to face each other and take turns discussing what each of them wrote on their post-its. After two minutes the partners were switched and the new pairs were formed to continue discussions with the new set of people. At the end of the timebox, we asked the last pairs to walk together to the four posters on the wall and place their post-its into respective categories: \n1) Worked Well/ Can’t control \n2) Worked Well/Can control\n3) Didn’t work so well/Can’t control\n4) Didn’t work so well/ Can control After performing an affinity mapping and a dot-voting the group selected top three issues that they felt were in their control to address. Generate Insights/Decide What To Do Every selected issue got picked up by a self-organized sub-group. Using a template each sub-group designed a team level experiment defining the action they propose to take, an observable behavior they expect to see after taking that action and the specific measurement that will confirm a success of the experiment. Close the Retro We closed the retro by getting a feedback on the retro format, taking photos of the insights generated by the team.\nThese were passed on to the main facilitator for further analysis and preparation for the final retrospective event. Modifications for distributed teams For those teams that had remote team members or were fully distributed, we used a FunRetro tool. Flexibility to configure columns and the number of votes, along with easy user interface, fun colors and free cost made this tool a good substitute for an in-room retrospective. Final Retrospective (3 hours) Once all individual sub-teams retrospective were completed, we consolidated the project-level improvement proposals. These insights were reviewed, analyzed for trends and systemic issues and then shared during Tier 2 Final Retrospective. Set the stage We used story cubes to reflect and share how each of the participants felt about this project. This is a fun way to run a check in activity, equally effective with introverted and extraverted participants. The result is a collection of images that build a shared story about the project: We also reviewed an aggregated happiness histogram from each individual sub-teams to learn about the mood of 60 people on this project. Gather data Since the retrospective period was very long, building a timeline together was really helpful in re-constructing the full view of the project. We asked participants to sort the events into the ones that had a positive impact on the project (placing them above the timeline) and the ones that had a negative impact on the project (placing them below the timeline). The insight we gained from this exercise alone were invaluable! Generate Insights Next we paired the participants and asked them to walk to the consolidated recommendations posters. As a pair, they were tasked with selecting the most pressing issues and bringing them back for a follow up discussion at their table. Each table used the LeanCoffee format to vote on the selected issues, prioritize them into a discussion backlog and explore as many of them as the timebox allowed. Participants used roman voting as a way to decide if they are ready to more on to the next topic or need more discussion about the current one. Closing each discussion, participants recorded their recommended action. At the end of the timebox all actions from each table were shared with the rest of the group to get feedback. Decide What To Do/Close In the final hour of the retrospective the action owners shared their proposed next steps with the senior leadership team and reviewed the insights from the consolidated teams’ feedback. Was this experiment successful? Absolutely! One of the biggest benefits of this retrospective was this collective experience of working across sub-teams and designing organizational improvements together. Could we have done it better? You bet! As the project continues, we will be looking to run the retrospectives more frequently and will take into account things we learnt in this experiment. What did we learn? Designing a retrospective of this size is a project in itself. You need to be clear about the vision, the stakeholders and the success criteria for the retrospective. Do your research, tap into the knowledge of agile community and get inspired by the experience of others. Take what you like and then adapt to make it work in the context of your organization. Ask for help. Involve additional facilitators to get feedback, speed up the execution and created a safe space for individual sub-teams. Inclusion trumps exclusion. Invite consultants as well as full-time employees into your retrospective to better understand the project dynamic. Beware of potential confusion around retrospective practice. Be ready to explain the benefits and highlight the differences between a retrospective and a postmortem. Bringing senior leaders into the last hour of final retrospective can negatively affect the dynamics of the discussions. Either work on prepping them better or plan on re-establishing the safe space after they join. What would we like to do next? Continue promoting the retrospective practice across the organization. Offer a retrospective facilitator training to Scrum Masters, Agile Project Managers and anyone who is interested in learning how to run an effective retro. Establish retrospective facilitator circle to help maintain and improve the practice for all teams. Inspired by our experiment? Have your own experience worth sharing? We’d love to hear from you and learn what works in your environment. Blog about it and tweet your questions at @hbcdigital. World Retrospective Day Whether you are a retrospective pro, have never tried one in the past or your experience is anywhere in between, please do yourself a favor and mark February 6, 2018 on your calendar. \nA group of experienced retrospective facilitators is currently planning a record-breaking World Retrospective Day with live local workshops on every continent and in every time zone along with many on-line learning opportunities. We are engaging with the industry thought leaders to make this one of the best and most engaging learning experience. We hope to see you there!", "date": "2017-07-27"},
{"website": "HBCTech", "title": "Advanced tips for building an iOS Notification Service Extension", "author": ["Kyle Dorman"], "link": "https://tech.hbc.com/2017-07-07-ios-notifciation-service-extension-tips.html", "abstract": "The HBC Tech iOS team is officially rolling out support for “rich notifications” in the coming days. By “rich notifications”, I mean the ability to include media (images/gifs/video/audio) with push notifications. Apple announced rich notifications as a part of iOS 10 at WWDC last year (2016). For a mobile first e-commerce company with high quality images, adding media to push notifications is an exciting way to continue to engage our users. This post details four helpful advanced tips I wish I had when I started building a Notification Service Extension (NSE) for the iOS app. Although all of this information is available through different blog posts and Apple documentation, I am putting it all in one place in the context of building a NSE in the hopes that it saves someone the time I spent hunting and testing this niche feature. Specifically, I will go over things I learned after the point where I was actually seeing modified push notifications on a real device (even something as simple as appending MODIFIED to the notification title). If you’ve stumbled upon this post, you’re most likely about to start building a NSE or started already and have hit an unexpected roadblock. If you have not already created the shell of your extension, I recommend reading the official Apple documentation and some other helpful blog posts found here and here . These posts give a great overview of how to get started receiving and displaying push notifications with media. Tip 0: Sending notifications When working with NSEs it is extremely helpful to have a reliable way of sending yourself push notifications. Whether you use a third party push platform or a home grown platform, validate that you can send yourself test notifications before going any further. Additionally, validate that you have the ability to send modified push payloads. Tip 1: Debugging Being able to debug your code while you work is paramount. If you’ve ever built an app extension this tip may be old hat to you but as a first time extension builder it was a revelation to me! Because a NSE is not actually a part of your app, but an extension, it does not run on the same process id as your application. When you install your app on an iOS device from Xcode, the Xcode debugger and console are only listening to the process id of your application. This means any print statements and break points you set in the NSE won’t show up in the Xcode console and won’t pause the execution of your NSE. You actually can see all of your print statements in the mac Console app but the Console also includes every print/log statement of every process running on your iOS device and filtering these events is more pain than its worth. Fortunately, there is another way. You can actually have Xcode listen to any of the processes running on your phone including low level processes like wifid , Xcode just happens to default to your application. To attach to the NSE, you first need to send your device a notification to start up the NSE. Once you receive the notification, in Xcode go to the “Debug” tab, scroll down to “Attach to Process” and look to see if your NSE is listed under “Likely Targets”. If you don’t see it, try sending another notification to your device. If you do, attach to it! If you successfully attached to your NSE process you should see it grayed out when yo go back to Debug > Attach to Process. You should also be able to select the NSE from the Xcode debug area. To validate both the debugger and print statements are working add a breakpoint and a print statement to your NSE. Note: Everytime you rebuild the app, you will unfortunately have to repeat the process of sending yourself a notification before attaching to the NSE process. Amazing! Your NSE development experience will now be 10x faster than my own. I spent two days appending “print statements” to the body of the actual notification before I discovered the ability to attach to multiple processes. Tip 2: Sharing data between your application and NSE Although your NSE is bundled with your app, it is not part of your app, does not run on the same process id (see above), and does not have the same bundle identifier. Because of this, your application and NSE cannot talk to each other and cannot use the same file system. If you have any information you would like to share between the app and the NSE, you will need to add them both to an App Group. For the specifics of adding an app group check out Apple’s Sharing Data with Your Containing App . This came up in HBC Tech’s NSE because we wanted to have the ability to get logs from the NSE and include them with the rest of the app. For background, the HBC Tech’s iOS team uses our own open sourced logging library, CleanroomLogger . The library writes log files in the app’s allocated file system. To collect the log files from the NSE in the application, we needed to save the log files from the NSE to the shared app group. Another feature you get once you set up the App Group is the ability to share information using the app group’s NSUserDefaults . We aren’t using this feature right now, but might in the future. Tip 3: Using frameworks in your NSE If you haven’t already realized, rich notifications don’t send actual media but just links to media which your NSE will download. If you’re a bolder person than me, you might decide to forgo the use of an HTTP framework in your extension and re-implement any functions/classes you need. For the rest of us, its a good idea to include additional frameworks in your NSE. In the simplest case, adding a framework to a NSE is the same as including a framework in another framework or your container app. Unfortunately, not all frameworks can be used in an extension. To use a framework in your application, the framework must check the “App Extensions” box. Most popular open source frameworks are already set up to work with extensions but its something you should look out for. The iOS app has one internal framework which we weren’t able to use in extensions and I had to re-implement a few functions in the NSE. If you come across a framework that you think should work in an extension, but doesn’t, check out Apple’s Using an Embedded Framework to Share Code . Tip 4: Display different media for thumbnail and expanded view When the rich notification comes up on the device, users see a small thumbnail image beside the notification title and message. And when the user expands the notification, iOS shows a larger image. In the simple case (example above), you might just have a single image to use as the thumbnail and the large image. In this case setting a single attachment is fine. In the iOS app, we came across a case where we wanted to show a specific square image as the thumbnail and a specific rectangular image when the notification is expanded. This is possible because UNMutableNotificationContent allows you to set a list of UNNotificationAttachment . Although this is not a documented feature it is possible. var bestAttemptContent = request . content . mutableCopy () as? UNMutableNotificationContent let expandedAttachment = UNNotificationAttachment ( url : expandedURL , options : [ UNNotificationAttachmentOptionsThumbnailHiddenKey : true ]) let thumbnailAttachment = UNNotificationAttachment ( url : thumbnailURL , options : [ UNNotificationAttachmentOptionsThumbnailHiddenKey : false ]) bestAttemptContent . attachments = [ expandedAttachment , thumbnailAttachment ] This code snippet sets two attachments on the notification. This may be confusing because, currently, iOS only allows and app to show one attachment. If we can only show one attachment, then why set two attachments on the notification? I am setting two attachments because I want to show different images in the collapsed and expanded notification views. The first attchment in the array, expandedAttachment , is hidden in the collapsed view ( UNNotificationAttachmentOptionsThumbnailHiddenKey : true ). The second attachment, thumbnailAttachment , is not. In the collapsed view, iOS will select the first attachment where UNNotificationAttachmentOptionsThumbnailHiddenKey is false . But when the nofication is expanded, the first attachment in the array, in this case expandedAttachment , is displayed. If that is confusing see the example images below. Notice, this is not one rectangular image cropped for the thumbnail. Note: There is a way to specify a clipping rectangle using the UNNotificationAttachmentOptionsThumbnailClippingRectKey option, but our backend system doesn’t include cropping rectangle information and we do have multiple approprite crops of product/sale images available. Conclusion Thats it! I hope this post was helpful and you will now fly through building a Notification Service Extension for your app. If there is anything you think I missed and should add to the blog please let us know, techevangelism@hbc.com.", "date": "2017-07-07"},
{"website": "HBCTech", "title": "Hudson's Bay Company at QCon", "author": "Unknown", "link": "https://tech.hbc.com/2017-06-12-hbc-at-qcon.html", "abstract": "Heading to QCon? Don’t miss these two sessions! If you can’t make it, stay tuned here for slides and recordings from the conference. Removing Friction In the Developer Experience If you follow this blog at all, you know that we talk a lot about how we work here. Whether it’s out approach to adopting new technology , the work of our POps team or our team ingredients framework, we’re not shy when it comes to our people and culture. With that in mind, it only makes sense that Ade Trenaman , SVP Engineering at Hudson’s Bay Company, will be part of the Developer Experience track at QCon New York in June. Titled “Removing Friction In the Developer Experience” , Ade will highlight a number of the steps we’ve taken as an organisation to improve how we work. His session will cover: how we blend microservice / serverless architectures, continuous deployment, and cloud technology to make it easy to push code swiftly, safely and frequently and operate it reliably in production. the organisational tools like team self-selection , team ingredients (see above), voluntary adoption and internal startups that allow us to decentralise and decouple high-performing teams. Survival of the Fittest - Streaming Architectures Michael Hansen will also be at QCon this year. Mike’s talk will help guide the audience through a process to adopt the best streaming model for their needs (because there is no perfect solution). In his own words: “Frameworks come and go, ​so this talk is not about the “best” framework or platform to use, rather it’s about core principles that will stand the tests of streaming evolution.” His talk will also cover: major potential pitfalls that you may stumble over on your path to streaming and how to avoid them the next evolutionary step in streaming at Hudson’s Bay Company Hope to see you there!", "date": "2017-06-12"},
{"website": "HBCTech", "title": "Kubernetes", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2019-07-26-kubernetes.html", "abstract": "HBC’s production applications run in Docker containers on a public cloud. Over the past 18 months, Kubernetes has emerged as a core building block for distributed systems. All major cloud platforms (AWS, Google, Microsoft) now offer Kubernetes-as-a-service. Our engineering team is excited about the potential of Kubernetes for our production environment. Here is a list of presentations about Kubernetes that we have found educational. Kubernetes Is A Platform Platform Joe Beda ( @jbeda ) at DevOpsDays Seattle 2018 Kubernetes Design Principles Saad Ali ( @the_saad_ali ) at KubeCon North American 2018 How Atlassian Built Our Own Kube Clusters and Why You Shouldn’t Do the Same Nick Young ( @youngnick ) at KubeCon North American 2018 Highly Available Kubernetes Clusters - Best Practices Meaghan Kjelland and Karan Goel at KubeCon North American 2018 Kubernetes: Crossing the Chasm Ian Crosby ( @iandcrosby ) at GOTO Amsterdam 2018 Kubernetes Operability Tooling Bridget Kromhout ( @bridgetkromhout ) and Brendan Burns ( @brendanburns ) at DevOpsDays Seattle 2019 Kubernetes As Code Joe Duffy ( @funcofjoe ) at New York Kubernetes Meetup June 2019 Kubernetes for Java Developers Arun Gupta ( @arungupta ) at Kubernetes Day India 2019 Develop Hundreds of Kubernetes Services at Scale with Airbnb Melanie Cebula ( @MelanieCebula ) at QCon London 2019 Evolving Legacy Systems into Kubernetes at Lyft Lita Cho ( @litacho ) and Jose Nino ( @junr03 ) at KubeCon North America 2018 Mastering Kubernetes on AWS Yaniv Donenfeld ( @ydonfeld ) at AWS re:Invent 2018 Deep Dive into Amazon EKS Brandon Chavis and Eswar Bala ( @bala_eswar ) at AWS re:Invent 2018 How Intuit Does Canary and Blue Green Deployments with a Kubernetes Controller Daniel Thomson and Alex Matyushentsev at KubeCon Europe 2019 Kubernetes Application Migrations: How Shopify Moves Stateful Applications Between Clusters and Regions Ian Quick ( @ianquick ) at Systems @Scale 2018 Streamlining Kubernetes Application CI/CD with Bazel Gregg Donovan ( @greggdonovan ) and Chris Love ( @chrislovecnm ) - KubeCon Europe 2019 Noobernetes 101: Top 10 Questions We Get From New Kubernetes Users Neependra Khare ( @neependra ) and Karthik Gaekwad ( @iteration1 ) at Kubernetes Day India 2019", "date": "2019-07-26"},
{"website": "HBCTech", "title": "Migrations", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2019-06-06-migrations.html", "abstract": "At HBC, we are always looking for ways to improve the digital experience for our customers. We strive to improve the customer experience as well as reduce operational cost. Like many large organizations, we have a significant investment in our digital platform. Over the past five years, we have completed multiple system migrations. This has reduced complexity and allowed our organization to focus on future opportunities. Here is a list of presentations about system migrations that we have found helpful. Paying Technical Debt at Scale: Migrations @ Stripe Will Larson ( @lethain ) at QCon San Francisco - November 2018 Spotify’s Journey to the Cloud Spotify Engineering - Google Cloud Next 2018 The Great Migration: from Monolith to Service-Oriented at Airbnb Jessica Tai ( @jessicamtai ) at QCon San Francisco - November 2018 Microservices at HBC Sean Sullivan ( @tinyrobots ) at Portland Java User Group September 2018 From ActiveMQ To Amazon MQ : Why And How We Moved To AWS’s Managed Solution Phil Whelan ( @philwhln ) at Bench Engineering - January 2019 High Reliability Infrastructure Migrations Julia Evans ( @b0rk ) at KubeCon Seattle 2018 Monoliths, Migrations, and Microservices Randy Shoup ( @randyshoup ) at Reactive Summit Montreal 2018 Etsy: Migrating a Monolith to the Cloud Keyur Govande ( @keyurdg ) at SREcon19 Americas - March 2019 Migrating a Monolithic Application to Microservices at Google Soeren Walls and Sergio Felix at Google Cloud Next 2019 Lessons Learned from Our Main Database Migrations at Facebook Yoshinori Matsunobu ( @yoshinorim ) at SREcon18 Americas - March 2018 Evolving Legacy Systems into Kubernetes at Lyft Lita Cho ( @litacho ) and Jose Nino ( @junr03 ) at KubeCon North America 2018 Soundcloud: Migrations under Production Load - How to Switch Your Database without Disrupting Service Vilde Opsal ( @thevildebeast ) at SREcon18 EMEA - August 2018 Spotify: The Story of Why We Migrate to gRPC Matthias Grüter ( @mattgruter ) at CloudNativeCon Europe - May 2019 Dropbox migration to gRPC Ruslan Nigmatullin and Alexey Ivanov at Dropbox Engineering - January 2019 Database Schema Migrations with Zero Downtime Michiel Rook ( @michieltcs ) at DevOpsDays Copenhagen - April 2019 Building Resilience in Production Migrations at Netflix Sangeeta Handa ( @smhanda ) at QCon San Francisco 2018 Kubernetes Application Migrations: How Shopify Moves Stateful Applications Between Clusters and Regions Ian Quick ( @ianquick ) at Systems @Scale 2018 Migrating Monolithic Applications with the Strangler Pattern Kenneth Jackson, Harsha Sharma, and Christopher Marsh-Bourdon at AWS Summit New York - July 2019", "date": "2019-06-06"},
{"website": "HBCTech", "title": "Deployments", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2019-05-29-deployments.html", "abstract": "At HBC, we use modern engineering practices to manage our e-commerce platform. Our engineers have adopted multiple techniques to ensure that production code deployments go smoothly. Here is a list of presentations about deployments that we have found helpful. Disband the Deployment Army Michael T. Nygard ( @mtnygard ) at GOTO Aarhus - October 2012 10+ Deploys Per Day: Dev and Ops Cooperation at Flickr John Allspaw ( @allspaw ) and Paul Hammond ( @ph ) at Velocity conference - June 2009 Deployment patterns for DevOps and Continuous Delivery Danilo Sato ( @dtsato ) at DevOps Singapore - May 2016 Small Batch Deployments Sean Sullivan ( @tinyrobots ) - May 2019 Deployments at Flow Commerce Mike Bryzek ( @mbryzek ) at NYC Continuous Delivery meetup - June 2016 Four Minute Deploys Lei Lopez at SRECon Americas - March 2017 Ten Deployments Per Day Sean Sullivan ( @tinyrobots ) - March 2019 Deployment Automation at Uber Sebastian Yates at SRECon Americas - March 2017 Safe Lambda Deployments Sean Sullivan ( @tinyrobots ) at Seattle Serverless Meetup - February 2019 How Intuit Does Canary and Blue Green Deployments with a Kubernetes Controller Daniel Thomson and Alex Matyushentsev at KubeCon Europe 2019 Front-end Application Deployment Patterns Ross Kukulinski ( @rosskukulinski ) at KubeCon North America 2018 I deploy on Fridays (and maybe you should too) Michiel Rook ( @michieltcs ) at DevOpsCon - December 2018 The Evolution of Continuous Delivery at Scale @ Linkedin Jason Toy ( @jastoy ) at QCon San Francisco - November 2014 10K deploys per day - the Skyscanner journey so far Stuart Davidson ( @spedge ) at QCon London - May 2018 Deployments Endgame Sean Sullivan ( @tinyrobots ) - April 2019 Continuous Deployment at Facebook Scale Boris Grubic and Fangfei Zhou - Systems @ Scale 2019 Releasing the World’s Largest Python Site Every 7 Minutes (deployments at Instagram) Perry Randall at SREcon19 Asia/Pacific 2019 Spinnaker in Production: Lessons Learned from the Trenches at Netflix Cameron Fieber ( @cfieber ) at Spinnaker Summit 2018 CI/CD Across Multiple Environments Vic Iglesias, Benjamin Good, and Karl Isenberg at Google Cloud Next 2019", "date": "2019-05-29"},
{"website": "HBCTech", "title": "How to Deal With Time Zones in Database and Distributed Systems", "author": ["Daniel Kirby"], "link": "https://tech.hbc.com/2019-04-19-time-zones.html", "abstract": "Time zones can be one of the hardest and most confusing sources of technical issues in modern software systems. In this article we’re going to go through some common rules of thumb for dealing with datetimes when architecting those systems. First, a standard The standard representation you should use for datetimes by default is ISO 8601. The ISO 8601 standard looks like this: YYYY-MM-DDTHH:mm:SS±hh:mm For a concrete example, the date and time that Marty McFly went back to the future was: 1955-11-12T22:04:00-08:00 There are obvious advantages to this standard. For one, it will natively sort lexicographically in any programming language. For another, it includes the time zone to apply maximum specificity. But the most important advantage is that it is a standard, and if you stick to it whenever possible you will find your stack easier to develop as more and more programming languages integrate features to parse and format this standard natively. I’ve worked with ISO datetimes natively in the most recent releases of Python, Java, and JavaScript. It’s particularly nice on the frontend because it guarantees the user will see the appropriate time in their current time zone. So, if the ISO 8601 standard is so great, why isn’t it commonly used in databases for its easy lexicographical sorting? Why aren’t there time zones in SQL? Time zones didn’t exist before common usage of railroads for transportation. Prior to the railroad every town and city would maintain its own local time, and travellers would readjust their watches when they arrived in a new place. But by the time railroads criscrossed North America, Europe, and Asia it became necessary to standardize local time across wide swathes of the planet. In much the same way that the invention of railroads spurred the invention of time zones, the creation of distributed systems, microservices, and global internet businesses has spurred the adoption of time zone standards in software. However, some technologies were designed without these considerations in mind. Before the invention of distributed systems most databases and the servers that ran them were colocal in the same time zone, often on the same computer. So basically, SQL (and often other database systems) broadly assumes that datetimes you give it are in the local time zone of the machine and that the software written to update the database is also in that self-same time zone. For this reason when dealing with RDS systems it’s important to follow the following patterns. Whenever possible, pass datetimes to the service, module, or ORM that accesses the database with time zones attached. This will assure that no unwarranted assumptions are made between dislocated services. Set a standard and stick to it for what time zone the database uses. Unfortunately it’s probably not in your best interests to include the time zone within the stored records of the database, this is because this would break the pattern SQL was designed to use, and for a legacy database require you do rather complex mass data operations to bring columns up to your new spec. (Daylight Saving Time begins and ends on different days every year, so it’s not merely a matter of adding the zone.) Three DateTime Anti-Patterns In this section I’m going to talk about some common errors regarding handling datetimes and how to fix them. Datetimes are more complicated than even I am letting on here. But thankfully your friendly neighborhood library methods are here to help. The most complex time zone issues I’ve ever seen have involved multiple of the following sticking points. 1. An assumption was made about the zone of a datetime by one piece of a system that was not made by another These issues can often be spotted with a little back of the envelope knowledge about your local time zone. For instance if you live and work in the North American Eastern Time zone (ET), and you notice that the datetimes in your database are all off by exactly 4 hours (or exactly 5, in the winter) then you might have a datetime being accidentally converted from ET to UTC. 2. Time zone conversion was done manually This kind of error can serve to magnify the above error. Suppose you have one service that is operating in ET and a database that is storing UTC datetimes. You might think it’s sufficient to merely add four hours to the datetime, but this won’t work during the winter when ET switches from UTC-04:00 to UTC-05:00. Dynamically tracking when DST starts and ends adds additional complexity for you, because those dates are set every year (in the United States) by Congress. And of course, if the client service is ever redeployed in a different time zone this code will now fail not just in winter, but all the time. In general you should trust your programming language’s datetime representation to know what to do to correctly handle time zone conversions or other time difference calculations. 3. Time zones were dropped manually Sometimes when dealing with frontend code in different browsers datetime formatting can cause unexpected errors. For instance, in most browsers the alternate form of the ISO 8601 standard YYYY-MM-DDTHH:mm:SS±hhmm (without the colon) is accepted, but Safari is not one of them. This leads some developers to haphazardly chop out the time zones, reducing the accuracy of data when viewing your webpage internationally. This one should be a no-brainer in certain industries. For instance television, sports, and other live-streamed events absolutely positively need those time zones included for good user experience. Final thoughts If you can, use ISO 8601 with the time zone when communicating between services. When writing database code make sure the DAO is in charge of deciding what time zone to store data in. Don’t rely on upstream systems to translate datetimes into the DAO’s time zone. Avoid writing code that manually does time zone conversions, always trust the language library to do this for you rather than broadly assuming you need to apply a specific offset.", "date": "2019-04-19"},
{"website": "HBCTech", "title": "Presentations we love: 2019", "author": "Unknown", "link": "https://tech.hbc.com/2019-04-14-presentations-we-love.html", "abstract": "This is a list of our favorite presentations from 2019. We will update this list throughout the year. Mature Microservices and How to Operate Them Sarah Wells ( @sarahjwells ) at QCon London 2019 PID Loops and the Art of Keeping Systems Stable Colm MacCárthaigh ( @colmmacc ) at QCon New York 2019 Speeding Up Innovation Adrian Cockcroft ( @adrianco ) at CraftConf - May 2019 Cultivating Production Excellence Liz Fong-Jones ( @lizthegrey ) at QCon London 2019 ( slides ) Building and Scaling High Performing Technology Organizations Jez Humble ( @jezhumble ) at Agile India - March 2019 If You Don’t Know Where You’re Going, It Doesn’t Matter How Fast You Get There Nicole Forsgren ( @nicolefv ) at Google Cloud Next - April 2019 Safe Lambda Deployments Sean Sullivan ( @tinyrobots ) at Seattle Serverless Meetup - February 2019 Software Design in the 21st Century Martin Fowler ( @martinfowler ) at Etsy Engineering speaker series - February 2019 The AWS Billing Machine and Optimizing Cloud Costs at Stripe Ryan Lopopolo ( @lopopolo ) at DevOpsDays Seattle 2019 Etsy: Migrating a Monolith to the Cloud Keyur Govande ( @keyurdg ) at SREcon19 Americas - March 2019 Target’s Application Platform Jay Chandrashekaran and Jim Beyers at Google Cloud Next - April 2019 Applying the Serverless Mindset to Any Tech Ben Kehoe ( @ben11kehoe ) at ServerlessDays Boston - March 2019 How AWS builds Serverless Services Using Serverless Chris Munns ( @chrismunns ) at ServerlessDays Boston - March 2019 DevOps Vs. SRE: Competing Standards or Friends? Seth Vargo ( @sethvargo ) at Google Cloud Next - April 2019 Modern Continuous Delivery Ken Mugrage ( @kmugrage ) at DevOpsDays Copenhagen - April 2019 Progressive Delivery James Governor ( @monkchips ) at QCon London 2019 Testing in Production at Scale at Uber Amit Gud ( @amitgud ) - SRECon19 Americas - March 25, 2019 Driving Technology Transformation at WeWork Hugo Haas ( @whugoh ) at QCon New York 2019 How Retailers Prepare for Black Friday on Google Cloud Platform Kiran Davuluri, Jimit Ladha, and Andre Fatala at Google Cloud Next - April 2019 Surviving Black Friday: tales from an e-commerce engineer at Glossier Aaron Suggs ( @ktheory ) - PaymentsFn - April 17, 2019 Platforms at Twilio: Unlocking Developer Effectiveness Justin Kitagawa ( @justinmkitagawa ) at CraftConf - May 2019 Moving Fast at Scale Randy Shoup ( @randyshoup ) at CraftConf - May 2019 Scaling your Architecture with Services and Events Randy Shoup ( @randyshoup ) at GeeCON 2019 Cultivating Architecture Birgitta Böckeler ( @birgitta410 ) and Martin Fowler ( @martinfowler ) at CraftConf - May 2019 ( slides ) Database Schema Migrations with Zero Downtime Michiel Rook ( @michieltcs ) at DevOpsDays Copenhagen - April 2019 Observability: Superpowers for Developers Christine Yen ( @cyen ) at Monitorama PDX - June 2019 Taking Human Performance Seriously In Software John Allspaw ( @allspaw ) at Monitorama PDX - June 2019 Integration Testing with Docker and Testcontainers Kevin Wittek ( @kiview ) at JEEConf - April 2019 Scala Error Management: Future vs ZIO John De Goes ( @jdegoes ) at Dublin Scala User Group - May 2019 ( slides ) A Tour of Scala 3 Martin Odersky ( @odersky ) at Scala Days Lausanne 2019 ( slides ) Scala Best Practices Nicolas Rinaudo ( @NicolasRinaudo ) at Scala Days Lausanne 2019 Deconstructing the Monolith at Shopify Kirsten Westeinde ( @kmkwesteinde ) at Shopify Unite - June 2019 Bringing DevOps to the Database Baron Schwartz ( @xaprb ) at Percona Live - May 2019 ( slides ) Optimizing Database Performance and Efficiency Baron Schwartz ( @xaprb ) at Percona Live - May 2019 ( slides ) Develop Hundreds of Kubernetes Services at Scale with Airbnb Melanie Cebula ( @MelanieCebula ) at QCon London 2019 Adopting TypeScript at Scale (AirBnb) Brie Bunge ( @briebunge ) at JSConf Hawaii 2019 Docker and Java Mohammed Aboullaite ( @laytoun ) at Voxxed Days Singapore 2019 Reliable processing in a streaming payment system at Uber Emilee Urbanek ( @_urbs_ ) and Manas Kelshikar at MoneyCon 2019 Payments as a service at AirBnb Sophie Behr and John Chew at MoneyCon 2019 Evolution of Revenue Optimization at Dropbox Kirill Sapchuk and Evgeny Skarbovsky at MoneyCon 2019 Payment Transaction Routing at LinkedIn Tim Tan ( @tim-tan ) at MoneyCon 2019 Books: Scalable, Flexible, and Immutable Storage of Square’s Financials Anthony Bishopric ( @sentiental ) at MoneyCon 2019 Payment Network Tokens at Netflix Kasia Trapszo and Remi Duvergey at MoneyCon 2019 Streaming Your Shared Ride at Lyft Thomas Weise ( @thweise ) at Berlin Buzzwords 2019 Releasing the World’s Largest Python Site Every 7 Minutes (deployments at Instagram) Perry Randall at SREcon19 Asia/Pacific 2019 Kubernetes As Code Joe Duffy ( @funcofjoe ) at New York Kubernetes Meetup June 2019 Cloud Native CI/CD with Jenkins X and Tekton Pipelines Christie Wilson ( @bobcatwilson ) and James Rawlings ( @jdrawlings ) at QCon London 2019 ( slides )", "date": "2019-04-14"},
{"website": "HBCTech", "title": "Make Your Own Serverless CI", "author": "Unknown", "link": "https://tech.hbc.com/2019-04-08-make-your-own-ci.html", "abstract": "Automation is essential to maximizing throughput, especially when it comes to being able to confidently release quality software. I believe that anything you find yourself repeating is a great candidate to automate. In most cases, these repetitive tasks can be represented as simple functions! So that got me thinking… maybe I can leverage AWS Lambda for this– things like pull-request review hooks and automatic package versioning + publishing. In many ways, this is like creating a more powerful and customizable version of CodeBuild , CircleCI , or Concourse from scratch (all of which I’ve drawn inspiration from after using)! Moreover, this serverless solution costs nothing when it’s not in use and it’s much more flexible! Ultimately, this led to the creation of our own GitHub webhook “bot” we call Techie . In this post, I want to share how easy it is to create your own extensible bot! After we’re done, you’ll have a Techie look-alike that is configurable with it’s own YAML file and able to leave comments on untagged pull requests, perform CI code reviews, and automatically version and publish your npm packages! Feel free to view the completed source on GitHub . Overview For brevity, I’m only going to touch on the main aspects of our bot; you can reference everything else in the repo! Here’s how our bot will work: Lambda receives a request triggered by a GitHub webhook event Attempt to load and parse our bots config ( lambot.yml ) Run the configured actions Let’s get to the code! Making it Configurable I like the simplicity of YAML syntax, so we’ll expect the configuration to look something like this: hooks : # Automatic versioning & publishing semver : true # Prefer labels on pull requests; notify if missing. labels : true # Run commands on each push and sends a Github status codereview : commands : - echo Installing node modules... - npm i - echo Running tests... - npm test In our lambda function let’s do the following: Get the configuration, preferably from master or another predefined branch so we can consider it our config single source of truth . I’ve also experimented with using the config in the branch where the event is emitted; sometimes that works better! Parse the configuration and asynchronously run the tasks. Send a response (success or failure). const yaml = require ( ' js-yaml ' ); const tasks = require ( ' ./tasks ' ); // An object of supported tasks (functions) // Helper function that attempts to run a task function runTask ( name , taskData ) { // Ensure valid task provided if ( ! tasks [ name ]) return Promise . reject ( `Invalid task name of ' ${ name } ' provided.` ); return tasks [ name ]. call ( null , taskData ); } // This is our lambda function module . exports = async ( event , context , callback ) => { const response = JSON . parse ( event . body ); const repoOwner = response . repository . owner . login ; const repoName = response . repository . name ; // Retrieve lambot.yml // If missing, hook failure (lambot.yml is required) // Else, load and execute tasks const pendingTasks = await githubClient . get ( ' config ' , { repoOwner , repoName }) . then (( res ) => yaml . safeLoad ( Buffer . from ( res . data . content , ' base64 ' ))) . then (( config ) => Object . keys ( config . hooks ). reduce (( acc , hook ) => { // If hook is enabled, attempt to run it and push to pending task queue if ( config . hooks [ hook ]) { acc . push ( runTask ( hook , { githubEvent , response , config : config . hooks [ hook ] })); } return acc ; }, [])) . catch (() => { callback ( null , respond ( 500 , ' Missing or invalid lambot.yml. ' )); process . exit ( 0 ); }) // Wait for all tasks to run await Promise . all ( pendingTasks ); return callback ( null , respond ( 200 , ' Success ' )); } I know this is a lot of code to digest, so let’s recap how this will work: We’re expecting a YAML configuration ( lambot.yml ) to be present in the repository. If a configuration is missing we’ll respond with a failure, otherwise, let’s try to run the specified tasks asynchronously (they each return a promise). Wait for all tasks to run, then send a final response. Easy enough! Now you might be wondering what a task function looks like… so let’s keep going. Creating Tasks You might have noticed our example config includes three hooks… semver , labels , and codereview . Let’s breifly go through how they will work, starting with the simplest– labels. Label Task Pull-request labels help us stay organized. The purpose of our label task will be to post a friendly comment on newly opened, unlabeled pull-requests, encouraging the user to add a label. Here’s the code: // Newly opened pull-requests without labels will receive a friendly notice async function labelTask ({ githubEvent , response }) { if ( githubEvent === ' pull_request ' && response . action === ' opened ' && response . pull_request . labels . length === 0 ) { const repoName = response . repository . name ; const repoOwner = response . repository . owner . login ; const prNumber = response . number ; // Post comment using the GitHub API await githubClient . post ( ' comment ' , { repoOwner , repoName , prNumber , message : ' _Hi :wave:, it \\' s Lambot!_ \\n\\n ' + ' I noticed this pull request has no assigned labels. :cry: \\n\\n ' + ' Please remember to label your pull requests. That helps keep things organized around here. :slightly_smiling_face: ' }) } return Promise . resolve (); }; Semver Task Versioning and publishing npm packages can become quite monotonous. That’s when I got the idea… wouldn’t it be great if we could handle that automatically based off the head commit (it includes the title + commit history if you’re squashing commits), and look for [breaking|feature] (or [ci skip] ). For example, a pull-request with [breaking] Major API changes would cause our package to increment and publish a major version. Now, one challenge when working with Lambda + API Gateway is that your task must complete within 30 seconds . Even if we could install, lint, and test our code in 30 seconds I don’t like the possibility of random timeouts… so this is where we’ll get fancy ✨.​ For tasks that require heavy lifting, we can offload them to AWS ECS– which can run as long as they need, in any environment we want, only when we need them, therefore being cost-effective. Fantastic! Here’s what we’ll do: const BOT_NAME = ' lambot ' ; // Automatically version and publish repositories that are npm packages async function semverTask ({ githubEvent , response }) { // Only listen to push events on master (we don't want to version + publish every branch!), // ignore commits from our bot (to avoid an infinite loop of versioning + publishing) // ignore [ci skip] commits if ( githubEvent !== ' push ' || response . head_commit . author . username === BOT_NAME || response . head_commit . message . includes ( ' [ci skip] ' ) || response . ref !== ' refs/heads/master ' ) { return Promise . resolve (); } // get_version and push_changes are helper functions baked into our Docker image const commands = [ ' echo \"Installing node modules...\" ' , ' npm ci ' , // Determine next version ' increment=$(get_version) ' , // Create versioned commit; publish package; push versioned commit & tag ' echo \"Bumping version (type: $increment), publishing, and pushing...\" ' , ' npm version $increment ' , ' npm publish ' , ' push_changes ' ]; // Run the semver utility in our custom docker image return spawnDockerTask ({ GIT_REPO : response . repository . clone_url , GIT_BRANCH : response . ref , DOCKER_COMMANDS : commands . join ( ' && ' ) }); }; In our semver task, we’ll call spawnDockerTask (a utility for running a task in ECS) and pass a list of commands to execute in our container, as well as any additional information our container will need. If the task starts successfully, spawnDockerTask will return a resolved promise. Adding the Webhook Finally, we need to setup a repository to use our new bot. This is the easiest part! Go to your Github repositories Settings > Webhooks and add a new webhook with the following: Payload URL: https://your-api-gateway-url/webhook Content type: application/json Secret: your secret (see: ./lib/config.js ) Events: Send me everything. (or select only the events you are using) Wrapping Up Here’s a fun exercise: try implementing the codereview task yourself. Hint: You’ll need to use the config property passed to our task function. With codereview , the goal is to allow a custom sequence of commands to run on each pushed commit. It’s similar to our semver task, but it also utilizes the status API to report if the commands pass or fail. Once you do that, you’ll practically have built your own CI tool! How cool is that?! Automation is fun and it maximizes productivity; a win-win if you ask me! We also learned about Lambdas, ECS, and the GitHub API. Now you are fully equipped to create your own custom tasks! Feel free to reference the GitHub API for all the different events you can use. What’s even better is that your bot can run in any repository (as long as it has access and a valid config file)! What else is there to automate? That’s now up to you! Thanks for reading. Originally published at beuteiful.com on April 5, 2019.", "date": "2019-04-08"},
{"website": "HBCTech", "title": "Small batch deployments", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2019-04-02-small-batch-deployments.html", "abstract": "The Hudson’s Bay Company is an organization known for challenging the status quo. HBC’s engineering group is no different. HBC engineers are constantly looking for ways to improve the speed and quality of the software that runs our business. Continuous Delivery Like many organizations, we have improved our systems by adopting Continuous Delivery practices. Continuous Delivery isn’t a new concept. But it does take time for organizations to embrace all of the principles and to achieve its full potential. Work in small batches One of the core principles of Continuous Delivery is “work in small batches”. Engineering teams that focus on small batch delivery can achieve higher quality and better efficiency. Small Batch Deployments HBC employs a technique that we call “small batch deployments”. Our engineering teams favor small code deployments over large code deployments. The benefits of small deployments are self-evident: small deployments are easier to understand small deployments are easier to review small deployments are easier to test small deployments are easier to rollback Small Batch Success HBC is not the only organization that practices small batch deployments. This approach has proven successful at Facebook , Instagram , Flickr , Etsy , Starbucks , and Flow Commerce . The notion of “small batches” has been discussed in software development literature for over a decade. The following presentation illustrates how prevalent “small batch” has become in the field of software engineering: Final thoughts The practice of “small batch deployments” has proven successful at HBC. Our engineering teams have been able to deliver higher quality systems with less risk to the business. This approach benefits both our engineers and our business stakeholders.", "date": "2019-04-02"},
{"website": "HBCTech", "title": "Presentations we love: 2018", "author": "Unknown", "link": "https://tech.hbc.com/2018-12-31-presentations-we-love.html", "abstract": "2018 was a year of growth and learning at HBC Tech. Our organization embraced new technologies and new ways of building application software. As the year comes to an end, let’s recognize some notable technical presentations from 2018. Design Microservice Architectures the Right Way Michael Bryzek ( @mbryzek ) at QCon New York 2018 Closing Loops and Opening Minds: How To Take Control of Systems Colm MacCárthaigh ( @colmmacc ) at AWS re:Invent 2018 Monoliths, Migrations, and Microservices Randy Shoup ( @randyshoup ) at Reactive Summit Montreal 2018 Scaling Your Architecture with Events and Services Randy Shoup ( @randyshoup ) at Build Stuff 2018 Attitude Determines Altitude - Engineering Yourself Randy Shoup ( @randyshoup ) at YOW! Conference 2018 Microservices at HBC Sean Sullivan ( @tinyrobots ) at Portland Java User Group September 2018 The Data Behind DevOps: Becoming a High Performer Dr. Nicole Forsgren ( @nicolefv ) at DevOps Enterprise Summit London 2018 You only have to change one thing to do the DevOps Ken Mugrage ( @kmugrage ) at DevopsDays Oslo 2018 No microservice is an island Michele Titolo ( @micheletitolo ) at QCon New York 2018 Lyft’s Envoy: Embracing a Service Mesh Matt Klein ( @mattklein123 ) at QCon New York 2018 Try Catch Blocks for your Distributed System Cecilia Deng ( @cicikendiggit ) at REdeploy 2018 AWS SDK for Java 2.0 Sean Sullivan ( @tinyrobots ) at Portland Java User Group August 2018 Testing in production at LinkedIn Szczepan Faber ( @mockitoguy ) at Devoxx Poland 2018 Run Less Software Rich Archbold ( @rich_archbold ) at DevOpsDays Galway 2018 ( slides ) Effective Scala: Reloaded Mirco Dotta ( @mircodotta ) at Voxxed Zurich 2018 Building a Social Graph at Nike with Amazon Neptune Marc Wangenheim ( @mwangenh ) and Aditya Soni ( @adityasoni84 ) at AWS re:Invent 2018 High Reliability Infrastructure Migrations Julia Evans ( @b0rk ) at KubeCon Seattle 2018 Clouds with Silver Linings: Maximising Learning From Incidents Ian Malpass ( @indec ) at DevOps Minneapolis August 2018 Web performance made easy Ewa Gasperowicz and Addy Osmani at Google I/O 2018 Java API Design Best Practices Jonathan Giles ( @jonathangiles ) at Devoxx Poland 2018 CI/CD for Serverless and Containerized Applications Clare Liguori ( @clare_liguori ) at AWS re:Invent 2018 Serverless Architectural Patterns and Best Practices Drew Dennis and Maitreya Ranganath at AWS re:Invent 2018 Kafka and Event-Oriented Architecture Jay Kreps ( @jaykreps ) at Kafka Summit SF 2018 Is Kafka a Database? Martin Kleppmann ( @martinkl ) at Kafka Summit SF 2018 FiloDB: Real-time, In-Memory Time Series at Massive Scale Evan Chan ( @evanfchan ) at Scale By The Bay 2018 Build containers faster with Jib Q Chen ( @coollog ) and Appu Goundan ( @loosebazooka ) at Velocity Conf June 2018 Kubernetes Is A Platform Platform Joe Beda ( @jbeda ) at DevOpsDays Seattle 2018 Democratizing Distributed Systems Brendan Burns ( @brendandburns ) at GOTO Amsterdam 2018 The Great Migration: from Monolith to Service-Oriented at AirBnb Jessica Tai ( @jessicamtai ) at QCon San Francisco 2018 From Monorail to Monorepo: Airbnb’s journey into microservices Jens Vanderhaeghe ( @jvanderhaeghe ) at GitHub Universe 2018 Full Cycle Developers at Netflix Greg Burrell ( @gburrell_greg ) at QCon San Francisco 2018 Taming polyglot development using Docker at Netflix Nadav Cohen ( @nadavc ) at Productivity Engineering Silicon Valley meetup - February 2018 Why You Need A Software Delivery Machine Rod Johnson ( @springrod ) at GOTO Copenhagen 2018 Paying Technical Debt at Scale - Migrations at Stripe Will Larson ( @lethain ) at QCon San Francisco 2018 How AWS Minimizes the Blast Radius of Failures Peter Vosshall ( @petervosshall ) at AWS Re:Invent 2018 Stealing The Best Ideas From DevOps Thomas Limoncelli ( @yesthattom ) at DevOpsDays NYC 2018 Productivity Engineering at Google Michael Bachman at Productivity Engineering Silicon Valley meetup - April 2018 Big Data, Fast Data @ Paypal Sid Anand ( @r39132 ) at YOW! Conference 2018 Velocity at scale - principles to live by Ben Mackie ( @bdmackie ) at YOW! CTO Summit 2018 Working at Netflix Brendan Gregg ( @brendangregg ) at YOW! CTO Summit 2018 Preparing for Scala 3 Adriaan Moors ( @adriaanm ) and Martin Odersky ( @odersky ) at Scala Days New York 2018 Functional Programming with Effects Rob Norris ( @tpolecat ) at Scala Days New York 2018 Cats Effect: The IO Monad for Scala Gabriel Volpe ( @volpegabriel87 ) at ScalaIO FR 2018 From Fast-Data to a Key Operational Technology for the Enterprise Colin Breck ( @breckcs ) at Reactive Summit 2018 DevOps at Uber Kiran Bondalapati ( @bondlog ) at Code-Conf 2018 ( slides ) Enabling Microservices at Apple Scott Mitchell ( @scott_mitch_ ) at Devoxx Belgium 2018 Strings are evil: methods to hide the use of primitive types Noel Welsh ( @noelwelsh ) and Adam Rosien ( @arosien ) at ScalaDays Berlin 2018", "date": "2018-12-31"},
{"website": "HBCTech", "title": "Microservices at HBC", "author": ["Sean Sullivan"], "link": "https://tech.hbc.com/2018-10-04-microservices-at-hbc.html", "abstract": "HBC’s commerce platform is constantly evolving to support new business requirements. Our platform’s microservice architecture enables engineering teams to rapidly build and deploy new features. In September, HBC engineers Fabrizio Fortino and Sean Sullivan gave presentations about the past, present, and future of microservices at the Hudson’s Bay Company. Event Driven Microservices Dublin Microservices User Group September 27, 2018 Fabrizio Fortino ( @fabriziofortino ) Microservices at HBC Portland Java User Group September 25, 2018 Sean Sullivan ( @tinyrobots )", "date": "2018-10-04"},
{"website": "HBCTech", "title": "Email: from macro-service to micro-service", "author": ["Kinshuk Varshney"], "link": "https://tech.hbc.com/2018-09-24-email-from-macro-service-to-micro-service.html", "abstract": "Email Engineering recently migrated all transactional emails to CNS v2. This was a slow migration as CNS v1, although, referred to as a micro service was anything but one! We inherited CNS v1 towards the end of last year and started migration to CNS v2 in the beginning of this year. In doing so, we followed what could be labeled as a flavor of Strangler pattern . In this blog, I will share how we accomplished this task and also provide a detailed view of the new CNS (CNS v2). CNS HBC Customer Notification Service (CNS) processes and sends transactional emails for Saks, Saks Off 5th and Lord and Taylor banners. CNS consumes events from Order Management System (OMS), renders the event payload into emails and sends them via CheetahMail. CNS also sends SMS notifications via Vibes SMS for applicable events and users. With CNS v2 and consequently the breakdown of v1 monolith into true micro services, the acronym CNS now stands for Customer Notification “System”. CNS v2 CNS v2 is developed for the Cloud and is 100% hosted on AWS. It breaks down the v1 monolith into three micro services, namely- svc-mq-consumer , api-customer-notification and lib-email-render . The v2 architecture and the three new components are explained below. Architecture Svc-mq-consumer This component is an IBM MQ message handler/consumer. After polling for messages from the queue, it POSTs the message payload to the api-customer-notification’s /send_email endpoint. Api-customer-notification This component, written as a Lambda function, exposes the CNS API. After receiving a /send_email request from the consumer, the api invokes the lib-email-render Lambda function for rendering it into HTML. On receiving the HTML response, the api’s thin Email Gateway wrapper sends the HTML payload with relevant data to CheetahMail. The api stores both the message payload received from the consumer and the final rendered email HTML in Aurora MySql DB. It also exposes this data as a powerful dashboard for troubleshooting and QA testing. As the dashboard is exposed to all HBC and is used by multiple teams, it is gaining popularity and traffic and now merits to exist on its own as a web component in the coming days. Lib-email-render The lib Lambda function holds all the business logic needed to render an email HTML from an event message. Most of the business logic from CNS v1 is migrated to this component. Strangling CNS v1 Phase 1 - Using CNS v1 As Message Consumer and Repeater Once CNS v2 started taking shape, the team discussed steps to phase out CNS v1. First, we decided that this should be a banner by banner transition. This would help in streamlining the team’s efforts and also prevent business impact across banners if something were to break. Next, we agreed on using CNS v1 as simply a message consumer, which would receive the message off the IBM MQ queue and send the payload to CNS v2’s api-customer-notification component. This meant that our two core components (api and lib) were in play and the team could focus its efforts on developing and testing those. We launched both Saks and Lord and Taylor on CNS v2 in Production with this setup on Apr 5. Phase 2 - Using svc-mq-consumer As Message Consumer for O5 Once Saks and Lord and Taylor were on CNS v2 (using CNS v1 as message consumer), we focused our attention to migrate Saks Off 5th to CNS v2.\nBy now, the two new CNS v2 components (api and lib) were stable in Production (for Saks and Lord and Taylor). We took some time to enhance CloudWatch logging and metrics for both these new components. We also enhanced the api dashboard in this time to facilitate troubleshooting and added features such as email preview and resend. At this time, the team felt confident to undertake a full migration for Saks Off 5th, including replacing CNS v1 with our brand new svc-mq-consumer component. Since CNS v1 was not used in this flow, we had to shut down CNS v1 completely for Saks Off 5th. This was essential as the underlying infrastructure- the queues, OMS - were common to both CNS v1 and v2 and having v1 linger around could lead to unexpected results. O5 migrated completely on CNS v2 in Production on Aug 14. Phase 3 - Shutting Down CNS v1 and Clean up By now, all three CNS v2 components were taking live Production traffic.The team took some time to put CW logging and metrics in the consumer component. Finally, we decided it was time to shut down CNS v1 for good. Time to draw the curtains, roll the credits! It was great knowing you, CNS v1! Three teams- Engineering, QA and Infra, joined forces to co-ordinate this effort. It was crucial that old CNS v1 Docker instances were removed, new CNS v2 components were enabled and their connectivity with queues etc. were established and verified. Also, the end to end flow from order placement on website to receiving emails was to be verified. The Go pipelines that were used to build and deploy code for CNS v1 were to be paused and subsequently disabled (so that no one is able to resurrect CNS v1 even by mistake). Finally, the nagios alerts for CNS v1 were disabled and its Mongo DB servers decommissioned. CNS v1 was shut down for all banners on Sep 6. CNS v2 CI and CD CNS v2 follows Continuous Integration (CI) and is set up for Continuous Delivery (CD), but the latter is currently disabled. Two pieces need to be in place for us to push code to Production with a high degree of confidence- A reliable infrastructure, with high availability and scalability, and a development workflow with adequate checks and measures that guarantees only high quality, bug free code reaches Production. While hosting all of CNS v2 in AWS takes care of the robust infrastructure piece, it is the development workflow that the team is working to continuously improve. When we inherited CNS v1, the unit tests’ code coverage was barely 40%. Today, CNS v2 has close to 75% code coverage (and is increasing every day). The CloudWatch (CW) alerts are set up by banner and by email type and are integrated with PagerDuty. We are in the process of improving our metrics in CW so that if incidents happen, we are able to quantify their impact with precision. We are also integrating our Lambda deployments with New Relic so as to provide greater visibility to SRE and Ops teams. CW Logging is already in place for all components. We feel that we are almost there and should be able to enable CD in the next couple of weeks. Final Thoughts While we are forever in debt of the old CNS, the new CNS opens up a world of possibilities. All transactional emails across all banners are getting a fresh, modern look as I write this, made possible with new found powers of CNS v2. Bay emails migration to CNS follows soon. Business (Marketing) loves the new CNS and they feel emboldened to try out new features and products in the coming days and months. The team has a spring in its stride and has so much to look forward to. Happy days lie ahead filled with all good things!", "date": "2018-09-24"},
{"website": "HBCTech", "title": "Would You Rather be Awesome or Deadly?", "author": "Unknown", "link": "https://tech.hbc.com/2018-07-30-hbc-health-checks.html", "abstract": "Over a year ago we began to experiment with new ways of helping our teams in finding their next improvement opportunities. We started with an assumption that while every team is unique, there must be a way to approach a “team health” conversation in a similar fashion across them all. By using a standardized assessment we also expected to find some areas in which organizational improvements would be beneficial. In our quest for a suitable tool, we borrowed, fine-tuned and iterated to finally arrive at the health checks that fit our needs. This blog post is a reflection on the evolution of team health checks in HBC. Searching for the right tool Basic Google search came back with a number of options for measuring team effectiveness. Eliminating paid options and the options described in a very dry corporate language, we ended up looking at the good old Spotify Squad Health Check described by Henrik Kniberg and the gamified Kahoot! version of it shared by Yuval Yeret. While both authors described the use of these assessments for co-located teams, the Kahoot! version had all the potential to be a good fit for distributed teams as well. It’s worth noting that while the majority of our teams are co-located, it is not unusual for any of them to have a team member who one day would need to work from home, from a hotel room in a different country or even from a boat! Naturally, our tools have to be flexible enough and work equally well for people “in the room” and for their “accidentally-virtual” colleagues. While by itself Kahoot! doesn’t generate any visual graphs, with a bit of conditional formatting and macros one can easily turn the raw data extracted into something more visually engaging. The first round of Kahoot! Health Checks was kicked off with 8 teams in September 2016. Based on their feedback, the following four qualities of this approach stood out the most: Fast Nobody likes meetings. Luckily, using Kahoot! we could shorten the time needed to run health checks to 15 min, including the time to generate a chart. Followed by a focused 15 min mini-retrospective with the participants, in only half an hour we were able to discover and zoom into the biggest problem areas of a team. Anonymous One may argue that team members should feel psychologically safe and be open enough to take this survey using their real names. When running this health check, we observed a mix of safety levels. In some teams, the real names were used; for others the ability to use “nicknames” really enabled the openness of their responses. For many team members, though, this feature provided an extra opportunity to have fun and show off their creativity in coming up with funny nicknames. Fun Gamified format, engaging music, bright colors and fun pictures - these are just a few elements that make this survey fun. Unlike some of the traditional surveys that are taken by each team member in isolation, this one brings the entire team together for a shared experience and laughter. Bonus feature - this shared experience increases the feeling of psychological safety, preparing the team for deep conversations over survey results. Accurate In spite of a relatively simplistic, fast and humorous approach, the results of the surveys turned out to be pretty accurate, highlighting the areas of improvements and the areas that the teams felt good about. Fine-tuning for the cultural fit To our biggest surprise, as much as the teams in US supported our first experiment with the health checks, the teams in Ireland cringed at it. As it turned out, the language of the response options (“Awesome”, “Crappy” or “Meh”) which we copied from the original Spotify squad check, didn’t really resonate with them. It was the time for us to customize the health check for a cultural fit and learn some Irish slang ! We polled our Dublin team members to find the Irish equivalents of the response options we needed. This was when I discovered that “ Grand ” is just “OK or Meh” in Dublin; “ Deadly ” is actually pretty Good , even Excellent ; whereas “ Banjaxed ” is “ Broken, beyond repair ”. With this fine-tuning we also replaced “Players vs. Pawns” category with “Autonomy”, which in our opinion better reflects the intent of this statement: \"We are in control of our own destiny. \n We decide what to build and how to build it for the best impact.\" The next version of the health checks was taken by 21 teams from Dublin and New York and completed by October 2017.  For the first time in the HBC Tech history a comprehensive view into the health of our teams across the org was created and provided the insights into some of the systemic issues in organization. Improving the teams and the organization Making things visible was just a first step on our bumpy road towards improvement. Next we ran team-level retrospectives, shared the summary view with the entire tech team and facilitated an Open Space event . Open Space As I was walking the circle, introducing the group to Open Space, I was a bit nervous. This was another big experiment for HBC. How will the group take it? What if there would be an awkward silence and nothing would get proposed as topics for our theme “ How do we get better at Speed and Ease of Release?” The magic of Open Space did not disappoint again this time!! The moment an invitation to propose the topics was announced, people jumped from their seats and ran to grab the paper and markers. It was almost like they were finally given the stage and opportunity to talk about their passion! We only used four rooms and short twenty minutes time slots. And yet, in less than 3 hours we were able to uncover a ton of ideas, build connections and form “task forces” to continue beyond the Open Space. The equalizing power of a circle, freedom of choice where to participate, five principles and the law of mobility created a space for new ideas to develop and passion to emerge If you haven’t tried an Open Space before, I would encourage you to learn more about it, look for an opportunity to experience it at a conference as well as reach out to me or other Open Space facilitators with questions. It can really make a difference in your organization! Leadership Kanban board This Open Space brought together people from all levels of our engineering organization. As action items started to develop post-Open Space a number of them ended up on “leadership plate”. To be more precise, on the leadership Kanban board. This was actually pretty cool to experience this level of visibility and observe the real time progress. Iterating to make it our own Six month later, we were getting ready for the next round of checks. We wanted to get insights into some of the new issues that were on the top of everyone’s mind at that moment. Path to Mastery At that time, organization was in the middle of re-defining career structure . With the addition of this new category we were hoping to address the following questions: How effective are we in creating an environment for our team members to experiment with different roles? Are we enabling T-shaped skills development or boxing team members into narrow roles? Or in the healthcheck terms: Awesome/Deadly: “I feel supported and encouraged to experiment with different roles. My options are open and clear.” Crappy/Banjaxed: “I feel like I’m stuck in a narrow role, my career options are limited or unclear.” Psychological Safety Most of you have heard of Google’s Project Aristotle by now. At HBC we wanted to get some insights, have WE got what it takes? How are our teams doing on Psychological Safety? Is this the area we need to pay attention to? Or in the healthcheck terms: Awesome/Deadly: “In my team I can take risk, be myself and speak my mind without fear of negative consequences to my image, status or career.” Crappy/Banjaxed: “I avoid voicing my opinion when it differs from what the team thinks. It is safer to “go with the flow” in my team. Side effects from the new health checks As we’ve taken the teams through the new round, we’ve noticed many more “Aha! Moments” in team discussions. \nHaving clear definition for what we consider “Awesome/Deadly” vs. “Crappy/Banjaxed” helped the team members see opportunities in modifying their own actions. Just calling out “Psychological Safety” as one of the categories, helped teams realize the value it has and enabled deeper conversations. “Path to Mastery” discussion inspired the teams to consider more pairing, organize learning sessions and engage in other intentional T-shaping activities. Fun fact - a number of the teams realized that they’ve forgotten about the team outings and agreed to schedule one in the immediate future. If nothing else, we’ve made a positive contribution to local economy! Parting words I am glad you made it this far! Thank you for reading, it’s been a long one. \nHope learning about our experience got you at least curious. As I leave you ready to experiment with YOUR health checks, I’d love you to remember these four points: Don’t overthink it. Pick a tool and give it a try. Get the feedback and make the tool relevant. Expand your focus and find new areas for improvement. Don’t forget to have fun - teams that play together, stay together!", "date": "2018-07-30"},
{"website": "HBCTech", "title": "Building A Better Keyboard Navigation", "author": "Unknown", "link": "https://tech.hbc.com/2018-05-30-ada-accessibility-and-keyboard-navigation.html", "abstract": "The simplest way to understand the importance of web accessibility is to open a web browser, put on a blindfold, and try navigating a website. Despite a small percentage of users with disabilities, their human right to navigate the internet still stands. In this post I’ll share some of my learnings from making our navigation more accessible. W3C Recommendations for Accessibility Not familiar with this type of work, I referred to WAI-ARIA Authoring Practices , and found the following recommendations helpful. Just in case, WAI stans for “Web Accessibility Initiative”, and ARIA stands for “Accessible Rich Internet Application”. Leverage WAI-ARIA Roles, States, and Properties Manage Focus Inside Composite Elements All Components Need To Be Reachable Via The Keyboard 1. Leverage Roles, States, & Properties Landmark roles are defined by many of the HTML5 elements . For example, the <nav> element gets the aria role navigation , by default. These default roles aid assistive technologies used to browse web pages, but to make our markup more accessible, we need to also utilize aria states. Aria states like aria-haspopup or aria-expanded are what every screen reader dreams of. Using aria-haspopup lets screen readers know there’s a submenu available. The aria-expanded attribute will indicate to a screen reader that a menu is expanded or callapsed, it’s also great for applying UI changes with CSS, ie; opening and closing flyout menus that are being interacted with. .nav_link [ aria-expanded = true ], a :hover { display : block ; } When testing with screen readers, there were some less than useful audio feedback around the number of items in a popup menu. The screen reader didn’t make it completely clear how many items were being displayed in a submenu. This was caused by various levels of nested <ul> elements. Stuck with our existing HTML markup for now, changing the role of each anchor element from link to menuitem produced more relevant audio feedback. I raised the idea of ditching the traditional navigation <ul> markup in favor of a <span> containing a bunch of <a> elements. Given everything has the correct aria attributes, I didn’t forsee any issues, but it seems the internet still favors <ul> for nav structures. 2. Manage the Focus Ring. (own it, don’t hide it) Not every element on a page needs to be in the “tab order”, but all interactive elements should be focusable through scripting. It’s obviously not a good idea to manually set the ‘tab-index’ property, but setting it to ‘-1’ allows us to focus that element with javascript. Regardless if it’s a focusable elemement by default. Changing tab-index from -1 to 0, or using the “Roving Tab Index”, is a great way to manage the focus ring and tab sequence. This also helps isolate parts of a form or a page into focusable groups, minimizing the number of tab stops to navigate. The other way is using the aria-activedescendant , but the benefit of tab-index is the user agent will scroll to bring the element into view if it’s not. WAI-ARIA Authoring Practices recommends the tab sequence should include only one focusable element of a composite UI component. Or, the element that is to be included in the tab sequence has tabindex of “0” and all other focusable elements contained in the composite component have tabindex of “-1”. For example, a nav item in a menu bar. Once a composite component contains focus, the menu bar in this case, pressing the enter key will shift focus to the first element inside of it and keys other than Tab and Shift + Tab will move focus among its focusable elements. See the section on “expected keyboard navigation” below. The important thing to highlight here, is that this technique removes unneccesary elements from the natural tab sequence, simplifying the user experience for keyboard users by not focusing every single element while tabing through a page. Instead, users can tab from component to component, choosing to dive deeper or move on. This creates a more efficient navigation and limits the number of key presses required to get to a specific part of the page. If a user is using a mouth stick to type, this is extemely helpful. You can learn more about this technique in 5.5 Keyboard Navigation Between Components (The Tab Sequence) and 5.6 Keyboard Navigation Inside Components . 3. Create Expected Keyboard Navigation When the user decides to dive deeper into a composite component, there are some standard key strokes and expected functionality. Some of these may not be familiar to mouse users, but are to users relying on the keyboard. W3C specifies the following keys and actions when developing a keyboard interface. A few have optional recommendations, but specify that it’s up to the author to decide. Bottom line here is to maintain a consistent functionality across your application and its composite components. Also, making sure to move the focus ring in an expected direction or location. Key Action Space or Enter Opens the submenu and places focus on its first item. Down Arrow Opens its submenu and places focus on the first item in the submenu. Up Arrow Moves focus to the previous item, optionally wrapping from the first to the last. Right Arrow Moves focus to the next item, optionally wrapping from the last to the first. Left Arrow Moves focus to the previous item, optionally wrapping from the last to the first. Home Moves focus to the first item in the current menu or menubar. End Moves focus to the last item in the current menu or menubar. Any key that corresponds to a printable character Moves focus to the next menu item in the current menu whose label begins with that printable character. Escape Closes the menu that contains focus and return focus to the element or context Tab Moves focus to the next element in the tab sequence, and if the item that had focus is not in a menubar, closes its menu and all open parent menu containers. Shift + Tab Moves focus to the previous element in the tab sequence, and if the item that had focus is not in a menubar, closes its menu and all open parent menu containers. For the full spec on keyboard navigation, you can refer to the W3C spec here . Implementation Tid Bits We started with the example scripts that W3C provides in their menubar demos . These are free to use, fairly simple to follow and made a great starting point. The main class is applied during the React lifecycle at componentDidMount() and passed our navigation <ul> . It simply traverses the DOM for <ul> , <li> and <a> elements setting the appropriate states and attributes for each. We added a data-column attribute to our react template to help traverse back up the dom tree to easily provide users with the ability to navigate between columns using the left and right arrow keys. Below is a quick screen capture demonstrating the roving tab index technique. Challenges In our submenus, few links in our small breakpoint menu are hidden from our large breakpoint. We needed a setTimeout() to put a break in the order of operations so the browser would parse the CSS and then apply our keyboard navigation code. This allowed us to use ‘getComputedStyle()’ to skip over any elements that had display: none; . Otherwise, the hidden elements broke our tab sequence because the script indexed the hidden anchor elements in the DOM. Because they were hidden there was nothing to focus next in the tab order. Another setTimeout() was needed to properly focus the first item when a submenu was opened. This was because of our “fancy” transition on the submenu <ul> from opacity: 0 to opactiy: 1 . A similar problem again, focusing an item isn’t possible if it’s hidden, so we needed to pause our script to let that transition to run, allow the browser to render the changes and report them back, allowing javascript to get the latest, updated styles. Having 2 actions for each nav item, one for the category landing page, and the other for the popup. This isn’t apparent when using a mouse and having the hover effect, but with just a keyboard, it needs to be clear to the user that there are 2 options. For now, we’ve styled the element with a down arrow when it’s in focus, gave the element a role of link , and an attribute of aria-haspopup . This provides visual and audio feedback. Optionally, and perhaps a future improvement would be to create a second element to focus and control the popup menu, separating it from the link to the category page. The W3C provides their own guidance on this here . Smaller breakpoints still need to be included and we have to go back and work on this. It’s possible a user may have a bluetooth keyboard connected to a tablet. Conclusions & Next Steps It’s fairly easy to make your site’s navigation more keyboard friendly and there are some great resources from the Web Accessibility Initiative that will help you do it. Going through this exercise exposed a few areas we could apply this same logic and improve our site’s overall accessibility. This work also highlighted the need to internally communicate this perspective in our styleguide so our design and development teams can make sure we build things in an accessible way. We’ll be launching our new header for thebay.com in the coming months and eventually to all our sites. Web Accessibility is a human right Create efficient keyboard navigation patterns Limit the number of key presses required to get to a specific part of the page So, if you’re updating your site’s navigation, hopefully what we’ve shared from our experience will help you make your nav more accessible to users who rely on a keyboard to surf the web.", "date": "2018-05-30"},
{"website": "HBCTech", "title": "ODSC Workshop on Experimental Reproducibility in Data Science", "author": ["Karthik Rajasethupathy", "Jason Tam"], "link": "https://tech.hbc.com/2018-05-07-experimental-reproducibility-in-data-science.html", "abstract": "On May 2nd, we presented at the Open Data Science Conference in Boston, MA. We demonstrated\nhow to build a machine learning project from scratch with Sacred , an open source\nlibrary for experiment tracking, and how to view the results using Sacredboard . Workshop Abstract There are ways to incorporate experimental reproducibility into machine learning projects that are clean and lightweight.\nIn this introductory level workshop, we demonstrate how to use Sacred to motivate reproducible research and\nexperiment monitoring in machine learning. We discuss how this enables any data scientist to provide a solution\n(a model or set of predictions) to any problem, compare their solution to previous models results on the same test\ndata, and select the best model for production. Finally, we provide examples of machine learning problems in retail\nand demonstrate how data scientists can easily work across multiple problems. What is Experimental Reproducibility Specifying the inputs, contexts, and steps involved in producing a result such that one can execute those\ninstructions and produce the same result. Machine Learning is Experiments You might be thinking, what is the connection between machine learning and experiments? Well, every time we build a\nmodel, we’re making hypotheses on which data to use for training and testing, pre-processing steps to apply, features\nand/or architectures to engineer, and learning algorithm(s) to use that will best fit our training data and generalize\nto unseen data. Additionally, we’re running these experiments from dynamic and (sometimes) complex code bases, on computing environments\nwith a whole another host of specifications. So, while we are not working in a wetlab, we’re definitely running experiments and making many decisions that should\nbe recorded! Why do we need an approach to this Adhoc efforts at tracking experiments are incomplete , and messy . Incomplete : It becomes very hard to annotate everything that we want to track: the version of the code we’re\nrunning; logging the config that we’re using for a particular run of an experiment (from the steps used for\npreprocessing to the hyperparameters used in the model); the specifications of the host where the experiment is run;\nand so on… Messy : We want this information to be logged in such a way that we can easily add new parameters to track (without\nhaving to change a lot of code) and search through experiments that we’ve already tried. Looking through pages of a\nnotebook, or scanning excel sheets stored in various folders is not an efficient and desirable way to do this. What should we track to make Machine Learning reproducible? We suggest tracking the following: Version Control: What is the git hash of the repository when we run the experiment? Is the code that was run in a dirty state (some local changes)? To take this a step further, can we just store the source code that we ran? Config: This might include (but is not limited to) which data is loaded for training/testing, preprocessing steps,\nlearning algorithms, hyperparameters, etc Seed: Set/store a global seed so that any functions that have some randomness yield consistent results. Results: Store the performance of each run so that we can compare different experimental runs\nand select the best model. For certain models, we may also want to track its performance for each training\nstep. Enter Sacred Sacred makes this possible. Sacred is a tool that is designed to introduce experimental reproducibility into\nprojects with very little overhead. There’s 3 actors in the Sacred ecosystem: Ingredients , Experiments , and Observers . We can define Ingredients : from Sacred import Ingredient name_ingredient = Ingredient ( 'name' ) @ name_ingredient . config def config (): first = \"Jane\" last = \"Doe\" @ name_ingredient . capture def fullname ( first , last ): return \"Jane\" + \" \" + \"Doe\" We can define an Experiment that uses Ingredients to produce some outcome that we wish to record: from Sacred import Experiment from my_ingredients import name_ingredient , fullname greeting_ex = Experiment ( 'greeting_experiment' , ingredients = [ name_ingredient ]) @ greeting_ex . config def config (): greeting = \"Hello\" @ greeting_ex . automain def run ( greeting ): return greeting + \", \" + fullname () Finally, we can run this experiment from the command line with an Observer (e.g. a mongo server), which will record\neverything we have explicitly indicated that we wish to track (any parameter defined in a function with an @Ingredient.config decorator), along with what Sacred implicitly tracks (source code, version control info, seed,\nhost info, etc). Assuming the experiment code above is written in a file called greeting_experiment.py : python greeting_experiment.py -m sacred The -m sacred parameter specifies that the observer should record everything to the sacred db in a local mongo\ninstance. Sacred has a powerful command line tool in which we can modify the values of parameters before an experiment\nrun. python greeting_experiment.py -m sacred with greeting_experiment.greeting = Goodbye This will return the result Goodbye, Jane Doe . Sacred is an extremely lightweight and powerful tool - we urge you to check out Sacred , and our presentation materials for more examples of how to use Sacred in\nmachine learning with a variety of examples including hyperparmater optimization and model blending. Materials Examples using Sacred to do reproducible machine learning on titanic survivorship are available here: Repository Slides", "date": "2018-05-07"},
{"website": "HBCTech", "title": "Why you should volunteer for Smart Futures", "author": ["Daniel Mateus Pires"], "link": "https://tech.hbc.com/2018-04-20-smart-futures.html", "abstract": "Dublin is a great place for volunteering in Tech initiatives ! Two years ago I started mentoring in a CoderDojo , and, at HBC we recently joined the Smart Futures initiative. So, I want to share with you some volunteering tips, lessons learned along the way, and why we joined Smart Futures . How to pick an initiative With so many great tech initiatives out there and limited time to contribute, it can be a tough choice for a volunteer to pick one (or a few) initiatives to get involved with. Different initiatives target different issues (e.g. diversity, education) and as a volunteer your tasks will vary (e.g. teaching children, presenting to adults, organising events). 4 questions to ask yourself answers should be very subjective but I included mine for reference What issues matter the most to me ? Students choosing CS (Computer Science) in college do not have a concrete idea of what CS is. I found that many class-mates were deceived as they had a false and stereotypical expectation of what CS would be. On the other side, many highschool class-mates never even considered CS either (for the same reason) and might have missed a passion! Therefore, I want to educate students about CS to break stereotypes and allow them to make an informed choice. How can I address those issues ? I can use my CS knowledge and experience working in Tech to inform students Do I have other expectations about my volunteering experience ? I like teaching, explaining to children and presenting in general Is there an existing initiative aligning with my goals and expectations ? Yes, at least 2 that I know of: CoderDojo and Smart Futures Starting your own Joining an existing initiative is great: you get to learn from volunteers with more experience and the overhead of running an association (e.g. paperwork) is done for you. But.. depending on where you live / where you work, there might not be an option that suits you. And in the case that you are starting your own: many initiatives make resources available to the public through their website, and you can connect with their community of volunteers through their communication channels ( Slack / email). Some initiatives like CoderDojo are international initiatives and will help you set up your own coding classes. What is Smart Futures Smart Futures ’ objective is getting children interested in S.T.E.M subjects in secondary school and at third level.\nIt is coordinated by the Science Foundation Ireland . There are two types of events: you contacting directly schools or highschools to give talks / presentations / workshops to classes, the format is free Smart Futures asking you to send some volunteers to participate at events they are organising (occasional) You have no obligations of frequency, you only have to contact Smart Futures when you give a presentation to a school to help them map what schools are getting visited. Why it’s important Students want to fit in In a survey , Smart Futures asked 2,000 first year students across Ireland about what influenced them when selecting their college course:\n62% of student said ‘fitting in’ was the main reason for their choice ! You can help break stereotypes; make students see the vast range of careers in S.T.E.M. There are so many radically different careers in Technology alone it is hard to imagine that someone would not ‘fit in’, but it is also very hard to picture some of those roles as a child (without an awesome volunteer to help). Children need role models Diversity has been a main topic in Tech industry issues related talks in the past few years. CoderDojo have shown in their 2017 survey that the percentage of girl students (29%) match the number of girl mentors (29%) and they were left wondering if there was some correlation.. Children want to identify with their role models; the example above is about gender but we need mentors with all kinds of personalities and backgrounds. Going to the schools Events and coding classes need to be very well advertised to appeal to children that have no interest in Tech (yet) or ‘non Tech-aware’ parents. Going to the classes you are talking to everyone, including these students! It is truly a unique chance for them. To quote the Smart Futures website: Many students have no access to anyone working in science and technology, or may not know anyone that went to college. Our experience Jake Whelan and I have given 2 presentations to classes of ~50 children aged 10-11, here’s some take-aways from  preparing and giving those presentations. Presentation format The presentation format is not enforced by Smart Futures , although there is an example on their website. Nonetheless: personalise it to create a format that works for you, make it interactive and adapt the presentation to the age of your audience (a 10 yrs old is probably not interested in eCommerce) Should you talk about your company ? Children want concrete examples, and it is easy to go in details when talking about your daily life. However, you need to take your audience into consideration. For example: our roles are linked to online shopping; but a 10-11 yrs old will probably never have purchased anything on an eCommerce website! We decided to let the children know they could ask questions about applications unrelated to eCommerce that they use every day (e.g. in these classes Snapchat was very popular). Don’t be shy: show some code! We firmly believe that showing a simple snippet of code, explaining in details pieces of logic, asking the students questions about it and making them guess what specific parts are doing, is key to demystify what application development really is. But first you need to introduce the concept of coding itself, so here’s a rough transcript of how we explained it to 10-11 yrs old: When talking to a person you use a language to communicate ideas to that person. There are many “human languages”: Irish, English, French..\nBut a machine only understands 0s and 1s and it would be very hard for humans to communicate ideas only using 0s and 1s; so, coding languages were created: an intermediate that follows very strict rules for the machine to be able to convert it to 0s and 1s and just like in “human languages”, there are many coding languages: JavaScript, Python, Scala, Java etc.. Our coding example is a little game, which is based on a very simplified version of Cookie clicker (it was very popular among children 1-2 yrs ago). A web-page shows a cookie and a number that increments every time you click on the it. We spend as much time as needed an let the children ask all the questions they have in order to understand how the code works. 2 questions we got: Do you have to learn all the different languages ? Great occasion to point out that languages are very similar in their structure and follow the same logic Why do the words have different colors ? Coding languages follow very specific rules so each word has a role like variables, values, keywords etc.. just like we have verbs, nouns and adverbs! So the text editors we use color the words for us to help differentiate between the different roles. It is very encouraging to hear such practical questions being asked (like they could picture themselves coding already!) The goal of this little example is not to teach them how to code but to: see a very concrete example (write code and see the result on the web page) demystify code explain some concepts that are the building blocks of coding, like conditions and variables; which can be used later-on to give concrete explanations about how certain features of the applications they are familiar with, work (at a high level) You can find the code for the game on GitHub . Make some time at the end We always make sure to leave at least 1/3 of the time for questions at the end. The questions can be about anything! You showed code and the children now have a concrete idea of what development is like. What should happen is that they want to tie-up what they just learned with applications they are familiar with. e.g. How does a website know my username is already taken when I create an account ? Children will have a concept of storing files in a computer (which can be used to explain databases), and, thanks to the previous demo they now have an understanding of conditionals and variables ! That is enough to explain many features at a high level. This “Ask me anything” session at the end does a good job at shifting the way the students look at applications they use every day: now they do not accept them to be “black-boxes” anymore and look to understand how they work. Follow up Send resources to the teachers after the presentation, you might have sparked interests for programming so make sure they know where to go. My favorites are Code.org and Scratch , but I also recommend children to ask their parents to sign them up for CoderDojo (which is free) How do I get involved You can get involved individually or as a company by contacting smartfutures@sfi.ie , you will be invited to a presentation about the initiative and be registered as a volunteer. Help us find schools If you are a parent, or part of a school council, you can help us by reaching out to teachers/principals who would be interested in having us present to their students (in the Dublin area).", "date": "2018-04-20"},
{"website": "HBCTech", "title": "Let’s run an experiment! Self-selection at HBC Digital", "author": "Unknown", "link": "https://tech.hbc.com/2017-05-31-self-selection-hbc.html", "abstract": "Inspired by Opower’s success story, we ran a self-selection experiment at HBC Digital. Dubbed as “the most anticipated event of the year” it enabled 39 team members to self-select into 4 project teams. How did they do it? By picking a project they wanted to work on, the teammates they wanted to work with and keeping a “Do what’s best for the company” attitude.\nRead on to learn about our experience and consider giving a self-selection a try! A little bit of introduction: Who are we? HBC Digital is the group that drives the digital retail/ecommerce and digital customer experience across all HBC retail banners including Hudson’s Bay, Lord & Taylor, Saks Fifth Avenue, and Saks OFF 5TH. Our process, trifectas and team ingredients Our development process is largely inspired by the agile workflow and has the ideas of intrinsic motivation in its core.\nWhat agile flavor do we use? It depends on the team. Each team has a full autonomy in selecting Scrum, Kanban, XP, a combination thereof or none of the above as their process. As long as they remain small, nimble, able to collaborate and continuously deliver value, they can tailor the process to their needs. We do keep certain key components standard across all teams.\nOne of them is a “Trifecta” – a group of servant-leaders in each team: a Product Manager, an Agile Project Manager and a Tech Lead. They work together to support their team and enable the team’s success.\nWe value continuous learning and facilitate role blending by instilling our Team Ingredients framework. Originally designed by Heather Fleming, the Team Ingredients framework facilitates team-level conversations about the team strengths, learning interests and cross-training opportunities. Over the years the framework evolved from being a management tool for assessing teams from “outside in” to being a team tool that supports self-organizing and learning discussions. After a major revamp and gamification of the framework in 2016, we now use it as part of our Liftoff sessions and team working agreement conversations. Just like our Team Ingredients framework, our process continues to evolve. We experiment with new ideas and practices to facilitate teams’ effectiveness and create an environment for teams to thrive. The self-selection is our latest experiment and this blog post is a glimpse into how it went. Self-selection triggers and enablers Organizational change As an organization that grew through acquisitions, at one point we found ourselves dealing with an unhealthy mix of cultures, duplicate roles and clashing mindsets. To remain lean and agile, we went through a restructuring at all levels. Inspiring case studies When we were evaluating the best ways to re-form the teams, we came across Amber King and Jess Huth’s talk on self-selection at Business Agility 2017 Conference. The lightbulb went on! Amber and Jess were describing exactly the situation we were in at that time and were reporting the positive effect of running a self-selection with the teams at Opower. We followed up with them on Skype afterwards. Hearing their compelling story again and being encouraged by their guidance, we left the call fired up to give the self-selection a try! Self-selection manual When it is your turn to plan for self-selection, pick up a copy of Sandy Mamoli and David Mole’s book “Creating Great Teams: How Self-Selection Lets People Excel” This very detailed facilitation guide from the inventors of self-selection process is indispensable in preparing for and facilitating a self-selection event. Past success What worked in our favor was the fact that HBC Tech had tried running a self-selection in 2012 as part of a transition to “two-pizza” teams. The self-selection event was called a Speed Dating, involved 50 people and 6 projects. Fun fact - a number of today’s leaders were involved in 2012 event as regular participants. Transparency We kept the preparation process very transparent. Dedicated Slack channel, Confluence page with progress updates and participants’ info, communication at the tech all-hands meetings and Q&A sessions – everything to avoid creating discomfort and to reduce the fear factor amongst team members. Self-selection in seven steps 1. Get Leadership Buy-In One of the first steps in a self-selection is getting buy-in from your leadership team.\nWhether you start from feature teams or component teams , a self-selection event has a potential of impacting the existing reporting structure in your organization.\nHave an open conversation with each of the leaders to clarify the process, understand their concerns and answer questions. Is there a small modification you can make to the process to mitigate these concerns and turn the leaders into your supporters?\nFrom our experience, making a self-selection invitational and positioning it as “an experiment” fast-tracked its acceptance in the organization. 2. Identify Participants How many people will be involved in your self-selection?\nWill it include all of your existing project teams or a subset? Reducing the size of the self-selection to only a subset of the teams at HBC Digital made our experiment more plausible.\nBy the same token, it created a bit of a confusion around who was in vs. who was not. If you are running a self-selection for a subset of your teams, make sure that the list of participants is known and publicly available to everyone. Verify that the total number of participants is equal or smaller than the number of open spots on the new teams. Pre-selected vs. free-moving participants Decide if you need to have any of the team members pre-selected in each team.\nFor us, the only two pre-selected roles in each team were a Product Manager and a Tech Lead. They were the key partners in pitching the initiative to team members. All others (including Agile Project Managers) were invited to self-select into new teams. FTEs vs. Contractors If you have contractors working on your projects alongside the full-time employees, you will need to figure out if limiting self-selection to full-time employees makes sense in your environment. Since our typical team had a mix of full-time employees and contractors, it was logical for us to invite both groups to participate in the self-selection. After all, individuals were selecting the teams based on a business idea, a technology stack and the other individuals that they wanted to work with.\nWe did make one adjustment to the process and asked contractors to give employees “first dibs” at selecting their new teams. Everyone had equal opportunity after the first round of the self-selection. Observers Usually, you would want to limit participation to those directly involved in a self-selection.\nIn our case, there was so much interest in the self-selection experiment across the organization, that we had to compromise by introducing an observer role.\nObservers were invited to join in the first part of the self-selection event. They could check out how the room was set up, take a peek at the participants’ cards. They could listen to initiative pitches for all teams, without making an actual selection.\nObservers were asked to leave after the break and before the start of actual teams’ selection. 3. Work with Your Key Partners Adjust the process to fit your needs During our prep work we discovered that some team members felt very apprehensive about self-selection processes. To some extent, it reminded them of a negative experience they had in their childhood with a selection into sports teams.  We collaborated with current teams’ Trifectas to reduce potential discomfort with the following adjustments: We modified the “I have no squad” poster into “Available to help” poster for a more positive spin. We made a compromise on consultants’ participation, asking them to add their cards to “I am available to help” poster in the first round and letting them participate equally starting from the second round. We introduced a “No first come first serve” rule to keep the options open for everyone and avoid informal pre-selection. Product Managers and Tech Leads pitches. Coach them to inspire people with their short pitches about a product vision and a technology stack: Why is this initiative important to our business? How can you make a difference if you join? What exciting technologies will you get a chance to work with if you become a part of this team? What kind of team are we looking to build? Establish the team formula This part is really critical. Your team formula may include the core team only, or like in our case, include members from the larger project community (Infrastructure Engineers, UX Designers etc.)\nAs a facilitator, you want to understand very well the needs of each project in terms of specific roles and the number of people required for each role. Cross-check the total number of people based on the team formula with the number of people invited to participate in the self-selection. Avoid putting people into “musical chairs” at all cost! 4. Evangelize Take the uncertainty out of the self-selection! Clarify questions, address concerns, play the “what-ifs”, collect questions and make answers available to everyone. We learnt to use a variety of channels to spread the word about the self-selection: announcements at Tech All-hands meetings dedicated Q&A sessions with each existing group. Confluence Q&A page #self-selection Slack channel formal and informal one-on-one conversations (including hallway and elevator chats) discussion between the Tech Leads and Product Managers and their potential team members 5. Prepare Space It was important for us to find the right space and set the right mood for the actual self-selection event.\nThe space that worked for us met all of our criteria: 1) Appropriate for the size of the group\n2) Natural light\n3) Separate space for pitches and for team posters\n4) Away from the usual team spaces (to minimize distractions) Food Speaking of the right mood, we had enough good snacks brought in for all participants and observers! Depending on the time of the day, you may plan on bringing breakfast, lunch or snacks into your self-selection event.\nWe ran ours in the afternoon and brought in a selection of European chocolate, popcorn and juices. Posters Help the participants remember the rules and find the team corners by preparing posters. Be creative, make them visually appealing. Here is what worked for us: 1) One team poster per team with the project/team name, team formula and a team mascot. 2) Rules posters: “Do what’s best for the company” “Equal team selection opportunity” “Teams have to be capable of delivering end to end” 3) “Available to help” poster. This is very similar to “I have no squad” poster from Sandi Mamoli’s book. However, we wanted to make the message on that poster a little bit more positive. Participants Cards At a minimum, have a printed photo prepared for each participant and color-coded labels to indicated different roles. We invested a little more time in making participants cards look like game cards and included: a LinkedIn photo of the participant their name a current role their proficiency and learning interests in the eleven team ingredients a space to indicate their first, second and third choices of the team (during the event). Using our Team ingredients framework and Kahoot! survey platform we created a gamified self-assessment to collect the data for these cards. Participants rated their skill levels and learning interests for each of the ingredients using the following scale: 3 – I can teach it 2 – I can do it 1 – I’d like to learn it 0 – Don’t make me do it 6. Run It took us exactly one month to get to this point.\nOn the day of the self-selection the group walked into the room. The product managers, tech leads and the facilitator were already there. The room was set and ready for action! Initiative Pitches Participants picked up their cards and settled in their chairs, prepared to hear the initiative pitches and to make their selections. This was one of the most attentive audience we’ve seen! We didn’t even have to set the rules around device usage - everyone was giving the pitches their undivided attention. After a short introduction from the facilitator and a “blessing” from one of the leaders, Product Managers and Tech Leads took the stage. For each initiative they presented their vision of the product, the technology stack and their perspective on the team they’d like to build. It was impressive to see how each pair worked together to answer questions and inspire people.\nAt the end of the pitches, we took a short break. It was a signal for observers to leave the room. Two rounds of self-selection After the break, Product Managers and Tech Leads took their places in the team corners. We ran two rounds of self-selection, ten minutes each. During the first self-selection round people walked around, checked the team formula, chatted with others and placed their cards on a poster of their first choice team. Contractors and others, who didn’t want to make a selection in the first round, placed their cards on “Available to help” poster. At the end of the round, each tech lead was asked to give an update on the following: Was the team complete after this round? Were there any ingredients or skills missing in the team after the first round? During the second round , there were more conversations, more negotiations and more movement between the teams. Some people agreed to move to their second choice teams to help fill the project needs. The “Do what’s best for the company” poster served as a good reminder during this process. The debrief revealed that three teams out of four had been fully formed by the end of the second round.\nThe last team had more open spots still. It was decided that those will be filled later by hiring new people with the required skillset. The self-selection event was completed. It was a time to celebrate and to start planning the work with the new teams. 7. Support New Teams Transition Plan With the self-selection exercise, our teams formed a vision for their ideal “end state”. Afterwards, we needed to figure out how to achieve that vision.\nTech leads worked with their new team members to figure our the systems they supported,  the projects they were involved with at that time and mapped out the transition plan. Team Working Agreement Once all members of the new teams were available to start, we faciliated Liftoff workshops to help them get more details on the product purpose, establish team working agreements and help the teams understand larger organizational context. Coaching/Measuring Happiness Our experiment didn’t stop there. We continue checking in with the team through coaching, measuring happiness (we use gamified Spotify Squad Health check ) and facilitating regular retrospectives. What’s next? As our roadmap continues to change and as we get more people joining the organization, we may consider running a self-selection again with a new group.\nOr we may decide to move away from “large batches” of self-selection and experiment with a flow of Dynamic Reteaming . Time will tell. One thing is clear - we will continue learning and experimenting. How can you learn more? We hope this blog post inspired you to think about a self-selection for your teams. Still have questions after reading it? Get in touch with us, we’d love to tell you more! We are speaking Join our talks and workshops around the World: “The New Work Order” keynote at Future of Work by Heather Fleming, VP People Operations & PMO Removing Friction In the Developer Experience at QConn New York by Adrian Trenaman, SVP Engineering Discover Your Dream Teams Through Self-Selection with a Team Ingredients Game at Global Scrum Gathering Dublin by Dana Pylayeva, Agile Coach Great books that inspired us Sandy Mamoli, David Mole “Creating Great Teams: How Self-Selection Lets People Excel” Diana Larsen, Ainsley Nies Liftoff: Launching Agile Teams & Projects Heidi Shetzer Helfand Dynamic Reteaming. The Art and Wisdom of Changing Teams", "date": "2017-05-31"},
{"website": "HBCTech", "title": "CloudFormation Nanoservice", "author": ["Ryan Martin"], "link": "https://tech.hbc.com/2017-05-19-cloudformation-nanoservice.html", "abstract": "One of the big HBC Digital initiatives for 2017 is “buy online, pickup in store” - somewhat awkwardly nicknamed “BOPIS” internally. This is the option for the customer to, instead of shipping an order to an address, pick it up in a store that has the items in inventory. A small part of this new feature is the option to be notified of your order status (i.e. when you can pickup the order) via SMS. A further smaller part of the SMS option is what to do when a customer texts “STOP” (or some other similar stop word) in response to one of the SMS notifications. Due to laws such as the Telephone Consumer Protection Act (TCPA) and CAN-SPAM Act, we are required to immediately stop sending additional messages to a phone number, once that person has requested an end to further messaging. Our SMS provider is able to receive the texted response from the customer and POST it to an endpoint of our choosing. We could wrap such an endpoint into one of our existing microservices, but the one that sends the SMS (our customer-notification-service) is super-simple: it receives order events and sends notifications (via email or SMS) based on the type of event. It is essentially a dumb pipe that doesn’t care about orders or users; it watches for events and sends messages to customers based on those events. Wrapping subscription information into this microservice felt like overstepping the bounds of the simple, clean job that it does. So this is the story of how I found myself writing a very small service (nanoservice, if you will) that does one thing - and does it with close-to-zero maintenance, infrastructure, and overall investment. Furthermore, I decided to see if I could encapsulate it entirely within a single CloudFormation template. How we got here Here are the two things this nanoservice needs to do: Receive the texted response and unsubscribe the customer if necessary Allow the customer notification service (CNS) to check the subscription status of a phone number before sending a SMS In thinking about the volume of traffic for these two requests, we consider the following: This is on [https://www.saksfifthavenue.com] only (for the moment) Of the online Saks orders, only a subset of inventory is available to be picked up in the store Of the BOPIS-eligible items, only a subset of customers will choose to pickup in store Of those who choose to pickup in store, only a subset will opt-in for SMS messages Of those who opt-in for SMS, only a subset will attempt to stop messages after opting-in For the service’s endpoints, the request volume for the unsub endpoint (#1 above) is roughly the extreme edge case of #5; the CNS check (#2) is the less-edgy-but-still-low-volume #4 above. So we’re talking about a very small amount of traffic: at most a couple dozen requests per day. This hardly justifies spinning up a microservice - even if it runs on a t2.nano, you still have the overhead of multiple nodes (for redundancy), deployment, monitoring, and everything else that comes with a new microservice. Seems like a perfect candidate for a serverless approach. The architecture As mentioned above, a series of order events flows to the customer notification service, which checks to make sure that the destination phone number is not blacklisted. If it is not, CNS sends the SMS message through our partner, who in turn delivers the SMS to the customer. If the customer texts a response, our SMS partner proxies that message back to our blacklist service. The blacklist service is a few Lambda functions behind API Gateway; those Lambda functions simply write to and read from DynamoDB. Because the stack is so simple, it felt like I could define the entire thing in a single artifact: one CloudFormation template. Not only would that be a geeky because-I-can coding challenge, it also felt really clean to be able to deploy a service using only one resource with no dependencies. It’s open source, so anyone can literally copy-paste the template into CloudFormation and have the fully-functioning service in the amount of time it takes to spin up the resources - with no further knowledge necessary. Plus, the template is in JSON (which I’ll explain later) and the functions are in Node.js, so it’s a bit of The API Here at HBC Digital, we’ve really started promoting the idea of API-driven development (ADD). I like it a lot because it forces you to fully think through the most important models in your API, how they’re defined, and how clients should interact with them. You can iron out a lot of the kinks (Do I really need this property? Do I need a search? How does the client edit? What needs to be exposed vs locked-down? etc) before you write a single line of code. I like to sit down with a good API document editor such as SwaggerHub and define the entire API at the beginning. The ADD approach worked really well for this project because we needed a quick turnaround time: the blacklist was something we weren’t expecting to own internally until very late in the project, so we had to get it in place and fully tested within a week or two. With an API document in hand (particularly one defined in Swagger ), I was able to go from API definition to fully mocked endpoints (in API Gateway) in about 30 mins. The team working on CNS could then generate a client (we like the clients in Apidoc , an open-source tool developed internally that supports Swagger import) and immediately start integrating against the API. This then freed me to work on the implementation of the blacklist service without being a blocker for the remainder of the team. We settled on the blacklist approach one day; less than 24 hours later we had a full API defined with no blockers for development. The API definition is fairly generic: it supports blacklisting any uniquely-defined key for any type of notification. The main family of endpoints looks like this: /{notification_type}/{blacklist_id} notification_type currently only supports sms , but could very easily be expanded to support things like email , push , facebook-messenger , etc. With this, you could blacklist phone numbers for sms independently from email addresses for email independently from device IDs for push . A simple GET checks to see if the identifier of the destination is blacklisted for that type of notification: > curl https://your-blacklist-root/sms/555-555-5555 { \"message\" : \"Entry not blacklisted\" } This endpoint is used by CNS to determine whether or not it should send the SMS to the customer. In addition to the GET endpoint, the API defines a PUT and a DELETE for manual debugging/cleanup - though a client could also use them directly to maintain the blacklist. The second important endpoint is a POST that receives a XML document with details about the SMS response: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <moMessage messageId= \"123456789\" receiptDate= \"YYYY-MM-DD HH:MM:SS Z\" attemptNumber= \"1\" > <source address= \"+15555555555\" carrier= \"\" type= \"MDN\" /> <destination address= \"12345\" type= \"SC\" /> <message> Stop texting me </message> </moMessage> The important bits are the source address (the phone number that sent the message) and the message itself. With those, the API can determine whether or not to add the phone number to the blacklist. If it does, the next time CNS calls the GET endpoint for that phone number, the API will return a positive result for the blacklist and CNS will not send the SMS. The POST to /mo_message lives at the top-level because it is only through coincidence that it results in blacklisting for SMS; one could imagine other endpoints at the top-level that blacklist from other types of notifications - or even multiple (depending on the type of event). Let’s see some code First there are a couple functions shared across all the endpoints (and their backing Lambda functions): function withSupportedType(event, context, lambdaCallback, callback) {\n  const supportedTypes = ['sms'];\n  if (supportedTypes.indexOf(event.pathParameters.notification_type.toLowerCase()) >= 0) {\n    callback(event.pathParameters.notification_type.toLowerCase());\n  } else {\n    lambdaCallback(null, { statusCode: 400, body: JSON.stringify({ message: 'Notification type [' + event.pathParameters.notification_type + '] not supported.' }) });\n  }\n}\n\nfunction sanitizeNumber(raw) {\n  var numbers = raw.replace(/[^\\d]+/g, '');\n  if (numbers.match(/^1\\d{10}$/)) numbers = numbers.substring(1, 11);\n  return numbers;\n} These are there to ensure that each Lambda function is a) dealing with invalid notification_types and b) cleaning up the phone number in the same manner across all functions. Given those common functions, the amount of code for each function is fairly minimal. The GET endpoint simply queries the DynamoDB for the unique combination of notification_type and blacklist_id : const AWS = require('aws-sdk'),\n      dynamo = new AWS.DynamoDB();\n\nexports.handler = (event, context, callback) => {\n  const blacklistId = sanitizeNumber(event.pathParameters.blacklist_id);\n  withSupportedType(event, context, callback, function(notificationType) {\n    dynamo.getItem({\n      TableName: event.stageVariables.TABLE_NAME,\n      Key: { Id: { S: blacklistId }, Type: { S: notificationType } }\n    }, function(err, data) {\n      if (err) return callback(err);\n      if ((data && data.Item && afterNow(data, \"DeletedAt\")) || !onWhitelist(blacklistId, event.stageVariables.WHITELIST)) {\n        callback(null, { statusCode: 200, body: JSON.stringify({ id: blacklistId }) });\n      } else {\n        callback(null, { statusCode: 404, body: JSON.stringify({ message: \"Entry not blacklisted\" }) });\n      }\n    })\n  });\n}\n\nfunction afterNow(data, propertyName) {\n  if (data && data.Item && data.Item[propertyName] && data.Item[propertyName].S) {\n    return Date.parse(data.Item[propertyName].S) >= new Date();\n  } else {\n    return true;\n  }\n}\n\n// Set the whitelist in staging to only allow certain entries.\nfunction onWhitelist(blacklistId, whitelist) {\n  if (whitelist && whitelist.trim() != '') {\n    const whitelisted = whitelist.split(',');\n    return whitelisted.findIndex(function(item) { return blacklistId == item.trim(); }) >= 0;\n  } else {\n    return true;\n  }\n} Disregarding the imports at the top and some minor complexity around a whitelist (which we put in place only for staging/test environments so we don’t accidentally spam people while testing), it’s about a dozen lines of code (depending on spacing) - with minimal boilerplate. This is the realization of one of the promises of the serverless approach: very little friction against getting directly to the meat of what you’re trying to do. There is nothing here about request routing or dependency-injection or model deserialization; the meaningful-code-to-boilerplate ratio is extremely high (though we’ll get to deployment later). The PUT (add an entry to the blacklist, managing soft-deletes correctly) exports.handler = (event, context, callback) => {\n  const blacklistId = sanitizeNumber(event.pathParameters.blacklist_id);\n  withSupportedType(event, context, callback, function(notificationType) {\n    dynamo.updateItem({\n      TableName: event.stageVariables.TABLE_NAME,\n      Key: { Id: { S: blacklistId }, Type: { S: notificationType } },\n      ExpressionAttributeNames: { '#l': 'Log' },\n      ExpressionAttributeValues: {\n        ':d': { S: (new Date()).toISOString() },\n        ':m': { SS: [ toMessageString(event) ] }\n      },\n      UpdateExpression: 'SET UpdatedAt=:d ADD #l :m REMOVE DeletedAt'\n    }, function(err, data) {\n      if (err) return callback(err);\n      callback(null, { statusCode: 200, body: JSON.stringify({ id: blacklistId }) });\n    })\n  });\n} and DELETE (soft-delete entries when present) exports.handler = (event, context, callback) => {\n  const blacklistId = sanitizeNumber(event.pathParameters.blacklist_id);\n  withSupportedType(event, context, callback, function(notificationType) {\n    dynamo.updateItem({\n      TableName: event.stageVariables.TABLE_NAME,\n      Key: { Id: { S: blacklistId }, Type: { S: notificationType } },\n      ExpressionAttributeNames: { '#l': 'Log' },\n      ExpressionAttributeValues: {\n        ':d': { S: (new Date()).toISOString() },\n        ':m': { SS: [ toMessageString(event) ] }\n      },\n      UpdateExpression: 'SET DeletedAt=:d, UpdatedAt=:d ADD #l :m'\n    }, function(err, data) {\n      if (err) return callback(err);\n      callback(null, { statusCode: 200, body: JSON.stringify({ id: blacklistId }) });\n    })\n  });\n} functions are similarly succinct. The POST endpoint that receives the moMessage XML is a bit more verbose, but only because of a few additional corner cases (i.e. when the origin phone number or the message isn’t present). exports.handler = (event, context, callback) => {\n  const moMessageXml = event.body;\n  if (messageMatch = moMessageXml.match(/<message>(.*)<\\/message>/)) {\n    if (messageMatch[1].toLowerCase().match(process.env.STOP_WORDS)) { // STOP_WORDS should be a Regex\n      if (originNumberMatch = moMessageXml.match(/<\\s*source\\s+.*?address\\s*=\\s*[\"'](.*?)[\"']/)) {\n        var originNumber = sanitizeNumber(originNumberMatch[1]);\n        dynamo.updateItem({\n          TableName: event.stageVariables.TABLE_NAME,\n          Key: { Id: { S: originNumber }, Type: { S: 'sms' } },\n          ExpressionAttributeNames: { '#l': 'Log' },\n          ExpressionAttributeValues: {\n            ':d': { S: (new Date()).toISOString() },\n            ':m': { SS: [ moMessageXml ] }\n          },\n          UpdateExpression: 'SET UpdatedAt=:d ADD #l :m REMOVE DeletedAt'\n        }, function(err, data) {\n          if (err) return callback(err);\n          callback(null, { statusCode: 200, body: JSON.stringify({ id: originNumber }) });\n        });\n      } else {\n        callback(null, { statusCode: 400, body: JSON.stringify({ message: 'Missing source address' }) });\n      }\n    } else {\n      callback(null, { statusCode: 200, body: JSON.stringify({ id: '' }) });\n    }\n  } else {\n    callback(null, { statusCode: 400, body: JSON.stringify({ message: 'Invalid message xml' }) });\n  }\n} A couple things to call out here. First - and I know this looks terrible - this function doesn’t parse the XML - it instead uses regular expressions to pull out the data it needs. This is because Node.js doesn’t natively support XML parsing and importing a library to do it is not possible given my chosen constraints (the entire service defined in a CloudFormation template); I’ll explain further below. Second, there is expected to be a Lambda environment variable named STOP_WORDS that contains a regular expression to match the desired stop words (things like stop, unsubscribe, fuck you, etc). That’s pretty much the extent of the production code. Deployment - CloudFormation Here’s where this project gets a little verbose. Feel free to reference the final CloudFormation template as we go through this. In broad strokes, this template matches the simple architecture diagram above: API Gateway calls Lambda functions which each interact with the same DynamoDB database. The bottom of the stack (i.e. the top of the template) is fairly simple: two DynamoDBs (one for prod, one for stage) and an IAM role that allows the Lambda functions to access the databases. On top of that are the four Lambda functions - which contain the Node.js code (this is the “YO DAWG” part, since the Javascript is in the JSON template) - plus individual permissions for API gateway to call each function. This section (at the bottom of the template) is long but is mostly code-generated (we’ll get to that later). In the middle of the template lie a bunch of CloudFormation resources that define the API Gateway magic: a top-level Api record; resources that define the path components under that Api; methods that define the endpoints and which Lambda functions they call; separate configurations for stage vs prod. At this point, we’re just going to avert our eyes and reluctantly admit that, okay, fine, serverless still requires some boilerplate (just not inline with the code, damn it!). At some level, every service needs to define its endpoints; this is where our blacklist nanoservice does it. All-in, the CloudFormation template approaches 1000 lines (fully linted, mind you, so there are a bunch of lines with just tabs and curly brackets). “But wait!” you say, “Doesn’t CloudFormation support YAML now?” Why yes, yes it does. I even started writing the template in YAML until I realized I shouldn’t. Bringing CloudFormation together with Node.js To fully embed the Node.js functions inside the CloudFormation template would have been terrible. How would you run the code? How would you test it? A cycle of: tweak the code => deploy the template to the CloudFormation stack => manually QA - that would be a painful way of working. It’s unequivocally best to be able to write fully isolated and functioning Node.js code , plus unit tests in a standard manner. The problem is that Node.js code then needs to be zipped and uploaded to S3 and referenced by the CloudFormation template - which would create a dependency for the template and would not have achieved the goal of defining the entire service in a single template with no dependencies. To resolve this, I wrote a small packaging script that reads the app’s files and embeds them in the CloudFormation template. This can then be run after every code change (which obviously would have unit tests and a passing CI build), to keep the template inline with all code changes. The script is written in Node.js (hey, if you’re running tests locally, you must already have Node.js installed locally), so a CloudFormation template written in JSON (as opposed to YAML) is essentially native - no parsing necessary. The script can load the template as JSON, inject a CloudFormation resource for each function in the /app directory, copy that function’s code into the resource, and iterate. Which brings us to The other thing to note about going down the path of embedding the Node.js code directly in the CloudFormation template (as opposed to packaging it in a zip file): all code for a function must be fully contained within that function definition (other than the natively supported AWS SDK). This has two implications: first, we can’t include external libraries such as a XML parser or a Promise framework (notice all the code around callbacks, which makes the functions a little more verbose than I’d like). Second, we can’t DRY out the functions by including common functions in a shared library; thus they are repeated in the code for each individual function. Conclusion So that’s it: we end up with a 1000-line CloudFormation template that entirely defines a blacklist nanoservice that exposes four endpoints and runs entirely serverless. It is fully tested, can run as a true Node.js app (if you want), and will likely consume so few resources that it is essentially free. We don’t need to monitor application servers, we don’t need to administer databases, we don’t need any non-standard deployment tooling. And there are even separate stage and production versions. You can try it out for yourself by building a CloudFormation stack using the template . Enjoy!", "date": "2017-05-19"},
{"website": "HBCTech", "title": "The POps Up Plant Shop", "author": ["HBC Digital"], "link": "https://tech.hbc.com/2017-05-18-pops-up-plant-shop.html", "abstract": "How do we keep our teams happy and high-performing? That’s the focus for the People Operations (POps) team. The POps mission is: To build and maintain the best product development teams in the world through establishing the models around how we staff and organize our teams, how we plan and execute our work, and how we develop our people and our culture. Our work includes: facilitating “self-selection” exercises which allow people to choose who they work with and what they work on. working with teams to make sure they have the right team ingredients . providing people with the best tools . encouraging people to share their work via speaking at conferences and meetups , writing for our tech blog , teaching classes and other channels . We also like to have some fun, too. Surprise and Delight This week we coordinated an intercontinental “POps Up Plant Shop” for our people in NYC and Dublin. Between the two offices, we distributed 350 plants. Crotons, ivies, succulents and more were on offer. Everyone loved the surprise. While POps is focused on working with our tech teams, we noticed a few folks from other departments at HBC taking plants for their desks - a good indicator that what we’re doing is working! Beyond adding a dash of color the office, offices plants are proven to increase happiness and productivity which aligns perfectly with the mission of the POps team.", "date": "2017-05-18"},
{"website": "HBCTech", "title": "Mobile Design Sprint", "author": ["HBC Digital"], "link": "https://tech.hbc.com/2017-05-02-saks-app-design-sprint.html", "abstract": "HBC Digital is a large organization. We are hundreds of technologists responsible for the retail experiences for many of North America’s largest retailers including Saks Fifth Avenue, Saks OFF 5TH, Gilt, Lord & Taylor and the Bay. Our breadth allows us to work on complex challenges with huge upsides. The number of opportunities available to us, however, requires commitment from our teams to ensure we are focused on the right problems. Recently our mobile team took part in a week-long design sprint. The goal of the five-day process was to answer critical business questions through design, prototyping and testing ideas with customers, who are always at the center of our work. They wanted to make sure they were solving the right problem for our customers. The design sprint was inspired by past exercises we’ve conducted with Prolific Interactive , however, this iteration was facilitated by the Senior Program Manager on our mobile team. The goal was to use the Saks Fifth Avenue app to “reduce shopping friction, unifying the customer experience across physical and digital stores”. The Process Each day of the five-day sprint had a particular focus: Day 1 - Goal Setting and Mapping the Challenge Day 2 - Sketching Ideas and Setting a Direction Day 3 - Prototyping Day 4 - Prototyping Day 5 - User Testing The exercise involved experts from across Hudson’s Bay Company including product, engineering, UX, business partners from Saks Fifth Avenue stores and our customers. Opportunities Any team embarking on a design sprint should outline their goal and opportunities at the start of the sprint. These help to keep the team focused throughout the exercise. We identified three specific opportunities for our team: Refine the vision for the Saks app Seek business opportunities of being a partner with other divisions in HBC Quickly vet ideas in line with Saks’ business themes What We Learned The “expert panel” conducted with our business partners from stores was one of the big wins of the week. The group setting allowed for lots of interaction and Q&A. Everyone on the team had the first-hand experience of hearing about the pain points of our partners in stores which paid huge dividends during our storyboarding and prototyping sessions. Day 5 was “judgement day”. We created a test environment in our Saks Downtown store to mimic the in-store experience we envisioned during our prototyping session. By demoing in-store with Saks Fifth Avenue shoppers, we were able to get real-time feedback from our customers as they interacted with the prototype. The ability to iterate based on customer feedback before entering production will help to reduce our engineering overhead. An added bonus of the sprint was how it energized our people. The team decided what to focus on, experimented with new technologies and connected directly with our store operations team and customers. All of these opportunities boosted morale and engagement. Some of the things we plan to change for next time include: adjust the timing of some activities (diligent time keeping of activities will pay off when mapping out the agenda for our next design sprint) involve more people from our engineering team to improve the fluidity of our prototyping sessions invest more time in preparation ahead of the exercise to improve our efficiency What’s Next With the design sprint complete, we are moving on to the feasibility/technical discovery process and defining the MVP. The tech discovery process for the MVP will feature a hackathon next month to test and build on some of the themes and technologies we identified as opportunities in the design sprint. The user testing with customers in-store during the design sprint will also heavily influence our work during the hackathon. Stay tuned to this blog or head over to the App Store and download the Saks Fifth Avenue app to keep an eye on what we’re building.", "date": "2017-05-02"},
{"website": "HBCTech", "title": "Meetups: April Recap and What's Happening In May", "author": ["John Coghlan"], "link": "https://tech.hbc.com/2017-04-26-april-meetup-recap-and-looking-ahead.html", "abstract": "April Meetups: 105 guests, 48 seltzers, 45 All Day IPAs, 19 pizzas & 2 great speakers. On April 20, we hosted the NYC Scrum User Group for the third time in 2017. Rob Purdie , founder of the group and Agile Coach at IBM, gave an update on IBM’s Agile Transformation. The talk repeatedly returned to the theme of ensuring your team is “doing the right work”, warning the room of agilists that becoming very efficient at doing work that doesn’t matter is the fastest way to get nowhere. It reminded me of a quote written on the wall of our office: “Our greatest fear should not be failure, but of succeeding in life at things that don’t really matter.” While every NYC SUG Meetup has been great, this one stood out for its accessibility and high levels of audience engagement. A few days later we hosted Li Haoyi (pictured above) who gave a great talk on ‘Designing Open Source Libraries’ at our NY Scala University Meetup . He focused on intuitiveness, layering and documentation as the three keys to creating an open-source library that will keep engineers happy and drive engagement. Haoyi, the author of many popular libraries and fresh off a talk at Scala Days, drew the biggest turnout yet to our new Lower Manhattan HQ . We had to order more pizza 10 minutes after we opened the doors! His honest insights and great delivery also set a record for laughs. Looking Ahead Here are some of the tech events on our calendar in May. Hope to see you there! May 1 - Dana Pylayeva, HBC Digital’s Agile Coach, is organizing Big Apple Scrum Day , a one day community conference focused on Scrum/Agile principles and practices. This 2017 theme is Always Keep Growing . May 6-7 - We’re sponsoring !!Con (pronounced “bang bang con”), “two days of ten-minute talks (with lots of breaks, of course!) to celebrate the joyous, exciting, and surprising moments in computing”. May 10 - Evan Maloney, Distinguished Engineer at HBC Digital, will be speaking at the Brooklyn Swift Developers Meetup at Work & Co in DUMBO. His talk will trace through the evolution of our project structure and development workflow to arrive at where we are today: a codebase that’s about halfway through a transition to Swift. Some folks from our mobile team will be visiting from Dublin for this one! May 11 - Petr Zapletal of Cake Solutions will deliver a talk on how to avoid common pitfalls when designing reactive applications at our NY Scala University Meetup . May 24 - Demo Day for our ScriptEd class - a group of high school students who have been learning web development from HBC Digital engineers in our offices every week since September. May 25 - NYC PostgreSQL User Group Meetup - details coming shortly. May 26 - Summer Fridays start!", "date": "2017-04-26"},
{"website": "HBCTech", "title": "HBC Digital is Sponsoring !!Con", "author": ["HBC Digital"], "link": "https://tech.hbc.com/2017-04-25-hbc-digital-sponsoring-bang-bang-con.html", "abstract": "On May 6-7 one of the year’s most unique tech events is taking place in NYC. !!Con (pronounced “bang bang con”) is two-days of ten-minute talks featuring a diverse array of speakers and topics . You won’t find a lineup like this at your typical tech conference - punch cards, cyborgs, glowing mushrooms, queer feminist cyberpunk manifestos and airplane noise are just a few of the topics on the agenda. Given the excitement around this conference, tickets went fast - sold-out-in-minutes-fast - but there will be videos and a live stream so the 200+ person waiting list and those unable to be in NYC next weekend will still be able to enjoy the talks. Stay tuned to @bangbangcon on Twitter for more info. We’re thrilled to be supporting this year’s !!Con as an AWESOME! Sponsor. Be sure to say hi to one of our friendly engineers and snag some HBC Digital swag if you’re there!", "date": "2017-04-25"},
{"website": "HBCTech", "title": "Pau Carré Cardona To Speak at O'Reilly AI Conference", "author": ["HBC Digital"], "link": "https://tech.hbc.com/2017-04-12-pau-carre-cardona-at-oreilly-ai-conference.html", "abstract": "The O’Reilly Artificial Intelligence Conference is coming to New York City in June. From June 27-29, the top minds in AI will be meeting for “a deep dive into emerging AI techniques and technologies with a focus on how to use it in real-world implementations.” We are excited that one of our software engineers, Pau Carre Cardona, will be leading a session as part of the “Implementing AI” track on June 29. Pau’s talk will expand upon his widely read blog post on how we have applied deep learning at Gilt to complete tasks that require human-level cognitive skills. He’ll touch on how we have leveraged Facebook’s open source Torch implementation of Microsoft’s ResNet for image classification and his open-source project TiefVision which is used to detect image similarity. You can find more details on Pau’s session here: Deep Learning in the Fashion Industry .", "date": "2017-04-12"},
{"website": "HBCTech", "title": "Where to find our team in March", "author": ["HBC Digital"], "link": "https://tech.hbc.com/2017-02-28-where-to-find-us-in-march.html", "abstract": "We have a busy month lined up: March 6 – We’re hosting the NY Scala Meetup featuring Gary Coady, Sr Software Engineer at HBC Digital, leading a talk on “Cleaning Up with Free Monads.” - RSVP March 8 – In honor of International Women’s Day, we’re hosting the Techfest Club Meetup. The Meetup will feature a talk on “The New Face of Personalization” from Cassie Lancellotti-Young, EVP Customer Success at Sailthru. - RSVP March 9 – Heather Fleming, VP, People Operations and Product Delivery, is delivering the keynote address on “The New Work Order” at the Future of Work Summit in Austin, Texas. - MORE INFO March 9 – Ryan Martin, Sr Director, Engineering, is sitting for a fireside chat about Lamda during AWS Loft Architecture Week in NYC. - MORE INFO March 16 – Our Agile Coach, Dana Pylayeva, is leading a workshop on “Growing By Sharing: Transitioning a Group to a Self-Directed Model” with Mary Pratt when we host the NYC Scrum User Group Meetup. - RSVP March 22 – We’re hosting the Elasticsearch User Group Meetup in NYC. HBC Digital Engineers Neil Girardi, Jose Martinez and Ed Perry will highlight some of the innovative ways we have leveraged the Elastic Stack. - RSVP March 25 – We’re hosting the Dublin Microservices Meetup in Dublin. The Meetup will feature a talk on “Solving Service Discovery: How Node.js microservices can find each other without a registry” from Richard Roger, CEO at nearForm. - RSVP", "date": "2017-02-28"},
{"website": "HBCTech", "title": "Don’t just read about DevOps culture, play-test it!", "author": "Unknown", "link": "https://tech.hbc.com/2017-02-14-spotlight-on-devops-p1.html", "abstract": "Don’t just read about DevOps culture, play-test it! A lot of people talk about DevOps Culture. Yes, you can learn about a culture by reading a book or a blog post. A much more effective and fun way to learn about a culture is by experiencing it. This blog post is your invitation to experience DevOps culture through a simulation game! My interest in DevOps originated from a very unlikely turn that my career took 7 years ago. An opportunity came up to push myself completely out of my comfort zone in a developer’s world. I’d taken on a job of DBA Manager and found myself in a harsh, alerts-driven world of pagers, disaster recoveries and escalation procedures. The sense of urgency and pressure was incredible and made me wonder why I never knew about it as a developer. Fast-forward a few years to my next role as an Agile Coach. I came across “The Phoenix Project” . I read the book from cover to cover, re-living painful moments of the past years, yet growing fond of this new “DevOps” approach.  How can I share the new learning and make it resonate as strongly with others? Why not try to make it into a simulation game?\nEquipped with Gamification course and “The Art of Game design” , I put together the first version of the “Chocolate, Lego and Scrum Game”. Just like in DevOps, amplifying the feedback loop is extremely important in game development!  Over the next two years, I’ve taken every opportunity to play the game with different groups, collecting feedback, modifying the game and taking it again into “production” for new rounds of play-testing and learning. What made this game unique was its focus on the DevOps culture and “close to real life” quality of simulation. The game starts with showcase of a large organization with departmental silos. Development teams are using Scrum to manage their work, Operations have their own process.  As in a typical bureaucratic culture, the flow of information is broken. Information is shared on the “need to know” basis. Each team has its own goals and the mission of the organization is unclear. During the game this fictitious organization transitions from silos to locally optimized silos to an organization optimized for a continuous flow of value. Every player in the game gets a special role to play individually as well as a part of his/her team.  Together players build products with LEGO and learn to respond to ever-changing market demand.  They wait for environments to be built by Operations, get interrupted by security upgrades and even get attacked by a hacker! The game engages everyone to the extent that they forget about time. They experience a range of emotions as they go through their DevOps journey and transition toward a generative culture of collaboration and shared goals. While this DevOps transformation is a gamified simulation, the lessons people learn are very real and can be applied to their actual DevOps transformations!\nHere are just a few examples of the “A-ha!” moments highlighted by the participants at Scrum Gathering Porto and at Lean-Agile practitioners of NJ meetup: “Even after DevOps transformation some Ops people want to keep being gate keepers. Hard to give up traditional roles!” “Potentially shippable” doe not equal ”in production.” “Cross-training Dev and Ops streamlined the process of getting products to production.” “Share skills! Bottleneck is formed when only one person knows it” Curious about playing this game in your organization? In a spirit of sharing skills and not being a bottleneck, I have documented detailed facilitation instructions, floor plans, facilitator scripts and the game cards in my new “Introduction to DevOps with Chocolate, LEGO” book recently published by Apress. Go ahead - develop your DevOps transformation muscle memory and experience teams’ behavioral patterns.  Feel the difference DevOps culture makes in establishing trust and psychological safety in your organization.\nHave fun facilitating the game with your teams and please share your learnings.", "date": "2017-02-14"},
{"website": "HBCTech", "title": "Sundial PagerDuty Integration", "author": "Unknown", "link": "https://tech.hbc.com/2017-02-06-sundial-pagerduty-integration.html", "abstract": "Sundial A few months ago, HBC Tech announced Sundial . Sundial is an open source batch job scheduler for Amazon ECS.\nOver the course of the last few months, Sundial has seen a significant adoption both inside and outside of HBC Tech. Until Sundial v0.0.10, emailing was the only way of notifying job failures. At the beginning when the number of jobs running on Sundial was small (and so was the number of failures!), it was fairly easy to spot emails of failed jobs and act accordingly. Lately though, in the Personalization Team, Sundial schedules about a thousand job executions per day and it’s easy to imagine the amount of noise in our inbox generated by job notifications. Beside the noise, it has happened more than once that failure of critical jobs went unnoticed. This was of course unacceptable. Since PagerDuty is the de facto standard in HBC Tech when it comes to on call procedures and since PagerDuty offers a nice and reliable events API , we’ve redesigned the notification mechanism\nand integrated PagerDuty with Sundial. Configuring PagerDuty on Sundial Configuring your job to support both Emails and PagerDuty notifications is very straightforward and can be done by adding the following json snippet to your job definition: { \"notifications\" : [ { \"email\" : { \"name\" : \"name\" , \"email\" : \"email\" , \"notify_when\" : \"on_state_change_and_failures\" } }, { \"pagerduty\" : { \"service_key\" : \"my_pd_service_key\" , \"num_consecutive_failures\" : 3 , \"api_url\" : \"https://events.pagerduty.com\" } } ] } Where notify_when defines when email notifications will be sent. Possible values are: always , Always notify when a process completes on_failure , Notify when a process fails on_state_change , Notify when a process goes from succeeding to failing and vice versa on_state_change_and_failures , Notify when going from failing to succeeded and on each failure never my_pd_service_key is the key obtained in the Service Page in PagerDuty num_consecutive_failures is the number of consecutive failures after which Sundial will trigger an alert in PagerDuty Please note that the subscriptions object in the Process Definition json has been deprecated, so if you’ve already adopted Sundial and want to start using the new notifications , you will have to\n update your json accordingly. More details can be found in the Sundial v.0.0.10 release page", "date": "2017-02-06"},
{"website": "HBCTech", "title": "Voluntary Adoption in Action: HBC Digital Adopts Slack", "author": "Unknown", "link": "https://tech.hbc.com/2017-02-06-slack-and-voluntary-adoption.html", "abstract": "Musings on Decentralised Control and Voluntary Adoption in Large Organisations. When I think of Slack, I think first of the great book by Tom DeMarco on the\norganisational “slack” we need to play, innovate, get big things done. It’s an\namazing read, and I recommend without reservation. More recently, when I think\nof Slack, I think of the massive grassroots movement at HBC Digital that\nswitched us from HipChat to Slack in just a few short weeks, without any top-\ndown edict or stop-the-world migration.  We acheived this by leveraging the\nsimple idea of  ‘voluntary adoption’: if a technology, framework, tool or\nservice is really good then your teams will adopt it naturally, without\ncoercion. The corollary of voluntary adoption is that if you find that you’re\nhaving to push a solution on a group of people and they’re resisting, pushing\nback, or not getting it, then it’s a good sign that the solution might not be as\ngood as you previously thought. Through merger and acquisition, we found ourselves in a position with multiple\ntech teams using different chat solutions, creating artificial divisions and\ncross-team awkwardness. We could have mandated a move to one of the incumbent\nchat solutions at HBC and dragged everyone across the divide: a solution that\nwould have been a long hard march. Instead, we looked about at the current most-\nloved tool, Slack, kicked off a couple of channels, invited some of our teams in, \nand said, “hey, try it out.” Within days we encountered some interesting\neffects: first, people loved it; and second, they wanted clarity to know if\neveryone could just move there together. Without having to force or coerce\nanyone, we’re now all together on one system: Slack. So what do we learn from this application of voluntary adoption? First, we got\nthe outcome we wanted, fast, and it stuck. Second, but perhaps more\ninterestingly, was that we traded off one kind of organisational stress for\nanother. Top down, authoritative control offers clarity and a sense of control,\nand the expense of individual choice. “Everyone, we’re using Tool X” has a\nclarity, but smart folk quickly reject being told to use a tool they don’t like\nand that leads to stress and angst. “Everyone, we don’t have an agreed standard\nyet so why not try this as well as the current solutions?” feels rudderless and\nperhaps somewhat chaotic for those in the midst of it: adoptees are confused and\nwonder which one to choose. However, this approach unleashes a Darwinian process\nof natural selection: a decentralised, collaborative optimisation process that\nwill either squash a bad idea up front or elevate a good idea into something\ngrand. We apply voluntary adoption at multiple levels - in our open-source offerings,\ninternal libraries, tools, and how we work together as teams - and the\nramifications for voluntary adoption for us as engineers and product innovators\nare profound. If you’re going to invest time into building something, spend time\non making it a dream for the newcomer: easy to use, surprising, delighting.\nThink: you are not in a position to force your solution on someone; however, you\ncan make your solution something that’s a dream to adopt. Voluntarily.", "date": "2017-02-06"},
{"website": "HBCTech", "title": "Keeping an Extra Eye on Your Stack with CloudWatch Events", "author": ["Emerson Loureiro"], "link": "https://tech.hbc.com/2017-01-25-keep-an-eye-on-your-stack-with-cloudwatch-events-lambda.html", "abstract": "Why an Extra Eye? There’s a lot going on in AWS; EC2 instances coming up, new releases being rolled out, services scaling up and down, new services and the underlying infrastructure being setup. If you own software running production, you probably know the drill; you need to ensure that the software and its supporting infrastructure are healthy and stable. You will also eventually need to diagnose production issues. In short, you need a way to keep an eye on your software. The scale at which we run things here at Gilt , with over 300 services in production, each with multiple instances, means this is even more important. In AWS, CloudWatch Events is a powerful tool for monitoring your resources. In very simple terms, it allows you to receive notifications about some of your infrastructure in production, and then lets you decide what to do with it. This last part is where Lambdas come in, and I’ll go into the details of how that’s done in a minute. First, let’s look at some of the events you can receive with CloudWatch. EC2 : instance state changes ( starting , running , stopping , stopped , and terminated ); CodeDeploy : deployment state changes ( started , succeeded , failed ); AutoScaling : instance terminated, instance added to AutoScaling Group; ECS : task state changes The Basic Framework The framework for consuming these events consists of three parts: a CloudWatch rule , a permission , and a target . In our case, the target is a Lambda , but it can also be a SQS queue, an SNS topic and a few other things. The CloudWatch rule determines the actual event you are interested in receiving. The Lambda is what will receive the event and allow you to act on it (e.g., send an email notification). Finally, the permission binds the rule to the Lambda, enabling the Lambda invocation whenever the rule is met. In a little bit more details, the CloudWatch rule consists of a source - i.e., the AWS service where the event originates - a detail-type , specifying the specific event you are interested in receiving - e.g., failed deployments - and finally a detail which is essentially a filter. For CodeDeploy deployments, for example, that would be which deployment groups the events should be fired for. Here’s an example of a rule we actually use in production at Gilt . This rule will fire events whenever instances under the production and canary AutoScaling groups of the service some-service are terminated. { \"detail-type\" : [ \"EC2 Instance Terminate Successful\" ], \"source\" : [ \"aws.autoscaling\" ], \"detail\" : { \"AutoScalingGroupName\" : [ \"some-service-production-auto-scaling-group\" , \"some-service-canary-auto-scaling-group\" ] } } You can create the rule in different ways, for example via the console, and the code snippet above is the representation of the rule from CloudWatch’s point of view. Here at Gilt , we usually use CloudFormation stacks for creating resources like that, and I will illustrate how to do that in a little while. But, first, how exactly have we been using that? There are two use cases, actually. In one case, we want to be notified when an instance is terminated due to a healthcheck failure. This essentially means that the instance was running and healthy, but for whatever reason the healthcheck failed for some amount of time, and the instance was killed by the auto scaling group. This is definitely something we want to be aware of, as it may indicate a pattern on certain services - e.g., the instance type for this service is no longer enough, as it keeps on dying with an out of memory error. The other use case is for deployments. Either to know when the deployment for a new release is finished, or to help piecing together a timeline of events when investigating production issues. The CloudWatch and Lambda Setup Now let’s get into the details of how we have set that up, starting with the instance termination events. As I said early on, for creating resources in AWS, typically we rely on CloudFormation stacks, so it’s no different with our CloudWatch event + Lambda setup. Here’s the CloudFormation template that creates the CloudWatch rule for instance termination events. InstanceTerminationEvent : Type : AWS::Events::Rule Properties : Name : my-instance-termination-rule State : ENABLED Targets : - Arn : arn:aws:lambda:us-east-1:123456789:function:my-instance-termination-lambda-function Id : my-instance-termination-rule-target EventPattern : source : [ ' aws.autoscaling' ] detail-type : [ ' EC2 Instance Terminate Successful' ] detail : AutoScalingGroupName : - !Ref YourAutoScalingGroup - !Ref AnotherOfYourAutoScalingGroup When the rule is matched to an event, given the conditions stated on the template, it will invoke the target - in this case a Lambda. The ARN of the Lambda is then provided as the value of the target. This rule is essentially stating that, when an EC2 instance is terminated within the auto scaling groups defined, the Lambda should be triggered. The event itself is then used as the input to the Lambda. Remember though that you also need a permission to allow the rule to invoke the target. Here’s how we define it on a CloudFormation template. InstanceTerminationLambdaPermission : Type : AWS::Lambda::Permission Properties : Action : lambda:* FunctionName : arn:aws:lambda:us-east-1:123456789:function:my-instance-termination-lambda-function Principal : events.amazonaws.com SourceArn : !GetAtt [ ' InstanceTerminationEvent' , ' Arn' ] The Lambda itself is an exception; instead of creating it via a CloudFormation template, we simply defined it via the console. That way, it’s simpler to test it out as and perform code changes. Below is our Lambda - in Python - which takes the instance termination events and sends an email to our team with details around the instance that has been terminated, the time, and also the cause. In this particular case, as I mentioned above, we are only interested in instances that have been terminated due to a health check failure, so the cause on the emails will always be the same. It’s worth pointing out though that the email notification is just one option. You can also, for example, integrate your Lambda with something like PagerDuty if you wish to have more real time alerts. import boto3 # A wrappper for the instance termination event we get from AWS class Ec2TerminationEvent : def __init__ ( self , event ): detail = event [ 'detail' ] self . instance_id = detail [ 'EC2InstanceId' ] self . cause = detail [ 'Cause' ] self . terminated_at = detail [ 'EndTime' ] self . is_health_check_failure = self . cause . endswith ( 'ELB system health check failure.' ) ec2_client = boto3 . client ( 'ec2' ) # Fetching the instance name instance = ((( ec2_client . describe_instances ( InstanceIds = [ self . instance_id ])[ 'Reservations' ])[ 0 ])[ 'Instances' ])[ 0 ] instance_tags = instance [ 'Tags' ] self . instance_name = None for instance_tag in instance_tags : tag_key = instance_tag [ 'Key' ] if tag_key == 'Name' : self . instance_name = instance_tag [ 'Value' ] def lambda_handler ( event , context ): print ( 'Received EC2 termination event: {}' . format ( event )) ec2_termination_event = Ec2TerminationEvent ( event ) if ec2_termination_event . is_health_check_failure : print ( 'Event for instance {} is a health check failure' . format ( ec2_termination_event . instance_id )) send_email ( ec2_termination_event ) return 'ok' def send_email ( ec2_termination_event ): ses_client = boto3 . client ( 'ses' ) # Simply change the email address below to the one for your # team, for example destination = { 'ToAddresses' : [ 'your-email-address@your-domain.com' ] } email_subject = { 'Data' : 'EC2 Instance {} Terminated' . format ( ec2_termination_event . instance_id ) } email_body = { 'Html' : { 'Data' : create_message_text ( ec2_termination_event ) } } email_message = { 'Subject' : email_subject , 'Body' : email_body } # Also change the email addresses below to the ones applicable # to your case send_email_response = ses_client . send_email ( Source = 'email-address-to-send-to@your-domain.com' , Destination = destination , Message = email_message , ReplyToAddresses = [ 'your-email-address@your-domain.com' ]) message_id = send_email_response [ 'MessageId' ] print ( 'Successfully sent email with message id {}' . format ( message_id )) def create_message_text ( ec2_termination_event ): instance_id_line = '<b>Instance</b>: <a href= \\\" https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:search=' + ec2_termination_event . instance_id + ';sort=tag:Name \\\" >' + ec2_termination_event . instance_id + '</a><br>' ec2_termination_event . instance_name_line = '' if ec2_termination_event . instance_name is not None : instance_name_line = '<b>Instance name</b>: ' + ec2_termination_event . instance_name + '<br>' cause_line = '<b>Cause</b>: ' + ec2_termination_event . cause + '<br>' terminated_at_line = '<b>Terminated at</b>: ' + ec2_termination_event . terminated_at + '<br>' return instance_id_line + instance_name_line + cause_line + terminated_at_line For our deployment notifications, the setup is fairly similar. For this, as I said before, we are only interested in events for deployments that have succeeded (even though the setup here can be easily extended to include failed deployments too!). Here is the CloudFormation template snippet for creating the rule and the permission. CodeDeploySuccessNotificationEvent : Type : AWS::Events::Rule Properties : Name : my-deployment-successful-rule State : ENABLED Targets : - Arn : arn:aws:lambda:us-east-1:123456789:function:my-deployment-successful-lambda-function Id : my-deployment-successful-target EventPattern : source : [ ' aws.codedeploy' ] detail-type : [ ' CodeDeploy Deployment State-change Notification' ] detail : state : [ SUCCESS ] application : [ !Ref YourCodedeployApplication ] CodeDeploySuccessNotificationEventLambdaPermission : Type : AWS::Lambda::Permission Properties : Action : lambda:* FunctionName : arn:aws:lambda:us-east-1:123456789:function:my-deployment-successful-lambda-function Principal : events.amazonaws.com SourceArn : !GetAtt [ ' CodeDeploySuccessNotificationEvent' , ' Arn' ] And below is the Lambda that receives the event, and sends out an email notification. On the email it will be included the application name, the deployment group where the deployment has happened, as well as the release version. Our actual Lambda in production also fires a deployment notification to NewRelic. In there you have a history of the releases for a given service, and how metrics have changed since each release. That can come in handy when establishing timelines and finding out exactly which release is broken. import boto3 import httplib # A wrapper for the deployment successful we receive\n# from CodeDeploy. Extracs the application, deployment\n# group, release version, etc. class CodeDeployEvent (): def __init__ ( self , event ): detail = event [ 'detail' ] self . application = detail [ 'application' ] self . deployment_id = detail [ 'deploymentId' ] self . deployment_group = detail [ 'deploymentGroup' ] codedeploy_client = boto3 . client ( 'codedeploy' ) deployment = codedeploy_client . get_deployment ( deploymentId = self . deployment_id ) self . version = None self . release_file = None revision = ( deployment [ 'deploymentInfo' ])[ 'revision' ] if ( revision [ 'revisionType' ] == 'S3' ): s3_location = revision [ 's3Location' ] self . release_file = s3_location [ 'key' ] last_slash_index = self . release_file . rfind ( '/' ) last_dot_index = self . release_file . rfind ( '.' ) self . version = self . release_file [ last_slash_index + 1 : last_dot_index ] def lambda_handler ( event , context ): code_deploy_event = CodeDeployEvent ( event ) print ( 'Received success deploy {} for application {} and deployment group {}' . format ( code_deploy_event . deployment_id , code_deploy_event . application , code_deploy_event . deployment_group )) send_email ( code_deploy_event ) return 'ok' def send_email ( code_deploy_event ): ses_client = boto3 . client ( 'ses' ) # Simply change the email address below to the one for your # team, for example destination = { 'ToAddresses' : [ 'your-email-address@your-domain.com' ] } email_subject = { 'Data' : 'Deployment {} Successful' . format ( code_deploy_event . deployment_id ) } email_body = { 'Html' : { 'Data' : create_message_text ( code_deploy_event ) } } email_message = { 'Subject' : email_subject , 'Body' : email_body } send_email_response = ses_client . send_email ( Source = 'email-address-to-send-to@your-domain.com' , Destination = destination , Message = email_message , ReplyToAddresses = [ 'your-email-address@your-domain.com' ]) message_id = send_email_response [ 'MessageId' ] print ( 'Successfully sent email with message id {}' . format ( message_id )) def create_message_text ( code_deploy_event ): deployment_id_line = '<b>Deployment id</b>: <a href= \\\" https://console.aws.amazon.com/codedeploy/home?region=us-east-1#/deployments/' + code_deploy_event . deployment_id + ' \\\" >' + code_deploy_event . deployment_id + '</a><br>' version_line = '<b>Revision</b>: ' + code_deploy_event . version + '<br>' application_line = '<b>Application</b>: ' + code_deploy_event . application + '<br>' deployment_group_line = '<b>Deployment group</b>: ' + code_deploy_event . deployment_group return deployment_id_line + version_line + application_line + deployment_group_line Final Thoughts We have had this setup running in one of our teams here at Gilt for quite a few months now, and the results are satisfying. Instance termination events, given their real time nature, allow us, for example, to act quickly and prevent potential outages on our services. Also, it has already allowed us to identify services that had not enough memory allocated, and thus needed code changes or a change of instance type in order to stabilize them. In short, it’s giving us a level visibility we never really had before and enabling us to be more proactive towards keeping our services in good shape. Finally, deployment notifications add more to the debugging side of things. They let us establish a timeline of events - e.g., when releases have gone out - and with that more quickly identify releases and code changes that have broken a particular service in production. Ultimately, this speeds up the process of bringing a service back to a healthy state. We feel like our current setup is enough for our needs, but certainly we will be looking at expanding the range of events we are watching out for when the need arises. At the end of the day, it’s all about having quality information in order to help keep our services running well.", "date": "2017-01-25"},
{"website": "HBCTech", "title": "Perfect Overnight Cold Brew", "author": "Unknown", "link": "https://tech.hbc.com/2017-01-25-perfect-overnight-cold-brew.html", "abstract": "When HBC’s Mobile team worked at 1 Madison Avenue , my morning coffee ritual involved getting a large black iced coffee from myWayCup as I exited the 6 train at 23rd Street. What they served at myWayCup—a private-label version of Intelligentsia Coffee’s House Blend—was so good that I switched to iced coffee year-round—even through brutal New York winters—a trait that often earned me quizzical looks when ordering my preferred drink during a snowstorm. About a year later when the Mobile team moved back to the 2 Park Avenue office, I searched the neighborhood for iced coffee I liked as much, but came up empty. The cold brews I tried tended to be syrupy and super-concentrated, while the ones made with a hot brew process had all the subtlety scorched out of the beans, leaving a jagged, edgy texture. And too often, stores didn’t turn over iced coffee frequently enough in the winter, so you’d end up with something that had become stale after days of storage. Without a local favorite, I started experimenting with making my own iced coffee. At times, there were catastrophic failures. At least two glass carafes gave their lives in pursuit of coffee perfection, and an otherwise white wall at the office somehow acquired a coffee streak arcing towards the floor. I even managed to melt one of my coffee grinders on top of a stove I didn’t realize was still hot. Despite these embarrassing setbacks, the technique continued to evolve and improve, and I eventually switched from the laborious process of rapid cooling a hot brew to the simpler—but far lengthier—process of an overnight cold brew. True, there’s no instant gratification: my coffee intake now requires preparing a day in advance, but the result is a well-balanced brew. It’s not the thick, need-to-dilute-it-with-water cold brew that used to get delivered to our office in metal kegs. (A co-worker once posted notices warning of the jitters that ensue when forgetting to water it down.) Nor does it have the unrefined taste of beans that have had great violence done to them by exposure to extreme heat followed by cooling. To me, this technique yields the perfect iced coffee. What I use to brew: Intelligentsia House Blend beans: Delicious coffee, full-bodied but not over-roasted or overly bitter. Using beans as opposed to pre-ground coffee ensures maximum freshness when you brew. Ground coffee oxidizes quickly and will soon taste stale if not used right away. (I signed up for Intelligentsia’s mail order subscription service so I’m never out of fresh beans.) A Bodum Bistro Electric Burr Grinder : This is a conical burr grinder , which means the grounds come out a consistent granularity. Most home coffee grinders are blade grinders that deliver grounds of varying size. In a French press, the finer grounds will not be caught by the filter; those grounds will end up on your tongue like silt, which will not make for pleasant drinking. If you’re a coffee enthusiast, you should seriously consider a conical burr grinder. A 51oz (1.5 liter) Bodum Chambord French Press . This size yields about 2-3 days of coffee at my rate of consumption, which is ideal; beyond 3 days, the coffee would begin tasting stale anyway. A pitcher of NYC tap water filtered through a Brita . I start the brewing process in the morning, and I keep the French press on the counter until nighttime so I can stir periodically when I get the chance. Then, when I go to bed, I put the carafe in the fridge so it’s cold and ready to drink when I get up in the morning. The brewing steps I follow: Set the grinder’s coarseness one notch to the left of the French Press icon on the dial. Set the grinder to grind for 15 seconds. Fill the hopper with beans. Press the grind button and remain calm. Take the fresh grounds out of the grinder— be sure to stop and smell the coffee! —and dump them in the glass carafe of the French press. Return any beans remaining in the hopper to an airtight container. Fill the carafe nearly to the top with cold water. Filtered tap water or bottled water is recommended, depending on the desirability of the water coming out of your faucet. (Remember, good beans + bad water = bad coffee.) Mix the water and grounds together by stirring for 10 seconds. Cover the top of the carafe with cellophane wrap or aluminum foil to avoid excess exposure to air. Leave out at room temperature for 16 hours, stirring periodically for 10 seconds if possible. (I usually end up stirring about 5 times or so for the typical batch.) Put in fridge for the final 8 hours. Remove from fridge and stir for 10 seconds. Press the grounds and serve the coffee. Store what remains in an airtight glass container, and drink within 3 days. Overall, it’s a 24-hour process, although there are some tricks you can use to cut down on brew time. More frequent, vigorous and longer stirring will help shorten the process, as will repeated pressing at the end. (Pressing the grounds, removing the press, stirring the grounds, and pressing again.) You can adjust according to your equipment, beans and personal taste, but in my experience you need at least 16 hours of brew time to let the flavors fully form. Hope you enjoy it as much as I do!", "date": "2017-01-25"},
{"website": "HBCTech", "title": "NYC Scrum User Group - January 19th", "author": "Unknown", "link": "https://tech.hbc.com/2017-01-11-nyc-scrum-user-group-meetup.html", "abstract": "We’ll be hosting our first meetup of 2017 in partnership with the NYC Scrum User Group on Thursday, January 19th. This is our first time hosting this group and we’re off to a great start: Ken Rubin will be joining us to lead a talk on Agile. More on Ken: Ken is the author of Amazon’s #1 best selling book Essential Scrum: A Practical Guide to the Most Popular Agile Process. As an agile thought leader, he founded Innolution where he helps organizations thrive through the application of agile principles in an effective and economically sensible way. He has coached over 200 companies ranging from startups to Fortune 10, and is an angel investor and mentor to numerous exciting startups. As a Certified Scrum Trainer, Ken has trained over 24,000 people in agile / Scrum as well as object-oriented technology. He was the first managing director of the worldwide Scrum Alliance, a nonprofit organization focused on transforming the world of work using Scrum. The title of the talk is “Agile Transition Lessons That Address Practical Questions” and will address questions like: Is there a way to quantify the cost of the transition? How many teams or what scope should the initial transition effort cover? Should we use an internal coach or hire an external coach? How does training fit in to the adoption? How do we measure our success? Should we use an existing scaling framework or develop our own? If you plan to attend, please RSVP on the NYC Scrum User Group Meetup page . As always there will be refreshments, networking opportunities and a chance to chat with the speaker. We hope to see you there!", "date": "2017-01-11"},
{"website": "HBCTech", "title": "BackOffice Hike", "author": ["Ryan Martin"], "link": "https://tech.hbc.com/2017-01-05-backoffice-hike.html", "abstract": "Now that winter is here and the cold is keeping us inside, I thought it would be good to dream of warm days and look back at a BackOffice team outing from October. On what turned out to be an unseasonably warm and beautiful Monday, a bunch of us took the day off and hiked the Breakneck Ridge Trail . It’s a 3.5-mile loop that climbs quickly (read: steep scrambling) to amazing views of the Hudson River Valley. It’s accessible via Metro-North out of NYC, so it’s a great target for a day trip from the City. Here are some photos of the trip - can’t wait for the next one.", "date": "2017-01-05"},
{"website": "HBCTech", "title": "December Meetups at HBC Digital", "author": ["John Coghlan"], "link": "https://tech.hbc.com/2016-12-07-december-meetups.html", "abstract": "We’re closing out 2016 with two more meetups at our offices in Brookfied Place. New York Scala University - December 13 On Tuesday, December 13 at 6:30pm, Puneet Arya, Senior Application Developer at HBC Digital, will talk about how MongoDB and the Play Framework get along. The talk will touch on: What is MongoDB and how it can be used. How MongoDB works with Play Framework. What configuration is needed to make Mongo DB work with Play. Explore CRUD operations for MongoDB with Scala and Play Framework. Please RSVP here if you’d like to attend. HBC Digital Technology Meetup - December 15 We’re partnering with the NYC PostgreSQL User Group on Thursday, December 15 at 6:30pm to host Bruce Momjian . Here’s the description of Bruce’s talk: Postgres 9.6 adds many features that take the database to a new level of scalability, with parallelism and multi-CPU-socket scaling improvements. Easier maintenance is achieved by reduced cleanup overhead and adding snapshot and idle-transaction controls. Enhanced monitoring simplifies the job of administrators. Foreign data wrapper and full text improvements are also included in this release. The talk will also cover some of the major focuses of the next major release, Postgres 10. Speaker: Bruce Momjian is co-founder and core team member of the PostgreSQL Global Development Group, and has worked on PostgreSQL since 1996. He has been employed by EnterpriseDB since 2006. He has spoken at many international open-source conferences and is the author of PostgreSQL: Introduction and Concepts, published by Addison-Wesley. Prior to his involvement with PostgreSQL, Bruce worked as a consultant, developing \ncustom database applications for some of the world’s largest law firms. As an academic, Bruce holds a Masters in Education, was a high school computer science teacher, and lectures internationally at universities. Please RSVP here if you’d like to attend.", "date": "2016-12-07"},
{"website": "HBCTech", "title": "Increasing Build IQ with Travis CI", "author": ["Andrew Powell"], "link": "https://tech.hbc.com/2016-11-16-increasing-build-iq-travis-ci.html", "abstract": "Continuous Integration is a must these days. And for social, open source\nprojects it’s crucial. Our tool of choice for automated testing is Travis CI . Like most tools, Travis does what it does\nwell. Unfortunately it’s not very “smart”. Heaven help you if you have a large\nor modular project with a multitude of tests - you’ll be waiting an eternity\nbetween builds. And that’s exactly what we ran into. We have a repository that contains 30 NPM\nmodules , each with their own specs (tests). These modules are part of an aging\nassets pipeline that I briefly mentioned last week. As such each module is subject to a litany of tasks each time a change\nis made. Travis CI is hooked into the repo and for each Pull Request would run\nspecs for every module, assuring that there are no errors in the code changes\ncontained in the PR. When you’re only working on one or two modules the run-time\nfor the tasks is relatively low; typically 1 - 2 minutes. That of course depends\non things such as npm install time, as each module requires an install for testing. Multiply that by 30 and you start to see where the problem arises. Waiting Sucks Without targeted build testing we’re left waiting or task-shifting until the\nbuild completes successfully. Our need was clear: figure out what files were\naffected, map and filter the results, and run only the specs for the modules\nchanged in any particular Pull Request or push. That’s where travis-target comes into play. Here’s a snippet of our repo structure, for reference: ui-tracking\n|--src\n   |--common\n      |--event_registry\n      |--tracking_metadata\n   |--tracking\n      |--cart\n      |--etc.. Target Acquired In order to target modules, we need to know what their normalized names are; the\nnames that we publish them to NPM under. Because of some legacy stuff baked into\nour pipeline, we store modules at group/name but publish them as group.name .\nSo let’s fire up travis-target (bear in mind we’re using ES6 syntax that Node v7 supports). const target = require ( ' travis-target ' ); const pattern = /^src \\/ / ; let targets = await target (); Let’s pretend that the common.event_registry and tracking.cart modules were\nboth modified in one Pull Request (a common pattern for us) - our results\nwould could look like this: [ ' README.md ' , ' src/common/event_registry/js/event_registry.js ' , ' src/common/event_registry/js/registry.js ' , ' src/common/event_registry/package.json ' , ' src/tracking/cart/js/cart.js ' , ' src/tracking/cart/package.json ' ] But that’s just silly, so let’s give travis-target some options to work with: const target = require ( ' travis-target ' ); const pattern = /^src \\/ / ; let targets = await target ({ pattern : pattern , map : ( result ) => { let parts ; result = result . replace ( pattern , '' ); parts = result . split ( ' / ' ); return parts . slice ( 0 , 2 ). join ( ' . ' ); } }); By passing pattern in options, we’re telling travis-target to filter on (or\nreturn only those results which match) the regular expression pattern. That gives\nus an initial result set of directories starting with src/ . [ ' src/common/event_registry/js ' , ' src/tracking/cart/js ' ] You’ll notice that the initial example of results contained some duplicate\ndirectories; travis-target cleans that up for you. Next, we specify the map function on options. That’ll let us transform the each\nelement in the Array of results so that it’s ready to use. Our results would\nnow look like this: [ ' common.event_registry ' , ' tracking.cart ' ] Great Justice Using this last result set, we now know which modules were affected in the PR,\nand we know which modules to run specs for. Our next steps are firing off a\nsequence of shell commands using @exponent/spawn-async ,\nwhich plays nicely with async/await patterns now supported in Node v7.1 with\nthe --harmony-async-await flag. Since we implemented this pattern, build times for our PRs are in the 1-2 minute\nrange; a vast improvement and one sure to bring developer happiness in some small\ndegree. Cheers! Originally published at shellscape.org on\nNovember 16, 2016.", "date": "2016-11-16"},
{"website": "HBCTech", "title": "Where to find our team in October", "author": ["John Coghlan"], "link": "https://tech.hbc.com/2016-10-07-where-to-find-our-team-in-october.html", "abstract": "Here’s where to find HBC Tech this month: Oct 11 - Dana Pylayeva, Agile Coach, is leading a Product Discovery Game Workshop at the Agile/Lean Practitioners Meetup - RSVP Oct 13 - Sophie Huang, Product Manager, is on the “From Downloads to Daily Active Users” panel at the 🍕🍺📱 (Pizza + Beer + Mobile) Meetup - RSVP Oct 18 - Justin Riservato, Director of Data Engineering & Data Science, is on the “Push Data to Every Department” panel at Looker Join - SOLD OUT Oct 18 - We’re hosting the HBC Digital Technology Meetup featuring Tom Beute, Front End Engineer, and Jared Stetzelberg, UX Designer, talking about “Creating A Live Styleguide: Bridge the Gap Between Tech & Design Teams” - RSVP Oct 25 - We’re hosting the Dublin Swift Meetup at our Dublin office. This meetup will feature 3 great speakers including our own Ievgen Salo giving a talk on Advanced Collection in Swift. - RSVP Oct 26 - We’re co-hosting the Dublin Scala Users Group with our friends from the Functional Kats Meetup. This Meetup will feature Juan Manuel Serrano on Scala and be held at Workday’s Dublin office. - RSVP See you soon!", "date": "2016-10-07"},
{"website": "HBCTech", "title": "New open source project: scala-fedex", "author": ["Ryan Caloras"], "link": "https://tech.hbc.com/2016-08-21-scala-fedex.html", "abstract": "We recently made the decision to switch from Newgistics to FedEx SmartPost for customer returns at Gilt. A couple of factors contributed to the decision, but the prettiness of FedEx’s API was not one of them - it’s not exactly the most developer friendly API you can find. FedEx Web Services are a collection of APIs to do everything from generating shipping labels to tracking packages. Unfortunately, they’re still using SOAP and WSDLs! That means poor support for a modern language like Scala. Their sample Java client contained a bunch of unfriendly Scala code (e.g. Xml, blocking requests, and Java Classes rather than native Scala). Enter scala-fedex Using scalaxb we were able to generate a native non-blocking Scala client from FedEx’s WSDL. We then added a wrapper to further reduce the boilerplate of the raw generated client. The final result being a thin async Scala client for FedEx’s Ship Service API. It also provides a much cleaner example of usage than the previously mentioned Java code. We can now use this to generate FedEx return labels on Gilt using native reactive Scala code! To support the community, we decided to open source and publish scala-fedex. You can find more specifics on the scala-fedex repo .", "date": "2016-08-21"},
{"website": "HBCTech", "title": "How to convert fully connected layers into equivalent convolutional ones", "author": ["Pau Carré Cardona"], "link": "https://tech.hbc.com/2016-05-18-fully-connected-to-convolutional-conversion.html", "abstract": "The Problem Say we want to build system to detect dresses in images using a deep convolutional network. What we have is a database of 64x128 pixels images that either fully contain a dress or another object (a tree, the sky, a building, a car…). With that data we train a deep convolutional network and we end up successfully with a high accuracy rate in the test set. The problem comes when trying to detect dresses on arbitrarily large images. As images from cameras are usually far larger than 64x128 pixels, the output of the last convolutional layer will also be larger. Thus, the fully connected layer won’t be able to use it as the dimensions will be incompatible. This happens because a fully connected layer is a matrix multiplication and it’s not possible to multiply a matrix with vectors or matrices of arbitrary sizes. Let’s assume we have 1024x512 pixels images taken from a camera. In order to detect dresses in an image, we would need to first forward it throughout the convolutional layers. This will work as convolutional layers can adapt to larger input sizes. Assuming the convolutional and max pool layers reduce the input dimensions by a factor of 32 , we would get an output of 32x16 units in the last convolutional layer. On the other hand, for the training and test images of 64x128 pixels, we would get an output of 2x4 units. That size of 2x4 units is the only one the fully connected layer matrix is compatible with. Now the question is, how do we convert our camera 32x16 units into the fully connected 2x4 units ? The Wrong Solution One way to do it is by simply generating all possible 2x4 crops from the 32x16 units. That means we would generate 403 samples of 2x4 units ( (32 - 2 + 1) x (16 - 4 + 1) = 403 ). Finally, we would go one by one forwarding those 403 samples throughout the fully connected layers and arrange them spatially. The problem with that approach is that the cost of cropping and forwarding images throughout the fully connected layers can be impractical. On top of that, if the network reduction factor is lower or the camera images have a higher resolution, the number of samples will grow in a multiplicative way. The Right Solution Fortunately, there is a way to convert a fully connected layer into a convolutional layer.\nFirst off, we will have to define a topology for our fully connected layers and then convert one by one each fully connected layer. Let’s say we have a first fully connected layer with 4 units and a final single binary unit that outputs the probability of the image being a dress.\nThis diagram describes the topology: Converting the first fully connected layer The idea here is to transform the matrix A into a convolutional layer. Doing that it’s pretty straightforward as the rows of the matrix A can be interpreted as convolutions applied to the flattened input V . Let’s first write down the classical deep learning convolution operator: When both the signal and the filter are of the same size, the convolution will generate a vector of size one. Hence, the convolution will be equivalent to the dot product: Applying this property to our convolutional conversion task, we will be able to transform a linear operator into a vector of convolutions: Therefore, we have the following transformed convolutional layer for the first fully connected layer: More formally, we will have as many feature maps as rows the matrix A has. Furthermore, the i -th feature map will have as filter the i -th row of the matrix A . Here we are assuming that the input of the fully connected layer is flattened and also that the fully connected layer only receives a single feature map from the last convolutional layer. For multidimensional convolutions with many feature maps, the transformation will depend on the way the framework we use encodes the different layer types (convolutional and fully connected). In case of Torch , it’s pretty easy as one simply has to copy the biases and the weights of the fully connected layer into the convolutional layer. The caveat is that the convolutional layer has to be declared using the following parameters: Number of input feature maps : as many as output feature maps the last convolutional layer has. Number of output feature maps : number of outputs the fully connected layer has. Filter dimensions : the dimensions of the output of each feature map in the last convolutional layer (we assume the all of the feature maps have the same output dimensions). Converting the second fully connected layer After the first transformation we will have in the second fully connected layer an input that has many feature maps of size one. The equivalent convolutional layer will be the following: Number of input feature maps : as many input feature maps as output feature maps the last transformed convolutional layer has. It will also be equivalent to the input units the original second fully connected layer has. Number of output feature maps : as many output feature maps as outputs the second fully connected layer has. In our case we have a single output and therefore the layer will only have a single output feature map. In case we would have more outputs or an additional fully connected layer, we would need to add more feature maps. Filter values : the filter architecture is pretty simple as all the input feature maps have units of size one. This implies that the filters will be of size one. The value of the filter in the feature map that connects the n -th input unit with the m -th output unit will be equal to the element in the n -th column and the m -th row of the matrix B .\nFor our specific case there is one single output, thus m is equal to 1 . This makes the transformation even easier. Nevertheless, we should keep in mind that we could potentially have multiple outputs. For our example, the second fully connected layer will be converted into the following convolutional layer: Always be convolutional In this post we’ve discussed how to transform fully connected layers into an equivalent convolutional layer. Once the network no longer has fully connected layers, we will be able to get rid of all the problems they cause when dealing with inputs of arbitrary sizes. Nevertheless, when designing a new neural network from scratch it’s always a good idea to design it substituting all fully connected layers with convolutional layers. This way, there is not only no need for any conversion but we will also get far more flexibility in our network architecture.", "date": "2016-05-18"},
{"website": "HBCTech", "title": "Akka HTTP Talk with Cake Solutions", "author": "Unknown", "link": "https://tech.hbc.com/2016-04-07-akka-http-talk-with-cake-solutions.html", "abstract": "We are thrilled to be hosting Aleksandr Ivanov of Cake Solutions on Tuesday, April 12th. He’ll be presenting an excellent talk on Akka HTTP . Who is Aleksandr? We’re glad you asked: Aleksandr Ivanov is a senior software engineer at Cake Solutions, one of the leading European companies building Reactive Software with Scala.  Scala is his main language of choice since 2011. He’s taken part in various projects, from developing backends for trivial web applications to enterprise-level reactive system and machine learning apps. Beside engineering, he’s taking an active part in the life of the developer community, giving talks at local meetups, writing articles and helping others on the mailing list, Gitter and Stack Overflow. He’s always ready to hear and discuss interesting projects and events, share his experience or simply have a nice conversation over a drink. Refreshments will be served! Important : You must provide First Name & Last Name in order to enter the building. Please RSVP!", "date": "2016-04-07"},
{"website": "HBCTech", "title": "Front End Engineering Lightning Talks with HBC Digital", "author": "Unknown", "link": "https://tech.hbc.com/2016-03-24-front-end-engineering-lightning-talks-with-hbc-digital.html", "abstract": "Join us for an evening of lightning talks by 4 of HBC Digital’s Front End Engineers and an introduction by Steve Jacobs, SVP, Digital Technology and Demand Marketing. Ozgur Uksal - Front End Engineer: Working with Typescript Lei Zhu - Front End Engineer: Redux Norman Chou - Front End Engineer: Using React router Rinat Ussenov - Front End Engineer: Juggling bits in Javascript Refreshments will be served! Important : You must provide First Name & Last Name in order to enter the building. Please RSVP!", "date": "2016-03-24"},
{"website": "HBCTech", "title": "gulp-scan • Find Yourself Some Strings", "author": ["Andrew Powell"], "link": "https://tech.hbc.com/2016-02-15-gulp-scan-find-strings.html", "abstract": "We recently ran across the need to simply scan a file for a particular term during\none of our build processes. Surpringly enough, we didn’t find a Gulp plugin that performed only that one simple task. And so gulp-scan was born and now resides on npmjs.org . Simply put - gulp-scan is a Gulp plugin to scan a file for a particular string\nor (regular) expression. Setting Up As per usual, you’ll have to require the module. var gulp = require ( ' gulp ' ); var scan = require ( ' gulp-scan ' ); Doing Something Useful gulp . task ( ' default ' , function () { return gulp . src ( ' src/file.ext ' ) . pipe ( scan ({ term : ' @import ' , fn : function ( match ) { // do something with {String} match }})); }); Or if RegularExpressions are more your speed: gulp . task ( ' default ' , function () { return gulp . src ( ' src/file.ext ' ) . pipe ( scan ({ term : / \\@ import/gi , fn : function ( match ) { // do something with {String} match }})); }); Pretty simple. There’s always room for improvement, and we welcome contribution on Github .", "date": "2016-02-15"},
{"website": "HBCTech", "title": "Codedeploy Notifications as a Service", "author": ["Emerson Loureiro"], "link": "https://tech.hbc.com/2016-02-10-codedeploy-notifications.html", "abstract": "After moving our software stack to AWS, some of us here at HBC Tech have started deploying our services to production using AWS’s Codedeploy . Before that, in a not-so-distant past, we used an in-house tool for deployments - IonCannon. One of the things IonCannon provided were deployment notifications. In particular, it would: Send an email to the developer who pushed the deployment, for successful and failed deployments; Send a new deployment notification to Newrelic; Optionally, send a Hipchat message to a pre-configured room, also for successful and failed deployments. These notifications had a few advantages. If you - like me - prefer to switch to something else while the deployment is ongoing, you would probably want to be notified when it is finished; i.e., “don’t call me, I’ll call you”-sort of thing. The email notifications were a good fit for that; Having notifications sent via more open channels, like Newrelic and Hipchat, meant that anyone in the team - or in the company really - could quickly check when a given service was released, which version was released, whether it was out on a canary or all production nodes, etc. In Newrelic, in particular, one can see for example, all deployments for a given time range and filter out errors based on specific deployments. These can come in handy when trying to identify a potentially broken release. Codedeploy, however, doesn’t provide anything out-of-the-box for deployment notifications. With that in mind, we have started looking at the different options available to achieve that. For example, AWS itself has the necessary components to get that working - e.g., SNS topics, Codedeploy hooks - but that means you have to do the gluing between your application and those components yourself and, with Codedeploy hooks in particular, on an application-by-application basis. Initially, what some of us have done was a really simple Newrelic deployment notification, by hitting Newrelic’s deployment API in the Codedeploy healthcheck script. This approach worked well for successful deployments. Because the healthcheck script is the last hook called by Codedeploy, it was safe to assume the deployment was successful. It was also good for realtime purposes, i.e., the deployment notification would be triggered at the same as the deployment itself. Despite that, one can easily think of more complicated workflows. For example, let’s say I want to notify on failed deployments now. Since a failure can happen at any stage of the deployment, the healthcheck hook will not even be called in those cases. Apart from failed deployments, it’s reasonable to think about notifications via email, SNS topics, and so on. All of that essentially means adding various logic to different Codedeploy hooks, triggering the notifications “manually” from there - which for things like sending an email isn’t as simple as hitting an endpoint. Duplication of that logic across different services is then inevitable. An alternative to that would be Cloudtrail and a Lambda. However, given the delay for delivering Cloudtrail log files to S3, we would lose too much on the realtime aspect of the notifications. One good aspect of this approach, though, is that it could handle different applications with a single Lambda. So, the ideal approach here would be one that could deliver realtime notifications - or as close to that as possible - and handle multiple Codedeploy applications. Given that, the solution we have been using to some extent here at HBC Tech is to provide deployment notifications in a configurable way, as a service, by talking directly to Codedeploy. Below is a high-level view of our solution. In essence, our codedeploy notifications service gets deployments directly from Codedeploy and relies on a number of different channels - e.g., SNS, SES, Newrelic, Hipchat - for sending out deployment notifications. These channels are implemented and plugged in as we need though, so not really part of the core of our service. Dynamo DB is used for persisting registrations - more on that below - and successful notifications - to prevent duplications. We have decided to require explicit registration for any application that we would want to have deployment notifications. There are two reasons for doing this. First, our service runs in an account where different applications - from different teams - are running, so we wanted the ability to select which of those would have deployment notifications triggered. Second, as part of registering the application, we wanted the ability to define over which channels those notifications would be triggered. So our service provides an endpoint that takes care of registering a Codedeploy application. Here’s what a request to this endpoint look like. curl -H 'Content-type: application/json' -X POST -d '{ \"codedeploy_application_name\": \"CODE_DEPLOY_APPLICATION_NAME\", \"notifications\": [ { \"newrelic_notification\": { \"application_name\": \"NEWRELIC_APPLICATION_NAME\" } } ] }' 'http://localhost:9000/registrations' This will register a Codedeploy application and set it up for Newrelic notifications. The Codedeploy application name - CODE_DEPLOY_APPLICATION_NAME above - is used for fetching deployments, so it needs to be the exact name of the application in Codedeploy. The Newrelic application name - NEWRELIC_APPLICATION_NAME - on the other hand, is used to tell Newrelic which application the deployment notification belongs to. Even though we have only illustrated a single channel above, multiple ones can be provided, each always containing setup specific to that channel - e.g., SMTP server for Emails, topic name for SNS. For each registered application, the service then queries Codedeploy for all deployments across all of their deployment groups, for a given time window. Any deployment marked as successful will have a notification triggered over all channels configured for that application. Each successful notification is then saved in Dynamo. That’s done on scheduled-basis - i.e., every time a pre-configured amount of time passes the service checks again for deployments. This means we can make the deployment notifications as realtime as possible, by just adjusting the scheduling frequency. Our idea of a notification channel is completely generic. In other words, it’s independent with regards to the reason behind the notification. In that sense, it would be perfectly possible to register Newrelic notifications for failed deployments - even though in practical terms it would be a bit of nonsense, given Newrelic notifications are meant for successful deployments only. We leave it up to those registering their applications to make sure the setup is sound. Even though we have talked about a single service doing all of the above, our solution, in fact, is split into two projects. One is a library - codedeploy-notifications - which provides an API for adding registrations, listing Codedeploy deployments, and triggering notifications. The service is then separate, simply integrating with the library. For example, for the registration endpoint we described above, the service uses the following API from codedeploy-notifications under the hood. val amazonDynamoClient = ... val registrationDao = new DynamoDbRegistrationDao ( amazonDynamoClient ) val newRelicNotificationSetup = NewrelicNotificationSetup ( \"NewrelicApplicationName\" ) val newRegistration = NewRegistration ( \"CodedeployApplicationName\" , Seq ( newRelicNotificationSetup )) registrationDao . newRegistration ( newRegistration ) Splitting things this way means that the library - being free from anything HBC Tech specific - can be open-sourced much more easily. Also, it gives users the freedom to choose how to integrate with it. Whereas we currently have it integrated with a small dedicated service, on a T2 Nano instance, others may find it better to integrate it with a service responsible for doing multiple things. Even though the service itself isn’t something open-sourcable - as it would contain API keys, passwords, and such - and it’s currently being owned by one team only, it is still generic enough so it can be used by other teams. We have been using this approach for some of our Codedeploy applications, and have been quite happy with the results. The notifications are being triggered with minimal delay - within a couple of seconds for the vast majority of the deployments and under 10 seconds in the worst-case scenarios. The codedeploy-notifications library is open source from day 1 and available here . It currently supports Newrelic notifications, for successful deployments, and there is current work to support Emails as well as notifications for failed deployments. Suggestions, comments, and contributions are, of course, always welcome.", "date": "2016-02-10"}
]