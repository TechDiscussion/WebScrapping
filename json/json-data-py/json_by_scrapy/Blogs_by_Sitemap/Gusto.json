[
{"website": "Gusto", "title": "How to run Docker and get more sleep than I did", "author": ["Vaibhav Mallya"], "link": "https://engineering.gusto.com/how-to-run-docker-and-get-more-sleep-than-i-did/", "abstract": "Bugs in Gusto’s codebase mean that people don’t get paid. Mortgage payments, college tuition, child support—they all rely on accurate, timely paychecks. That’s why we need tens of thousands of tests across Ruby, Python, and Go, to run on every commit. Our heavily Dockerized continuous integration (CI) pipeline helps us do this, at-scale, for all of our repositories. I started writing the best-practices list below after hundreds of hours and many late nights spent staring at logfiles, fixing Docker breakages, and troubleshooting buggy code. It’s been pretty helpful to other Gusto engineers, and I hope it is for you too. Where this list began I thought I was seeing things one night. I was debugging ActiveRecordDoc’s integration with our Docker-based continuous integration (CI) pipeline. ActiveRecordDoc is a homegrown Ruby gem that validates PII-related fields we have on most of our database tables. Unfortunately, the CI step that ran the gem’s Rake tasks was randomly hanging, making the test suite extremely flakey for the entire engineering team, reducing iteration velocity, and harming confidence in deploys. After spending most of the evening digging through logs, I decided to try running the exact same script locally. I eventually saw my local terminal hang the same way as the CI step. I tried to repeat the process—but it worked fine, over and over again. Was I hallucinating? I tried nine more times. The script finally hung right before it was supposed to execute a special Ruby function. I kept my hands off the keyboard, and the script stayed hung. I cautiously hit the “Up” arrow. Execution continued. Huh. Reading through the code, it looked like the hanging Ruby expression needed to output the name of every single loaded Rails model class on a single line. This was easily 300,000 characters sent to STDOUT at once. I wondered if the raw quantity of characters on a single line was causing an issue. I stopped logging the line, and the flake mode stopped. Welp. My best guess is that some buffer in the kernel wasn’t getting flushed normally, which effectively served as backpressure on the Docker process’ execution as a whole. Somehow, sending any input flushed the buffer and continued execution. Some Googling revealed a few similar bugs ( here , here , and here ) that were never fully resolved. I got around this for ActiveRecordDoc by adding a SafeLog Ruby function that would chunk the output if a single line exceeded 10,000 bytes. The flakes were fixed. Great! Dramatic recreation: When we find a bug Basics Docker provides a mechanism for defining, instantiating, and maintaining isolated execution environments. We think of Docker containers as heavier weight than processes, but lighter weight than true virtual machines. They excel at helping us run CI, generate reproducible environments, and better utilize host resources for deployed apps. Because they are not completely isolated—for example, sharing UIDS/GIDS with the host system (but only on host systems like Linux that support UIDs/GIDs)—they can be challenging to durably use when orchestrated. Terminology Image : Immutable binary blobs stored on-disk or in Dockerhub that can be instantiated into “containers”. docker images . Container : An instantiated image that runs arbitrary commands. docker container ps Host System : The  \"underlying\" system that is running a docker container. If you're running Docker containers on your laptop, the laptop is the host system. Volume : A filesystem that a docker container is able to access. A container can share volumes with other containers, and the host filesystem. docker volume ls docker-compose : An orchestration mechanism for Docker containers.  docker-compose ingests a provided .yml file and generates raw Docker commands. In theory, everything you can do with docker-compose, you can do with docker. Versioning We once had a CI outage because the mysql default image on Dockerhub got upgraded from 5.7 to 8. The Ruby installation relied on 5.7, which didn’t exist anymore on newer hosts. Some best-practices we derived: When inheriting from a base image, don't leave the base image unversioned (FROM: ruby). Always version it: (FROM: ruby:2.3.4). In your docker-compose.yml file, always explicitly specify a version of docker-compose with the version key. There are major differences between versions 2, 3, etc. Make sure the version of the docker engine you’re using is itself fixed across your fleet. Upgrade it periodically. The Docker binary is sometimes upgraded in backwards-incompatible ways. Layering Layers are simply files stored on the host operating system that are composed by Docker’s Union Filesystem driver to generate the final image. Intelligent layer usage can simplify deployment and debugging, while copy-pasting or misunderstanding layers can lead to a lot of needless work. Each local line in a Dockerfile generates a \"layer\" for the image that's ultimately generated. Each layer depends on every previous layer. Each layer is cached, which can significantly speed up your Docker builds. A Docker Image is a represented by a linear collection of layers. The lower the churn on a line in a Dockerfile, the earlier in the file it should be listed. Higher-churn lines should be near the bottom of the file. This will speed up builds by letting the Docker image builder rely on caching. If you find yourself copy-pasting the same set of layers between different Dockerfiles, take a look at multi-stage builds. They can help eliminate redundancy across your codebase. Isolation When we were initially setting up our CI system, we would regularly see failure modes where tests would fail in non-reproducible ways. After a lot of digging, it turned out that a new build’s containerized databases would sometimes connect to an old build’s containerized databases, proceed normally, and promptly read polluted state. We now emphasize isolation pretty heavily with all of our Docker usage to prevent this sort of failure mode from reoccurring. Use the --project-name flag with docker-compose (to prefix all generated containers) with something uniquely trustworthy like BUILDKITE_JOB_ID, or a PRNG -derived number. Explicitly run docker-compose down or docker stop after your code finishes. If you have to manually orchestrate docker containers, name volumes and networks uniquely so that there is no cross-run pollution. Resource Utilization As we ramped up our Docker usage, we got unexpected “No space left on device” errors on long-running CI hosts. We discovered that Docker images, volumes, and networks are automatically persisted to disk, grow quickly, and are not automatically garbage-collected. We also found that one live container on a host could easily take up too many resources and crowd out other containers if unchecked. A single docker container has no resource constraints and is only bottlenecked by what the kernel / host OS will give it. docker and docker-compose support special flags to restrict a container's memory, cpu, swap consumption. Run docker system prune --volumes periodically to clear out images, volumes, networks, etc that are unassociated with currently-running containers. To avoid race conditions with live containers you can use a date filter as so: docker container prune --filter \"until=24h” to only prune containers that are a day or older. Deploying We initially used Dockerhub to store and retrieve all of our images. Unfortunately, Dockerhub writes failed regularly and were a significant cause of CI flakiness. It was also easy to accidentally overwrite images, with no audit trail present. There are SaaS alternatives like Docker Trusted Registry, Artifactory, and others, but they are extremely expensive and we don’t yet need all their features. We currently self-host an instance of the open-source Docker Registry to serve as a write-around cache and eliminate Dockerhub’s reliability concerns on reads. We believe that relying solely on image_name:latest to serve the latest instance of image is an anti-pattern, since the old image will be overwritten. Instead, make sure there is always a copy of your image in Dockerhub. For example, if you push up an image named, say, gusto/zenpayroll:latest , make sure the image with a tag of its raw MD5/SHA also exists in Dockerhub (gusto/zenpayroll:abc123). Docker images & layers are just stored as files on the filesystem, and are straightforward to bake into bootstrapping scripts. This speeds up startup time (eliminates network reads with docker pull ) and can reduce requests to your artifact stores. Dramatic recreation: When we fix a bug Volumes When we started, we naively assumed that Docker volumes were a simple solution to a broad class of problems we had around speed, development, and secrets. Unfortunately, we found they turn out to have meaningful challenges around permissioning, speed, and troubleshooting. If you’re developing in Docker, be sure to volume in code from your host filesystem, instead of ADD’ing or COPY'ing, to drastically decrease iteration cycles. Never bake secrets and private cryptographic materials into an image. If you can't retrieve secrets from a service, volume them in from the host filesystem. Prefer named volumes to anonymous/unnamed volumes. It's easy for undeletable root-owned files to be created on the host filesystem. When possible, execute container operations as a user that does not have root permissions on-disk. If you volume in files from the host container, they’ll import the exact same bits for their UIDs/GIDs—even if there are no equivalent UIDs / GIDs in the container’s execution context, or they map elsewhere. Tread carefully with your ADD directives! Miscellaneous Oddities There are a lot of corner cases with Docker. Stay vigilant and keep a lookout for weirdness. Docker will nondeterministically hang—forever—if you try and print out too much output in a single call to STDOUT. This is a long-standing Docker bug, is astonishingly easy to replicate, and has been the source of a several weird flakey tests for us. To get around this, make sure you split up extremely long lines in subsequent STDOUT calls. We don't know what the exact threshold looks like yet (and may be system-dependent), but a few thousand characters on a single line has been a good ballpark for us to stay below. Volumes on OS X are incredibly slow. If you’re running a Rails server from a Docker container, and trying to volume in hundreds of assets, it will be extremely slow due to how serialization of open() calls work in Docker for Mac. This limitation does not exist when running Docker in Linux since Docker actually uses the host OS’s kernel, while Docker for Mac runs an Alpine Linux VM behind the scenes . Debugging Tips It’s pretty common to need to debug running docker containers. Since we’re running Rails tests pretty commonly, we might need to, for example, modify RSpec files to insert pry statements, or rebuild images on a CI host with different package versions installed. We discovered a few tips to make debugging easier. Ensure a bare-bones editing environment (vim/nano, curl, etc) exists as part of your Dockerfile, even for images derived from lightweight base images like Alpine. If you need to debug/modify/reboot a live container, it'll be significantly easier. Exploit layers and the FROM directive heavily. One of the most expensive parts of a Dockerfile will usually be the apt-get install MyCustomPackages && MoreCustomPackages . If you place these into their own layers, you can modify the rest of the file without needing to rebuild everything. Sometimes, if you're unsure why something is installed, the fastest debugging tip will be just to cut out all the layers except the one you think is introducing the bug, and dropping in a shell in a running container to observe the environment. While docker run spins up a new docker container, docker exec -it allows you to attach to an existing container. Use docker exec -it bash liberally for debugging broken containers. docker ps shows you all containers that are currently running, along with their container ids. Use in conjunction with docker exec -it containerid123 bash to debug. Get comfortable with the --verbose option. Although it can feel overwhelming, it can help debug many gnarly orchestration issues, with docker-compose and raw Docker. Get good at reading long log lines. Most of bugs I’ve found were discovered by reading endless amounts of logs and seeing what was missing, or shouldn’t have been there in the first place. Conclusion Docker has been a great tool to support our self-hosted CI/CD pipeline. It's unlocked significant cost savings (30%+) over cloud solutions, saved engineering time, and helped scale our release engineering workflows. It does have its quirks, but if you manage them effectively, it’s entirely possible to keep your infrastructure stable—and still get to bed on time.", "date": "2018-11-27"},
{"website": "Gusto", "title": "How I fell out of love with HOCs", "author": ["Alicia Tran"], "link": "https://engineering.gusto.com/how-i-fell-out-of-love-with-hocs/", "abstract": "Recently, there’s been a lot of buzz about render props over HOCs, but not a lot of real-life examples to showcase this. My team at Gusto recently launched Time Tracking , a feature that lets employers track their team’s hours and sync it directly to payroll, which has been a great use case for using render props. At a high level, both render props and HOCs (Higher Order Components) are a useful way to abstract out reusable code in React and avoid the pains of mixins. For those who aren’t familiar, here’s a simple example of each: HOC (Higher Order Component) An HOC is a function that takes in a component and returns a component. Typically the component returned is wrapped with some additional behavior. const hocWithUser = WrappedComponent =>  \n  class HocWithUser extends React.Component {\n    render() {\n      const user = { user: 'Alicia' };\n      return <WrappedComponent {...this.props} {user} />\n    } \n  } hocWithUser is the function which takes as its parameter a component called the WrappedComponent . hocWithUser ’s render function returns WrappedComponent , but with an additional prop called user, in this case specified to be ‘Alicia’. The way this would be used is: const Component = props => <div>{props.user}</div>\nconst ComponentWithUser = hocWithUser(Component);\n\n// This is what will get rendered for ComponentWithUser:\n// <div>Alicia</div> Here, ComponentWithUser is the wrapped result where hocWithUser will supply the “user” prop into Component to be rendered. Render Prop A render prop refers to a component that has a “render” function as a prop and calls this instead of implementing its own render. class User extends React.Component {\n  render() {\n    return this.props.render({ user: 'Alicia' });\n  }\n} Here, User basically defers implementing its own render function, and instead calls its render prop while passing in the user data. Here’s how this render prop component could be used: const ShareUser = () =>\n  <User render={ ({ user }) => <div>{user}</div> } />\n);\n\n// This is what will get rendered for ShareUser:\n// <div>Alicia</div> In ShareUser , we specify what the render function to be passed into User would be. In this case, it’s a function that expects a user object, and then it renders this user and surrounds it with divs. Render Props vs HOCs Both render props and HOCs function similarly in that they’re useful in constructing reusable components and sharing that functionality. If we look at how each of these components is used, however, you’ll see that render props are much more explicit about what’s happening. It’s clear that ShareUser will render the user and wrap that in a div. In the case of the HOC, it’s not obvious what’s happening until we look at the internals of hocWithUser . Great—I already know what render props are, but when would I actually use them? In my case, I was working on adding a dropdown of pay periods, basically a range of dates, to be added to multiple views. There were several other constraints to consider: The pay period data needed to be loaded before our other data was fetched so that we knew which dates to fetch data for in the children containers. The dates are also part of the url so that we have a way of knowing which pay period is selected on page load. Naturally, the dropdown should derive what the selected pay period is, based on the url. If the dates were invalid or there isn’t one selected, it should just default to the currently active pay period. This logic is need for multiple views. Because of the last point, I initially ended up with a lot of code duplication across the multiple views. To consolidate all of that logic, I ended up using both HOCs and render props for a PayPeriodPicker component. The HOC would be in charge of fetching the pay period data since at Gusto we follow a pattern of using HOCs for container components. The render prop would manage the internal state that determines which pay period is selected or defaults to the current one. This is what the render prop component ended up looking like: class PayPeriodPicker extends React.Component {\n  static propTypes = {\n    payPeriods: ImmutablePropTypes.iterable.isRequired,\n    render: PropTypes.func.isRequired,\n  };\n\n  constructor(props) {\n    // logic to determine default/selected pay period\n    this.state = { selectedPayPeriod };\n  }\n\n  onSelectPayPeriod = payPeriod => {\n    // update the url based on selected pay period\n  };\n \n  render() {\n    // logic to map this.props.payPeriods to options \n   const { selectedPayPeriod } = this.state; \n    return this.props.render({\n      payPeriodPicker: (\n        <Select\n          options={options}\n          value={selectedPayPeriod}\n          requestChange={this.onSelectPayPeriod}\n        />\n      ),\n      selectedPayPeriod,\n    })\n  }\n} This looks like a typical component up until the render function where it’s not actually rendering an element, but instead calling the render function that is passed into it as props. That render prop function is called with a single parameter that contains the payPeriodPicker select dropdown as well as the dates that should be selected. Let’s take a look at how this component is used to get a better idea of how it works: const PayPeriodOverviewPage = props => (\n  <PayPeriodPickerContainer\n    payPeriodStartDate={payPeriodStartDate}\n    payPeriodEndDate={payPeriodEndDate}\n    render={({\n      payPeriodPicker,\n      selectedPayPeriod,\n    }) => (\n      <PayPeriodOverviewContainer\n        payPeriodStartDate={selectedPayPeriod.get('start_date')}\n        payPeriodEndDate={selectedPayPeriod.get('end_date')}\n        payPeriodPicker={payPeriodPicker}\n      />\n    )}\n  />\n); I didn’t include the implementation of PayPeriodPickerContainer but it’s essentially the container component that fetches the pay periods and maps them to be the props for the PayPeriodPicker component. Here we pass in payPeriodStartDate and payPeriodEndDate , which are taken from the url and passed into PayPeriodPickerContainer so that it can determine what the selected/default dates are. Then we pass in a render function which takes the payPeriodPicker and selectedPayPeriod object and returns the PayPeriodOverviewContainer , one of the views that requires the dropdown component and the selected pay periods before it can fetch its own data. We just pass in the selectedPayPeriod start and end dates, as well as the payPeriodPicker component. The render function is a way to have a more explicit API for what data is being passed between the PayPeriodPicker component and the PayPeriodOverviewContainer . This could’ve also been implemented as an HOC, but the details would’ve been abstracted away and you’d end up with something closer to: const PayPeriodOverviewWithPicker = withPayPeriodPicker(PayPeriodOverviewContainer); While this is cleaner, it’s harder for someone else to come in and understand what withPayPeriodPicker is actually doing until they look at the implementation of the HOC. With render props, it’s a lot more declarative in what’s being added. How do we test this? This assumes setup with enzyme and mocha-chai and sinon. We can test the render prop like any other function prop: stub it and make sure it’s called with the correct parameters. const render = () =>\n  mount(<PayPeriodPicker {...props} render={renderStub} />);\n\n\nbeforeEach(() => {\n  renderStub = sinon.stub();\n  render();\n});\n\nit('calls render with the correct pay period', () => {\n  expect(renderStub).to.have.been.calledWithMatch({  selectedPayPeriod  });\n}); Since we’re returning a component to be rendered, let’s specify the render prop function to just render the component and make sure it shows up. beforeEach(() => {\n  renderStub = ({ payPeriodPicker }) => payPeriodPicker;\n});\n\nit('renders a Select field', () => {\n  expect(render().find(Select)).to.be.present();\n}); To test the components that use the render prop, you can stub out what the render function returns and make sure they are used accordingly. const stubbedPayPeriodPicker = <div>pay period picker stub</div>;\nconst render = () => \n  mount(<PayPeriodOverviewPage {...props} />);\n\nbeforeEach(() => {\n  const overviewPage = render();\n  payPeriodPickerWrapper = mount(\n    overview.find(PayPeriodPickerContainer).prop('render')({\n      payPeriodPicker: stubbedPayPeriodPicker,\n      selectedPayPeriod,\n    });\n  );\n});\n\nit('renders the overview container with the correct props', () => {\n  const overviewContainer = payPeriodPickerWrapper\n    .find(PayPeriodOverviewContainer);\n  expect(overviewContainer).to.have.props({\n    payPeriodPicker: stubbedPayPeriodPicker,\n    payPeriodStartDate: selectedPayPeriod.get(‘start_date’),\n    payPeriodEndDate: selectedPayPeriod.get(‘end_date’),\n  });\n}); When should I use render props over HOCs? Render props were useful for my specific problem due to the hierarchical aspect where pay periods needed to be loaded and configured before we could render the other components, and because this logic was used in multiple places. While I did still use HOCs for data fetching, it could just as easily be rewritten to use render props instead. The declarative nature of render props makes it way more readable and understandable for other engineers. The other benefit of render props is testing. It’s a lot easier to test each individual component in isolation for both the render prop and the component that encapsulates it. For HOCs, testing the component that encapsulates everything requires knowing the details of how each HOC is implemented and doesn’t lend itself well to stubbing out that functionality. About the Author Alicia is a software engineer from Los Angeles who loves coffee too much. She is trying and failing to cut back.", "date": "2018-12-09"},
{"website": "Gusto", "title": "I Didn't Want to Be a Manager Anymore—and the World Didn't End", "author": ["Noa Elad"], "link": "https://engineering.gusto.com/i-didnt-want-to-be-a-manager-anymore-and-the-world-didnt-end/", "abstract": "In 2017, I became an engineering manager at Gusto. One year and many learnings later, I left the role. To many, career growth in software engineering = becoming a manager. This idea is in part what drove me to take on a management role. I wanted to share my story, in case it’s helpful to others. This post is about: My journey into engineering management (and back). Lessons I learned along the way. Takeaways for the reader contemplating their own career path. Illustration by Camellia Neri I grew up in Israel and, after completing my Master’s in Computer Science, moved to San Francisco to work for Gusto . I joined the Benefits Engineering team to build software that helps small businesses offer health benefits to their employees. The team was great, and I was constantly learning new things. I was engaged and growing in my role. But then, something happened that I hadn’t anticipated. Lesson #1: If you’re interested in a career transition, tell people about it! About a year into my role, my manager transitioned to another engineering team. This was a side-effect of a fast growing company, with engineers moving around to fill necessary roles. His transition caught me off guard, but then I realized that I could be the one to replace him. I was familiar with the team dynamics, the benefits domain, and our product and design partners, and felt that I could guide them toward our goals. However, I was not even considered for the role. I felt pretty frustrated and annoyed. But after talking to my team, I realized why it happened. No one at Gusto (including myself!), knew that I was interested in leading a team. So naturally, when that role opened and needed to be filled quickly, my name didn’t come up. If you’re interested in a career transition, tell people about it! Who should you tell? Well—first and foremost your manager, but also their manager, your peers, and other leaders at the company. You can start the conversation by saying, “I'm interested in a transition to role X, because of Y. Do you have any advice for me?” I’m not saying you should email the whole company about it, but spreading the word has two big upsides: Your manager and peers start evaluating you for that role and can give you concrete feedback that will help you reach your goal. Transitioning takes time, and when those opportunities come around, decisions need to be made quickly. The names considered are usually those who spoke up and expressed interest in the past. Make sure the right people know that you’re interested. Coming out of this experience, I started telling people that I was interested in leading a team at Gusto. I didn’t express this as an immediate hard requirement (“I want to manage a team ASAP or I’m leaving”), but instead I brought it up in 1:1 conversations, and asked for advice about the skills I should develop to help me get ready for that kind of role. This made it easier to talk about what I wanted without feeling like I was demanding something I couldn’t achieve right away. Soon after, another opportunity came along. A few existing leaders of another team were transitioning, and we needed to bring in other folks to take their place. Coincidentally, I had connected with the head of the team some weeks before this. We were both visiting the Denver office at the same time and had an informal 1:1. I brought up my goal (remember lesson #1?), so he knew that I was interested in leading a team. When the shuffle happened, he reached out to me and asked if I wanted the opportunity. I said yes. Remember when I said management decisions need to be made quickly? This was one of those moments. When a manager leaves a team, people can feel jarred and uncertain. Leadership then has to move forward quickly to mitigate these effects. Opportunities like this come up often, especially in fast-growing companies. Lesson #2: Get to know yourself Soon I had jumped into a new team, a new domain, and a new set of challenges. I had a lot to learn, so I looked for inspiration from role models I had admired in the past, like my former manager. I wanted to be just like them. To me this meant: Being able to unblock my team on any project, which also meant knowing the code better than anyone else. Taking on the grunt work so my team could do the interesting things. Helping the team figure out how to become better engineers and better teammates. Being the external communicator on behalf of my team: coordinating with product, design, compliance, support, sales, technical writers, and other engineering teams. To make things more complicated, I had to switch domains—I’d moved from Benefits Engineering to Payroll Engineering—which meant I was starting from scratch. I didn’t know the code and the domain, I didn’t know the stakeholders, and I didn’t know the team, my team. My first few months felt daunting. I tried to learn as much as I could while “faking it till I make it.” I was extremely fortunate to have a manager that coached and encouraged me when I was suffering from imposter syndrome. Being a manager gave me a glimpse into “the other side.” I discovered that it involves a lot of behind-the-scenes work: preparing for one-on-ones, being involved day-to-day in what your team members are working on, finding opportunities to give them feedback, and being very thoughtful about how you deliver that feedback. Phew. How on earth did my role models manage to do it all?! A colleague of mine, Kelly Sutton , analyzed data from ~20 engineering managers at Gusto. He found that while managers with four or less direct reports were still writing code pretty frequently, managers with five or more reports were writing zero lines of code. This result astounded me but also put into perspective how much I struggled to write code when I had exactly four direct reports. Number of direct reports Average PRs/week 4 or less reports 1-3 PRs 5 or more reports 0 PRs About a year into management, I felt burnt out. I was wearing three different hats, and each deserved more attention than they were getting. The Individual Contributor Hat It was very important for me to continue coding. In my mind, an ideal manager could unblock team members on any problem, apply a critical eye to every technical decision, and suggest a more effective way to implement any requirement. Stepping away from the code felt like it could block my personal technical development—an area which already felt stagnant because I was spending a lot of my time wearing the two other hats. The Tech Lead Hat This role, which at Gusto we call ‘Technical Anchor,’ revolves around setting the team’s technical direction, building up the team’s technical muscle, and contributing on project and capacity planning. This type of work was the most appealing aspect of leadership for me—the ability to have influence at a more strategic level and see the bigger picture of software development. The Manager Hat This role, which we call ‘People Empowerer,’ is the more classical manager role. It encompasses people leadership: 1:1s and coaching, providing feedback, performance evaluations, promotions, headcounts, hiring, and all that jazz 👐. In hindsight, it’s fair to say that this aspect of my role is the one I knew the least about going in, and it caused me to feel a lot of imposter syndrome. It took up way more of my time than I had anticipated. I really wanted to do a good job at this, and it wasn’t apparent from the outside what this would require. As a non-manager, most of your exposure to management work is through 1:1s and performance evaluations with your own manager. In that former role, these tasks took a much smaller part of my time. However now on the other side, even with a small team of four, they ended up taking quite a bit of time. The A-Ha moment Looking back, I remember one particular day. I had many manager-related tasks scheduled, and on the way to work I suddenly realized that these tasks did not scare me anymore. In fact, I felt confident that the day would go well. This should have been a turning point—managing was no longer a daunting role. I knew how to approach it and I wasn’t terrible at it. And yet, I dreaded the day that was ahead of me. Later on, I tried to figure out why I felt this way. I realized that although I had gained confidence in my ability to perform this type of work, I still didn’t look forward to it. It felt like work that needed doing, and I wanted it to be done well and by well-intentioned people, but I didn’t feel a particular desire to do it myself. This was my ah-ha moment. The management tasks were not bringing me any satisfaction. I realized that I didn't want to be a manager anymore. So now what? Lesson #3: Remember lesson #1?! If you’re interested in a career transition, tell people about it! Part of me felt like I was back at square one. I dreaded telling my manager about my revelation. He’d been the one who had given me a chance and so patiently coached me into this role. What would he say? Would he lose confidence in me? Would he try to negotiate with me to stay in this role? Remembering Lesson #1, I wasted no time, and brought it up at our next 1:1. It was difficult, but I just bit the bullet and told him why I was unhappy in this role. He heard me, and to my surprise, wanted to take immediate action to help me make the change. As a result of our conversation, we opened up a job posting for a new manager on my team, which would allow me to stay on as both a tech lead and an individual contributor. My replacement joined about a month later. This change was such a relief for me. It allowed me to focus and spend more time in roles that I really enjoyed. Additionally, this felt like career progress and not a demotion, because Gusto didn’t treat it as such. Our engineering organization doesn’t treat the different hats as promotions or demotions. If you’re considering different companies to join, think about whether you will be supported in a non-traditional career path, should you want one—and keep in mind that you may only discover you want it later. If you want a career change Here are some questions you can ask yourself: What do I like about my current role? How will those aspects change if I move to a different one? What are aspects of the new role that I'm looking forward to? Can I do more of that type of work without switching roles? And if so, how can I get recognition for it? What are aspects of my role that I don’t enjoy? Why not? What are aspects of the new role that I know little about? How can I get more exposure to them? Am I communicating about these topics clearly enough with my manager and other relevant people? Do they know what I want? Can they help me answer some of these questions more in depth? As you discover more about your needs and interests, talk to people around you about how you can make your current role work better for you. You’ll lay the groundwork for what you want next in your career.", "date": "2019-01-07"},
{"website": "Gusto", "title": "The Surprisingly Simple Way to Be the Best Mentor Ever", "author": ["Eric Schuchman"], "link": "https://engineering.gusto.com/the-surprisingly-simple-way-to-be-the-best-mentor-ever/", "abstract": "Over the course of my career, I’ve mentored quite a number of people beyond those that I personally managed. This includes formal mentorship programs that I’ve volunteered for, heads of product management, and of course friends and colleagues. Illustration by Camellia Neri When I first started mentoring others outside of my own team at Gusto, I found it challenging to figure out the right structure for the mentorship. How should I run the meetings? How often should we meet? How can I be helpful given that I don’t actually see the results of their work? There’s a lot of generic advice online about how to be a good mentor (“let them drive” is a common saying), but not many good answers for these questions at a tactical level. I’ve found that the best approach to being an effective mentor is surprisingly simple: ask your mentee to walk you through specific situations that they found difficult, and challenge them with thoughtful questions. A mentor’s toolbelt More often than not, a mentee will not be somebody that you work with directly. What this means is that you don't have full context around decisions being made in their company or by them, so you really can't critique them or give feedback as you would for somebody that you're directly managing. As a mentor, you have three basic tools in your belt to help your mentee: Teach them frameworks / best practices , based on your knowledge and your own experience, that they are looking to learn. Lend a sympathetic ear , listening to what they have to say and making them feel heard by somebody who truly understands their situation. Have them walk you through challenges they’re having so you can ask more questions and make suggestions, often incorporating #1 and #2 above in your recommendations. I've found the last tactic to be the most effective. For example, if your mentee is having trouble with stakeholder management, have them talk about a specific recent example. If they’re having trouble with roadmap management, have them show you how they manage the roadmap today and explain the challenges they’re having. You can then ask lots of questions to gain enough context to make suggestions. Ideally, your suggestions are in the form of both a framework (\"here's a generic way to think about problems like this\") and a solution (\"one potential way to solve this is __________\"). After your meetings, your mentee should take a homework assignment to try to incorporate what they’ve learned into their actual job so that at your next meeting they can share what did/didn't work. You two can then iterate together. Kicking off the mentorship At the beginning of the mentorship, I've found it useful to be up front with my approach (described above) to set expectations. Let your mentee know that you want them to drive the partnership, and that what's most effective is for them to bring specific challenges and examples to the table for you to discuss together. Often people are a little nervous about sharing their struggles, but typically open up once you build rapport and explain why it’s so important for them to bring up tactical examples and situations for discussion. I’ve also found that some mentees start off a little dubious of my own motivations. Why am I taking time out of my busy day to help? What am I expecting in return? So, I also typically share with my mentee why I’m excited about the partnership during our first meeting. Generally speaking, my motivation for being a mentor is that I want to practice my own coaching skills and get a lot of personal satisfaction out of helping others, and my only ask is that my mentee be forthcoming with feedback for me about how I can be more helpful, which suggestions worked, and which didn’t. Meeting cadence I suggest a meeting cadence that allows your mentee to actually work on things in between sessions. This can vary dramatically depending on both what type of help your mentee is looking for and the seniority level of your mentee. For example, I had a mentee who was in the first few years of her career and was looking for help with how to approach everyday communication with her manager and peers. She had many opportunities to practice new tactics every day, so we chose to meet every other week so that we could iterate faster. I also had mentee who had recently been promoted to director of product and was struggling with how to best communicate her strategy and vision to her company. Opportunities for this were largely leadership meetings, all-hands, etc. which do not happen frequently, so we chose to meet every six weeks. The one exception I’ll stipulate is that you find a mentee with whom you’d like to work through a number of challenges or skills in parallel. For example, one of my mentees wanted help on multiple fronts including long-term roadmap planning, short-term prioritization of his backlog, and many conflicting viewpoints from others in the company. In these situations, meeting more frequently may make sense. Want to Continue the Conversation? Being a mentor can be incredibly fulfilling and a great way to practice coaching skills—regardless of whether you’re an individual contributor or manager If you’d like to be mentored by some incredibly talented people at Gusto or be a mentor for some of our up-and-coming stars, you can check out our open roles here . I’d also love to hear from you. How have you structured mentorships in the past? What’s been useful to you as either a mentor or a mentee? Leave a message here or find me on LinkedIn .", "date": "2019-01-24"},
{"website": "Gusto", "title": "Debugging Sidekiq Poison Pills", "author": ["Brittney Johnson"], "link": "https://engineering.gusto.com/debugging-sidekiq-poison-pills/", "abstract": "That one time a memory leak almost took down one of our apps -- and how we fixed it Like many web applications out there, Gusto uses asynchronous jobs to process various sets of information. Sidekiq is one such background processing library for Ruby, which we use at Gusto. Other such libraries include Resque, Beanstalkd, and delayed_jobs. Recently, a series of design choices made over time led to the complete halt of all asynchronous jobs in one of our main applications. This was particularly disastrous because it happened overnight while running a lot of routine maintenence code. Our maintenance code handles some really important work: it ensures that people are paid accurately and receive the appropriate health insurance. When almost all of it was prevented from running, we had an incident on our hands. Here's how we mitigated the incident that night — and prevented it from ever happening again. Uh-oh, there’s a problem The next morning, I opened Slack to an innocuous message from a coworker: “Hey, do you know how to debug Sidekiq problems?” “Sure I do,” I thought to myself, as I navigated over to our Sidekiq. Then, “uh oh.” I saw that we had hundreds of thousands of jobs enqueued, well over our usual number. We usually only had a few dozen running at a time, or maybe a few thousand if maintenance was running. I checked out the currently running jobs: We had five workers running five processes/threads each, and all 25 processes were working on jobs of the same class. Let’s call that class DisasterJob. At this point, some other early-risers were starting to notice that our app was having problems. Our customer support teams were wondering why certain key functionality wasn’t happening. Other engineers started posting about what they were seeing. We first worried about our jobs backing up so far that they filled up Redis: Then we wondered if the app code was surfacing any errors: When the app code didn’t show errors, we checked our container logs: Whoa— what? That last message made us realize what was happening. Instances of DisasterJob were hitting the memory limit on their worker container and taking the whole worker down with them, including the other four threads on the worker. And this was happening over and over again. In software, we call this situation a “poison pill.” A poison pill usually means a job that you purposefully enqueue to stop a job runner that you don’t otherwise have the ability to stop. Unfortunately, this job class was repeatedly and un-purposefully doing that. How we stopped the problem Our first thought was to fix the out-of-memory problem was to increase memory allocation, but it takes a while to scale up new workers and we weren’t even sure how much more memory the jobs would need to run to completion. Deleting these bad jobs seemed like a faster option. DisasterJob is a recurring job, so it was okay to delete enqueued jobs of that class since we could re-enqueue them later. So I tried deleting all of the enqueued DisasterJobs: queue = Sidekiq::Queue.new(\"maintenance\")\nqueue.each do |job|\n  job.delete if job.klass == 'Recurring::DisasterJob'\nend That made a big dent in our number of enqueued jobs overall, but it didn’t clear out the twenty-five jobs that were currently running, crashing, re-enqueuing, rerunning, re-crashing, etc. Jobs that are already running can’t be killed, so we needed to think of a different strategy. “Why does this keep happening? Shouldn’t the jobs just die, take a worker with them, and then go be dead?” I asked. It turns out that we were running an older version of a reliability strategy for Sidekiq, called Reliable Fetch. With Reliable Fetch, jobs on processes that die get sent to the “reliable queue.” This diagram glosses over the storage details of reliable queue, but is a good comparison of the enqueuing and work strategies When a new worker starts up, it pulls jobs from the reliable queue before pulling from the normal queues. So jobs that took down their worker were the first to get enqueued to a new worker! And subsequently take that one down as well. These queues have the same name as their original queue, but with “_#{number}” appended. So, we tried clearing out those queues that we knew about, too: # we ran this five times: maintenance_0 through 4\nqueue = Sidekiq::Queue.new(\"maintenance_4\") \nqueue.each do |job|\n  job.delete if job.klass == 'Recurring::DisasterJob'\nend Still there were jobs being re-enqueued! This code snippet got us a list of all the queues, and we cleared out every single queue of that job. all_keys = []\n$redis.scan_each(match: '*') do |key|\n    all_keys << key\nend\nall_keys.select{|k| k.include?('queue')}.map{|k| [k, ($redis.llen(k) rescue \"another data type for key\")]}.sort_by{|arr| arr[1].to_i} Phew! We finally had no more DisasterJobs running, and the rest of our enqueued jobs were starting to drain through. Should we prevent this from happening again? Before diving into prevention, first we need to ask: Should this be prevented? I’m a fan of Cory Chamblin’s RubyConf talk , which suggests that we should only fix problems that cost more considering the probability of them occurring than the cost of their fix. Given that, this was a pretty catastrophic failure, and the cost to the business and the people we serve if maintenance is not run is high. We risk messing up someone's payroll or health insurance, and there’s no room for error when it comes to that type of work. Although DisasterJob’s memory usage was the root cause of our problem, the real issue was that bad jobs would be immediately re-enqueued in a new worker after taking one down. This was due to the Reliable Queue reliability strategy. After a bit of research, we found that the creator and maintainer of Sidekiq recommends that users switch to a newer reliability strategy: Super Fetch . Super Fetch cleans up jobs on processes that die into a private queue, and then pushes that private queue back into the regular queue. So instead of being immediately re-executed, bad jobs have to wait in line and let other jobs be executed first. As an experiment, after updating to Super Fetch, we tried running all of the DisasterJobs again. They continued to take down their workers, but this time jobs of other classes were able to run too! Of course, the enqueued jobs drained a little more slowly because the workers they were on occasionally fell victim to DisasterJob, but “things are running a little slowly right now” is much better than “nothing is running at all right now.” Finally, we wanted to fix the root cause: DisasterJob. It was actually an old class, written three years ago and faithfully running successfully each night ever since. Why it suddenly became so problematic for us is also an interesting story, but we’ll save it for another blog post! If you’re currently using Reliable Queues with your sidekiq, switch to Super Fetch. And consider playing around with the Sidekiq API—it’s very powerful!", "date": "2019-02-21"},
{"website": "Gusto", "title": "Layering authorization into a web application", "author": ["Flora Jin"], "link": "https://engineering.gusto.com/layering-authorization-into-a-web-application/", "abstract": "How we introduced granular authorization into our application and API. Illustration by Camellia Neri Last year, my team extended Gusto’s authorization system to give admins granular access to their companies’ accounts. In software security terms, authorization is the concept of what a user can do in a system, while authentication refers to who a user is. For context, Gusto is an application that serves small and medium businesses to meet their HR, payroll and benefits needs. An admin is a typical user in our system, who performs a lot of actions in their account, such as running payroll for employees. As Gusto has scaled as a company, so have our customers, and our one-size-fits-all approach to authorization no longer supported our customers’ needs. This is a journey that I expect many software engineering teams embark on. My team worked on this project for about ten months, and emerged with ideas and learnings that we thought were worth sharing. The learnings I’ll share in this post are: take time to clean up the codebase, design for flexibility, choose the right rollout plan, make it harder to introduce regressions. Take time to clean up The project spanned several quarters because we were tasked with layering in granular authorization to every affected part of our existing codebase. Sometimes this was straightforward, and several endpoints could be granted as all-or-nothing. However, as with many legacy monoliths, over time we accumulated some “god” models - models that know too much or do too much. This often required picking apart each field in our API response, and determining whether or not we should expose it for a given permission. A lot of our work was actually auditing how these fields were used. An example is the “gender” field on an Employee. We originally had little clue how this was used. By auditing the codebase, we found that it is used for several benefits enrollment flows, and therefore admins with the Manage Benefits permission would need access to read and modify it. We were essentially going over our API with a fine-tooth comb. In our scrutiny, we found parts of our API that were no longer in use, and instances where the API could be simplified. Veteran Gusto engineers on the team helped identify controllers that were written long ago so we could update them to adhere to our current conventions. It is rare that an engineering team can dedicate the resources to do this sort of tech debt work, so we took the opportunity to make the codebase more consistent and delete a large amount of code as a result. Design for flexibility To decide what types of permissions to provide in our app, the product and design teams conducted interviews with customers to learn more about their needs. We honed in on eight permissions that corresponded to core job functionalities, such as Manage Benefits and Pay People . We originally had more, but we didn’t want to prematurely slice our app in a way that wouldn’t map well to our customers’ real world use cases. Since we were in a learning phase for the overall product, we expected the set of permissions to change. Along with the natural uncertainty that comes along with launching a new feature, we expected that the following types of changes to permissions would be quite common: A new permission is added or an existing permission expands. For example, we were building a new Time Tracking feature, so we already knew this type of change was on the horizon. An existing permission is split into two. We had decided to err on the side of simplicity for our first set of permissions, so there was a large risk that some of the permissions would need to become more granular. A permission is deleted, or part of its functionality goes away, since features come and go. In addition, the eight permissions we envisioned overlapped. Two of them in particular, Pay People and View Financial Reports , both needed read access to payroll data, bank reports, and more. Thinking about how this would look in our codebase, a simple approach for fetching payroll data could look like the following: if user.is_admin? && (user.has_permission?(PAY_PEOPLE) || user.has_permission?(VIEW_FINANCIAL_REPORTS))\n  # respond with payroll data\nelse\n  # raise 401 Unauthorized\nend However, evaluating the simple approach against the types of changes that we knew would be made to permissions, we realized that it would be quite painful to make those changes. When a new feature is added, we just need to permission the new code. This one is not too painful since the scope of the new code being added is contained. When an existing permission is split in two, we would need to update each place that the existing permission is referenced, and decide whether it is needed by both the new permissions, or just one of them. This work is not well contained, and each of these decisions being made may require re-auditing the code to remember how it’s used. We have a similar issue when we delete a permission, or when part of its functionality is removed. For each place we reference it, we need to update the application logic, although this is easier than #2 where we would need to re-audit the code. Not to mention updating the tests… With these considerations, we decided to introduce an in-between layer of granular permissions that are mapped from our user-facing permissions to something consumable in our codebase. For flexibility, we used an in-memory map that looks something like this: PERMISSION_MAP = {\n  ToggleablePermission.PAY_PEOPLE => [READ_PAYROLLS, MANAGE_PAYROLLS, READ_TIME_OFF],\n  ToggleablePermission.VIEW_FINANCIAL_REPORTS => [READ_PAYROLLS, READ_ACCOUNTING_INTEGRATIONS],\n} For each admin, we can use the map to look up which granular permissions they have, and then the API code can look more like this: if user.is_admin? && user.has_permission?(READ_PAYROLLS)\n  # respond with data\nelse\n  # raise 401 Unauthorized\nend To separate permissions from the API controllers altogether, we used the authorization gem CanCanCan . The library enables you to specify which resources are accessible within a particular context using their rule-based authorization definitions. With our own lightweight DSL wrapper, our authorization specification looks like this (in a separate file where admins’ abilities are defined): subject(Payroll, company_id: params(:company_id)) do\n  can [:read], with: Permissions::READ_PAYROLLS\n  can [:create, :update, :destroy], with: Permissions::MANAGE_PAYROLLS\nend With this, our API code doesn’t need to know about the new permissioning system: authorize :read, resource # raises 401 if you don’t have access to the resource These simple abstractions helped make the code more manageable and separates the business logic of the toggleable permissions from individual resource endpoints. In the example above, reading payroll data is needed across permissions. For other resources, we expect them to always be bundled under one permission. For instance, we expect the data that enables our employee time off features to always be managed under a single permission. With this abstraction, we have the flexibility to split permissions apart into as big or small of chunks as needed. If we evaluate this solution against our original framework, the expected changes outlined earlier have become easier to make. In fact, since the project went live, we’ve already seen changing product requirements (splitting a permission in two), and the team addressed this easily because of this design. Choose the right rollout plan Working on a long-running project can not only be challenging from a project planning perspective, but also for thinking about how to release it. At Gusto, it is very common to put our in-progress features behind a feature flag . In our case, since the new permission system would result in an extensive number of changes, it would be a disaster to put all of our changes behind a feature flag, as inactive code paths are in danger of becoming stale and broken. Our existing admins had full access to the app, and we wanted to migrate them to have the Full Access permission after our rollout. Having the Full Access permission would be implemented by granting all of the granular permissions mentioned in the above section, and would functionally be the same as being an admin in the existing app. Given this requirement, we had the idea to migrate all of the existing admins to the new permissioning framework immediately, and give them the Full Access permission. For all unmigrated endpoints, admins would still have access to them just as before. For all newly permissioned endpoints, existing admins would have access by having the Full Access permission. This meant the changes to authorization logic would go live as soon as we deployed them, thus curtailing the amount of stale code added. Even if you don’t have a Full Access permission as we did, you can still create an ephemeral Full Access permission that exists while your permissioning project is  in-flight. With this implementation, the only part we feature flagged was the ability to edit an admin’s permissions. Make it hard to do the wrong thing This may be the most important learning. Any code that deals with authorization is a matter of security, and our customers trust us to secure their data appropriately. When you transition from a world where admins have access to everything, to a system where you have field-level authorization, you want to make sure that even if an engineer has missed all of your PSA’s about your upcoming changes, they aren’t putting your customers at risk. Who would ever possibly ignore a warning? The plan was for our team to build the initial version of the project, but all new features would need to be permissioned by their respective teams. One question we asked when we tackled each part of the architecture was what could happen if someone forgot about permissions. If your engineering organization is as large as ours, this isn’t just likely to happen, it’s an inevitability. We decided to be cautious around default behaviors, log warnings as a safety guard, and write convenient test helpers to help surface when something goes wrong. Be cautious around default behaviors When logging in to Gusto, admins land on their dashboard, which shows a list of to-do items that the admin should address. Since these would only be actionable to admins that have the appropriate permission, we needed to figure out how to hide these to-do items conditionally. We decided to make each to-do item hidden by default, meaning if it’s not explicitly mapped to a required permission, it will be hidden from the user. Because of this, as a feature developer, it should be pretty obvious that you’re doing something wrong if you forget to consider permissions, as any level of manual or automated testing should fail. By hiding the notification by default, we mitigate the risk of exposing sensitive information to the wrong users in case of engineering error. Logging of warnings Despite the warning flags mentioned above, if a to-do item gets added without a required permission, as a safety guard, we log a warning that gets surfaced in our monitored errors. This way, we have an extra layer of protection against a developer having done the wrong thing. Convenient test helpers We created test helpers that could be dropped into API tests to validate access to the endpoint based on whether or not you have the required granular permission. In creating them, we wanted to make them as easy to use as possible. One of the controller helpers looked something like this: it_behaves_like ‘a read action that requires permission’, Permissions::MANAGE_BENEFITS To invoke this test helper, you specify the required permission for the endpoint. Under the hood, the test verifies that the endpoint is accessible with the permission, and is not accessible when the user has all other permissions except the one that’s required. The easier it is to test, the more likely developers will write tests. Conclusion Living in the intersection of software framework design and security, this project presented interesting and unique challenges. While exciting, we had many lessons along the way. This was a large project both in terms of time and resources, and the area of code that we touched. However, all of these learnings fell in line with the overarching principle, and one of Gusto’s company values, “Don’t optimize for the short term”. My team and I would love to hear about your experiences of executing on similar types of projects that live in this intersection of software and security. Feel free to reach out to me on twitter . Special thanks to Upeka Bee and Noa Elad for their feedback on earlier drafts of this post.", "date": "2019-04-09"},
{"website": "Gusto", "title": "Building Toward a Modular Monolith", "author": ["Alicia Tran"], "link": "https://engineering.gusto.com/building-toward-a-modular-monolith/", "abstract": "Imagine you have a Rails monolith and want to add new functionality. Your options are to 1) continue adding to the monolith, or 2) create a new service. Which do you choose? What if there’s a third option? Background Adding new functionality to a monolith is a lot like trying to add a new plant to an unruly garden. Do you put it in the same planter with all the other plants? Do you splurge on a new planter? How do you make sure it has enough room to grow? In Gusto’s case, that new plant was Time Tracking . Time Tracking was a new feature we wanted to build and it seemed like a separate enough domain that it could be its own service. Before deciding, here are some guiding questions that we thought about: How tightly is the new functionality coupled to the existing code? What would be the touchpoints with the current monolith? Are the interactions real-time or asynchronous? Can we make use of callbacks or do we need to fetch data at runtime? What are the different parts of the monolith that interact with the new functionality? What would it look like if we were to separate those from each other? The basic functionality of Time Tracking includes employees clocking in and out and editing their hours, managers reviewing these hours, and admins syncing these hours directly to payroll. At first glance, none of that seemed too coupled to the monolith; we counted just a handful of potential touchpoints with monolith resources. The interactions would be primarily real-time so data would need to be fetched synchronously. Given those factors, these were the options we considered: Building inside the monolith Pros There would be no setup costs and we could get started right away We wouldn’t need an API to access the models in the monolith from Time Tracking since everything is globally accessible We would have transactional guarantees Cons The monolith would continue to get even more bloated If scaling became a concern, it’d be painful to try and ad hoc pull out Time Tracking into its own service Creating a new service Pros We would be starting fresh without all the baggage from our monolith--it’s a greenfield opportunity It would be easy to scale independently We’d be stopping the bleeding by not adding to the bloat of the monolith Cons There would be high setup costs for something with an aggressive launch timeline Ongoing cost of maintaining a separate service with testing and deploys Getting the service boundary wrong would be very costly I’ll admit that our first instinct was to go for the new service approach--microservices are popular and prescribed as the silver bullet to monoliths. However, microservices have their own set of challenges in testing and deploying and if we got the service boundary wrong, it’d be much more expensive to fix. Ultimately we went with a third option... Building a rails engine Pros It’s easier to get it up and running as it would live in the same repo but still provides some separation. The cost of getting the boundary wrong wouldn’t be as high as a separate service. It provides more flexibility; we could decide to split the engine out to be a separate service later, or not split it out, depending on how coupled the implementation is and future scalability needs. Cons Data from the monolith could still be accessed anywhere from engine, and vice versa. Building a rails engine seemed like a middle ground solution that gave us the flexibility to get the domain boundary wrong on the first iteration. Rather than worry about scaling issues from the start, we could keep the data in the same database until it needed to scale independently. However, having a rails engine doesn’t guarantee that the code would be decoupled since everything is still globally accessible. We needed to be mindful about drawing a clear boundary between Time Tracking and the rest of our codebase. How we built it The main things we needed to figure out was how the frontend client would communicate with the engine, and how the engine would communicate back with the monolith. As we solved these, we were optimizing primarily for keeping Time Tracking as separate as possible from the monolith. Service-client communication Authentication still lives in the monolith, but we wanted authorization and all other business logic to live in the engine. What we decided was to use a reverse proxy where the monolith authenticates the user, and then forwards the request to the engine. This would mean that we have two controllers for what would normally be just one controller: one that lives in the monolith and authenticates, and one that lives in the engine to process the actual request and authorize. Since the monolith and the engine both run in the same process, the request is not forwarded across a network boundary, but just calling the corresponding controller action. Let’s see what this looks like in code. In the monolith, we have a proxy controller, which we’ve namespaced under TimeTrackingProxy: app/controllers/time_tracking_proxy/trackers_controller.rb module TimeTrackingProxy\n  class TrackersController < ApplicationController\n    before_action :authenticate_user!\n\n    def index\n      response = invoke_controller\n      render json: response\n    end\n\n    def controller_klass\n      TimeTracking::TrackersController\n    end\n\n    def invoke_controller\n      controller = controller_klass.new\n      controller.request = request\n      controller.response = response\n      controller_response = controller.process(params[:action].to_sym)\n      # Rails wraps the response in an array if there is content\n      controller_response.is_a?(Array) ? controller_response.first : controller_response\n  end\nend First the proxy authenticates the user, then we call invoke_controller which instantiates the Time Tracking controller from the engine, forwards the request, and processes the response back. If we take a look at the controller defined in the Time Tracking engine: time_tracking/app/controllers/time_tracking/trackers_controller.rb module TimeTracking\n  class TrackersController < ApplicationController\n    def index\n      # business logic\n      respond_with @trackers\n    end\n  end\nend It looks like a typical controller, but without the authentication. The benefit of this is that if we pull Time Tracking out to be its own application, we would only need to change the proxy controllers and how they interface with Time Tracking. Similarly if authentication were a separate engine or service, we would also only need to change the proxy controllers. We also wanted to make sure that the proxy layer doesn’t contain any business logic and should only be forwarding requests. Service-service communication Although everything is globally accessible inside the engine, we wanted to create a clear API between the engine and the monolith. In order to draw that explicit boundary, we serialize and deserialize messages passed between the two. By only passing around serialized objects, we reduce the temptation for Time Tracking to make additional database queries or reach into private data with ActiveRecord models, and also be limited to the contract defined by the monolith’s API. We define these services in the monolith and inject them as dependencies into the engine. We followed the configuration specified in the Ruby on Rails guide but instead of injecting ActiveRecord classes, we inject service classes that have a defined interface. This lends itself better to Time Tracking being a separate application, where we would only need to change the configuration of the services it depends on. Testing All our tests live together inside the monolith, which means that the monolith is still being loaded when we run tests. When testing Time Tracking, we stub out all API calls made to the monolith. Currently this isn’t enforced anywhere but just something the team practices. We also have integration tests to verify that the communication between Time Tracking and the monolith are working in order. How is it holding up? What’s been working really well for us is having the clear API boundary and making sure we aren’t passing rich objects across that boundary. It helps ensure we only pass the information we need and don’t accidentally reach for more. Having the code separated into an engine rather than living in the monolith has also made it easier to navigate the code and onboard new teammates onto the domain. Some things that may not hold up as well if we were to truly pull the engine out to a separate service is the amount of data fetches Time Tracking makes from the monolith. It isn’t an issue now since this is all in-process, but across a network boundary it could cause some timeout problems. It’s a great reminder of continuing to be mindful about the domain boundary so that Time Tracking only knows what it needs to know. What would we change? While we were very mindful from the beginning about keeping Time Tracking separate, we could have done more to help enforce this. Having the tests run outside of the monolith from the get-go would have enforced that the engine isn’t accidentally reaching into the monolith since we’d be able to test the engine in isolation. Most of the communication is also synchronous request-response patterns when we could’ve leaned more heavily on publish-subscribe patterns to help separate concerns better. Rails engines alone do not provide much separation other than having the code live in a separate directory, but encapsulating the domains by only exposing what’s necessary in addition to the code separation has been a valuable first step towards a modular monolith. Some other great reads on creating a modular monolith : https://medium.com/@dan_manges/the-modular-monolith-rails-architecture-fb1023826fc4 https://engineering.shopify.com/blogs/engineering/deconstructing-monolith-designing-software-maximizes-developer-productivity", "date": "2019-04-30"},
{"website": "Gusto", "title": "Nonce-based Content Security Policy (CSP) in Rails", "author": ["Edward Qiu"], "link": "https://engineering.gusto.com/nonce-based-content-security-policy-csp-in-rails/", "abstract": "Introduction During my time at Gusto as a part of the Application Security team, I’ve been exploring ways to improve defense against Cross-Site Scripting (XSS) in modern web applications. At Gusto, we primarily use Ruby on Rails and React.js. Individually each framework comes with some XSS protections out of the box, but sharing information between frameworks makes contextual output escaping or encoding more difficult. In other words, the what, when, where and how for escaping user input questions become more difficult to answer. A Content Security Policy (CSP) allows us to define rules around content as another layer of defense in addition to sanitization and Rails’ and React’s built-in auto-escaping. What is a Content Security Policy? Content-Security-Policy (CSP) is an HTTP response header or a meta tag with a set of directives. The set of directives can be viewed as instructions for the browser on what type of content to trust and where and how such content can be sourced. script-src directive with some host-source directives allowing for CSP bypass For example, the script-src directive shown above allows us to define which host-sources the browser is able to load JavaScript files from. The browser will check the src attribute in a script tag against one of the host-sources listed and blocks the network request if the src attribute is not found. Network tab shows CSP enforcing \"mode\" blocking some fonts from loading If you would like to learn more about CSP, this post by Mike West is a great place to get started! This post will focus on CSP as an HTTP response header. What is the point of a Content Security Policy? In theory, a strict CSP ¹ enables an organization to follow the defense in depth security principle. However, in practice, a CSP is known to be painful to configure, deploy and maintain and a small mistake in configuration allows for CSP to be bypassed. If a deployed CSP is strict, it serves as another line of defense against XSS, clickjacking , CSS Exfiltration , and mixed content bugs . In a Ruby on Rails application, CSP is an additional line of defense to proper sanitization of user input and Rails’ erb and HAML built-in auto HTML escaping . A CSP increases the cost of attack for an attacker because they would need to construct a payload that bypasses sanitization functions, and host-sources, nonces or hashes in the CSP configuration. Besides reducing XSS risks, a CSP can provide protection against clickjacking with the frame-ancestors directive. A CSP can also reduce the attack surface for a CSS exfiltration attack by limiting an attacker's payload from making calls to hosts not whitelisted, sourcing malicious CSS documents, or injecting inline CSS. Defense in depth - an attacker must break through each layer of defense from sanitization of user input to CSP in order to reach the crown jewels Enforcing vs Report-Only There are two \"modes\" for CSP - enforcing and report-only. Enforcing means all code that is in violation of the CSP will be blocked from loading. Report-Only does NOT block any code in violation of CSP from loading. One might ask, what is the point of CSP in report-only \"mode\"? The purpose of report-only \"mode\" is to help the development process of CSP to be less painful as iterations on the policy will not break application functionality. So how does one tell the difference between the two \"modes\"? If there is Content-Security-Policy HTTP response header, then CSP is in enforcing \"mode\". If there is a Content-Security-Policy-Report-Only HTTP response header, then CSP is in report-only \"mode\". Additionally, when violations occur in report-only \"mode\", CSP errors will include the keywords report only in the browser's developer tools console. For example, when these violations occur in Chrome, the error message in Chrome's DevTools console will be prepended with [Report-Only]. Example of CSP violation in DevTools console when CSP is in Report-Only \"mode\" However, it is possible to send both Content-Security-Policy and Content-Security-Policy-Report-Only headers. One use case of sending both headers is when an organization has an existing CSP in enforcing \"mode\", but wants to test new directives or configurations without breaking functionality. A Content-Security-Policy-Report-Only header can be sent with the new configuration in addition to the existing Content-Security-Policy header for testing the new changes before they are integrated into the existing CSP. One way of checking which headers are set by the server is to make a request with curl and check the response headers. curl -SL -D - https://example.com | grep Content-Security-Policy Another way of checking headers is by using the Network tab in the browser's developer tools. Network tab in Chrome DevTools shows that both Content-Security-Policy and Content-Security-Policy-Report-Only headers are sent How Does Reporting Work? When violations occur, CSP is capable of logging them through the report-uri / report-to directive. Example report-uri directive When the report-uri/report-to directive is set and a CSP violation occurs, the browser will make a POST request to the URL value corresponding to report-uri/report-to with a JSON body An example of violation report {\n    \"csp-report\": {\n        \"blocked-uri\": \"inline\",\n        \"column-number\": 1,\n        \"document-uri\": \"http://gusto.com/path\",\n        \"line-number\": 21,\n        \"original-policy\": \"default-src 'self'; script-src 'unsafe-inline' 'strict-dynamic' 'nonce-[removed]'; report-uri https://subdomain.report-uri.com/r/d/csp/ReportOnly\",\n        \"violated-directive\": \"script-src\"\n    }\n} Traditional CSP vs Nonce-based CSP Traditional CSP is setting the directives as a whitelist of host-sources. Example of a directive in traditional CSP However, a research paper published by Google suggests that the majority of traditional CSPs are ineffective in mitigating against XSS. Often web applications require a large amount of refactoring to remove inline scripts or styles. The keyword-source , 'unsafe-inline' , is a compromise to deploy CSP without breaking functionality in a production environment, but the use of 'unsafe-inline' effectively disables the protection CSP would have provided. To solve this problem, the paper recommends taking the nonce-based approach to implementing CSP for dynamic web applications. A nonce is a randomly generated value that is not intended to be reused. A nonce-based CSP generates a base64 encoded nonce per each request then passes it through the HTTP response header and appends the nonce as an HTML attribute to all script and style tags. The nonces in all scripts and style tags are checked against the nonce in the response header. For all nonces in script and style tags that match the nonce in the header, the browser allows those scripts and styles to load during the browser’s subresource loading process . Example of nonce-based CSP behavior with script-src directive For those who are familiar with how CSRFs are mitigated, the CSRF or anti-forgery token embodies the same spirit as the nonce in CSP. Abstractly, the CSRF token and nonce are random values that are validated before an action is performed. Special thanks to Fredrick Lee and Julianna Roen for their feedback on earlier versions of the post. Appendix [1] Google has some guidelines on what they believe what a strict configuration of a CSP looks like.", "date": "2019-05-10"},
{"website": "Gusto", "title": "A Practical Guide to Building Secure SSO", "author": ["Tori Huang"], "link": "https://engineering.gusto.com/a-practical-guide-to-building-secure-sso/", "abstract": "Understanding how to build Single Sign On (SSO) using the OAuth 2.0 framework with OpenID Connect may seem like a daunting task, especially if this is your first time working with these tools. The internet is overflowing with information on the subject, but piecing everything together to form a working authorization system can be tricky. My team encountered these difficulties first hand. As part of the Partners Engineering team at Gusto, I work to build an amazing experience for accountants, advisors, and other customers using Gusto to serve their clients. This includes ensuring a secure and easy way to connect with Gusto. Working on SSO for Intuit was one of my first assignments as a new Gusto engineer. To learn more about authorization, I sifted through the multitude of OAuth 2.0 and OpenID Connect articles theoretically explaining what these tools were and found myself wishing there was one central location that walked me through exactly how to implement them. This practical guide to the OAuth 2.0 framework, enhanced with OpenID Connect, will show you how to build a secure SSO system. We will take a step-by-step look at every request and response - starting with a high level overview of the process. What is SSO? With any SSO process there are three key players: the user, the relying party, and the identity provider. The user is the person requesting access. The account where she is requesting access is the relying party. The identify provider is the account she would like to identify herself with in order to create or access her account with the relying party. While SSO does not necessarily need to start at the login screen, it is the most easily identifiable step in the process. The user here has an option to either log in to their Gusto account or use the SSO process to sign in via Google, Xero, or Intuit. Let’s look at an SSO example of a user named Melissa signing up with Gusto via her Intuit account. The three key players: Melissa - the Gusto user trying to access Gusto Gusto - the relying party where Melissa would like to gain access Intuit - the identity provider Melissa wants to use to create a Gusto account Melissa wants to go to the Gusto homepage, click an Intuit sign in button, enter her Intuit credentials, and be redirected to her Gusto homepage. For this process to work, Gusto needs Intuit to share some of Melissa’s basic user information such as email, first name, and last name. However, Intuit cannot just hand over Melissa’s information to anyone who requests it. Intuit will need to carefully verify that the request is originating from Melissa and that Gusto is the relying party making this request for her. This is not a straightforward process! There is a great deal of back and forth between Melissa, Gusto, and Intuit. Let's take a closer look at this process. Step by Step Requesting a Code Gusto needs to request Melissa’s data from Intuit. The first step is receiving a code from Intuit, which we will eventually swap for a token. Here is what that request looks like. GET https://appcenter.intuit.com/connect/oauth2?client_id=your-client-id&redirect_uri= https://app.gusto.com/user/auth/intuit/callback&scope=openid%20profile email&response_type=code&state=auto-generated-state-field Param Example What is Happening? Where to make request https://appcenter.intuit.com/connect/oauth2 The URL defined by Intuit where Gusto sends a GET request for a code. redirect_uri https://app.gusto.com/user/auth/intuit/callback The redirect uri defined by Gusto. Intuit will use this redirect uri to return a code for Melissa. client_id <gustos-client-id> The client ID defined by Intuit which is used to help identify that Gusto is making the request scope <openid + other optional variables> The information Gusto requests from Intuit for Melissa response_type code The response type you wish to use for authorization. These types are defined by the OAuth 2.0 framework. state <auto-generated-state-field> This should be generated by an OAuth 2.0 library. This helps prevent phishing attacks that can result in account takeovers. Once we request a code from Intuit, Intuit must confirm Melissa's identity and verify that she would like to give Gusto access to her Intuit account. This process typically looks like this: Intuit identifies Gusto by our client_id. Intuit asks Melissa to login. Intuit looks at the scope param requested with the code request. Scope specifies which data Gusto is asking for access to. Intuit needs to determine if Gusto has access to the requested data (defined by the scope) for Melissa. Intuit will ask Melissa if she wants Gusto to access her data. Do I have to use “code” for “response_type”? There are many response types to choose from when implementing the OAuth 2.0 framework. Authorization code grant is a common SSO response type. Authorization code grants are meant for web apps where the application is not the resource owner. In Melissa’s scenario where the resource owner is Intuit and the application is Gusto, an authorization code grant is appropriate. Depending on your specific use case , you may be better off with a different resource type. Why can’t I request a token directly? According to the OAuth 2.0 docs, “The authorization code provides a few important security benefits, such as the ability to authenticate the client, as well as the transmission of the access token directly to the client without passing it through the resource owner's user-agent and potentially exposing it to others, including the resource owner.” Basically, without requesting a code first the relying party cannot ensure that the requester is who they say they are. Returning the Code Once Intuit has verified Gusto can receive Melissa’s information, Intuit will send Gusto a code via a GET request to the redirect_uri defined in the Code Request params. GET https://gusto.com/user/auth/intuit/callback?state=<state-field-from-gusto>&code=<code-from-intuit> Param Example What is Happening? state <state> Defined by Gusto on initial request and returned unchanged by the identity provider code <code> Created by the identity provider, used to exchange for a jwt token State One of the major updates in OAuth 2.0 is the addition of a state parameter, which addresses a dangerous security gap in OAuth 1.0. The state parameter is defined by the requester and verified upon return of the code. Step One is complete! Finally. Now we have a code and can exchange that code for Melissa’s token. Exchanging the Code for a Token We finally have our first response from Intuit! Gusto can now request an access token for the user through a POST request to the token url. POST https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\ngrant_type=authorization_code\n&client_id=<gustos-client-id>\n&client_secret=<gustos-client-secret>\n&redirect_url=https://app.gusto.com/user/auth/intuit/callback\n&code=<code-provided-by-intuit> Param Example What is Happening? token url https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer Defined by Intuit grant_type authorization_code We selected response_type code, so we are exchanging a code for a token. These types are defined by the OAuth 2.0 framework. client_id <gustos-client-id> The client ID defined by Intuit which is used to help identify Gusto as the requester client_secret <gustos-client-secret> The client secret defined by Intuit for Gusto’s developer account. While the client_id is public, the client_secret should remain private at all times. This secret proves to Intuit that Gusto, the only entity with access to the client_secret, is making this request redirect_url https://app.gusto.com/user/auth/intuit/callback As an added measure of security, Intuit will verify that the redirect URL in this request matches exactly the redirect URL that was included in the initial authorization request for this authorization code. code <code-provided-by-intuit> The code we went through all that effort to retrieve! If successful, this POST request will return a JWT token which can be used to access information about Melissa. JWT (JSON Web Tokens) is a standard that defines a compact way for securely sending data as a JSON object . HTTP/1.1 200\nContent-Type: application/json\n{\n  \"access_token\": <jwt-access-token>,\n  \"expires_in\": 3600,\n  \"token_type\": \"bearer\",\n  \"x_refresh_token_expires_in\": 8726400,\n  \"refresh_token\": <refresh-token>,\n  \"id_token\": <jwt-id-token>\n} Validate the JWT Signature Congratulations - you have Melissa’s token! Your work is almost complete. Now we can decode the token using the public key. This public key is usually stored on the identity provider’s website. For example, Intuit’s public key is https://oauth.platform.intuit.com/op/v1/jwks . We take the public key and use a JWT library to decode the token with this key. Public key structure is defined by OpenID Connect Standards . The contents of the public key are defined by Intuit, so any user can access the public key and decipher JWT authorization data returned by Intuit. Finally Access Melissa’s Data Once deciphered, the JWT gives us access to Melissa’s Intuit user data. Gusto can take this data and use it to improve Melissa’s Gusto experience. In this case, we would store a record in Gusto’s database linking Melissa’s Intuit user ID to her Gusto user ID. The next time Melissa signs in to Gusto with Intuit, we will check our database for an existing mapping between her Intuit and Gusto accounts. When we find this mapping we can automatically log her in and redirect her to the Gusto dashboard. {\n \"sub\": <melissa-intuit-user-id>,\n \"aud\": [\n  <aud>\n ],\n \"auth_time\": 1552063954,\n \"iss\": \"https://oauth.platform.intuit.com/op/v1\",\n \"exp\": 1552067567,\n \"iat\": 1552063967\n} Strategy Manager Authentication gets complicated. Utilizing a strategy manager can help standardize the process. Strategy managers can handle multiple authentication providers, making it easy for developers to connect with new identity providers. Pitfalls Once you finally retrieve the token, it may feel as though your work is complete. However, while some places will send user information within the jwt token, others require an additional request for user information. Be sure to carefully read through the docs in order to determine if this additional step is required for your specific authorization scenario. Your strategy manager should accommodate this additional step. Another common pitfall is receiving an invalid_grant error when exchanging a code for a token. While this error seems unhelpful, it is probably telling you one of two things - your code has expired or the redirect url is invalid. In order to provide additional security, many identity providers will require the requester to define a list of acceptable redirect uris. If the redirect uri defined in the token request is not identical to the predefined acceptable redirect uri, you may receive an invalid_grant error. Resources Still confused? Walk through this process step by step with this OpenID Connect Playground . Another great resource for decoding jwt tokens is this JWT Debugger . https://blog.runscope.com/posts/understanding-oauth-2-and-openid-connect https://auth0.com/docs/protocols/oauth2#how-response-type-works https://stackoverflow.com/questions/13387698/why-is-there-an-authorization-code-flow-in-oauth2-when-implicit-flow-works-s", "date": "2019-05-15"},
{"website": "Gusto", "title": "An Open Email about Diversity", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/an-open-email-about-diversity/", "abstract": "In September of 2018, I wrote about our ambitions for diversity in engineering for the next six months and committed to another post in March 2019 sharing how we did. By sharing our learnings, we hope to encourage others to build a more diverse and inclusive team. In lieu of a typical blog post, I'd like to instead share an email that I wrote to our engineering team last month. You’ll see (1) how we did, (2) our aspirations for the next six months, and (3) a new way of working to help us along the way. The email is only slightly edited, mainly to remove Gusto-specific jargon and update our most recent numbers (the current column). -- From: Eddie To: Gusto Engineering Subject: [FYI] Our Engineering Diversity & Belonging Aspirations and Approach Hi team, As you know by now, we’re committed to building a diverse and inclusive engineering team. As a reminder of why, we have three primary reasons : Better for the business: A more diverse team will help us build better products for our diverse set of customers. Better for the team: Diversity is important for a good engineering culture and increased work happiness. We want to hire and work with the best people. Better for the world: It’s the right thing to do. We see ourselves as part of a greater community with a shared responsibility to make the world a better place. We’ve come a very long way since we started this journey back in 2015. Overall, 32% of our engineering team (defined as those in our software engineering and information security teams) comes from an under-represented group (URG). At Gusto Engineering, we define an individual as coming from an URG if they identify as a woman, Black, Latinx, American Indian, Alaska Native, trans, or a non-binary person. As you may know, we had hoped to increase diversity in our senior engineering positions (L3+). Six months ago, only about 9% of our senior engineers came from an URG. Today, that number is 18%. Here are some other diversity statistics that you should feel proud of: As stated before, 32% of our engineering team comes from a URG. Our voluntary attrition rate for women engineers since 2016 is still zero . 41% of recently-promoted engineers were women. We’re making progress, but we still have a long way to go. And we’re not taking our foot off the pedal now! We spent the last month thinking long and hard about how we can scale our efforts. In the end, we’ve decided to shift the Engineering Diversity & Belonging Committee from a workstream model to an accountability model. That is, instead of the committee organizing much of the work themselves, we’ll help teams set and reach their diversity and belonging aspirations. Through this approach, we hope to partner with the teams inside Gusto that can have the most impact. By doing so, we hope to (1) increase diversity in our senior engineering positions from 18% to 25% over the next 6 months and (2) make this amazing team a place where everyone feels that they belong. Thank you for all your continued support for the Engineering Diversity and Belonging Committee. If you have any feedback or suggestions on how we can improve, please let us know by joining our Slack channel, or come by our bi-weekly meeting. Below, you'll find what we've decided to focus on in the Engineering organization: Focus Aspirations for the next 6 months Responsible Team Current # of woman/non-binary directly reporting to Eddie At least one Eddie 0 URG candidates who get a phone screen 1 30% Eng Recruiting 23% URG managers 3 15% Eng Staff 24% URG individuals who answer positively 4 to the employee engagement question: “I would recommend Gusto as a great place to work” Variance among URGs and non-URGs by no more than 5% Eng Staff -4% 2 URG promotion rates Variance among URGs and non-URGs by no more than 10% Promotion Committee Put up for promotion: 18% Approved for promotion: -13% 2 Compensation across gender and ethnicity Compa Ratio difference of 3% or less. Compensation Committee Compa ratios: 100.6% for Men 100.5% for Women 101% for Ethnicity Senior URG Engineers 25% All 18% 1 30% may not seem very aggressive at first glance. However, we feel more comfortable with this due to available URG representation for our currently open positions (of which 85% are for senior engineers). 2 Negative percentage indicates the number is in favor of non-URG. Positive numbers indicates the numbers is in favor of URG. 3 We recently exceeded our aspirations here! We’ll continue to push ourselves to achieve greater equality here and adjust our aspirations accordingly in the future. 4 Answered “agree” or “strongly” agree. Other possible answers are “strongly disagree”, “disagree” and “neutral”. -- As we’ve always done, we’ll continue to provide regular updates on how we are doing with diversity and belonging! Just as our Diversity and Belonging Committee is helping the relevant teams reach their aspirations, we hope that you, the reader, will keep Gusto accountable to our overall aspirations. We also hope that other technology organizations will be encouraged to share how they’re addressing diversity and belonging in their workplaces! Comments on Hacker News", "date": "2019-03-27"},
{"website": "Gusto", "title": "Debugging Memory Bloat", "author": ["Brittney Johnson"], "link": "https://engineering.gusto.com/debugging-memory-bloat/", "abstract": "Or, that time a single column update crashed our app unsplash-logo Fredy Jacob A while back, a single class of jobs took down all of our async processes due to Out Of Memory problems. I told you how we prevented the general problem of a single class of jobs halting all of our async work. Now we’ll dive into how a class of jobs that had been happily running every night for years suddenly threw a temper tantrum and broke everything. Something’s wrong. From an investigation into a larger incident, we discovered that a class of asynchronous job was using a huge amount of memory and taking down its container. I’ll call that class of job “DisasterJob.” We needed to reduce the memory this job needed to run, which meant figuring out what was causing the high memory usage in the first place. All the info we had This was everything I knew going into the investigation: DisasterJob was old code: it had been written 3 years ago, and had run every night since. DisasterJob’s primary purpose was to look at Benefit Applications in our system and update the start date for the benefits on those applications if the original start date was no longer feasible. E.g. if the Benefit Application originally had a start date of 12/1/2018 when it was created on 11/5/2018, but time had passed and today’s date was now 12/2/2018, we’d update the start date to 1/1/2019 instead. If DisasterJob didn’t run, customers might accidentally sign up for the wrong benefits due to an incorrect start date. I didn’t know much beyond that, and neither did anyone else. The people who originally wrote and reviewed the code had either left the company or forgotten what they did 3 years prior. Where to start? Repro! Given that nobody knew what the code was actually doing, it was impossible to just make an educated guess around where the problem was coming from and try to fix it. I tried running the code in my local rails console to see if I could reproduce the issue: > DisasterJob.new.perform(args)\n=> true Ok, so… it worked? But I had no clue what the memory usage was. Time to use a profiler! After a bit of Googling, I decided to use the memory_profiler gem. It was pretty straightforward to run. require \"memory_profiler\"\n\nmemory_report = MemoryProfiler.report do\n    DisasterJob.new.perform(args)\nend\n\nmemory_report.pretty_print … but whoa was that output huge! The gem prints out lots of sections, including “allocated memory by gem”, “allocated objects by class”, “allocated memory by file”, and much more. I didn’t know which part of the report to look at! But I did know I didn’t want it all dumped into my terminal, so I changed the config to print to a file: require \"memory_profiler\"\n\nmemory_report = MemoryProfiler.report do\n    DisasterJob.new.perform(args)\nend\n\nmemory_report.pretty_print(scale_bytes: true, to_file: “my_report”) The beginning of the output file named “my_report” looked like this: Total allocated: 736968 bytes (8013 objects)\nTotal retained:  72855 bytes (684 objects)\n\nallocated memory by gem\n-----------------------------------\n    221032  binding_of_caller-0.8.0\n    190321  activesupport-4.2.10\n    122840  contracts-0.11.0\n     87985  activerecord-4.2.10\n     75424  my_repo/app\n...\n...\n... Cool, numbers! unsplash-logo Jungwoo Hong But how to make the numbers go down? 🤔 Again, I didn’t know much about how the guts of the code worked, and didn’t know if the memory usage listed was good or bad. In fact, I didn’t even know what category of memory usage the report listed was important! I decided to run the code over and over, with different parts commented out each time, and compare the first numbers I saw. I ran into a bit of trouble the second time I ran DisasterJob. It was so fast and hardly used any memory! I realized that my first run of DisasterJob had, well, done its job: The start date on the BenefitsApplication was appropriately set to being in the future. The second run early exited because the BenefitsApplication no longer needed to be updated. Instead of finding a new BenefitsApplication to update each time I ran the code, I put the code in a transaction so that I could easily rerun the job without worrying about having to find new data each time: require \"memory_profiler\"\n\nmemory_report = nil # define the var outside of the transaction block to retain access\nActiveRecord::Base.transaction(joinable: false, requires_new: true) do\n    memory_report = MemoryProfiler.report do\n        DisasterJob.new.perform(args)\n    end\n\n    raise “Don’t actually save!”\nend\n\nmemory_report.pretty_print(scale_bytes: true, to_file: “my_report”) Then I started commenting out lines. I took the first few numbers the report gave me for a few runs and put them in a simple spreadsheet: Whoa . The chart above has a bar for the memory usage of DisasterJob (1) unchanged, (2) with the first line of its code removed, (3) with the second line of its code removed, and (4) with the third line of its code removed. The last bar is way smaller than the others, meaning that removing the third line of DisasterJob’s code somehow shrunk its memory usage by 80%. I don’t know what that third line was doing, but it looks like it caused most of the code’s memory usage! Inspecting further, I found that the third line was a method that called a few other methods itself, so I repeated this commenting-out-and-rerunning-the-profiler process. I tried to comment out half of a method at a time and then narrow or expand which lines were commented from there. In short, I was binary-searching DisasterJob’s codepath for low memory usage. Eventually, I got something like this: I was able to confirm that commenting out one single method call, which did not delegate to any other methods, dropped the “total allocated memory” volume from ~0.75gb to ~.15gb. In particular, it dropped ~0.6gb of activerecord usage to nearly nothing! The culprit? A simple column update: BenefitApplication.update!(start_date: new_start_date) WHAT. Why is a single column update using that much memory? I wondered the same thing, dear reader. After a few ~hours~ days of headbanging, I sleuthed out the true culprit: a before_save callback on the BenefitApplication model. 😱 It turned out that before saving that simple start date update, our code was running a method that made an extremely expensive query [1] . If you remember, DisasterJob’s purpose was to push the start date for BenefitApplications forward. The expensive query in the BenefitApplication model was being used to find data about health insurance plans on the new date. In the benefits industry, thousands of new benefits plans are introduced each calendar year. This meant that when BenefitApplications were pushed to a start date in a new calendar year, a lot of new data was added to the inefficient query. Each time we ran DisasterJob, all of the health insurance plans in our system were being grouped and analyzed, even if they had expired years ago. I scoped down the data we were querying to a much smaller and more relevant subset before running calculations on it. I filtered out expired health insurance plans or plans that weren’t applicable to the company that was applying for benefits. This resulted in pulling a lot less data into memory. Nice. Are you sure it worked? Yup! With my code change, DisasterJob ran using ~.35gb of local memory instead of ~.75gb and still accomplished its task. Plus, when we re-enabled DisasterJob in the production environment, it ran without error. Yay! But why didn’t DisasterJob have this problem any other time in its three year history? Good question! It just so happened that the run of DisasterJob on this day caused an update to a lot of BenefitApplications, resulting in a particularly inefficient data query compared to  any day before. Cool! How should I use this info? Get familiar with the memory profiler gem, and always think critically about your DB queries. I would even recommend running a few of your common codepaths and async jobs through a memory profiler and a speed profiler. You might find a way to make a huge improvement! In the course of the investigation, I discovered that the expensive query wasn’t even working properly. Yikes! Luckily the fix for the memory allocation also fixed the correctness of the query. ↩︎", "date": "2019-06-07"},
{"website": "Gusto", "title": "Ruby Blocks Simplified", "author": ["Julianna Roen"], "link": "https://engineering.gusto.com/ruby-blocks-simplified/", "abstract": "One of the most unique and often misunderstood features of Ruby is blocks. Blocks are Ruby’s version of closures and can be used to make code more reusable and less verbose. But keywords such as yield can be hard to grok at first and make this functionality a bit intimidating to work with. This article aims to go over block essentials and build up your knowledge piece by piece. What is a block? Blocks are anonymous functions whose return value will be applied to the method that invokes it. That’s quite a mouthful 😳 so let's work up our understanding. Blocks consist of code between a set of curly braces or a do/end pair. The former is the single-line definition and the latter is the multiline definition. method { |i| ... }\n\nmethod do |i|\n  ...\nend The single-line definition is primarily used for one-liners. For the sake of consistency I’ve used the do/end syntax in all of the examples If you’ve worked with Ruby before, I almost guarantee you’ve seen a block. The each and map methods are two of the most commonly used iterators that invoke block usage. [\"⭐️\", \"🌟\"].each do |star|\n  puts star\nend\n\n# Output\n⭐️\n🌟 How do you create a block? If we wanted to create a simple function that prints an input wrapped in \"⭐️\", we could write something like this… def star_wrap(el)\n  puts \"⭐️\" + el + \"⭐️\"\nend\n\nstar_wrap(\"💙\")\n\n# Output\n⭐️💙⭐ If we wanted to rewrite this function using block notation , we could do it like so… def star_wrap\n  puts \"⭐️\" + yield + \"⭐️\"\nend\n\nstar_wrap do\n  \"💜\"\nend\n\n# Output\n⭐️💜⭐ As shown above, the return value of the block contents are what is passed to the yield keyword in the function. We can also make the block more customizable by passing a parameter to the function we created. def wrap_with(el)\n  puts el + yield + el\nend\n\nwrap_with(\"⭐️\") do\n  \"💚\"\nend\n\n# Output\n⭐️💚⭐️ If we want to refer to values from our function in the attached block, then we can pass arguments to yield and reference it in the block parameters… def wrap_with(el)\n  puts el * 5\n  puts yield(el * 2)\n  puts el * 5\nend\n\nwrap_with(\"⭐️\") do |els|\n  els + \"🖤\" + els\nend\n\n# Output\n⭐️⭐️⭐️⭐️⭐️\n⭐️⭐️🖤⭐️⭐️\n⭐️⭐️⭐️⭐️⭐️ But why use blocks over a regular function? So far it doesn’t really seem like we’re doing anything that a typical method can’t do. Blocks come in handy when we want to apply shared logic to various contexts in an encapsulated way. For example, let’s say we knew that we always wanted to print the output of a series of commands between a set of \"⭐⭐⭐\". We can use blocks to apply the wrapping logic to different contexts without having to make auxiliary functions. def star_wrap\n  puts \"⭐⭐⭐\"\n  puts yield\n  puts \"⭐⭐⭐\"\nend\n\nstar_wrap do\n  server = ServerInstance.new\n  data = server.get(\"orange/heart/endpoint\")\n  data.to_s\nend\n\nstar_wrap do\n  fetcher = DatabaseFetcher.new\n  data = fetcher.load(\"purple_heart_data\")\n  data.exists? data : \"no heart data\"\nend\n\n# Output (hypothetical)\n⭐⭐⭐\n🧡\n⭐⭐⭐\n\n⭐⭐⭐\n💜\n⭐⭐⭐ As shown above, stars are always printed before and after the code executed in a block. Even though \"🧡\" was fetched quite differently than \"💜\"️, the star_wrap method allows us to apply the star wrapping logic to both contexts in a contained manner. Block error handling Any method can accept a block, even if it is not referenced in the function. The block contents will simply do nothing. def stars\n puts \"⭐⭐⭐\"\nend\n\nstars do\n  puts \"💙\"\nend\n\n# Output\n⭐⭐⭐ We were able to invoke all of the blocks in the examples above because we used the keyword yield . So, what if we called yield and did not supply a block? An error will be raised. def star_wrap\n puts \"⭐️\" + yield + \"⭐️\"\nend\n\nstar_wrap\n\n# Output\nLocalJumpError: no block given (yield) We can amend this issue by using the block_given? expression to check for block usage. def star_wrap\n  if block_given?\n    puts \"⭐️\" + yield + \"⭐️\"\n  else\n    puts \"⭐️⭐️⭐️\"\n  end\nend\n\nstar_wrap\n\n# Output\n⭐️⭐️⭐️ Passing a block as a parameter If we wanted to be more explicit in our invocation of a block and have a reference to it, we can pass it into a method as a parameter. def star_wrap(&block)\n  puts \"⭐️\" + block.call + \"⭐️\"\nend\n\nstar_wrap do\n  puts \"💛\"\nend\n\n# Output\n⭐💛⭐ In this instance, the block is turned into a Proc object which we can invoke with .call . Using blocks in this manner comes in handy when you want to pass blocks across functions. We specify the block parameter by passing it as the last argument and prepending it with & . Below, the methods star_wrap_a and star_wrap_b do the exact same thing … def star_wrap_a(&block)\n puts \"⭐\" + block.call(\"✨\") + \"⭐\"\nend\n\ndef star_wrap_b\n puts \"⭐\" + yield(\"✨\") + \"⭐\"\nend\n\nstar_wrap_a do |el|\n el + \"💙\" + el\nend\n\nstar_wrap_b do |el|\n el + \"💚\" + el\nend\n\n# Output\n⭐✨💙✨⭐\n⭐✨💚✨⭐ Blocks in the real world In a default Rails app, the application.html.erb view is loaded for every page whose controller inherits from ApplicationController . If a child controller of ApplicationController renders a view, its contents are yielded to application.html.erb . With this functionality in place, boilerplate HTML that must be applied to all pages of the app can be done so easily. <!DOCTYPE html>\n<html>\n  <head>\n    <title>Block Investigation</title>\n  </head>\n  <body>\n    <%= yield %>\n  </body>\n</html> For example, in Gusto’s Partner Directory, the top blue bar resides in a base view file which yields to a different layout for each route. And those are the essentials! Blocks don’t have to contain complicated logic in order for us to make use of them. They are typically used to abstract away shared logic that can be applied to a multitude of contexts. If written thoughtfully, code can be made more DRY and readable through harnessing their functionality. To solidify any of the concepts mentioned above, I recommend trying out the code samples in an interactive console or by writing your own examples. Happy ⏹-ing.", "date": "2019-07-30"},
{"website": "Gusto", "title": "What is Data Engineering?", "author": ["Lindsey Whitley"], "link": "https://engineering.gusto.com/what-is-data-engineering/", "abstract": "Before I started as a Data Engineer two years ago, I had no idea what the role entailed or how it differed from data science and data analytics. Job titles with the word \"data\" in them are known to be an enigmatic black box. That's true even for folks in technical roles. This post is what I would have wanted to read when I was trying to fit the pieces of the data pipeline together. Can you give me an overview first? What we do What we don't do Collect data from various internal and external sources Analyze data to provide business teams with data-driven insights Transform data into use-able formats Create or train Machine Learning models Load data into convenient and controllable locations for other teams to use Build features in Gusto’s people platform product Build and maintain infrastructure for ourselves and other data teams Validate or invalidate experiments via data analysis How do you relate to the other data teams? At Gusto, Data Platform Engineers support the other data teams - Data Analysts and Data Scientists . If those teams need access to data, we move and adjust it to a usable format for them. If they need a server that can crunch numbers for them quickly, we maintain that server. If they need a tool to explore data interactively, we make sure they have it - whether it’s an external tool that we keep connected or a tool we build ourselves. What part of the business does your work impact? Primarily, our work impacts the business side of Gusto. For example, we recently switched our customer satisfaction (CSAT) surveys to a new vendor, and the customer-facing teams who rely on the surveys needed to be able to access the data in a similar way as before. My job was to collect the raw responses from the third-party vendor, transform the data to make it query-able, and then make sure a partner on the Data Analytics team had access to it. From there the Data Analyst made graphs and tables so that the customer-facing teams were able to see their stats in a way that was already familiar to them. By supporting Data Scientists, Data Analysts, and anyone who can write SQL, we are able to support the data needs of the whole company. Why do you need to do all this to data? If you only needed to use data from one database, you may not need to move it beyond creating a read-only cluster specifically for this purpose. Even then, it’s likely that the database is structured in a way that works for the application or the engineers, rather than for the people who need to analyze the data. If your users (i.e. data analysts and business teams) need information from more than one database or want to use data from third party tools, you’ll need some way to group all the data together (i.e. a data warehouse). If your database doesn’t keep a record of each write, you may need some way to allow users to get a historical view of the database. I could keep going, but you get the idea. The data is very unlikely to be in the most ideal state for queries that yield fast insights, which is why it needs to be copied and restructured. So you get data from more than one place? What are all the data sources? In our case, we gather data from our four internal databases which contain the data created by our apps. For example, whenever a customer runs a payroll, multiple rows are created or updated across multiple tables. We also gather data from external tools via their APIs. These include Qualtrics, Marketo, and Zendesk Chat, among others. Another way to get data from an external tool is to connect to the database directly. We do this with Salesforce, but we also copy data into certain Salesforce tables from our internal databases. Ok, but what product do you create? In our case, we expose a data lake to the Data Analytics and Data Science teams. The data lake is meant to be a place of discovery for these teams. Since the data is raw, it takes less work for the Data Engineering team to manage, but it doesn’t eliminate data that could be useful for skilled explorers. More broadly within Gusto, we expose a data warehouse of tables that are structured to be queried quickly and only contain a subset of all the data in the lake. For Gusto, all of our data goes through the lake before it gets to the warehouse, and only the data that we know is useful and worth cleaning gets to the warehouse. These tables are meant to be more easily understood and allow for varying levels of access to sensitive data through different schemas. Overall, we can think about the difference like that between a lake and tap water. Lake water in some places is safe to drink, but usually it’s not and in most cases getting the water is a lot less convenient than drinking from the tap. Tap water on the other hand is enriched with fluoride, is stripped of lead and bacteria, and is at your fingertips when you want it. Similarly, the lake is a huge amount of data (all that my team collects). To make any use of it, you need to know what’s in it and how to treat it to write a query that returns anything a human can make sense of. The warehouse has been cleaned for you; it’s in tables that make sense for known use cases; and you can get answers out of it quickly. How do you make the lake and the warehouse? First, we run a scheduled ruby CLI command to get raw data from the data sources and store it in S3 in the rawest possible format. Storing raw means that if we decide later on that we want an additional field or a mapping changes, we won’t have to reload everything, we’ll just change the next step. Next, we use Apache Spark or Amazon Athena to perform a simple ETL ( Extract, Transform, Load ) on the data that we want in the data lake. An ETL could be as simple as reading a table from json files, making small changes to the structure, and writing the new table to csv files. The data lake is also stored in S3, but all files are parquet, whereas the raw data could be collected in any format, like json or csv. For our purposes, this ETL stage leaves the data with the same columns as in raw and switches the file format to parquet . Changing files to parquet, a compact columnar format, means that Presto (or, more specifically in our case, Athena) can query them very quickly. An example of breaking down one row of raw data into separate tables and rows Finally, once we know what to put into the data warehouse, we run Redshift SQL queries to perform another ETL which loads the new arrangement into a table in the data warehouse. This ETL stage does things like combine tables, unnest complex column types like arrays or structs, and remove sensitive fields. How do you decide what is going to be in the Data Warehouse? The short answer is we don’t. For the most part, the Data Analytics team decides what should be available and in what format. Sometimes we are given the requirement and then write the code to move it. Sometimes we build the tools that allow a Data Analyst to build and maintain a table on their own. How some of our data sources come together to create the lake and then in turn the warehouse I think I followed most of that… is that what all Data Engineers do? Not all Data Engineering teams will do the same work. Some Data Platform teams focus exclusively on infrastructure, while ETL teams might exclusively handle the process of moving and cleaning data. That sounds interesting; how can I learn more? I can’t say exactly what you’ll need to know, but here’s what helped me feel confident in my role: Take time to learn more about infrastructure. We use Docker to deploy most of the projects that we own, so being able to work with the CLI, read a Dockerfile, and dig into containers has been immensely helpful. I’ve also gotten more comfortable building pipelines in Buildkite , writing Terraform files, and navigating the AWS console. Brush up on your SQL . You might not be writing a ton of raw SQL right now, but as a Data Engineer you’ll likely being writing more complicated SQL and need to be able to understand the queries that other data teammates will be writing. Ask lots of questions, and be fearless. There are still projects that my team owns but that I haven’t worked on a ton. I push myself to pick up tickets in those domains with the knowledge that I can ask my teammates questions, even though it is usually scary to work on new code and admit what I don’t know. Keep looking for more resources .", "date": "2019-08-22"},
{"website": "Gusto", "title": "How ACH works: A developer perspective - Part 2", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-2/", "abstract": "At Gusto , we rely heavily on the ACH network to pay employees and to remit various payroll taxes to federal and state agencies on behalf of our clients. In part 1 of this post, I outlined the basics of how we originate ACH debits and credits. About 95% of the time, things go exactly as planned. This post will explain the other 5% cases (called ACH returns) and how they are handled. Here's a few of the common reasons an ACH file we originate can get returned: Non-sufficient funds (or NSF) -- The bank account we're trying to debit does not have enough monies in it. Invalid account number -- The routing/account number we're trying to credit or debit does not exist (a routing number identifies the Bank to the Federal Reserve, and an account number identifies the customer's account to the Bank) Payment stopped -- The owner of the bank account we're trying to debit/credit told their bank that they did not authorize this transaction and wants it reversed. In all, there are 69 different reasons an ACH file can get returned. I've posted the full list of ACH return codes . How Returns are Handled For the rest of this post, I'll talk specifically about returns on ACH debits (that is, us debiting money from a customer's account). If you want to know how returns on ACH credits work, you need only to switch the word \"debit\" with \"credit\" in this post. If you recall from part 1 of this post, in order to debit a customer's account by $100.00, we need to SFTP an ACH file to our ODFI's servers. Our ODFI will send this file to the Federal Reserve that evening and and will increment our balance by $100. It's important to note that our ODFI has incremented our balance without even knowing if Alice has the funds in our account, or if the account even exists. This is why finding a bank willing to be your ODFI is no easy task -- the bank takes on a small amount of risk by letting you originate ACH transfers through them. When Alice's bank receives the ACH file from the Federal Reserve the following day, they generally have 24 hours after that to tell the Federal Reserve whether or not the payment should be returned. Depending on the return reason, the RDFI may or may not take the full 24 hours to response. For example, if the bank is returning the ACH file because of an invalid account number, they may return the ACH file immediately. If the bank returns the ACH file because of a NSF, the bank may wait for the entire 24 hours to see if their customer deposits money into their account before then. The RDFI notifies the Federal Reserve that they are rejecting an ACH file by uploading an \"ACH Return\" file to the Federal Reserve. The contents of the return file say (1) which part of the ACH file they are rejecting (since an ACH file can contain multiple credits/debits in it) and (2) the reason for rejection (one of the Rxx) codes in the list of ACH return codes ). That evening, our ODFI will check in with the Federal Reserve to see if there are any ACH return files waiting for them. If there are, our ODFI will download the files, and then forward them to us by placing the file in the same SFTP server we originated the ACH file from. At the same time our ODFI will decrement our account by $100 to take back the money they originally credited us for. For record keeping, we'll see a debit of $100 on our bank statement with the description \"ACH Return\". It's our job to to check the SFTP server every day, download and parse all the ACH response files, and update our systems when an ACH file gets returned. For example, at Gusto, if we originated an ACH debit because a company ran payroll, we'll cancel that payroll if the ACH file gets returned. How Change Requests are Handled If ACH returns are the developer's equivalent of an \"error\", ACH change requests are \"warnings\". Sometimes, we will send an ACH file that isn't quite right, but is close enough for the RDFI to understand and process. In this case, the ACH transaction will go through, but the RDFI will upload an \"ACH Change Request\" file to the Federal Reserve, which then gets placed into our SFTP server through our ODFI. The change request file contains the correct information that we should have used in the ACH file. It's our job to check the SFTP server, parse the change request file, and update our system so that the next time we submit an ACH file to the RDFI, it contains the correct information. For example, at Gusto, we'll automatically correct the customer's information in our system and shoot an email to our customer notifying them of the changes that their bank requested. Here's a few of the common reasons an ACH file we originate can result in a change request: Incorrect name of account holder -- We might have used \"Allie\" instead of \"Alice\" as the name of the account we tried to debit. Savings/Checking selection switched -- We said we wanted to debit a checking account, when in fact the account is a savings account. Incorrect account number -- We used the wrong bank account number, but we were close enough that the bank knew which account we actually meant to use (perhaps through a checksum). It's important to note that ACH is not a real-time system. Rather, things are processed in batches at 6:30pm EST, 12:30am EST, and 3:00am EST. As a result, funds can take days to settle. For a payroll company moving more than a billion dollars annually through the ACH network, it's critical that we account for these edge cases correctly and in an automated way. If there's interest in the comments, I'll write a part 3 of this post, detailing these timelines and how we account for them properly. Comments on Hacker News Appendix Provided below are ACH return codes and change codes. ACH Return Codes R00 Manually Cancelled R01 Insufficient Funds R02 Account Closed R03 No Account/Unable to Locate Account R04 Invalid Account Number Structure R05 Unauthorized Debit to Consumer Account Using Corporate SEC Code R06 Return per ODFI's Request R07 Authorization Revoked by Customer R08 Payment Stopped R09 Uncollected Funds R10 Customer Advises Not Authorized, Improper, or Ineligible R11 Check Truncation Entry Return R12 Account Sold to Another DFI R13 Invalid ACH Routing Number R14 Representative Payee Deceased or Unable to Continue in That Capacity R15 Beneficiary or Account Holder Deceased R16 Account Frozen R17 File Record Edit Criteria R18 Improper Effective Entry Date R19 Amount Field Error R20 Non-Transaction Account R21 Invalid Company Identification R22 Invalid Individual ID Number R23 Credit Entry Refused by Receiver R24 Duplicate Entry R25 Addenda Error R26 Mandatory Field Error R27 Trace Number Error R28 Routing Number Check Digit Error R29 Corporate Customer Advises Not Authorized R30 RDFI Not Participant in Check Truncation Program R31 Permissible Return Entry R32 RDFI Non-Settlement R33 Return of XCK Entry R34 Limited Participation DFI R35 Return of Improper Debit Entry R36 Return of Improper Credit Entry R37 Source Document Presented for Payment R38 Stop Payment on Source Document R39 Improper Source Document/Source Document Presented for Payment R40 Return of ENR Entry by Federal Government Agency R41 Invalid Transaction Code R42 Routing Number/Check Digit Error R43 Invalid DFI Account Number R44 Invalid Individual ID Number R45 Invalid Individual Name/Company Name R46 Invalid Representative Payee Indicator R47 Duplicate Enrollment R50 State Law Affecting RCK Acceptance R51 Item related to RCK Entry is Ineligible or RCK Entry is Improper R52 Stop Payment on Item Related to RCK Entry R53 Item and RCK Entry Presented for Payment R61 Misrouted Return R67 Duplicate Return R68 Untimely Return R69 Field Error(s) R70 Permissible Return Entry Not Accepted/Return Not Requested by ODFI R71 Misrouted Dishonored Return R72 Untimely Dishonored Return R73 Timely Original Return R74 Corrected Return R75 Return Not a Duplicate R76 No Errors Found R80 IAT Entry Coding Error R81 Non-Participant in IAT Program R82 Invalid Foreign Receiving DFI Identification R83 Foreign Receiving DFI Unable to Settle R84 Entry Not Processed by Gateway R85 Incorrectly Coded Outbound International Payment ACH Change Codes C01 Incorrect DFI Account Number C02 Incorrect Routing Number C03 Incorrect Routing Number and Incorrect DFI Account Number C04 Incorrect Individual Name/Receiving Company Name C05 Savings/Checking Selection is Wrong C06 Incorrect DFI Account Number and Incorrect Transaction Code C07 Incorrect Routing Number, Incorrect DFI Account Number, and Incorrect Transaction Code C08 Incorrect Receiving DFI Identification C09 Incorrect Individual Identification Number C13 Addenda Format Error C14 Incorrect SEC Code for Outbound International Payment C61 Misrouted Notification of Change C62 Incorrect Trace Number C63 Incorrect Company Identification Number C64 Incorrect Individual Identification Number/Identification Number C65 Incorrectly Formatted Corrected Data C66 Incorrect Discretionary Data C67 Routing Number Not From Original Entry Detail Record C68 DFI Account Number Not From Original Entry Detail Record C69 Incorrect Transaction Code", "date": "2014-05-13"},
{"website": "Gusto", "title": "How to Make Your Pull Requests Reviewer-friendly", "author": ["Casey Rollins"], "link": "https://engineering.gusto.com/how-to-create-a-great-pull-request/", "abstract": "Have you ever been assigned to review a pull request (PR) that looked like this? If so, you probably felt overwhelmed at where to start, uncertain as to what the pull request is about, and unsure of what the author expected from you. Pull requests are much more than a display of code changes. They’re a historical artifact for your codebase and deserve as much time and attention as the code they contain. A pull request is an opportunity to convey what , why , and how a set of changes were made. When an engineer (or you) stumbles upon your code in months or years, they can trace back to the pull request to find more information about a given change. Additionally, a pull request acts as a guide to your code for your reviewer. It gives the author a chance to explain the context of the problem and the background of their solution to the reviewer, and allows them to call out what they’re looking for in a review. If you aren’t sure how to create a great pull request, here are a few tips. 1. Use a template Creating a template for your team can ensure that you’re all on the same page when it comes to what should be documented in a pull request. It promotes consistency and encourages engineers to share their current understanding with teammates and future code-owners. GitHub makes it easy to set a pull request template for an entire repository and has a handy guide explaining how to create one. Some things to consider when creating your team’s template are: How do we ensure quality? What questions will help us in the future when looking back on this PR? Here’s an example template that I use, but you should work with your team and organization to come up with a template that’s right for you. Document the purpose This is where you should describe the what and why of your pull request. What user story is being implemented or what problem is being solved? What value is this change providing? Why is this change being made or feature being added? It’s important to note that the why is significantly more important than the what here. An engineer may be able to deduce what was done by reading the code, but they may never know why a change was made if the reason isn’t documented. For example, the following pull request description doesn’t tell a reviewer much more than the code would: In contrast, this description explains to the reviewer and future engineers why the change was made and why it’s important to preserve the functionality. Explain the changes This is where you should describe the how of your pull request. How was the solution or feature implemented? What new classes, files, or modules were created or modified to implement the story? By explaining your implementation approach and the changes you’ve made, you can save your reviewer the time they would spend reverse engineering your code. Provide additional context Sometimes, the changes in a pull request don’t convey the amount of investigation and effort that went into debugging an issue. Occasionally the solution to a difficult problem is a one line change, but providing additional context and background to explain to your reviewer and to future engineers how and why the one line change fixes the problem will go a long way. Remember that anyone in your organization (or outside of it, if your repository is public) could be reading your pull request. Don't assume that the engineer reading your pull request will be as familiar with the problem as you are. Include screen captures Are you committing a user-facing change? If so, adding a screenshot or a GIF allows your reviewer to see the new functionality and gives them higher confidence that your change works as expected. Additionally, adding screen captures can allow product managers and designers to review and approve the changes as well. Pro-tip: macOS Mojave supports gif capturing natively, but Gifox and GIPHY Capture are powerful tools for creating screen captures as well. Add testing notes At Gusto, we don't have a quality assurance team. Instead, engineers are responsible for ensuring quality in all of their features. As such, it's important that we document how to manually test user-facing features that require complex setup, or how to manually test changes that aren't user-facing at all. Save your reviewer time by clearly documenting testing steps in the pull request description. 2. Review your work After you've opened your pull request and filled out the PR description, it's time to review your work. Pretend you're the reviewer and walk through the changes in each file. If you're using GitHub, you can provide in-line comments to grab your reviewer's attention. You also might catch a mistake or two that should be fixed before you assign a reviewer. You can use in-line comments to proactively address something that looks out of the ordinary. If you think your reviewer will be uncertain of why a certain change was included, point out the reason. You can also use in-line comments to start a conversation with your reviewer, or to ask questions about your approach. 3. Keep pull requests small Finally, keep pull requests at a manageable size by splitting out unrelated changes. If you need to refactor a class in order to add a new feature, then you should develop, review, and land the refactor before working on the feature. First, this makes each PR smaller, decreasing the odds that something will slip through the cracks. Second, this will make it easier for your reviewer to confirm that your refactor is only changing structure and that behavior is unchanged. It can be tempting to land an entire feature at once, but I would encourage you to challenge yourself to break down the work into several small pull requests. Can you add the backend services for a feature in one PR and build the frontend in another? Keep in mind that larger PRs will likely need additional review time and attention due to their size. However, if the size is overwhelming to the reviewer, a large PR is likely to get less time and attention. Conclusion So, with your team, develop a process to help you improve communication and documentation in your pull requests. You might find that a descriptive pull request isn't enough to ensure quality throughout your codebase. This is both the responsibility of the author and the reviewer. And, by the way, providing a meaningful code review is hard too. Stay tuned for a post on this soon. Thank you to my former colleagues and Quan Nguyen for several of the ideas in this post, and to Lindsey Whitley for editing.", "date": "2019-09-23"},
{"website": "Gusto", "title": "Does this code spark joy? Tidying up your code bit by bit.", "author": ["Tori Huang"], "link": "https://engineering.gusto.com/does-this-code-spark-joy/", "abstract": "Lifestyle experts want to bring simplicity to people’s lives through the magic of tidying up. People all over the world claim this method has brought happiness, and organization, to their cluttered lives. If only there was a Life-Changing Magic of Tidying Up for developers! Whether you built or inherited it, most engineers have owned disorganized code at some point in their career. This messy code can result in bugs, prolonged development time, or increased difficulty with onboarding new engineers. Tidying is the solution. Tidying does not mean a huge clean, spending hours upon hours working on a big mess all at once. Tidying means finding small areas of disorganization you improve by cleaning things up bit by bit. Tidying, in the context of code, results in purely structural changes to a confusing and messy section of code. The main motivation for tidying is a human one. As humans, we like things to be organized . We like carefully named functions and helper methods to break down complex problems into smaller, digestible pieces. Well organized code can be especially beneficial to engineers jumping in to a new codebase or in preparation for a bigger refactor. At Gusto, our resident tidying expert, Kent Beck, has been encouraging engineers to start spending small amounts of time tidying up old code. I decided to give it a try, so I paired with Kent to tidy up our revenue share process. He walked me through some key strategies he takes while tidying up code; here they are, along with examples of how they changed our Gusto code. Tidying means structural (not behavioral) changes When tidying it is important to differentiate between structural and behavioral changes. Behavioral change means changing the way the code operates. Changing the return value of a function is an example of a behavioral change. Structural change, on the other hand, does not change how the code operates. Structural change rearranges and/or reorganizes the code. Separating functions into helper methods is an example of a structural change. As we encountered during our pairing session, it can get a little tricky identifying behavioral vs structural changes. We looked at a function that creates PartnerRevenueSharePayouts from an invoice. Upon investigation, we found the return value of the method - partner_id - was not used anywhere except for tests. Not a good reason for a return value! We removed the call to partner_id at the end of this function. At first glance, this update could be categorized as a behavioral change, however it was not an observable behavioral change. Therefore, it is a structural change. Blank spaces are usually a good indicator for helper methods When tidying, we want to separate our code out into the smallest methods possible. Divide your program into methods that perform one identifiable task. Keep all of the operations in a method at the same level of abstraction. This will naturally result in programs with many small methods, each a few lines long. - Kent Beck We add blank spaces between blocks of code to separate sections that are accomplishing different tasks. Blank spaces are a great way to identify areas of code that can be separated into helper methods. Helper methods also give us the opportunity to communicate! Developers can define the purpose of their code through carefully named methods. Take the untidied code on the left. At first glance, you may be able to translate some of the code’s purpose - a payout is created if the partner is related to the given partner_id . With the tidied helper method solution on the right, that context is given for free. We are building partner revenue share payouts. The partner revenue share payout is created unless the revenue share is owned by the partner. Helper methods not only keep things tidy, they help us communicate as well! Always keep tests green Tests should pass at all times when making changes. What does this actually mean for us as we code? Keep changes small . Re-run tests with every change. If you make a change and it does not work, do not keep pushing forward. Revert your change and try again. Committing frequently, and only on green builds, will help keep your work organized. Purposefully named commits help keep track of changes. If you make a mistake, you can always revert to the last commit and find all green tests. Failing tests are stressful. Remembering what you changed between the failing tests and the last passing tests is stressful (and difficult). The goal is to make things as stress-free and easy as possible. Small changes, frequent tests. If you cannot simplify, go for symmetry Sometimes it is not possible to pare things down with purely structural changes. For example, say you have two methods that accomplish similar tasks but use different records. It is not possible to refactor these methods into one without changing the behavior; however, you know there is potential to refactor in the future. When you encounter this situation, the goal is to make these similar methods as symmetrical as possible. The more identical the methods, the easier it will be to refactor into one method later. At first glance, the untidied code above appears to be creating revenue share payouts twice. Upon further investigation, Kent and I determined we are actually creating two different types of revenue share, payouts for accounting firms and payouts for software partners. This code is difficult to refactor. The BuildRevSharePayoutsService is a complex service with many different outcomes. Instead of attempting to refactor something this complex, we can break up these two completely different functions into purposefully named helper methods. Now it is clear that we are creating two different types of revenue share payouts. By making these methods as symmetrical as possible, we make things easier to refactor for future engineers. Take breaks This is common programming advice, but I still find myself needing the reminder. Kent and I were stuck on a difficult problem for about 20 minutes while tidying together. After a 5 minute break, we regrouped and figured out the solution almost immediately. Do not waste time banging your head against the wall. Tidying up is tiring work and a little reset can clear things up. Go forth and tidy! Tidying is an easy way to tackle messy code. Before embarking on a large refactor, onboarding new engineers, or simply for your own sanity, tidying up your code can help bring organization back to your working hours. Small steps towards a tidy codebase really add up. Hopefully these tips and tricks I learned from Kent were helpful to you. Good luck on your tidying endeavours!", "date": "2019-09-09"},
{"website": "Gusto", "title": "Solving the double-click in Backbone.js", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/solving-the-double-click-in-backbone-js/", "abstract": "“No! No! You can't triple stamp a double stamp! You can't triple stamp a double stamp, Lloyd!” -Harry from Dumb and Dumber A common problem in Backbone.js apps comes from users furiously double clicking submit buttons, resulting in multiple Ajax requests being submitted to the server. Underscore.js has a debounce function that can be used to rate-limit multiple Ajax requests. A common approach is to wrap every click event that results in an Ajax request with the debounce function. Unfortunately, this was not an ideal solution for us because of the following: We have a very large Backbone.js app (perhaps one of the largest out there!), and combing through every single Ajax event would be tedious and not DRY . Plus, developers would have to remember to properly debounce any new submit buttons they add to the app. We could solve (1) by extending the behavior of Backbone.Model and wrap the save function with Underscore's debounce. However, because of the way debounce was implemented, the save function would be rate-limited in the global score, not the instance scope. That is, if 2 different models called save at nearly the same time, the second model's save might be ignored, even though it's a different model. We ended up solving this problem with a third approach: extending the behavior of Backbone.Model and wrap the save function with our own version of debounce that properly scopes to each instance. Here's the code in CoffeeScript : class DebounceModel extends Backbone.Model\n\n  _debounceTime: 100\n  _canSave: true\n\n  save: ->\n    if @_canSave\n      @_canSave = false\n      setTimeout((=> @_canSave = true), @_debounceTime)\n      super(arguments...) All of our Backbone.js models then inherit from DebounceModel and are automatically protected from user's sending multiple Ajax requests due to double-clicks. Problem solved!", "date": "2014-04-02"},
{"website": "Gusto", "title": "Building a Testing Culture", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/building-a-testing-culture/", "abstract": "Here at ZenPayroll, writing software that pays tens of thousands of employees across the country inspires us as engineers to write bullet-proof code. As a team, we are committed to putting in the extra time to make our features as robust as possible and ensuring that the code we write today will be resilient to the changes and refactors of tomorrow. Our greatest ally in this fight is testing, and its a core tenant of how we write software at ZenPayroll. Fully testing our features has always been an important part of our development process, and as we have grown as a team this has become even more critical to our workflow. To ensure our code-quality remains up to par, every proposed feature, refactor, and bug fix is submitted with a full test suite, and we all hold each other accountable for this in the code review process. This has been a saving grace for us in a variety of situations, ensuring that payroll is always delivered where and when it's supposed to and preventing bugs from sneaking their way into production. Writing Specs We write tests everyday at ZenPayroll, and seeing them go green definitely gets us excited. While we don't always TDD our code, we utilize it when it's useful and always aim for 100% test coverage. Our attitude towards testing stems from thinking about what may change in the future, and enumerating how the code should work now and why , through our specs. It's a excellent reminder mechanism for us and helps to \"future proof\" our code as we move on to new features and occassionally forget precisely why something was done. We've hit a stride with our testing that has allowed our development process to flow smoothly, and we'd like to share the tools and approaches we've taken to get there. Hopefully, these tools can be added to your arsenal on the quest towards 100% code coverage! Unit Tests We start by writing unit tests to ensure our models are storing and acting on data the way we expect them to. RSpec is our weapon of choice for clean Ruby testing and we focus on writing specs that stress specific components, rather than flows, to keep our unit tests clear, fast, and descriptive of how the code is intended to behave. As new engineers join, this is a great place to understand how a particular piece of code is used in practice and this is only possible with readable tests - we strive to maintain easy to understand specs, even in complex situations. When dealing with changing tax laws, often very specific actions need to be taken to ensure that we remain compliant with the US government. We shoulder this responsibility for thousands of employers across the country and take their trust very seriously. Tests help us know that we are continuing to calculate and file for them correctly. When developing payroll features, we write specs that capture tax calculations and verify their validity. This makes sure that we are alerted whenever rates change or our tax calculations return unexpected results - nothing gets an engineer's attention like failing tests! JavaScript and UI Testing We have a thick single page app that is designed to give our customers peace of mind that their information is correctly and safely stored, and that their tax filings are in good hands. Ensuring data is correctly displayed and passed to the server from the frontend is just as important to us as keeping our Rails models well tested. We serve our test assets using Konacha and use the Poltergeist PhantomJS driver, which raises exceptions if any JavaScript errors are thrown - an excellent extra check when testing our JS. Mocha then powers all of our JavaScript testing, which includes our routers, models, collections, and view logic. With a full suite of frontend specs, we can feel confident when pushing new changes that our UI responds properly. But how do we test views with data that resembles production? In order to keep our test data in tune with our server-side updates, we have a fixture generator controller spec , which hits GET controller actions and generates JSON fixtures from their responses. We then feed these JSON fixtures into our Backbone models when setting up our test views. This runs once before our test suite; it helps us test our RABL layer and ensures that our frontend models are being populated with what they would have if they directly synced with our server. Integrations Tests Finally, we write integration specs as a general overview of long flows in our application. We rely on these type of tests the least, but they are helpful in making sure that our app is loading correctly and checking from a high level that everything is in place. We use Capybara driven by PhantomJS , as we have found it provides the most consistency and speed when run in CI. We also have Capybara Screenshot setup to capture what the screen looked like at the time of error to help us track down the failures. Integration specs have also allowed us to automate checking that our setup guidelines are accurate. The tax setup required for every state is unique, so ensuring that our tooltips are up to date with changing requirements can be a pain to do manually. We setup custom request specs that capture images of all our guidance messages for every state during tax setup and sends them to product and compliance team. This helps them make sure our application's guidelines are always accurate without wasting time going to check all of these flows manually. Automated testing, for the win! Continuous Integration Continuous integration is a crucial piece of our development process. Every commit that is pushed to a code review, and later merged into our development branch, triggers a build on our CI server. It's the first thing we look at when reviewing code, and links directly to every commit that touches the codebase. This can lead to a lot of tests running and it is critical that our builds run quickly and don't get backed up. The Way it Used to Be A year ago, all of our tests ran on a beefy Jenkins machine in our closet. We'd have a few code reviews a day, the full test suite would finish in 15 minutes, and life was good. Reviews would get merged into the development branch, the suite would be run, and we were free to proceed with deploys! However, as we started to grow as a team and as a product, so did our build time. Before not too long the test suite was taking an hour, builds were getting backed up, and our responses from Jenkins became fewer and farther between. Sometimes it would take half a day to hear back on the status of a review. Our builds on the development branch started to fall behind, flaky tests started to emerge, and sometimes deploys were going out without a response from CI. Gasp! Something had to change. Going for Gold We knew we had to get back to our roots and find a build system that was fast, reliable, and supported enough concurrent builds to keep our development cycles running smoothly. We tried a variety of services but almost always found something that wasn't wellsuited to our needs, whether it was due to setup difficulty, poor parallelization, frustrating UI, or limited support. After evaluating a variety of options, we ultimately decided on Solano CI and have seen massive productivity boosts since our switch. Solano has impressive performance and parallelization options directly out of the box, which piqued our interest. Combined with a clear UI, comprehensive dependency support, easy setup, total customizability, and a command line interface, it was a perfect fit for us. They also offer unique advanced features such as build profiles, which allowed us to break out more expensive tests into a separate, periodic suite. To top it off, their support is incredibly knowledgeable and responsive, they have worked closely with us on fine-tuning our build setup for performance and stability, reducing our build time from over an hour to under 10 minutes. Back to testing heaven! Wrap Up Today, our commands to merge a code review into development will automatically reject a change that hasn't yet passed CI. Of course, our deploys are all dependent on successful CI status linked to the commits being pushed to production. We've left the days of uneasy deploys behind and are confident that we're covered on any change we make - a feeling that we are committed to preserving. Testing will always be integral to how we build new features at ZenPayroll and will continue to help us build resilient software at a fast pace, but it is just one of the many layers that help us produce high quality code. We use testing as a core piece of a development process that includes code reviews, occasionally pairing, keeping clean data , and smoke testing features on staging, in order to produce technology that we can be proud of. If testing automation and developing software in this manner is important to you too, we would love to hear from you ! Keep on testing, folks.", "date": "2015-02-11"},
{"website": "Gusto", "title": "How to build a service-driven engineering team", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-to-build-a-service-driven-engineering-team/", "abstract": "How do you transform your engineering culture? It starts by finding the thing that stitches you together. For our team, that bond comes from a culture of helping others. Over the years, Silicon Valley has churned out programmers who think about everything but that. Since engineers are generally hard to hire, companies have had to go to extreme lengths to bring talented people on board and prevent them from slipping away. But somewhere along the way, things got wonky. In a certain tech company’s San Francisco office, the engineers are the only team that’s served dinner. Inequities like these stack up, and over time, changes how people think. It makes engineers less about serving others, and more about serving themselves. So when we started building our engineering team, we explicitly said we’re going to do things differently. At Gusto, our engineers are a group of ( shoeless ) people who came together over a simple idea. When someone needs a hand, every single person on the team is more than willing to slide off their headphones, pull up a chair, and do whatever is possible to help them out. Our teams are focused and accountable for their goals, but at the same time, have a constant thirst for helping others. There are no fiefdoms or territories here, and successes are always celebrated together. In the end, that culture is what our engineering team is all about. These are a few of the ways we’re working on making that service ethos front and center at Gusto. Look for the helpers We aren’t programmers because we like shiny new things. We’re programmers because we want to make people’s lives better. If you go through the recruiting process at other companies, one of the popular ways they sell candidates is by telling tell them they’ll get to work on tough technical problems and then they’ll feel super awesome once they solve them. Here, we’re kind of the opposite. Yes, we definitely have hard problems to solve — ahem, 15 billion dollars goes through our payroll system each year — but that’s not our main selling point. People who thrive here get a thrill from solving impactful people problems. You have to shift your mindset a little to get that feeling, but once you do, it allows remarkable stuff to happen. So how do you find folks who jive with this idea? Look for people who get energized by helping people. One of the easiest ways to do this is by asking interviewees what excites them. Their answer to that question is a good indicator of where they find satisfaction. Do they just want to solve hard, technical challenges or do they actually want to help people? If they focus too much on the languages and technology stacks, and not on the people part, that’s very telling. It’s also important to leave room for the candidate to ask questions during the interview. It sounds like a no-brainer, but you can learn a lot about people when you just let them talk. Give back at every step The extent to which software can make painful things painless still astounds me everyday. Many of us think about that in terms of just helping customers, but it’s also about helping each other out as teammates. Yes, our engineers plug away on product features, but they also work on tools that our customers never see. For example, engineers will sometimes purposely sit next to an operations team to observe how they’re going about their day. If they see a task they’re doing over and over again, someone will pipe up and say, “Hey, if I write a program, this could save you a couple hours every week. Would that help you?” And often their response is a wholehearted, “yes.” We call this idea “not being the priestly class” — service, not entitlement, is our purpose. That service mindset is the whole reason we’re here. It’s also important for us to give back to the community at large. The team has written some pretty cool stuff, so we'll try to open source any code that we think others might find helpful. Doing this has a ripple effect — it allows us to sharpen our code, and get the word out about our mission. When we learn something new, we also make sure to write about it on our engineering blog so others can benefit from our experience. Another way we give back is by mentoring others in the tech community, through programs like Hackbright Academy , a coding school for women, and Level the Coding Field , a weekend hackathon for 8th and 9th graders. Each program allows us to help underrepresented groups get excited about engineering, and it means a lot to our team (and we hope for the participants too). This all ties back to our belief that you have to do good, give back, and ultimately, do what’s right . No matter what we’re building, we make sure every line of code has two subjects in mind: the people we work with and the people we serve. They’re the heroes and heroines of the the programs we write. And when you instill that kind of spirit into your team, it somehow lights up everything you do. Interested in learning more about the Gusto engineering team? Check out our careers page today.", "date": "2016-02-04"},
{"website": "Gusto", "title": "Update on Gusto's Engineering Diversity Goals", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/update-on-gustos-engineering-diversity-goals/", "abstract": "Back in September, the engineers at Gusto (formerly ZenPayroll) made a public commitment to increase diversity within our team . As a first step, we set a goal to raise the number of full-time female engineers from 11% to 18% — the same number of female computer science majors in this country. We promised that we’d report back in January 2016 with our results, so here we are today. I’m happy to share that as a result of our efforts, we reached (and surpassed!) our initial 18% target. Our engineering team now includes 21% full-time female engineers. Here are our current gender numbers at Gusto: In the above chart, \"engineering\" is defined as full-time female software engineers, so that number excludes interns and contractors. \"Tech\" is defined as our engineers, technical product managers, and designers. We’re not prioritizing diversity just to make ourselves feel good — we’re doing this because diversity helps us build better software. More perspectives allow us to see things we never saw before, and we can make better decisions as a result. Even if there were zero benefit to us, we’d still be doing this because we believe it’s our role to make the world a little better. By bringing more diversity to the engineering field, we hope more women will study computer science and end up with positions at great tech companies. That’s our hope, and we’re going to continue to do whatever we can to work toward that. Top three things we learned from our diversity recruiting efforts In just over four months, we doubled the ratio of female to male engineers. Achieving this milestone was no easy task. It took a lot of introspection and hard work. In our previous post , I outlined the strategy we would take to increase our female headcount. Now that we have four months of data under our belt, I can finally share a few of the things we’ve learned along the way: 1) Sometimes the most radical ideas are the most effective In the fall, we made an extreme decision to outbound source 100% female candidates until we hit our target — and we stuck with it. The idea was pretty crazy in the beginning, but the impact was huge. To other companies looking to bring more gender diversity to their teams, I highly recommend this strategy to help you kickstart your efforts. 2) Start recruiting early for Grace Hopper We went all-in at the Grace Hopper Celebration of Women in Computing , and it was so worth it. To stand out, we started our recruiting efforts way before the actual conference began. As a corporate sponsor, we were able to recruit more actively by taking advantage of their book of resumes. Starting two months prior, we conducted as many phone interviews as humanly possible. We screened 2,000+ resumes and interviewed nearly 100 candidates. Our team was blown away by the stellar female candidates we met and the competition that’s emerging as a result of the conference. *We invited a few of the Grace Hopper attendees to our AirBnB and cooked breakfast for them!* 3) Hold yourself accountable by making your goal public We didn’t just set our diversity goal internally — we talked about it publicly to make sure others could keep us accountable. A number of the female candidates we hired since September told us that the main reason they applied to Gusto was because of our blog posts. Sharing our inclusive engineering culture with the world has dramatically helped us reach candidates that share our values . If you don’t have a blog, set one up, so you can have a platform for sharing the things you and your team believe in. What’s next? Building an engineering team that values diversity is about more than just hitting hiring numbers. It’s about making that belief a part of your culture. While we’re proud of the work we've done so far, we also realize that our journey has just begun. We’re working with our People team to help us strategize ways to increase diversity across other dimensions — such as ethnic diversity — throughout 2016. We’ll keep you posted on our progress right here on the blog. Thank you for following along over the past few months. Our team has a long way to go in terms of creating a more gender-balanced team, but we’re taking the steps to get there. And over time, our hope is that all of these small steps will add up to something really meaningful in the end.", "date": "2016-01-14"},
{"website": "Gusto", "title": "Evolving JavaScript Part 1: Goodbye Backbone", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/evolving-javascript-part-1-goodbye-backbone/", "abstract": "Note: This is the first post in a multi-part series about the evolution of our Javascript application over time. Here are parts 2 and parts 3 . The advancements in the JavaScript community over the last few years have been staggering. With the release of powerful new tools, languages, and frameworks, many frontend development teams have suddenly found that much better technologies are available to them. Here at Gusto, we took advantage of this and made a switch from Backbone to React . We have been writing React at Gusto for over a year now. Our development teams are building all new features using React while simultaneously converting existing Backbone views as updates are needed. Along with this, we've done a complete overhaul of our frontend system, abandoning CoffeeScript in favor of ES6/7 , using static analysis tools like ESLint heavily, utilizing our own flavor of the Flux architecture, and adopting build tools like Webpack to ease the burden on the Rails asset pipeline. This will be the first entry in a multipart series about building user experiences at Gusto, detailing (1) our migration from Backbone to React and Flux and (2) our move from the Rails Asset Pipeline to Webpack. In our first three installments, we'll be focusing specifically on transitioning from Backbone to React, breaking our experience down into three sections: (1) How we used to build UI with Backbone (and the ways it was deficient for us) , (2) why we decided to migrate to React , and (3) a tutorial giving an inside look at how we write React today . Why Our Stack Used to Make Sense When development on the Gusto product began in January 2011, we chose the best tools available to us at the time: opting for a JS stack consisting of CoffeeScript, Backbone.js, and Handlebars , all served by the Rails Asset Pipeline . CoffeeScript was an obvious choice coming from a Ruby background - not only did it abstract away many of the overly verbose expressions in JavaScript (and provided class support!), it was shipped by default with Rails. Backbone.js was all the rage just a few years back and provided everything we needed for a thick, well modularized client-side app -- everything from the API layer to populate the data model all the way to client side routing and events. We tested our code using Mocha , Chai , and Sinon , and utilized Teaspoon as our Rails-based test harness -- allowing us integrate our frontend specs with our Rails-generated JSON fixtures. Ultimately, we were able write well-tested frontend code  and abstract away our most common UI paradigms using inheritance, leaving us with a system that, at the time, felt productive and robust. These tools served us well for many years, enabling us to build rich user experiences in true single-page fashion. But as our team began to grow and the prospect of building UI-heavy systems like workers compensation and health insurance around the corner, we started to want more from our frontend architecture. Breaking up with Backbone We first felt the limitations of our frontend system when we were ramping up our state expansion efforts and pushing to be a nationwide payroll provider. Every state's setup is relatively similar in nature, but each one has a variety of specific UI needs. In order to build the 50 states in an efficient manner, we had our backend State Builder DSL dynamically drive the frontend views. This was the first time we were trying to implement powerful patterns that extended well beyond the common use cases. When building a proof of concept in Backbone, we found it was painful for a variety of reasons: Non-Trivial Composability Backbone is notorious for leaving references to \"zombie views\" around due to the lack of a built-in mechanism for cleaning up event handlers, so libraries like Coccyx became necessary to help deal with these issues. In addition to needing to be diligent about cleaning up our subviews, we found that occasionally our top level views looked something like this: class ShowView extends Backbone.View\n  template: HandlebarsTemplates['employees/show']\n  className: 'employee-show-view'\n\n  initialize: (options) ->\n    @company = options.company\n\n  ...\n\n  appendSection: (viewClass, options) ->\n    view = new viewClass(_(model: @model).extend(options))\n    @registerSubView(view)\n    @$('#employee-sections').append(view.render().el)\n    view\n\n  render: ->\n    @tearDownSubViews()\n    @$el.html(@template(employee: @model.toJSON()))\n\n    @appendSection(AddressesView)\n    @appendSection(SplittableShowView)\n    @appendSection(CompensationsView, company: @company)\n    @appendSection(FederalTaxesView)\n    @appendSection(SpecialExemptionView, company: @company)\n    @appendStateTaxViews()\n    @appendDismissView()\n    @appendRehireView()\n    @appendFormsView()\n    @appendPaystubView() Just to determine what our view would look like, we had to reference both the template and the view to see where on the DOM the subview was being mounted and how it was being attached. Ideally, we want to be able to take a quick scan of a file and understand exactly how it was using subcomponents, as well as what it looks like when rendered. Events Triggering Manual DOM Updates A common pattern in Backbone is to listen to a variety of DOM events (click, change, etc) and when fired, manually update the DOM using jQuery to hide and show different elements. As a payroll system, we often deal with complex data entry, and we capture this information using heavily interactive forms. One example of this is our payment method form, which asks for different things based on whether you need to be paid by check or by direct deposit. class PaymentDetailsFormView extends Backbone.View\n  template: HandlebarsTemplates['contractors/payment_details_form']\n  className: 'contractor-payment-details-form-view'\n\n  events:\n    'change .payment-method': 'updatePaymentMethod'\n    'keyup .routing-field': 'updateBankName'\n\n  initialize: (options) ->\n    @company = options.company\n\n  ...\n\n  updatePaymentMethod: ->\n    paymentMethod = @$('.payment-method').val()\n\n    if paymentMethod is ENVIRONMENT.PAYMENT_METHOD_CHECK\n      @$('#pay-by-check').removeClass('hide')\n      @$('#pay-by-direct-deposit').addClass('hide')\n    else if paymentMethod is ENVIRONMENT.PAYMENT_METHOD_DIRECT_DEPOSIT\n      @$('#pay-by-check').addClass('hide')\n      @$('#pay-by-direct-deposit').removeClass('hide')\n    else\n      @$('#pay-by-direct-deposit, #pay-by-check').addClass('hide')\n\n  updateBankName: ->\n    routingNumber = @$('.routing-field').val().replace(/_+/, '')\n\n    if routingNumber.length == 9\n      $.ajax(\n        type: 'GET'\n        url: API_PREFIX + '/banks/' + routingNumber\n      ).done((response) =>\n        if response['name']?\n          @$('#bank-lookup-name p').text(response['name'])\n          @$('#bank-lookup-name p').addClass('name-shown')\n        else\n          @$('#bank-lookup-name p').text('')\n          @$('#bank-lookup-name p').removeClass('name-shown')\n      )\n    else\n      @$('#bank-lookup-name p').text('')\n      @$('#bank-lookup-name p').removeClass('name-shown') Although this ultimately accomplished our product goals, the code is hard to make sense of. We've added many extraneous classes and identifiers simply to locate elements on the page. We have several event handlers working simultaneously, and it's difficult to reason about what the UI will look like based on a given state. We started to question what would happen based on the order of events and how these interactions played together. We started to realize that the DOM was our only real representation of application state, and inherently difficult to use as a source of truth in our code. Inefficient DOM Manipulation Another common Backbone-ism is to listen to changes on a model or collection and accordingly re-render the view in response to those changes. class NavView extends Backbone.View\n  className: 'nav-menu'\n\n  initialize: (options) ->\n    @user = options.user\n    @company = options.company\n\n    @listenTo(@company.companyMigration, 'change', @render)\n\n  ... While this allows our Backbone views to be a little more declarative and gives us some loose data binding, we are essentially tearing the entire view off the DOM, re-rendering, and putting it back onto the DOM. Not only have we just blown away what is effectively our application state, large scale DOM manipulation is the heaviest thing your client side app can do. Backbone provides few options for optimizing this, aside from handling the manual manipulations ourselves, which we have shown to be difficult to maintain as soon as multiple handlers get involved. So, what do we do? We knew we needed a change when we felt our productivity was beginning to suffer. As our views evolved over the years, some of the more complex flows became unmaintainable. Development speeds had slowed down immensely, frustration began to grow knowing that we lacked sufficient tooling for our system, and we knew there were better options out there. The next post in this series, titled Evolving JavaScript Part 2: Hello React , outlines our decision to move to React and the challenges we faced during the migration. We hope you'll read on! Comments on Hacker News", "date": "2016-07-12"},
{"website": "Gusto", "title": "Gusto Tech Talk: Scaling Front-end Architecture", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/gusto-tech-talk-scaling-front-end-architecture/", "abstract": "We recently hosted our first tech talk at Gusto! One of our engineers, Nikhil Mathew (pictured above), spoke about the journey Gusto went on to scale our massive front-end application. In particular, he covered our decision to move from Backbone.js to React.js , and from the Rails Asset Pipeline to Webpack . The talk ended with an awesome live coding session, demonstrating how Gusto does front-end development. Nikhil is going to publish a written form of his talk here on this blog shortly, but in the meantime, check out the video and slides from his talk: Download the slides in pdf form. Over the years, Gusto has grown to help more than 30,000 businesses with their payroll, benefits, and HR. Underlying this software are really innovative software design patterns, tools, and architecture. We'd love to share more of our learnings with the broader technical community. Going forward, we're committed to giving tech talks at least on a quarterly basis (and hopefully more frequently!) Hope to see you at our next tech talk!", "date": "2016-06-10"},
{"website": "Gusto", "title": "Evolving JavaScript Part 2: Hello React", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/evolving-javascript-part-2-hello-react/", "abstract": "In the previous post , we detailed the reasoning that led to us moving away from Backbone. In this installment, we'll highlight how we framed the search that led us to React and subsequently dive into a React tutorial of how we write React components at Gusto. Optimizing for your own use case When evaluating upgrade options, you should think carefully about what your application's most important use cases are and how you can choose a technology that optimizes for them. For us, we optimized for 3 things: Incremental Upgrade Path We had built out a large, monolithic frontend application and we had to be pragmatic in how we approached the upgrade path. An entire rewrite of our frontend application was not an option. We needed a framework without a high entry cost that would allow us to migrate our application incrementally. This would enable us to continue to build a steady stream of features for our customers during the transition from Backbone to React. Reuse of Common Components Having already gone through a rebrand and a layout update in the past year alone, we understand the value of consolidating markup and reusing common components across an application. By having a single source of truth for all UI building blocks in the application, design updates become much easier to roll out and we can feel confident about the effect across the application. Ease of Creating Interactive Forms As a payroll, benefits, and HR company, we have more data entry and interactive forms than most other web applications. Whether it's onboarding an employee, running payroll, or electing new health insurance plans, we are more often collecting information from our users rather than providing views to consume data. As a result, we opted for a framework that optimizes for the view layer and allows us to write reactive forms declaratively. Enter React.js Picking reliable and battle-tested technologies is critical when dealing with people's payroll and benefits, and React first caught our eye with its heavy usage at Facebook and Instagram. Encouraged by the success stories of React's usage in production, we built out a proof of concept in our application and fell in love almost immediately. These are a few of our favorite principles behind the React paradigm, and we will be going in depth with examples of how we use them in our tutorial. Declarative UI React has a data binding system that consists of internal state and props . state can be updated in response to user events with a call to setState , while props are arguments passed to a component to allow for easy reuse. Because state and prop changes trigger immediate re-renderings of the component, we get native data binding that allows us to write components that are declarative in nature and will automatically update based on changes to these properties. That means no more manual DOM updates and complicated if/else logic to determine what to change, but rather a clear definition of how our component will look based on a given state. Optimized DOM Updates React's shadow DOM was a huge development for the JS community, proven by its widespread adoption by frameworks like Ember and Angular . React has an internal representation of what is on the DOM and whenever changes are made to the data model, a new DOM will be recomputed in memory (fast) and will do the minimum possible operations to update the DOM to the new state. Encapsulated/Composable React ships with a PropType validation system that not only documents the arguments that your components take, but also throws warnings when components are used improperly. This allows us to write components that are reusable and is especially useful for a set of components that we call \"elements\", which constitute the building blocks of our UI. These include things like buttons, forms, inputs, modals, and other reusable parts of our system. Combined with React's JSX templating language for composing these components, building common UI paradigms becomes as simple as writing markup, removing the need to manually wire up components through event listeners. Testable The best React components are pure functions that will always render the same output based on a set of inputs. Because of their functional nature, we can rest assured that something outside of the component will never reach in and mutate the component unexpectedly, and therefore can write declarative, readable tests for how our UI should behave based on a certain set of properties. Interfaces well with our Design Team We have a rockstar design team here at Gusto, and it has been a pleasure to work with them on our growing UI. They own our elements and make our jobs as engineers easy by providing all the basic building blocks we need to build features without the need to write custom markup. By being able to reach for reusable components that are flexible and styled, we are able to quickly build experiences that are on brand, functional, and beautiful. Immediate Productivity Gains Because React is only the view layer of your application, it leaves the routing and data management up to your discretion. We had already built out a robust Backbone-based data model that was well integrated with our RESTful JSON APIs, and we hoped to reuse this with a new system. We were able to accomplish this by abstracting our Backbone models behind Flux stores, and making our components agnostic of the underlying data layer -- providing us with an easily replaceable layer to interface with Backbone. Integrating React into our Application Not only does React allow for our Backbone data model to be easily consumed, but it can be rendered by Backbone views and routers. This was pivotal in allowing us to begin writing React immediately and convert Backbone views as needed rather than all at once. We are still using Backbone routers to handle our client side URLs and we have simply extended them to handle React components as well. class BenefitsRouter extends Backbone.Router\n  initialize: (options) ->\n    @company = options.company\n\n  routes:\n    'benefits': 'benefits'\n  ...\n\n  benefits: (highlight) ->\n    view = React.createElement(BenefitsPage)\n    AppView.showView(view, 'company-benefits') Our AppView is our application container that handles displaying whatever view is in focus. We use the showView function to animate between different pages and we have modified it to mount both Backbone and React views accordingly. Although we try to write full views in React, occasionally it is necessary to render React from Backbone views. class AddressView extends Backbone.View\n  template: HandlebarsTemplates['addresses/address']\n  className: 'address-address-view'\n  mixins: [ReactSubComponent]\n\n  render: ->\n    @unmountSubComponents()\n    @$el.html(@template(@context()))\n    @renderAddressExtensionView()\n    return @\n  ...\n\n  renderAddressExtensionView: ->\n    view = React.createElement(AddressExtension, { addressId: @model.id })\n    @renderSubComponent(view, @$('.address-extension-view').get(0)) We are provided the renderSubComponent method from a small mixin we wrote that hooks into Coccyx so that React components are properly torn down with the view. Notice here how we don't pass the Backbone model into the component, but rather just an identifier so the component can retrieve the data itself from the store. This will guarantee that if we need to remove Backbone in the future, our React components will be using a store interface that we can refactor behind. Helping Hands While React itself is an incredible library for building UI, we couldn't have gotten the massive productivity boosts in our frontend systems without some complementary tools. Immutable.js Immutable.js is another great library released by Facebook, and allows us to be confident that the objects we operate on are not mutated unexpectedly. For example, if you were to update the state object in a React component without using the setState method, the React lifecycle would be broken as it would not be able to hook into the changes. Sometimes, when mutable objects are shared, changes can happen in far off regions of the application that are hard to track down. After being bit by this issue one too many times, we decided to adopt Immutable for all of our complex objects used in our components. Immutable is a functional library that provides powerful methods for updating nested objects and always returns a new copy of the updated object. The guaranteed immutability allows to more simply reason about our data used by our components. This also allows to confidently use the PureRenderMixin , because even though this optimization only looks at shallow equality to determine if it needs to update a portion of the tree, the presence of a different immutable object would guarantee a change has occurred. Enzyme Enzyme is a testing library that was developed by AirBnB to make testing React components intuitive. Coming from a Backbone background, where jQuery is often the library used for asserting the DOM was constructed properly, the React test utils feel clunky and confusing by comparison. Enzyme provides an API that is familiar to us (a flexible find method that can assert classes, tags, and React components alike) and also gives us easy options for shallow, full, and static rendering. Flux , Babel , ESLint , Webpack With the use of these tools for data management, higher level language support, static analysis, and highly optimized bundling, we have been able to make the most of everything React has to offer and provide a developer experience on the frontend we previously thought to be impossible. We will take a deep dive later into our tooling and how it has taken our development experience to the next level. We’re thrilled with our new setup and hope this article demonstrates a few of the strengths of React and the pain points that it solves for us. Now that we’ve covered the reasons for moving off of Backbone and onto React, we’d like to demonstrate actually building a React application. The next post in this series, titled Evolving JavaScript Part 3: Building a React Application , does just that. Comments on Hacker News", "date": "2016-07-12"},
{"website": "Gusto", "title": "Evolving JavaScript Part 3: Building a React Application", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/evolving-javascript-part-3-building-a-react-application/", "abstract": "In the previous posts, we covered why we moved away from Backbone and started building new features with React . To further reinforce why we invested in our migration from Backbone to React, we've built out a tutorial that demonstrates some of the UI principles we follow here at Gusto. We'll be creating an employee directory application together utilizing common React patterns, and below you'll find a link to a GitHub repo that will allow you to follow along. https://github.com/Gusto/gusto-react-tutorial App setup To streamline the tutorial and focus on building React components, this app has been setup with a variety of tools to get you up and running. Webpack Loaders are provided to parse ES6 (and a handful of ES7 properties), JSX, CSS, and images. The ES6 module system is used for importing and exporting code. Hot loading for the React components has been configured. The webpack-dev-server is used for maximum development speed. ESLint We've adopted AirBnB's style guide (with a handful of modifications). I highly recommend you configure your editor to live lint if you have not already. Karma, Mocha, Chai, Sinon, Enzyme Karma is setup as our test runner and you can run the whole test suite using npm test . You can also start a test server that will run as you change files with npm run test-server (recommended). Node API A small API serving a list of employees can be found at localhost:8080/api/employees . The API will start up along the webpack-dev-server when running npm start . Getting Started The repo that was provided was designed to follow along with this tutorial and includes everything you need to build the React application. The master branch will ultimately represent the finished product that we will build, so start by running git checkout step-0 to see the initial setup and follow along with the post! Provisioning Ensure that you have the latest version of node and npm installed. You can install these view Homebrew with brew install node . Install the dependent JS packages with npm install . Start the server with npm start and you should see a \"Hello, World!\" screen. If you haven't already, download the React DevTools for a powerful in-browser debugger that will further illustrate the concepts we will be covering. Repo Overview All of the code we will be modifying for the scope of this tutorial will live in the build/js directory. In here you will notice an index.js file that will import and start the app, as well as a clients and components directory. Clients These JavaScript classes provide a small abstraction for interfacing with our API. The build/js/clients/employees_client.js file contains an EmployeesClient class for interfacing with the server as well as an Immutable Employee record that defines the schema we expect for employee data. EmployeesClient exposes a method called all which will fetch all employees from the API, wrap the result in a collection of Immutable records, and return a promise to handle the response from within the component. The Employee constructor utilizes Immutable's record functionality, which enforces that the structure of the data matches what we would expect. This is powerful because it allows us to create a single definition for what an employee should look like and validate that the structure is maintained throughout the app as well as ensure that any instances of this data will never be mutated. Elements Similar to what was described in our own app, these \"element\" React components will represent a few small building blocks of the application that we can use for abstracting away common functionality. You will notice things like a Page and a Loader component and you can treat these as parts of the user experience that are flexible enough for use across an application. If we were to expand on the functionality we build today, these could be utilized to create a consistent experience as well as provide a single place to update these building blocks if they were to change. Displaying Employee Data To get started building our employee directory, the first thing we will do is fetch some data about our employees and display it on the DOM. In order to do that, we'll create what we call a container component for fetching data. This article details a common pattern for abstracting data management away from display logic while using React. The author quotes: A container does data fetching and then renders its corresponding sub-component. That’s it. This pattern allows for components to be more reusable, since they are not tied to retrieving the data. We will write a very thin layer that handles interfacing with the server and passing the data as props to a component that we want to render it. This will allow that component to receive the data from anywhere and still be able to render it accordingly. Let's start by creating a component called the DirectoryContainer that will use our EmployeesClient helper to fetch the data from the server and provide us a collection of Immutable Employee records to render onto the DOM. While the data is being fetched from the server, let's use the Loader component to give the user some feedback that their data is on the way, and when the data is loaded, let's temporarily print the number of employees to see that the API request was successful. build/js/components/employees/directory_container.jsx import React, { Component } from 'react';\nimport EmployeesClient from 'js/clients/employees_client';\n\nimport Loader from '../elements/loader';\n\nexport default class DirectoryContainer extends Component {\n  state = { loading: true, employees: [] };\n\n  componentWillMount() {\n    EmployeesClient.all().then((employees) => {\n      this.setState({ loading: false, employees });\n    });\n  }\n\n  render() {\n    return (\n      <Loader loading={this.state.loading}>\n        {this.state.employees.length}\n      </Loader>\n    );\n  }\n} To see this in action, let's modify our App component to display this DirectoryContainer and wrap it in a Page component to ensure it is styled properly. build/js/components/app.jsx import React, { Component } from 'react';\n\nimport Page from './elements/page';\nimport DirectoryContainer from './employees/directory_container';\n\nexport default class App extends Component {\n  render() {\n    return (\n      <Page>\n        <DirectoryContainer />\n      </Page>\n    );\n  }\n} Now that our component is mounted in the App , you should see a loader appear and then display number in the top left of the page when the request is complete. Nice! In the DirectoryContainer , we start by defining an initial state using the ES7 class properties syntax. This state definition will get evaluated in the constructor during instantiation time and will serve as the initial value of our component's state before our fetching is complete. Notice how we defined an empty array as the default value for the employees key in the state of the component - this will allow the component to render without errors before the data is fetched by preventing calling length of undefined ( this.state.employees ) in the render method. By looking at the Loader class, we can quickly see from the propTypes that the component takes a loading boolean and children - it will show a spinner if the loading prop is true and display the children if it is false. Accordingly, we define our DirectoryContainer component to be in the loading state by default and pass our content as children to the Loader so that it will display them when loading is complete. We then hook into our DirectoryContainer 's componentWillMount lifecycle method, allowing us to execute an API call before the component is mounted. Once we get a response from the server, we set the employees into the state of the component and set loading to false, which will trigger a re-render, hide the loader, and show the number of employees. If you open up the React DevTools and search for the DirectoryContainer component, you'll notice that in the state there is an array of Immutable records that represent our employees. They have a few attributes, first_name , last_name , and email (defined in build/js/clients/employees_client ), so let's create a component that will be able to render this array of employees and display them each individually as a part of a list. This will be our Directory component and it will get it's data from the DirectoryContainer . build/js/components/employees/directory.jsx import React, { Component, PropTypes } from 'react';\nimport { Employee } from 'js/clients/employees_client';\n\nexport default class Directory extends Component {\n  static propTypes = {\n    employees: PropTypes.arrayOf(PropTypes.instanceOf(Employee)).isRequired\n  }\n\n  _renderEmployee(employee) {\n    return (\n      <li key={employee.email}>\n        <div><strong>First Name:</strong> {employee.first_name}</div>\n        <div><strong>Last Name:</strong> {employee.last_name}</div>\n        <div><strong>Email:</strong> {employee.email}</div>\n      </li>\n    );\n  }\n\n  render() {\n    return (\n      <ul>\n        {this.props.employees.map(this._renderEmployee)}\n      </ul>\n    );\n  }\n} And then of course, let's switch out our number display to instead render our new directory of employees. build/js/components/employees/directory_container.jsx import React, { Component } from 'react';\n...\nimport Directory from './directory';\n\nexport default class DirectoryContainer extends Component {\n  ...\n  render() {\n    return (\n      <Loader loading={this.state.loading}>\n        <Directory employees={this.state.employees} />\n      </Loader>\n    );\n  }\n} Now, you should be able to see a list of employees within the app! Since our render function is just plain ol' JavaScript, we simply map over the list of employees and return a JSX element for each employee in the list. You'll notice we added a key prop to each li - this is part of how React does it's internal diff'ing. It needs an unique identifier for each sibling of the same type in the tree so it can properly resolve what needs to be updated. Using an index is an anti-pattern here, so always provide a unique value that can be used by React to optimize DOM updates. In our Directory component, notice how we were able to strictly define not only that we should receive an employees prop but that it should be an instance of the Employee Immutable record we described in the beginning. This is really powerful, not only have we designed a composable component that can be used in a variety of contexts, but we actually validate that it gives us all the data we need to render the employee and ensure that our component will render as expected. Huzzah! Testing Components Testing is a crucial part of our development at Gusto, and our frontend code is no different. The introduction of React has made testing more enjoyable as we write our code and the fast test execution makes for great feedback when developing. The repo has a full test suite included with it, but we will highlight just a few examples here to demonstrate the point. Background React ships with its own set of Test Utilities , but we as team have found these clunky and difficult to use. We have opted to use AirBnB's Enzyme which will allow us to use a more intuitive API for asserting component correctness. Also look out for the different types of rendering in order to optimally mount the component based on each test's needs. If you open up a new terminal and run npm test , you'll notice that we have broken our App test because it no longer greets the world! Let's update this spec to fix it. Pro tip: from this point on, use npm run test-server to watch for file changes and automatically have the test suite run on update. spec/components/app_spec.jsx import React from 'react';\n\nimport App from 'js/components/app';\nimport { shallow } from 'enzyme';\n\nimport DirectoryContainer from 'js/components/employees/directory_container';\nimport Page from 'js/components/elements/page';\n\ndescribe('App', function() {\n  beforeEach(function() {\n    this.spec.component = shallow(<App />);\n  });\n\n  it('renders a Page', function() {\n    expect(this.spec.component.find(Page)).to.have.length(1);\n  });\n\n  it('renders the DirectoryContainer within the Page', function() {\n    expect(this.spec.component.find(Page).find(DirectoryContainer)).to.have.length(1);\n  });\n}); Here, we will very simply assert that we have rendered a Page component with a DirectoryContainer inside. Notice here that we will use the shallow render method provided to use by Enzyme - meaning that the DirectoryContainer will not actually be mounted and attempt to fetch employees, but instead will render a stub for the sake of the assertions. Next let's test that our DirectoryContainer component starts in the proper state and accordingly shows the Loader before rendering the Directory . spec/components/employees/directory_container_spec.jsx import React from 'react';\n\nimport DirectoryContainer from 'js/components/employees/directory_container';\nimport { mount } from 'enzyme';\n\nimport Loader from 'js/components/elements/loader';\nimport Directory from 'js/components/employees/directory';\nimport EmployeesClient, { Employee } from 'js/clients/employees_client';\n\ndescribe('DirectoryContainer', function() {\n  context('before the API request is complete', function() {\n    beforeEach(function() {\n      this.spec.promise = new Promise(function() {});\n      this.spec.sandbox.stub(EmployeesClient, 'all').returns(this.spec.promise);\n      this.spec.component = mount(<DirectoryContainer />);\n    });\n\n    it('starts in the loading state with no employees', function() {\n      expect(this.spec.component.state('loading')).to.be.true;\n      expect(this.spec.component.state('employees')).to.have.length(0);\n    });\n\n    it('passes loading=true to the Loader', function() {\n      expect(this.spec.component.find(Loader).prop('loading')).to.be.true;\n      expect(this.spec.component.find(Directory)).to.have.length(0);\n    });\n  });\n}); We use Sinon to stub our EmployeesClient method with a promise that we can control from the test ( this.spec.sandbox is setup in spec_helper.js - it simply cleans up all Sinon stubs after each test). By doing so, we have the component in a loading state and can test that the state is passed down to the Loader and the Directory is accordingly not yet rendered. Finally, let's test that when the API request is complete, it passes the data down to the Directory . spec/components/employees/directory_container_spec.jsx import React from 'react';\n...\ndescribe('DirectoryContainer', function() {\n  ...\n  context('when the API request is complete', function() {\n    beforeEach(function() {\n      this.spec.employees = [\n        new Employee({ first_name: 'Bob', last_name: 'Bobberson', email: 'bob@bob.com' })\n      ];\n      this.spec.promise = Promise.resolve(this.spec.employees);\n      this.spec.sandbox.stub(EmployeesClient, 'all').returns(this.spec.promise);\n      this.spec.component = mount(<DirectoryContainer />);\n    });\n\n    it('is no longer in the loading state', function() {\n      this.spec.promise.then(() => {\n        expect(this.spec.component.state('loading')).to.be.false;\n        expect(this.spec.component.state('employees')).to.have.length(1);\n      });\n    });\n\n    it('renders the Directory with the employees', function() {\n      this.spec.promise.then(() => {\n        expect(this.spec.component.find(Directory)).to.have.length(1);\n        expect(this.spec.component.find(Directory).prop('employees')).to.eql(this.spec.employees);\n      });\n    });\n  });\n}); In order to simulate the API request being complete, we resolve the promise and assert that the callback was successfully invoked - stopping the loader and setting the employees into state. We have to wrap our assertions in the promise to ensure the execution order is correct and that the assertions will be made after the component has updated. Finally, we test that we properly render the Directory and give the right props. Adding Search Capabilities Now that we have built out the basic directory functionality, let's see how we can extend this by adding a search bar to filter the employees. We will filter out the records on the frontend to simplify the example, but this can easily be modified to create an AJAX driven search. You can get all the code needed to complete this portion of the tutorial with git checkout step-1 . Based on the way we have encapsulated our components using the container pattern, it should be simple to filter down the employees being displayed using a higher-order component . Let's create an abstraction on the Directory called the SearchableDirectory that will filter employees based on the search term. build/js/components/employees/searchable_directory.jsx import React, { Component, PropTypes } from 'react';\nimport { Employee } from 'js/clients/employees_client';\n\nimport TextInput from '../elements/text_input.jsx';\nimport Directory from './directory';\n\nexport default class SearchableDirectory extends Component {\n  static propTypes = {\n    employees: PropTypes.arrayOf(PropTypes.instanceOf(Employee)).isRequired\n  }\n\n  state = { search: '' };\n\n  _onSearch = (event) => {\n    this.setState({ search: event.target.value });\n  }\n\n  _matchesSearch(attribute) {\n    return attribute.toLowerCase().indexOf(this.state.search.toLowerCase()) >= 0;\n  }\n\n  _filteredEmployees() {\n    if (!this.state.search) { return this.props.employees; }\n\n    return this.props.employees.filter((employee) =>\n      this._matchesSearch(employee.first_name) ||\n        this._matchesSearch(employee.last_name) ||\n        this._matchesSearch(employee.email)\n    );\n  }\n\n  render() {\n    return (\n      <div>\n        <TextInput value={this.state.search} onChange={this._onSearch} />\n        <Directory employees={this._filteredEmployees()} />\n      </div>\n    );\n  }\n} Finally, let's drop in SearchableDirectory instead of the Directory in the DirectoryContainer . build/js/components/employees/directory_container.jsx import React, { Component } from 'react';\n...\nimport SearchableDirectory from './searchable_directory';\n\nexport default class DirectoryContainer extends Component {\n  ...\n  render() {\n    return (\n      <Loader loading={this.state.loading}>\n        <SearchableDirectory employees={this.state.employees} />\n      </Loader>\n    );\n  }\n} And our search is up and working! Notice that because we used the container pattern to encapsulate the directory, it was trivial to add a higher level component to filter out the employees before the directory rendered them. This kind of reusability and maintainability is where React really shines. In the SearchableDirectory component, we start by defining our propTypes (an array of Employee records), and an initial search state of '' . We create a _onSearch handler and assign it using the fat arrow to ensure the function is bound to the right context and can access the state of the SearchableDirectory , even when passed down to the TextInput . We have created a controlled component that will update the state and receive its new value to display with each keystroke. This method of data binding makes things like live search really easy, because the component will now update with each letter that has been typed. Since we know that this.state.search will be updated whenever needed, we wrote a _filteredEmployees function that will return only the employees that match the search criteria. The result is passed to the Directory and they are displayed as needed, live updating with every keystroke! Conclusion Together we were able to build a simple application to fetch data from an API, show the results on the DOM, and test it end to end with little friction. More importantly, we were able to easily refactor and add functionality to search through the employee without having to change any of the components we had written originally, a testament to how well we encapsulated our code. Our components made use of the state and prop data binding for creating declarative views that live updated in response to user input. We used Immutable's record functionality for defining a data schema and enforcing the structure through the app with prop types. We utilized shallow rendering in our tests where applicable and Enzyme's wrapper API for easy to read tests and assertions. Ultimately, we were able to easily build a small but well written application, made possible by the ecosystem that React has created and how it has changed how we think about frontend systems. Here at Gusto, React has not only changed the way we think about writing views but has shaped how we build user experiences altogether. The switch from Backbone from React has paid great dividends for our team and we continue to work towards refining our patterns and tooling. React is phenomenal library for building UI. Combined with tools like Webpack and patterns like Flux, it only becomes even more powerful. Webpack and Flux each warrant their own deep dive, and as promised, we will be publishing more content on how we moved off the Rails Asset Pipeline in favor of Webpack (reducing our development time significantly) as well as how we have used the Flux architecture to bridge the gap between Backbone and our server. We recently hosted a tech talk (thanks for all who attended, looking forward to seeing you at the next one!), covering several of these concepts and the journey we took to get there. You can see the video here and we will be continuing to host these events in the future detailing our technical endeavors. If you enjoy writing React code and building robust, extensible UI's, we want to hear from you! For anyone considering taking the dive and moving to React -- I highly encourage giving it a try. As mentioned before, if you feel React can help provide the structure you need to solve your application's most important problems like we did, the effort will be well worth it. We've invested significantly in our migration towards modern JavaScript and we've never looked back -- creating a developer experience that we felt was worthy of our engineering team and robust enough to provide a top notch user experience. Happy coding folks! Comments on Hacker News", "date": "2016-07-12"},
{"website": "Gusto", "title": "Interning at Gusto, and reflections on how I got here", "author": ["Jesse Zhou"], "link": "https://engineering.gusto.com/interning-at-gusto-and-reflections-on-how-i-got-here/", "abstract": "My first job ever was at a local running store in my hometown. I was a fresh high school graduate looking to make some money before I went off to college, and I remember spending a couple of weeks trying to find a job with no luck. One day, while skateboarding around my area, I happened to see a “Help Wanted” sign in the window of this running store. Not having anything to lose, I went in and asked if I could help out for the summer. Even though I never had a job before, the owner of this store took a chance and said I could start the next week. I was so excited. I couldn’t believe it, it was my first real job where I actually could earn money for myself! I remember telling my mom about this opportunity I got, and I remember her hugging me and saying, \"Congratulations on finally being an adult!\" I felt an amazing sense of independence that I never felt in my life. I was ready to learn everything I could, and more ready to give it my all. At the running store My daily routine at the store was more or less the same every day. Every morning, I would open new shipments of shoes and other running peripherals, and restock the shelves. Whenever a customer came in, I would try to help them by recommending particular shoes that would hopefully solve whatever foot problem they had. In the afternoon, when things were a little slower, I cleaned the store and tried to learn how to code on the computer when there was nothing else to do. One day’s worth of shoe shipments! I was there eight to nine hours a day most days of the week. I learned about foot issues like bunions and hammertoes, and how to pick the right shoes that would optimize the comfort for each customer. I learned about the advantages of different brands and how each shoe felt on my foot. I learned how to use the cash register, and I learned how to talk to many different kinds of people, from security guards whose feet were cramped and exhausted, to software engineers, who were preparing for marathons and had no idea what to do. One interesting thing I found while working at this store was how many different hats I had to wear as a regular employee. Sometimes, the owner of the store (and my boss) needed help with some software if it was acting funny, and sometimes, she needed someone to design a poster for a sale that was happening in the near future. For such tasks, she usually relied on me to pull through. So I wasn’t just someone selling shoes, I was someone who was helping my boss keep the business growing. Knowing this and helping members in my local community have healthy feet brought me a lot of personal satisfaction, something that I didn’t get from studying all the time in high school. It also taught me a lot about personal responsibility and giving something you believe in your heart and soul, just like my boss did with her store. Summer 2013. Selling a pair of shoes to one of my high school friends. At Gusto Fast forwarding to 2016, I was lucky enough this summer, right before my 4th year of college, to get a software engineering internship here at Gusto. It took a lot of studying and perseverance to get this far, so just like how I felt in my first job, I was prepared to learn as much as I could while I was here. I quickly found that the great thing about working here was that everyone prided themselves in helping small businesses that were risking it all. Everyone here believes in some way that giving back to the community is important. Coming from the running store so many years ago, this really resonated with me. Small businesses not only help their communities, but influence their cultures and people in profound ways. They also give some people like me their first jobs, and teach responsibilities and people skills that can’t be learned anywhere else. It was great that Gusto focused not only on small businesses, but each local community they resided in as well. This idea that community is everything is carved deep into Gusto’s culture. We have lunches and dinner where everyone — from sales to product, from care to engineering — eats together and talks about various topics that have nothing to do with our professional lives. In a way, it was refreshing to get different perspectives from a diverse number of employees, all who were very different but still shared what the founders call our core values. What I love about how they explained our core values is that everyone going into Gusto has these values already: they aren’t learned or taught. Everyone here believes that we are here to make things better, and it makes working here a joy every day. What’s amazing about this sense of community is that it stretches beyond just San Francisco. The interns were lucky enough to have the opportunity to check out the offices in Denver, Colorado and even hundreds of miles away, Gusto always feels like one team and one family. To maintain such a high quality of consistency and camaraderie is rare to find in a company these days; one of my fellow interns even said that when it comes to culture, Gusto definitely got it 100 percent right. Summer 2016. Gusto Interns in Denver with CEO Josh Reeves. Sometimes, while working at Gusto, I would take a breather and feel some nostalgia for my job at the running store. In one of these nostalgic moments, I would think about how I used to have to wear different hats at the store to help keep the business going and growing. It just so happens that almost every Gusto engineer does the same exact thing in an engineering context. At Gusto, we believe that every engineer is a full-stack engineer. I remember first getting here and being almost overwhelmed by the sheer magnitude of technologies that made our application possible, from React to Backbone to Ruby on Rails, to even testing frameworks like Mocha and RSpec (since engineers here test their own code instead of relying on a QA team). And that’s just a very small subset of everything! I remember thinking that there was no way in hell that anyone here knew the stack inside and out. Each framework and technology was different, so how could someone even go about mastering them all? Yet, after pair programming with many different full-time engineers over the period of a few weeks, I was amazed to see how many engineers could immediately identify a problem and explain to me how to go about solving it. It didn’t matter if it was a bug on the client side or the backend, or if it was a confusing test to write, or anything else like that. Some people just knew the code base so well that they immediately knew where to look whenever I hit a roadblock and had a question for them. San Francisco. My mentor this summer, Rylan Collins: wearer of many hats (also looks like Thor). Sometimes, problems were a little harder to solve, and required more than just a single person to power through the issue. This is when I noticed how much engineers at Gusto love helping each other out. No matter what an engineer was doing, if someone asked him or her a question, they would stop what they were doing and spend time trying to solve the issue together. The amount of trust shared among the engineers here brings a quality of work and passion that empowered me to always try my best and keep working, even if it was a problem I didn’t exactly know how to do on the top of my head. Looking back at it all Like at the running store, there were some days at Gusto in which I had to put in longer hours, sometimes late into the night after everybody left. But the great thing was that it never really felt like I was working, but rather learning how to do things and use certain technologies for myself with the helpful guidance of other people. Seeing other people work hard and knowing that what I was doing was helping people had a weird feedback loop effect on me. I kept pushing myself to learn more because I began to believe in the product so much. I started to work #withGusto, as people around here like to say. The work I did here ended up being incredibly meaningful to me and satisfying, especially because of my past experiences at that running store. The fact that I was helping people like my old boss focus more time on their businesses rather than on taxes or forms was always on the back of my mind. The team I worked on this summer, HR Engineering. Great people and even greater friends There are times when I will look back and be thankful for what I learned in the running store, because working there humbled me and helped me become a better person, and ultimately a better prospective engineer. I will sometimes look back and also realize how far I’ve come, from learning HTML and JavaScript through Codecademy in the backroom of that running store, to now being able to contribute to a great startup in the heart of San Francisco. Working at that store also showed me that every opportunity, no matter how small and seasonal it is, can have an immense rippling effect on not only your own development but the community around you. I was actually able to talk about my experiences at the running store while interviewing for Gusto, and how it helped shape my mentality today. I realized that being able to look back at this journey helped me appreciate this internship at Gusto so much more. Interning at Gusto also reinforced for me the message that each small business is a world of opportunity and a textbook full of lessons for everyone, from the employees themselves to the customers who choose to support and explore a local business over a larger vendor. I still go back to that store. In fact, it’s the only place in the world where I will buy my running shoes, purely because the owner was the only one that took a chance on an 18 year old who didn’t know the first thing about having a job. I actually found out that my old boss now uses Gusto too, which makes my whole journey a little poetic in some way. I’m so glad that I had that first job because I learned so much from having that opportunity, and working at Gusto made me feel like I was paying it forward. Gusto’s mission is to put people first over everything else. We believe that people aren’t resources, and that people are the center of every successful endeavor, especially businesses. We believe that every employee matters, and that stuff like Payroll, Benefits, and HR should be easy for everyone. Just like how small businesses put an incredible investment into improving the culture of their communities, Gusto puts an investment into helping the people at these businesses do more by letting us take care of the hard stuff, and making work a joy rather than a pain. I’m convinced that by letting these businesses and their people do the best work of their lives, Gusto is doing what it can to make the world a little better, one small community at a time. My last day at work. Go #withGusto!", "date": "2016-09-07"},
{"website": "Gusto", "title": "Thinking about Team Structure", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/thinking-about-team-structure/", "abstract": "(cross post from Into Thin Air ) Distributed teams aren’t new to software engineering. It’s safe to say that those who have been in the industry for a while have worked with people who aren’t physically located with us. In my experience, the situation has nearly always been where most of the engineers are in one physical location, with a few scattered around the world. Out of sight, out of mind. I was in charge of a few teams that had this hub-and-spoke setup a couple of years ago when we started thinking about optimizing collaboration. We realized team collaboration is most efficient when everyone was in the same physical location. Our objective, then, was to get as close to physical presence as possible. Microsoft did research into something they called Embodied Social Proxies a few years back, where they concluded that having a dedicated, always-on machine for video chat not only included the remote engineer in tacit conversation, but caused others to collaborate more with him. So, our remote engineers became “floating heads” on a monitor. Embodied social proxies work to counteract the “out of sight, out of mind” tendency we have as humans. And in the last few years, new companies have surfaced offering formal solutions to this problem. Gusto has a Double Robotics telepresence robot that we’ll be bringing online soon, for example. With technology options like embodied social proxies (always-on video chat) and Slack, we know we can get remote engineers to collaborate effectively with their teams. So when we think about team structure, one approach is to simply attach Denver engineers to those in San Francisco so that all teams have both remote and physically present engineers. Engineering teams in San Francisco are established with deep knowledge of our codebase. New Denver-based engineers would ramp up quickly working with them. But something about that option feels off. Pods, man. I’ve had a lot of conversations with engineering and product leaders here at Gusto about team structure. One line of thinking predominates — we should endeavor to create autonomous development teams in Denver. On the surface that feels right, but why? In The future is podular , David Gray writes: If you want an adaptive company, you will need to unleash the creative forces in your organization, so people have the freedom to deliver value to customers and respond to their needs more dynamically. One way to do this is by enabling small, autonomous units that can act and react quickly and easily, without fear of disrupting other business activities — pods. Imagine a world where autonomous pods existed in Denver, just as they do in San Francisco. Imagine a world where Denver-based development teams could make and execute on important, revenue-impacting decisions in isolation. Imagine a world where we could point to Denver and say something like “our mobile apps are developed there.” That’s the world we want to create. It’s aspirational, and it’s efficient. Getting there. I’m still the one and only engineer in Denver (although we’ve been doing a lot of interviewing). While my job is to help build out our engineering teams here, I also need to be fluent in our codebase. Unlike other companies, where engineering leaders act like baseball managers watching their players field grounders and hit doubles, ours are like cricket captains — they take to the field and still play the game well. Code fluency creates pragmatic managers. So, I’m working on features in our codebase just like everyone else, and am attached to our Growth team in San Francisco as per aforementioned hub-and-spoke setup. But as more engineers start with us here in Denver, we’ll pull a pod out of “Growth” and spin it up here. We’ll learn. And, sometime in 2017, we’ll start spinning up more pods after we’ve learned from the Growth guinea pig. Giddy up.", "date": "2016-11-18"},
{"website": "Gusto", "title": "This is how we Gusto: Our Development Workflow", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/this-is-how-we-zenpayroll-our-development-workflow/", "abstract": "A lot of engineers looking to join Gusto are interested in hearing more about our development workflow. Put more simply, how do Gusties write software? The answer to that is constantly changing, as we’re always refining our workflows to fit our ever changing needs, but I’ll cover in some detail the way we do things today. Before going into the nitty-gritty, it’s important to understand our high-level engineering principles: Push high-quality software. This is payroll after all. The code we write is ultimately used to move hundreds of millions of dollars and pay thousands of people. There’s no room for error. Don’t optimize for the short-term. This applies to more than just the engineering team. It’s a company-wide value that every person in the company — engineer or otherwise — lives by. We’re building our company to last 50 years or more, and we want our code to last just as long. Keep it simple and straightforward. Lots of moving parts in a system make it brittle. We try to keep our processes as simple as possible, but no simpler. With those principles in mind, here’s an overview of how a feature goes from an idea to production here at Gusto. 1)    Spec Everything we build starts with a spec. Sometimes the spec can be as simple as a few lines of text explaining what the feature will do (common for small tweaks). Other times, the spec can be full-blown balsamiq mockup. The spec for a feature can come from 3 places: The product team, the engineering team, or the support/compliance team. Generally, specs for large user-facing features come from our product team, as they have a great sense of the important new features that our customers need. Specs for back-end systems generally come from the engineers themselves — things like fraud-monitoring systems, payroll calculation systems, and systems to automate things like faxing forms to the IRS. Finally, our support team oftentimes come up with specs on tools they need to serve more customers and serve them better. Wherever the spec comes from, a developer is intimately involved in its development by providing feedback, suggestions, and a sense of engineering tradeoffs. 2)    Branch off development We use git for source code management. Once a spec is finalized and ready to be implemented, we will “git checkout -b new_feature_branch_name” off of our development branch and start laying down some code. 3)    Coding/Testing Coding commences! Since code-quality is very important to us, we strive for 100% test coverage in everything we write. This applies to both our back-end and front-end. We use RSpec for testing our backend payroll API ( Ruby on Rails ) and Mocha for the gusto.com app, which is a large single-page application ( Coffeescript on Backbone.js ). In addition to unit and functional tests, we also write integration tests using Capybara . 4)    Pairing (optional) We generally don’t pair at Gusto, but there are times when it’s useful to work on a feature together, especially if some part of the feature is particularly complex or requires intimate knowledge of another person’s code. For this reason, we have a couple dedicated pairing stations set up around the office. 5)    Code review Once development on a branch is complete, we then send the branch for code review to another engineer. We use an awesome tool called Phabricator to do this. Usually, the code reviewer is someone who tangentially has some knowledge of the part of the code that the feature is built upon. Code reviews are immensely beneficial to us. Most importantly, is a great way for an engineer to get suggestions on how to better improve and refactor the branch. It also roots out any potential bugs that may have been overlooked. Another benefit is that code reviews help distribute knowledge of the codebase to other engineers. Finally, a lot of learning of better programming techniques occurs during reviews, improving the skills of everyone on the team. As iron sharpens iron, so we sharpen each other. Usually there are a few iterations of reviews until the reviewing engineer is satisfied and accepts the branch. It’s also worth mentioning that we code review almost everything, as we’ve found that even seemingly simple one-line changes can have huge unintended consequences. 6)    Rebase development Once a code review is accepted, the feature branch is rebased on the development branch (“git rebase development”). After any conflicts are resolved, the feature branch is merged into development (“git checkout development; git merge feature_branch_name”). The nice thing about using Phabricator is that this rebase/merge combo can be executed with a simple “arc land” command. 7) Leeroy – Run test suite Whenever a branch is merged into development, we have a really beefy test server that runs our entire test-suite (back-end, front-end, and integration tests) on the development branch. The name of this test server is Leeroy, who does all this with the help of Jenkins . We also have a sweet team dashboard displayed on a TV in the office that lets us know if tests fail. Another useful thing Leeroy does is locally run any new model validations on our (anonymized) production data to make sure the new code plays nicely with real data. If every single test passes, we will proceed to the next step. 8)    Deploy to staging Jenkins automatically deploys our development branch to a staging environment. Our staging environment is meant to almost exactly mirror our production environment. Under the hood, Jenkins is really just issuing a “ cap staging deploy” command. When our development branch is successfully deployed, Jenkins lets us know via email. 9)    Sanity testing on staging Our feature branch is close to being on production! Someone on the product team will usually play with the feature on our staging environment trying their best to break things. 10)  Merge to master and deploy to production If things look good on staging, we’ll merge our development branch into our master branch and then deploy to gusto.com using a simple “cap production deploy” command. Hooray! Our feature is now live! Generally, we’re deploying to production several times a week. We try to keep our features small and will often break large features into smaller chunks to keep them more manageable. There’s nothing worse than trying to code review huge branches. We’re always looking for ways to improve our development workflow, so if you have any suggestions or tips of what was effective for you, please let us know! P.S. Gusto is hiring ! If you’re a talented engineer looking to work with smart, passionate people on a product that will change the world, please reach out to us!", "date": "2013-11-14"},
{"website": "Gusto", "title": "Do Things That Scale: Engineering a Modern Business", "author": ["Gusto"], "link": "https://engineering.gusto.com/do-things-that-scale-engineering-a-modern-business/", "abstract": "As applications grow, their size and complexity erode away at their efficiency. As engineers, we know that there are two basic ways to achieve scalability: money and architecture. Money We can brute force the problem by throwing dollars at it. App cannot handle enough requests? Buy more servers. It’s taking up too much memory? Resize! It’s the short-term, easy solution, but it isn’t pretty. While your CFO shudders at the soaring operational costs, your engineers watch in horror as hundreds of thousands of lines of unmanageable code pile on top of their once quaint application. Architecture The other, more sustainable solution is distributing both the load and business logic across multiple applications. Each handles a defined subset of the problem and optimizes accordingly. Adopting Service Oriented Architecture means that each individual piece is more focused, and the system as a whole is more efficient as a result. Instead of building one application to do everything ‘okay,’ build multiple applications that solve one problem as well as possible. Amazon CTO Werner Vogels revealed that constructing their homepage resulted in over 100 service requests . And that was in 2006. Asked about adopting an SOA at Amazon: It has been a major learning experience, but we have now reached a point where it has become one of our main strategic advantages. We can now build very complex applications out of primitive services that are by themselves relatively simple. We can scale our operation independently, maintain unparalleled system availability, and introduce new services quickly without the need for massive reconfiguration. We believe that what is true of strong engineering practices is also true on the business side. When building Gusto, we specifically set out to solve one problem. It’s right there in our name. But, like a highly distributed architecture, we knew that everything we built could make a much bigger impact if it could communicate and work well with other back-office software. Traditional business software has been the equivalent of a MonoRail : overcrowded, inefficient, brittle, and sluggish. But we know that payroll is just one distinct piece of a multi-faceted system. Our mission is not only to build a complete payroll solution that’s modern and delightful, but also to actively develop an API to communicate with every related business tool. For example, imagine you’re managing a large sales force. At the end of every month, you review your team’s performance in your sales CRM. You calculate who hit or exceeded their goals, note that in your performance tracking application, and then give them a bonus on Gusto. That’s far too many steps. But what if your applications could talk to each other? Your sales CRM automatically pushes monthly data into your performance tracking application, which seamlessly adds bonuses to your employee’s next paycheck. The future of the back office is defining what you’d like to be done and feeling confident that your tools will handle the execution. The individual business owner should be able to use us with any other software they like, whether that’s a time and attendance solution or even a custom-built tool. We’re taking what we know about designing scalable web applications and applying them to all the tools required to run a business.", "date": "2013-11-27"},
{"website": "Gusto", "title": "Keeping Your Data Clean", "author": ["Nick Gervasi"], "link": "https://engineering.gusto.com/keeping-your-data-clean/", "abstract": "Active Record validations are an integral part of any Rails application, and with good reason. Maintaining a comprehensive set of validations guarantees that only valid and consistent data will be written to your database. That’s great, but what about data that is already in your database? While validations ensure that your data is valid when written, they do not guarantee that it will stay that way. So how often does valid data suddenly become invalid? More often than you may think. Here are just a few examples of how it can happen: You or another developer on your team adds new validations but forgets to properly migrate existing data. Your application updates records using methods that skip validations (e.g. update_attribute, update_column, or save with :validate => false). You use uniqueness validations and have multiple app processes running. Two processes try to insert a record at the same time, the uniqueness validation passes in the both processes, and you end up inserting two records instead of one. So you might have some invalid data, so what? Why should you care? Here are two reasons: 1. Having a single failing validation will block the whole record from being updated. This means your users may see errors for fields they aren’t even editing. This can be even more confusing if the field failing validation is an internal field and isn’t even on the form. {<1>} Fig. 1 - How to frustrate your users 2. Validations are like assertions; they represent assumptions that your application is making about the underlying data. Having invalid data means those assumptions are no longer true which means your code may start behaving unexpectedly. One of the greatest benefits we’ve found in keeping our data clean is that it helps unroot bugs that we otherwise might not have seen. When we find a validation error and start digging into it we often end up finding a significant bug in the code caused by some obscure, yet valid user edge-case. Our Solution - Introducing BrokenRecord Ok, so you’re convinced that keeping your data valid is important. Here’s what we do at Gusto to make sure our data stays valid, and it has helped us time and time again to catch bugs before our customers do. The principle is very simple: run validations on all records in an automated job and alert the dev team when something fails. There were a few design requirements however: The job should run on production data, but should not run on production servers. We didn’t want to tie up our production CPU’s or worse, potentially alter the data as a side effect of a validation or callback. While the job should use production data, it should use the source code from our dev branch. This helps us catch bugs before they’re deployed. Triggering the job on code changes is not good enough since data can become invalid without any code changes. Therefore, we scheduled the job to run every three hours. We needed to skip some specific validations. For example, we have some validations that lock a record from being updated. Calling valid? on one of these records would return false even though the committed data is consistent. We wanted to skip some models entirely. For example, models that were introduced by third-party gems such as paper_trail. We wanted the job to run quickly, so we leveraged the parallel gem to run validations in parallel. We addressed the first three requirements by setting up a scheduled job on Leeroy (our Jenkins CI server). To satisfy the last three requirements we built a simple rake task and we’ve been running it internally for several months. Running validations periodically has been so helpful to us that we’ve decided to open-source it.We packaged the rake task into a gem and named it BrokenRecord . The source code is available on GitHub . To use BrokenRecord, simply add this to your Gemfile and run bundle install: gem 'broken_record' You can use BrokenRecord to scan all the records for all models in your application, or you can scan specific models: # Scan all records for all models in your application\nrake broken_record:scan\n\n# Scan all records of the User model\nrake broken_record:scan[User] BrokenRecord can also be configured to always skip some models and to run a given block of code before each scan. We configure BrokenRecord in an initializer. Here’s an example: if defined?(BrokenRecord)\n  BrokenRecord.configure do |config|\n    # Skip the Foo and Bar models when scanning.\n    config.classes_to_skip = [Foo, Bar]\n \n    # BrokenRecord will call the block provided in before_scan before\n    # scanning your records. This is useful for skipping validations\n    # you want to ignore.\n    config.before_scan do\n      User.skip_callback :validate, :before, :user_must_be_active\n    end\n  end\nend There’s a traditional way of looking for bugs by starting out stepping through code in your head in much the same way a computer does. Then there’s this other way where you start out looking for inconsistencies in your data. It’s sort of like looking at bug finding from a bottoms up approach (data to code) instead of a top down (code to data). \"Hygiene is important. That’s one of my failings. So I’m always being called on that.\" -Dan Aykroyd Keeping our data clean has been tremendously helpful to Gusto, so we hope you’ll find this gem useful too. Your data may be a diamond in the rough right now, but treasure it, polish it carefully, and it’ll shine!", "date": "2014-01-29"},
{"website": "Gusto", "title": "Diversity Goals for Gusto's Engineering Team", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/diversity-in-zenpayrolls-engineering-team/", "abstract": "The Gusto engineering team has been growing quickly and we've been excited with all the amazing software engineers that recently made a decision to join us. But there was also a simple observation that someone made: None of these recent hires were female. This started a number of discussions about the importance of diversity in the engineering team, some of which I'll outline in this post. But first, I'm going to jump right to the punch line by sharing the outcome of these discussions: We've completely refocused Gusto's engineering recruiting efforts to build a more diverse engineering team. Focus is important to any endeavor, so while there are many dimensions to diversity, we've decided as a very first step to aim to have 18% female engineers on our team by the end of this year. It seems like an easy thing to say we value diversity, but do we truly understand and believe this? One of the first discussions we had was figuring out why diversity is important to us. Why diversity is important to us We value diversity in our engineering team for three main reasons Diversity in our team will help us build better products for our diverse set of users -- small business owners of all kinds and their employees. Whenever thinking about what to build and how to build it, a more diverse team will enable us to have more ideas and perspectives, leading to better and more elegant solutions. The product implications are huge, but diversity is also important for our culture and work happiness. Most everyone here at Gusto enjoys getting to know people from different backgrounds and experiences. A diverse team creates more opportunities to learn all sorts of interesting and unique things from each other. Simply put, increased diversity equals increased happiness in a team. Diversity is important to us because it falls inline with our company value to \"do what's right\". It's well known that African American, Latino, and women are under-represented in STEM fields . By creating more demand for diversity in software engineering, we hope to do our small part in helping to incentivize under-represented groups to consider choosing a career in STEM. We see ourselves as part of a greater community with a shared responsibility to make the world a better place. Our current diversity numbers Another company value we have at Gusto is transparency, and we really liked Pinterest's transparency on their diversity numbers , and their publicly stated goals for the future . Frustratingly, we've found that though other companies are following their lead and starting to publish their diversity numbers, they only publish numbers for a vague \"tech\" category, without defining what that means. Publishing \"tech\" diversity numbers is a great first step, but we hope that companies will be more transparent by explaining what that means, as well as publishing more specific numbers for \"engineering\". Here's our current gender numbers at Gusto: As of September 1, 2015, our engineering team comprises of 11% full-time female engineers. Our technical team -- which we'll explicitly define as our engineers, technical product managers, and designers -- is comprised of 22% female full-time employees. Our diversity goal through January 31, 2016 Our first goal is to have 18% of our full-time engineers be female. While this is an ambitious goal given the relatively short period of time, we believe it's achievable with lots of dedication and focus. How did we come up with 18%? This is today's national average of female Computer Science majors. At the same time, we understand that this is just the first milestone for us. The percentage of female Computer Science graduates once peaked at 37% in 1984 , and we have a long-term goal to reflect similar numbers in our software engineering team. How will we do it? Hitting our first diversity recruiting goals won't come easy, so it's important to put a strategy in place. Some people incorrectly equate having diversity goals to lowering the hiring bar for certain groups. We firmly believe that there are equally strong female engineers out there and our goals can be hit by tweaking our recruiting process to optimize for diversity. Here are some of things we're doing to hit our goals: We're outbound sourcing 100% female candidates. We have a lot of male candidates who apply through our jobs page or are referred to us, so we feel comfortable doing this. Many of us know great female engineers, but just don't think to reach out to them for various reasons. We're having a number of \"referral sourcing parties\" focusing on combing our personal networks to reach out to them. We're going all-in at the Grace Hopper Celebration of Women in Computing . We're a Gold sponsor and hope to see you there! Grace Hopper is not just a way for us to recruit female engineers, but is also an opportunity for both men and women at Gusto to learn from others in the community and be more involved in the conversation. Our diversity initiatives are much more than just hitting numbers. We've rewritten our job postings to try to remove gender bias in the descriptions . We're going to have each engineer and recruiter participate in unconscious bias training. Google Capital, who led our Series B investment, is helping us out here. We're going to find ways to more outwardly share our amazing and inclusive engineering culture that values difference of opinions, is positive and constructive, and deeply cares about the problems that we're solving for small business owners and their employees. A diverse engineering team is not something that happens on its own. A lot of planning and work needs to go into creating a diverse work environment. We believe that diversity is in itself a core strength that will enable us to write better software and build better products. But we also feel that it is right for us to do our part to improve diversity in engineering careers. We'll write an update on January 31, 2016 to share our results! We hope you'll help keep us accountable.", "date": "2015-09-02"},
{"website": "Gusto", "title": "My Internship Experience at Gusto", "author": ["Rachel Huang"], "link": "https://engineering.gusto.com/my-internship-experience-at-zenpayroll/", "abstract": "Do you believe in aliens or ghosts? What about alien ghosts? What about ghostly aliens? These were the tough and passionately debated questions bounced around at lunch on my first day. It’s amazing how much you can learn about a person from their supernatural philosophies, and it makes for quite an interesting first impression of software engineers. Until I joined Gusto this summer, I had never had an internship. I had never done much more programming than in the classes at school. And I had associated software engineering with some nebulous career of coding. A lot. But, as someone majoring in computer science and going into her fourth year of college, it was something I figured I should try, demystify, and experience at least once. I didn’t know where I wanted to work, or what kind of work I wanted to do, and payroll had certainly never crossed my mind. But as I interviewed with the engineers here, it became clear that this company wasn’t just about paying people on time (although that was a critical part). It was about people and giving them the best possible experience. That really resonated with me and so I chose to come here. And given my first lunch chat, it definitely was not a mistake. tl;dr I experienced and learned a lot about a lot of things. However, that is hardly surprising; given my familiarity with this field was zero, it was impossible not to learn a vast amount because everything was new. Of course, there is the technical aspect, listing out the languages and projects like a resume, but there’s always much more than just that, especially here. My 12 weeks at Gusto were filled with code and people and moved at a full-throttle pace, but when I reflect on my time here, a lot of it isn’t just based around the programming or the features I finished. I mean it was – coding was my job – but I think you can learn that technical side almost anywhere. What makes it worthwhile is the culmination of everything else. I had no baseline going into Gusto, but the things I believe I got out of this experience would have been critical and just as new and enlightening. Amidst coding, shooting (and missing) archery targets for team bonding, and changing project ideas halfway through the summer, it’s been quite a ride, and these are just a few snapshots of the things I’ve learned. Startup stereotype Yes, there is free food. Yes, the office space has a very open concept. And yes, people do work long hours. But in terms of the checklist of the startup stereotype, Gusto doesn’t fit in that box. It’s much, much more than that. It has a strong backbone and a solid foundation with a vision that has a much longer shelf life than a few years. The cofounders want this to be their last job, their only job, and that marathon mentality is admirable and shared by the company, which gives it incredible, staying power. The technical gambit Front-end, back-end, frameworks, even a little payroll, and I had indeed been correct about the code. There was a lot. And it was completely overwhelming to say the least. A labyrinth of code, and each new story I was assigned seemed to drop me in a new arena. A full-fledged team member from the get-go, and the expectations were high, but just as high was everyone’s eagerness to help me. And with a growing willingness to explore, a lessening fear of breaking things already in place, and the ever-present support, I started seeing things more clearly, and my fumbling became familiarity. Fast, vibrant, absurd growth: Over the span of three months we went from one office space to three, with a Denver site picked and prepped for a second, central location. After the first month of my internship, I was still meeting new people every day. Three weeks into my time here and I was considered old hat, a veteran employee. It was truly exponential growth in human form. But the caliber of the people did not falter, we weren’t hiring for the sake of hiring, and that was pretty intense and amazing to be around. Culture is foundational On my second day of work around 11:30am, every single engineer started closing up his or her laptop and gathering at the front desk. This I found would become routine; this was engineering lunch, one of the many, purposeful times to step back and relax, regroup, and rethink. Culture is a remarkably core part of Gusto, the axis it spins upon, and I didn’t realize how important of an asset it could be. From all-hands meetings to address issues upfront, to engineering offsites for the team to take a breather, to the values and the principles that were created by them and for them, culture is pervasive in the space and embodied by every employee. The way that things get done and the way that decisions are made is really what drew me to Gusto. People are fundamental I can’t remember how many times I had to ask questions, mainly to my mentor, about everything from what the MVC model entailed to what was the shortcut to find all instances of in RubyMine. I believe I’m an introvert, so people are sometimes hard for me, but being here was a very good reminder of how essential they are. In school, working with people is cheating, but in the real world it’s the best thing you can do. And these were some of the best people I’ve encountered. They broke the mold or caricature of the stereotypical programmer, all dark spaces in isolated places, coding for hours on end. Every engineer I have come across is willing to pair or play ping-pong, and the space feels open. Open in the physical sense of the office and open in terms of ideas and issues. Communication and collaboration are key, both within and across teams, and everyone cares so, so much. It’s not perfect, but it’s something they’re definitely doing right. Where Gusto's culture, growth, and people will take them in the next five, ten years will undoubtedly be crazy and extraordinary. They’re always iterating and improving and I feel very privileged to have spent my summer working here with people who are passionate caring. I was all tongue-tied, terrified jitters and hesitancy when I first started but, as cheesy as it is, I’m glad I did. I think the ghosts helped.", "date": "2015-08-24"},
{"website": "Gusto", "title": "Wrapping C libraries with Ruby using SWIG", "author": ["Stephen Hopkins"], "link": "https://engineering.gusto.com/simple-ruby-c-extensions-with-swig/", "abstract": "At Gusto, most of our application is written in Ruby, but our tax calculations are done in a C library. Creating Ruby extensions using SWIG (Simplified Wrapper and Interface Generator) has saved us countless hours by automatically generating glue code between Ruby and C. SWIG can be used to generate the glue code between C/C++ and various scripting languages, such as Perl, Python, Ruby, and Tcl. In this post, we’ll create a Ruby gem that wraps a simple C library. So take a swig of beer, coffee, awesome juice or whatever you like, and let's get started! First step is to download and install SWIG . Next, let's create a new gem: $ bundle gem payroll_calculator We’ll start with a simple C library: /* ext/libpayroll.h */\ndouble calculate_sui_tax(const double amount, const double rate); /* ext/libpayroll.c */\n#include \"libpayroll.h\"\n \ndouble calculate_sui_tax(const double amount, const double rate)\n{\n  double tax = amount * rate;\n  return tax;\n} Now we need to wrap our C library! Interface files are the input to SWIG. To tell SWIG to wrap the above function, we create the following interface file. /* ext/libpayroll.i */\n%module libpayroll\n%{\n  #include \"libpayroll.h\"\n%}\n \nextern double calculate_sui_tax(const double amount, const double rate); This tells SWIG to create a Ruby module called Libpayroll , and all the functions listed will be wrapped. To run swig against the interface file, type: $ swig -ruby ext/libpayroll.i This will generate ext/payroll_calculator_wrap.c , which can be compiled into a shared library used in Ruby. To compile, we’ll need an extconf.rb , to configure a Makefile to build the extension. # ext/extconf.rb\nrequire 'mkmf'\ncreate_makefile('libpayroll') To create the extension: $ cd ext\n$ ruby extconf.rb\n$ make\n$ cp libpayroll.bundle ../lib Note: if you’re on a linux machine, you will probably see a .so file instead of a .bundle file Now we can test this out.  Go back to the root directory, and fire up irb: $ irb -Ilib -rpayroll_calculator\n2.0.0-p247 :001 > Libpayroll.calculate_sui_tax(9000, 0.34)\n => 3060.0 We can simplify this with a few rake tasks: # Rakefile\nrequire 'bundler/gem_tasks'\nrequire 'rake/clean'\n \nCLEAN.include('ext/**/*{.o,.log,.so,.bundle}')\nCLEAN.include('ext/**/Makefile')\nCLOBBER.include('lib/*{.so,.bundle}')\n \ndesc 'Build the libpayroll C extension'\ntask :build_ext do\n  Dir.chdir(\"ext/\") do\n    sh \"swig -ruby libpayroll.i\"\n    ruby \"extconf.rb\"\n    sh \"make\"\n  end\n  cp \"ext/libpayroll.bundle\", \"lib/\"\nend require 'rake/clean' will give you two additional rake tasks, rake clean and rake clobber , which will help you remove unnecessary files after building your extension.  The next three lines tell the rake tasks which files to include when cleaning and clobbering. Additionally, we’re adding a task rake build_ext , which will generate the C extension from the SWIG interface file. Now that we have the basics setup, let's dive a bit deeper into some more features of SWIG. Let us now say that we have a function in libpayroll.c like this: /* libpayroll.c */\nint calculate_income_tax(const double amount, double *tax, double *rate){\n  if(amount >= 0 && amount <= 1000) {\n    *rate = 0.25;\n  } else if (amount > 1000 && amount <= 5000) {\n    *rate = 0.30;\n  } else if (amount > 5000) {\n    *rate = 0.35;\n  } else {\n    return 1;\n  }\n \n  *tax = *rate * amount;\n  return 0;\n} With this function, we have two return values: the tax and the amount.  In Ruby, we would probably want this returned as a hash. SWIG typemaps make this a piece of cake! Our function currently takes three arguments, but the last two are pointers, and we only care about their values after we call the function, and therefore should really be return values. In Ruby, we want calculate_income_tax to take one argument. To change how SWIG will wrap this function, we first use the in typemap: /* ext/libpayroll.i */\n%typemap(in, numinputs=0) (double *tax, double *rate) {\n  $1 = (double *)malloc(1 * sizeof(double));\n  $2 = (double *)malloc(1 * sizeof(double));\n}; This means to take a function signature that has double *tax, double *rate and ignore them. $1 and $2 are then what are actually sent into the C function. Here, we just allocate memory - in this case the size of one double. Next we handle the return values. We can use the argout typemap to modify the output of the function: /* ext/libpayroll.i */\n%typemap(argout) (double *tax, double *rate) {\n  if(result == 0) {\n    $result = rb_hash_new();\n    rb_hash_aset($result, rb_str_new2(\"tax\"), rb_float_new(*$1));\n    rb_hash_aset($result, rb_str_new2(\"rate\"), rb_float_new(*$2));\n  } else {\n    $result = Qfalse;\n  }\n} Whatever we assign to $result is what ends up being returned. We can check the original return value by looking at the result variable. If the result is 0, meaning success, we initialize a new Ruby hash. *$1 and *$2 refer to double *tax and double *rate , respectively. Calling rb_float_new creates a new Ruby float object from the C double. Finally, we assign the values to keys tax and rate in the Ruby hash. If the result is not zero, we want to return false in Ruby, which in the Ruby C API is Qfalse . (True and nil are Qtrue and Qnil , respectively) When we create Ruby objects, the Ruby garbage collector will take care of releasing the memory when it is no longer needed. However, the memory from our initial malloc calls need to be released to avoid memory leaks. This can be done in the freearg typemap. /* ext/libpayroll.i */\n%typemap(freearg) (double *tax, double *rate) {\n  free($1); \n  free($2);\n} Finally, we add the method signature so that SWIG knows to wrap it: /* ext/libpayroll.i */\nint calculate_income_tax(const double amount, double *tax, double *rate); Note that all typemaps for a given function must be located above the method signature in the interface file. Let's try it out! $ rake build_ext\n$ irb -Ilib -rpayroll_calculator\n2.0.0-p247 :001 > Libpayroll.calculate_income_tax(500)\n => {\"tax\"=>125.0, \"rate\"=>0.25} \n2.0.0-p247 :002 > Libpayroll.calculate_income_tax(2000)\n => {\"tax\"=>600.0, \"rate\"=>0.3} \n2.0.0-p247 :003 > Libpayroll.calculate_income_tax(9500)\n => {\"tax\"=>3325.0, \"rate\"=>0.35} \n2.0.0-p247 :004 > Libpayroll.calculate_income_tax(-100)\n => false This only scratches the surface of SWIG. For more details, check out documentation . For the complete source code from this tutorial, fork on GitHub .", "date": "2014-03-25"},
{"website": "Gusto", "title": "Benefits of Writing a DSL in Ruby", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/benefits-of-writing-a-dsl/", "abstract": "Here at Gusto, we're all about abstracting away the complexities that come with compensation. Payroll has traditionally been a bureaucratic hornet's nest, and implementing a modern and delightful product in such an atmosphere is an engaging technical challenge--one that is difficult to achieve without automation. Gusto is currently pushing to provide nationwide service (we're at 24 states and counting!), which demands we satisfy a bevvy of unique requirements for each state. Initially, we found ourselves writing a lot of boilerplate code by hand, instead of concentrating on what made each state a snowflake. We soon realized that this was a problem that could reap enormous benefits from tooling--namely, creating a domain specific language to accelerate and streamline the development process. In this article, we're going to build one such DSL that broadly resembles what we use in-house, if a bit simpler. Identifying Use Cases for a DSL Writing a DSL is a lot of hard work, and it isn't a silver bullet for all problems. In our case, however, the pros far outweighed the cons: Consolidation of state-specific code In our Rails app, we have several models where we have to implement specific code for each state. We need to generate forms, store and manipulate mandatory information pertaining to employees, companies, filing schedules and tax rates. We make payments to government agencies, file the generated forms, calculate payroll taxes, and more. A DSL implementation allows us to consolidate and organize all of the state-specific code into a dedicated directory and primary file. Scaffolding for states Rather than starting from scratch for every new state, using a DSL provides scaffolding to automate common tasks across states, while still providing flexibility for full customization. Reduced surface area for errors Having a DSL creates the classes and methods we need to eliminate boilerplate code and provides fewer touch points for developers. By thoroughly testing a DSL and guarding against invalid input, chances for error are reduced dramatically. Provides a toolkit to accelerate expansion We've created a framework that makes it easier to implement unique compliance requirements for new states. A DSL is a focused toolkit that reduces the time it takes to develop going forward. Writing the DSL For the scope of this tutorial, we will focus on creating a domain specific language that will allow us to collect identification numbers for companies and payroll parameters for employees (used for calculating taxes). While this is a mere glimpse into what we can accomplish with a DSL, it still provides a comprehensive introduction to the subject. Our final DSL will look something like this: StateBuilder.build('CA') do\n  company do\n    edd { format '\\d{3}-\\d{4}-\\d' }\n    sos { format '[A-Z]\\d{7}' }\n  end\n\n  employee do\n    filing_status { options ['Single', 'Married', 'Head of Household'] }\n    withholding_allowance { max 99 }\n    additional_withholding { max 10000 }\n  end  \nend Awesome! This is clean, concise, and expressive code, using an interface designed for solving our challenge.  Let's get started. Define Parameters First, let's define what we need our DSL to accomplish. First question to ask: what kind of information do we need to store? Every state requires companies to register with local authorities. Upon registration in most states, companies are provided with identification numbers to pay taxes and file forms. On a company level, we need a dynamic way to store different identification numbers for each state. Withholding tax is computed based on the number of allowances an employee claims. These are values that are found on state level W-4 forms. For each state, a variety of different questions can be asked to determine state income tax rates, such as your filing status, dependent exemptions, disability allowances, and more. For employees, we need a flexible method of defining different attributes for each state , in order to compute taxes accordingly. The DSL we will write will handle company identification numbers and basic payroll information for employees. We will then use the tool to implement California. While California has several other details that are necessary to consider when computing payroll, we will focus on these areas in order to get an overview of how to implement a DSL. I have provided a link to a basic Rails application in order to easily follow along with this tutorial that can be found here . The setup of the app models is as follows: Company : Represents the company entity. Stores information such as the name, the corporation type, and date established. Employee : Represents a single employee that works at a company. Stores information such as name, payment information, and when they were hired. CompanyStateField : A Company has many CompanyStateFields , and each one handles one state's information pertaining to a company. This includes state identification numbers. California requires two numbers from employers, the Employment Development Department number (EDD) and the Secretary of State number (SoS), more information can be found here . EmployeeStateField : An Employee has many EmployeeStateFields , and each one handles one state's specific information for a employee. This includes information found on state W-4's like withholding allowance and filing status. California's form, the DE 4 , requires that we collect withholding allowances, an additional withholding dollar amount, and the employee's filing status (Single, Married, or Head of Household). We have setup single table inheritance for the CompanyStateField and EmployeeStateField models.  This allows us to define state specific subclasses for CompanyStateField and EmployeeStateField and use only one table per model to handle states. To bolster this, both tables have a serialized data hash that we wil use to store values specific to a state. Although this data will not be queryable, it allows us to store state specific information without cluttering our database with unnecessary columns. Our app is setup to handle states, and now our DSL must create the state specific classes that will implement California's functionality. Tools of the Trade Metaprogramming is where Ruby really shines as a language. We can define methods and classes at runtime as well as make use of a variety of metaprogramming tools that make creating a DSL in Ruby a joy. Rails itself is a DSL for creating web applications, and the framework draws a lot of its \"magic\" from Ruby's metaprogramming capabilities. Below are a list of the some of the methods and objects that are useful for metaprogramming. Blocks The block syntax allows us group code together and pass it as an argument to a method. They can be defined with do end syntax, or be encapsulated within curly braces, they are synonymous. You will have most likely seen these when using a method like each . [1,2,3].each { |number| puts number*2 } This is an excellent tool for creating DSL's because they allow you to define code in one context and execute it in another. This is powerful for creating readable DSL's by abstracting method definitions into other classes; we will go through many examples of this throughout the tutorial. send The send method allows you to invoke a method (even private ones) on an object by passing in the name as a symbol. This is useful for invoking methods that are usually called within the class definition, as well as for interpolating variables for dynamic method calls. define_method Ruby's define_method provides the ability to create methods without having to use the traditional def within the class definition. define_method takes a string for the method name and a block to determine what code should be executed when the method is invoked. instance_eval instance_eval is an essential tool for developing DSL's, much like blocks are. instance_eval takes a block and executes it within the context of the receiver. For example: class MyClass\n  def say_hello\n    puts 'Hello!'\n  end\nend\n\nMyClass.new.instance_eval { say_hello }   # => 'Hello!' In the above example, the block contains a call to the method say_hello , even though there is no corresponding method definition in the context. The instance returned by MyClass.new is the receiver and we execute say_hello within that context. class MyOtherClass\n  def initialize(&block)\n    instance_eval &block\n  end\n\n  def say_goodbye\n    puts 'Goodbye!'\n  end\nend\n\nMyOtherClass.new { say_goodbye }   # => 'Goodbye!' We again define a block that will call a method that is not defined in the context. This time, we pass the block to the MyOtherClass constructor and evaluate its contents within the context of the default receiver self , which is an instance of MyOtherClass . Pretty sweet! method_missing This is the magic behind Rails's find_by_* methods that match every column of your tables. With method_missing , any method you call that is not defined will go through this method, passing its name and any arguments along as well. This is another great tool for DSL's because it allows methods to be defined dynamically with no prior knowledge of what might be called, leading to highly customized syntax. Designing and Implementing the DSL Now that we have some background about our toolkit, we should think about how we want our DSL to look and what it would be like to use it for developing states. In this sense, we are working backwards; rather than starting by defining classes and methods, we will invent an ideal syntax and build classes around it to suit our needs. Think of this as an outline of how we want our DSL to be implemented. Let's take a look again at what our final implementation will look like: StateBuilder.build('CA') do\n  company do\n    edd { format '\\d{3}-\\d{4}-\\d' }\n    sos { format '[A-Z]\\d{7}' }\n  end\n\n  employee do\n    filing_status { options ['Single', 'Married', 'Head of Household'] }\n    withholding_allowance { max 99 }\n    additional_withholding { max 10000 }\n  end  \nend Let's break this down piece by piece and incrementally write the code that will turn this into the classes and methods we need to implement CA. If you want to follow along with the repo provided, you can git checkout step-0 and fill in the code as we go through it. Our DSL is called the StateBuilder and is a class. We start each state by calling a class level build method with a state abbreviation and a configuration block. Within this block, we can then make calls to methods we will define called company and employee , and pass them each a block to configure our specific models ( CompanyStateField::CA , EmployeeStateField::CA ). app/states/ca.rb StateBuilder.build('CA') do\n  company do\n    # CompanyStateField::CA configuration\n  end\n\n  employee do\n    # EmployeeStateField::CA configuration\n  end\nend As mentioned before, our logic is encapsulated in a StateBuilder class. We call the block passed to self.build within the context of a new instance of StateBuilder , so company and employee must be defined methods that each accept a block of their own. Let's setup the scaffolding for this by defining a StateBuilder class that fits this specification. app/models/state_builder.rb class StateBuilder\n  def self.build(state, &block)\n    # If no block is passed in, raise an exception\n    raise \"You need a block to build!\" unless block_given?\n\n    StateBuilder.new(state, &block)\n  end\n\n  def initialize(state, &block)\n    @state = state\n\n    # Evaluate the contents of the block passed in within the context of this instance of StateBuilder\n    instance_eval &block\n  end\n\n  def company(&block)\n    # Configure CompanyStateField::CA\n  end\n\n  def employee(&block)\n    # Configure EmployeeStateField::CA\n  end\nend We've got the basic setup of our StateBuilder ! Since our company and employee methods will define our CompanyStateField::CA and EmployeeStateField::CA classes, let's design how we want each of the blocks passed to their respective methods to look. We need to define each of the attributes our models should have, as well as some basic information about them. What's great about creating a custom DSL is that we do not have to use traditional Rails syntax for getters/setters and validations; instead let's use the concise syntax we looked at earlier in the article. It's time to git checkout step-1 , folks! For companies in California, we need to store two identification numbers: the California Employment Development Department number (or EDD number) and the California Secretary of State number (or SoS number). The EDD number has a format of \"###-####-#\" and the SoS number has a format of '@#######', where @ is any letter and # is any digit. Ideally, we would use the name of our attribute as a method call and pass in a block that defines a format for that particular attribute (I smell a use case for method_missing !). Let's go ahead and write out what these method calls would look like for the EDD and SoS numbers. app/states/ca.rb StateBuilder.build('CA') do\n  company do\n    edd { format '\\d{3}-\\d{4}-\\d' }\n    sos { format '[A-Z]\\d{7}' }\n  end\n\n  employee do\n    # EmployeeStateField::CA configuration\n  end  \nend Notice here we have switched out Ruby's do end syntax for curly braces, but it still accomplishes the same task of passing in a block with code to be executed to the method. Let's follow this same process for the employee side configuration for California. On the California Employee's Withholding Allowance Certificate (DE4) , employees are asked for their filing status, number of withholding allowances, and any additional withholding amounts an employee may have. The filing status can be either Single, Married, or Head of Household, the withholding allowance must be under 99, and let's set a maximum of $10,000 for the additional withholding dollar amount. Let's implement these attributes and validations like we did for the company method. app/states/ca.rb StateBuilder.build('CA') do\n  company do\n    edd { format '\\d{3}-\\d{4}-\\d' }\n    sos { format '[A-Z]\\d{7}' }\n  end\n\n  employee do\n    filing_status { options ['Single', 'Married', 'Head of Household'] }\n    withholding_allowance { max 99 }\n    additional_withholding { max 10000 }\n  end  \nend And we've arrived at our final CA implementation! Our DSL now defines attributes and validations for both our CompanyStateField::CA and our EmployeeStateField::CA using custom syntax. Now, we need to translate our syntax into class definitions, attribute getters/setters, and validations. Let's move on to implementing the company and employee methods within the StateBuilder itself and get this functionality working. Proceed to git checkout step-2 , metaprogrammers! We will implement our methods and validations by defining what to do with each block in the StateBuilder#company and StateBuilder#employee methods. Let's take a similar approach to what we did when we first defined the StateBuilder : create a \"scope\" that will handle these methods and instance_eval our block in its context. Let's call our scopes StateBuilder::CompanyScope and StateBuilder::EmployeeScope and setup our StateBuilder methods to create a new instance of each class. app/models/state_builder.rb class StateBuilder\n  def self.build(state, &block)\n    # If no block is passed in, raise an exception\n    raise unless block_given?\n\n    StateBuilder.new(state, &block)\n  end\n\n  def initialize(state, &block)\n    @state = state\n\n    # Evaluate the contents of the block passed in within the context of this instance of StateBuilder\n    instance_eval &block\n  end\n\n  def company(&block)\n    StateBuilder::CompanyScope.new(@state, &block)\n  end\n\n  def employee(&block)\n    StateBuilder::EmployeeScope.new(@state, &block)\n  end\nend app/models/state_builder/company_scope.rb class StateBuilder\n  class CompanyScope\n    def initialize(state, &block)\n      @klass = CompanyStateField.const_set state, Class.new(CompanyStateField)\n\n      instance_eval &block\n    end\n  end\nend app/models/state_builder/employee_scope.rb class StateBuilder\n  class EmployeeScope\n    def initialize(state, &block)\n      @klass = EmployeeStateField.const_set state, Class.new(EmployeeStateField)\n\n      instance_eval &block\n    end\n  end\nend We use const_set to define a subclass of CompanyStateField and EmployeeStateField with the name of our state. This yields a CompanyStateField::CA class and a EmployeeStateField::CA class, both inheriting from their respective parents. Now we can focus on the final level, the blocks passed to each of our attributes (like sos , edd , additional_withholding , etc). These will be executed within the context of CompanyScope and EmployeeScope , but if you run the server you will see undefined method errors. Let's use the method_missing method to catch these cases. With our current setup, we can assume every method being called is the name of an attribute and the blocks passed into these methods are how we want to configure that attribute. This gives us the \"magic\" ability to define the attributes we want and store them in the database. Warning : using method_missing without any cases to call super can lead to unexpected behavior. Misspellings will be hard to track down as they will all fall into method_missing . Be sure to create cases for when method_missing should call super when expanding on these principles. Let's define a method_missing function and pass those parameters to a final scope we will create called AttributesScope , which will define our store_accessors and validates according to what methods are called within each attribute block. app/models/state_builder/company_scope.rb class StateBuilder\n  class CompanyScope\n    def initialize(state, &block)\n      @klass = CompanyStateField.const_set state, Class.new(CompanyStateField)\n\n      instance_eval &block\n    end\n\n    def method_missing(attribute, &block)\n      AttributesScope.new(@klass, attribute, &block)\n    end\n  end\nend app/models/state_builder/employee_scope.rb class StateBuilder\n  class EmployeeScope\n    def initialize(state, &block)\n      @klass = EmployeeStateField.const_set state, Class.new(EmployeeStateField)\n\n      instance_eval &block\n    end\n\n    def method_missing(attribute, &block)\n      AttributesScope.new(@klass, attribute, &block)\n    end\n  end\nend Now whenever we call a method in the company block of our app/states/ca.rb file, it will hook into this method_missing function. The name of the method that was called will be the first argument to method_missing and it is also the name of the attribute we defining. We create a new instance of AttributesScope with the class we will be modifying, the name of attribute we are defining, and a block to configure that attribute. In our AttributesScope , we will call store_accessor to define getters/setters for the attribute, and we'll use the serialized data hash to store it. class StateBuilder\n  class AttributesScope\n    def initialize(klass, attribute, &block)\n      klass.send(:store_accessor, :data, attribute)\n      instance_eval &block\n    end\n  end\nend Last but not least, we need to define the methods we call in our attributes blocks ( format , max , and options ) and turn them into validations. We accomplish this by manipulating our method calls into validates calls that Rails expects. class StateBuilder\n  class AttributesScope\n    def initialize(klass, attribute, &block)\n      @validation_options = []\n\n      klass.send(:store_accessor, :data, attribute)\n      instance_eval &block\n      klass.send(:validates, attribute, *@validation_options)\n    end\n\n    private\n    def format(regex)\n      @validation_options << { format: { with: Regexp.new(regex) } }\n    end\n\n    def max(value)\n      @validation_options << { numericality: { greater_than_or_equal_to: 0, less_than_or_equal_to: value } }\n    end\n\n    def options(values)\n      @validation_options << { inclusion: { in: values } }\n    end\n  end\nend And our DSL is up and running! We have successfully defined a CompanyStateField::CA model that stores and validates our EDD and SoS numbers as well as a EmployeeStateField::CA model that stores and validates the withholding allowance, filing status, and additional withholding amounts of our employees. While our DSL was implemented to automate fairly simple tasks, each of our scopes is setup to be expanded. We can easily add new hooks to our DSL, define more methods on our models, and build upon the functionality we created here. Our implementation effectively reduces repetition and boilerplate code across the backend, but it still requires each state to have a unique view on the client side. We have expanded our in house implementation to also drive our frontend for new states, and if there is interest in the comments, I will write another blog post to expand on what we have created here. This tutorial details just a portion of how we used our own DSL to tackle state expansion. Tools like this have proved extremely valuable in expanding our payroll services to the rest of the US, and if problems of this nature interest you, we are hiring ! Happy Metaprogramming! Comments on Hacker News", "date": "2014-09-29"},
{"website": "Gusto", "title": "Generating fixed width documents in Ruby: Introducing Fixy", "author": ["Gusto"], "link": "https://engineering.gusto.com/generating-fixed-width-documents-in-ruby-introducing-fixy/", "abstract": "A few weeks ago, we posted an article explaining how ACH works from a developer perspective . A question that came up a lot through the comments is how we generate the ACH debit files that are transmitted to the bank. This is a particularly interesting question, because it really is asking how we interface with systems that do not provide a modern restful API. Most of these older systems that have been around for 10-20 years, use fixed-width documents for communicating between platforms. To name a few examples: ACH debit file: Upload a fixed-width ACH document into the bank's FTP server to initiate payments. Download an FTP server to retrieve transaction errors. ( specifications ) W2 E-filing: Upload a fixed-width document detailing employee's tax details through the goverment's web portal. A few days later, a confirmation will be released to let us know whether the file is valid or not. ( specifications ) 1099 E-filing Upload a fixed-width document detailing contractor's tax details through a separate goverment portal. A few days later, a confirmation will be released to let us know whether the file is valid or not. ( specifications ) At Gusto, we strive to achieve simple, paperless payroll for our customers. To reach that goal, automation is key using whatever means as available (at times, we have to get pretty creative). That's why we created Fixy , a gem to make it easy to work with fixed-width documents. Fixed-width documents overview Fixed-width documents are composed of multiple fixed length records . Every line in the document is a record. Below is the document structure for the a 1099 e-filing document. In turn, every record follows a very defined structure, which describes how every character within that line should be generated. Below is a quick sample of what most documentation looks like. A first approach to generating documents When working with a document of this kind for the first time, it's tempting for the sake of simplicity to generate a document like this: def record_t(year, transmitter)\n\trecord  = ''\n\trecord << '1'\n\trecord << year.to_s\n\trecord << ' '\n\trecord << sanitize_and_pad(transmitter.tin, 9)\n\trecord << sanitize_and_pad(transmitter.control_code, 5)\n\t# ... and so on ...\n\trecord\nend While this approach works, it's not as scalable as it is very error prone due to off-by-1 errors in offset. It's also hard to maintain because record declaration, definition, formatting, are tightly coupled. Introducing Fixy We created Fixy because we wanted to express a document in a re-usable, more manageable way, that is also debug-friendly. Let's walk through defining the record T above. The first thing to do is to define the length of the record, and required formatters. Most of the time, all records within a document have the same length and formatters. For that reason, let's start with creating a base class for all our records: class BaseRecord < Fixy::Record\n  include Fixy::Formatter::Amount\n  include Fixy::Formatter::Numeric\n  include Fixy::Formatter::Alphanumeric\n\n  set_record_length 750\nend Next is field declaration. This is very much a dump of the documentation table. Occasionally, this section can be auto-generated directly from the documentation. class RecordT < BaseRecord\n\n  # ----------------------------------------------------------------------------\n  #         Field Name                Size      Range         Type\n  # ----------------------------------------------------------------------------\n\n  field :record_type               , 1 ,  '1'     ,  :alphanumeric\n  field :payment_year              , 4 ,  '2-5'   ,  :alphanumeric\n  field :prior_year_data_indicator , 1 ,  '6'     ,  :alphanumeric\n  field :transmitters_tin          , 9 ,  '7-15'  ,  :alphanumeric\n  field :transmitter_control_code  , 5 ,  '16-20' ,  :alphanumeric\n  # ... more fields ... Because every record depends on specific information, we'll want to update the construction as we see fit. In our case, all we need to build this record is the current filing year: attr_accessor :year\n  def initialize(year)\n      @year = year\n  end Finally, we need to define which data every field should contain. We don't need to worry about how it gets formatted, because we already define the formatter, and allocated space in the sections above. For convenience, we can use a lambda, or a method for calculating the field value: field_value :record_type                                 , 'T'\n  field_value :payment_year                                , -> { year }\n  field_value :prior_year_data_indicator                   , ''\n  field_value :transmitters_tin                            , AGENT_EIN\n  def transmitter_control_code\n    AGENT_1099_TRANSMITTER_CODE\n  end\nend # closing bracket for class Our record is now fully defined. A document, which is composed by a set of records, is described through a Fixy::Document object. Continuing with our example, our 1099 E-filer document would look as follows: class Efiler1099 < Fixy::Document\n\n  attr_accessor :year\n\n  def initialize(year)\n  \t@year = year\n  end\n\n  private\n\n  #\n  # Document format:\n  #\n  #   T Record: Identifies the Transmitter of electronic file.\n  #   A Record: Identifies the Payer, and the type of payments\n  #   B Record: Idenfifies the Payee, and the payment amounts\n  #   C Record: Summary of B Records\n  #   K Record: Summary of State Totals\n  #   F Record: End of Transmission\n  #\n  # Example of document:\n  #\n  #   T - A - B - B - B - C - K - A - B - B - B - C - K - K - F\n  #\n\n  def build\n    append_record RecordT.new(year)\n    Company.with_contractors.each do |company|\n      append_record RecordA.new(year, company)\n      company.contractors.each do |contractor|\n      \tappend_record RecordB.new(year, contractor)\n      end\n      append_record RecordC.new(year, company)\n      append_record RecordK.new(year, company)        \n    end\n    append_record RecordF.new\n  end\nend That's it! with our document ( Fixy::Document ) and records ( Fixy::Record ) fully defined, we can now seamlessly generate the fixed-width document using the following command: Efiler1099.new(2013).generate_to_file('1099.txt') Alternatively, an interactive HTML debug version of the document can also be created: Efiler1099.new(2013).generate_to_file('1099.txt', true) Using Fixy in your own projects We packaged Fixy into a gem , and released the source code on GitHub . The ability to create, maintain, and debug fixed width documents has been incredibly useful to Gusto, and we hope it will be useful for you too. Enjoy! :)", "date": "2014-05-20"},
{"website": "Gusto", "title": "Building a Data-Informed Culture: An Introduction to Data at Gusto", "author": ["Daniel Sternberg"], "link": "https://engineering.gusto.com/building-a-data-informed-culture/", "abstract": "Here at Gusto we are focused on creating a data-informed culture. For us, this means that we apply our values, principles, and experiences when we gather, analyze, and incorporate data into our decision-making. The Data team is responsible for building our data-informed culture. We curate and develop tools using the data created by our over 40,000 customers to help us grow efficiently, control risk and avoid fraud, and build product features that help Gusto’s customers become data-informed too. How did the Data team begin? By early 2015, demand was growing internally for access to data. At the time this demand was satisfied by product managers and engineers running their own ad-hoc SQL scripts on production databases and operations teams generating reporting via their own tools. We realized that we needed to build a team that would bring all of these disparate sources together. That way, we could empower the whole company to make more effective data-informed decisions. Once we decided to invest in being data-informed, we had to choose whether to focus on growing a data science team to get value out of the data or a data engineering team that could create the foundational infrastructure first. After internal discussions and speaking to friends at other companies who have gone through this process, we came to the conclusion that it was most important to lay the right foundation first by developing a warehouse that could provide a single source of reliable and consistent data. This was the right move for us even if it meant fewer quick wins because it would enable future data scientists and analysts to more efficiently leverage their core competencies - building models and generating insights. Over the past year we have replicated and piped all of our major data sources into a single warehouse that teams across the company can access. Our Data team is at ten members and growing, and as we’ve grown we’ve also clarified roles and responsibilities. The Data Science & Engineering team consists of engineers and data scientists, with engineering focusing on developing and maintaining data infrastructure and tools while data science builds predictive models and statistical tools. The Data Analytics team works closely with people across the company to make sure they have access to the most up-to-date insights relevant to their work, whether that’s helping our Care team manage their queue or building a new core dashboard for our Growth team members. The what and why of our infrastructure We’ve gone through a couple of iterations of our data platform and have currently settled on the the tools and structure in the diagram below: Tools we use We prefer to use open source tools where possible (marked with a *). Our analytical database is an AWS Redshift cluster, with S3 as our underlying data lake Amazon’s Database Migration Service replicates our production app databases to individual schemas in Redshift Apache Airflow * orchestrates our ETLs. We use Airflow to support a variety of tasks, such as: Ingesting data from 3rd party vendor APIs SQL statements that create higher level views (more on this later) Testing and QA tasks to alert us of faulty logic and downstream changes from our production apps so that we don’t propagate bad data to our end users Snowplow * for event tracking in our apps, which supports easy integration into Redshift out of the box Looker as a BI front-end that teams throughout the company can use to explore data and build core dashboards Aleph * as a shared repository for ad-hoc SQL queries and results Structure of our data warehouse At the lowest level , we have our raw replicated data sources - production app tables, events and third party integrations. While our apps automatically encrypt the most sensitive pieces of information like bank account numbers, EINs and SSNs, access to these schemas is strictly controlled because they contain personally identifiable information (PII). In the middle , we have our BI tables  - user-friendly versions of the data in our apps and third-party data sources, denormalized for easier analysis. We also separate PII (personally identifiable information) into a different schema and control access to it. At the highest level , we have team-specific views - these are joins and rollups of multiple BI tables that specific teams use to drive core dashboards and look at on a daily basis. As a fast-growing SaaS business we don’t have major data scaling issues (yet!), but this actually provides us with some unique advantages. For example, we’ve found that when an analyst needs a new ETL (extract-tranfsorm-load) task for a project they’re working on, it’s more efficient for the whole team if they can write it themselves rather than having to hand it off to an engineer. Since the raw tables are available in Redshift to our ETL application, when an analyst wants to make a new BI table to expose new data sources to end users, they can simply find the relevant tables and columns in the raw tables, write queries to build the tables they need as part of an Airflow task, and expose them in Looker for use in dashboards and further analysis. Direct access to raw data is strictly controlled. Where we’re going The newest data investment for us is in data science. Gusto processes billions of dollars in payroll each year and adds many new customers every day. This means we need to have amazing teams and great software to keep everything humming, and we’re lucky to have both. We think we can make things run even smoother and provide more value to our customers by incorporating predictive models and statistical tools into our teams’ workflows and into our own product. Here are a few of the specific areas we’re focusing on: Stopping fraudsters in their tracks in partnership with our Risk team Helping Gusto grow efficiently by focusing our efforts on businesses who are likely to get the most benefits out of our product today Leveraging insights from our payroll and benefits data (like in our recent Minimum Wage Report ), so we can help our customers get more meaning out of the information we have. As both our team and data grow over the coming months and years, we want to continue to make it easier for our analysts and scientists to get the data they need, and for engineers to turn these insights into amazing products and services for our team and our customers. We’re mindful that in order to do this our tools and pipelines will need to change as we grow, as will the way we organize ourselves. We’re looking for data scientists and engineers to join us, so if these problems sound interesting to you, take a look at our job openings !", "date": "2016-12-13"},
{"website": "Gusto", "title": "Code Schools: The Bar", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/code-schools-the-bar/", "abstract": "(cross posted from Into Thin Air ) I’ve been drinking a lot of coffee. A lot. People here in Denver are making significant career changes and becoming software engineers. This is good. And I’ve been drinking coffee with them. These people have worked for the Peace Corps, directed non-profits, built profit and loss statements as accountants, and defended clients in court. They are turning to code schools to help them get into the tech sector. Places like the Turing School of Software & Design and Galvanize have impressive programs that teach folks the trade of software development. Students graduate with competence in jQuery, React, Angular, Ruby, Rails, and MySQL. Most of these are frameworks, languages and systems we use here at Gusto. And the most common question I get from code school graduates, as we co-caffeinate, is: \" Beyond what we learn in code school, are there other technical qualities Gusto looks for when hiring software engineers? \" The answer: \" Yes, you’ll have to demonstrate working knowledge of CS fundamentals. \" Why? CS fundamentals, including knowledge of data structures, algorithms, and runtime complexity, create a language that engineers use to communicate about code. And honestly, a lot of this stuff is intuitive once you know what to call it. Take this code snippet, for example: A developer without CS knowledge might look at this and think “Hmm. This feels inefficient.” A developer who understands CS fundamentals might say, “Hmm. O(N²) runtime complexity is no bueno.” “O(N²),” then, becomes part of the language that those who understand CS fundamentals use when they talk to each other about their programs. It carries with it implicit meaning that requires no further explanation. Engineers just get it. And that is efficient. Ok. The Bar. The bar Gusto sets for engineers is the same whether they have a CS degree from Stanford or a degree from a code school like Turing. We want to see A) an understanding of CS fundamentals, B) strong coding capability, and C) good collaboration with the interviewer. Being able to build websites or mobile apps won’t be enough. You’ll have to study up on and understand CS fundamentals. And this seems dispiriting to those who have spent 6 months learning to code. But code school graduates can be successful with A, B, and C. It has happened before with Gusto. But they had to work hard on “A” first. Get good at \"A.\" And to that end, I exhort diving into these resources. Khan Academy — Computer Science Algorithms Chapters 1 and 2 of Cracking the Coding Interview by Gayle Laakman McDowell Career Cup’s mock interview video series MIT’s Computer Science Lectures After spending time learning the language of CS, I highly recommend engaging a placement firm with experience helping people find software engineering jobs at tech companies, such as Triplebyte — a highly respected placement firm we’ve worked with in the past. And check out this article by Kelly Sutton — a new Gusto engineer who writes about his experience with Triplebyte during his job search. I'm one of you. I have empathy for what it’s like for someone with a non-technical background to try and break into tech as a developer. Just check out my LinkedIn profile . I have a degree in Spanish. And throughout my 16-year career, I’ve had to constantly prove I have CS chops. Code school graduates should expect to do the same as they start working as professional developers. Reach out to me on Twitter @jgrahamthomas , or shoot me an email at jeremy.thomas@gusto.com if you have questions or want advice.", "date": "2016-12-13"},
{"website": "Gusto", "title": "Denver = Engineering Career Stagnation?", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/denver-engineering-career-stagnation/", "abstract": "(cross-posted from Into Thin Air ) People If I leave San Francisco and work for a company in a place like Denver, I’d be working with a bunch of “B Players.” I mean, the best people are in Silicon Valley, right? Silicon Valley is filled with talented engineers. I left my career as a Sr. Director in San Diego to start a company in San Francisco in part because I believed I needed to be there in order to realize my full potential as a technologist. Surrounding myself with the best minds would bolster my skills in the trade. Four years later, the best software developer I know lives in San Diego (and never finished a four-year degree). But it’s undeniable that Silicon Valley has the highest concentration of software engineers from top-tier schools in the world. They’re drawn to the thousands of great technology companies looking to hire them there. Subjectively, 50% of conversations I overheard in a given coffee shop there were about tech, and that percentage is close to zero here in Denver. There are thousands of interesting technology companies in Silicon Valley. And there are thousands of opportunities to mingle with fellow techies through the abundance of ‘meetups’ there. The world looks to Silicon Valley for technological innovation. But the import of the place is generally obfuscated in day-to-day life — developers spend 99% of their time with the same small group of people who show up, just like they do, to work every day. Exposure to Silicon Valley, then, is limited to what developers extract from their place of work. And that exposure is no different in Silicon Valley than it is with any good tech company in any city. “Good tech company” is the operative phrase. The Denver area has hundreds of them. Off the top of my head there’s Stack Overflow, Inspirato, Sendgrid, FullContact, Gusto (where I work), TapInfluence, Uber, Galvanize, Google (hiring 1,500 in Boulder), Oracle, IBM (Watson is amazing), Havenly, ReadyTalk, NetSuite, Techstars, HomeAdvisor, Craftsy, Rachio, Sphero, LogRhythm, OnDeck, and OpenTable. Working for any one of them will feel no different than working for a company in Silicon Valley. Compensation If I worked in Denver, the salary cut I’d take would make it harder for me to come back to San Francisco. People would look at my comp and think I’m just an “ok” engineer, no? Denver is cheaper than Silicon Valley. On average, developers earn 15% less per year. If you’re making $140K/year in California, you’re making $121K/year in Colorado. Does this delta really affect a developer’s perceived value? It might, if “salary” is a heuristic used by developers to understand where they stand relative to their peers, and where said peers are geographically distributed. But companies don’t care. Recruiters understand that market rates vary. And the “How much are you currently making?” question is one that’s asked during an interview, where context around market conditions is already understood and can be further expounded upon. Denver’s lower salaries do not hurt a developer’s chance to being hired into a good position in Silicon Valley. Denver = Career Stagnation? Do cities like Denver equate to career stagnation for software developers? This developer thinks not. I’ve been impressed with people we’ve been talking to about Gusto’s open positions in Denver, with many of them concurrently receiving offers from us and companies in Silicon Valley. There are great engineers here. There are great companies here. There are great careers here.", "date": "2016-12-28"},
{"website": "Gusto", "title": "6 Months at Gusto: What I experienced - Part 2", "author": ["Sihui Huang"], "link": "https://engineering.gusto.com/6-months-at-gusto-what-i-experienced-part-1/", "abstract": "What I did , experienced , and learned during my first 6 months at Gusto Having had three very different internship experiences before joining Gusto, I knew culture could make a big impact on how you feel about your work. Yet, I still underestimated its power. When a company takes its values and culture seriously and carries them out in day-to-day works, everything seems different and has a bigger meaning. After sharing what I did during my first six months at Gusto, I want to share what I experienced in this second post and, in my third post , I will share what I learned. What I experienced We are one team, and it’s all about serving our customers. Every Gustie has to shadow customer phone calls during onboarding. I was really excited about that. Knowing that our customers — people who believe in our product and use it — were on the other side of the phone made my heart skip a beat. “What a great way to start my work!”, I thought to myself. Yet I was half wrong: Serving our customers wasn’t just the beginning of my career at Gusto, it was the main theme around which all of our daily work revolves. For a company of our size, we specialize into different teams and focus on different kinds of tasks like redesigning landing pages, answering customer phone calls, writing help center articles, or building out a new feature. At the end of the day, it’s all about serving our customers through improving our product and service, and sometimes through serving each other. Cross-functional work happens a lot at the office. Every now and then, folks from the Customer Care team will pull up a chair and sit next to an engineer or a product manager to discuss what they learned from recent customer calls, many of which involve tuning behaviors in the app. Many conversations happen during lunch. A few months ago, while I was sharing a table with folks from our Partner team, they shared that our accountants comprised a significant portion of our users and that this segment could benefit significantly from a better product experience. Their voice was heard and a new EPD (engineering, product, and design) team was developed specifically for that purpose. We also have quarterly help-a-thons where everyone submits ideas about the tools they think could help them to perform their work more efficiently, and engineers will get together to build out some of them. It feels quite amazing to be on the same team with everyone else in the company and working towards the same goal together. Much like playing a team sport, it is very motivating and fulfilling. While implementing tracking and data visualization for the employee directory was not the most interesting task for me as engineer, what motivated me was knowing that this data would really help product managers to gain more insights and make decisions to better serve our customers. I’m empowered, not managed. There are no managers at Gusto. Instead, we have PEs: people empowerers. My PE, Nick, showed me what it means to be empowered. Our 1-on-1s are meetings about me, meetings set to empower me to perform better and grow. In our 1-on-1s, I don’t report to Nick what I accomplished. Rather, I talk about problems I encountered and seek advice and guidance from Nick. A few months in, besides regular tactical 1-on-1s, we started to do monthly introspective 1-on-1s which focus on my personal career growth. In the first introspective 1-on-1, I wrote down my goals for the next two years. For each of the following introspective 1-on-1, Nick and I will first separately write down areas I have improved in the past month and areas of improvements for the following month. During the meeting, we compare what we each wrote down and come up with action items for the next month. For example, we agreed that my past couple projects were heavy in frontend related work and I needed to gain more backend experience. So we came up with action items which included: For Nick, to keep an eye out and allocate more backend related projects to me; and for me, to dig into Rails in order to be prepared for upcoming backend projects. I also felt empowered each time I sought help from my teammates. Every single time I asked a question, instead of getting annoyed by the interruption, they always went beyond simply answering my question and provided extra context to help me better understand the reasoning behind different design decisions. My teammates don’t see my questions as interruptions that slow them down, they treat it as opportunities to help me grow, so we as a team can grow better and stronger for the long run. We value introspection. We believe introspection helps us to stay on track and be proactive instead of reactive. Three times a year, we have company-wide GustoFIED, which is our performance review that stands for Feedback, Impact, Engagement, and Development. There’s also the company-wide annual retreat: Gustaway. This year, we went to Camp Navarro for two days and a night where we relaxed, bonded, and reflected on our values. On each Gustie’s anniversary, he/she will also get a golden ticket to take a vacation anywhere in the world. We are all owners and we speak up. Ownership mentality follows naturally when everyone in the company is empowered to work towards the same goal. At Gusto, I witnessed how engineers act as owners of the business. Coming across others’ code is common when working with a code base contributed by ~60 engineers. There is code written five years ago, and as a startup, the way we write code and the tech we use are evolving quickly. I notice engineers at Gusto don’t have the concept of each owning separate parts of the code. We all own the whole code base together. When engineers see parts that can use some refactoring, they won’t forget about it and do nothing. Instead, they either go ahead and refactor them or pair with the engineer who wrote the code[^1]. Engineers often take initiative and claim things to work on. As owners of the code base, we know it the best and feel responsible to keep it healthy: Even if it sometimes means telling product managers we need to deliberately set aside time in the sprint to tackle tech debt. When you are a owner of the business, you are expected to speak up. When I first joined, I had trouble coming up with items during our bi-weekly retrospectives , yet I noticed everyone else in the team had feedback to offer. As part of the team, I felt responsible to provide feedback and help keep the team stay healthy. It was only after the ownership mentality replaced the default idea of “I’m just here to get my job done and go home” that I started to be more sensitive about how things were going and what we should watch out for. Since then, I noticed how everyone in the company spoke up and raised their concerns whenever they spotted something that could be improved. We don’t care much about format or timing for giving feedback, it is always welcome and valued.The power of culture really shows in this case. Surrounded by people who care about the business and speak up constantly, I feel motivated to do the same thing. We grow and change rapidly. The HR Engineering team was formed right before I joined. Before then, it was the Payroll Experience team. A few months in, a new team, the Partners team, was built within the HR team. Not long ago, the Developer Experience team came to life with the focus of adapting new tools and improving existing tools so engineers can work efficiently and happily. Around the same time, the Engineering Excellence Committee was formed because as we are constantly building out new features, our tech stack evolved and we agreed that we should pay more attention to tech debt. Working in a rapidly growing and changing environment is very exciting. As the team grows, new challenges come along and push us to become better and more mature. You are also expected to grow along with the team. Being able to adapt to changes quickly, take initiative and bigger responsibilities happens all the time. It’s very much like playing a video game: Once you start to get a bit comfortable, a new level of challenges are unlocked. In the last part of the series, I reflect on what I learned during the past 6 months. Most of the lessons were engineering-related and most applicable to fresh college grads without much industry experience. 1: Yesterday, Phan stopped by Rylan’s desk and said “Most engineers accomplish a lot by writing a lot of code. Yet, you accomplish a lot adding minimum amounts of code because you always refactor the old code before writing new ones. I admire you for that. You are my hero, Rylan!” Originally published at sihui.io .", "date": "2017-01-18"},
{"website": "Gusto", "title": "6 Months at Gusto: What I learned - Part 3", "author": ["Sihui Huang"], "link": "https://engineering.gusto.com/6-months-at-gusto-what-i-learned-part-3/", "abstract": "What I did , experienced , and learned during my first 6 months at Gusto After sharing what I did and experienced during my first 6 months at Gusto, it’s time to reflect on my learnings. The following are lessons I learned based on my own personal experiences. While I hope my college fellows will find one or two points helpful, one should keep in mind that I’m still an apprentice myself. What I learned Culture matters a lot. It’s only after being at Gusto for couple months, I began to realize how lucky I was and felt extremely grateful for being able to work at a company with great culture and surrounded by extraordinary people. It’s a place people come to help each other grow and get things done. Fellow college friends: Take company culture into account when you weighing different offers. It matters a lot. It directly affects how you are going to spend your time at work. You want to be productive and grow rapidly. Surrounding yourself with people who share the same values as yours and who you could look up to is a privilege you should seek. One of the things that nudged me to join Gusto was after a full-day office visit and many conversations with Gusties, instead of feeling exhausted, I felt refreshed and even felt I somehow became a bit nicer than usual. It was because everyone I met at the office was extremely friendly, genuine, and passionate. Ask. Ask. Ask. Never hesitate to ask questions is one of the most valuable lessons I learned, and I still constantly remind myself about it. I still get surprised by how much you can learn from a simple question. It helps me to get the right things done the right way. A lot of times, before asking a question, I thought the answer would be a simple yes, no, or a line of code. Instead of getting the answer I originally looked for, a dialog was opened and the question changed from “how can I do this?” to “why I am doing it this way?”, “is this the right place to do this?”, or even “is this method/component really needed in the first place?”. When I bring a question to a senior colleague, it often gets escalated to a bigger context and a higher level. Instead of getting a simple yes or no, I often receive something much better: A better practice I should consider, a pitfall to watch out for, or an existing method/component that lives somewhere in the code base and does exactly what I want. Avoiding unnecessary work and getting things done the right way is much better than making things work and refactoring them later. Asking clarification question is crucial, especially when a task first gets assigned. I learned this lesson the hard way. When I was implementing data tracking for a couple features, our product managers said they needed A, B, and C, which at first seemed to be clear and made sense. Then I went out and implemented what I thought they meant. Only after it was implemented, we realized A, B, and C, meant different things to me versus them Yes, we all speak English[^1]. But for the same words, we might have completely different contexts and assumptions in our head. I learn to ask a lot of questions, not only about what exactly should be done, but also why we need it and how we are going to use it. For example, I was asked to track the number of clicking for each tab on a page. Only after I asked how were these numbers would be used, I figured the purpose was to see if users click around and play with the page. Tracking clicks for each tab individually is a very different work from tracking if tabs are clicked in general. Asking clarification questions upfront offers an opportunity to examine assumptions from both sides. Ask, and you will always thank yourself later for asking. Communication when transitioning projects between people is critical. I took over the survey project from Darin, an Engineer from the growth team who started the project during his two-week rotation to the HR team. Functionality for sending out survey emails was mostly ready, and I started to work on in-app functionality right away. I noticed a couple TODOs left in the code related to sending survey emails. And I did nothing about it because I was too excited about building the next part and simply assumed our product manager was aware of the TODOs. Thankfully, our product manager caught the missing behavior right before launching. Only until then, I recognized the false assumptions in my head and realized what a code monkey I had been. Assumptions can be our biggest enemy, and you need to be especially careful when a project is transitioning from one engineer to another. Alway make sure that everyone — product managers, engineers, and designers — are on the same page before starting. Don’t rush into coding, think like an engineer before putting your hands on a keyboard. Rushing into coding does more harm than good. Just DON’T do that. Timeline estimation is hard. It’s true when people say you should multiply your original time estimation by a factor of 2.5. If you think you can complete a task in a week, say two weeks and a half. Only after working for almost a half year, I started to see how necessary it is to follow this advice. Writing software is something that should not be rushed. The more pressure you have and the less time you get, the less enjoyable the process becomes and the more error-prone your code will be. Do yourself and your teammates a favor, follow the advice. And don’t be afraid to let product managers know you don’t think the given timeline is realistic. Review your (damn) code first. Silly bugs or even typos being spotted during code reviews was one of the moments I felt ashamed of myself. Those errors could be easily spotted if I took a through final look at my changes before requesting a code review. I have to admit I don’t always like to look through my own code. Sometimes it’s because I got tired from continuously coding for a long time but still wanted to get the code landed on time. It’s not only embarrassing to ask people to review your code when it contains obvious bugs, but also rude: The same way you leave a messy kitchen to your roommates. It’s a waste of time for both sides. Don’t rely on anyone else, own your own code. It’s very easy for junior developers to think they can completely rely on senior engineers. At least that’s what I first thought: If the code review passed, the code must be good to go and safe to land. The truth is: none of us are perfect. No one is responsible to think through all different edge cases for your code but yourself. Leaving that burden to your code reviewer is irresponsible. As this series of posts comes to its end, a story comes to my mind: A man came upon a construction site where three people were working. He asked the first, “What are you doing?” and the man replied: “I am laying bricks.” He asked the second, “What are you doing?” and the man replied: “I am building a wall.” As he approached the third, he heard him humming a tune as he worked, and asked, “What are you doing?” The man stood, looked up at the sky, and smiled, “I am building a cathedral!” The past six months was the beginning of my journey at Gusto. As much as I’m excited about continuing to grow and sharpen my skills, I also can’t wait to see the Cathedral we have in mind comes to life bit by bit. 1: Not Ruby nor Javascript, which might actually make things clearer. Originally published at sihui.io .", "date": "2017-01-18"},
{"website": "Gusto", "title": "Bringing Top Talent to Denver/Boulder", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/bringing-top-talent-to-denver-boulder/", "abstract": "(cross-posted from Into Thin Air ) I attended a dinner last week with about thirty influential folks in the Denver/Boulder tech scene (I’m not actually sure how I got invited). The topic was how might we work together to make ours a more attractive region for top-tier tech talent to live and work. There were a lot of good ideas about why people might prefer Silicon Valley, New York, or Seattle to Denver. Most of them revolved around density — the notion that the attractiveness of a region is correlated with the number of good technology companies found within it. What’s interesting is that the Denver/Boulder area does have density. And this led some of us to believe that more people would be drawn to Colorado if they knew about the abundance of technology companies here. For example, these companies all have a significant presence or are headquartered in the Denver/Boulder area: So, why not put up billboards on the 101 in San Francisco and in DIA (owned by The City of Denver) as part of a coordinated marketing campaign with a slogan like: Colorado: where great tech companies and affordable houses are abundant. This campaign would touch both technologists in the Bay Area who might want an affordable lifestyle (the reason I moved to Denver), and the millions who connect through DIA every year on their way to New York or San Francisco or Seattle. Further, many are unaware of anecdotes that support the viability of Colorado’s tech sector. For example, Gusto did its own data-driven survey of Salt Lake City, Austin, and Denver in an effort to find a home for its co-HQ. Denver stood out as the most attractive market. Google is expanding its 350-person Boulder office to 1,500. I heard a rumor that they’re over-subscribed on internal transfers wanting to fill the new 1,150 positions opening up there. It would seem the lifestyle afforded by the region is attractive to many who work for that company. We’re Taking Action Coming back to the dinner I mentioned, a subset of us are coming together to conceive of and execute upon ways we can collectively promote the Colorado technology sector. I’m excited about what we might do together. Stay tuned.", "date": "2017-01-27"},
{"website": "Gusto", "title": "Chris Ferguson is an Engineering Gustie!", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/chris-ferguson-is-an-engineering-gustie/", "abstract": "Following GitHub’s lead , we’re going to start writing about new folks who join our EPD (engineering, product, and design) team here in Denver. We’re stoked to welcome Chris Ferguson as the Technical Architect for our Growth Foundation team in Denver! Chris will be working with his team to re-architect integration between Gusto and systems that support our Sales and Marketing teams (like Salesforce). Chris and I worked together for four years at The Active Network in San Diego, and I’m thrilled to be working with him again.", "date": "2017-02-01"},
{"website": "Gusto", "title": "Matt Lewis is a Denver-Engineering Gustie", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/matt-lewis-is-a-denver-engineering-gustie/", "abstract": "Matt Lewis, a native to Wisconsin (but without the accent for some reason), joins us as our newest engineer in Denver today! Matt spent some time working with Rachio in Denver before starting with the Payroll Engineering pod at Gusto. If you walk by his house, you might glimpse him “working on working on” his KLR650. You’ll find Matt on Twitter @mplewis or at mplewis.com .", "date": "2017-02-09"},
{"website": "Gusto", "title": "Dave Corby is an Engineering Gustie!", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/dave-corby-is-an-engineering-gustie/", "abstract": "We’re thrilled to say that Dave Corby has joined Gusto’s Growth Foundation team in Denver as an engineer! Dave worked as an engineer at Amazon in Seattle for a couple of years. He moved to Boulder and worked with Formation Systems there before joining Gusto. You might very well catch him in the audience at Comedy Works on Larimer St., or indeed one day on stage there.", "date": "2017-02-01"},
{"website": "Gusto", "title": "Onboarding Denver Engineers", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/onboarding-denver-engineers/", "abstract": "(cross-posted from Into Thin Air ) 5 months ago I was the only Gusto engineer in Denver. Now we have 6. All of us are new to the company, and most of us are coming up to speed on the frameworks we use here at Gusto (React, Rails, etc.). Our vision is that Denver-based teams will operate autonomously, owning important pieces of our product. Realizing that vision is made complicated by the fact that there are no tenured engineers here. We’re all “Gusto newbies.” Our first fully-autonomous team is attached to our payroll product. Most of our Payroll Care team is in Denver, and it makes sense to establish a development team here that works closely with that team. But the payroll domain contains our oldest, and most battle-hardened code. Coming up to speed on that domain is tricky given all of the edge cases that are covered within it. So, how do we build an effective payroll development team in Denver when its members are all newbies? The answer: ask tenured engineers to temporarily attach themselves to the team to help it come up to speed. Attaching Tenured Engineers Most of the engineers who have been with Gusto for > 2 years worked on payroll. After all, payroll was the product for a good chunk of our company’s lifetime (we used to be called “ZenPayroll”). So we asked an engineer named Akhil, who was employee 35-ish at Gusto, to help bring our new Denver-based team to coding proficiency by de-mystifying the Payroll system for us. Akhil agreed to fly to Denver for three days a week and dedicate his time to pairing with us as we work on payroll-related features. And it has been awesome. Within the first week we shipped two features to Production. And Akhil has been instrumental in helping us in both understanding the nuances of our design patterns, and providing context around why certain architectural decisions were made. Rotation We also established a program that will see three tenured, SF-based Gusto engineers move to Denver for a three-month period. They will amplify what Akhil has been able to do with our Payroll team, standing as code-related Subject Matter Experts for new engineers we hire into our Denver team. What We’ve Learned Paradoxically, we’ve learned that having strong ties between our two offices is salient to bringing our Denver-based engineering team to a state of autonomy. We have an always-on portal between the two offices to facilitate impromptu conversation. But more importantly, having SF-based engineers take time to be physically present with us in Denver has been hugely helpful, and this an approach I’d recommend to any company looking to set up a new engineering location.", "date": "2017-03-10"},
{"website": "Gusto", "title": "Kendra Lyndon is a Denver-Engineering Gustie!", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/kendra-lyndon-is-a-denver-engineering-gustie/", "abstract": "(cross post from Into Thin Air ) Kendra Lyndon , a Seattle native, is our newest Denver-based engineer at Gusto! This is Kendra’s first gig after graduating from Galvanize, and she’s already making a (positive) dent in our codebase only two-weeks in. And if you’re lucky, you might hear Kendra keeping the piano-tuning industry alive as she plays Claire de Lune on any idle piano she might pass by (she’s classically trained).", "date": "2017-03-23"},
{"website": "Gusto", "title": "Modularizing Salesforce Integration at Gusto", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/modularizing-salesforce-integration-at-gusto/", "abstract": "(cross-posted from Into Thin Air ) On March 9th, 2017 I gave a talk at our office in Denver about an architectural approach we’re taking to modularize our codebase. Of note, this approach is experimental and is not widely implemented at Gusto. Sixty or so Denver-based engineers attended, and we had a great Q&A (and beer-drinking) session afterward. Here’s the talk in written form: I’ve been doing this for a while. “Monolithic” codebases are ones where all of the source code gets contributed to a single repository, which bloats over time. In order to understand how this happens, it’s important to contemplate what’s at stake for an early-stage company. Gusto started in late 2011. We were called “ZenPayroll” back then. And there was no certainty that the company would be successful. Scalability lacks primacy in a company’s early days. Instead, the company needs to iterate quickly and prove that is solving actual business problems with its product and becoming “ default-alive .” But the byproduct of striving to reach the default-alive state is that code piles up in a single repository. Now, in early 2017, the “zenpayroll” repository has signs of bloat. And this manifests in several ways. Gusto’s Engineering, Product, and Design team is divided into five product areas. While “Gusto” now comprises dozens of independent applications, much of the code for these product areas exists within the same repository. And no one can deploy code to Production unless every team’s tests pass. Sometimes, teams introduce “flaky tests” — tests that fail under certain conditions (such as a date change from 12/31 to 1/1). And one team’s “flaky” test can hold up the deploy pipeline for a distant feature. Plus, it takes several minutes to run all of the tests within this repository. This is a long time for a developer to wait to understand if her commits play nicely with everyone else’s code. Further, we see features from a given sub-domain littered throughout the codebase. For example, one team works to synchronize Gusto data with Salesforce. Our Sales and Marketing teams rely on this data to manage leads generated on our software. The word “Salesforce” appears thousands of times throughout our main repository. Let’s understand how “zenpayroll” synchronizes with Salesforce. Every month we issue payment invoices to our customers. This data is used, in part, by our Sales team to calculate commissions within that system. So, when a payment invoice is created or changed, we synchronize it with Salesforce through their API. We then store Salesforce IDs to help with de-duplication. “Zenpayroll” knows a lot about Salesforce… class PaymentInvoice < ActiveRecord::Base\n  ...\n\n  include Salesforce2::RealtimeSynchronizable\n  include Salesforce2::Mapping::PaymentInvoice\n  \n  ...\n end The PaymentInvoice class knows about Salesforce… module Salesforce2::RealtimeSynchronizable\n  extend ActiveSupport::Concern\n  include Salesforce2::Synchronizable\n\n  included do\n    after_save :trigger_salesforce_synchronization, if: :synchronization_enabled?\n    before_destroy :trigger_salesforce_destroy, if: :synchronization_enabled?\n  end\n\n  private\n\n  def trigger_salesforce_synchronization\n    Salesforce2::Manager.perform_async(self.class, self.id)\n  end\n\n  def trigger_salesforce_destroy\n    Salesforce2::Manager.perform_async(self.class, self.salesforce_key, :remove)\n  end\nend And we have a module, that gets “mixed in” to the PaymentInvoice class in order to do “Salesforce-y” things. module Salesforce2\n  class Manager\n    include Sidekiq::Worker\n    ...\n    def perform(synchronizable_class, synchronizable_id, operation = :synchronize)\n      synchronizable_class = Object.const_get(synchronizable_class) if synchronizable_class.is_a?(String)\n      if (:remove == operation)\n        remove(synchronizable_class, synchronizable_id)\n      elsif (:synchronize == operation)\n        return unless ((synchronizable = define_synchronizable(synchronizable_class, synchronizable_id)).present?)\n        synchronize(synchronizable)\n      end\n    end\n    ..\n  end\nend And then a Manager class gets involved, whose purpose is to add Salesforce business logic to the API call… module Salesforce2\n  module Mapping\n    module Fields\n      def salesforce_fields(salesforce_object_type, params = {})\n        method = \"salesforce_#{salesforce_object_type}_fields\"\n        self.send(method,params.merge({ has_been_synchronized: self.has_been_synchronized? }))\n      end\n    end\n  end\nend And, we map PaymentInvoice attributes to Salesforce fields… Yes, it’s a lot of Salesforce-related code within the “zenpayroll” codebase. In an ideal world, “zenpayroll” knows nothing about Salesforce. This might help free it up to focus exclusively on its domain. https://martinfowler.com/articles/201701-event-driven.html In this New World, “zenpayroll” broadcasts changes to its domain. Interested services, such as our “Growth Foundation Service,” can subscribe to these broadcasted events, and do work with them. What’s interesting about this pattern is that event data is carried with the broadcast, so there’s no need for a service to query “zenpayroll” for supporting data. Further, “zenpayroll” necessarily has no idea that its payment invoice data will be picked up by the Growth Foundation Service and synchronized with Salesforce. It merely informed the world that a payment invoice record was changed. Kafka is to events what air is to sound waves — it’s the medium through which data is transported. We experimented with other messaging systems, such as Amazon SQS, but went with Kafka out of FIFO considerations (at the time, SQS could not guarantee “first in, first out” message delivery). First, we created a gem that would provide all of the plumbing required for our “zenpayroll” Rails project to transmit events to Kafka. After including the gem, we add a simple mixin to an ActiveRecord class to publish events. publish_to_kafka 'company_migration', async: true, template: 'company_migrations/external' It was inspired by the paperclip gem , and we can see it in use here on the PaymentInvoice class within the “zenpayroll” repository. Instead of including all of that “Salesforce-y” code within the class, we simply publish a message to Kafka indicating there’s been an update to the object. And that’s made possible by including this “publish_to_kafka” mixin, which hooks into the Rails callback pipeline to do its broadcasting. The Kafka payload is JSON. (note, the KafkaRails gem is only available inside Gusto…for now.) As this architecture is experimental, we decided to use the Heroku Kafka plugin to host our Kafka (and Zookeeper) instance. class PaymentInvoiceService < Service\n  TOPIC = 'zenpayroll.payment_invoice'\n\n  def initialize\n    super(TOPIC, [Publishers::HerokuConnectPublisher])\n  end\n\n  def on_receive(topic, message)\n    Rails.logger.info(JSON.pretty_generate(message))\n  end\nend Here’s a basic example of the PaymentInvoiceService microservice, part of the broader Growth Foundation Service depicted in the event sequence diagram, that subscribes to “zenpayroll” PaymentInvoice events. This example is pretty simple, as it merely logs its payload to the console and log file. If a given service is expecting a certain JSON structure, and said structure changes, the service is compromised. Payload changes need to be managed carefully. While Kafka helps with “first in, first out” processing order, it’s still not guaranteed. This is especially so in a situation where, for example, a user makes changes to a payment invoice in rapid succession on the UI, but the system publishes them out of order. The Growth Foundation Service could update Salesforce with outdated data in this situation. Our implementation of “Event-Carried State Transfer” is unidirectional and doesn’t work for systems requiring synchronous communication, or acknowledgement that a given message has been processed. And, of course, it’s much harder to debug a distributed system than one that’s monolithic. Code ownership has much clearer team boundaries. If the Growth Foundation Service produces an error, we know which team is responsible for it. We can use the Event-Carried State Transfer pattern to update multiple distributed systems with one message. For example, we might consider concurrently synchronizing Salesforce and our data warehouse with a single message. As the “zenpayroll” codebase shrinks, tests run faster. And tests within new services we create will run quickly, too.", "date": "2017-03-31"},
{"website": "Gusto", "title": "Memoization in Ruby (made easy)", "author": ["Kirill Klimuk"], "link": "https://engineering.gusto.com/memoization-in-ruby-made-easy/", "abstract": "Whenever I write any sufficiently large Ruby app, I end up writing an expensive computation. I end up calling that computation over and over again even if the result is the same. I realize that’s a silly waste of resources and decide to save the result. From that experience, I’ve learned a few standard patterns for memoization and used them too many times to ever want to rewrite them again. If you sail along with me on this journey, I’ll take you down memoization river with its many turns and bring you to the DRY land of a general solution. Solving a Totally Contrived Example The Problem The journey starts out simple: I write an expensive query. def users_with_discounts\n  User.includes(payment_plan: :discounts).where(paying: true).to_a\nend But alas, I end up calling this users_with_discounts method quite a bit all over the place. It takes a while each time, so I wanna keep the results around. 1. A One-Line Solution My next step is to memoize the result by saving it to an instance variable. The ||= operator makes this easy for me since if the instance variable is already defined, I’ll get back my value, but, otherwise, I’ll go ahead and do the query. def users_with_discounts\n  @users_with_discounts ||= User.includes(payment_plan: :discounts).where(paying: true).to_a\nend 2. A Multi-Line Solution We take another turn: now I need to do some post-processing for the query. That means it’s not as nice to use the ||= operator for memoization anymore. Ugh. No worries, though: I’ll check if the the instance variable is present manually in the first line, and if not, do the processing and save it away in the last. def users_with_discounts\n  return @users_with_discounts unless @users_with_discounts.nil?\n  \n  users = User.includes(payment_plan: :discounts).where(paying: true).to_a\n  @users_with_discounts = users.select do |users|\n    users.payment_plan.discounts.any? && !users.payment_plan.delayed?\n  end\nend A Gotcha — Nil I might as well mention now that in the case where our value returns nil for the previous solutions, I’ll end up doing the re-computation every time. Imagine if we had: def users_with_discounts\n  @users_with_discounts ||= computation_that_returns_nil\nend computation_that_returns_nil gives back nil . That means that the ||= operator will set @users_with_discounts to nil . Which means that as soon as users_with_discounts is called again the operator will find a nil value for @users_with_discounts and try to set it again. And so the merry-go-round will go. 3. Dealing with Nil — with a Sentinel Value One of the ways I can solve this is to use a sentinel value in order to signal that we don’t need to do this computation over and over again. If I find that our computation can return nil , I can intercept the result and set it to false , or even [] , or (if you’re on on the functional train) I could use the Nothing part of the Option monad . Please do note that I still don’t use my good friend ||= . It swallows false in the same way it swallows nil . def users_with_discounts\n  return @users_with_discounts unless @users_with_discounts.nil?\n  @users_with_discounts = computation_that_returns_nil || false\nend However, using this solution also has a side-effect. While my unmemoized method used to return a Maybe[ArrayOf[User]] , now it’ll be returning the ever more complicated Or[Boolean, ArrayOf[User]] . If I don’t want to randomly change expectations of what this method returns, I’ll have to be more crafty. 4. Dealing with Nil — with a Cache The other way I can deal with the presence of nil is by putting it behind a cache. For the purposes of that cache, I’ll be using Ruby’s handy Hash . def users_with_discounts\n  @users_with_discounts ||= {}\n  if @users_with_discounts.has_key?(:return_value)\n    @users_with_discounts[:return_value]\n  else\n    @users_with_discounts[:return_value] = computation_that_returns_nil  \n  end\nend The first time we call users_with_discounts , it will have its return_value key set. That means that on further calls, regardless if that key was set to nil or not, I’ll still have the result of the computation directly returned to me. I also end up with the same exact type for our result as when we started. A Gotcha-Methods with Parameters Let’s say that now I’d like to do the query with some changeable arguments. Does that mean I should stop memoizing? That depends. Do I expect the result to be the same for a given set of arguments? Am I okay with storing the results for each set of arguments? If I say yes to both of the above, hope is not yet lost. I can still memoize my results. I just can’t do it the same way as I did above. Since the output of any function like 1+x is different depending on x , I’d have to take arguments into account. 5. Dealing with Methods with Parameters Getting back to our query, I answer yes to the two questions above and memoization is in the stars. I can re-use the caching solution with a key difference: I will use the arguments as a key. def users_with_discounts(scoped_to={})\n  @users_with_discounts ||= {}\n  return @users_with_discounts[scoped_to] if @users_with_discounts.has_key?(scoped_to)\n\n  users = User.includes(payment_plan: :discounts).where(\n    paying: true, \n    **scoped_to\n  ).to_a\n  \n  @users_with_discounts[scoped_to] = users.select do |users|\n    users.payment_plan.discounts.any? && !users.payment_plan.delayed?\n  end\nend Now, for every scoped_to that is passed in, we’ll store its results away for reuse in our hash — making every future call next to instantaneous. Why Did You Make Me Read All This? A fair question. Honestly, it’s so that you can both (a) commiserate in the problem that I’ve had to solve like a half bajillion times and (b) wonder why I’m still solving this problem after I’ve solved it like a half bajillion times. That’s precisely what I’ve been wondering. I’ve been waiting for the day I can just write a memoized on top of my method and forget I ever had to suffer through one of these implementations forever. memoized\ndef users_with_discounts(scoped_to={})\n  users = User.includes(payment_plan: :discounts).where(paying: true, **scoped_to).to_a\n  users.select do |users|\n    users.payment_plan.discounts.any? && !users.payment_plan.delayed?\n  end\nend Needless to say, that day has come. It is the general, DRY solution I’ve promised you all the way at the start of this post. A Generalized Memoizer module RubyMemoized\n  class Memoizer\n    attr_reader :context, :method\n\n    def initialize(context, method)\n      @context = context\n      @method = method\n    end\n\n    def call(*args, &block)\n      return cache[[args, block]] if cache.has_key?([args, block])\n      cache[[args, block]] = context.send(method, *args, &block)\n    end\n\n    def cache\n      @cache ||= {}\n    end\n  end\n\n  def self.included(klass)\n    klass.extend(ClassMethods)\n  end\n\n  module ClassMethods\n    def memoized\n      @memoized = true\n    end\n\n    def unmemoized\n      @memoized = false\n    end\n\n    def method_added(method_name)\n      if @memoized\n        @memoized = false\n\n        unmemoized_method_name = :\"unmemoized_#{method_name}\"\n        \n        memoizer_name = :\"memoizer_for_#{method_name}\"\n        define_method memoizer_name do\n          memoizer = instance_variable_get \"@#{memoizer_name}\"\n          if memoizer\n            memoizer\n          else\n            instance_variable_set \"@#{memoizer_name}\", Memoizer.new(self, unmemoized_method_name)\n          end\n        end\n\n        alias_method unmemoized_method_name, method_name\n\n        define_method method_name do |*args, &block|\n          send(memoizer_name).call(*args, &block)\n        end\n\n        @memoized = true\n      end\n    end\n  end\nend Its Power is Immense I wanted this to be a nice and lazy way of solving my memoization needs. So to prove that it’s exactly that, we’ll (crudely) solve the problem of finding the nth Fibonacci number. class FibonacciCalculator\n  include RubyMemoized\n\n  def calculate(n)\n    return n if (0..1).include?(n)\n    calculate(n - 1) + calculate(n - 2)\n  end\n\n  memoized\n\n  def memoized_calculate(n)\n    return n if (0..1).include?(n)\n    memoized_calculate(n - 1) + memoized_calculate(n - 2)\n  end\nend The calculate method is pretty lame. It never stores solutions to its sub-problems and always recalculates them. The memoized_calculate method is its cousin that does. On the surface, it doesn’t seem like there’ll be much of a difference between the two implementations. However, with the power of benchmarking, I will prove to you it is not so. require 'benchmark'\n\nBenchmark.bm do |measurer|\n  calculator = FibonacciCalculator.new\n\n  measurer.report 'with memoization' do\n    10.times { calculator.memoized_calculate(35) }\n  end\n\n  measurer.report 'without memoization' do\n    10.times { calculator.calculate(35) }\n  end\nend When we run the above code, here’s the breakdown we receive: user       system    total     real\nwith memoization    0.000000   0.000000  0.000000  ( 0.000469)\nwithout memoization 46.010000  0.160000  46.170000 (46.597599) Memoization wins 0.005 seconds to 46.598 seconds. I’d say I got my memoization needs covered. And so can you! And Here’s How It Works The Memoizer class contains basically everything from our discussion above: looking at line 11, we can see the hash cache pattern from solution (4) above. Not only that, but we also see a more aggressive version of solution (5) for any arbitrary set of arguments and block. All of that is powered by our original idea to store data in an instance variable from solution (1) in the cache on line 16. The rest of the code is quite a bit of metaprogramming. Here’s a rundown of what it does: Whenever a method is added after the memoized class method is called, it will create a few new methods — the old unmemoized method will be renamed (line 49) and the new memoized method (line 51) will call a memoizer that was created specifically for it (line 40). That means that this memoization will work on a per instance and per method level. The memoized and unmemoized class methods end up working in much the same way that the public and private methods do — everything underneath them will either be memoized or not up until the next call to either one of them. But There are Tradeoffs Remember the two questions I posed when we were talking about memoizing methods with parameters? Let’s re-examine them again in the context of our new generic memoizer: Do I expect the result to be the same for a given set of arguments? One way of reframing this questions is to ask, “ Are the methods pure? ” In other words, given a set of inputs, we will always get the same outputs. This won’t be true if we’re manipulating the state of our object to make the computation, since the state will need to change in between method calls. That means the output of our method will be different every time. For example: the method def sum(x); 1 + x; end is a pure but def sum(x); 1 + x + external_number; end is not. So why does this matter? If we were working with the second method and using our memoizer, we’d get the wrong answer any time external_number has changed. That would certainly be unexpected. Here’s a dirty secret: our beloved users_with_discounts method is not pure. The database state is external and can change at any time, so the same inputs will not give me back the same outputs. Then why did I say yes to my question? In this particular case, I wanted to have a particular set of results to work through. If the state of the database had changed during the time I was manipulating my results, I didn’t want to see them — so I was effectively pretending that my method was pure. If new users with discounts came up during this time, I’d rather process them in a separate job that was running over just these new ones than having the first method call return one set of results and the other method call return another set — causing some changes to be applied to one set of objects but not to another. Am I okay with storing the results for each set of arguments? It uses a bit of memory to reduce the amount of processing work. So, if you’re storing millions of ActiveRecord records in that computation, remember that your process size will balloon for the duration of your computation. It’s also important to remember that if you end up calling the method with thousands of arguments that you’ll be storing thousands of results sets. In either case, remember to make this memory available for garbage collection with a neat trick: assigning the variable for the object that did the computation to nil (in some scenarios, running GC.start does not necessarily hurt either). Conclusion I suffered through many implementations of memoization. You saw me suffer through an example. I hope you don’t have to suffer. That’s why I put together this gem: https://github.com/kklimuk/ruby_memoized . Just include it into your Gemfile with gem 'ruby_memoized' and get memoizing today! Acknowledgments Thank you to Justin Worth, Matt Wilde, and Yi Lang Mok for reviewing this article for both form and substance. This article was originally published here .", "date": "2017-04-10"},
{"website": "Gusto", "title": "How ACH works: A developer perspective - Part 3", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-3/", "abstract": "At Gusto , we rely heavily on the ACH network to pay employees and to remit various payroll taxes to federal and state agencies on behalf of our clients. In part 1 of this post, I outlined the basics of how we originate ACH debits and credits. In part 2 of this post, I outlined how ACH returns are handled. Part 3 of this post will go into the timing of these ACH transfers. As with the previous post, I'll talk specifically in the case of ACH debits (i.e. us debiting money from a customer's account). Timing on ACH credits is exactly the same. Leg 1: Originator -> ODFI (our bank) by Day 1, 7:00pm Depending on who your bank is and the agreement you've made with them, they will ask you to SFTP your ACH file to them by a certain time for processing. This is sometimes referred to as the ACH cut-off time. Here at Gusto, we have a great relationship with Silicon Valley Bank , who has an ACH cut-off time of 7pm. Once the 7pm deadline hits, our bank will validate that our ACH files pass a series of sanity checks (account numbers makes sense, amounts don't exceed certain underwriting thresholds, etc.). They'll also take steps to verify that the files came from us, and not someone pretending to be us. Leg 2: ODFI -> Federal Reserve -> RDFI (receiver's bank) by Day 2, 12:01am Once our bank has deemed our ACH file acceptable, they'll forward the ACH file to the Federal Reserve for processing. The ACH protocol is a next-day settlement system. That means that ACH debit requests sent to the Federal Reserve are processed around midnight and made available to the RDFI (receiving bank) around the same time. Leg 3: RDFI -> Receiver by Day 2, ~5:00am The receiving bank will pick up the notification of the credit sometime in the morning when they open for business (let's say 5am for simplicity's sake) and will decrement the funds in their customer's account at that time. As you can see, timing for non-returned ACH transfers is quite straightforward: ACH files originated before 7pm are settled the following morning. ACH Returns Things start taking longer when an ACH file is returned. Leg 4: RDFI -> Federal Reserve -> ODFI by Day 4, 12:01am When the RDFI receives word of the ACH debit at the start of day 2, they are given until the end of the next business day to tell the Federal Reserve that they want to return the ACH debit. Sometimes, a bank moves quickly and will notify the Federal Reserve by the end of the same day. Most of the time, however, the banks will notify the Federal Reserve as late as possible, which is the end of day 3. Once the Federal Reserve receives a return, they will let ODFI know that the ACH debit was returned that evening. There is a notable exception to the next-day deadline: If the customer notifies the bank that they did not authorize the ACH debit (for example, in the case of a fraudster using a stolen bank account), the RDFI is allowed 60 days to return the ACH debit. Because of this, the ACH protocol is very consumer friendly, since the originator of the ACH debit must now return the money they debited and try to get back whatever was given in return for the debit. Leg 5: ODFI -> Originator by Day 4, ~5:00am The ODFI bank will pick up the notification of the return sometime in the morning when they open for business (again, let's say 5am for simplicity's sake) and will forward it on to the originator. Below is another way to visualize the ACH timeline when there are returns involved. It’s important to note that the ACH system never provides positive confirmation that an ACH debit has gone through successfully. The only response an ACH originator may get is one notifying them of a return. Because of this “no news is good news” policy, it is wise for originators of ACH debits to wait for 3 additional business days of “no news” to ship their product to the customer. Though the ACH system is described as a next-day settlement system, in practice, it is not because of this. So, for example, a business that submits an ACH debit on a Monday generally should not consider the funds to be good until Thursday morning. An ACH debit submitted on a Wednesday should not be considered good until Monday morning. A faster protocol? Because of the inherent delays in the ACH system, alternate protocols and means of transferring money (for example, Dwolla and Bitcoin ) have begun to see pockets of popularity. Their biggest downside of course, is their lack of widespread adoption. Nearly all banks today are part of the ACH network and $39 trillion are moved through the ACH system annually. An addendum to the ACH protocol to support same-day ACH settlement is something that is almost unanimously desired by the ACH community. The good news is that NACHA, the governing organization behind ACH, has recently announced plans to roll out a same-day ACH protocol. The challenge is coordinating the adoption of the new protocol across all participating banks. Once fully implemented, the same-day ACH system would likely cut one day from the timelines outlined here. Parts 1, 2, and 3 of this series should give a pretty good overview of the ACH protocol at a high level, although for the sake of clarity, much has been simplified in these posts. There are tons more details and nuances that make ACH payments a very interesting problem. At Gusto, we move billions of dollars annually through the ACH network and we're exploring ways to shorten these timelines for our customers. If problems of this nature look interesting to you, we're hiring talented engineers . If there's interest in the comments, I'll write a part 4 of this post, detailing the actual ACH file format (spoiler alert: It's a fixed width file format that you can generate using Fixy . Comments on Hacker News", "date": "2014-07-08"},
{"website": "Gusto", "title": "What it’s Like Coding in a Large Team", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/what-its-like-coding-in-a-large-team/", "abstract": "(cross-posted from Into Thin Air ) In 2003 I joined a small startup in San Diego called ProfitLine as a software engineer. I was part of an eight-person development team, comprising one development manager, three database administrators, and four software engineers. By Gusto standards, coding there was the Wild West. Back then, the Capability Maturity Model , or CMM for short, was a measure of a development shop’s maturity. A score of 0 meant that progress was achieved mostly through heroics. A score of 5 meant you were working for NASA, with every piece of code diligently documented, tested, and endorsed by stakeholders (usually with an actual wet signature). I think our shop was a 0. And this was by design, as it should be for most startups. Our business was still in the throes of proving its value to the market. And our mandate was to build and test new features quickly. In order to do this, we de-prioritized automated testing and instead leaned heavily on our human QA team for feature quality. We were an ASP.NET shop and worked predominantly in C#. AJAX hadn’t been invented yet, and every click of a button produced a synchronous server-side request that made itself known to the user with an intermittent blank page as the browser loaded a fresh HTML document. The document came with a hidden encrypted string called “View State” representing server-side state for reconstitution in the next request. There was no JQuery or AngularJS or React. We rolled our own Javascript, having to solve for all of the browser permutations (IE 6 and Safari mostly) on our own. But our use of javascript was limited, as instead we opted to use the magic of the ASP.NET framework which wanted server-side visits for nearly every UI permutation. There was no GitHub. Instead, Visual Source Safe was touted as the preferred Microsoft source-control system. And it was prone to drop commit history due to algorithmic flaws that manifested during branching and merging. Importantly, we didn’t do code reviews. GitHub’s pull request feature has introduced the code-review culture we used to see only with open source projects to companies. Today, almost every development shop in “Silicon Valley” does code reviews, where code changes are always scrutinized and approved by a fellow engineer before they’re committed to a mainline branch. In 2003, this culture wasn’t pervasive. When I left ProfitLine in 2005, we were still operating at CMM level 0. I left to take a job with KPMG in Australia, where I would do only pockets of software development based on project needs. And when I returned to the States, I took a job on the management side of things with Active.com and didn’t write code professionally for about five years. In 2012, I co-founded a San Francisco-based startup as the CTO, and took the habits I learned while coding at ProfitLine with me. We didn’t grow much. Five years later I’m a software engineer at Gusto. Writing code here is very different than writing code for a five-person company searching for survival. Don’t merge. Rebase. My first lesson was that merging the development branch into my feature branch to keep it up to date was bad. Instead of merging, we rebase in order to maintain a linear commit history and reduce the noise in a given pull request. Test code structure matters, too. Most of the code I write here at Gusto is code that tests code, as test coverage is critical for a fast-moving organization like ours. Indeed, much of what a code review focuses on is the structure of testing code, which felt foreign to me at first. Test everything. A lot of startups are ditching their QA teams in favor of building an engineering culture that takes responsibility for quality. And this doesn’t work without good test coverage. Changes happen. A lot. We have a lot of developers working on our codebase, and the mainline branch is constantly being updated with new commits. At first I went rebase crazy when working from a feature branch. Over time, I developed a cadence (rebase every few days) that kept my feature branch relatively current while also preserving sanity. Production deployments happen several times per day. When I worked at Active.com we deployed code every few weeks. And it was a big ordeal. IT gave us forms to fill out and a calendar to work with for scheduling. Releases could take no longer than the allotted time and needed a clear, documented rollback plan that would be implemented if we fell behind schedule. It was very, very stressful. At Gusto, we run continuous testing automation through Buildkite, and developed a bot called “Sensei” that integrates with Slack, Jenkins, and Buildkite to release code to Production. Developers are empowered to trigger software releases using these tools at any hour of any day. Depth is everywhere. It would be hard to program without search engines or Stack Overflow. Indeed, questions like “Should I upgrade to React 15?” or “How can I achieve zero-downtime when deploying schema changes?” can be researched online. But there’s nothing like sitting down with a fellow engineer who is well informed on the topic and becoming informed through bi-directional communication. Specialization emerges as development teams increase in size. Indeed, at Gusto we have specialists who “go deep” on narrow aspects of our stack, and who act both as thought leaders and advocates for their area of expertise. I find I can lean on folks to help resolve esoteric issues here more than I could anywhere else I’ve worked. In conclusion, I’ve been working in the technology space for some 17 years, and I feel like 40% of what I know has materialized in the 9 months I’ve been working with the numerous and talented engineers here at Gusto. It’s intense and rewarding to say the least.", "date": "2017-08-16"},
{"website": "Gusto", "title": "6 Months at Gusto: What I did - Part 1", "author": ["Sihui Huang"], "link": "https://engineering.gusto.com/6-months-at-gusto-what-i-did-part-1/", "abstract": "What I did , experienced , and learned during my first 6 months at Gusto Today marks my six-month anniversary at Gusto . As time flies by, I learn more and more about Gusto: its mission, its culture, and most importantly, its people. The more I learn, the more grateful and lucky I feel for being part of it. As I graduated from college and transitioned into a young professional, the past six months have come with a lot of changes. Many of my friends who are still in college ask me about my experience at Gusto and what I have been learning. So I decided to write a three part post to share what I did , experienced , and learned over the past six months at Gusto, from a fresh college grad’s perspective. What I did Six-week Engineering Onboarding Implemented features I started coding the first day I joined the team. My first task was meant to be an introduction to our code base and regular workflow: The task was simple enough so that I didn’t get overwhelmed, yet at the same time complex enough to give me a taste of how it’s like working with our code base. After the “appetizer”, I started to implement a bigger feature that touched our full stack. Did a lot of pair programming We are big fans of pair programming. Throughout my onboarding, I paired with a lot of different engineers. I had at least five pairing sessions per week for the first two weeks. Sometimes we worked on my task, sometimes we worked on theirs. Working with more seasoned engineers not only got me familiar with our tech stack quickly, but also gave me the opportunity to learn about each engineer as a person. Attended a series of intro courses taught by Gusto engineers Throughout my 6-week onboarding, a series of classes were scheduled. There were classes focused on high level engineering concepts, from how we move money around through ACH to our overall architecture. There were also more practical classes that went over our tech stack, from Rails basics, security, asynchronous services, and maintenances, to hands-on frontend labs on React.js and Flux. Shadowed a Viking Master, a.k.a. on-call engineer, for a week At Gusto, each engineer take turns to be a Viking Master[^1]. Surprisingly, I enjoyed being a Viking Master a lot. I’ve heard some horror stories about being on-call, mostly involving pagers and being woken up in the middle of the night. Luckily, not many people run payroll at 3 in the morning, and we don’t carry pagers around. Being a payroll Viking Master is more like being a detective. For each ticket, I would try to reproduce the error, locate related source code, and fix it. It was a lot of fun because you never know what you would learn along the way. I picked up some nuances of Rails[^2] which I might have never encountered otherwise. Being a Viking Master also pushes you out of your comfort zone: Each JIRA ticket is like a train ticket that takes you to a part of the code base that you might not have even noticed before. You are forced to familiarize yourself with the new code as fast as possible in order to resolve it in an efficient manner. While tracing a bug through the code base is quite fun in itself, what’s more exciting is pairing with the author who wrote the code and fixing the bug together. Most of the time, after spotting the part of the code that causes errors and getting a sense of the underlying problems, pinning the author[^3] and pairing with him/her is the most effective approach. Last but not least, reading through JIRA and Zendesk tickets definitely brought me closer to our customers, which not only gave me more empathy towards them and provided insights of how our users interact with our app, but also motivated me to make our service better. Two weeks of rotation At Gusto, everyone is empowered to do the best work of their lives. Although each engineer has a pre-assigned team, we all spend two weeks rotating to one or two other teams in order to gain a more holistic view of different parts of our technology and product. I initially joined the HR Engineering team and chose to rotate to the Core Payroll team and the Payments Platform team, each for a week. I picked these two teams because Payroll was the cornerstone of our business, and engineering the system that moves tens of millions of dollars per day sounded fun. The rotation was very beneficial: It helped me to gain a better picture of the engineering team as a whole and also laid out some foundation for cross-team tasks I encountered later.[^4] Leaving My Fingerprints After my onboarding, the HR team worked hard to launch a brand new employee directory. I contributed in various areas such as implementing small missing pieces, converting existing Backbone.js components into React.js components, as well as working along side with product managers and the data team for tracking and visualizing usage data of our new features. After the successful launch of employee directory[^5], I started to work on our employee survey feature [^6]. Software development is itself a lot of fun, and the team you develop the software with and how you work together can make it even more enjoyable and fulfilling. I will share what I experienced (Part 2) and learned (Part 3) during my first six months at Gusto, mostly from an engineering perspective. 1: Gears for VMs to wear 2: One nuance I learned was: the way ActiveRecord saves NestedAttributes by declaring autosave: true might cause some strange behaviors. Say there is a nested numeric field, Hour , for the Payroll model. Calling valid? on a payroll object after changing its nested hour from 7 to 7. will return true . But it shouldn't because 7. is not a valid numeric field. Strangely, if the record is changed from 7 to 8. , calling valid? on the payroll object will correctly return false . After digging into the source code, I figured the reason was ActiveRecord didn't think the nested field hour had been changed when a tailing decimal was added. _field_changed? uses type_cast to determine if a file has been changed. And type_cast turns 7. back to 7 which in fact, stops hour from being added to changed_attributes , which is used by changed? to determine if the entity has been changed or not. And changed_for_autosave? uses changed? to determine if a NestedAttribute has changed. 3: Did I mention git blame ? 😏 4: We do have engineers jump around teams. Phan, one of the engineers who seems to know every part of the system, has jumped from Payroll, to Benefit, to DevOp, and back to Payroll. It's also not uncommon for engineers to take on projects that doesn't belong to their team. 5: and my 3 week Asia trip 6: The project was started by Darin, an engineer in the growth team, during his two-week rotation to our team. Originally published at sihui.io .", "date": "2017-01-18"},
{"website": "Gusto", "title": "“The Engineer”", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/the-engineer/", "abstract": "(cross-posted from Into Thin Air ) “Oh, you’re ‘The Engineer’!” “Uh, hi, I’m Jeremy.” “We’re all so excited you’re here!” “As am I!” This was a common exchange I had with “Gusties” (what Gusto calls its employees) in the Denver office as my first day here began. For some reason, my arrival had been eagerly anticipated, and I was curious to understand why. You see, Gusto is well-established in San Francisco. There it has an office of over 250 people on 3rd and Bryant, with about 20% of them being software engineers. Business has been great, and in July of 2015 Gusto opened an office in Denver in order to accelerate growth. The aim wasn’t to create a satellite office that would be perennially subservient to the will of the mothership. No. Denver would be a co-HQ. And salient to this strategy would be the establishment of what Gusto calls an “EPD” team (Engineering, Product, and Design) of equal weight and strategic importance to San Francisco. During the interview process, this history was made known to me, and I knew what I was getting into. But it wasn’t until I started here in Denver that I fully understood how important engineering was to folks who had been working so hard to build the office. Gusto’s Care and People teams are already rockin’ in Denver. EPD is the last missing piece. After a few days here, I’ve realized what makes people excited isn’t me — it’s what happens after me. My job is to establish a beachhead through which other engineers follow. The process will be methodical. The bar set by our teams in San Francisco will be identical in Denver, which means hiring will be slow and deliberate. Silicon Valley-based companies are starting to appreciate the advantages of co-locating outside the Bay Area. The high cost of living there has turned places like Denver, Colorado, into great markets for engineering talent. Transparency is lauded here at Gusto, and we’ve decided to write about what works and what doesn’t as Gusto builds its new EPD team in Denver. Our hope is that other companies can learn from this experience, and perhaps give us feedback to improve ours in return. As of now, I’m the first and only Gusto engineer in Denver. There’s a long road ahead! Oh, and if you’re an engineer in Denver and you’re interested in learning more, we’re hiring: gusto.com/careers .", "date": "2016-11-02"},
{"website": "Gusto", "title": "Embracing Functional Programming in Ruby", "author": ["Kelly Sutton"], "link": "https://engineering.gusto.com/embracing-functional-programming-in-ruby/", "abstract": "This post was originally posted on kellysutton.com . This message has been modified to fit your screen. At Gusto , we’ve been knee-deep in a substantial refactor of our system for running payrolls. Running a payroll requires taking several different inputs such as how much an employee should get paid, where did they work, how much did they work, how much taxes should they pay, how much taxes have they paid this year, and so on and so on. As a company that offers a payroll service, keeping this piece of the system in tip-top shape is important for the business. Customers love Gusto for its simplicity and speed when it comes to running payroll. Over the years, this system grew beyond its original mandate. Rather than just serve payroll for one state, it now serves them for all 50 states and the District of Columbia. Although customers love our payroll, internally new engineers had a difficult time understanding the code and making changes safely. This system needed a tune-up, so we embarked on a sizable refactor. Because the process of calculating what you need for a payroll is one big formula, we set the goal of making this system “more functional” as in functional programming. We wanted to take the process of calculating a payroll and make it one big stateless operation. The server-side code at Gusto is written in Ruby, a language usually known for its object-oriented and metaprogramming roots. Nonetheless, we wanted to integrate some more functional concepts into our code in the hopes of increasing the system’s safety and clarity. The result has been maintainable code that is easier to reason about and safer to change. Embracing the PFaaO Ruby is an expressive language, but it does not lend itself to some common functional practices. Although Ruby allows for closures and first-class functions via Proc s, one does not see many Proc s passed around as objects in idiomatic Ruby. Throughout our work, we discovered that you can create expressive interfaces with clean internals by embracing both OO and functional aspects of Ruby. To do this, we used a pattern called Pure Function as an Object (PFaaO) . Essentially, you design objects as you would a pure function but dress them up as Ruby classes. A pure function is a function without observable side effects that always returns the same value for a given set of inputs. That means no talking to the database, no modifying the state of other objects, no accessing the system clock, etc. When we write a PFaaO in Ruby, we want to build an object that has no side effects. A simple PFaaO might look like the following: class PayrollCalculator\n  def self.calculate(payroll)\n    new(payroll).calculate\n  end\n\n  def initialize(payroll)\n    @payroll = payroll\n  end\n  private_class_method :new\n\n  def calculate\n    PayrollResult.new(\n      payroll: payroll,\n      paystubs: paystubs,\n      taxes: taxes,\n      debits: debits\n    )\n  end\n\n  def paystubs\n    # ...\n  end\n\n  def taxes\n    # ...\n  end\n\n  def debits\n    # ...\n  end\nend There’s quite a bit going on here, so let's break it down bit by bit. First, our class has only one effective public interface: PayrollCalculator.calculate . Because we've declared the constructor private using private_class_method :new , the instance method #calculate is effectively private. [1] This means that all of the other instance methods we declare are implicitly private, even though there is no explicit private block within this class. Because there’s no way to .new up an instance, there is not a vector to call any instance methods. Our method only has one public interface and its designed operation is effectively stateless, therefore we only need to exercise one interface in our tests. Put some data in, assert that the data coming out is what we expected. Referential Transparency for Free In our above example, let’s say that the process of calculating taxes is expensive from a time perspective. [2] Thus, we want to make a time/space tradeoff to consume more memory to minimize the number of times we need to compute taxes. In our example, calculating both #paystubs and #debits will require the result of #taxes . Now because each of these private methods is a pure function, we have referential transparency. This means we can replace a method and its parameters with its return value. Think of it like algebra: Given the function f(x) = x + 5 , you can safely replace any occurrence of f(2) with the value 7 . What does this mean for a Rubyist? Free and safe memoization: def paystubs\n  calculate_paystubs(taxes, ...)\nend\n\ndef debits\n  calculate_debits(taxes, ...)\nend\n\ndef taxes\n  @taxes ||= calculate_taxes(@payroll)\nend Memoization is a form of caching, and can be fraught with issues if the memoized value does not actually come from a pure function. But because we make everything within the PFaaO pure, we can safely memoize this method call. This is interesting because it looks like this class is no longer stateless: it now assigns local values. However, since the only interface is the single .calculate class method, each instance of our PFaaO is single-use. Any intermediate state can never be accessed by externally. Because this cached state is not observable externally, our function is still technically pure. Much in the way a developer can abstract synchronous and asynchronous behavior, you can do the same with functional purity. Any local state changes are irrelevant in the lifecycle of the PFaaO. These local state changes are not observable from the outside world. Expanding PFaaOs As I’ve grown in my career, I have become less interested in how software is written but how it is maintained. Software maintenance is the blessing and the curse of any successful project: Congratulations! You have a business with lasting value. Our condolences! You must now pay for all of your mistakes. Nonetheless, it is always preferred to have a business that exists with technical debt, than to have a bankrupt company with a pristine code base. PFaaOs in Ruby are great because they are easy to maintain. Not only are they easy to test, but they are predisposed to healthy growth. What do I mean by that? Let’s again take the example of our #taxes method. Early in Gusto’s history (back when it was still known as ZenPayroll), we only offered payroll services in California. Thus, we only needed to worry about payroll taxes for California. In the grand scheme of things, California is a simple state when it comes to payroll taxes. Our taxes method might have looked like nothing more than the following: def taxes\n  federal_taxes(@payroll) +\n    california_taxes(@payroll) +\n    local_taxes(@payroll)\nend Now let’s say we expanded into a new state, New York. Now our method grows a little bit: def taxes\n  federal_taxes(@payroll) +\n    california_taxes(@payroll) +\n    new_york_taxes(@payroll) +\n    local_california_taxes(@payroll) +\n    local_new_york_taxes(@payroll)\nend As we expand into every state, [3] this method will grow to be quite large! Furthermore, each of these methods adds to the length of our PayrollCalculator class. Without constant gardening, the class could become difficult to understand. But because each of our methods within a PFaaO is itself a pure function, we are able to extract classes as we see fit and make each one a new PFaaO. We can safely replace our growing methods with new PFaaOs: def taxes\n  PayrollCalculator::Taxes.calculate(@payroll)\nend As we tease apart these different PFaaOs, we also get a much better idea of the input requirements for these service classes. Our @payroll is a large parameter object, and each extracted PFaaO probably only needs a subset of its data. So we can get away with something like: def taxes\n  PayrollCalculator::Taxes.calculate(\n    @payroll.only_pay_and_location_data\n  )\nend Here we assume that the Payroll#only_pay_and_location_data returns a slice of the total data within the instance as a new Value Object . This Value Object represents only the data required to calculate the taxes part of running a payroll. Data is Immutable by Default Another important ingredient for scalable PFaaOs is the requirement that all data be immutable by default. This is a drastic change from how most folks traditionally write Ruby. Every time you reach for your = , you'll need to replace it with a #set or #put . Rather than modifying objects in place, you will get used to returning new copies with new values. ( Hamster , which provides great immutable data structures, can help you from having to hand-roll FP functionality.) What does this mean for Rails? It will often mean creating functions or classes that take ActiveRecord objects and convert them into immutable value objects. For us, we carve out these value objects into the namespace of what we're doing. For example, here are the two representations of a payroll in our system: # app/models/payroll.rb\nclass Payroll < ActiveRecord::Base\nend\n\n# app/services/payroll_calculator/payroll.rb\nclass PayrollCalculator::Payroll < ValueObject\nend The ActiveRecord version of a payroll represents the data that lives in the database. It is a superset of the data required for actually running a payroll. Although they have the same name, they do not have the same attributes. For example, the ActiveRecord version of Payroll will have a processed_at attribute, whereas the Payroll that lives in the calculation domain does not. In the words of Domain-Driven Design , each namespace here is a different Bounded Context. We implement adapters to take ActiveRecord payrolls and turn them into PayrollCalculator payrolls, and vice versa. The upside of this is something you might see in any other large system with well-defined abstractions; changes in models don’t cross domains. In our example, we can change the structure of the Payroll in our database without needing to change the calculation code. We would only need to change our adapter. Furthermore, this context is entirely separate from the machinations of Rails. We could easily and safely pull this into its own gem or a separate service entirely. Were our ActiveRecord objects be parameters to our calculator, adding or removing columns from the ActiveRecord objects could cause a series of cascading, painful, and dangerous changes. For young Rails apps, this level of indirection is overkill. As apps grow and multiple teams begin contributing to the same application, Bounded Contexts like these are necessary. Conclusion We’ve been slowly refactoring our payroll calculators toward this model and use it to safely process upwards of $1 billion per month. The results have been remarkable: adding or changing payroll code is now a much safer operation. Because each change is much more isolated, a developer only needs to concern herself with the local implementation. Although this post does not cover it, testing PFaaOs with immutable data is a breeze. We find ourselves performing less setup for each method and class. Our tests remain fast as they do not hit the database. It’s not all sunshine and rainbows, though. This approach does result in a larger volume of code. My rough estimate would peg it at about a 1.5x - 2x increase in code volume. Some developers dislike the the sprawling nature of the many PFaaOs that result.  Although the total lines of code will increase, this approach should help your team develop a better understanding of the data requirements of each Bounded Context. Put another way: you don't need to pass around whole ActiveRecord objects, but just small bundles of their attributes. Before embracing this completely, discuss with your team to set up a few ground rules. We typically shoot for about 100 lines per class, but your team might decide on something different. Make sure to get on the same page and agree that your app is at the size where it might benefit from this style of thinking. For some teams, the extra layers of abstraction between ActiveRecord and doing interesting things with the data might seem like overkill. In many situations, it will be. Again, I encourage you to have a healthy discussion with your team to decide if the benefits of this approach outweigh the negatives. For us, we’re employing it everywhere appropriate. Give this pattern a shot and let me know how it goes! Special thanks to Justin Duke , Eddie Kim , Bo Sørensen , Matt Lewis , and Julia Lee for providing feedback on early drafts of this post. Keen writers will know that nothing is ever really private in Ruby. There is always #send. ↩︎ At Gusto, calculating taxes is expensive! Did you know that there are more than 6,000 payroll taxes within the United States? Each one may or may not need to be applied for a given payroll, based on the different parameters of the payroll itself. ↩︎ Today, Gusto provides payroll services in every state including D.C. with some of the lowest error rates in the industry. ↩︎", "date": "2017-10-02"},
{"website": "Gusto", "title": "Hacking with Gusto", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/hacking-with-gusto/", "abstract": "Once or twice a year, Gusto’s engineering, product, and design (EPD) team likes to pause, convalesce, and spend three days writing “non-roadmap-approved” code together. Engineers, designers, and product managers with ideas pitch them to other EPD members to have them join their teams. Non-EPD folks throw their hat in the ring too by getting others excited about their ideas in a formal “pitch-off” a couple of weeks before the hackathon. Teams form around ideas, with each having a “lead.” This time there was no set team structure. Some comprised a single person. Others had 8. Key to this experience was finding places away from the office where we could focus on our micro-projects. The San Francisco team came together at The Laundry , and our Denver team worked out of an AirBnB loft near Union Station. Both were structures adorned with old brickwork, which is compulsory for any good hackathon location. And everybody coded. Even Eddie Kim, our CTO, got the development environment raring to go on his laptop. Projects ranged from histograms produced by an AI-like analysis of tax payment trends to camera-based systems that might help hourly employees clock in and out of work by pausing for a brief scan on their way in and out of the office. We saw one project add “communities” to our HR product so employees could connect around shared interests, while another made it easier for people to report expenses. And yet another introduced a system that would integrate metadata into rails models to better track data typing and mandatory fields, among other things. We demo’d our hacks to a company-wide audience during the second half of Day 3. A panel of judges from all company disciplines convened to designate winners in each category, which were: Cole’s Customer Experience prize Andy’s financial responsibility prize Tomer’s product innovation prize Phan’s code deletion prize Josh’s Team Collaboration prize Sherry’s Growth Experiments prize Joe’s Security prize Nikhil’s development experience prize Lexi’s Biz Team prize Maryanne’s People Team prize We gave guidance early on that people’s hackathon projects could have nothing to do with Gusto (one engineer created a checkers board). Indeed, it was important for us to create a boundary-less construct in order to optimize for creativity. But we were floored by the number of projects that came back to our core business, which stood as one data point that folks here care deeply about our customers and how their experience can be continually improved.", "date": "2017-12-04"},
{"website": "Gusto", "title": "Growing our Engineering Team in Denver: Diversity begets Diversity", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/growing-our-engineering-team-in-denver-diversity-begets-diversity/", "abstract": "About a year ago, we published the results and shared some learnings from a concerted effort to build a more diverse and inclusive engineering team at Gusto. More recently, we’re proud to share that we have 50/50 gender balance in our incoming cohort of 2017 recent college graduate software engineers. We’re proud of how far we’ve come, but we also know we – and the industry at large – have much more to do to build a truly diverse and inclusive environment. I was reminded of this recently when I asked one of our female engineers if she would consider doing a three-month stint working in our small-but-growing Denver engineering office. She shared with me that she was really excited about the opportunity. However, she also shared that her excitement was somewhat dampened by the fact that she would be working in an engineering team that is less diverse than the one that we have in San Francisco. If, however, she were able to pair up with another woman on the team to do the three-month stint, that would be a pretty exciting adventure to her. Her comments made me realize an important dynamic: Diversity begets diversity . If you already have a diverse engineering team, this makes working for your company more attractive to those with different backgrounds, leading to a virtuous cycle of building a diverse engineering team. The inverse is true as well. A lack of diversity in your team makes it less attractive for someone from an underrepresented group to join your company. The story I shared is a real-world example of diversity debt . The later you start, the more difficult it is to build a diverse team. Our Denver engineering team is new, and we’re really excited to prioritize building a diverse and inclusive team from the start. That’s why, as we ramp up our Denver presence, we’re starting as early as possible to form a diverse team. Here are some concrete things we're doing as we build our small but fast-growing Denver engineering team : We’re sending two women from our SF engineering team to Denver for three months. In addition to working with Denver engineers to help share knowledge and writing code to continue to build the Gusto product, they’ll also help seed our culture and recruit a diverse team. Our recruiting team is sourcing only female engineers. Currently, the vast majority of our inbound applicants are men, so we are confident this is the right approach. We’ve created an engineering apprenticeship program in Denver where we’ll hire and mentor two junior software engineers, one each from an underrepresented gender and ethnic group. Why are we doing this? Gender diversity is not the only thing we think about. We’re certain there are lots of junior engineers from underrepresented groups who have a lot of potential, but may not come from traditional software development backgrounds. We’ll be looking at people who are graduating from coding bootcamps like Galvanize , Turing , and General Assembly . We believe that starting early is important to forming a diverse and inclusive engineering team. For new tech companies, or established companies looking to start an engineering team in a new location, I’d encourage you to make diversity and inclusion a priority from the start. We certainly hope our efforts will attract people with all kinds of backgrounds.", "date": "2017-03-22"},
{"website": "Gusto", "title": "Our Latest Cohort of Software Engineers is 50% Female", "author": ["Diane Liu"], "link": "https://engineering.gusto.com/our-latest-batch-of-software-engineers-is-50-female/", "abstract": "In celebration of the 2018 Women’s March, we’re kicking off a series of posts highlighting Gusto Engineering team’s diversity and inclusion efforts. For this first installment, we sat down with some women from our 2017 new graduate cohort. In 2017, Gusto hired their largest new graduate cohort to date, with 10 engineers — 8 in San Francisco and 2 in Denver, and half of these new graduates were women. We interviewed some of them to talk about their experiences at Gusto so far and what advice they have to offer other women engineers starting off their careers. From left to right: Brittney, Alicia, Diane, and Lucy. Why did you join Gusto? Alicia: My main criteria when I was looking for a job was a place where I could learn and grow a lot, and a place with amazing people and culture. For me, Gusto was the perfect size in that it wasn’t too small, where I’d be having to learn everything by myself, but it wasn’t too big where it’d feel bureaucratic. There are still plenty of opportunities and areas where I can feel ownership. As for the people, Gusto definitely goes the extra mile to show how much they value the candidate — it’s not just how well I code but who I am as a person. Brittney: Company culture was important to me, too. I wanted to work with people who were passionate about solving a problem. When I was interviewing with Gusto, I read an interview with our CEO about how running a startup or small business is like running a train while building the train while laying the tracks. Four of my uncles and three of my aunts have started small business of their own, so when I was growing up I saw firsthand how challenging it is. I’m passionate about helping small businesses lay their tracks, and it’s important to me to work with others who are passionate about it, too. Diane: What set Gusto apart from other companies I was considering was definitely the people and the culture. One thing that really stood out to me when I was interviewing at Gusto was I actually had more female technical interviewers than male, which I’ve never experienced before. Representation is important! It really mattered to me during interviewing, and it’s something that I continue to see while I’ve been working here — the fact that our new grad cohort has gender parity is a prime example. Lucy: Number of women engineers is definitely one of the important reasons that I chose Gusto. While at Gusto, one of my interviewers was a woman engineer, and she told me that the percentage was about 20% at that time. When I got offer from Gusto, I was deciding between Gusto and similar position in a similarly sized company. If I were to join the other company, I would be the 2nd or 3rd women out of around 100 engineers and I was not confident to handle that situation. What are some highlights of your experience at Gusto so far? Alicia: One of my favorite experiences as a new grad was definitely the boot camp meant to onboard the new grads starting at the same time. It seemed unusual for a small company to invest in spending so much time to onboard when most would start work the first day, but it really reinforced to me what Gusto values — that we don’t optimize for the short term. Brittney: My first big project was super interesting technically as well as impactful. I built a portal to bulk upload information about our health care plans, which replaced an old and tedious process. I got to design and implement this project end-to-end, with plenty of help when I needed it. Our internal operations team was really excited about the project because it saved them dozens of hours, so it was really fulfilling to see how immediately impactful my work was for individuals as well as for Gusto as a whole. Diane: Boot camp was a great opportunity for all of us to feel a sense of community from day one, and we’re still super close and supportive of each other. That was definitely a highlight that I didn’t anticipate. Besides that, being able to own my own project as a new hire and contribute to our September product launch within a month of starting were both very exciting, and demonstrates how much trust Gusto places in their engineers, regardless of seniority. Lucy: Our September launch was a really exciting moment for me because a bunch of HR features that I’ve worked on for months were released to customers. My teammate Danny and I had a chance to present one of the features, offer letters, to fellow Gusties and investors on the demo day. The presentation impressed one of the investors and he tweeted afterward about how Gusto streamlined hiring and onboarding process for small businesses! What are some challenges you’ve experienced in your time at Gusto, and how have you resolved them? Brittney: In previous internships, I was scared of my managers. It felt like admitting I didn’t know something would get me fired. When I started at Gusto, I was a bit stuck in this mindset. At an engineering women’s lunch, one of our senior female engineers explained that we call our managers “People Empowerers” for a reason: they’re there to help you progress in your career, not punish you. After that, I became a lot more open and honest with my PE about what I was struggling with. He helped me identify strategies for improvement as well as understand what I was already doing well. Now, I see my weaknesses as actionable opportunities instead of secrets to hide. Lucy: I have one-on-one meetings every two weeks with my PE and we spend about 45 minutes talking about how I’m doing and how I want to grow. I can tell that he really listens and tries to support me. A few months ago, I was talking about wanting to have more exposure to backend since I had done quite a few frontend-heavy projects at that time. And later when there was a new project, I was given the opportunity to take on the more backend-heavy part. I feel that my career growth is really being cared for and supported. What would you say to fellow girls and women who are just getting started as programmers? Brittney: Don’t feel like you need to be “one of the guys.” If you are naturally, of course that’s fine, but not all talented software engineers look and act like the hackers on TV or that guy in the high-level computer science classes who always uses vocabulary that you don’t know. There’s room for femininity and vulnerability in tech. Diane: I think that when you’re just getting started, it’s very easy to feel intimidated and like you’ll never get to the level of everyone else that you see around you. The thing is, everyone was once a beginner — there are so many great mentors out there who have traveled the exact same path you are just starting out on, and are more than willing to guide you. Lucy: There is a study showing that when a problem happens, men tend to blame outside factors like the environment while women tend to blame themselves. But please don’t feel that you’re less talented than others when you encounter difficulties when you start your career. As far as I know, everyone has feelings of being overwhelmed more or less and has a lot to learn at the beginning. Experience will be built over time. Interested in joining Gusto? We're hiring! About the Authors Alicia Tran Alicia graduated from USC in 2017 with a BS in Computer Engineering and Computer Science. She’s originally from LA, where she was involved with developing software for local nonprofits and social enterprises through a student organization. In her free time she does art, yoga, and reading. Brittney Johnson Brittney’s an East Coast transplant who graduated with a BS/MS from MIT in 2017. She taught an intro to programming class for high school girls before starting at Gusto. Her favorite podcast is “The Adventure Zone,” and she spends most of her free time trail running in the East Bay Hills. Diane Liu Diane is a Bay Area native and graduated from Northwestern in 2017 with a BS/MS in Computer Science. While at Northwestern, she organized NU’s hackathon, WildHacks, as well as weekly instructional Hack Nights. Now, you can catch her cooking, doing yoga with Alicia, or buying coffee gadgets that she doesn’t need. Lucy Lufei Lucy was born and raised in China. She went to Hong Kong for college, where she studied industrial engineering but fell in love with coding in one of the programming classes. After she graduated from college, she went to USC for graduate school to study Computer Science, which had always been her dream. Besides coding, she loves exploring different kinds of outdoor activities — skiing is her favorite now.", "date": "2018-01-22"},
{"website": "Gusto", "title": "The Power of Small Teams", "author": ["Vijay Raghuraman"], "link": "https://engineering.gusto.com/the-power-of-small-teams/", "abstract": "Prior to Gusto, I built products in a world where the Product Management (PM) team “owned” the product. The PM team would come up with the product strategy, write the specs, and hand everything off to engineering. From functional specs to development and then on to testing and release, all of these steps were orchestrated by project managers, who helped create and refine formal procedures. The boundaries were well-defined and visible: PMs sat on one side of the building and engineers on another. When I joined Gusto, I discovered that EPD (Engineering, Product and Design) teams work very differently. Teams are organized into missions. A mission has a well-defined charter around solving challenging problems for our customers, whether internal or external . Missions mix groups of people across Engineering, PM, Design, and other cross-functional teams like Data Science, Credit Policy, or Operations. This diverse group sits next to each other, constantly interacting and developing a strong camaraderie. As an example, we recently worked on paying employees faster by “reverse wire.” When we rolled it out to our first pilot customer, we ran into an issue fetching their payroll funds. Our operations team member made us aware of the issue, our engineer diagnosed the issue and looped me in, and I was on a call with our vendor within 15 mins. After many back-and-forth calls with the vendor, we gingerly babysat the first payroll transaction to ensure the customer’s employees got paid on time! I thus witnessed first hand the power of small teams. Small teams that care about the same challenging customer problems take on ownership without needing to be told exactly what to do, work out the right amount of process for them, and unleash creativity in unexpected ways. And while this approach may sound obvious, I remember “escalations” like the above to be much more stressful before Gusto. We would have meetings one after the other and my role was to provide business justification to team members who were much further removed from the customer problem at hand. In contrast, thanks to the power of small teams, I see a lot more smiles here! Are there drawbacks to this world? Sure, no system is perfect. For example, when we were building the functionality above, we had two engineering teams that didn't know what the other was doing because they were working on completely different parts of the code. I have observed that providing and encouraging easy mechanisms to give and get feedback helps us improve quickly. The solution in this case was a lightweight 5 min stand-up every week that drew the teams even closer together. The important learning is to develop organizational structures deliberately and intentionally. When in doubt, err on the side of fewer formal, top-down processes and let teams self-organize around a well-understood mission centered on solving important customer problems.", "date": "2018-02-26"},
{"website": "Gusto", "title": "A Day in the Life of a Gusto Engineer", "author": ["Jocelyn Kuswanto"], "link": "https://engineering.gusto.com/a-day-in-the-life/", "abstract": "Inspired by Increment magazine’s “What it’s like to be a developer at” series, here’s my take on what it’s like to be an engineer at Gusto. Our high-level engineering principles have stayed the same, but we have updated a lot of the tools we mentioned in our first post about “Our Development Flow.” What are the most common tools that developers use at Gusto? We use a range of tools across all our teams. Here are some of the popular ones: Docker Slack IDEs (RubyMine, Sublime, Atom) Confluence Project tracking (Jira, Pivotal Tracker) Git, GitHub Jenkins Coffee is my most important tool. Which languages do developers code in? We use Ruby for the backend and React on the frontend. Some of our services use Go, and the data team uses Python. What is the development process (the lifecycle of a piece of committed code) like? We like to wear many hats at Gusto, so part of the development process is to investigate, write out expectations, and prioritize stories. After stories have been broken down into manageable pieces, we’ll go through the typical flow of writing code that fulfills the requirements, writing automated tests, and submitting a pull request (PR) for review. Pair programming is part of our engineering culture and it’s really versatile at Gusto. Scheduling meetings to pair is common and there are a variety of reasons such as onboarding a new member, fixing an urgent production issue, or even casually diving into another team’s new feature. We have dedicated pairing stations around the office so it’s easy to find a spot to plug in and share your screen. Also, not being present in the office doesn’t stop us from pairing! We happily pair through Slack calls when necessary. Gusto Fast Facts What is code review like? We have a very standard approach to code reviews. Internal tools alert assignees about code reviews through GitHub and Slack. Any Gusto engineer can search and view other pull requests (PRs). We have an automated build, which runs through Buildkite, and each PR must pass our entire suite of tests. However, my favorite approach toward code reviews at Gusto is pair reviews, especially if the PR is particularly involved or complicated. When I am confident that my code meets requirements, I’ll schedule time with someone who I feel is most knowledgeable or has the most context on my PR. These reviews happen in-person, at a pairing station. That way we can have very active discussions and short feedback loops, while clearing any misunderstandings on the spot. I’ve found that this is extremely helpful when wanting to discuss my implementation and trade-offs of other ideas I had in mind. Also, as the reviewer, it gives me an opportunity to get more context from the original owner. How is testing done, and what kind of tests are run? We have an extensive suite of automated unit, integration, and end-to-end tests. Our continuous integration (CI) process runs all our tests with every commit, even on branches, using Buildkite. The main tools we use for writing tests are RSpec and Mocha for the back-end and front-end, respectively. We also leverage these additional frameworks and libraries to better support all of our testing needs: Chai Sinon.JS VCR Capybara with PhantomJS Karma Enzyme Additionally, we enforce the use of linters, such as RuboCop , ESLint , Sass-lint , for code consistency across all of our repos. How is code deployed? We practice continuous delivery, so any PR that is merged, must be safe to deploy. A PR will first merge into our development branch that will automatically deploy to our staging environment. If the change looks good on staging, we will merge the PR to master, which is then deployed to production. All engineers have access to Slack bots that can trigger a deploy. When triggered, it’ll find the latest commit on development that has passed our test suite and send messages to an AWS SQS queue. That then gets picked up by a Jenkins job and kicks off our deployment job. All of our assets will build, compile, and get pushed onto S3 after the PR is merged. This speeds up our deploys because our assets are already available when we want to deploy a particular change. We also leverage Cloudfront to quickly and securely distribute them to our users. We use Capistrano, a Ruby gem, to handle our rolling deploy, which will stop and start servers individually. It’ll fetch the latest code from GitHub and restart the server while ensuring customers can still access our app even in the middle of a deploy. The restart also runs scripts to configure the box as expected, which includes bundling all necessary gems and establishing required connections to the database. Additionally, we’ve recently enabled our app to run migrations out-of-band with the help of gh-ost . It essentially streams data from the original table into a new table with an updated schema and swaps the tables when safe. This ensures our app will have zero downtime during all deploys (exciting updates about our migrations coming soon!) Swag from our most recent hackathon . What is an average day-in-the-life of someone on one of the development teams? This varies for different teams, and even from person-to-person, but usually one can expect: Daily stand up, either in person, through Slack, or over our SF-Denver portal Pairing session Personal time to code, merge, and deploy PRs, and do code reviews Context-sharing and technical discussions in person and through Slack Occasional lunch sessions such as brown bags, where team members will talk about slated projects or have technical deep-dives What makes Gusto a special place to be a developer? Everyone is extremely friendly! People are extremely willing to roll up their sleeves and jump into your code, even if it’s in unfamiliar territory. I’ve had people from completely different teams debug really hairy issues with me. To tack onto that, I believe we’re constantly striving to be better. We call the dedicated weekly, on-call engineer, the “Viking Master” (VM). Because of the nature of on-call roles, it’s not as desirable as working on new features, but we take strides to improve the process. Whether it involves adopting completely different processes, pairing with another engineer, or even taking a couple silly pictures, we’re always trying to make unpleasant experiences at least a little bit more palatable. A Viking Master on call Can you tell us a bit about your team and what you are working on? Engineering has six different missions—Payroll, Benefits, HR, Growth, FinTech, and Infrastructure. I’m on the Payroll mission, which itself is split up into three subdomains. They are called Running Payroll, Setup and Onboarding, and Payments and Filings. I work in the Running Payroll subdomain and am part of the Run Payroll Upgrade Team. We are currently refactoring our run payroll flow from Backbone to React. Because it’s one of our older flows, most of our teammates who originally built it are no longer at Gusto. Although it raises a lot of questions about the behavior and intentions of some of the interactions, it really challenges and flexes muscles required to interpret legacy code and fix unexpected behaviors. It’s rewarding to reduce our reliance on an older, less predictable framework and lay the groundwork so developers can build quickly and assuredly on top of our work. About the Author Jocelyn is a software engineer at Gusto and San Francisco native.", "date": "2018-02-19"},
{"website": "Gusto", "title": "5 Rails Callbacks Best Practices Used at Gusto", "author": ["Kelly Sutton"], "link": "https://engineering.gusto.com/the-rails-callbacks-best-practices-used-at-gusto/", "abstract": "Folks interviewing at Gusto are often surprised to discover that Gusto chooses the Ruby on Rails framework to write financial software. Ruby and Rails are known for their conventions, optimizations for programmer happiness, and an anything-goes mentality (monkey-patching, anyone?). Nonetheless, the language and framework excel when it comes to expressiveness. When dealing with thousands of federal, state, and local taxes—all of which change every single year—the ability to move quickly and safely is important. It’s for these reasons we continue to use Rails as one of the primary technologies at Gusto. Rails is not without its tradeoffs. Particularly, we’ve found Rails callbacks to be problematic. Callbacks allow you to bind actions to the lifecycle of models, known as ActiveRecord objects, in Rails. To give folks an insight into some of the best practices we use at Rails, this post explores our best practices for dealing with Rails callbacks. These are guidelines but not hard rules; we stick to them most of the time but there’s a time and a place to break them. Here are our guidelines: Omit when possible Asynchronous by default Prefer after_commit to after_save Avoid conditional execution Don’t use for data validation Guideline #1: Omit When Possible Callbacks ensure that anyone updating or saving a record won't forget to perform an operation that should be performed on #save . Most parts of our applications directly modify our ActiveRecord::Base subclasses instead of using a service object to perform an operation. More and more, we are trying to use service objects to encapsulate groups of operations rather than callbacks. Let's take a look at a code example for a hypothetical company onboarding flow: # Bad\nclass Company < ActiveRecord::Base\n  after_commit :send_emails_after_onboarding\n \n  private\n \n  def send_emails_after_onboarding\n    if just_finished_onboarding?\n      EmailSender.send_emails_for_company!(self)\n    end\n  end\nend\n \n# Good\nclass Company < ActiveRecord::Base\nend\n \nclass CompanyOnboarder\n  def onboard!(company_params)\n    company = Company.new(company_params)\n    company.save!\n    EmailSender.send_emails_for_company!(company)\n  end\nend By locating our logic in a service object and not a callback, we are not adding logic that will need to run after every #save . Because we have a service class to handle signing up a company, we also reduce the coupling between Company and EmailSender. Guideline #2: Asynchronous by default Whenever we add a callback, that is code that will execute before we can respond to a request. If a class defines 20 callbacks, that's 20 blocks of code that must execute before we can respond to the user. Generally, this will make requests take longer. Requests that take longer result in a sluggish experience on the front-end. Therefore if you must write a callback, make sure it gets out of the critical path of the request by making the bulk of itself asynchronous. (Note: This only applies to logic that does not need to exist within the same transaction.) For us at Gusto, that means enqueuing a Sidekiq job. Let's look at an example: # Bad\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification, on: :create\n \n  private\n \n  def create_welcome_notification\n    # We're incurring an extra database request here, which\n    # is something we want to avoid during \n    # critical operations like signing up a \n    # new customer\n    notifications.create({ title: 'Welcome to Gusto!' })\n  end\nend\n \n# Good\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification, on: :create\n \n  private\n \n  def create_welcome_notification\n    WelcomeNotificationCreator.perform_async(id)\n  end\nend\n \nclass WelcomeNotificationCreator\n  include Sidekiq::Worker\n \n  def perform(company_id)\n    @company = Company.find(company_id)\n    @company.notifications.create({ title: 'Welcome to Gusto!' })\n  end\nend Now, you might say, “But the 'Good' example is way more code than the 'Bad' one! It now takes 2 files!” You would be correct. Writing callbacks in a safe manner by moving the bulk of the logic into a worker will result in more code. The resulting code, however, will reduce coupling on our models and make each component more testable. Speaking of tests, here's a good way to test callbacks like this using RSpec : # spec/models/company_spec.rb\nrequire 'rails_helper'\n \nRSpec.describe Company, type: :model do\n  describe '#save' do\n    subject { company.save }\n    let(:company) { build(:company) }\n \n    it 'schedules a WelcomeNotificationCreator job' do\n      expect {\n        subject\n      }.to change{ WelcomeNotificationCreator.jobs.size }.by(1)\n      last_job = WelcomeNotificationCreator.jobs.last          \n      expect(last_job['args']).to eq([subject.id])\n    end\n  end\nend\n \n# spec/workers/welcome_notification_creator_spec.rb\nrequire 'rails_helper'\n \nRSpec.describe WelcomeNotificationCreator do\n  subject { described_class.new.perform(company.id)}\n  let(:company) { create(:company) }\n \n  it 'creates a notification' do\n    expect {\n      subject\n    }.to change(Notification, :count).by(1)\n    expect(Notification.last.title).to eq('Welcome to Gusto!')\n  end\nend Guideline #3: Prefer after_commit to after_save When writing a callback where you want to execute code after a save, create, or update, default to using an after_commit block. after_commit is the only callback that is triggered once the database transaction is committed. Putting it simply, it's the only callback whose state will match \"the outside world,\" specifically the same state that worker process will see. Without an after_commit callback, you may see strange errors in Sidekiq to the tune of Cannot find Company with ID=12345 . These errors will be frustrating to track down because they will only raise once before passing without a problem. What's happening with this error is that Sidekiq is picking up the job before the database has committed the new record to it. To the outside world, it looks like your new record does not yet exist. It can be an incredibly frustrating race condition to debug. The safe default is to use after_commit . Let's take a look at 2 different code examples: # Bad\nclass Company < ActiveRecord::Base\n  after_create :create_welcome_notification\n \n  private\n \n  def create_welcome_notification\n    # The database transaction has not been committed at this point,\n    # so there's a chance that the Sidekiq worker will pick up the job\n    # before our `Company` has been persisted to the database.\n    WelcomeNotificationCreator.perform_async(id)\n  end\nend\n \n# Good\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification, on: :create\n \n  private\n \n  def create_welcome_notification\n    WelcomeNotificationCreator.perform_async(id)\n  end\nend Please note: The *_changed? helpers are not available within after_commit . If you need to conditionally execute a callback, please be aware of this and use the alternative method if looking at the previous_changes hash. Or... Guideline #4: Avoid conditional execution As a general rule, do not write callbacks that are conditional. Instead, try to make callbacks that are idempotent and can be safely run multiple times. This is especially important when you begin moving your callback logic to Sidekiq jobs. (See the best practices on Sidekiq to see why Sidekiq jobs must be idempotent.) Note that this will enqueue more jobs onto Sidekiq, but many of these jobs should be no-ops. If you must conditionally execute a job, please define that logic within the method body and not in the callback signature. The reason for this is to make sure that the conditions only live in a single place. It's much easier for the conditional to grow in the method body instead of on the line that defines the conditional. Let's take a look at a few examples: # Bad\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification, if: -> { should_welcome? && admirable_company? }\n \n  private\n \n  def create_welcome_notification\n    WelcomeNotificationCreator.perform_async(id)\n  end\nend\n \n# Good\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification\n \n  private\n \n  def create_welcome_notification\n    if should_welcome? && admirable_company?\n      WelcomeNotificationCreator.perform_async(id)\n    end\n  end\nend\n \n# Best\nclass Company < ActiveRecord::Base\n  after_commit :create_welcome_notification\n \n  private\n \n  def create_welcome_notification\n    WelcomeNotificationCreator.perform_async(id)\n  end\nend\n \nclass WelcomeNotificationCreator\n  include Sidekiq::Worker\n \n  def perform(company_id)\n    @company = Company.find(company_id)\n \n    return unless @company.should_welcome? && @company.admirable_company?\n \n    # We passed our check, do the work.\n  end\nend Please keep in mind that things like on: :create are their own flavor of conditional execution. Generally, try to push that logic into the worker. Guideline #5: Don’t use for data validation It is possible to use callbacks for data validation, but that should be avoided. ActiveRecord makes certain assumptions on how and when validations will be run, and putting these in callbacks breaks some of those assumptions. Instead, use the ActiveRecord Validations API . Conclusion With great power comes great responsibility, and Rails callbacks are no exception. They are expressive tools but also come with drawbacks. We are generally moving away from using callbacks and toward using rich service objects in our applications, but we still have many callbacks that exist. We still have a lot left to do, but we’ve found that the code that follows these guidelines requires less maintenance.", "date": "2018-02-12"},
{"website": "Gusto", "title": "How I Interviewed My Interviewers", "author": ["Ketki Duvvuru"], "link": "https://engineering.gusto.com/how-i-interviewed-my-interviewers/", "abstract": "A few months ago, I received an offer to join Gusto’s Payroll product management team. I had just completed a fairly rigorous interview process and was pretty excited about the opportunity based on the great conversations I’d had with members of the team. Before I accepted it though, I knew there was more I wanted to learn about the place where I might be spending most of my waking hours for the next several years. This wasn’t my first job hunt — I started my career as a QA Engineer-turned Product Manager at Box, where I spent 6 years. I eventually left to join Slyce, an 8-person startup, as their Head of Product. As my time at Slyce ramped down, I began looking for my next role. My career soul searching led me to realize I was looking for a mission-driven company with a strong team, a B2B model, and consumer product DNA. After speaking with twenty other companies, I believed Gusto could be the company I wanted to work for. Although Gusto had extended me an offer, I knew the interview wasn’t over yet — the tables had turned, and it was finally my turn to ask the questions! There were three members of my interview panel I was particularly interested in speaking with more. Here’s what I asked them, and why: Eric Schuchman Why the conversation mattered: Eric is the most tenured product manager at Gusto, besides CPO and co-founder Tomer London. Eric was also the first PM I chatted with during my phone screen, and again during my onsite interviews. What I wanted to know: Because Eric had been at Gusto for so long and had seen it grow so dramatically, I was curious about what factors and challenges kept him engaged and encouraged him to stay. I wanted to understand how compelling the core of the team and the mission felt to him once the honeymoon period and novelty of a fast-growing startup had worn off. Potential warning signs: If Eric had sounded pessimistic or jaded, I would have worried that the challenges in the organization had worn him down and now outweighed the interesting opportunities. I’d experienced in the past how negative long-tenured people could impact the morale of their entire team, and it’s a situation I wanted to avoid going forward. What I learned: Here’s what Eric said mattered most to him in a role: Adding value to the world Building a sustainable business Learning personally Growing others Working with great people Eric told me how he joined Gusto as an individual contributor, owned various product areas, and eventually expanded his capacity by becoming a PE (People Empowerer — Gusto’s term for managers), and eventually, a PE-of-PEs. Throughout this time, he felt his priorities were continually fulfilled by Gusto. Eric feels that Gusto’s culture allows him to focus on doing the best work of his life to this day. Hila Shemer Why the conversation mattered: Hila’s interview question was the most challenging one I faced. I knew she was a sharp thinker, and I was eager to ask her about her own journey at Gusto. Additionally, companies across the Valley are actively grappling with diversity and inclusion, so I knew that speaking with a fellow female product manager would give me a sense of the opportunities available to all members of the team. What I wanted to know: I asked Hila about the skills and qualities she felt Gusto values in its product managers. Potential warning signs: If a high performer like Hila hadn’t seen her career progress, that could indicate that Gusto faced issues with unconscious bias. Furthermore, if the skills Hila outlined as important to Gusto didn’t match with what I believed a good PM needed, it would have been a signal that I wouldn’t grow at Gusto in the way I wanted. What I learned: Hila told me how she had grown from an individual contributor to a PE after joining Gusto at a similar point in her career as me. She taught me that Gusto values delivery on commitments, advocacy on behalf of customers, and thoughtfulness beyond one’s immediate domain. Hila has been successful thanks to her ability to ruthlessly prioritize, execute on projects, and empower others. Chatting with Hila increased my confidence that I would be able to thrive at Gusto, based on both my own skill set and how I believe strong PMs should work. Nick Baum Why the conversation mattered: Nick would be my PE if I joined Gusto, and it was important to know whether we would have a good relationship. Nick had recently completed a lengthy job hunt himself and was new to Gusto. During the interview, he was a great sounding board to offer advice about the companies he considered and what attributes he looked for as he built his team. What I wanted to know: I knew I needed to understand Nick’s: Preferred management style Leadership strengths and weaknesses Thoughts on what Gusto was doing well Thoughts on what Gusto needed to do better Potential warning signs: I’ve always learned fastest when working with people who embody the skills and personality traits I value. It would be a major problem if I couldn’t respect Nick over the course of our first conversations. What I learned: Nick and I talked at length about his background. I particularly enjoyed learning about his previous life founding and running a small startup called StoryWorth. He told me how he focused both on building a delightful experience for users, and investing in his team to help them grow in their careers. I found that I agreed with his overall product philosophy, and was satisfied that our personalities would work well as a team. After interviewing the team, I accepted my offer to join Gusto’s Payroll Mission! I felt confident the opportunity would be a great fit for my goals. I’ve been at the company for a few weeks now, and look forward to creating a world where work empowers a better life, and becoming a better product manager in the process. Go #withGusto!", "date": "2018-03-13"},
{"website": "Gusto", "title": "Continuing Our Commitment to Diversity in Engineering", "author": ["Upeka Bee"], "link": "https://engineering.gusto.com/continuing-our-commitment-to-diversity-in-engineering/", "abstract": "Introducing The Mango Team Last year at Gusto we formed the Engineering Diversity Steering Committee, informally known as the Mango Team. Why mango? Because at Gusto, we tend to name our inclusivity initiatives after fruit. You’ll have to join us to learn why that is 😉 The women of Gusto's Payroll Engineering Team What is the Mango Team? We are a cross-functional team between Engineering and Recruiting. Currently, the team includes: Eddie Kim (our CTO and co-founder) Maryanne Brown Caughey (Head of our People Team) Michelle Hoberman , Amy Wu , John Kempe , Élan Bailey (our technical recruiters) Four of our engineers: Justin Worth , Julia Lee , Seema Ullal , and myself Steffi Wu (leads our Pineapple Team) The team focus is solely on diversity within the engineering organization. According to the data, we see this area as needing the most help, and over the years we’ve already invested in a variety of programs that changed the composition of our team from 11% women engineers in 2015 to 24% today . While we will work on many aspects of diversity this year, until the end of this quarter we will focus on gender diversity, specifically filling senior roles on our engineering team. Our goals are both high-level and tactical. As we know well, role models and sponsors are key parts of a well-rounded career, and we aim to provide support and ensure upward mobility for our existing female engineers. You can’t be what you can’t see The Mango Team was formed after a frank one-on-one that I had with our CTO and co-founder Eddie Kim. I expressed disappointment about the lack of diversity in both the managerial and technical tracks on Gusto’s engineering leadership. How could I feel that I belonged and that I could drive my career forward when there were no female leaders around me? I also felt concerned about the lack of role models for our female engineers, the majority of whom are within the first 2 levels (out of 6 levels total) of our internal career levels. Illustration by Jonas Mosesson This wasn’t an easy conversation to have and I had to consider carefully the consequences of “rocking the ⛵️” or appearing to complain too much, at a company where I had barely spent 3 months. My ask of Eddie was to form a team to focus on solving this problem. In the lead up to my meeting with Eddie, I spoke with a few senior women engineers at Gusto who had championed this cause in the past. They had worked hard in this area but it had become unsustainable given their full-time responsibilities. I came to realize that we can’t rely solely on champions and needed to devise a sustainable effort with tangible goals. Julia Lee is the first female engineer at Gusto, and has championed efforts around diversity in the past The outcome of that conversation was more than I could have asked for. Eddie was not only empathetic, but also supportive of my ask. We brainstormed about who should be on the team and he spearheaded forming the team and the logistics around it. Illustration by JLR Reyes My career coding professionally is coming up on 12 years now, and I have driven similar initiatives in the past. I did not expect that a co-founder and CTO would be willing to get personally involved to tackle and issue raised by an engineer. It was a pleasant surprise that Eddie was so open to taking action on this issue. Mango Team Goals Our high level objectives fall into two categories: Hiring engineering candidates from underrepresented groups, specifically for senior roles Improving upward mobility of people from underrepresented groups on our engineering team As an engineering organization we have made a lot of progress in this area already. We have come a long way since 2015 , when only 11% of the engineering was women. Currently 24% of our engineering team are women, 29% of engineers who joined us last year were women, and our cohort of new graduate hires last year was 50% women . Gusto engineering levels range from 1 to 6, with 1 being a new graduate engineer. As such, our tangible goals by the end of this quarter are to: Increase the percentage of women in level 3 and above to 42% of all female engineers Increase women in engineering leadership staff to two A culture of less talking and more doing We understand that there is a lot of advice out there about how to achieve diversity goals. However, as there is no one-size-fits-all solution that works overnight, we’ve adopted a scrum-like iterative work management strategy, with low process overhead. We maintain a backlog of strategies and learnings and the team meets every 2 weeks where we assign owners to tasks from the backlog. Our operating philosophies are: Data first We take a data-first approach to understand the extent of the issue and most importantly, to create tangible goals against which we could measure our progress. We have looked at various types of data, including analyzing our interview process to understand whether we disproportionately lose female candidates at a certain step. We aim to treat each task on our backlog as an experiment and move forward only if we succeed on a small sample. Be scrappy Given each team member has a different full-time role, the goal is to be as scrappy as possible and prioritize techniques that will be most effective. We also try to leverage other company resources to achieve our goals by raising awareness through presentations and other forms of outreach. It’s okay to ask for help! We have received various offers of help: from individual engineers on our team, to experimental advertising budget from the People Team, to resources for organizing events from the Pineapple Team (our company-wide D&I committee) Illustration by Camellia Neri . How we plan to achieve our goals Keep learning We bring in diversity experts from leading companies such as Pinterest to advise us on various strategies. We gather feedback from female engineers about our hiring process. We speak with Gusties who have experience tackling these issues at other startups. We use all these sources to feed back into our backlog. Balance the pipeline We focus on finding ways to bolster support for the recruiting team. Internally, we encourage people to make referrals and introductions to candidates to drum up interest. We examine our interview process for bias by looking at the data and gathering feedback from senior engineers about whether it appeals to experienced candidates. Reach out with care We believe that candidates considering new roles want to know that we care. That’s why we invest in external communication and discussion. That includes focusing on our content  — for example, supporting female engineers who already love technical writing or speaking, periodic profiling of women in engineering, and storytelling as a means of encouragement and inspiration. We also foster discussion by organizing events around various themes related to career development. Develop from within As much as we have company leadership buy-in, we want buy-in from our engineering managers (or PEs, as we call them, which stands for People Empowerers) to help steer and guide career development. That’s why we encourage experienced engineers to be sponsors and mentors through our Engineering Mentorship program. Illustration Camellia Neri Gusto Engineering has come a long way since only 11% of the team were women. While it’s exciting that we are now at 24% women engineers and we’re proud of the strides that we’ve made, we also recognize that we still have a lot work to do, particularly when it comes to racial diversity. As we keep tracking the data on the Mango Team, I am excited to continue learning and sharing what works. 🚀 Come join us! We are hiring . 🙌🏼 About the Author Upeka is a software engineer from San Francisco, who has worked in the cloud software industry for 11+ years. She's passionate about frontend tech, engineering excellence and diversity in engineering. She's an avid plant assassin.", "date": "2018-03-04"},
{"website": "Gusto", "title": "5 Lessons from Taking a Sabbatical as a Co-Founder/CTO", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/5-learnings-from-taking-a-sabbatical-and-travelling-alone/", "abstract": "One of the benefits we have at Gusto is a one-month paid sabbatical after five years of employment with the company. As a co-founder of the company, I was among the first employees to be eligible to take the sabbatical. As someone who’s never taken so much time off and traveled alone before, I felt a little bit of excitement and lot of anxiety. I’m a somewhat shy person and meeting new people isn’t exactly my forte. Would I get lonely and want to go home early? What if something bad happens at Gusto while I'm away? What if I don’t get that epiphany or creative insight that so many tech workers say sabbaticals are useful for? Would I still be as excited about Gusto when I come back? Lots of questions swirled through my mind, but I finally booked a one-way plane ticket to Chile with no plans other than to figure things out once I land. By the end of my trip, my mindset completely shifted. All the questions I had were eventually answered, my mind quieted, and and I learned a few important life lessons which I’ll share here. Lesson #1: Don’t plan everything out and check the boxes. Don’t go on TripAdvisor and figure out a way to optimize your itinerary to see all the top 10 things to do in a day. In our world of Pivotal Tracker, JIRA boards, OKRs, sprint planning, and standups, it’s easy to fall into that trap, especially if you’re an engineer and love to optimize. Instead, take extra time to experience it. It’s better to hike up a valley to have Gray Glacier gradually reveal itself to you over a period of eight hours than to sign up for an overcrowded boat ride that gets you there in an hour so you can snap a few photos before moving on to the next destination. The photos of the glacier you show your friends are exactly the same, but your experience and memories of it are completely different. It’s the difference between having a crawfish boil with friends versus ordering pre-peeled crawfish at a restaurant that you shovel into your mouth with a fork. Stopping to eat lunch in front of a lake en-route to Gray Glacier in Patagonia. Lesson #2: Traveling alone isn’t just about being alone. Traveling alone isn't only nice because you get much-needed “me time” to reflect on your life. You definitely do get a fair bit of that, but truth be told, it gets old pretty quickly and you start to feel lonely, even as an introvert. More than getting valuable alone time, I learned that traveling alone is nice because it opens up more opportunities for you to meet people from different walks of life you would otherwise not meet if you were travelling with friends. After getting tired of reading my book on the bus, I mustered the courage to strike up a conversation with the person sitting next to me. He turned out to be Peruvian traveling on vacation as well. Though he was from a completely different world from my own, we became the best of friends for the next few days, simultaneously learning about each other’s worlds while sharing amazing new experiences in Chile together. Lesson #3: First impressions are often wrong. I met a lot of people that I prematurely judged before taking the time to get to know them. One person I was sitting next to had headphones on and didn’t make eye contact with me for a couple hours. Though I didn’t talk to him, I thought him to be unfriendly. Despite my impression of him, I decided to talk to him anyway. I realized my first impression of him was completely wrong. We started planning all our hikes together, jumped in freezing cold lakes together, and shared many interesting facts with each other over meals. When I told him I would’ve liked to extend my stay one more night at the hotel but the front desk told me that there was no more availability, he secretly campaigned to the hotel manager (who he had a connection to) to try to make it happen for me. On multiple occasions, people I took the time to get to know turned out to be very different from how I initially imagined them to be. I learned that when I’m afraid or intimidated to initiate conversation with someone, I tend to project a slightly negative judgment on that person. Perhaps this is one of the ways my mind rationalizes not talking to them. Once I fought my intuition and struck up a conversation with that person, I found I really enjoyed getting to know them. We should resist our impulses to judge what someone is like before actually engaging with them. After getting to know someone, you find yourself pulling tables together so you can eat next to each other. Learning #4: To disconnect from work, tell your colleagues you’re going to disconnect. One of the biggest personal challenges I set for myself on the sabbatical is to learn how to disconnect from the day-to-day when I’m on vacation and focus on myself. This may sound obvious at first, but I learned that there can be a virtuous or vicious cycle to disconnecting. If you tell your colleagues you’re going to disconnect and won’t be very available, they send you fewer messages. If you get fewer messages, you start to check your inbox less often. This in turn reinforces the fact that you’re not as available. Eventually, the messages that need your direct response peter out. The opposite is also true. If you respond to a message, it signals to your colleagues that you’re available, which encourages them to send more messages to you, compeling you to be more available. As someone who’s always checked email on vacation and never used an email auto-responder in my life, I started my sabbatical not being successful at disconnecting from work. After years of being “on,” it became habit to check Slack and email as soon as I whip out my smartphone to look for directions to get back to my hotel or find a translation for a word. It’s exacerbated by the fact that you’re alone and sometimes check out of boredom. It was also hard to get over a sense of guilt about being disconnected. The only way I was able to turn this vicious cycle into a virtuous cycle was to completely uninstall Slack and email from my phone. Miraculously, this worked. Colleagues started feeling empowered to make decisions without my input. Messages requiring my direct response slowly petered out. By the second week, I eventually felt at ease with not checking messages daily. Lesson #5: It’s hard to meet people just by sitting alone in bars or restaurants. But other options exist. People say staying at hostels are the best way to meet other people when you’re traveling. At this stage in my life (I'm 35), I’ve grown a little too old and accustomed to comfort to stay at hostels. If you’re willing to splurge a little (and I think you should on something as rare as a sabbatical), programs like Explora are great at fostering a community of travelers and make it much easier to meet people. You still have to put yourself out there and get comfortable with the awkwardness of walking up and introducing yourself to another traveler, but I found it a lot easier than meeting someone at a bar or restaurant and a lot more comfortable than a hostel. These programs do tend to attract older couples (who were among the most interesting people I met!), but there are also enough like-aged travelers to form a good gang. Some very kind Canadian travelers I met. Final thoughts. I feel incredibly lucky to have had an opportunity to take a sabbatical through Gusto. I started my sabbatical with anxiety, apprehension, and guilt, but ended with greater happiness, some important life lessons, and a deep sense of gratitude for my life. It was truly a once-in-a-lifetime experience. I feel thankful to be able to do a personal sabbatical AND have a great place to go back to work afterwards. Did I get lonely on my sabbatical? Yes, especially in the beginning when I was still learning the above lessons. Towards the end though, I got really good at breaking through my shyness, initiating conversations, and making new friends. The last day of my trip felt a lot like summer camp ending: We were giving each other teary-eyed goodbyes and making promises to see each other again. Did things break down at work? No. Gusto charged on without me, and especially so once I learned to disconnect and my colleagues felt more empowered to make decisions without me being there. I think this was a huge win for my team in its ability to scale. Did I achieve an epiphany or have the “ah-ha” moment during my sabbatical that tech workers sometimes say they have? Personally, I didn’t. I won’t be making a bunch of drastic changes my first day back at work as a result of my time away. But I do believe that taking an extended break and traveling alone gave me a broader perspective, some great life lessons, and made me a happier person. And that’s definitely good for Gusto. I’m incredibly excited to get back to work helping Gusto achieve our mission and I come back a wiser and happier person, with a much broader perspective. Comments/discussion on Hacker News", "date": "2018-03-14"},
{"website": "Gusto", "title": "Eliminating Flaky Ruby Tests", "author": ["Nikhil Mathew"], "link": "https://engineering.gusto.com/eliminating-flaky-ruby-tests/", "abstract": "Moving Fast and Not Breaking All The Things Every day, Gusto engineers work on systems that move billions of dollars, secure sensitive customer information, and pay hundreds of thousands of employees. Our customers' livelihoods depend on our product, and we need to be able to respond to issues quickly, maintain fast iteration velocity, and ensure the stability of our applications in order to support them. None of this would be possible without a rock-solid continuous integration (CI) system that gives us confidence that our changes are safe to rollout. However, even after investing heavily in building a scalable CI system to support our infrastructure, we noticed flaky tests cropping up in our builds more and more often as our application grew. This had destructive effects on developer productivity: builds were needlessly retried, trust in our test suite was eroded, and frustration with our CI system grew among the team. Non-deterministic tests led to slower iteration velocity because we couldn’t rely on our build results, and fixes were prevented from being shipped in a timely manner. For a high-growth tech startup like Gusto, this could have proven financially disastrous. In response, we set out on a team-wide effort to debug and stabilize our test suite. Root causes for flaky tests proved difficult to track down, but we found they were often traceable to a handful of common pitfalls. We’ve assembled a list of the most common failure modes below. This best-practices checklist is used by Gusto developers to maintain a clean, deterministic RSpec test suite. Our test suite, using these best-practices, currently validates 300,000 lines of application code by running 40,000 RSpec examples over 250 times a day, with minimal levels of flakiness. Dangerous Capybara Wait Matchers For many Rails apps, flakiness presents itself in Capybara request specs. There’s a lot of moving pieces to these tests — a browser is running your application while a computer is triggering UI events faster than any ordinary human could. It can be difficult to ensure assertions are timed correctly when lots of client-side interactions and AJAX requests are occurring. When you use a Capybara method that doesn’t wait for an element to appear, you may inadvertently encounter a race condition. For example, if you expect to: In your client, receive a 422 response from the server Render an error message based on that response You will want to ensure your test waits for the error message to be rendered. Unfortunately, Capybara has a variety of methods that do not wait for an element to show on the page, and the docs aren’t as explicit as we would like. Here is a list of methods we avoid when writing Capybara request specs: Avoid dangerous methods that do not wait visit(path) / current_path all(selector) / first(selector) Accessors: text , value , title , etc Use safe methods that do wait find(selector) , find_field , find_link has_selector? , has_no_selector? Actions: click_link , click_button , fill_in If you use safe methods, Capybara will wait until the configured timeout ( Capybara.default_max_wait_time ) for the element to appear. Using these safe methods that do wait when asserting against the DOM will ensure tests are resilient to asynchronous actions in JavaScript code. Database Pollution Database pollution occurs when we do not have an self-contained database to work off of when running a new test example. Common failure modes include unexpected numbers in COUNT queries and failed uniqueness constraints while trying to create new records. While the failures seems cryptic, we can apply a simple solution for the entire suite. database_cleaner — If a test creates records or modifies the database, it’s important the changes are removed before future test runs. We can hook into the RSpec lifecycle to clean the database between each test example. The database_cleaner gem with some simple RSpec configuration options will do this for us: require ‘database_cleaner’\nRSpec.configure do |config|\n  config.before(:suite) { DatabaseCleaner.clean_with(:truncation) }\n  config.before(:each) do |example|\n    if example.metadata[:use_truncation]\n      DatabaseCleaner.strategy = :truncation\n    else\n      DatabaseCleaner.strategy = :transaction\n    end\n    \n    DatabaseCleaner.start\n  end\n  \n  config.append_after(:each) { DatabaseCleaner.clean }\nend By default, we opt for using the transaction strategy because it is faster; a transaction is opened at the beginning of the example, and simply rolled back when it completes. If we use truncation, we need to issue an additional set of queries to truncate the tables. We highly recommend using the transaction strategy wherever possible as the DB time saved really adds up. Occasionally, truncation is required, most notably when running request specs with Capybara . The thread running your Rails server uses a separate database connection from the one running your test, so neither thread can see database records from the other before they are committed. You’ll notice the usage of append_after(:each) for invoking the DatabaseCleaner.clean method, ensuring that the DatabaseCleaner runs only after all the shutdown steps for Capybara have been run. This prevents any race conditions between the app thread and the server thread from leaving orphaned records in the database. In Rails 5.1+ apps, the database connection is shared across threads and the transaction strategy can be used to avoid this altogether. TEST_ENV_NUMBER — Giving each test process a unique database will make the test suite more deterministic. Rails will create a single dedicated test database, but when running tests in parallel (for example, using parallel_tests ), every process can use the TEST_ENV_NUMBER environment variable to connect to a unique database. Order Dependent Tests Sometimes, although a test file may succeed as a whole, specific examples may fail when individually run. This means that each test is not running in an isolated environment, leading to cross-test pollution, and flaky failures. RSpec tries to combat this by providing randomized test ordering, allowing you to specify a reproducible, random run order via the --seed flag. Although by default RSpec runs tests in their defined order, setting the --seed flag helps root out these failures early in the development process. For example: describe ‘order dependent specs’ do\n  let(:response) { $redis.get(‘my_third_party_response’) }\n  let(:parsed_response) { JSON.parse(response) }\n  let(:client) { Client.new }\n  \n  context ‘with params’ do\n    let(:params) { { limit: 10 } }\n    before do\n      $redis.set(\n        ‘my_third_party_response’,\n        client.get_response(params),\n      )\n    end\n    \n    it ‘returns the params’ do\n      expect(parsed_response).to have_length(10)\n    end\n  end\n  \n  it ‘has the correct IDs’ do\n    ids = parsed_response.map { |j| j[‘id’] }\n    expect(ids).to match_array [0..9]\n  end\nend The second example in this file only passes because the first example populated a shared resource. If the second example were to be run on its own, it would fail since there’d be no value populated in Redis. It’s an easy mistake to forget to hoist the before block up to apply the test setup applies to both examples. Do yourself a favor and let RSpec help you by randomizing your tests. Non-Deterministic Attributes When testing how a particular attribute behaves, always use deterministic attribute values. Testing inclusion validations is one common example of this. In general, avoid using variables that change from run to run like: context ‘with a valid attribute’ do\n  let(:model) { build(:animal, type: type)\n  let(:type) { [ ‘cat’, ‘dog’, ‘dragon’ ].sample }\n  \n  subject { model.valid? }\n  it { is_expected.to be_valid }\nend This code expresses that cat , dog , and dragon are all legitimate values for the animal type. However, a different value will be used on each test run, meaning this test is not resilient to changes in the system. If dragon were removed as an allowed Animal#type , this test likely would not fail in a pull request for that change. Opt for explicitness in these cases and write a separate test case for each type. Faker is a commonly used library for creating realistic test values. Its random generators create helpful, and entertaining, data, but they occasionally introduce unwanted randomness. context ‘with a changed email’ do\n  let(:model) { build(:my_model, email: Faker::Internet.email) }\n  let(:new_email) { Faker::Internet.email }\n  subject { model.save! }\n  it ‘sends an email’ do\n    model.email = new_email\n    expect { subject }.to change(ActionMailer::Base.deliveries, :count).by(1)\n  end\nend This will work most of the time, but inevitably a test run will cause Faker to return the same email, making the test fail. In examples like this where we need explicitly different values, leave Faker at home and hardcode them. This also applies to using factories with factory_bot where values need to be unique. When populating columns enforced by a unique index, be sure to take advantage of sequences to guarantee there won’t be unintended uniqueness constraint violations. FactoryBot.define do\n  # Bad\n  factory(:user) do\n    email { Faker::Internet.email }\n  end\n  # Good\n  factory(:user) do\n    sequence(:email) { |n| “#{n}_#{Faker::Internet.email}” }\n  end\nend ID Dependence Many applications use auto-incrementing IDs in the database, but you can’t always guarantee that the record you create will have the same ID in each test run. When tests run in a transaction, every record created will bump the auto-increment value on your table. Because you cannot guarantee how many records are created before the test, there is no reliable, static value for new records’ ID. Additionally, when tests run with truncation, the auto-increment will be reset for that table. Instead, always use the model.id accessor when writing assertions. context ‘when saving a file with an artifact’ do\n  let(:document) { create(:document, artifact: ‘test’) }\n  subject { document.save_file! }\n  # Bad\n  it ‘writes to the right path’ do\n    expect(File).to receive(:open).with(‘/tmp/documents/1/artifact’)\n  end\n  # Good\n  it ‘writes to the right path’ do\n    expect(File).to receive(:open).with(“/tmp/documents/#{document.id}/artifact”)\n  end\nend Globally Cached Values Memoizing values with instance variables is a very common pattern in Ruby. However, when this is done at the class level, tests can pollute one another by persisting stubbed values across examples. # state_checker.rb\nclass StateChecker\n  UNSUPPORTED_STATES = [‘CA’, TX’]\n  \n  def self.supported_states\n    @supported_states ||= STATES — UNSUPPORTED_STATES\n  end\nend\n\n# state_checker_spec.rb\ndescribe StateChecker\n  describe ‘.supported_states’ do\n    before do\n      stub_const(‘STATES’, [‘AA’, ‘BB’, ‘CC’, ‘DD’])\n      stub_const(‘StateChecker::UNSUPPORTED_STATES’, [‘AA’, ‘BB’])\n    end\n    it ‘removes the unsupported states’ do\n      expect(StateChecker.supported_states).to match_array %w(CC DD)\n    end\n  end\nend At a glance, this feels like a reasonable way to test the logic of the MyService.supported_states method. However, any subsequent tests that use this method will read the stubbed value instead of the actual supported states value, because the class instance variables are not cleared. We should therefore avoid stubbing when testing memoized methods. However, if it must be done, the memoized method can be reset with MyService.instance_variable_set(:@supported_states, nil) . Time-Based Failures As a payroll company, we deal often with code that handles business days, quarter-end filings, and other time-based computations. For example, when running a payroll, we compute the day you should receive your paycheck (known as the “check date”). This value is computed based on business days, and can be different based on which day of the week payroll is run. We’ve noticed that sometimes these tests fail because they rely on an implicit assumption that they will always be run on Mondays. In order to combat this, we make sure to use Timecop , stressing each particular date range explicitly. describe Payroll do\n  describe ‘#check_date’ do\n    before { Timecop.travel(today) }\n    after { Timecop.return }\n    context ‘on a Friday’ do\n      let(:today) { Date.new(2018, 4, 13) }\n      it { is_expected.to be_monday }\n    end\n    context ‘on a Monday’ do\n      let(:today) { Date.new(2018, 4, 16) }\n      it { is_expected.to be_wednesday }\n    end\n  end\nend As best as possible, we try to Timecop all examples that return different results depending on what day they are run. We’ve even wrapped the before and after blocks you see in the example above into a method called time_travel_to that will handle Timecop cleanup after each example. Reproduction The next time you come across a flaky test in your test suite, be mindful of the above list, and see if you can reproduce it locally with the following steps: Run RSpec using the same seed that was used in your CI run Run the entire list of files that were run before your failing test Use the --bisect flag to let RSpec do the work of finding the polluting example Reference this list of common polluters to see if any fit the bill Conclusion As Gusto’s business and codebase grew, we noticed flaky test suites slowing down iteration cycles and compromising developer productivity. By investing in a stable CI system and flaky test elimination, we’ve been able to iterate more quickly on changes, ship features to our customers more rapidly, and improve developer happiness. While it requires diligence to stay in front of these and other issues, the above guidelines have helped us root out non-deterministic RSpec tests in our builds and keep our test suite running smoothly.", "date": "2018-04-02"},
{"website": "Gusto", "title": "The shortest path is the scariest", "author": ["Omri Ben Shitrit"], "link": "https://engineering.gusto.com/the-shortest-path-is-the-scariest/", "abstract": "How optimizing for learning helped us scale to 1% of small businesses in the US “Your vision is great, but it's all about execution.” If you’ve worked in startup environments or have tried to raise money for one, you probably heard this stated in some way, shape, or form. There are seven billion people in the world and it's quite likely that several other companies have already thought of your vision, tried to execute on it, and failed. Companies may have failed to deliver on your vision for a variety of reasons unrelated to execution. But for all companies starting under the same market conditions, execution -– the ability to make your vision into a reality –- is the key differentiator. As an engineering, product, and design (EPD) team, we strongly believe that our ability to execute on our vision is one of our key competencies and has fueled our success to date. So what is execution? And how do we judge what it means to be good at executing as an EPD team? To be good at execution as an EPD team we must: Continuously identify the next most important problem we should be solving for our customers. We’ve written about this in the past . Understand (roughly) the cost and benefit of solving the problem, and our confidence level in both side of the equation. Find the optimal set of tradeoffs between product scope and engineering costs in solving the problem. Why shipped software can be the wrong metric of success In its most basic form, the Agile process focuses on shipping working software in close collaboration with customers. Teams are encouraged to ship incremental product value every sprint, and there’s some amount of guilt associated when sprints end with less than 80% of the value being shipped as production code. The motivation behind this focus is quite logical: getting working software in the hands of customers tells us what value they get out of it . If the value they get from it doesn’t meet our initial hypothesis, then the return on investment for this project is lower than anticipated and we can stop working on this problem and go back to step one and identify the next problem. While this process makes sense in specific contexts, to get really good at execution and find the optimal path, we need to look a lot more closely at a few other dimensions and vary our process accordingly. How to know when your Agile process is suboptimal The two dimensions we focus on initially is our confidence level in the cost and value of the project. This sets the tone for the type of methodology we’re going to use to tackle the problem. The confidence level can be derived in multiple ways, but usually the most telling sign is to pay close attention to the team working on the project when they’re first being asked to tackle the problem . “This is either a one-week or a 6-month project,” one engineer would share, as they’re contemplating all the possible ways the implicit assumptions in our intentionally ambiguous and low-fidelity problem statement will blow up as rubber meets the road. “I worry that if we built it, customers won’t get the value we think they’ll get,” another engineer would share as they’re calling upon all the tacit knowledge they’ve built over the years about our target customer, and specifically remembering one support call they shadowed when one customer didn’t express any need for this. The high variance in the team’s assessment of the project and the sense of worry, or lack of both, is our “sniff test” for the project. What’s the distance between the problem space and the edge of the company? High confidence in cost, high confidence in value: Projects where we have high confidence in their cost and value are core to what we do as a company. When we extend our payroll product to support a new electronic data exchange with a tax agency, we know exactly the value it provides to our customers and what it would take for us to do so because we have honed that muscle. This is an area of the company where our business is already established. High confidence in cost, low confidence in value: Projects where we have high confidence in their cost, but low confidence in its value to customers, are at the edge of our product offering as a company. Even though they present no technological challenge, because it's “yet another feature” on the current tech stack which we’ve used for years, there are question marks around the customer demand. This is an area of our company where our business is venturing beyond of our established product offering. High confidence in value, low confidence in cost: Projects where we have high confidence in their value, but low confidence in their cost, are at the edge of our existing tech stack and/or code base. These can be on the more obvious extreme side — “our first machine learning attempt at automating what we do today with humans and has proven to be successful” — but more often than not they’re much subtler. For example, you might be iterating on your core data model which made a one-to-one assumption between two entities and now you’re changing it to one-to-many, but that could fundamentally change how many other parts in your system works. This is an area of our company where “the how” of solving a problem is venturing beyond our established code base or the team’s experience with the technology. Low confidence in cost, low confidence in value: Projects where we have low confidence in their cost and value are at the edge of everything we currently do as a company from a product and engineering perspective. These present the most risk, which makes them even more exciting. Lose the hammer, go for the swiss army knife Once we understand the context, we then look to truly understand whether basic Agile process is our tool of choice. In our experience, it works best only when you have high confidence in cost and low confidence in value . The optimal thing you could do to get to high confidence in value is ship software iteratively so that you can test your hypothesis with customers in production and get feedback as soon as possible. When we’re working on a known quantity, we keep process to a minimum because frankly, we don’t need anything else other than a to-do list to get it done. For projects with low confidence in cost, we amp way down the focus on shipped software and we focus exclusively on a process to journey us through the Dunning Kruger curve . Project Kickoffs At the kick-off of the project we do many throwaway technical spikes and code experiments until we feel confident we’ve hit the peak of “Mt. Stupid,” followed by the “Valley of Despair.” In the previous example of the data model change, this could mean something as simple as to change the semantics of the entity relationship to one-to-many (Why would this be hard? It's a one-line code change), and seeing how many tests break if we ran our test suite and how many errors would happen if we took production data and tried to exercise the codebase against it. The goal is not to answer any important questions of how we’re going to make this change but rather, what is the surface area of the blast radius from making the change. A good sign to know if you’ve reached the “Valley of Despair” is when the number of things on your backlog of “things you need to think about” has multiplied by five to ten. The good news, there’s no way but up from here. Reaching the slop of enlightenment “Maximising the rate of learning by minimising the time to try things.” — Tom Chi, co-founder of Google X To start making our way up the slope of wisdom, we focus on identifying the smallest yet complete set of code experiments that will provide us with confidence in our abilities to solve the problem and at what cost, separating proof from optimization. At the heart of a steam engine, is the fundamental ability for steam pressure to push a piston up and down, regardless of how efficient or inefficient this engine is in converting 100% of the steam energy. Similarly for code, we focus on identifying and proving feasibility for key questions. These vary based on the type of the problem, but for instance, when integrating a new third-party API, we might ask ourselves: Does this API’s authentication and authorization capabilities meet our needs? Can we orchestrate all needed behaviors? Are we able to correlate the API’s incoming asynchronous error events with their original requests? If so, using which data elements? This is inherently different from writing standard production code, which could add a lot of other elements to the development process such as integrating with existing systems like a production credentials vault, exceptions and log tracking systems, working with longer continuous integration cycles, etc. While these are critical for the optimized version of the API integration, they provide no value in learnings and increase iteration time in trying things. The real special sauce is in the side effects By kicking off a project (which undoubtedly is very exciting given the uncertainty or potentially anxiety-inducing in a good way if you’re more on the anxious side like me) with the Part 1 loop, the project team builds deep context into the problem space in an accelerated time frame. Key dependencies and assumptions are put to rapid tests and no implicitness is left uncovered. We rally broad team support for decisions early on by deliberately “going down the rabbit hole” of the major decision points at the projects start, which paves the way for minimal rehashing of decisions being revisited. By substantially reducing the cognitive overload of uncertainty early on, we create an abundance of clear headspace to focus on delivering a delightful product at our high standard of code quality, without the constant “well, what about…” chipping away at focus. Try it with your team! Deliberate retrospectives “What could we have learned faster this week given what we knew at the time?” This question encourages the team to constantly evaluate the iteration cost they’re paying to learn new things, and specifically moves away from learnings based on what we know now. Hindsight is always 20/20. We want the team to develop better judgement about decisions they took given the information they had at the time, and not what we know today . This gap in decision-making judgement is your success criteria. If you’ve hit zero, you’ve made it! Bake uncertainty into planning Focus estimates on a range of outcomes. There are many ways to bake uncertainty into your planning process. One simple tactic we use is range estimates based on best/expected/worst. If you break down your project plan into a list of line items for “things you need to figure out and/or build” and keep each line item’s estimate to no longer than a week, it shouldn’t take very long to break down each line item. Doing so will give you a “heat map” of where your biggest unknowns are, as well as a great opportunity to have conversations with your stakeholders on the tradeoffs between scope, costs, and assumptions. Embrace risks throughout the project Normalize talking about the worries, risks and unknowns. Kick off the project by asking: What are we most worried about? What are our biggest risks? What are our biggest unknowns? What are the cheapest ways for us to mitigate risks and increase our confidence level? Write things down Ambiguity is better stored in documents than people's minds. Write down all the unknowns the team needs to follow up on. The tricky thing about ambiguity is that it can create high cognitive overload, which can get in the way of forward progress. Having a single place where all questions are captured, as well as when we need answers by  before we’re blocked or slowed down, is a great way to combat that and also help your product team and stakeholders prioritize what they should look into next based on its time sensitivity. If you’re passionate about the craft of getting things done, come join us, we’re hiring! We’ve developed a specialized career track we call “Technical Anchor” for engineers who are interested in leading other engineers in getting things done. About the author Omri is a Software Engineer from San Francisco. Omri empowers engineering teams at Gusto and is passionate about lean product development processes. He hates wasting calories on bad food and loves making bad puns (to his team's chagrin).", "date": "2018-04-15"},
{"website": "Gusto", "title": "Can I have a big impact without becoming an engineering manager?", "author": ["Upeka Bee"], "link": "https://engineering.gusto.com/can-i-have-a-big-impact-without-becoming-an-engineering-manager/", "abstract": "Gusto Women In Engineering Roundtable Dinners Recently Gusto’s Mango Team , our Engineering-specific diversity and belonging committee, hosted an intimate roundtable dinner for experienced women in engineering. The topic of the night was “Can I make a big impact without becoming an engineering manager?” As part of our efforts on the Mango team and our goals for the next two quarters, we wanted to facilitate conversations that are relevant to more experienced women in engineering. While women with more years working in tech often get peppered with requests to act as role models for others (speaking, mentoring etc.), we suspected that they probably crave opportunities to learn-from and connect-with other women who are as experienced or more experienced than they are. We also wanted to share their very real stories as a source of inspiration to women starting out in this career. Our topic As an engineer and self-proclaimed introvert, I have been thinking about this topic for a long time. I love coding and I love my noise-cancelling headphones. But as year 10 of a 12-year programming career rolled around, I wondered: Is staying an individual contributor software engineer your whole life a good career choice? Should I be a manager? Is there a middle-ground — what some companies call the tech lead or anchor role? What path is interesting for me? Does my company support senior individual contributors (ICs)? Not all companies can and do. Do senior ICs get paid as much as managers, directors, or vice presidents? Can I make a big impact without becoming an engineering manager? Almost a year ago, I took on a role of technical anchor at Gusto, for a project to upgrade the frontend of our run payroll product flow. This first experience turned out to be a massive project and akin to drinking from a firehose for a first-time technical anchor. It led me to understand the depth of this topic, how the one-size-fits-all approach doesn’t work for everyone, and how much a company’s codebase can be affected by organizational approaches that we take. The Abstract Here’s the abstract that we sent to our roundtable dinner attendees. What are the secrets to growing your engineering career without becoming a manager? Not all engineers are interested in becoming managers, but is it a good choice for your career? What are some ways we can enable senior engineers who are not managers to have impact within our organizations? The logistics Our goal for the evening was to facilitate a thoughtful, authentic conversation, and enable the guests to connect with each other. I worked closely with Steffi Wu , who co-leads Gusto’s diversity and belonging program, to produce this event. We started out by codifying the topic and abstract. We secured a budget, date, time and location. General Catalyst Partners , one of Gusto’s investors, hosted us at their lovely office near South Park in San Francisco. Then we compiled an amazing list of experienced women in engineering who we thought could contribute to our topic. We included past co-workers and women we admire as strong role models in the industry. Dinner from Tacolicious at General Catalyst Partners office in South Park The conversation The lead up to the dinner was indescribably exciting for me. Like seeing a close friend after many years, I felt there was going to be so much to talk about . To prepare, we organized questions into three main categories: Choosing your engineering career path, including questions about being an IC vs. manager and making a career switch Coding while female, including questions about challenges faced, diversity, etc. Growing in your career Attendees mingle before sitting down for dinner Choosing Your Engineering Career Path We began the evening with a question to a woman who has an almost twenty year career of being a professional software engineer. She had recently taken on a role of CTO at a small startup. Let’s call her Princess Leia. “Princess Leia, as you've gone from being an IC for a long time to CTO of the Rebellion, how has your role changed? What are the new challenges?” She opened up the conversation by stating that the best advice she had ever received when trying to make her decision was from a man: “The problem with you is that you are a woman.” As controversial as it sounded at first, she appreciated his point about the systematic sexism that she would have to confront. I could already tell that it was going to be a great evening. She described being hesitant and lacking the confidence to take on this new role, and how her (male) mentor advised her that she was ready. The problem as he described was that she, as many other women do, lacked the confidence of many equally (or less) qualified male counterparts. And to add to this, research shows men are typically judged on their potential, while women are judged on their actual performance. “If you've been both a CTO and a VP of Eng in your career, can you speak about the differences in those roles and how they functioned at your company?” Another woman described her journey of moving from a senior IC to being the Head of Engineering at her company. She had observed a void in leadership and began taking on the leadership role without asking for permission. From then on she worked hard and the official change happened quickly. However, the increased responsibility and workload was not without side effects. She spoke about how she had to learn to take care of herself both physically and mentally, as well. “If you've stayed an IC for a long time, what are the different ways you’ve made an impact in your company?” A woman spoke about her experience working for a company that grew so quickly that senior ICs became highly specialized — so much that moving between management and IC tracks was difficult, especially going back to the IC track after having been in management for a while. She described her own impact, owning and executing on a system that helped the company monetize, which invariably was very valuable to the company. Another two women who were coworkers, described a system that a large tech company (that we all know of) had in place for keeping careers interesting for senior ICs. They had an “open-market” system that allowed teams to advertise roles that were open and also allowed engineers to move freely within these teams. This method had helped these two women to experience, learn, and work on many different teams within the company, over their long tenures. Another interesting phenomenon that some women described was around hiring policies. It’s a well-known fact that engineers move freely within tech companies as opportunities arise. A woman who has had many jobs in her long career described how a more established tech company took a hostile approach when she decided to leave the company after a long stint. In comparison, a newer tech company took a much more welcoming approach, saying “Feel free to to come back whenever you wish.” Ironically, the latter company has many more instances of “boomerang” employees. “For those of you who have a deep interest in social impact, tell us your story and what your career looks like now.” Another interesting segment of the conversation was about social impact. Some attendees had a deep interest in social impact through organizations such ACLU and Chan Zuckerberg Initiative. They described why such a path was appealing. One women described how social impact initiatives are typically bad business models — for example, putting systems in place to help find a cure for a disease will, if you’re successful, eventually lead to elimination of the disease. Another woman spoke of being moved to action through recent political events and how her day-to-day work looks a bit different from being an engineer at a technology company. Coding While Female Then it was time for the question I had been waiting to ask the whole night. “For a multitude of reasons, senior women in engineering find themselves under-leveled or under-compensated. Have you ever asked for a correction? How did you find the strength to rock the boat?” I expected an awkward silence, thinking that no one would be willing to come forth with such personal details. Instead, one woman yelled “Anger!” and launched into a story about asking for a level correction on the very first day of her new job. Her company had set the expectations about her new role incorrectly during hiring, and while her title was corrected immediately, her compensation wasn’t adjusted for another 6 months. I was at once inspired and in awe of this woman. Having been through an incorrect levelling situation myself (and taking a year to find the courage to speak about it), I was surprised to learn that the situation wasn’t as uncommon as I had thought. Before I knew it, two more women chimed in about having to negotiate their compensation and hating it — but fighting through. They talked about pushback from recruiters who threatened that their offers would expire and employed other hostile tactics. Another spoke of asking for a level-change from their manager and being told no. She described how she fought by providing evidence of her peers’ levels as well as benchmark compensation in the industry, and finally threatening to quit. “Have you experienced coworkers in the workplace treating you as more junior than your experience and level should indicate, and if so, how do you manage this?” This was a difficult question, and to be frank, I’m not sure we came up with a good answer to this question in our discussion. There was consensus that this needs to be addressed at a system-level, related to unconscious bias training and allies within companies. Attendees spoke about how it’s important for leadership to set these examples and how people from underrepresented groups should look for these traits when looking for jobs. One attendee brought up a very important point about how the burden of change should be on the majority and not the minority, in this case women in engineering. Readers, if you have any thoughts or ideas about this we would love to hear them — please leave a comment! Growing Your Career Finally, attendees discussed and recommended multiple resources for learning and growing in careers. These included: Peopleware: Productive Projects and Teams It’s Okay To Be The Boss What Works for Women at Work Ally Skills Workshop The Manager’s Path Conclusion I woke up the next day feeling elated. The conversation had been riveting. There was so much to process. We were thrilled to receive thank you notes, with some attendees saying it was their favorite event of the kind that they had attended, and others who loved our gift bags (read: socks! ) One of my favorite moments of the night was reflecting with Josh, our CEO, about the burden for change. An attendee had raised the point that the effort to drive change should not be placed on the minority (in this case women in engineering), and instead should be driven by the majority. This got us thinking about more ways that we could improve within Gusto, such as Ally Skills Trainings , which we are now facilitating internally at Gusto. Finally, though there wasn’t enough time to discuss everything we wanted, I was grateful for the feelings of connection and renewed inspiration. Moderating wasn’t as hard as I had expected and in fact, it was fun! I felt a lot of connection because a lot of people in the room had faced a lot of the same challenges that I have in my career. Our next roundtable dinner is about “Significant Moments in Your Career” coming up in July 2018 at our new Gusto office in the Dogpatch . Please DM us on Twitter if you would like to be invited to our next dinner! We hope you will join us at a future event as we continue to host these important conversations. About the Author Upeka is a software engineer from San Francisco, who has worked in the cloud software industry for 11+ years. She's passionate about frontend tech, engineering excellence and diversity in engineering. She's an avid plant assassin.", "date": "2018-06-11"},
{"website": "Gusto", "title": "What I Learned By Teaching Fourth Grade Girls to Code", "author": ["Lindsey Whitley"], "link": "https://engineering.gusto.com/what-i-learned-by-teaching-fourth-grade-girls-to-code/", "abstract": "Emily helping a student get started. How it started A teammate and I were headed downstairs for lunch when he said something like, \"Beyond focusing just on Gusto, we should be doing more to change the ratio of women to men in engineering.\" Little did he know I had been toying with the idea of teaching girls to code, but I wasn't sure until then that I wanted to do it with Gusto. Choice is power. Yes, I love my coworkers, the challenges of data, the ever-changing landscape of software, and the chance to solve problems for our customers. Ultimately, though, I'm grateful for my job because it gives me the means to be independent. This industry provides flexibility to learn new things every day, to work with different teams, to live almost anywhere, to adjust my schedule for bumps in the road, or to save/invest/spend as I please. I am lucky that I grew up with a family and community that always encouraged my agency and voice. Teaching girls to code empowers them in the same way by exercising their brains and pushing their boundaries. Since I hadn't done anything like this before, I talked to our Head of Engineering in Denver to pick his brain about any red tape. Gusto was totally supportive, so all I had to do was plan and execute the event. What happened 1. Garner support from my team. I wasn't about to teach fifteen to twenty girls to code by myself, so I needed to enlist the help of my fellow engineers. To get them involved, all I really had to do was explain the vision I had for this event - getting girls to identify with \"engineering\" as a viable career option -  and they were in. Their involvement ended up being key to the quick execution of this event because ten heads are way better than one. I talked to a few of my teammates individually first; then I brought it up with the whole team at lunch. Simply saying, \"Let's teach elementary school girls to code. I'll figure out a lesson plan and organize the event. Will you help facilitate?\" was enough to whet their interest. The only further explanation needed was that the weakness of diversity in our pipeline is due, in part, to the overall lack of diversity in the industry, and we can start changing that more proactively. 2. Find girls to teach. Our goal was to teach girls from a mix of socio-economic and ethnic backgrounds. Dora Moore teaches a mix of students that reflects the demographics of the diverse CapHill neighborhood of Denver. A stroke of luck - a teammate's friend, Stephanie, is a fourth-grade teacher at a school about two miles from our office, called Dora Moore School. Even better, when we talked to Stephanie, she was as excited about this idea as we were. She brought in the other fourth-grade teacher, Dory, which allowed us to teach every single fourth-grade girl at the school. 3. Decide what to teach. Since I don't have much experience creating curricula, especially for fourth graders, I searched online for what to teach. I found an organization called TechGirlz that provided different lesson plans and platforms, including the one we used, called Blockly . Blockly has a few different \"games,\" of which we used the Puzzle and the Maze . To make sure that all of the students understood the basics, we started them out on the Puzzle. Afterward, the Maze introduced loops and if/else statements while giving them a way to relate to the code spatially. An example problem used in the workshop with its solution. In this case, the number of blocks (shown on the right) are limited to avoid brute force solutions. 4. Organize logistics. I worked with Stephanie to arrange for us to come to her classroom. This was easier than having the students come to Gusto because it avoided permission slips from parents and the transportation of twenty students. She handled the school logistics and I managed to find a two-hour window for seven engineers to help. Fortunately, the classroom had enough Chromebooks for the students so we didn't have to come up with twenty laptops to take with us. 5. Prep the instructors. Another stroke of luck - Emily, one of our San Francisco-based engineers, was in Denver for a three-month rotation. Aside from being an extremely capable engineer, she has experience working with kids and the confidence and charisma to keep order in a classroom. With Emily on board to teach, all I had left to do was finalize logistics. After I created the slide deck, the team convened to go over the plan and the activity for the event. This was also a great time to get feedback on the presentation, the plan for timing, and the activity itself. 6. Do the workshop. We got to the classroom about fifteen minutes before we planned to begin and got set up. Emily led the intro discussion about what engineering is, why she does it, and what we'd be doing. Then we were off to the races. We wrapped up about ten minutes early to give the girls time to ask Emily, Kendra, and me any questions they had. The team setting up laptops and checking that the tool would be accessible on the school's wifi. What we learned Before deciding what curriculum to use, understand the students' current knowledge. The fourth graders we taught hadn't taken geometry yet. This kind of information is probably accessible online, but with such a helpful teacher, I could have asked more questions to understand how to incorporate the math they are learning into the activity. Unbeknownst to me, the girls have a technology class and had done projects similar to Blockly. If I had known this, we could have spent a bit less time on the intro and more time on more interesting problems. There will always be a wide range of prior knowledge among students. For example, one girl finished the project on her own, but another needed much more help to keep moving along. Combat this disparity by using part of the workshop to do pair programming. The first time around, pairing was an afterthought, and the girls received no instruction on effective techniques. Giving ground rules for how to pair and setting the expectation that it would be part of the activity would have encouraged teamwork and avoided diminishing self-esteem brought on by asking for help. Provide focus for questions. Fourth graders are curious about everything. If you don't provide them with guidelines, questions can quickly go from \"How did you know you wanted to do this?\" to \"How old are you?\" If you do remind them what you're there for, they will ask insightful, helpful questions. Topics we might use to focus their questions in the future include: how we collaborate, what educational path we took to get where we are, and what an average day consists of. Lindsey and Emily answering questions from the class. Sell the idea that coding is a valuable skill. To lead the companies of the future, they need to know about technology, even if they don't want to write code themselves. They are probably already interested , so use that to your advantage. Connect coding to other things they are learning - math, critical thinking, problem solving, etc. Having a basic understanding of software will give them leverage in working with engineers. If they want to be members of Congress, knowing how the software that they will use and oversee works will only make them more successful. Help them learn to fail and persevere. Among the girls who told us that they didn't like the activity, most were simply frustrated. After doing a bit of research, I found that lack of confidence and fear of failure are two leading reasons that girls don't consider STEM. If we can help them face and overcome difficulty, we can help build their confidence in their ability to learn. This helps them far beyond coding or any academic setting. Whether they hate science or dream of circuits, building strong, resilient, hard-working girls will help them succeed in anything that comes their way. How we're following up I plan to spend a bit of time in the next few weeks scouring the internet and any academic papers I can get my hands on for the most effective strategies. Some questions that I would like to answer: Do girls need to see women in STEM as role models consistently to have an impact? Or does it just take a couple of interactions to make STEM a possibility? Are there strategies that we're missing? Should we do something other than work on small problems? Would it be more effective to work on longer projects? Would an on-site at Gusto add to their excitement? My intuition is that prolonged presence of role models makes a bigger difference, so I'm reaching out to Dora Moore's tech teacher to learn more about where we can be involved there. Emily and Chris ready to take questions. Why I'll continue I can't imagine many places where an engineer would be encouraged to use their expensive time to plan events that use even more engineering time to teach girls to code. Not by accident, I have found exactly that at Gusto. One day when I was starting to lose steam on follow-up, a teammate reminded me that I cared about the mission of these workshops and that I had the support of my team to accomplish it. With his reminder, I turned back to my keyboard with renewed intention to research, plan events, and, hopefully, inspire others to do likewise. For me this chance to nudge my industry toward greater equality is about doing what's right , more than what's right for the bottom line, though it accomplishes that too . To tell the truth, my initial thought process was solely focused on giving other girls the confidence to do what they want , as my parents and community gave me at their age. The fact that my community still values the same things helps me push harder. If you're interested in not only doing great engineering work, but also contributing to the community, that's what we do here at Gusto. We're hiring for engineers in both Denver and San Francisco! Check out our job openings . Resources Articles Study: Firms with More Women in the C-Suite Are More Profitable How To Encourage Girls To Get Interested In STEM Tools Blockly - learning platform with easy to use games TechGirlz - lesson plans and platforms, help with publicity and overall structure Slide deck", "date": "2018-06-19"},
{"website": "Gusto", "title": "How my role as CTO has changed as we've grown to 100 engineers", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-my-role-as-cto-has-changed-as-weve-grown-to-100-engineers/", "abstract": "My two co-founders and I started Gusto with a mission to make payroll, benefits, and HR easier for small businesses. Tomer, Josh, and I founded the company from the master bedroom of a house in Palo Alto with nothing more than a vision for the future and a commitment to do whatever it takes to turn it into a reality. Six years later, we’re serving more than 60,000 small businesses (over 1% of all employers in the United States), employing more than 600 people (including around 100 engineers), and together, we’ve achieved a valuation of more than one billion dollars. I regularly hold office hours at Gusto where any engineer can stop by and ask me whatever  they want. One question I sometimes get is: As the CTO and co-founder of the company, how has your role changed? 1 Engineer: Nerd in a garage, er, closet. When we first got started, I was living in San Francisco and my two co-founders were living together in a house in Palo Alto. I thought it was important for us to live together while we built our first prototype (we were all single at the time). Unfortunately, there were no more bedrooms that I could rent, but there was a fairly large walk-in closet in the master bedroom. After convincing all the tenants in the house to let me rent the closet for $300/month, I borrowed a friend’s air mattress, packed my bags, and moved in. I was the CTO, but at that stage of a company, it felt silly to say that because there isn’t really anything to be the “chief” of. Software Engineer better described my role. I was learning as much as I could about the convoluted payroll system and how the ACH system works while simultaneously coding 12-14 hours a day with headphones on — and it was awesome. The only meetings on my calendar were breaks for lunch, dinner, and going to the gym to exercise. Our goal during this period? To build a payroll backend system so that we could start paying ourselves with it. 2-10 Engineers: One team, one dream. After getting a basic payroll system (California-only) running and closing a seed funding round, we moved to a loft apartment in San Francisco and I shifted from coding 90% of the time to coding 60% of the time. The remainder of my time was spent sourcing, interviewing, and closing engineers. Many things have changed in my role as a CTO, but the time spent recruiting has always been consistent (and probably always will be). From this point on, I’ve always spent about 30-40% of my time recruiting. Being a player-coach Once we hired a few engineers, I went from being the only full-time developer to working in a small team of developers. I still spent a significant amount of time coding, code reviewing, and fixing bugs, but I would occasionally take my headphones off to explain to the team how ACH works or to make a call on if we should start the practice of code reviewing. If there was anything I could do to preserve maker time for the other engineers — like buy and configure a computer to run our CI pipelines — I would do it. Things like 1-on-1s, sprint planning, and daily standups started creeping onto my calendar. I had engineers reporting to me, but for the most part, there wasn’t a hierarchy and it still felt like a group of peers coding together in a room. During this period, the media embargo launching Gusto (then ZenPayroll) was lifted and after pulling an all-nighter, the engineering team nervously monitored server logs and refreshed the page to make sure our Linode servers would survive being “Techcrunch’d.” 11-50 Engineers: Clinging to coding. On the heels of our company acquiring customers rapidly and raising a Series A, I found myself with a team of more than 10 engineers. Coordinating work across all these engineers became more difficult. In addition to size, the tenure of some of the early engineers required me to make our engineering organization more structured. For example, some of our engineers who had been working at Gusto for nearly two years were starting to ask about a compensation framework and what career progression within Gusto would look like. To be honest, I hadn’t given these topics the amount of thought they deserved. At the same time, despite the larger team, there was still a never-ending list of features to ship and bugs to fix. I probably still had the most intimate knowledge of our codebase, so when a problem arose or a feature needed to be shipped, it was incredibly tempting for me to just jump in the code and fix it. More often than not, that’s exactly what I did. A trip to New York In 2015, I knew that I needed to step away from coding and dedicate more time to being a good people manger. To accomplish this, I decided to fly to New York to hole up in a hotel room for three days to read a few highly-recommended management books. On my flight there, I thought there wouldn’t be any harm in coding a new payroll feature — I’d have three full days once I landed to focus on my reading, after all. Boy, was I wrong. When I landed, I felt the implacable urge to finish the feature in its entirety. By the end of the three days, I had only read a very small fraction of the books and I failed miserably at my goal to dedicate more time to people management. There were only so many hours in the day and juggling both people management (at Gusto, we call it “People Empowerment”) and individual contributor responsibilities was incredibly hard. As a lover of coding, I naturally tended to finish that last feature and punt on the people management topics until a little later. To me, the code issues always felt more urgent. On top of that, I struggled with a deep sense of guilt about not doing my part. When I did get around to the people management topics, I didn’t spend as much time on it to deliver the same quality as I did with my code. My team wobbled a little bit as a result and I likely did more harm than good by trying to not give up coding. The binary choice you must make At this point, I believe technical co-founders have a binary choice: Stay on the technical track and hire a professional manager (usually given the VP of Engineering title), or give up coding and focus on the management aspects yourself. It really isn’t possible to do both. I decided to focus on growing Gusto’s engineering team, and not our code. The technical books on my desk starting getting replaced with books like Mindset , High Output Management , and The Score Takes Care of Itself — still three of my favorites today. Mindset was critical to helping me prepare myself mentally for the personal journey ahead. High Output Management helped me to learn many of the processes behind managing teams. The Score Takes Care of Itself taught me how to go beyond management and also be an inspirational leader. Learning and applying those lessons while resisting the temptation to code was an incredibly hard transition for me, but one that was necessary to scale the engineering team in a healthy way. 51-100 Engineers: Embracing a new role. By the time we reached around 50 engineers, I was spending 60% of my time being the best manager I possibly could to my direct reports, and training other engineering managers in the team to do the same. The remaining 40% of my time was spent recruiting. The better I got at people management responsibilities, the more I enjoyed it. I would spend time on things like improving engineering onboarding, managing performance, diversity and inclusion , culture , change management, and putting processes in place for big projects like balancing technical debt. The only time that I would code would be during our semi-annual hackathons . Loving my new role Not everything I do now is enjoyable at the moment in time that I’m doing it. Being in meetings all day still sucks, and having difficult conversations with individuals is never fun. Nevertheless, I’ve really come to embrace my new role in the company. For example, I spent several weeks negotiating a plan for us move more code ownership of our payroll domain to Denver. Some of those meetings were pretty long and tough, but as a result of that work, we now have a rapidly-growing Denver engineering team that I’m extremely proud of. My enjoyment now comes less from the things I do myself and more from the impact that I helped others to have. Sometimes you only see this after some time has passed, so you derive satisfaction by learning to look at things on a much broader time horizon. 100-250 Engineers What happens next? The next chapter will require scaling our engineering team even more while maintaining the same level of quality as we’ve had in the past. This means hiring, growing, and retaining talent will remain the most important part of my role (p.s. We’re hiring! ). For me, I’m committed to doing this in a way that we’re proud of by building a diverse and vibrant environment where we can do our best work. If there’s one thing that I’ve learned from a fast-growing startup, it’s that change is the one constant — and I’m excited to see what the future holds. Comments on Hacker News Thanks to Kirill Klimuk and Vaibhav Mallya for nudging me to write this", "date": "2018-06-27"},
{"website": "Gusto", "title": "Growing Payroll in Denver", "author": ["Jeremy Thomas"], "link": "https://engineering.gusto.com/growing-payroll-in-denver/", "abstract": "We joined Y Combinator in 2012 with the idea that we could change the way people think about payroll . The name of our company back then, “ZenPayroll,” emphasized our focus on the payroll domain (although we already had aspirations to go beyond payroll, as contemplated in our YC pitch deck). We rolled out to fellow Y Combinator companies first. With a few successful cycles under our belt, we opened up to “friends of friends” with the stipulation that all companies be based in California. You see, each state has its own rules around how tax data should be reported, and we wanted to be good at California before venturing across the border. Obsessing about making it easy for small- and medium-sized businesses to manage payroll was (and continues to be) paramount to our mission. We believe we can help business owners find the freedom to focus on the non-administrative aspects of their businesses. And with this mission in hand, we continued to grow. In 2013, we processed about $100 million in payroll. By April 2015, we reached a point where we were processing billions of dollars in payroll annually for 10,000-ish customers in multiple states. Growth was good, and we needed to expand our team. In July 2015, we announced the opening of our Denver office with a plan to seed that location with our customer experience team. Other teams would follow over the next few years. And with our new Denver location we were ready to announce big changes around expanding into new market segments. Our mission is to create a world where work empowers a better life. There are several steps we need to take to get there. But the first involves making running payroll, benefits, and HR as easy as running water. In September 2015, we changed our name to “Gusto” and announced we were going into the benefits business . A year later, we announced we were getting into the HR business . A significant portion of our customers expressed interest in an all-in-one people solution (payroll, benefits, HR) in order to further simplify their business operations. And in order to get into the benefits and HR markets, we needed to reorganize our Engineering, Product, and Design (EPD) team. In the ZenPayroll days, all of EPD focused on the payroll domain. But in order to become an all-in-one people solution we needed to create two new product teams to support our go-to-market strategy: HR and Benefits. Today we refer to our product teams as “missions” here at Gusto. Three of them are go-to market products we offer our customers (HR, Payroll, and Benefits). The two others are vital cross-product focus areas. Growth, for example, focuses on product-based experiments that help us do a better job reaching new customers. That mission also has a concentration area around master data management to help our business and product teams derive insight from the millions of data artifacts our system generates every day. FinTech focuses on moving money between bank accounts so employees get paid on time. They are masters at understanding the nuanced nature of ACH and other money transfer methods. Further, our Developer Experience (DEX) team loosely rolls into FinTech (via Infrastructure), and is the reason why we can do things like ship code to Production 5 to 10 times per day. Each mission is led by an engineer, product manager, and sometimes a designer. Most of our missions are divided into pods, with each pod comprising a handful of engineers and a lead. But some of our missions are pod-less, opting to dynamically assemble teams based on project needs. We feel that experimentation around team structure is super important. Two of our missions, Growth and Payroll, have pods in Denver. Most of Growth’s data team is there. The Denver payroll pod focuses on tax payments and filings. As headcount in Denver continues to increase (we have about the same number of people in both offices), we asked ourselves if it made sense to bolster our EPD presence there. Many of our operational and engagement teams had re-centered in Denver. Was there a chance to optimize for geography? We looked at how other companies went about opening their second engineering locations, and were inspired by how Instagram opened its NYC engineering office . We liked how they placed “big bets” on their new location, moving strategically important initiatives there. And we realized that payroll, being core to day-to-day work for most of our Denver-based business teams, spiked high on geographic optimization. Our Customer Care teams do a lot of payroll support, and being able to walk over and talk through issues with engineers in person would add a ton of value. Our Tax Operations and Resolution teams file for our customers on a quarterly (and sometimes monthly) basis. They’d benefit for the same reasons. Gusto would be a better business over the long term if we invested invested growing the size of our Payroll mission there. So that’s what we’re doing. The Denver Post announced last month that we’re hiring a lot of engineers (and PMs and Designers) in Denver in 2018 . Much of that is tied to this decision. Gusto EPD will always be headquartered in San Francisco. And the Payroll mission will always have teams in both cities. It’ll take time to do this properly. Payroll is core to our business and is esoteric, to say the least. We’ve planned for a long transition period with key milestones throughout. And when we get through the transition, it’s going to do great things for our business. If you’re interested in joining, check out our job postings for Denver and San Francisco . Or, send me an email just to say “hi,” jeremy.thomas [at] gusto.com.", "date": "2018-01-22"},
{"website": "Gusto", "title": "Zero Downtime Table Migrations using a Double Write Methodology", "author": ["Alex Evanczuk", "Stephanie Tsai"], "link": "https://engineering.gusto.com/old-write/", "abstract": "A database table’s lifecycle is constantly becoming more and more complex.  It grows beyond its original territory, taking on new responsibilities and interacting with other parts of the code base in unpredictable ways. In order to keep the code ecosystem flourishing, it’s important to periodically refactor tables with too many responsibilities to make it easier to develop new features and fix bugs. In the health benefits world at Gusto, we found ourselves facing a giant table that we needed to break up in order to complete product features. In this post, we detail the steps we took to make this a zero downtime migration. Background Our initial health benefits system had two tables to store employee benefits data: subscriptions and waivers. The subscriptions table began to outgrow its initial purpose, and it also shared functionality with the waivers table. Other details about these tables are not important to know for the purposes of this post; our focus will be on the data migration to our new table - selections. The new selections table is a combination of a part of the subscriptions table (unprocessed subscriptions) and waivers. Adding the selections table allowed us to remove the waivers table and delete unprocessed subscriptions from the subscriptions table. We’ll go into more detail about the table mappings later. In order to do the migration to the selections table with zero downtime, we followed a standard procedure that allowed us to confidently migrate the data and start using selections in place of unprocessed subscriptions and waivers. Examples of this procedure can be found in this Stripe blog post or this post from a Paypal data engineer . We’ll discuss how we followed this approach and what we learned throughout the process. There are three major steps: The double-write : Write data to both the old and new tables The single-read : Instead of reading the (identical) data from the old table, read it from the new table. The single-write : Once we are exclusively reading from the new table, we can stop writing to the old table and delete the old, no longer read data. The Double Write The first phase of our project duplicates the data into our new selections table and ensures that the selections table and the unprocessed subscriptions and waivers tables stay in sync. This allows us to confidently use selections in place of unprocessed subscriptions and waivers. Double writing transforms one set of data to a new set of data using a mapping function. The simplest mapping function, in which one object in the source table corresponds to one object in the target table, is shown below: In our case, the initial objects are unprocessed subscriptions and waivers and the new objects are selections. Our mapping function looks like this: Our goal is to reach and maintain this mapping. For each old object, we must ensure that there is exactly one new object with corresponding attributes. For example, an unprocessed subscription must have exactly one corresponding selection, and attributes on both objects, such as employee ID, benefit type, and policy ID, must match. The double write process consists of two steps to maintain the integrity of the mapping: Step 1) Each time we perform a CRUD operation on our old table, we must do the same write to the new table. In the above example, we add the code in green to create the new data - for each unprocessed subscription, there must be a selection. We wrap all of the double write code in a transaction that ensures atomicity of the writes and rolls back if one write fails. Step 2) Existing unprocessed subscriptions and waivers also need matching selection objects. We run a migration that creates selections for each of the already existing unprocessed subscriptions and waivers. After these two steps, we expect our mapping function to return a selection for any unprocessed subscription or waiver. To have an accurate double write, we need to catch all the places where we write unprocessed subscriptions or waivers. We create a Sidekiq job that runs a “differ” between the objects in our new selections table and all unprocessed subscriptions and waivers. The differ checks that each unprocessed subscription and waiver in the old tables has a matching selection. We report differences to a Slack channel in a readable format, allowing us to track and fix the errors easily. Above is an example of the output of our differ. Here we see that the subscription has attribute policy_id set to 21, while the matching selection has policy_id set to 22. Our job is to investigate where we are updating the subscription’s policy_id but not the selection’s! We run our differ until we are confident that we are successfully double writing selections from unprocessed subscriptions and waivers.  The differ was set to run asynchronously for recent records every hour and all records every day, but it also could have been run in realtime on every object save. We had many learnings about the double-write part of this project. Our differ only caught errors generated by code paths that were actually run. This is a problem when some code paths are not frequently hit. If we were to do this project again, we would first move all CRUD operations against the old tables into a service class. Then our old tables could warn if they weren’t being written to through this service class (perhaps by setting an instance variable on the object within the processor). This would allow us to verify the double-writes within live code, and would give us a pinpointed location where a double-write is not happening correctly. The Single Read After we implement double writing to the selections table, we can read from either the old tables or the new table. The single read phase ensures that we are no longer reading from the old tables. We implement single reads by making two types of changes: Deriving primitive values (i.e. boolean, string, integer, etc) using the selections table instead of the subscriptions or waivers table. Here is an example on one of our core models, Enrollment: class Enrollment\n  def unprocessed?\n    subscriptions.unprocessed.any?\n  end\nend We are using unprocessed subscriptions to derive a primitive boolean value. Replacing this to read from the selections table is straightforward: class Enrollment\n  def unprocessed?\n    selections.unprocessed.any?\n  end\nend Now we are single reading from our new selections table! Modifying places where a class expects an interface of the old model. Now that the data is the same on both the old and new tables, we can pull the data off of the new table. For example, we pass in selections instead of subscriptions to an existing class that previously expected subscriptions. There is no rock-solid guarantee that we have completely implemented single reads. However, we can override attributes on our subscriptions and waivers tables to catch places where we are still using these old tables. We do this using metaprogramming to override methods and Bugsnag to report the errors, but it can also be done by explicitly defining override methods and using a logging or reporting service that is supported by your infrastructure.  Metaprogramming allows us to write our code in a more DRY way, but at the cost of some clarity. class Subscription < ApplicationRecord\n\n  COLUMN_NAMES = [\n    { name: 'id' },\n    { name: 'policy_id' },\n    { name: 'processed' },\n  ].freeze\n\n  COLUMN_NAMES.each do |column|\n    column_name = column[:name]\n    define_method(column_name) do |*_args|\n      unless read_attribute(:processed)\n        NotifySelectionsSingleReadError.verify(caller) \n      end\n      read_attribute(column_name)\n    end\n  end\n\n  class NotifySelectionsSingleReadError\n    def self.verify(backtrace)\n  if !whitelisted(backtrace)\n    raise “Should not be using the old tables!” if Rails.env.test?\n    Bugsnag.notify(“Should not be using the old tables!”) \n    \tend\n    end\n  end\nend The NotifySelectionsSingleReadError class includes a whitelist for places where we should still be using our old model, such as in our double writes.  Raising only in test ensures developers do not write new code that uses the old tables. With this verification check in place, only three (largely automatic) steps remain to fully implement single reads: Step 1: We proceed to fix our test suite, which now fails for a non-whitelisted read of our old table. Step 2: In our local development environments, we modify the verification to fail if reading the wrong model and run through several key scenarios in our application until we can complete each scenario with no errors.  For example, we go through an employee enrollment flow, process a subscription, and run our maintenance sidekiq jobs, checking that we never read from our old tables. Step 3: Over the course of several days, we keep an eye on our bug reporting service (Bugsnag) for incorrect single reads in production and fix them as they come in. Much like the double-write part of the project, we had several learnings about this phase.  Firstly, we could have similarly created a special service class that was responsible for reading subscriptions and selections. This would have allowed us to first focus on refactoring reads into this class, and then having the class determine which object type should be read. This approach, however, would still not have helped if the new table had a different interface.  Therefore, an intermediate change that would have taken longer, but would have made the single read step very easy, would have been to refactor our reads to use value objects , so later reading from selections would be as simple as creating the same value object differently in a single place. There were many parts of our code base that took a subscription and converted it to a value object. This made the single read pretty straightforward, because we just needed to construct the same value object with selections instead. Another thing we learned when single reading is to think beyond the ORM (in this case,  ActiveRecord). Database-level reads, such as foreign-key constraints, need to be removed or changed. For microservices or cross-platform architectures, you’ll have to think about other objects that have foreign keys that point to your old tables. We ran into several issues where users had bookmarked CRUD-like URLs that referenced unprocessed subscriptions, which no longer existed after implementing single reads! The Single Write Once single reads are in place, we no longer need our original tables. In fact, we could fill our old tables with junk data or remove the tables entirely and nothing should break. We still want to audit our double write phase, however, to ensure that none of the writes to our new table are dependent on the old tables. For example, we find that we sometimes write to one of our old tables and then do a 1:1 copy to our new table, rather than use application logic to do the same write twice. After a manual audit to ensure complete independence between writes to the old tables and the new table, we take the following steps: We stop creating new rows in the old tables by removing any create calls for unprocessed subscriptions or waivers. We destroy all waivers and unprocessed subscriptions in the old tables after we stop creating new rows. We start removing remaining CRUD operations (which by now are dead code) against the old tables. Because of our work during previous steps, removing the CRUD operations can be done rather indiscriminately, by simply removing any code that writes to the old tables. Here is an example of a CRUD operation where we destroy a subscription along with a selection: class SelectionsController\n  def destroy\n    ActiveRecord::Base.transaction do\n selection = employee.selections.find(params[:id])\n selection.destroy!\n subscription.destroy!\n    end\n  end\nend After steps 1 and 2, we no longer need to destroy subscriptions, and the code in red can be removed! Lastly, to verify that all steps are complete, we remove the whitelist generated in the single read phase and apply the strategies from the single read phase (i.e. fixing the test suite, monitoring bugsnag, etc.).  We also add a regular audit that verifies that our waivers table has 0 rows and our subscriptions table only contains processed subscriptions! Learnings from this phase are similar to those in the double-write phase.  With CRUD service classes, single-writing would be much simpler; we would remove the subscription version of the change in the service class and then rename the service class if we wanted to. We’ve finished the three phases of our zero downtime migration and ended up with a cleaner, leaner model that has a clearly defined role. The new model is more easily understood, simple to work with, and allows us to write new features. Refactors like this demonstrate the importance of giving your initial data design a lot of thought. Codebases are constantly evolving, however, and refactors are sometimes unavoidable. Having a solid execution plan is important for when it comes time to refactor. Thanks for reading! Stephanie Tsai and Alex Evanczuk Stephanie Tsai has been a benefits engineer at Gusto for almost 2 years. She graduated from Stanford, plays cello, badminton, and board games, and drinks a potentially unhealthy amount of boba. Alex Evanczuk has been on the benefits team for over a year and a half. He graduated from University of Pennsylvania and has no other hobbies.", "date": "2018-08-14"},
{"website": "Gusto", "title": "A framework to help product managers achieve their potential", "author": ["Eric Schuchman"], "link": "https://engineering.gusto.com/a-framework-to-help-product-managers-achieve-their-potential-2/", "abstract": "When I first started managing product managers six years ago, I found it hard to provide my team with specific feedback to help them get to the next level in their careers. There’s no shortage of articles and blog posts about the different stages of a product management career, but they’re all based on titles (PM, Senior PM, Director of Product, etc.) and where people with these titles are located within an organization (Senior PMs report to Product Leads, and Product Leads report to Directors of Product, etc.). These articles almost never answer the key question of what, exactly, makes somebody ready to be a Director of Product vs. a Product Lead. What are the tangible skills they need to have in order to be successful at that next level? It’s even harder to go from giving advice on how to improve to being able to say how close or far one might be from a promotion. At Gusto, we addressed this by breaking down product management into four key attributes. Each attribute has specific expectations for every stage of a product manager’s career, providing a map to managers and individual contributors alike. The 4 Product Management Attributes PMs at Gusto are evaluated on four attributes, each reflecting an area of potential impact. Contribution to Product Milestones This covers most of what one might consider the basics of a product manager role: managing a roadmap, writing specs, designing delightful user experiences, and prioritizing projects. Earlier in a product management career, this means delivering on well-scoped pieces of projects/features. Later on, it means managing multiple product lines with a proven track record of business results. Contribution to Company Strategy Product managers are responsible for driving insights across the business as a whole. Earlier in one’s career, a PM should understand and execute a strategy set by leadership while also owning small research projects. Later on, a PM should be delivering strategic insights that influence the entire company. Contribution to Product Team Success PMs should invest in the evolution of the team as a whole via mentorship, hiring, and the creation of  new frameworks or processes. We expect this not only of people managers but also of individual contributors. One does not need to be a manager to help with recruiting, mentorship, or in general to be a leader. Contribution to Other Teams’ Success Product managers need to lead cross-functional efforts, effectively communicate product priorities with others, and take feedback. They should also be able to help with mentorship, recruiting, and processes outside of the core product management team, and be a leader in the company as a whole, helping other teams (such as engineering and design) be more successful. Gusto’s PM Career Framework At Gusto, we don’t have titles (you can learn more about that here ). Instead, we map out careers by “levels” - each product manager has a level based on the scope of their role. Promotions occur when employees move from one level to the next. We don’t make the levels of specific employees public knowledge, but they are free to share if they so choose. Each product management attribute is broken out by level, creating a rubric that can be used to evaluate how a product manager is performing. The rubric focuses on results, making it easy for both the PM and their manager to measure the PM’s contribution. How to Use this Framework Where and when to use it Managers of PMs use the career framework at every step of a PM’s journey at Gusto. Evaluating candidates : When hiring, we gear our interview questions and the evaluation of those questions towards the expectations set in the career framework. For example, at Level 3, we expect PMs to be able to deliver meaningful strategic insights, and therefore we ask them more strategy-based questions in the interviews. Onboarding new hires : When new hires onboard to Gusto, we use the career framework to help them understand what we expect of them. Not only does this provide transparency, but it also helps the new hire identify his or her own strengths and weaknesses, giving them the opportunity to seek coaching from their manager in these specific areas. Performance reviews : Our biannual performance review process is centered around this framework. PMs evaluate themselves, managers evaluate their teams, and peers provide feedback to each other based on the attributes for each level. For example, I may ask the peers of a Level 3 PM to provide specific feedback with examples about how the PM was able to or not able to “Effectively prioritize requests from other teams into their team’s roadmap”. Career development plans are then created to help PMs improve in weaker areas while also leaning into and taking advantage of their strengths. Promotions : Promotions occur when a PM has been consistently performing at the attributes set for the next level up. As PMs grow in seniority, longer time periods are required to show consistency. For example, Level 5 PMs are expected to deliver business outcomes for Gusto for entire product lines, which takes many years. How to use it The career framework isn’t meant to be used by itself. People managers spend time with new hires to ensure they understand the it and then use it as the basis for career discussions throughout the year. Additionally, we have internal documents that break down these attributes in even greater detail. For example, when we state that a Level 5 PM should “Mentor PM team members across the company”, we elaborate that this person should be able to hire, mentor, and retain PMs from Level 1 up to Level 5. We also provide concrete examples of what other PMs have done to illustrate the attributes at each level. What if a PM is in between levels? By design, most PMs will be in between the levels of our career framework. They will typically be meeting expectations for their current level and, over time, will show their ability to contribute at the next level. At the point where they are clearly and consistently able to perform at the next level across all PM attributes, they are promoted. We bring new hires onto the team at the highest level at which they will perform effectively across all of the attributes. This way they are able to meet or beat our expectations, which avoids the situation in which somebody who is talented ends up disappointing a manager or peers who were expecting more (i.e. a PM who would be a superlative Level 2 is brought in at a Level 3 but not able to meet all of the Level 3 attributes). Should I create a career framework for my team? This is the easiest question of them all. The answer is Yes. Building and maintaining a career framework is a lot of work, but here’s why it’s worth it: Transparency : Since the framework is shared for all to see, people can self-rate themselves and identify growth areas. It’s clear when people are ready for a promotion or when they have a bit more to go without their manager needing to tell them. Objectivity : Having clear criteria for promotions removes subjectivity from the promotion process and helps reduce unconscious bias, which levels the playing field for all people from all backgrounds. Consistency : All teams and managers use the same framework, which ensures that all employees are held to the same standards regardless of whom they report to. Time : As a manager, I’m constantly having career conversations with my team. Before we built our career framework, I found myself having the same conversations over and over, often with the same people. In the long run, creating the framework has saved me a huge amount of time. Even if your team is small, having a clear career framework is worthwhile. On my team specifically, I have direct reports who aren’t PMs and are actually the only people in the company who perform their specific job function. I’ve created career frameworks specific to them, and while it was quite a bit of work, it has always paid off. Build, Test, Iterate We’ve iterated on our career framework over the course of the past four years, but there’s still a long way to go. What’s top of mind for us? Individual Contributor vs. Manager career tracks : Our career framework is intended to apply to both individual contributors and managers. However, we’ve gotten feedback that at senior levels, it would be helpful to split them up (often referred to as dual career tracks) to clarify expectations. More concrete examples : Something we’re experimenting with is an even longer-form version of our career framework that provides specific examples of contributions that meet each attribute. This makes the criteria less abstract, but also results in a document that is over a dozen pages long. Want to Continue the Conversation? I know that I’m not the only one who is passionate about product manager career development. If you’d like to develop your own career with Gusto, you can check out our open roles here . I’d also love to hear from you - How do you develop the career of product managers at your company? How do you product manage your own career? Leave a message here or find me on LinkedIn . Bio Eric has been empowering product management teams at Gusto for the past four years. He loves building products that customers love, helping product managers grow, and making charcuterie.", "date": "2018-10-16"},
{"website": "Gusto", "title": "Taking off with Apollo", "author": ["Kurt Kotzur"], "link": "https://engineering.gusto.com/taking-off-with-apollo/", "abstract": "What building an internal application revealed about Apollo For almost a year, the Gusto Development Experience (or DEx) team has been working on an internal application called Flightdeck. Engineers visit Flightdeck for regular engineering tasks, like running database migrations. Our long-term goal is for Flightdeck to be a tool with all phases of Gusto software development and maintenance. Flightdeck: like this, but for your web applications. Learnings and warnings Most of the engineers on our DEx team have product engineering experience. We have worked on our customer-facing app as well as applications used by internal operations teams. These applications are mostly React.js built atop RESTful APIs. When starting development on Flightdeck, we knew that we loved React’s declarative and productive paradigm. However, we also were jaded from the front-end and back-end boilerplate needed to integrate it with a RESTful API. Having heard about GraphQL from React Conf, GitHub's API, and elsewhere, we decided to try it. We chose Apollo because of its good documentation and community involvement. We’ve passed the first months with Apollo on our young application. Here are the biggest things we learned about the framework. Container components are hard to unit test For the uninitiated, “container” components are the components returned by the graphql higher-order component (HOC). In order to test them, you must mock the GraphQL layer; this involves recreating the type definitions in your GraphQL backend, in addition to any mutation signatures. For a young application in which mutations are likely used in only one place, recreating their signatures feels unpleasantly tautological. To make matters worse, the test utils' documentation isn't great. In Flightdeck, container components are only used in one place each. Given that, and that we're encouraging Capybara integration tests for discrete new features, we've skipped unit testing the container components for now. (We’re still unit testing all of our other components.) Because Gusto engineers have strong reflexes to maximize test coverage, this has resulted in integration tests for all of our recent features. We recognize that integration tests may become more difficult to write if our app grows much larger, and we’re hoping for improvements in apollo-test-utils ’s documentation. Colocating components and their data requires some boilerplate, but it’s worth it We’ve arrived at a simple pattern for components at different levels of nesting to define their data requirements. This is inspired by Relay's \"fragment containers\". Each “view” (non-container) component defines a static fragments attribute. This attribute is an object whose keys are GraphQL type names, and whose values are GraphQL fragments. This attribute has an entry for any type whose data is required by the component or one of its children. If a child depends on a type's data as well, the child fragment is included in the parent’s corresponding fragment. The name of each fragment must be unique across the set of all fragments used in any given GraphQL document. We’ve found the naming convention ${ComponentName}_${TypeName} as in TodoItemList_Todo , to be the best combination of readability and uniqueness guarantees. class TeamShow extends Component {\n  static fragments = {\n    team: gql`\n      fragment TeamShow_Team on Team {\n        name\n        mission {\n          name\n        }\n        failedBackgroundJobs {\n          ...JobTabs_FailedBackgroundJob\n          application {\n            name\n          }\n          environment {\n            id\n          }\n        }\n        runtimeExceptions {\n          ...ExceptionTable_RuntimeException\n          project {\n            id\n          }\n        }\n      }\n      ${JobTabs.fragments.failedBackgroundJob}\n      ${ExceptionTable.fragments.runtimeException}\n    `\n  }\n\n  render = () => (\n    <div>\n      {\n        this.props.failedBackgroundJobs.map(job => (\n          <JobTabs job={job} key={job.id} />\n        ))\n      }\n      {\n        this.props.runtimeExceptions.map(exception => (\n          <ExceptionTable exception={exception} key={exception.id} />\n        ))\n      }\n    </div>\n  )\n} Polling is a nice option Apollo polls your backend with the same query if the pollInterval prop (Apollo 2+) or option (Apollo 1) is supplied. For pages of your application that have well-isolated data requirements, it’s a much simpler option than writing mutation cache updates. However, beware of errors: if you hit any, polling will log tens of errors to your browser devtools. Polling pains Be careful with required fields Think very carefully about what fields are required in your GraphQL type definitions, i.e. those that use the ! operator.  We've already run into multiple Apollo-layer exceptions from overzealous usage of this operator. If you’re certain that the field will always be present, as in field that directly maps to a NOT NULL database column populated by your backend, then it’s the right choice. For fields that have presence validations at the backend validator/model layer, you may want to make them nullable in the GraphQL mutation signature. If a null value is passed to a mutation whose definition includes a not-null argument, the mismatch will be surfaced as the unergonomic error “cannot return null for nullable field…”. If your backend model layer is the highest layer at which you validate presence, you will be able to surface more semantically meaningful error messages. Performing this validation at the view layer (before the data reaches GraphQL) could also help. Mutation resolvers that return static values are okay Most GraphQL examples illustrate querying for fields on the return value of a mutation. We’ve found that it often makes sense to call a mutation without any such querying. For example, we have one mutation resolver that simply enqueues a background job, and always returns a JSON response { “success”: true } . We wrote a simple “Status” type, with a single required boolean success field, to make this easier. Expose datetime fields as strings We chose to expose datetime fields as strings rather than integers (which would represent seconds since the Epoch). This lets us use a single backend service to handle date formatting, and lets us skip a front end dependency to handle time zones. Beware __typename__ The __typename__ field is present by default in GraphQL query results. This could cause problems as an extra field when feeding a query result into a mutation. To make this more seamless, the addTypename configuration field for ApolloClient can be leveraged: new ApolloClient({\n    link: new HttpLink({ uri: 'http://localhost:4000' }),\n    cache: new InMemoryCache({\n        addTypename: false\n    }),\n}); This is a little different for react-apollo 1.x: new ApolloClient({\n  addTypename: false,\n  networkInterface: createNetworkInterface({ uri: '/graphql' })\n}); Future Improvements to our Flightplan We’ve made a lot of improvements by reflecting on these points, but we’re not done yet. Here are some of our ideas for future improvements to our Apollo stack: Be more consistent when handling error fields Most of Flightdeck’s interactions with its backend consist of CRUD actions with corresponding validations. For this reason, it’s common for our GraphQL type definitions to have an errors field that contains the details of validation failures for an action, if any. These fields are currently defined on an ad-hoc basis for each type. We’d like to try leveraging interface types so that we can define this field only once. More consistent handling of dates I mentioned earlier that we’re defining date fields on GraphQL types as strings, to prevent having to handle formatting and time zones on both the front and back end of Flightdeck. However, we’re still invoking a date formatting class for each of our many date field definitions. A cleaner approach may be to define a custom scalar type that knows how to coerce inputs and results. Here’s what that might look like in graphql-ruby : Types::Scalar::DateTimeType = GraphQL::ScalarType.define do\n  name 'DateTime'\n  description 'A date and time in ISO 8601 format: \"2018-01-01T23:59:59.123Z\"'\n  coerce_input(\n    -> (value, _ctx) do\n      begin\n        DateTime.parse(value)\n      rescue ArgumentError\n        raise GraphQL::CoercionError, \"cannot coerce `#{value.inspect}` to DateTime\"\n      end\n    end,\n  )\n  coerce_result -> (value, _ctx) { value.nil? ? nil : value.to_time.utc.iso8601(3) }\nend Installing eslint-plugin-graphql This eslint plugin enables checking your GraphQL documents against your GraphQL schema for a host of problems, including missing operation names and type name formatting. Installing the plugin is simple, but generating a schema for it to consume requires a bit more thought. Conclusion We’ve had a good time with Apollo so far. It’s freed us of a lot of boilerplate, and allowed us to focus more time on thinking about and writing features. We’d definitely recommend you give it a spin, but just remember our suggestions above when things get bumpy. Bio Kurt has worked as a software engineer at Gusto for 3 years. He's passionate about developer tooling and front-end technologies. He’s a telenovela connoisseur.", "date": "2018-09-12"},
{"website": "Gusto", "title": "Staying Ambitious on Diversity: New Goals for Gusto Engineering", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/staying-ambitious-on-diversity-new-goals-for-gusto-engineering/", "abstract": "Note: At Gusto, our diversity and belonging efforts focus on composition (i.e. ratios) and belonging (i.e. creating a culture of inclusivity for everyone). We believe both are important. This particular post focuses on providing an update on our composition numbers. In September 2015, we committed ourselves to creating a more diverse engineering culture at Gusto. Our goal was to increase the proportion of female engineers on our team from 11% to 18% in three months. We successfully achieved that goal, and today, women make up 26% of our engineering team. Our broader tech team, made up of the engineering, product management, and product design teams, is also 26% women. Numbers rarely tell the full story, but here are a few other stats that make me proud of where we are today: 30% of the engineers who joined us in 2017 and 2018 are women. 0 female engineers have left Gusto since 2016. 27% of engineers recently promoted are women (proportional to our gender composition today). Along the way, we’ve had a lot of learnings about how to make diversity initiatives successful. I’m sharing them here in the hope that other teams can learn from us and create their own diversity engines. 1. With enough focus and a bias towards action, you can change your ratio over time I’ve heard too many engineering managers blame their team’s lack of diversity on a poor pipeline of female candidates. We’ve found that with a bias towards action over a long enough period of time (years, not months), creating a more diverse team is entirely within our control. Focusing on actionable activities like outbound sourcing exclusively for female candidates and sponsoring the Grace Hopper Celebration was instrumental in helping us achieve our goals. 2. Diversity begets diversity Much has been written about diversity debt . We’ve found the corollary to be true as well: A diverse team tends to make it easier to hire even more diverse candidates. Once our engineering team was comprised of 20% women, we stopped sourcing exclusively female candidates in San Francisco. Despite that, our team continued to get more diverse. Women started referring more women into the company and we found it easier to find amazing team members from underrepresented groups. We also began to get press coverage for our diversity efforts, which again led to a more diverse candidate pool. What started as a recruiting problem had turned into a recruiting advantage over other tech companies. 3. Public accountability works Back in 2015, it was daunting to share our diversity numbers and goals so publicly. But that public commitment also kept us accountable, ensuring that we did everything in our power to hit our goals. More importantly, it signaled to prospective engineers that we took diversity and belonging very seriously at Gusto. Our New Diversity Goals We’ve come a long way and have learned quite a bit about building diverse and inclusive engineering teams. At the same time, we know there is much more to achieve and learn. Gender is one dimension of diversity, but there are many others. We’re broadening our diversity efforts to include underrepresented races (such as Black and LatinX engineers) and persons with disabilities. We’re also seeing a lack of diversity when we cut our data by seniority. Only 11% of our senior engineers belong to an underrepresented group (URG). For clarity, we’re using the definition of URG that is proposed by the NSF . Over the next six months, we are publicly committing to have 15% of our senior engineers belong to an an underrepresented group. Here’s how we plan to meet our goals: We’re going to be hosting three more round-table dinners to discuss diversity issues in a safe space. We’ve found that this is a great way to meet other senior engineers and share knowledge on diversity issues. We’ve hired two dedicated recruiters who will be focusing 100% of their time sourcing candidates from underrepresented groups. We’re going to be hosting and speaking at conference and events on diversity focused topics. For example, we spoke at the LadyCoders conference already this year. We’re going to do an internal audit to make sure engineers from underrepresented groups continue getting equal opportunity for internal mobility, engagement, and retention. We believe that great ideas can come from anywhere, so if you have suggestions on how we might achieve our goals, please let us know in the comments below. Like before, we'll publish an update on March 1, 2019 to share the results and lessons of our efforts. We hope you'll help keep us accountable! Comments on Hacker News", "date": "2018-09-17"},
{"website": "Gusto", "title": "How to get hired as a Product Manager", "author": ["Nick Baum"], "link": "https://engineering.gusto.com/how-to-get-hired-as-a-product-manager-at-gusto/", "abstract": "As a Product Manager and top interviewer at Google, the founder of a venture-funded startup , and a product lead here at Gusto, I’ve interviewed several hundred candidates for PM roles. In this post, I will give you some insight into what I look for in applications and interviews. Keep in mind that this is just my perspective, and other companies and interviewers may look for slightly different things. With that said, this post will give you a good place to start if you want to apply for one of Gusto’s open PM roles . Before the Interview Update your LinkedIn profile. When it’s an option, I always click on the LinkedIn profile instead of looking at the resume. Hiring managers literally review hundreds of candidates, and it’s much faster to do so with a consistent format. Keep your resume short and to the point. When looking at a resume, I’m scanning for very few things. I want to know where you worked, in what role, and for how long (years & months are better than dates). When describing your work, keep it as simple and factual as possible. Don’t focus on the technologies (“Word”, “HTML”) or processes (“scrum”, “agile”) you’re familiar with – I want to know what parts of the product you were responsible for and what the results were. Write a company-specific cover letter. Few people write cover letters these days, but a well-written cover letter can really stand out. The cover letter doesn’t have to be long, but the key is to focus on the company you’re applying for. Don’t write a cover letter about yourself (that’s what the resume is for); tell me why you’re excited about this particular role. This means that you can’t just write one cover letter and tweak it for each company, but that’s why a good cover letter is such a positive signal. During the Interview Make it conversational. Your goal in any interview is to make the interviewer feel like they’d want to work with you. That’s it. With that in mind, you should try to make the interview feel as much as possible like a good, productive meeting with a peer. Imagine you’re tackling a real problem and working on it together with the interviewer. Don’t treat it as a test you’re being judged on. Drive the interview. While you want to keep it conversational, you still want the interviewer to feel like you’re taking ownership. A great way to do that is to lay out a clear framework for how you’re going to approach the problem. Another way is to actively manage your time. Ask the interviewer how long they want to spend on any given section, then make sure you get to an answer in the time allotted. Ask questions to clarify the problem. Product interviewers often deliberately leave the initial question vague, to see if the candidate can narrow in on the heart of the problem. A key mistake is to jump straight into answering the wrong question, instead of asking clarifying questions first. To avoid this, a good tactic is to keep asking “why?” This has the added advantage of giving you time to think about your answer while the interviewer explains. Take a step back. Product Managers are expected to have a very broad and cross-functional view. Think about every external stakeholder and internal team that might be impacted. Question the assumptions behind the question. Why are they asking it? Is there a higher-level perspective? Prioritize clearly. If asked to prioritize between features, you should first establish a clear framework to guide the conversation. My recommendation is to start with a simple cost-benefit (i.e. ROI), then break either side of the equation down as necessary. Some additional variables I like to introduce are reach (% of user base impacted), frequency of interactions, and risk (likelihood of success). Be careful not to overcomplicate it. It’s better to have a simple framework that you apply correctly than a complex one that leaves you and the interviewer confused. Listen to prompts. Interviewers will often discreetly try to steer you if you’re going down the wrong path, so be very attentive whenever the interviewer chimes in. A prompt is usually not a deal breaker as long as you course-correct accordingly. To make sure you’re on the same page, you may want to repeat what they say or write down key points on the whiteboard. After the interview Ask for feedback. Often times your interviewers will have specific constructive feedback. This can be especially helpful at the early stages, where their feedback can help you do better in future rounds. Note that sometimes companies may not be able to give very detailed feedback due to liability concerns, but asking for it still lets you demonstrate a learning mindset. \"I'd love to get your feedback on how the interview went. What are some things I could have done better?\" Don’t take it personally if you aren’t moved forward. Companies are generally more afraid of false positives (hiring someone bad) than they are of false negatives (not hiring someone good). This is especially true for successful, growing companies, who have a lot of candidates and where interviewer time is at a premium. Therefore, a single “meh” interview can be enough for them to pass, even if you might have nailed it under slightly different circumstances and done great in the role. Know your market value. By the time a company gives you an offer, they’ve already invested a lot of time in finding you, and they’re highly motivated to close. You’ll want to understand how the company compensates. For example, a Google offer looks quite different than an offer from a small startup or mid-staged company. Are they cash heavy, equity heavy , or a mixture of both? Is cash or equity important to you at this time in life? What do you want? Work with the recruiter to get the offer package that aligns best to what you care about and are looking for. I hope these interview tips help as you search for your next product role. At the very least, I can promise they will if you apply to one of Gusto’s open PM roles and end up interviewing with me :) Best of luck, –Nick https://www.linkedin.com/in/nickbaum/", "date": "2018-11-07"},
{"website": "Gusto", "title": "Giving High Leverage Code Reviews", "author": ["Casey Rollins"], "link": "https://engineering.gusto.com/high-leverage-code-reviews/", "abstract": "Code reviews have several purposes, the most common of which is checking for bugs before code is shipped. But code reviews can be leveraged for so much more. For example, at  Gusto, the primary purpose of code reviews is to spread knowledge and ensure quality. At their simplest, code reviews are a quick scan of someone else’s changes with an approval to merge. At their most in-depth, they can involve pairing, reviewing code line by line, and multiple feedback cycles. Hopefully, you’ve been given a small and digestible pull request with a clear description to review. But with any pull request, getting started and knowing what to look for can feel overwhelming and unclear. In this post, I’ll give you an overview of how to approach a pull request to provide meaningful code reviews to your team. 1. Break It Down If you’ve been assigned to review a pull request that is too large (think hundreds or thousands of line changes), request that it be broken down into multiple PRs that are more manageable in size. Breaking down a feature is a skill in its own right, so offer to help your teammates with this. If the pull request can’t be broken down further but is still too large, consider doing an “in-person” code review where you walk through the pull request together. Think of this as a pair programming session, encouraging it to be a two-way street where the author explains their changes and the reviewer asks questions as they arise. As you walk through the pull request, leave in-line comments to reflect what was discussed in the review, so that context is still recorded for the future. 2. Use a Checklist It’s hard to remember everything to check in a pull request, and constantly switching contexts might cause gaps in your review, too. For example, if you’re focusing on the readability of a change, you might miss a risk to security or performance. When I’m reviewing a pull request, I often do multiple “passes” where I focus on one attribute at a time. I start at the beginning and review the pull request with a single attribute in mind before moving on to the next. When I’ve worked through the checklist, I submit the review. This checklist moves from general to specific checks because it’s important to focus on the high-level attributes first. It doesn’t make sense to offer a variable name suggestion if you’re also suggesting that an entire class or function be refactored. Don’t hesitate to review multiple times and zoom in as you go. Here are the items and prompts in my checklist. Feel free to use this as a reference, and consider drafting a custom checklist by yourself or with your team. Meets Requirements If your team’s tickets contain acceptance criteria, it should be easy to determine if the pull request meets the expected requirements. If not, here are some prompts to help you find the answer to this question. Does this pull request satisfy the expected requirements? Does the proposed UI match given mockups and designs? Does the proposed change fit within the scope of the ticket? Architecture & Design If your team creates detailed design documents before building, this should be an easy check. However, sometimes features become more complex than anticipated, and the design gets mixed with implementation. In these cases, it’s important to take a step back and ensure that you agree with the architecture. How was this feature implemented? Can I easily discern the design from the code? Does it follow an agreed upon design? If not, does the structure of the implementation make sense? Is it simple or over-engineered? Is the code open to extension in the future? Is it specific or generic? Have abstractions been made too early? If a new engineer joined your team tomorrow, could they understand this implementation? Interactions & Side Effects At Gusto, we still work in a monolithic application, so seemingly innocent changes can have unintended side effects on core functionality. If your system is less complex or if you’re working on a greenfield project, this may be less important. Is it possible that these changes have unintended effects in other parts of the system? If an existing function was changed, were all usages updated? If external dependencies were added, have they been evaluated? Test Coverage At Gusto we don’t have a QA team, so this is another critical check. Our test suite is exhaustive and must remain that way. This check shouldn’t be green just because a test file exists. Review the tests. There is nothing worse than false positive tests or flaky tests. Are there tests? Should there be tests? Do the added tests have the potential to flake in the future? Is the correct functionality being tested? Are the tests structured appropriately? Are edge cases accounted for and being tested? Are there edge cases that aren't being handled? Performance The importance of this section depends on the type of work you do. At Gusto we don’t handle billions of requests per second, but we do need to be mindful of performance when loading dozens of resources and relationships. Work with your team to determine how often you need to consider performance. Has optimization been properly balanced with readability? Is there a potential performance bottleneck? What queries are being made? Are they retrieving more data than is being used? Does the proposed change have the potential to affect performance in other areas of the system? Readability & Style Payroll and benefits are highly complex domains, so our code needs to be simple and easily understood. As a result, I place an emphasis on this area, regardless of the size of the pull request. Can you understand the code without explanations from the author? Could you debug this code without the author’s help? Could you extend the code in the future without the author’s help? Is it clear what the purpose of a variable is from the name? Is it clear what the intent of a function is from the name? Is it clear what the expected type of an object should be? 3. Focus On Delivering Value The goal of any pull request is to deliver value, whether it be to our external customers or internal users. Reviewers are here to help improve the product, not to bike shed on small details. As you review a pull request, point out which comments do and do not block merging. If you’re making a refactoring suggestion, approve the pull request, but ask that the author incorporate the change if they have time. If you find an error, make it clear that it’s not a suggestion and that it needs to be resolved before the code is shipped. I typically do this by prefixing comments with “blocker”, “not a blocker”, “nit”, and “suggestion”. Overall, don’t aim for perfection. Perfect code doesn’t exist. If the code is error-free and better than it was before, approve the pull request. 4. Be Helpful The goal of a code review is not to prevent bugs - that's what tests are for. Code reviews are an opportunity for learning just as much as they are an opportunity to ensure quality in your codebase. In the first few years of my career, I learned a ton from being on both sides of thorough code reviews. I’ve found that the best way to teach in code reviews is to simply leave longer and more informative comments. Add links to documentation and articles that help explain a concept or an alternative approach. Use GitHub’s suggestion feature to offer an alternative solution in a comment that the author can commit directly. Don’t just point out flaws and problems in code; explain why a certain change is problematic and how it can be improved. If there’s a more performant way to write a query, leave a comment demonstrating how the change would be made with a link to the documentation. Comments such as “This is slow and inefficient.” aren’t improving the code or teaching the author. As an author, I frequently ask “why?” if a reviewer suggests a change so that I understand why their proposed solution is preferable. If I don’t ask a question, I won’t learn and improve. If you stumble on a block of code that seems off, but you aren’t sure what should be changed, ask questions! This will prompt the author to think more, explain their approach, or clear up the code by rewriting it. A comment like “I’m not sure what’s happening in this block. It looks like you’re iterating over a list to perform a calculation, but I think the result is lost. Can you explain what’s happening?” will lead to a useful discussion. 5. Be Human At the core of a code review, you’re providing feedback to your peers, which might be hard. But receiving feedback is harder. Everyone on your team is trying to do their best work, so take care in delivering your message. For example, if you’re pointing out an error or asking a question, make it a team effort, not their fault. This might look like: “Can we remove some of the duplication in this file?” instead of “You missed an edge case”. Don’t forget to leave positive feedback in code reviews, too. If you appreciate the documentation in the pull request or learned something from the code, leave a comment and let the author know! Acknowledge great work and thank the author for their time and attention to detail. Conclusion The next time you’re assigned a code review that seems daunting, hopefully you’ll feel confident in knowing how to approach it. Break it down and use a checklist to remember what you want to review. Above all, focus on delivering value to your customers and helping your team improve. Remember that code reviews are about feedback, and delivering feedback well is a skill. This post can teach you how to leverage a code review for more than finding bugs, but it can’t teach you how to perfect giving feedback . You have to practice flexing that muscle yourself. Thank you to my former colleagues and Quan Nguyen for several of the ideas in this post, and to Lindsey Whitley and Lucy Fox for editing.", "date": "2019-11-15"},
{"website": "Gusto", "title": "Gusto Engineering's Diversity Update: It's Not Just About the Numbers", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/its-not-just-about-the-numbers/", "abstract": "Over the past few years, we’ve shared semi-annual updates on Gusto’s journey to create a more representative engineering team. In our March 2019 update , we shared what we hoped to accomplish in the next 6 months and a new way of working (the \"accountability model”) to help scale our efforts. In this post, you’ll hear (1) what we've accomplished since then, (2) some learnings that we’ve had along the way, and (3) what we hope to accomplish over the next 6 months. What we accomplished over the last 6 months In the spirit of accountability, I’ll share below the areas we hoped to focus on from our March 2019 post and how successful we were. Focus Then Now # of woman/non-binary directly reporting to Eddie 0 0 % of engineering candidates who get a phone screen that are from an underrepresented group (URG) 23% 30% % of engineering managers who are from an URG 24% 34% Percentage point difference (non-URG vs URG) in positive 1 answers to the question: “I would recommend Gusto as a great place to work” -4 percentage points 2 +9 percentage points 2 Compa-ratio 3 across gender and ethnicity Men: 100.6% Women/Non-binary: 100.5% Ethnicity: 101% Men: 100.4% Women/Non-binary: 100.2% Ethnicity: 99.45% % of senior engineers who are from an URG 18% 19% 1 Answered “agree” or “strongly agree”. Other possible answers are “strongly disagree”, “disagree” and “neutral”. 2 Negative percentage points indicate that non-URG were more likely to answer the question favorably. Positive percentage points indicate that URGs were more likely to answer the question favorably. 3 Compa-ratio measures the group’s average salary in relation to the midpoint of Gusto’s salary range 32% of our overall engineering team comes from one or more underrepresented group. Overall, we’re very proud of the progress we’ve made in the past 6 months, especially in creating a more diverse engineering management team and increasing representation in our candidate pool for our recruiting efforts. A couple learnings we had along the way One of the things that changed over the last six months is that we moved from a workstream model to an accountability model . That is, instead of our committee doing much of the work themselves, they hold individuals and teams accountable through check-ins and project management. A major learning for us was that the accountability model works best for metrics where the accountable team has ownership of the levers that can drive change. For example, holding the Recruiting team accountable to the goal of a more diverse candidate pool was effective, because they control the way that we source. Our sourcing team committed to spending 40% of their time sourcing engineers from underrepresented groups, which enabled them to move the needle on our metrics. Gusto is a very metrics-driven culture and our tendency is to use lots of data to fully understand the problems we’re trying to solve. But when it comes to creating a more inclusive place to work, we learned that it’s important to supplement our data with a qualitative understanding of how people experience their time at Gusto. As obvious as it sounds, just asking someone how they feel could give you the best insights. We’ll continue to track progress through data where they matter, but we also will lean into activities that will be more qualitative and subjective in nature. What we hope to accomplish next We’re very excited to continue to make more progress on making our engineering team a more representative and inclusive place to be a part of. Over the next six months, we intend to focus on creating a deeper sense of belonging within our teams. We hope to accomplish the following: Understand and improve how Gusto is experienced by different underrepresented groups by organizing a series of internal roundtable lunch chats with different groups of engineers. We’ll gather qualitative feedback and create an action plan to address as much of the feedback as we can. Better support the “onlys\" in an engineering team. The onlys are individuals from an underrepresented background who don’t have someone else from a similar background in their team to support them. Being in this situation can oftentimes feel very isolating. We want to understand where the onlys exist in our engineering team and find ways to better support them. We’re very excited to be experimenting with Atlassian’s Balanced Teams Diversity Assessment tool to help us here. Better recognize and reward diversity and belonging work. Time we spend to create a better community at work should not be extra-curricular. Rather, we see it as a core part of one’s role as a Gusto engineer. As a result, we want to update our Engineering Attributes to ensure we are rewarding work that individuals do to create a better community and engagement at Gusto. We’re excited for what the next 6 months will look like for us and we’ll continue to provide more updates as we learn more. Comments on Hacker News", "date": "2019-11-22"},
{"website": "Gusto", "title": "A Visual Guide to Using :includes in Rails", "author": ["Julianna Roen"], "link": "https://engineering.gusto.com/a-visual-guide-to-using-includes-in-rails/", "abstract": "If you're new to developing Rails applications, you've probably come across the term N + 1 queries. You probably also know that this is something you want to avoid. As an engineer who joined Gusto straight out of undergrad with no Rails or full-time industry experience, I was initially overwhelmed by this concept. But, when broken down into bite size chunks, the solution to fixing N + 1 queries is actually quite simple. N + 1 queries occur when a group of records are loaded from the database in an inefficient way, along with any records associated with them. The examples below dive into how we can solve this issue with :includes and will help demystify how this method works under the hood. Note that the code snippets are using Ruby 2.3.4 with Rails 4.2.11. Outlining the Problem Let’s say we have a model called Employee who has many Forms . # == Schema Information\n# Table name: employees\n# id\n# name\nclass Employee < ApplicationRecord\n  has_many: :forms\nend\n\n# == Schema Information\n# Table name: forms\n# id\n# employee_id\n# kind\nclass Form < ApplicationRecord\n  belongs_to: :employee\nend And we have 5 total employee records and forms that we want to load because we want to do some sort of mutation. Employee.all.map { |employee| employee.forms }.flatten The SQL for this command looks like: > SELECT `employees`.* FROM `employees` ORDER BY `employees`.`id`\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` = 1\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` = 2\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` = 3\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` = 4\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` = 5 There are 6 total hits to the database being made since we load employees in the first query and then make 5 additional queries to grab each employee's forms. In other words, N + 1 SQL selects occur where N = 5. Making queries ⚡️ with :includes Rails provides an ActiveRecord method called :includes which loads associated records in advance and limits the number of SQL queries made to the database. This technique is known as \"eager loading\" and  in many cases will improve performance by a significant amount. Depending on what your query is, :includes will use either the ActiveRecord method :preload or :eager_load . When does :includes use :preload? In most cases :includes will default to use the method :preload which will fire 2 queries: Load all records tied to the leading model Load records associated with the leading model based off the foreign key on the associated model or the leading model So, if we introduced :preload to our query we would produce only 2 SQL selects where forms would be loaded, based off the foreign key Form#employee_id . Employee.preload(:forms).map { |employee| employee.forms }.flatten\n\n> SELECT `employees`.* FROM `employees`\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` IN (1, 2, 3, 4, 5) The SQL for this example would look the exact same if we were to replace :preload with :includes . Employee.includes(:forms).map { |employee| employee.forms }.flatten\n\n> SELECT `employees`.* FROM `employees`\n> SELECT `forms`.* FROM `forms` WHERE `forms`.`employee_id` IN (1, 2, 3, 4, 5) When does :includes use :eager_load? :includes will default to use :preload unless you reference the association being loaded in a subsequent clause, such as :where or :order . When constructing a query this way, you also need to explicitly reference the eager loaded model. Employee.includes(:forms).where('forms.kind = \"health\"').references(:forms) In this case, :includes will use the method :eager_load which will produce 1 query that uses left outer joins to build an intermediary table which is then used to construct the model output. > SELECT `employees`.`id` AS t0_r0, `employees`.`name` AS t0_r1, `forms`.`id` AS t1_r0, `forms`.`employee_id` AS t1_r1, `forms`.`kind` AS t1_r2 LEFT OUTER JOIN `forms` ON `forms`.`employee_id` = `employees`.`id` WHERE (forms.kind = \"health\") The SQL for this example would look the exact same if we were to replace :eager_load with :includes . We can remove :references in this case too. Employee.eager_load(:forms).where('forms.kind = \"health\"')\n\n> SELECT `employees`.`id` AS t0_r0, `employees`.`name` AS t0_r1, `forms`.`id` AS t1_r0, `forms`.`employee_id` AS t1_r1, `forms`.`kind` AS t1_r2 LEFT OUTER JOIN `forms` ON `forms`.`employee_id` = `employees`.`id` WHERE (forms.kind = \"health\") If you replaced :preload with :includes , however, the query would fail to execute. Can I use :includes with Form as the leading model instead? If the query were reversed, where forms were loaded first and we wanted to efficiently load the employee association, we can still use :includes . The query will load employees based off the collection of Employee#ids , which is referenced from Form#employee_id . Form.includes(:employee)\n\n> SELECT `forms`.* FROM `forms`\n> SELECT `employees`.* FROM `employees` WHERE `employees`.`id` IN (1, 2, 3, 4, 5) Can I eager load nested associations with :includes? Yes. Here are a few examples of different scenarios you could use if the Form model was modified to have additional associations such as has_one: :signer and has_one: issuer , which has_one: :address . Employee.includes(forms: :issuer)\n\nEmployee.includes(forms: { issuer: :address })\n\nEmployee.includes(forms: [{ issuer: :address }, :signer]) Benchmarking—Is using :includes always faster? How big is the performance gain by using :includes in your code? And since :includes invokes either :preload or :eager_load , which method ends up being faster? I tested the performance of the same queries listed in the examples above in a local instance of Gusto's database to see the difference. Each query was run 3 times with caching disabled and then averaged in the table below. Employee.first(n).map { |employee| employee.forms }.flatten\n\nEmployee.preload(:forms).first(n).map { |employee| employee.forms }.flatten\n\nEmployee.eager_load(:forms).first(n).map { |employee| employee.forms }.flatten The data shows that using :includes will improve the performance significantly when a :preload is invoked, but has the opposite effect for :eager_load in most cases. :eager_load constructs a complex query to gather information about the associations being loaded, thus it makes sense that this option would be slower (although even I was a bit shocked at the dramatic decrease). Measuring query performance in your own app is the best way to ensure you are using the most efficient methods available for your use cases. As a general rule, though, getting in the habit of using :includes will lead to performant code that creates the best experience for other developers in your codebase - and of course, customers interacting with your apps. Thank you to Quan Nguyen, Tori Huang, Jim Liu, and the Gusto Engineering Blog team for editing and feedback on earlier drafts.", "date": "2019-02-11"},
{"website": "Gusto", "title": "Lessons I Learned From Renaming in Ruby", "author": ["Jordan Cutler"], "link": "https://engineering.gusto.com/lessons-i-learned-from-renaming-in-ruby/", "abstract": "My first week on a new team I broke production. The feeling of getting a notification of my PR being reverted by the oncall engineer because it broke production was not fun. This happened because on my first week, I decided to try to improve some of the naming in our codebase; instead of everyone being ecstatic about the easier-to-work-with names, I ended up becoming scared to do any refactor like that again. Coming from a statically typed language background, I learned my mistakes the hard way when moving to coding in a dynamic language like Ruby. From this experience, I learned how refactoring can make you wish you never started touching it, how refactoring can make you scared to refactor again, but also how to do it correctly and the benefits to the team once it’s done. For this article, I will be specifically focusing on advice for renaming in a dynamic language, where it is difficult to find the usages you are looking for. The advice will largely pertain to monolithic codebases but can be relevant to all forms of refactoring. Here are the sections that will be covered Figuring out if it’s worth it Refactoring checklist My takeaways from this experience Figuring out if its worth it Sometimes the toughest part about refactoring is figuring out whether you should do it in the first place. In my case, I started on a tech spec and was finding it incredibly difficult to see what changes needed to be made without doing some basic naming changes first. I ended up splitting my tech spec into two parts. Part 1 was the refactor, and Part 2 was the behavioral changes. By doing this, my reviewers were able to just focus on the naming improvements without worrying about me modifying behavior. It also made the feature changes much easier since I was able to work with cleaner code. For more information on separating structural and behavioral changes, Kent Beck wrote an article about it here . A few questions you may want to ask yourself to determine if the refactor is worth it: Will you be using the refactored code often after updating it? Positive indicators Will be adding additional functionality that will use the refactored code. Negative indicators The code has not been touched for a long time and is not expected to be touched for awhile. If a name is misleading, how misleading is it and could it cause errors down the road if misunderstood? Positive indicators The name has implicit assumptions. For example: the method name is is_active? but actually means is_active_or_upcoming? Negative indicators It is unclear whether the name change would be better. If the refactor is performed incorrectly, who is impacted? Positive indicators Minimal impact to the customer if done incorrectly. Only errors on the backend logged. Negative indicators If done incorrectly, customers would not be able to perform basic functionality within the app. How easy is it to confirm the refactor was done right, all cases were caught, and no issues should be run into? Positive indicators Method name was very distinct to begin with, and easy to find all cases Test coverage is very good for the affected areas. Negative indicators Method name is hard to distinguish between other usages of the same method name that are not relevant. Test coverage is poor for the affected areas. You can generally use these indicators to determine whether a refactor is worth doing, and the value you place on each one. It’s best to consult with the rest of the team before doing the refactor, presenting your argument for why it should be done, but also opening up about the risk of performing it. Let’s assume you and your team decide the refactor is worth it. Tips/Checklist The first thing that should be done if this refactor is large in scope and there is a concern of breaking old usages: Create a second method with the new name, copy the code over, and call that method from the old one. Additionally, log a warning from the old method to indicate it’s still being called. Once this is done, we can search for the old usages and update them to the new method without worry that a breakage will occur. Text searching tips (Ruby specific) Let’s assume that we are searching for usages of a method on the database model Apple . Some of the following tips may seem obvious but it’s a checklist so they should be ;). Pay careful attention to the . where appropriate. Disclaimer: The editor I work with is RubyMine but most tips should apply or can be slightly modified to work for your editor. Search for usages of apple. . This can be an example such as apple.method_name Do the above search with apple.method_name first to clear up most of them. After doing the above, regex search for apple(.*)method_name to catch cases where there is a new line in between apple and method_name . Search for usages of apples. . This can be an example such as farmer.apples.first.method_name Search for usages of Apple . This can be an example such as Apple.first.method_name . Depending on what you’re searching for, you may want to do a “word” or “exact” search that will only search for cases without additional text. For example, without word search, “apple” would be shown in “applepicker” but with word search “applepicker” would not show up. Lastly, for any areas of the code you updated and are not sure about, feel free to take it as an opportunity to check test coverage to ensure what you changed is being tested. For Rspec, our own Kelly Sutton wrote an article about “counting the contexts” to sniff test the test coverage. Although my initial attempts at refactoring ended up breaking production, I ended up pair programming with my manager to make the fixes needed and to learn the tips above. I shared with him how the initial attempts at refactoring made me scared to make changes again, but he reminded me that it’s a good quality to take the initiative to change things if they can be better, and that I shouldn’t be afraid of refactoring in the future. Since my mistakes, I have had the confidence to make multiple successful refactors in our codebase. It’s kinda like how if you got into a car accident and didn’t get back to driving soon after recovering, you may have trouble getting back to driving at all. In the words of Rocky Balboa, “It ain’t about how hard you hit, it’s about how hard you can get hit, and keep moving forward.”", "date": "2019-12-06"},
{"website": "Gusto", "title": "7 Tips for Building a Startup Within a Startup", "author": ["Sahil Jolly"], "link": "https://engineering.gusto.com/7-tips-for-building-a-startup-with-a-startup/", "abstract": "Introduction As the engineering lead on the Flexible Pay team, I often get asked by my colleagues, “What is it like to work in a startup within a startup?” First, some context: Two years ago, Gusto decided to build the Flexible Pay feature, offering employees the opportunity to access their paycheck on their own timeline, and without any added cost to the employer. Instead of building it as part of an existing org, we chose to incubate this v1 product. The intention was to allow the team to go from zero to one as quickly as possible. So we carved out an envelope budget, and started the team with a PM and an engineer in a tiny conference room. By immersing themselves in the problem, talking to customers and through constant learning, experimentation and iterating, a product-market fit began to emerge. The project has now grown to a cross-functional team of 15. We encountered situations that are common to any incubated greenfield team within a big organization. If you’re starting such an endeavor or even thinking about it, here are some things to keep in mind. 1. Align your idea to get buy in For a typical startup, getting an idea off the ground involves pitching to investors. But unlike a startup, we were part of an existing organization with a mission, values, and business objectives. At Gusto, our mission is to create a world where work empowers a better life. Personal prosperity is a key pillar in achieving this mission. One of our early engineers often wondered why US workers were only paid at the end of the pay cycle rather than getting their wages after a day’s work. When you combine this with the fact that 40% of US workers need help to cover a $400 emergency, it created a compelling pitch to our leadership team. As a payroll company that facilitates workers receiving their paychecks, we were in a unique position to solve this problem by letting employees access their earned income prior to payday. By enabling personal prosperity for our customers, we could enhance Gusto’s core business and offer a differentiated benefit to both employers and employees. 2. Allow people to break their traditional roles It is critical to be flexible, especially early on, when there are more roles than people. The benefit of an established org around you is that it gives you access to a lot of advisers in various functions. They can help you avoid common mistakes, point you to the right resources and save you time and money. Eventually, though, your team must do the heavy-lifting and wear all the hats that your advisers wear during their day jobs. In our case, this involved marketing and customer acquisition, wireframing, conducting user research and providing support to early adopters. This, in turn, created a tight feedback loop that allowed us to adapt the product, test assumptions and change course when something wasn’t working. 3. Avoid hard dependencies in order to move fast Since we were building a feature on top of Gusto’s existing payroll product, there were pieces of Flexible Pay that required changing other teams’ codebase. Rather than asking the teams to incorporate our requests in their quarterly plans, we took the work on ourselves. This meant we had to ruthlessly prioritize and constantly make tradeoffs. Engineers had to be nimble and frequently switch context. Our engineers had previously worked on other parts of the system, which meant they could easily build complex flows interacting with multiple domains such as payroll, payments and onboarding. This would not have worked without constant communication, pairing and code reviews from other teams. Though there were moments when a change caught the dependent team off-guard, in most cases we were able to give plenty of lead time and get the team’s input and approval. 4. Identify and hire specialized roles early One strategy that gave the team a lot of autonomy was to have a dedicated budget. We carved out a portion from the overall company budget, with the intent to short-circuit the dependencies between recruiting, finance, legal, credit policy, and accounting. This allowed us to sit outside the company’s operational planning, and empowered us to make decisions around hiring, partnerships, external counsel and financing. Being a financial product in a highly regulated space, this meant we prioritized hiring a full-time legal expert, a financial analyst, and product partnerships lead as some of the early team members. 5. Rely on existing infrastructure As tempting as it was for us to have own dedicated service, it really helped to lean on existing infrastructure, developer tools, monitoring, alerting and security systems. This allowed us to focus on the core functionality that we were building, instead of having to figure out solved problems, such as getting a CI system in place. This works until you have found product-market fit. Once you do, it is just as vital to quickly pivot and build for the long term. One way to do this is to provide a strong technical vision alongside the product vision and make sure the team is not in 'prototyping mode' as you begin scaling. 6. Take the right shortcuts Since the team is expected to move fast, there will be a temptation to take on technical debt, cut corners, avoid tests, code reviews, or design reviews. While some of these may seem like things a startup would do, we still have the company’s established brand and reputation at stake, especially in the financial domain. We followed our engineering best practices, and ensured features were fully tested and code reviewed for the most part. Instead we were deliberate about cutting scope, and favored building simpler interactions over more complex ones. These simpler paradigms allowed for getting things in front of customers faster, throw out things that didn’t work, and only polish things which we intend to keep. 7. Be accountable Treat the rest of the company as your investors. Sitting outside the regular operational planning cadence doesn’t mean that our team didn’t set goals and measure results. We were meticulous with what and how we measured, ensuring we were getting the right insights to inform quick decisions. We shared progress updates with the core team members on a weekly basis, and provided updates to leadership when we made big strategic changes. Hopefully these learnings will help you think about building a startup within your company. While the approach cannot be applied to every initiative, it has been working well for us while building Flexible Pay. It provides us with the right balance of autonomy to make quicker decisions and support from Gusto’s specialized functions to overcome hurdles faster. As evidence we’re proud of going from zero to one with a sophisticated financial product in under a year!", "date": "2019-06-24"},
{"website": "Gusto", "title": "When Girls Lose Interest in Stem — and What You Can Do about It", "author": ["Lindsey Whitley"], "link": "https://engineering.gusto.com/this-is-when-girls-lose-interest-in-computer-science-and-heres-what-you-can-do-about-it/", "abstract": "Patches our design team created for the event I'm an engineer at Gusto . In my spare time, I teach middle- and high school-aged girls how to code . I do this because I want girls to know about all the career options that are available, even if their communities don’t. As I was researching the best ways to get students interested in STEM fields, this sentence stood out: \"Girls become interested in so-called STEM subjects around the age of 11 and then quickly lose interest when they're 15.\" Up until this point I had only focused on getting younger (i.e., elementary school) girls interested in computer science, hoping that the spark would last through college. But now I realized that I was missing a huge part of the equation. Why girls lose interest in STEM starting in high school High school is angsty. Photo by Ben Mullins on Unsplash I remember worrying about being liked by my friends and wanting to be noticed by a boy, all of which made it harder to focus on the future and who I wanted to be. Had it not been for my desire to rebel against what society told me I couldn't accomplish, I don't know whether I would have continued to focus on STEM. Today, most of my friends who are in STEM fields became interested in it after college. Welcoming people without formal training into roles they are capable of may be more common today, but it is still easier to prove to a prospective employer that you're worth hiring when you have formal training and start earlier. This means that keeping young women interested in STEM — well beyond middle school — opens up more career opportunities for them earlier in life. What we’re doing about it While developing a program to keep girls interested in STEM, I was inspired by shadowing sessions we did with a Denver code school called Turing . In these sessions, post-high school web development students came to the Gusto office, took a tour, asked questions about working here, and then worked with an engineer for an hour. Now that we were targeting high school students, who are younger, we needed to adjust our tactics. So here’s how we kicked off the program. We partnered with Girl Scouts of Colorado , which has a large pool of engaged high school students. They set up an application for our event and included an announcement in a newsletter they send to high school Girl Scouts. Creating an application helped us find girls who were genuinely interested in the event and willing to put in a bit of effort to be part of it. We made the experience worth students’ time. Since we anticipated that girls would be driving into Denver from the suburbs, we wanted to make each session longer (about four hours). This would give them a better travel-to-activity ratio. We targeted all levels of experience. We assumed that some girls would have written code before and some probably wouldn't know the basic concepts of coding. We went beyond just engineering. Unlike the Turing students who were further along in their STEM-journeys, we couldn’t assume that these younger girls had decided to do anything related to STEM. To account for this, we expanded our scope to include Product and Design. Photo by STEMShare NSW on Unsplash How we announced the program I sat down with teacher-turned-engineer Stephanie Bentley , to go through the Colorado Academic Standards for Computer Science and draw connections between what we planned to do and what the state expects students to learn about CS. For the announcement and application, our blurb looked like this: Girl Scouts is partnering with Gusto, a Silicon Valley- and Denver-based technology company, to teach girls entering 10th, 11th, and 12th grades what it’s like to write code that solves a real-world problem! Gusto will host a group of girls for three (on November 16th, April 12th, and June 28th) four-hour (from 10am-2pm) \"jam with software developers, product managers, and designers\" sessions with lunch included. In each session, girls will get a taste of an average day as a developer - from creating a feature to finding and fixing bugs in the code. Over the three sessions, we’ll go through a design sprint to define a problem to tackle with software and design a solution. We hope to inspire a future cohort of computer scientists! Missing school can be hard to justify, which is why we made sure the program addressed the state's computer science standards: 1.4 Large, complex problems can be broken down into smaller, more manageable components 2.3 Computer software is written for specific purposes 2.5 Client considerations drive system design 3.1 The creation of a computer program requires a design process 3.3 Collaborative tools, methods, and strategies can be used to design, develop, and update computational artifacts 3.4 Client-based design requirements and feedback are essential to a quality computational product or service Planning beforehand only required a few hours a week for a few weeks, including a couple of meetings with Stephanie to make sure we weren't totally missing the mark. The day of On the day of the event, I met the girls and their parents, and signed them in. Then we all got on the same page by discussing: Our goals for them Our expectations for their participation Who we are. What are the teams here? What is the structure? How do Engineering, Product, and Design work together? What do they want out of this? What’s their background? How familiar they actually are with code? (Depending on what they knew, we could target who they were shadowing.) During the design sprint, we explained why we do design sprints and then what our goal for the day was — to pick a problem to tackle in the coming sessions. We started by brainstorming. In ten minutes, each of us got a stack of post-its and a pen and wrote as many problems that we encounter in our daily lives as we could think of. We then explained each idea to the group and tried to group problems together as we went. Then the team voted on which problem we wanted to tackle together. Once we had a winner, we spent the rest of our time digging into what problem we really wanted to solve and which angle was most interesting to all of us. At the end of our session, we landed on the problem: Teenagers and adults have a hard time getting past discussions about the weather and into a deeper conversation. So how can we fix that? Since the group was pretty evenly split between teenagers and adults, we felt particularly able to explore and solve this problem. Talking about the generational gap is valuable on both sides: adults can share their school and work experiences, and teenagers provide fresh insights. In the design sprint sessions to come, we will nail down the root cause of this gap and start designing a solution to the problem and define an MVP. High school kids are smarter than you think One of my concerns at the start of the day was that we were throwing too much at these students in such a short period of time. However, no one was too shy to interrupt, ask for clarification, or express particular interest in a concept. While explaining a tool that my team manages, how we deploy, and what happens as a result of deployment to one of the students, I was grateful that I had jumped right in and allowed her to pick our direction by asking good questions. In our debrief at the end of the day, we asked everyone: What was the most valuable part of the day? What could you have done without? Do you have any general feedback on how everything went? We learned that while it's still valuable to ask participants what they thought in-person, there might be a better way to do this. Maybe we could have asked throughout the day, for rankings 1-10 for different activities, or in a way that gave them anonymity. This is something we'll be changing before the next two sessions. Re-reading the articles that led to this program, I remembered that girls often have misconceptions about STEM. When we ask for feedback in the next two sessions, I'll focus on understanding whether we are addressing these common issues: Is writing code social? Is writing code creative? Can you solve problems that matter with code? What kind of culture is prevalent in STEM? Are you considering any STEM fields or careers for your future? Why or why not? What we hope to do next In the next five months, we'll have two more sessions like this where we finish our design sprint and continue to demonstrate the social, logical, and transformative nature of writing software. Designing and creating a solution to a problem transforms it from a nebulous annoyance into a functional product. By pairing with the girls, eating lunch together, and allowing them to observe engineers working together, we hope they see that good code is written when people feel comfortable to bounce ideas off each other and pool their collective knowledge. I'm excited to see what we learn about why girls do or do not pursue STEM — and whether our program can make a difference.", "date": "2019-03-07"},
{"website": "Gusto", "title": "Tips for Engineering a Company Rebrand", "author": ["Jesse Zhou"], "link": "https://engineering.gusto.com/engineering-a-brand-refresh/", "abstract": "During Gusto’s most recent brand refresh, a massive amount of engineering work was required to bring the warm and sophisticated vision our designers came up with to life. The road to our rebrand wasn’t easy: it wasn’t as simple as changing the logo and a few CSS styles. While we were ultimately able to bring this new vision of our app to life, there were many lessons we learned from our whole reskinning process. We want to share these learnings in hopes that it will help you approach your own brand reskin both smartly and safely. Invest time into maintaining your style stack. Anyone involved in frontend engineering knows how dramatically the technologies can change over time. Tools are constantly updated and added, and new frameworks are always popping up for engineers to clamor over. A growing organization, once out of the explore phase , needs to have a discussion about what technologies to double down on and use daily to prevent distractions. In this regard, fine-tuning and maintaining a great “style stack” is no different. Just like in JavaScript land, how people write and manipulate CSS is an ever-changing landscape. Making sure your company is up-to-date with the most robust tools for what you need is extremely important, especially if you’re planning on reskinning your product. One problem we had before going into the reskin was that while we had a consistent style stack, it wasn’t the most up to date. This forced us to pull a few hacky tricks, like using a webpack-level feature flag to switch between our two different versions of CSS. We were also forced to use older technologies like Legacy Context, since we were still on React 15 at the time. Having such limitations is just part of owning a system that grows more and more every day. But a direct result of us not properly maintaining our style stack was that our velocity in reskinning the product went much slower. We constantly got distracted because we were fighting with the older technologies that governed the product. Since finishing our reskinning efforts, we now have a dedicated team that makes sure the application infrastructure within our monolith is always up to date, for both our backend and frontend technologies. Making sure you get engineers that both really care about your style stack and maintaining it will pay dividends down the road and make reskinning more effortless in the future. Create and maintain a great component library, and make sure it gets lots of love. Similar to maintaining a great style stack, maintaining a robust component library is also very important. A component library, generally speaking, is a library of UI components that share and reinforce a similar theme or design. Component libraries are the bread and butter of making great looking interfaces in an efficient manner, while still being consistent with the company brand. One issue we had was that before this project, there wasn’t a clear team that owned Gusto’s component library. It was just a combination of frontend engineers who cared about our components, but whose jobs weren’t directly related to maintaining and adding new things to this library. Not having a clear maintainer for our CL affected our main codebase because everyone was just adding things that would work for their own specific situations. It made reskinning the codebase a million times harder, since everyone was using the component library in unorthodox ways. Since starting our own reskin, we’ve started a new Design Systems team, whose job is to not only maintain the component library, but also come up with clear instructions for how to interact with the components in them. These instructions come in the form of clear APIs, concise documentation, newsletters, and office hours. Having this team is easily one of the best decisions our company has ever made. Add comprehensive visual regression tests to every part of the application. Visual regression testing is a new kind of test that essentially takes a screenshot of a base build and a screenshot of a new build, and diffs the resulting pictures pixel by pixel. Visual regression testing is important because it is both framework and language agnostic: it allows you to quickly identify and locate UI bugs without having to look at the HTML, CSS, or JS that was used to build it. For example, we used visual regression snapshots below to verify if we’ve fixed a missing set of icons in our development branch. These icons went missing while we were messing with a few things under the hood. Using a visual regression tool called Percy , we were able to quickly identify if our fix worked, without having to comb through our more complicated unit tests. Visual regression testing can catch UI errors that might be imperceptible to the human eye. While doing our own brand reskin, we just started to add visual regression testing to Gusto using Percy, but we did not nearly add enough screenshots to fully cover our application. Because of this, a few critical UI bugs like the one above slipped through the cracks and landed in production. It was only after a human noticed it did we add a new visual regression test for this page. Now, any future regressions would be caught, but we still let one bug slip out. Many times during the reskin, adding such tests for us were much like a reactive response after something slipped, rather than proactive effort to ensure future changes wouldn’t affect our app’s UI. If you are planning on changing the brand of your company without disrupting the current experience for your users, you will absolutely benefit from adding a visual regression test suite to your app, and making sure it’s covering as much of your app as possible before starting your reskin. You will be able to have much more confidence that your changes are having the intended effect that you want them to have. Spend a lot of time and effort into making feature specs really good. Feature specs (also called “request specs” in Ruby-on-Rails land) are end-to-end tests that spin up both the frontend and backend of an application and simulate user input on the application. It’s basically mimicking what a user would do and tests the result. Feature specs are usually justified because they’re the only way to ensure all of the moving cogs actually fit and work together. However, they also have a hidden benefit: they allow a developer to quickly navigate through complicated business logic, and get to a specific view through running a simple test. For us, all we had to do was to put a binding.pry in our feature spec code, and the test would stop executing while we played around with the view we wanted to reskin. Traversing complex business logic is handled by a feature spec. No human intervention needed! While Gusto as a whole understands the complicated laws that make up the Payroll, HR and Benefits industries, understanding the business logic that makes the application tick sometimes isn’t the most straightforward thing in the world, even for the most experienced engineers at our company. Writing a feature spec that can help guide you through the app is a low cost way to help developers render a specific page without having a human help them get to it. At Gusto, we use Capybara to write our feature specs. Similar tools in the industry right now include Selenium and Cypress . Take our word for it: making sure feature specs are added regularly will make the reskinning process that much easier for your company. Finally, delegate things accordingly One thing we should’ve coordinated better on was involving other teams earlier in the process. It was easy to convince ourselves in the beginning that we should minimize the impact the rebrand would have on other OKRs by assigning a small team to take care of everything. However, in a complex application like ours that serves hundreds of thousands of small businesses every day, this proved to be much easier said than done. Even with great feature specs and Percy coverage, some things are just inaccessible without the context a real person can provide. A rebrand at this scale takes much longer when such people aren’t available. Making sure you can reach out to at least one person in each team for the duration of the reskin will not only speed up the process, but make everything land a little safer as well. Never forget a rebrand is a company-wide effort While the engineering team was busy reskinning the product, there were of course many other non-engineering processes that were going on at the same time all of this was going on. Illustrators, designers, writers, marketers, you name it – all of us were involved in not only rebranding the company, but getting the company to where it is today. While reskinning your entire web application can be incredibly complex and will have its rough moments, it is truly an all-hands-on-deck moment that can bring a company together closer than before, especially if it’s done right. Thank you to Matt Blake, Quan Nguyen, and Robin Rendle for reviewing drafts of this post.", "date": "2020-02-17"},
{"website": "Gusto", "title": "Gusto & Xero: Building a Massive Integration Across Two Companies", "author": ["Julianna Roen"], "link": "https://engineering.gusto.com/gusto-xero-building-a-massive-integration-across-two-companies/", "abstract": "This article is based on my presentation “Gusto & Xero: 4 Offices, 2 Countries, 1 Massive Payroll Integration” given at Kiwi Ruby on November 1, 2019 in Auckland, New Zealand. In January 2019, Gusto became the preferred payroll provider of Xero in what was both companies’ largest software integrations to date. Payroll with Gusto from xero.com In other words, Xero’s US payroll customers were migrated to Gusto by the end of 2018 and new customers could easily set up Gusto payroll accounts through their Xero dashboard. This was a 7 month long process that began in July 2018 and required close collaboration between both companies. There were strict deadlines, multiple domestic and international offices, as well as a lot of customers’ pay on the line. So what exactly did we build and how did it go? Before we dive in, let’s introduce the two key players first. Who is Gusto? Gusto's payroll dashboard Gusto (the team behind this blog) is a “people platform” providing payroll, benefits and HR for over 100,000 small businesses in the US. Our mission is to make running a small business as easy as possible by building products that bring personal prosperity and peace of mind to business owners and their employees. Gusto currently serves the US exclusively with offices in San Francisco, Denver, and New York City. Who is Xero? Xero's main dashboard Xero is also a SaaS company that provides accounting software, payroll, and other financial services for small businesses and their advisors across the globe. Similar to Gusto, their mission is to take away the pain points of running a business with more of a focus on the financial and bookkeeping side of things. Xero has offices worldwide including in New Zealand, US, Australia, Canada, UK, and Singapore. How did the partnership with Gusto and Xero begin? No significant business collaboration forms overnight and that was certainly the case with Gusto and Xero. We had previously collaborated with them on a smaller scale accounting integration where we got to know each other’s teams and found success in this venture. What made us drawn to wanting to work with them again? Aside from being in similar tech space, Gusto and Xero share key core values such as being customer service driven. We both take pride in making beautiful, high quality software that makes a problem space that has traditionally been intimidating easier to approach. Testimonial from gusto.com Thus in July 2018, the teams came together again to integrate Xero’s US product with Gusto’s full-service payroll solution . What exactly did we build? There were two main focuses outlined in our contract that we needed to finish by the end of 2018: Migration: Migrate existing US Xero payroll customers to Gusto payroll. Provisioning: Allow US customers of Xero to set up Gusto payroll and have a seamless experience between the platforms. The main Xero projects that Gusto EPD worked on Sub-projects varied in size and consisted of teams of 1-3 engineers. Some projects were small and took about 2-3 weeks to complete—such as co-branding which involved displaying Xero’s logo in Gusto when a user SSO’d into our app. Other projects were more lengthy and required significant amounts of cross-collaboration between teams. The meatiest project by far, which we'll dive into a bit, was creating our migrations API. Building the API Prior to Xero, we had never had to migrate customers in bulk on a very tight timeline into Gusto's main app before. These requirements prompted the migration API project. One of the biggest questions we faced was how to ensure speed of migration as well as correctness of data (payroll is something you do not want to mess up). There was a lot of information missing from Xero's US payroll customers that Gusto needed since we offer full-service payroll—meaning we will file all tax documents on behalf of a company—while Xero did not. On top of this, when Xero happened to have data that Gusto needed, the format was not always compatible with our validations. For example, Xero’s phone formatting was more relaxed due to the variation of international phone combinations they accept while ours strictly requires a US 3 digit area code followed by a 7 digit number. In order to optimize for conversion and gather as much data as possible, we designed the API so that we could use the minimum amount of information to create a company and admin in Gusto. There was a tradeoff here in that having more relaxed constraints to take in companies meant that we would have fewer companies that would be completely ready to run payroll after they were sent over from Xero. To counterbalance this, we relied on our internal transfers team to reach out to customers and help them finish onboarding. To assist our transfers team, we made some enhancements to the tools they use so they could be more efficient in fully migrating companies. One way we accomplished this was by displaying the entire payload of migration requests to transfers team members so that they could pick it apart and manually backfill data that was not automatically saved. A sample migration record Testing the API We tested the API in several stages starting with our pre-production environment. Xero would POST mock companies to us that mimicked a realistic set up and we would fix any bugs that came up. Example migration request We used Slack threads to our advantage during testing and isolated each day’s correspondence into a thread. This organized communication between Gusto and Xero and made it easy to refer back to. Once we felt like we worked out all of the major hiccups surfaced in our pre-production testing, we moved into a beta phase where Xero POST’d actual companies to Gusto’s production environment. We started with companies that had simple setups and gradually began importing companies that were more complex (ex. employees in multiple states, states with complicated taxes, companies large in size). When migrations failed, we would log the issues, fix them, and then request that Xero send us those companies again. After testing a number of scenarios successfully, we then began importing companies in bulk. We started with smaller batches of 5–10 companies which slowly grew to batches of about 50–100. We were in close contact with our internal transfers team the entire time to ensure that we were not overwhelming them. And before we knew it, we were able to successfully migrate all of Xero’s customers onto Gusto before the end of the year. Xero and Gusto teams celebrating at a happy hour How did we set ourselves up for success? It’s definitely not every day that you have to collaborate closely with an external organization. Plus we had the added challenge of working across four offices (two in San Francisco, one in Denver, and one in Wellington, NZ). We deliberately did not want to burn the midnight oil by the end of the project, so we put practices into place that helped us synthesize as one team with clear goals and expectations. In the beginning, the team was provided A LOT of context Mike, our project PM, giving us the deets on Gusto and Xero Before we began development, our lead product manager for the project delivered an internal kick off which shared key business insights and motivations behind building this integration. By inviting all involved team members to know the details on what we were doing provided a strong foundation and momentum for getting started. We scheduled regular in-person meet ups Gusto and Xero working from the same office Following our internal kick off, we held an external kick off at Xero’s office where project motivations were stated again, but this time with both companies’ teams. The whole day was dedicated to context sharing, tactical conversations, ice breakers, and boba runs. Afterwards, Xero invited Gusto to work from their San Francisco office for the first week of development. This provided quality in-person time where the teams were able to familiarize even more with each other and strengthen our personal connections. We maintained our relationship throughout the project by holding bi-weekly show-and-tells split between our offices as well as meeting up for celebrations over big milestones. Getting to know each other and maintaining more personal relationships established greater trust between our teams and made it easier to communicate throughout the course of development. Our aligned values carried us through One of the most crucial factors that made our integration run smoothly from start to end was our aligned values. Both Gusto and Xero teams hold high customer standards above anything else. It wasn’t a matter of “we’re doing it this way because we said so,” but a matter of “what process makes the most sense for our end users?” With this collaborative mindset carrying us through, working with Xero was straightforward and actually quite a fun process. Gusto and Xero—a balanced partnership Since Xero is a global company, they are larger than Gusto and serve more customers than us. But just because they are bigger, doesn’t mean that we were the “smaller fish” in this project. Each team had equal say in the development process and was able to drive the project forward. Partnerships can come in many shapes and sizes and are capable of maintaining long-term healthy dynamics. When we prioritize the happiness of our customers and an equitable outcome for all parties involved, there is so much that we can accomplish in tech that will drive forward innovation and solve the problems of today and tomorrow.", "date": "2020-02-03"},
{"website": "Gusto", "title": "Onboard New Engineers #withGusto", "author": ["Brittney Johnson", "Raymond Luong"], "link": "https://engineering.gusto.com/onboards-new-engineers-withgusto/", "abstract": "How to Onboard Your Own Engineering Teammates Hurray! That engineer that you interviewed a few weeks ago just signed and your manager asked you to help onboard them. You’re not the new hire’s manager, but you do know what it’s like to be an IC (individual contributor) on the team, and your day-to-day work will be greatly impacted by how quickly your new teammate gets up-to-speed. Let’s make it happen! What is an onboarding buddy? At Gusto, we choose a peer to help the new engineer learn more about the team, codebase, and culture. It’s beneficial to have an onboarding buddy who isn’t their manager because as a peer you can feel more accessible. You’ll also be able to keep the manager accountable by knowing what your new buddy needs day-to-day and prompting their manager to do anything they may have forgotten.. Most importantly, you’ll help make the implicit explicit by answering questions and sharing information about how things work that isn’t currently documented. Bonus points if the onboarding process helps produce more documentation! Before they join Onboarding Document Work with the manager to make an onboarding document specific to them. Company-wide templates can be a great starting point, but they may present a lot of high-level information that can be overwhelming initially. Consider starting the document with the most critical day-to-day information that’s specific to the team or product area, then include some of the company-wide information. For example, at Gusto some team-specific documents that we include are: An org chart that shows all the teammates and the structure of the team Team project planning documents Quarterly OKRs The product roadmap A working agreement that defines norms for how the team works together Starter Tasks Identify small tasks to help the new engineer dip their toes in the code. Even if the task itself is fairly simple, the entire process of making a code change, going through code review, merging it, and deploying the change differs between companies and it’s valuable for the new engineer to learn the overall process around shipping code. Over time, these starter tasks should begin to become larger in scope. When looking for starter tasks, great candidates are tasks that aren’t critically urgent to the product but important enough to provide real value. For example, you might ask them to make a UI change that’s bothered you but you’ve never gotten around to doing, or add a new column to a table with additional information that users have been asking for but that never quite got prioritized. Once they’ve handled a few starter tasks, it’s time to ramp them up into a starter project. This project should probably take around one to two months, to ensure that it’s significant enough for the new hire to feel like they really have ownership and expertise around at least a part of the codebase. They shouldn’t work on it alone! Ideally, another experienced engineer will be pairing with them on the project and is equally responsible for getting the work done well. This ensures that the new hire always has a knowledgeable resource for their project, and they never feel like they need to “bother” someone because their teammate is working on the same thing they are. The starter project might be something that their manager might choose for them, but bringing suggestions and considerations to the table will help make sure that the right project gets chosen. Training Depending on the new engineer’s experience level and background, it can be beneficial to provide additional resources to help them learn the stack and set them up for success. For example, at Gusto we have a bootcamp that university hires (interns and new grads) attend together when they first join where we teach them the basics of React and Ruby on Rails. We also have occasional sessions open to all new hires about security, infrastructure, and the architecture of some of our most critical codebases. First week Intros They’re finally here! Learn what the company’s general onboarding schedule is like and set aside time to find them afterward to show them to their desk and introduce them to other folks on the team. Consider doing something more fun like a welcome lunch or a team event. Set up a one-on-one schedule so there’s a plan to check in on a regular basis. If they’re interested, also set up a regular pair programming session to help immerse them into the codebase. Depending on the new hire’s personality, it can also be helpful to facilitate intros, lunches, or coffee with other teammates, including cross-functional partners. Set Clear Expectations What is expected of them in the first week, month, year? How long does it typically take for engineers to be ramped up? What does it even mean to be “ramped up”? Their manager is in charge of most of this expectation setting. However, you can assist in helping reassure them that onboarding is indeed difficult but the fun learning opportunities lie more in the journey than the destination. Expectations can also depend on their background and experience level. For example, new grads might be nervous about meeting expectations in their first full-time job, so communicate that they're expected to spend a sizable portion of their first few months ramping up and learning industry practices. Remote folks might be wary of over-communicating or bothering with Slack, so make sure they feel empowered to stay in communication with the team. Engineers with substantial industry experience may not be familiar with the business domain, so help them calibrate their expectations for their own ramp time. Other folks might have responsibilities outside of engineering, so work with their manager to identify those responsibilities and figure out what their modified engineering capacity might look like. Dev environment Work together to set up their development environment if one is not already provided. Companies typically have a setup document or script with steps to follow. It’s typically a living artifact, so encourage them to document anything that’s different or new to make it easier for the next new engineer. First month Starter Tasks Continue helping them work on starter tasks or projects and answer any questions. If they are interested, pair regularly and encourage them to start driving the pairing sessions. Watching them try to navigate the product or codebase and seeing where they stumble is a great opportunity for you to learn the fuzzy areas that you’ve become accustomed to and figure out if there are ways to simplify. Set up shadowing sessions Depending on the company size, engineers can be a bit removed from the business or customer experience side. If possible, set up shadowing sessions with cross-functional partners to build empathy and learn more about the business. Whiteboarding As they become more immersed in the code, consider having them lead a whiteboarding session where they map out their understanding of a part of the domain and more tenured engineers can help fill in the gaps. This process can help them reflect and realize how much they’ve picked up in a few weeks! Another benefit is that it’ll produce documentation for others. On-Call If applicable, share more information about on-call processes and rotations, and pair together on solving issues. This is a great way to gain breadth and depth quickly because the issues are likely in a different product area than their main work. Additionally, this is a great way to build empathy for the users as well as the operations folks that are working with the users. Always Encourage questions Every question is a learning opportunity. Remind them that there are no dumb questions and that asking a question will then in turn enable them to answer that question for someone else down the line. Questions can also help identify confusing parts of the code or gaps in processes and lead to efforts to tidy up or improve the processes. Be Inclusive Consider asking the new hire what name and pronouns they use, or suggest that they include that information in their Slack profile. If they are remote or come from an underrepresented group, be mindful of making them comfortable and heard in meetings, especially in larger meetings where there are unfamiliar folks. Share information about existing affinity or interest groups to encourage them to find a community! Seek out feedback New folks have a superpower. They can more easily spot broken things that more tenured folks have become used to. Maybe it’s an overly complex process, an oddly-behaving feature, or a confusing part of the codebase. Check in regularly and encourage them to share ideas for how to improve. Celebrate their work! Onboarding is difficult — there’s a lot of new processes, concepts, and faces to learn. Celebrate their accomplishments, no matter how big or small. For example, share when they’ve landed their first PR with the rest of your team! Keep notes on technical things they’ve done as well as non-technical impacts they’ve made, and share those regularly. New engineers can bring insight to a team that didn’t have it before, and it’s nice to be appreciated for this that come naturally while you’re still in the process of ramping up on the specific technical work. That’s all! Just kidding. A lot more goes into fully integrating a new person into a team! There’s tons to learn and, if things go well, the new person will also end up teaching you as much as you teach them. These are suggestions based on personal experience from helping to onboard many of our own teammates, but ultimately you are the buddy. Your job is to support the new hire and their manager in making sure that the new hire can contribute to your team in ways great and small as soon as possible, and that they feel empowered enough to keep doing that for years to come. Do that in whatever way feels most authentic to you. Go forth and onboard #withGusto!", "date": "2020-03-23"},
{"website": "Gusto", "title": "Gusto's Engineering Principles and Values", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/our-engineering-values-and-principles/", "abstract": "Note: This is the first post of a 2-part series. As with any tech startup just getting off the ground, Gusto started with a very small engineering team in 2012. Communication overhead was low and very little got in the way of us writing code and building our initial set of features. We moved quickly and built a functioning payroll backend at a blistering pace. But as our company grew, so did our code base and the complexity of problems we faced. Instead of pumping out new payroll features everyday, we found ourselves figuring out how to scale to thousands of businesses, finding ways to catch fraudsters, getting a growing number of back-end services to talk to each other in a developer-friendly way, and catching all the unforeseen edge-cases that happen when you move billions of dollars through an antiquated payment system . At Gusto, each engineer is treated as an owner of the features they build and getting lots of input from teammates is important to us. But even with our collaborative team culture, as our team grew communication inevitably became more difficult and consensus became harder to come by. Furthermore, our user base grew beyond just business owners to also include accountants, employees, contractors, and even internal teams. We found ourselves being pulled in different directions and it was not always clear how to best prioritize our time. While none of these issues were hair-on-fire type problems yet, we knew they would continue to grow. At Gusto, we love experimenting with things that can make us work better, so we decided  to try out ideas to proactively curtail the challenges of scaling our team and product. One of the things we’ve done is write down a set of shared, fundamental principles to help us navigate the complexity. Just as all the theories of Euclidian geometry are based on a small set of simple axioms, we wanted our engineering principles to be the set of \"truths\" we rely on to help guide us through tough problems and decisions. After many iterations, we came up with the “Gusto Engineering Principles and Values.” It’s a framework that we can turn to whenever we’re faced with a difficult decision, whether it be about how to best architect something, how to prioritize our time, who to hire, or how we interact with each other. As the Gusto Engineering Principles and Values began to take shape, it evolved into something more than just a framework for making better decisions. It also turned into a document that reflects who we are and who we aspire to be in the future. In the next few weeks I'll write part 2 of this post, where I'll share the process we went through to come up with this document, starting with our small retreat! Download the Gusto Engineering Principles and Values pdf. Comments on Hacker News .", "date": "2015-06-01"},
{"website": "Gusto", "title": "Hello, New York! Launching Gusto’s Newest Homebase", "author": ["Lindsey Whitley", "Stephanie Sara Chong"], "link": "https://engineering.gusto.com/hello-new-york-launching-gustos-newest-homebase/", "abstract": "Day one in The Big Apple 🍎 Exactly six months ago, in September 2019, Gusto became a three-homebase company by opening a New York office in Lower Manhattan. This is our first office to focus primarily on Engineering, Product, and Design, or “EPD” as we call it around here. There's a sense that we're reaching a new level of maturity as a technical organization. Our goal with opening an office in NYC is to further support our growing, diversifying customer base by unblocking technical hiring. But when thinking about scaling, it's important we balance the tensions of moving fast with taking time to set a foundation. Our strategy must be equal parts ambitious and intentional. As two engineers who moved to New York as part of the landing team – nice to meet you! – we’d like to take a moment to share about the principles behind the office launch and what’s ahead. It’s a marathon, not a sprint Let’s be clear. What we’re doing in New York is ultimately about high growth. The paradox is that moving fast, in a sustainable way, depends on having a solid foundation. In an industry dominated by “ move fast and break things ,” some see slowing down as risky. Long-held tech wisdom says that raw speed gives startups the winning advantage. But scaling is a marathon, not a sprint. It doesn’t matter how quickly we hire if new hires aren’t aligned with our mission. It doesn’t matter how quickly we ship if features don’t get used. That kind of myopic thinking only fails our customers. At Gusto, doing what’s right means putting our customers first. That’s why these early months in NYC are so crucial. We must, above all else, lay a foundation that we are proud of, and that sets us up for sustainable success. So much room for activities! Designing teams for autonomy When Gusto opened our second homebase in Denver in 2015 , the office was focused on customer-facing teams and did not initially include EPD. Once we decided to start a Denver engineering team, the lack of an EPD presence made supporting new hires difficult because the technical expertise still sat in SF. In New York, we’re learning from these lessons of the past. From the start, leadership wanted the new office to include at least one completely autonomous EPD pod. That means a product manager and a designer, in addition to a full force of 3-5 engineers and an engineering manager. In doing so, we’re optimizing for stability over speed. Autonomous teams can ideate, execute, set direction, and deliver product strategy in connection to company goals. Riding the NYC subway == being autonomous Ultimately, the Employee Lifecycle team, or EEL, was chosen to be the first EPD pod in NYC. That decision was based on EEL’s desire to grow headcount and their ability to operate independently from the other offices. Comparatively, Benefits Engineering was not a good contender because of their need to be co-located with their Operations teams, who work closely with engineering and our customers. This has proven to be a good decision. Our NYC EEL pod launched their first feature last month – a brand new flow for employers to dismiss employees – with almost everyone involved based in NYC. A Gusto homebase would not be complete without other key roles, such as senior technical leaders and members of our People Operations and Recruiting teams. In addition, we have begun to seed other critical engineering pods, including Developer Experience, Infrastructure, and Payroll. Selecting individuals to fit the org design With an initial org design in place, the next piece of the puzzle involved choosing individuals. This order of operations was deliberate. It puts organizational business needs first and fits individuals into prescribed roles. To launch New York, leadership decided to send Gusties, or folks who work at Gusto, from both the SF and Denver offices, ensuring a breadth of experiences and contributions. Twelve of us made the move: eleven from SF and one from Denver. Since EEL’s priority was to form a complete team, specific individuals were selected for their domain knowledge, while some were asked to join the team to bring new perspectives. Other members of the landing team were selected more for their unique experiences. It’s crucial to have engineers with tenure who can serve as resources to fellow NYC engineers and form a complete interview circuit. We found that in Bo and Matan, two Payroll engineers, who have both worked at Gusto for 5+ years. When Josh, our CEO, came to visit our new homebase around Halloween, we “cooked” up a storm with his family and our costumes. The People team supports everything non-code-related. That includes Recruiting, since hiring is a big part of why we opened this office, and People Operations, to support the rest of us in our daily work and help us scale. To round out the landing team, we hired a handful of new Gusties – most of whom already lived in New York – to start with us on day one. They bring experience from recent jobs and help us New York-ify our Gusto culture. Each member of the landing team was selected to fill a specific need. Finnley was selected for her unique cuteness. Taking time to norm and form Starting a new office is rife with change. Pretty much all of us on the landing team can agree: our lives today are quite unlike what we could have imagined six months ago. Some of us moved across the country (one even did the 3,000-mile drive by himself), a few switched teams, and others are new to Gusto altogether. It’s that much more important that we allow these first few months to serve as a critical period for norming and forming . We make a point of regularly visiting New York Gusto customers, aka “Gustomers” (and enjoying their delicious food!). Newness saturates our days, so every little thing adds up. For new Gusties, that’s even more exaggerated. Gusto is an incredibly complex product (taxes and filings and money, oh my!). As a new product manager or designer, getting up to speed on the nuanced problems facing our customers requires non-trivial effort. Engineers need to build up context on our codebase, learn the design principles that govern our work, and adapt to the plethora of technologies we use day-to-day like Rails or Docker. It also takes time for teams to adjust. On the EEL pod, for example, most folks were new to Gusto, new to NYC, or both. The team has had to be intentional to draft working norms from scratch, taking into account everyone’s previous backgrounds and ideas. Their first project might have taken longer than expected, but that time is an investment in the team itself, not just the project. Newly distributed teams, like Developer Experience, have had to learn how to work with teammates in other timezones and communicate through video as a default. And of course, as a brand new office, we’ve needed time to come together. We’ve had to work hard to minimize unnecessary distractions since we’re still only 20 people. As an example, up until recently, candidates applying for New York were flown to SF or Denver for their onsite interviews. Once hired, they returned to those other offices to attend their first week of onboarding. Eventually, New York will have these capacities and more. For now, our office carries only the essentials, to minimize scope and allow us to fulfill our charter as a technically-focused office. You can take a Gustie out of San Francisco, but you can’t take the San Francisco out of a Gustie. Each week we drop off our compost at the farmer's market. Serendipity in a small office In Gusto’s San Francisco and Denver offices, there are more engineers, more technical teams, and a lot more people. As a result, Gusties tend to sit near other Gusties who work on similar tasks. Most of the time, that’s great for productivity. If you need quiet, the people around you probably do, too. If you have a question, there’s likely someone nearby who can answer or help figure it out. Here in New York, we can see everyone in the office by standing up. Since engineering sits in a cluster, DEx engineers are a more available resource to the other engineers. When a product engineer finds something broken, there’s no Slack channel needed; it’s as simple as taking off your headphones and asking a question. While it doesn’t seem like physical distance should make a difference, we’ve already seen that shorter feedback loops and facetime can ease the overhead of problem-solving. Green Gusties NYC, represent! When half the office goes to a climate march down the street. In one of our bi-weekly NYC retros, a couple of engineers expressed a desire for more cross-team pairing. Since we’re in a smaller office, it’s easy to make this happen. When the EEL pod recently needed to make changes to payroll code, pairing with Payroll Engineering helped to decrease the risk of introducing a breaking change. Pausing to breathe What better way to set ourselves up for success than to ~ literally ~ slow ~ down? Every Wednesday before lunch, a group of us meets in our largest conference room, adorably named The Bodega, to meditate together. We’ll play a guided meditation or listen to raindrop sounds on our phones. It’s only about 10 minutes, but it’s not the duration that matters – it’s simply taking a pause in the middle of the week. We remind ourselves that to get things done, sometimes we need to take a break. What now? We celebrate each monthiversary with a “baby blocks” photo. Here’s us today at six months! Change is the only constant at Gusto – and that’s especially true here in New York. Our tiny but mighty office will continue to expand as we actively hire for Payroll, Infrastructure, and Security. By the end of this year, we’ll have moved into a long-term office space that will, eventually, be homebase for several hundred Gusties. We did mention it’s a marathon, right? We’ve built up a lot of momentum in the last six months, but this is just the beginning. If you want to be part of the excitement, drop us a note on our Careers page .", "date": "2020-03-09"},
{"website": "Gusto", "title": "Chipping Away at a Monolith", "author": ["Tori Huang"], "link": "https://engineering.gusto.com/chipping-away-at-a-monolith/", "abstract": "My team recently embarked on a journey toward unbundling our part of Gusto’s monolithic Ruby on Rails app. A monolithic app is a single application that contains code across many domains. The boundaries are unclear. The domains are fuzzy. Typically, as a monolithic app grows, it becomes increasingly difficult for developers to make changes due to unexpected consequences. This is especially true for Ruby on Rails applications, as the ✨ Rails magic ✨ encourages side effects, indistinct domains, and generally spaghetti code . Gusto has a monolith problem. This is a good problem! It means we have been lucky enough to grow, build new features for our customers, and expand our abilities. However, for software engineers it slows down the development process as the tangled code results in unexpected bugs or overcautious programming. To help solve this problem, Gusto decided it was time to start unbundling the monolith. My team, Partners Engineering, builds the experience for accountants within Gusto. Our domain is so distinct from the rest of Gusto that there is potential to eventually separate entirely from the core Gusto app. We could build a separate app with its own entities, database, and API, communicating with the rest of Gusto but living separately from the monolith. We embarked on this project with this vision in mind - a completely separated Partners app with clearly defined boundaries. Where to start Understand your domain The first step in separating from a monolith is understanding your domain. My entire team was relatively new to Gusto, meaning we had a lot to learn about our own domain. If you’ve been with the codebase from the beginning, this might look different, but chances are there’s some interaction with another area of the code that you do not know as well. For several months, my team met weekly to discuss Domain-Driven Design and how our domain fits into it. Spreading these discussions out over a larger period of time gave the team space to discover more in our codebase, read more about different architectures, and get input from other teams. Over the course of several weeks we began to see a clear picture of an ideal Partners domain. Start small Deciding where to start felt like a daunting task. There were a lot of directions we could go. Should we begin with something small or should we gravitate towards the area that could have the biggest impact? Should we tidy up our code first or jump right in to defining boundaries between Partners and external groups? I was tasked with finding a starting point for my team. I started by creating a list of models that belong in the Partners domain. I was searching for a complex model that should undeniably reside within Partners. One model in particular rose to the top as core to our future domain - Accounting Firm. Accounting Firm is a key concept within our domain. It is heavily bundled with external domains, but not in a way that would be impossible to separate. It would be the perfect place to start unbundling and defining a Partners boundary. With this new Accounting Firm focus, I dove into a deeper audit of the model. I paid particular attention to concerns, callbacks, relationships, and methods that tied Accounting Firm to another Gusto model - Company. Company is a god model within Gusto. It belongs within the core Gusto domain as a model that Accounting Firm should never access directly. The results of this in depth analysis of the Accounting Firm model were overwhelming. By just taking time to review one model I created several epics to clean up the model. However, we needed to focus on one, achievable area where we could make an improvement. We decided to start by removing one method - accounting_firm.companies . This method was the perfect place to start because it: Reaches across multiple boundaries Is a crucial method for accounting firm Could be removed without help from external teams Would help us start defining Partners domain boundaries Tidy up While tidying up our code will not help us define domain boundaries, or unbundle the monolith, it can help us prepare to take these larger steps. It is an incredibly helpful practice that allows us to pave the way for bigger changes. During my investigation of Accounting Firm, I found a questionable area of code. In it there was orphaned code. There were strange relationships that resulted in two sources of truth. There was a test factory that created inaccurate duplicate data. While untidy code is not directly related to unbundling, it makes the unbundling efforts more difficult. I started unbundling with this basic cleanup. I removed the orphaned code. I spent several days fixing the bug in our tests that created duplicate data. It was not possible to tidy everything, but I tidied as much as I could. This paved a clear path for us to begin truly unbundling. Problems you may encounter We wanted two minds on this unbundling problem. So, my teammate Grex and I decided to pair full time for two weeks. Armed with our ownership mentality and the ability to approve our work as a team, we jumped into our unbundling epic. We encountered a few obstacles when tackling this unbundling project. When you are working within a system that is completely intertwined, without clear boundaries, it is impossible to predict every issue you will find. Here are a few of the stumbling blocks that Grex and I came across while working on this project. Crossing into other domains Our work frequently took us outside of our domain into other areas of our code - with and without clear code owners. We had to give ourselves permission to make decisions in these other domains. For example, we wanted to create a service to retrieve Companies. This service was generic enough that it could potentially be useful to other teams, especially teams working on similar unbundling projects. What should this service look like? Where should it go? Without a clear owner of Company, we had to give ourselves permission to make the best decisions with the knowledge we had. Without the power to make these fast decisions, our project might have taken six weeks instead of two. Besides, we could always make changes later! Shaving the yak When almost everything you see could be improved, how do you stay focused on the task at hand? Our team lead Matt calls this “ shaving the yak ” - also known as going down a rabbit hole. For example, Grex and I encountered many examples of the reverse method `company.accounting_firm`, which is also on our unbundling chopping block. We could potentially reuse some of the use cases we built for this purpose. Would it hurt to knock a few out while we were working on this anyway? Yes, yes it would. This distraction would surely have led us to further distractions. When unbundling, it is important to have a clear goal in mind and stick with that goal even if other opportunities for unbundling arise. Instead of becoming distracted, we wrote down ideas for areas to return to later and continued on our mission to eliminate accounting_firm.companies . Unbundling tests When estimating unbundling time, do not forget about tests. Many of our tests utilized accounting_firm.companies . Some of our tests even stubbed out this method to control what was returned. Once we removed the companies method, these tests started to fail. Fixing the tests took us a surprising amount of time, at least 50% of our pairing over the two weeks. Do not assume unbundling tests will be a fast process - it will probably take some time. Fear of breaking things Two weeks and many planning sessions later, Grex and I had replaced all instances of accounting_firm.companies with ten new service classes. These new services did a variety of things, from simply returning the ids of companies related to the firm to returning filtered and refined company data. We were theoretically ready to remove this method entirely, cementing the new boundary we had drawn between Accounting Firm and Company. There was one problem - we were scared. This had been such a crucial method, what if there were some uses that we had not caught with our text searches? Some critical function still using accounting_firm.companies that would cause some fatal bug that our tests did not catch? We decided to remove #companies in baby steps. First, we added a comment deprecating the method. Second, we added some bug tracking to determine if anyone was indeed secretly using this method. After 24 hours without any reports, we felt confident that this method had truly been eliminated. We deleted accounting_firm.companies . The harsh truth is, when unbundling there is a good chance you are going to break things. The best you can do is move slowly and cautiously, adding tests or logs where they were missing before to catch any bugs before they make it to production. What next? When you are working inside a monolith, unbundling can feel like an insurmountable task. This initial project was our first small step towards unbundling our domain. For the next several years, my team will likely be chipping away at our monolith. From this initial project, my team has created several epics and has an idea of where to move next. We have a better understanding of our domain and a little practice under our belt. When it comes to unbundling, figuring out where to start can be the hardest part. Put in the work to understand your domain and codebase. Then just pick somewhere you think is useful, and get going. It is an impossible problem to solve all at once. Chipping away at the monolith bit by bit is how it will slowly become untangled.", "date": "2020-07-22"},
{"website": "Gusto", "title": "Build What Matters", "author": ["Tori Huang"], "link": "https://engineering.gusto.com/build-what-matters-part-1/", "abstract": "Defining an outcome-driven experiment At Gusto, Engineering has an ongoing discussion about how to quickly ship meaningful features to our customers. I am part of the Partners Engineering team here, building our product for accountants and bookkeepers. As a team, we want to work on the right things for our customers. We want to feel confident that features will be impactful when building something new. My team decided to take a look at our process. We broke down a typical feature planning timeline and found that we typically spent three to five months from start to finish. This is a significant amount of time to spend on a feature, especially when we are uncertain how it will be received by our users. In an ideal world, my team would ship smaller features to users more quickly. Doing so would allow us to gather immediate real world user feedback and take smaller steps to correct our course of action. To move toward this ideal world, we wanted to experiment - particularly with delivering features that were built around outcome-based goals. With this mentality, I embarked on a mission with fellow Gusto engineer Alyssa Hester to find and complete an experiment solely focused on customer outcomes. In this first post of this series, we’ll talk about how Alyssa and I found an outcome-based experiment and defined a realistic minimum lovable product (MLP). In the second post, we’ll take a look at how we brought the experiment to life, building from a hardcoded feature to a fully scalable product. Focus on outcomes over output What do I mean when I say outcome-based experiment? Output is the stuff you build. For example, if a team creates a new reporting feature to generate bulk reports, then your team’s output is the reporting tool itself. This could include the designs, the code, and anything else the team produced as part of this new feature. Outcomes sometimes grow from output, but they are an entirely separate concept. Outcomes are changes in metrics and customer behavior - basically how customers respond to what we build - these are your team’s true goals. We focus on this because it tells us if customers truly find value in what we build. For example, outcomes for a new reporting tool could include increased retention, an improvement in customer satisfaction score, or an influx of customer referrals. Is your team’s actual objective to create a reporting tool or to bring more customers to your platform? The true focus of our work should be outcomes . The first step for our project was defining the desired outcomes. With our team’s mission in mind, to help accountants and bookkeepers who I will refer to as Gusto Partners, Alyssa and I decided our goal was to create a feature that would increase the number of clients our accountants add to Gusto. We wanted to find a project that would help both Gusto and our Partners grow. We started searching for a project based on this outcome, with the intention of defining more specific metrics once a project was selected. Learn about the customer The engineers on our team, myself included, wanted to have a more detailed understanding of our customer. To identify a project that would increase accountant client adds, we needed to understand what motivates accountants to add clients. In order to gain a better understanding of our customer, we relied on our customer-facing teams. I shadowed both our customer support and sales teams that specialize in working with our accountants. I was blown away by the level of knowledge they retain about each of their clients. Each sales team member could recite, from memory, a wish list of potential features for each of their clients. This experience provided me with some amazing insight into our accountants’ interactions with Gusto. Pick an outcome-based project After learning more about the customer, we sat down to discuss potential projects. Several ideas were turned away for being too focused on output, instead of outcomes. Some ideas were based around outcomes, but not accountant-based outcomes. Our Head of Accountants and API Engineering, Jeremy Thomas, came up with the final project idea - building a way to connect small businesses on Gusto that don’t currently work with an accountant or bookkeeper with our Gusto Partners. We have a large pool of trusted accounting firms who are always looking for new clients, and thankfully, Gusto is in a position to help them with that. At Gusto, we serve more than 100,000 business owners. From our research, we know that these businesses are 80% more likely to be in business in five years if they are working directly with an accountant. By capitalizing on our ability to recommend these trusted accounting firms to small businesses who would benefit from their services, we can help both our small business customers and partners achieve their desired business outcomes — a win win! We decided to bridge the gap between the two parties with a recommendation engine that would generate custom accounting firm recommendations for our small business customers. In terms of outcomes, our hypothesis was that: Bringing more clients to our Gusto Partners would benefit our Gusto small businesses. Bringing more clients to our Gusto Partners would encourage them to bring more small business clients to Gusto. With a project directive in hand, it was time to get to work. As we started to dig in, it became clear that the scope of this project could grow much larger than the four weeks we had left. Design alone could take several weeks. Meanwhile, the list of potential features was growing larger every day. When evaluating new features, we believed many would improve the success of our project, but we almost never had the time to build them. As a team, we needed to shift our mindset in order to launch an experiment within a reasonable timeframe. We needed to move away from thinking this experiment needed to be perfect before launch. We needed to throw away our perfection mentality. Escaping our perfection mentality As Gusto has grown, so has the impact of every new feature we ship. New features are instantly available to users. Every Gusto engineer has immediate access to the new code merged into our mainline branch. With so many eyes on every new change, it is easy to believe that every update or code release needs to be perfect. Having a perfection mentality means that a feature is not good enough until it is absolutely flawless. Here are a few things “perfection” could mean in software: Design that has been fully user-tested Code that solves for every potential edge case Software that is fully scalable Buy-in for this new feature from every department At face value, these all seem like very valuable qualities to have in a project about to launch. Of course, designs that have been user tested are more likely to be successful. As software engineers, we want to solve for the edge cases. We want our code to be scalable. We want to ensure the work we are doing will not have any negative impact on other projects at the company. However, these benefits come at a cost - an actual dollar value cost of time spent by our team and a lost benefit cost to our customers. Product managers, engineers, and designers are spending time perfecting these features when we could be launching a more simplified project and collecting quick feedback from real users. In other words, we could be extreme in shortening our iteration cycles. Users could be benefiting from a simplified version of this feature sooner, instead of waiting for something “perfect”. With four weeks left to ship, we did not have time to fall into this perfectionist mindset. We needed to define a project that would benefit our customers and that we could realistically ship within the given timeframe. Defining the most basic MLP We brought on a designer, data scientist Becca Carter, and product manager Ankita Verma to help us draw up the simplest minimum lovable product (MLP) possible. This is what we abandoned along with our perfection mentality: Design that was tested by real users Dynamically creating recommendations A user-tested recommendation algorithm that would provide the “perfect” recommended accounting firm to each company Any additional features Buy-in from marketing, sales, and business departments before starting work With our focus on getting this feature out within a reasonable time frame, this is what defined our MLP: A design we sketched out on a whiteboard in one hour A plan for the data science team to select several thousand small business owners for whom they could “manually” generate accounting firm recommendations A recommendation algorithm based on our best-guess of what companies care about when looking for an accounting firm NO additional features, just make the recommendation A weekly project update via email to keep any interested parties up to date on our progress The point of this project was to determine if a recommendation engine could encourage Gusto Partners to bring more clients to our platform while also helping our Gusto businesses find accountants. We wanted to answer the question - should we continue to invest time in this feature? We decided to launch something heavily manual that would provide us with enough data to determine whether we should build a fully scalable recommendation engine. Get Started We finally had our project, and it was time to get to work! Alyssa and I were given six weeks to complete this initial experiment and we had already spent three of those weeks working to identify and define the project. Now that we had an MLP, we needed to start building. In the final part of this series, we will walk through the process of taking an experimental feature from MLP to a full-fledged product.", "date": "2020-09-24"},
{"website": "Gusto", "title": "From Legislation to Code: How Gusto helped customers maximize their PPP Loan Forgiveness", "author": ["Quan Nguyen"], "link": "https://engineering.gusto.com/from-legislation-to-code-how-gusto-helped-customers-maximize-their-ppp-loan-forgiveness/", "abstract": "photo by https://www.pexels.com/@cqf-avocat-188397 Over the past few months, multiple teams at Gusto have been rallying behind initiatives that help businesses navigate the uncertainties brought by the Coronavirus pandemic. Among these projects is the Paycheck Protection Program (PPP) loan forgiveness report. Through working on this report, we’ve come to respect the challenges involved in interpreting legislation, specifically the CARES Act and its ever-evolving guidances. In this post, we’d like to share how we translate legislation clauses into lines of code. We’ll dive into a few translation examples and discuss how we respond to changes to stay up to date with the latest guidances. What is the PPP loan forgiveness? As part of the CARES Act, the PPP loan is available to small and medium-size businesses (SMBs) for use toward eligible expenses, e.g. payrolls, rent, utilities, mortgage interests. At the end of the loan’s covered period, businesses can submit an application for forgiveness to waive up to the entire loan amount. The PPP loan is a huge lifeline critical to the survival of many SMBs impacted by the pandemic. However, many SMBs don’t realize the stringent criteria required to meet full forgiveness eligibility. At a high level, the loan can be used to cover eligible expenses during an applicable period following the loan disbursement. During this period, at least 60% of the loan principal must be used toward payrolls. Additionally, employers are required to maintain both employee headcount and wages relative to baseline periods set by the law. Reductions in either headcount or wages could result in a lower forgiveness amount. Gusto forgiveness report Our goal was to build a report that allows customers to monitor how they track toward full forgiveness. Since virtually all forgiveness criteria rely on payroll data, Gusto, as their payroll provider, is in the best position to help customers accurately calculate their forgiveness status. Additionally, to keep the tool simple, wherever there are multiple options in the calculation we’ll calculate all possible permutations and select ones that yield the most optimal outcome. While understanding the intricacies of our internal payroll system was central to this project, the most challenging aspect was the translation of legislation and interpreting ambiguous sections that come with it. We intentionally didn’t write any code in the first two days of the project. Instead, we dedicated our entire team’s effort to read and understand the Loan Forgiveness section of the CARES Act . We also held regular meetings with our internal legal and compliance team to help us navigate new concepts we didn’t initially grasp. Our team discovered that the bill is structured very similarly to how we, as developers, write code. Clauses in the bill often make references to other sections. You’d have to jump through multiple sections to derive a full understanding of a sentence. In software development, these external references are simply function calls. We are trained to suspend our reading flow to dive deeper into another part of code as we inspect a specific class or method. Let’s walk through a few examples to illustrate what we mean. The translation process In this section, we’ll walk through the first part of the forgiveness calculation, which deals with the headcount quotient calculation. This quotient measures the number of employees during the loan covered period compared to that of a baseline period. If an employer reduces their headcount during the covered period, their headcount quotient will be less, which results in a lower loan forgiveness amount. The very first clause in the forgiveness calculation reads AMOUNT MAY NOT EXCEED PRINCIPAL. — The amount of loan forgiveness under this section shall not exceed the principal amount of the financing made available under the applicable covered loan. At the onset, this tells us that the loan amount is a user input to the report. We don’t know yet how to calculate the forgiveness amount, but we can translate this sentence into code as [forgiveness_amount, loan_amount].min The next section goes into more detailed instruction on how to calculate the quotient (A) IN GENERAL. — The amount of loan forgiveness under this section shall be reduced, but not increased, by multiplying the amount described in subsection (b) by the quotient obtained by dividing — (i) the average number of full-time equivalent employees per month employed by the eligible recipient during the covered period; by (ii) (I) at the election of the borrower (aa) the average number of full-time equivalent employees per month employed by the eligible recipient during the period beginning on February 15, 2019 and ending on June 30, 2019; or (bb) the average number of full-time equivalent employees per month employed by the eligible recipient during the period beginning on January 1, 2020 and ending on February 29, 2020; or As you can see, there are multiple references to sections that are either previously mentioned or will be subsequently defined. Let’s dissect these statements one by one. Statement (A) instructs us to perform the multiplication below. You may notice it refers to the eligible loan amount described in a previous section and the quotient, which is defined in the next two subsections. forgiveness_amount = loan_amount * quotient We can also derive the headcount reduction, which is useful to present as a separate line item in the report summary headcount_reduction = loan_amount * (1 — quotient) In order to figure out the quotient, we first need to understand the calculation of the average full-time equivalent employees or FTEs. Lacking a formal definition specified in the CARES Act, we employed the Small Business Administration’s (SBA) definition of a full time employee in earlier legislations, which defines a full-time employee as someone who works 30+ hours per week. As an example, a company having one 40-hour/wk employee and three 15-hour/wk employees will have an FTE value of 2.5 Equipped with this knowledge, we can now write a helper method that calculates the total FTE as follow: const DAYS_IN_WEEK = 7\nconst FULLTIME_THRESHOLD = 30\n\ndef calculate_fte(period)\n  period_in_weeks = Integer(period.end - period.begin + 1.day) / DAYS_IN_WEEK\n  total_fte = employees.sum do |employee|\n    avg_hrs_per_week = work_hours_for(employee, period) / period_in_weeks\n    # Ensures that each employee can only equate to 1 max FTE.\n    [avg_hrs_per_week / FULLTIME_THRESHOLD, 1].min\n  end\nend This is a great example that illustrates how ambiguity in legislation can lead to very different interpretations. As we shall see, the SBA’s subsequent guidance introduces two different approaches to calculate the FTE, both of which differ slightly from our initial interpretation. Fortunately, we can simply swap out the above helper to support the SBA’s new approach. The quotient can now be obtained by dividing the FTEs in the covered period (i) by the baseline FTEs in a selected lookback period (ii). We'll need to prevent division by 0 for companies that do not have FTEs during the lookback period. def quotient\n  calculate_fte(covered_period) / calculate_fte(lookback_period)\nend The covered period is dependent on the loan disbursement date, which we require users to enter in order to generate their report. The covered period’s duration was originally specified as the 8 weeks following the loan disbursement in the CARES Act. It was subsequently updated to 24 weeks by the PPP Flexibility Act. For the lookback period, employers have a few options depending on whether their business is seasonal or not. Rather than requiring users to make this selection, we calculate all possible options and automatically choose the most optimal that produces the best forgiveness outcome. We hope that the above translation steps give you a better understanding of our process. The complete forgiveness calculation includes two additional components, wages reduction and payroll cost, which we won’t cover in this post. Responding to evolving guidances Since launch, our forgiveness report has been used by over 30,000 companies, cumulatively tracking over $2.4 billion in PPP loan. We worked hard to ensure that our customers had the forgiveness report as early as possible, and vetted our report with dozens of lenders to ensure that it was consistent with how the lenders, who are responsible for processing millions of forgiveness applications, were interpreting the SBA and Treasury requirements. We also pulled together a working group of over 75 companies across the payroll industry, lending industry, and accounting community to ensure that our industries were aligned in providing useful forgiveness reports for small businesses Over the past five months, the SBA and Treasury have released more than 15 interim final rules, FAQs, and interpretations of the CARES Act. An example of such changes is the SBA guideline for filing the loan forgiveness application released on May 15th, 2020. This guideline deviated significantly from instructions laid out in the CARES Act in several key areas. In terms of FTE calculations, the SBA introduced two new methods, which we’ll refer to as simplified and granular. The simplified method assigns an FTE of 1.0 for all employees who work 40 hours or more per week and 0.5 for those who work fewer hours. The granular method is similar to our previous implementation but with a full-time threshold of 40 hours/week instead of 30 hours/week. Supporting more options means that we have to double our calculations to cover all possible permutations. However, the gain in simplifying the customer experience still outweighs the time added to perform the extra calculations. In general, we found that the simplified method tends to work well for employers who have mostly part-time employees whereas the granular works best for those who employ mostly full-time employees. The new guidance also introduced a Safe Harbor clause, which clarifies details about the rehire exemption mentioned in the CARES Act. The CARES Act alludes to the possibility for removing the headcount reduction if the employer is able to recover all their headcount by 6/30/2020. The new guidance provided more details, specifically the criteria for exemption eligibility and the baseline comparison periods. The biggest highlight of this SBA guidance was its resolution of a confusing clause in the CARES Act that requires employers to compare total wages in the 8-week covered period against those of a full quarter. Employers were understandably frustrated when they discovered that in order to attain full forgiveness they had to increase wages for applicable employees by approximately 20%. Under the new guidance, annual rates are used for salaried employees and hourly rates for hourly employees. This change effectively normalized the different lengths of the two periods. Therefore, wage reduction only kicks in once an employer actually reduces her team pay rates beyond 25%. From a product standpoint, the SBA guidance should be treated as a brand new report. However, our initial decision to organize the core calculations into three distinct modules, headcount, wages, and payroll cost, really pays off. It allowed us to retain most of the structure in the original code and gave us the flexibility to adapt minimal code changes to applicable modules. We released the modifications to support the SBA guidance shortly after its release. Multiple major changes were also introduced following the SBA guidance, including the Paycheck Protection Program Flexibility Act . This new legislation extended the cover period from 8 to 24 weeks and reduced the payroll threshold from 75% to 60%. We were able to quickly adapt and incrementally incorporate these updates into our report. While it was frustrating for us to deal with ambiguous and changing requirements throughout this project, we’ve gained a lot of respect for the challenges faced by SMBs during this time of uncertainty. It must be extremely difficult for employers to balance between keeping their businesses afloat while navigating changing guidances that directly affect their livelihood and those of their employees. It is a huge motivator for us to keep our report accurate with the latest guidances. Because removing this extra work on the customers' part means they’ll have more time to dedicate to their team and business needs. Thanks to Jeanette Quick, Ngan Pham, Jesse Zhou, and Ben Zhang for providing feedback on earlier drafts.", "date": "2020-08-27"},
{"website": "Gusto", "title": "Build What Matters, Part 2", "author": ["Tori Huang"], "link": "https://engineering.gusto.com/build-what-matters-part-2/", "abstract": "Hardcoded experiment to full-featured product I am part of the Partners Engineering team at Gusto, building our product for accountants and bookkeepers. Our team recently decided we could do more to work on experiment-based projects - putting out new features quickly and iterating on them based on customer feedback. We wanted to work on projects defined by outcomes. In part one of this series , we discussed how to identify an outcome-based experiment and define a realistic minimum lovable product (MLP) . In this final part of the series, we will take a closer look at how to actually bring an experiment to life as a full-fledged feature. If you think the hard work is complete once the project is defined, think again! Scope creep is a real and constant danger. Deciding how to iterate on an initial experiment can be easy - if you have the right data. Finally, there is some clean up to do! Fast experimentation can be messy and a responsible engineer should always strive to leave their code in a tidy state. Start with hardcoding My coworker Alyssa Hester and I were going to build a way to recommend our trusted accountant partners to Gusto small businesses. We wanted to create custom recommendations between these two parties based on their common characteristics. However, we needed to keep in mind, the goal of this experiment was to determine if we should move forward with building a recommendation engine. An ideal experiment would test a hypothesis with the minimum possible resources. This meant using a tool that is typically extremely frowned upon in a production environment - hardcoding. This experiment would be heavily hardcoded and purposefully not scalable. We would launch it to a select group of small businesses. For each of these small businesses, our data science team would use an algorithm to manually generate a recommended accountant partner. This was a starting point from which we could determine if it was worth the engineer time to build out a full-featured recommendation engine. With our hardcoded MLP, it was time to get to work and start building! However, as soon as we dove into the project we started to come up with new ideas that could potentially improve our outcomes. “No” is my new favorite word Throughout the project, our team came up with many wonderful ideas that would have undoubtedly increased the success of the project. There was only one problem - we didn’t have time to implement any of them. Every time we encountered a new idea that would improve our success, we needed to make a decision on whether to make time for a potential additional feature. To decide we asked ourselves, “Is there an easier way to do this while still bringing value to the customer? Do we need this new feature to understand if customers will use the project?” Some great ideas that we had to say “no” to: Idea: Providing more than one recommendation to the accounting firm Instead: We referred companies to our Partner Directory Idea: Providing detailed data on any connections made to our sales team via Salesforce Instead: CC’d our sales team on the initial email sent when making a connection between a company and a Partner Just because a realistic MLP has been defined does not mean the project is in the clear. Scope creep is a real danger that should be fought at all times. The ultimate goal of the project should be kept in sight - building a feature that would increase accountant client adds. Collect all the data The point of this experiment was to determine if we should invest in a full-featured recommendation engine. Experiments mean data. All of this work would have been pointless without collecting the necessary data to determine if future investment was worthwhile. We tracked every possible data point - views, clicks, connections. Anything that could be measured was measured! From this data we created a dashboard with all relevant information. It was this crucial information that allowed us to iterate on the experiment. Iterate on the experiment We launched our hardcoded experiment with a few days to spare! We sat back and waited for the results to roll in, tracking view and click through rates for companies viewing their recommended accounting firms. After a few months we determined that small businesses were very interested in viewing their custom recommendation for an accounting firm. However, based on performance data, the placement of our prompt to view this recommendation and the flow for connecting with the recommended accountant could be improved. We took these learnings and launched a second experiment, placing the recommendation prompt in a new location with an increased number of custom recommendations. It only took us a few days to build upon the existing experiment. With round two, the interest rate in recommendations improved further. After two rounds of experimentation and positive feedback from both small businesses and Gusto Partners, we decided it was worth the effort to build a fully scalable recommendation engine. We learned enough from each experiment to justify continued investment. After the second iteration, we decided it was time to create our full-fledged recommendation engine. Building the recommendation engine took significantly less effort than it would have without experimentation, as we were able to build on existing iterations. We are still waiting for data to bake on our recommendation engine in order to determine if we achieved the goals from our defined outcomes. In any case, we are regularly connecting small businesses on Gusto with our accountant partners, and initial results are promising! Clean up Engineers may be asking, with all this focus on outcomes and fast delivery, was there any impact to code quality? The answer is a hard yes. We purposefully decided to include non-scalable code into our codebase since this allowed us to build within the given timeline. While this was a necessary sacrifice, it certainly was not something I would want a coworker to git blame and attribute to my name a year from now. As an engineer, you are responsible for the cleanliness of your codebase. I would not consider this project complete until we cleaned up unused code after the experiment was turned off. After building the full-featured recommendation engine, Alyssa and I deleted unused code and ensured we left our codebase in a tidy state . Keep going with outcome-based experiments After going through two experiments and finally launching the full-featured recommendation engine, we are happy with the outcomes through this process. We spent minimal engineering, product, and design time shipping a minimum lovable product within only six weeks. Once the MLP was launched our customers in the experiment were able to benefit from these accounting firm recommendations within six weeks, instead of months. As an engineering team, we then iterated based on real customer feedback and data collected from our users. Finally, once we had gone through two experiment iterations, we were able to use what we learned and some existing code to speed up development of our full-featured recommendation engine. While we are still tracking final metrics for outcomes from our recommendation engine, the early signs are promising. Overall, this outcome-based experiment allowed us to help both our accountant partners and our small businesses. Our team hopes to run many more of these experiments in the future.", "date": "2020-10-23"},
{"website": "Gusto", "title": "Day in the Life of a Gusto Engineer: New Grad Edition", "author": ["Lindsey Whitley"], "link": "https://engineering.gusto.com/day-in-the-life-of-a-gusto-engineer-new-grad-edition/", "abstract": "It’s been two years since we gave the world a look at a “normal day” as a Gusto engineer . Since then the org has grown nearly three-fold and matured as an organization. So we decided to take another crack at it; this time, we’ve talked to engineers from different teams to give a wider view of life at Gusto. For this post, we specifically talked to engineers who are about a year out of college and who started working at Gusto right after graduating. They are at a critical stage in their careers, with lots of opportunity and hunger for growth, as well as lots of options about how to accomplish their goals. Olu Gbadebo (Benefits Engineering), Mariam Issa (Risk Engineering), and Sofia Carrillo (Accountants Engineering) gave us a peek into their work from the daily operations of being a Gustie to how they serve our customers and what sets Gusto apart from the crowd. Mariam Issa, a member of the Risk Engineering team Impact on Customers Each team has a unique opportunity to make our customers’ lives better. These new grads represent three distinct teams whose work is linked by its impact on the small businesses we serve. In these times when small businesses need us to deliver for them now more than ever, the outcomes of an engineer’s work at Gusto is even more pronounced and more motivating. Olu Gbadebo works on a pod within the Benefits Engineering team which, as he describes it “helps small businesses decide the best benefits for them by supporting the Customer Experience (CX) Advising team.” As early as his second week at Gusto, he was able to push code live. He works closely with CX to build new features, incorporate feedback, and fix reported bugs. The highest priority project he worked on made it possible for our customers to see multiple benefits options they could choose from, allowing them to better fit their health benefits to their employees and their business needs. Sofia Carrillo’s team builds features to save accountants time, so they can focus on advising and supporting their small business clients. She has spent the past six months improving the reports that accountants use to access their clients’ payroll and benefits information. Most recently, she built a new feature so accountants can download quarterly tax forms in bulk, separated into folders for each client, instead of having to painstakingly download each form individually. This is a huge time savings, particularly for accountants with a lot of clients: in one download, they have all the information they need every quarter. Sofia has also focused on making small UX changes—like making a client table searchable or adding a notification when a report is ready for download—that make a big difference to our accountant partners. Mariam Issa works on Gusto’s Risk Engineering team, which focuses on minimizing risk by detecting fraud and proactively preventing credit defaults to protect Gusto and our customers. Mariam helped redesign processes that allow us to offer faster payroll and contractor payment speed for customers. Following the typical ACH pattern, payroll needs to be run 4 days before employees are paid, but these tools help us speed up the process to 1 or 2 days. By increasing the speed of payrolls, we relieve cash flow stresses that small businesses often face. In the past year, she’s also built new tools and automated existing internal tools for Risk Operations, giving our customers faster service. Support As a new grad, having the right support and the safe space to fail, learn, and ask questions can make all the difference. Not only do engineers need to grow their technical skills early on, but they need to be constantly honing their “soft” skills too. All three new grads mentioned other engineers at Gusto as the biggest source of their technical learning and collaboration experience in the past year. Pairing as part of the onboarding and daily routine at Gusto help make the transition and learning process easy. Prior to joining Gusto, Olu had never developed with Ruby on Rails, but his team has helped the Ruby on Rails learning journey through pairing and sharing valuable resources. Mariam said, “The engineering community here at Gusto is incredibly supportive. You’ll find more often than not, engineers are willing to offer time to pair to resolve issues if you ask for the help.” Sofia was placed on a team with a lot of experienced engineers, and being the only junior engineer, she had the chance to pair with her teammates and learn from each of them. Gusto’s mentorship program, which is open to all engineers, also contributed to Sofia and Mariam’s success. The program matches engineers from different teams and varying experience levels. Instead of being focused on pairing or code reviews, like a teammate, a mentor offers a less technical perspective on career growth by checking in and offering more general guidance biweekly. Each engineer’s manager, or People Empowerer (PE) , was mentioned as especially valuable to the technical learning process, but also in the transition into full-time working life. “The biggest pillar of support,” Sofia said, “has been my managers, who are trained to focus on goals and growth as opposed to just making sure you just ‘get your work done’. Both of my PEs so far have been supportive and encouraging, while still challenging me to stretch myself and grow!” Olu Gbadebo, a recent graduate on Benefits Engineering Daily Operations Communication, especially in a distributed team environment, is essential to how our engineering teams operate. Olu’s pod on Benefits Engineering has a 15 minute stand-up every day to talk about what got done the previous day, what pull requests (PRs) need review, and what is coming up today. Some teams, like Sofia’s Accountants pod, sometimes meet on Zoom for standup, but mix in asynchronous standups in Slack. Aside from standups, we use several tools for communication about what work is being done and other logistics. Engineers at Gusto use email, JIRA, Slack, Zoom, and Github on a daily basis. An engineer can pick up a JIRA ticket to work on and request a pairing session on Slack with other engineers while working on the ticket. Depending on the size of the PR, engineers typically spend less time reviewing than writing code. Most PRs are small and digestible, usually requiring no more than 20 minutes per PR. In some cases, especially for larger pull requests, code reviews and pairing go hand-in-hand helping spread knowledge while speeding up the process. All engineers at Gusto spend portions of their day writing code, reviewing code, and pairing. However, for Olu and Mariam, the largest percentage of the day is spent solo coding with about 40% of that time devoted to testing. Sofia says that she spends the most time testing, but solo coding is next in line. All three of them said that pairing with other engineers happens regularly each week, with more pairing happening right when they joined Gusto. Olu added, “Every engineer on the team is always eager to pair.” Olu Sofia Mariam What team are you on? Benefits Accountants Risk What languages do you write? Ruby, Javascript Ruby, Typescript, Javascript Ruby, SQL What databases or data stores do you use? Postgres MySQL MySQL Which frameworks do you rely on? Rails, React Rails, React, GraphQL Rails Which tools are essential to your team’s process? Github, JIRA JIRA, GitHub, Slack Redash, Sidekiq, AWS How do you do standup? Daily Zooms Twice-a-week Zooms, otherwise via Slack Daily Zooms What sets Gusto apart? As a new grad starting a software engineering career, there are a lot of factors that can differentiate companies and experiences from each other. Some points can be easily measured like compensation, benefits, and location. Others, like the culture of the company and the day-to-day work itself, vary from perspective to perspective. Mariam values the fact that she was given the opportunity to work on high impact projects from the get-go. She loves knowing that she’s supported and challenged to consistently grow. Since her manager has made Mariam’s career goals a high priority, she has been able to keep doing impactful work. From Olu’s perspective the opportunities at Gusto to learn and grow as an engineer, especially for a new grad, distinguish it from other companies. He also said, “I’ve felt that Gusto is a great place to make an impact. In my first few weeks, I started launching features that helped the CX team.” Sofia said that the biggest separator for her was the community. There are a lot of engineering folks who really care about making our engineering community a collaborative and supportive environment. Since this is her first job out of college, she “wanted to make sure [she] was joining a company that was truly invested in growing and mentoring junior engineers.” Sofia Carrillo, an engineer working to improve the accountant experience on Gusto Pro What have you learned? In her first year after graduating, Sofia said that her biggest challenge has been communication and coordination. To quote her directly, “Nothing in school prepares you for working in a large organization with many different departments. Everyone has different knowledge and perspectives that they’re bringing to a meeting, so it’s important to make sure people are communicating and reaching alignment. I’ve learned that it’s hard to strike the right balance of being clear and giving context to everyone who needs it. Whether it’s in person, Slack, documentation, or code, communicating clearly and effectively is harder than you might think!” Olu’s biggest learnings have been getting comfortable with Ruby on Rails as his primary framework for writing code. Mariam said, “My biggest learning so far is that there’s not enough time to be hesitant about asking questions.” Transitioning from School to Work Going from college to a career can be scary. Not only are there a lot of known unknowns, but there is also a sense that the unknown unknowns are more numerous and more alien than before. Finding the right blindspots can help in making good decisions with confidence. Sofia points to the mentality shift that she learned at Gusto around tradeoffs as one of the biggest adjustments for her. “I think school primes you to think there are one or two ‘right’ solutions to a problem. But a lot of times those solutions don’t take into account real world things like deadlines, shipping value to users quicker, and other real life constraints that engineers have to account for.” Good engineering is less about the “right” solution and more about deciding between tradeoffs. Olu says working at Gusto has reinforced the value and importance of testing software. “All types of testing should be considered for both big and small projects. This includes unit, integration & QA testing. A lot of bugs can be prevented by writing good tests.” He also emphasizes the importance of a growth mentality for success. “Be quick to learn, be quicker to implement, and be comfortable with finding answers along the way. Your first job out of college is an opportunity to create a learning environment for yourself.” Mariam summarized the transition from college to work as such: “The ability to adapt quickly, particularly when working at a fast-paced tech company like Gusto, is essential (especially during a pandemic and economic crisis). Asking questions and being transparent about growth areas earlier on would have served me well since being comfortable in this space enabled me to build the right products and have a larger impact. I also began to recognize that a lot of other engineers share similar experiences early in their careers, so they were able to point me to helpful resources and give insight to their acquired knowledge.”", "date": "2020-12-15"},
{"website": "Gusto", "title": "Running AutoPkg in Github Actions", "author": ["William Theaker"], "link": "https://engineering.gusto.com/running-autopkg-in-github-actions/", "abstract": "AutoPkg is a software packaging system for macOS, commonly paired with Munki for distribution. For many organizations, Munki is the default way to coordinate the installation and updating of applications on macOS. At Gusto we use it to provision new machines and update existing ones. We’ve been running AutoPkg on Github Actions since late 2019 and wanted to share how we set up our build pipeline. AutoPkg refers to the steps required to package a specific utility as “recipes,” which define elements like code signature validation, supplemental installation scripts, and installer download locations. Github Actions is a CI/CD service offering Windows, GNU/Linux, and macOS runners charged per minute of run time. Github Actions is one of the few CI services that offer macOS runners billed by the minute, which is useful for short jobs like AutoPkg recipe runs. Why Our Previous AutoPkg Environment Was No Fun Prior to moving our AutoPkg builds to Github Actions, we ran our AutoPkg recipes on a dedicated VM that was hand built and manually updated. It was our house plant; things were mostly fine day to day, but regular maintenance was required and someone needed to watch it when we went on vacation. Any upgrades to the base OS or AutoPkg required careful testing and a rollback plan. In the worst case scenario a disk failure could have resulted in the loss of our entire build process, which thankfully never happened. Interacting with the box was a bit of a pain too, since we kept access pretty locked down for security reasons. The host machine was located on-premises and required VPN. We couldn't get our desktop support team interested in adding recipes when users requested new apps because we had no review process and everyone was afraid that clicking the wrong button would cause the machine to explode. The Structure of a Github Actions AutoPkg Run To make sure we’re rolling out the latest version of applications, we run our AutoPkg build daily. As mentioned earlier, we use Github Actions VMs to save us the headache of maintaining our own build infrastructure. Our daily Github Actions AutoPkg run does the following: checks out the latest version of our AutoPkg recipe overrides installs Munki and AutoPkg clones all the upstream recipe repos we use to give us an identical working state each time, while allowing our recipe overrides to fail if there are new upstream changes runs our fork of Facebook's autopkg_tools.py script, which iterates over a list of recipes, and successful builds are pushed into a separate Git LFS repo posts build results to a Slack channel so we can fix any recipe issues with a quick pull request GitHub Actions: Security and Configuration Considerations Use third-party Github Actions wisely . Third-party actions can be silently updated, introducing surprise bugs or malicious code. Since Git and Github have robust protections against SHA1 collisions, pin the SHA1 commit hash for actions instead of including actions by name alone. Limit the number of third party actions you use, especially for simple commands like “git push.” Using too many actions will make your jobs harder to debug, slow down your builds, and melt the ice caps faster by wasting electricity. For cross-platform tasks, the Ubuntu runners are cheaper than the macOS runners by an order of magnitude. Set timeouts in your Github workflow configuration, lest a zombie script keep the runner alive. Speed up your builds by only pulling in the latest commit. If you’re using Git LFS, use only the file pointers and not full installers. You can find examples of both in our sample git repository . Future Improvements In a future release of macOS, only signed package installers will be able to be deployed. I'd like to add our developer cert as a Github Actions Secret to sign internal packages and free software tools built from source. Since we’re running AutoPkg in ephemeral VMs, we discard cached downloads and end up downloading packages again each day even though we’ve already imported the same version into our internal package repository. We’re planning on submitting an upstream patch to the AutoPkg project to optionally read package download history from a file on-disk or hosted elsewhere, which should significantly speed up AutoPkg runs. How do I use Gusto’s AutoPkg build process? Create an empty Github repo with Actions enabled. Clone our Git repo and copy the contents of the autopkg directory to your own private repository. Copy workflows/ to .github/ in this repo. Create an AutoPkg recipe override (be sure to place it in overrides/). Add the override filename to recipe_list.json. Add the repo to repo_list.txt. Create another empty Github repo with Actions enabled. This will be your Munki repo. Copy the name of your Munki git repo to the “Checkout your Munki LFS repo” step in autopkg.yml. Add Github Actions secrets for SLACK_WEBHOOK_URL , SLACK_TOKEN , and GITHUB_TOKEN . We’re eager to see how other organizations are using Github Actions or similar CI systems for running tasks that previously required a dedicated VM, like AutoPkg or Terraform. Hopefully, sharing our process helps make managing software packaging on macOS a little bit easier for you.", "date": "2021-02-11"},
{"website": "Gusto", "title": "Service Level Objectives for On-call Peace of Mind", "author": ["Lindsey Whitley"], "link": "https://engineering.gusto.com/slos-for-peace-of-mind/", "abstract": "From Unsplash My team, Developer Experience (DEx), owns the Continuous Integration (CI) experience at Gusto. When I joined the team about a year ago, we were getting pressure from our customers (Gusto’s Product Engineers) to make our CI tool “better”. We framed our work to make CI “better” as an experiment with defined questions and metrics from the beginning. Could we use metrics to improve the experienced quality of CI? Once we knew that we were meeting some threshold of quality, could we use these metrics to prioritize future work? We gave ourselves a month to answer the first question and hoped that we’d have a while to test the second. Advantages of where we ended up By defining SLOs and monitors around the performance that we expected from our CI setup in Buildkite , we: Ensured that we would know about potential degradation in performance before our customers saw it Removed the need for a human to look at a dashboard Automated prioritization of new features vs “keeping the lights on” Gave ourselves a needle that we could see moving up and down with changes, as opposed to needing customer feedback more often Defining “better” We took on the task to understand what issues in CI were actually frustrating Gusto’s engineers. To gather this information, we opened up a couple of channels for feedback. We asked for opinions in our #engineering Slack channel. We also scheduled live feedback sessions with product engineers from each group or mission. In these conversations, we asked: What is good about our CI? Is speed an issue? When are you frustrated with our CI? Answers to our questions were grouped into buckets, and the most prominent were: Builds fail because something goes wrong with docker or mysql or something else outside my control as a Product Engineer. I hate when I have to wait for 15 minutes before the build even starts. We have so many flaky tests that I assume any rspec (ruby test suite) failure is caused by them. The test suite just takes too long to run. With all of this in mind, we knew what “better” would look like for our customers, but we weren’t sure that we were the only team who should be working on solutions to all of those problems. We drew some lines around what our team should fix and around what the developers themselves should fix. In this case, we defined the boundary as such Individual tests are under the control of Product Engineering. Therefore, flakey tests and slow test suites should be sped up by the teams who know the code best. The machines that run the tests are our responsibility. Therefore, we needed to make sure that CI builds didn’t fail because the machines were set up wrong or were too slow to stand up. If you haven’t heard of SLIs before, Google has written about them , a couple of times . The basics are that an SLI is the measurable ratio of good things to bad things . It goes down when performance degrades, and it goes up when performance improves. We turned our areas of responsibility into these SLIs: Note that for stability, we were not measuring the total number of failed builds. We specifically caught error statuses that were not 1 (which implies a test suite failure) or 0 (which isn’t a failure at all). Similarly, we were intentionally not interested in the total time for a build to finish, just the amount of time a build waited to start. This was the distinction we made because we had control over how many machines were idle and ready to pick up new jobs. If the number of machines was too low, we needed to scale smarter. Implementing Service Level Objectives With these SLIs, we negotiated with the customer to define Service Level Objectives (SLOs) for our two themes - speed and stability. Put simply, an SLO is a target value or range for an SLI or SLI ≥ target. In essence, we were deciding what percent of the time our SLIs needed to be above our target. SLO targets should be below 100% because you won’t have a perfect product. We live in a world with humans, weather, and uncertainty, which means downtime is inevitable, so we should plan for it. By accounting for downtime in the product experience, we also buy learning and development time for ourselves when we can take risks without ruining our SLI. After talking with customers, we decided to focus on stability first and settled on these SLOs: 99.5% of jobs via Buildkite Jobs in the past 30 days should not experience “infra failures” 95% of the time, the Average Buildkite Job Latency in the past 30 days should be less than 4 minutes We gave ourselves one month to experiment. At the end of the month, whether we had reached our goals or not, we planned to sit down with customers again and see if sentiment toward Buildkite stability and speed had increased. Afterall, the point was to make engineers more productive. Short of tracking their time, the best we can do to measure productivity is talk to them. Make Experimentation Easy Now that we knew what we were aiming for, we needed to get real data and make it easy to see progress. At the time, Datadog had just introduced an SLO widget , allowing us to turn the words we were using above into cohesive dashboard pieces. The centerpiece of our dashboard was the SLOs themselves. At the time, our speed SLO was green for “the past 7 days” and “the past 30 days”. Our stability SLO was red across the board, as anticipated. Here’s a screenshot of what these widgets look like today, almost a year after the experiment began: Further down the board, we added more granular measurements for specific error types. We made these viewable in just about every way possible - a total count of each error type, ratios of individual failure types to the total number of failures, change week over week for each error type, a graph of errors over time, etc. Until we interacted with the data, we did not know which view was most helpful. So we set up multiple ways to see the same data, and as we found one useful, we removed the other three. By the end of the experiment, we had a concise and easy-to-understand board that has made debugging easier since the experiment. Experimenting Change small things first Once we were confident that we were counting everything our SLI deemed “bad”, we were ready to experiment. Some of these “experiments” were low-hanging fruit like upgrading our version of Buildkite, changing environment variables, and removing redundant setup code. Regroup when needed At the start of the experiment we thought we understood how our failures should be grouped. As we continued increasing stability, though, we realized that the biggest bucket of error types could be broken down further. Once we rearranged buckets by identical failure messages, not just the same exit codes, we gained clearer priorities. Roll with the punches One of our changes was purely reactionary. While we were working on another failure cause, a binding.pry slipped into a commit on a feature branch. You may think this should not be a big deal since it didn’t get out into the development or master branches. But since all of the machines for a queue are in the same pool, it tied up machines, caused other jobs on those machines to fail when the machines were marked unhealthy, and drove wait times through the roof. The fix was simple - set a global environment variable (DISABLE_PRY=1). Big swings A bigger change that we wanted to tackle was upgrading our machines from Ubuntu 14 (Trusty) to Ubuntu 18 (Bionic). By the time we were considering this change, we were green for our 7 day and 30 day stability SLO, so we had space to take bigger swings. The short story is that this change didn’t work out within a month. Though we didn’t complete this big upgrade within the experiment time, we were able to try and fail because we had SLOs that kept us accountable to our customers. Without that built-in downtime, we might have been too scared to take a big risk and get feedback so quickly. As it was, we were able to identify the scaling and shut down problems quickly. After the experiment The experiment ended with us reaching our goals across the board -  all of our SLOs were met for Buildkite’s speed and stability. A few months after the experiment, we saw a dip in our stability SLI below the target objective. We didn’t hear a single complaint from engineers about stability in Buildkite while we were investigating the cause. After a week of this, we decided to talk to our customers and decrease our SLO target to 95% instead of 99.5%. We made this decision with the assumption that we had set the bar too high for ourselves in the first place. Other issues were more variable and more intrusive to engineers, so we could ease back without causing undue slowness. On a related note, we haven’t changed the speed SLO at all since implementing it. This SLI has only slipped under its target on rare occasions, which were quickly remedied and received little notice from customers. Concluding Thoughts If there’s a place your customers want improvement, start with SLIs and SLOs for your product. This allows you to focus on moving a needle you can see, as opposed to the one that is unmeasurable or just in someone’s head. This also allows you to prioritize issues automatically. For example, if all SLOs are met, the complaint that one build is slow can be put on the back-burner and not distract from regularly-scheduled work. If you feel like you’re drowning in maintenance or operational work, embrace SLOs.", "date": "2021-01-28"},
{"website": "Gusto", "title": "How ACH works: A developer perspective - Part 1", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-1/", "abstract": "Note: This is the first post of a 5-part series. Afterwards, read part 2 , part 3 , part 4 and part 5 . The Automatic Clearing House (ACH) network is the primary way money moves electronically through the banking system today. At Gusto , we rely heavily on the ACH network. For example, when a company runs payroll, we’ll use the ACH network to debit the company’s account to fund their employee’s pay. Once we’ve received these funds from the company, we’ll again use the ACH network to initiate credits into each of the employee’s accounts to pay them for their hard work. Today, our systems seamlessly move millions of dollars through the ACH system on a daily basis. But it wasn’t always that way. I remember when we first started building Gusto, we spent a many weeks struggling to understand how the ACH system works and learning how we can write software to tap into it. Part of the reason it was so hard to comprehend is that much of the documentation explaining the ACH system is targeted towards bankers, not software developers. To guide and encourage the next creators of modern financial software, this article will explain how ACH works from the software developer’s perspective. In order to initiate an ACH credit or debit on someone’s bank account, you first need to find a bank who is willing to proxy these transactions on your behalf. Only banks are allowed to initiate ACH transaction directly. You’ll need to open a business account with a bank and request the ability to originate — the technical term for send — ACH transactions through them. If they agree, they’ll become your ODFI, which stands for “Originating Depository Financial Institution,” but really it’s just a fancy term for “bank”. Your ODFI will set you up with a secure FTP server onto which you’ll upload ACH files. I won’t get into the actual format of these text files, but they basically contain information about who you want to debit from or credit to (identified by bank routing number and bank account number), and the amount of the transaction. When the ODFI receives your ACH file, they will forward it that evening to the Federal Reserve. At this point your ODFI will usually assume the ACH transaction you originated will go through and credit or debit your bank account for the ACH transaction that you originated. For example, if you initiated an ACH debit on Alice’s bank account for $100, your ODFI will increment your bank balance by $100. If instead you initiated an ACH credit on Alice’s bank account for $100, your ODFI will decrement your bank balance by $100. Early next morning, the Federal Reserve will tell Alice’s bank that an ACH transaction was originated on Alice’s account, and Alice’s bank will debit or credit her account accordingly. For example, if you initiated an ACH debit on Alice’s bank account, her bank will decrement Alice’s balance by $100. If you initiated an ACH credit on Alice’s bank account, her bank will increment Alice’s balance by $100. Alice’s bank received the ACH file from the Federal Reserve, and is therefore called the “Receiving Depository Financial Institution” (RDFI.) That’s pretty much it! If you FTP an ACH debit file on Alice’s bank account, the same process will happen with inverse amounts. Your account will credited with $100 and Alice’s account will be debited by $100, and all is balanced with the financial world. If you FTP an ACH credit file on Alice’s bank account, your account will be debited by $100 and Alice’s account will be credit with $100, and all is still balanced with the world. But what happens if you originate an ACH debit on Alice’s account and it turns out that Alice doesn’t have $100 in her account? Or, worse, what if your ACH debit file mistakenly contained the account number for Bob’s bank account? These are some of the edge cases that make using ACH a little more tricky and will be the subject of part 2 ! Comments on Hacker News", "date": "2014-04-23"},
{"website": "Gusto", "title": "How ACH works: A developer perspective - Part 5", "author": ["Sahil Jolly"], "link": "https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-5/", "abstract": "Since part 4 of this series, a lot has changed with the US FinTech landscape that enables companies like Gusto to move money safely and speedily. In this part, we will look at some recent developments announced by NACHA, specifically around Same Day ACH. Before we get going, make sure to refresh your memory on ACH basics and lingo from part 1 , as well as on ACH timing from part 3 . So what’s new? Same day ACH is a set of rules which give the option to ODFIs to transact money (debits or credits) and mandate RDFIs to settle funds on the same banking day. This theoretically allows Originators and Receivers to be able to send and receive funds within the same day. In September 2016, Phase 1 of NACHA’s Same Day ACH rules became effective, allowing same day credits. In September 2017, Phase 2 was rolled out for debits. These enhancements come with some caveats, so let’s see if same day ACH is right for you. Regular ACH timing (all times in PT): Same Day ACH timing: Leg 2 and Leg 3 can now happen sooner on Day 1. There are 2 new windows for the ODFI to forward the ACH request to the Fed (and the RDFI) and for the RDFI to settle the funds. Window 1: Fed receives ACH request by 7.30am PT, RDFI settles funds by 10am PT Window 2: Fed receives ACH request by 11:45am PT, funds settled by 2pm PT In addition, with Phase 3 in March 2018, the rules mandate RDFIs to make funds available to Receivers by 5pm RDFI’s local time. Here’s what the same day ACH timeline looks like for Window 2 (all times in PT) How do I use it? Originators can request same day ACH for transactions by setting the Effective Entry Date and Company Descriptive Date (check if required by your ODFI) fields to the same banking day as the day of submitting the the ACH file. These fields are on the batch header record of the ACH file . Unfortunately, there are some practical limitations to be aware of before implementing same day ACH: International transactions (IATs) are excluded from same day ACH rules. ODFIs must set an earlier cut-off time to receive and process ACH files with same day entries from the Originator. In our case, the cut-off time set by our bank is 9am PT for Window 2. This in turn means that we would need an even earlier payroll run deadline to meet the final same day ACH window. There is a $25,000 per-transaction limit on same day ACH entries. NACHA guidance prohibits splitting larger transactions to work around this limit, but allows genuine use cases of multiple transactions between the same accounts on the same day. Banks are positioning same day ACH as an alternative to wires and pricing it accordingly. Depending on your relationship and contract with your bank, the per transaction cost can be more than 100 times that of regular ACH. Lastly but quite significantly - the timeline for leg 4 and leg 5 remains unchanged. This means RDFIs still have 24 hours to respond to the ACH request and it can still take 2 business days for the Originator to be notified of any returns . The “no news is good news” policy remains intact and even with same day settlement, it makes sense to wait at least 2 business days before you can assume that your transaction was successful.", "date": "2018-01-17"},
{"website": "Gusto", "title": "How ACH works: A developer perspective - Part 4", "author": ["Edward Kim"], "link": "https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-4/", "abstract": "At Gusto , we rely heavily on the ACH network to pay employees and to remit various payroll taxes to federal and state agencies on behalf of our clients. We even use the ACH network for remitting employee-initiated donations to non-profits from their paycheck. In part 1 of this post, I outlined the basics of how we originate ACH debits and credits. In part 2 of this post, I went into the details of how ACH returns are handled. Finally, in part 3 of this post, I explained the timing of these ACH transfers. Part 4 of this post will go into the actual ACH file format. An ACH file is nothing more than a file with multiple lines of ASCII text, each line 94 characters in length. A line is a called a \"record\" in ACH parlance. There are 5 main record types in an ACH file: File Header Record Batch Header Record PPD Entry Detail Record Batch Control Record File Control Record An ACH file can contain one or more batches and a batch can contain one or more entries. I'll describe what batches and entries are in their sections below. Here's a visual format of how the 5 records are placed in relation to each other. As you can tell, a File Hearder Record must always be closed by a File Control Record, and a File Batch Record must always be closed by the Batch Control Record. Think of them as HTML tags. Before getting into the nitty-gritty of the file contents, there are a couple ground rules: If the contents in a field is alphanumeric and does not take up all the space allocated for the field, the contents should be post-padded with spaces until the entire allocated space is used up. If the contents in a field is numeric and does not take up all the space allocated for the field, the contents should be pre-padded with zeros until the entire allocated space is used up. File Header Record There is always exactly one File Header Record in each ACH file, and it's always the first record in the file. The File Header Record contains high-level information about the contents of the ACH file. Character Positions Field Name Description 1-1 Record type code Always \"1\" 2-3 Priority code Always \"01\" 4-13 Immediate destination A blank space followed by your ODFI's routing number. For example: \" 121140399\" 14-23 Immediate origin A 10-digit number assigned to you by the ODFI once they approve you to originate ACH files through them 24-29 File creation date Today's date in YYMMDD format 30-33 File creation time The current time in HHMM format 34-34 File ID modifier Always \"A\" 35-37 Record size Always \"094\" 38-39 Blocking factor Always \"10\" 40-40 Format code Always \"1\" 41-63 Immediate destination \n name The name of the ODFI. For example: \"SILICON VALLEY BANK    \" 64-86 Immediate origin name Your company's name. 87-94 Reference code Optional field you may use to describe the ACH file for internal accounting purposes Batch Header Record The batch record defines certain shared characteristics of each of the Entry Detail Records inside it. For example, the Batch Header Record will instruct the bank whether the entries inside it are credit or debits. They will also tell the RDFI what text to put on the receiver's bank statement for the credit or debit. Character Positions Field Name Description 1-1 Record type code Always \"5\" 2-4 Service code If the entries are credits, always \"220\". If the entries are debits, always \"225\" 5-20 Company name Your company's name. This name may appear on the receivers’ statements prepared by the RDFI. 21-40 Company discretionary data Optional field you may use to describe the batch for internal accounting purposes 41-50 Company identification A 10-digit number assigned to you by the ODFI once they approve you to originate ACH files through them. This is the same as the \"Immediate origin\" field in File Header Record 51-53 Standard entry class code If the entries are PPD (credits/debits towards consumer account), use \"PPD\". If the entries are CCD (credits/debits towards corporate account), use \"CCD\". The difference between the 2 class codes are outside of the scope of this post, but generally most ACH transfers to consumer bank accounts should use \"PPD\" 54-63 Company entry description Your description of the transaction. This text will appear on the receivers’ bank statement. For example: \"Payroll   \" 64-69 Company descriptive date The date you choose to identify the transactions in YYMMDD format. This date may be printed on the receivers’ bank statement by the \nRDFI 70-75 Effective entry date Date transactions are to be posted to the receivers’ account. You almost always want the transaction to post as soon as possible, so put tomorrow's date in YYMMDD format 76-78 Settlement date Always blank (just fill with spaces) 79-79 Originator status code Always \"1\" 80-87 ODFI identification Your ODFI's routing number without the last digit. The last digit is simply a checksum digit, which is why it is not necessary 88-94 Batch number Sequential number of this Batch Header Record. For example, put \"1\" if this is the first Batch Header Record in the file PPD Entry Detail Record This record contains most of the important information related to the account we're crediting/debiting and the amount of the transfer. Character Positions Field Name Description 1-1 Record type code Always \"6\" 2-3 Transaction code If the receiver's account is: Credit to checking account: \"22\" Debit to checking account: \"27\" Credit to savings account: \"32\" Debit to savings account: \"37\" 4-11 RDFI identification The RDFI's routing number without the last digit. 12-12 Check digit The last digit of the RDFI's routing number 13-29 DFI account number The receiver's bank account number you are crediting/debiting. It important to note that this is an alphanumeric field, so its space padded, no zero padded 30-39 Amount Number of cents you are debiting/crediting this account 40-54 Individual identification number An internal identification (alphanumeric) that you use to uniquely identify this Entry Detail Record 55-76 Individual name The name of the receiver, usually the name on the bank account 77-78 Discretionary data Always blank (just fill with spaces) 79-79 Addenda record indicator Always \"0\" 80-94 Trace number An internal identification (alphanumeric) that you use to uniquely identify this Entry Detail Record. This number \nshould be unique to the transaction and will help \nidentify the transaction in case of an inquiry Batch Control Record This record marks the end of the previous batch. There is always one Batch Control Record for each Batch Header Record. The Batch Control Record contains various checksums to ensure that you've generated the batch correctly and no information was lost during the transmission of the file. Character Positions Field Name Description 1-1 Record type code Always \"8\" 2-4 Service code This is the same as the \"Service code\" field in previous Batch Header Record 5-10 Entry count Total number of Entry Detail Record in the batch 11-20 Entry hash Total of all positions 4-11 on each Entry Detail Record in the batch. This is essentially the sum of all the RDFI routing numbers in the batch. If the sum exceeds 10 digits (because you have lots of Entry Detail Records), lop off the most significant digits of the sum until there are only 10 21-32 Total debit entry dollar amount Number of cents of debit entries within the batch 33-44 Total credit entry dollar amount Number of cents of credit entries within the batch 45-54 Company identification This is the same as the \"Company identification\" field in previous Batch Header Record 55-73 Message authentication code Always blank (just fill with spaces) 74-79 Reserved Always blank (just fill with spaces) 80-87 ODFI identification This is the same as the \"ODFI identification\" field in previous Batch Header Record 88-94 Batch number This is the same as the \"Batch number\" field in previous Batch Header Record File Control Record There is always exactly one File Control Record in each ACH file, and it's always the last record in the file. The File Control Record contains various checksums to ensure that you've generated the file correctly and no information was lost during the transmission of the file. Character Positions Field Name Description 1-1 Record type code Always \"9\" 2-7 Batch count The total number of Batch Header Record in \nthe file. For example: \"000003\" 8-13 Block count The total number of blocks on the file,  including the File Header and File Control records. One block is 10 lines, so it's effectively the number of lines in the file divided by 10. 14-21 Entry count Total number of Entry Detail Record in the file 22-31 Entry hash Total of all positions 4-11 on each Entry Detail Record in the file. This is essentially the sum of all the RDFI routing numbers in the file. If the sum exceeds 10 digits (because you have lots of Entry Detail Records), lop off the most significant digits of the sum until there are only 10 32-43 Total debit entry dollar amount in file Number of cents of debit entries within the file 44-55 Total credit entry dollar amount in file Number of cents of credit entries within the file 56-94 Reserved Always blank (just fill with spaces) Because it can be tedious and error prone to generate large fixed width documents like ACH files, we've created and open sourced Fixy , a tool for generated fixed width documents in Ruby. You can read more about this library here . Before RESTful APIs were all the rage, it seems that fixed width files were a popular way to pass information from one system to another. Many legacy systems, not just ACH, use fixed width files. Parts 1, 2, 3, and 4 of this series should give a pretty good overview of the ACH protocol and its implementation at a high level. For the sake of clarity, much has been simplified. In reality, there are many more record types that I did not cover. The official NACHA rulebook , where you can find the complete specification of the ACH files, is 479 pages long! Today, Gusto moves billions of dollars annually through the ACH network, and it's a critical part of how we pay employees for their work and the government for their taxes. If problems of this nature look interesting to you, we're hiring talented engineers . In part 5 , we will look at some recent developments announced by NACHA, specifically around Same Day ACH. Comments on Hacker News", "date": "2014-09-05"}
]