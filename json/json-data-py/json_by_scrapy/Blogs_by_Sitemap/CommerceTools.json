[
{"website": "CommerceTools", "title": "how we created an effective game based team feedback framework", "author": ["Evi Lazaridou"], "link": "https://techblog.commercetools.com/how-we-created-an-effective-game-based-team-feedback-framework-f71a3596cc4", "abstract": "About We're hiring TL;DR: How we improvised and created our own full-fledged team feedback framework and made it fun with a game for progress evaluation. This framework is an internal tool to improve the team collaboration and does not relate to our approach at performance evaluation. Every team is a complex of people whose interactions, communication and collaboration defines the growth, performance and the success of the team and every individual. This complex often entails diversity in mindset, viewpoints, age, gender, personality and chemistry among people. This diversity is vital when creative thinking is necessary, as it increases collective intelligence, but is also challenging when it comes to interactions among its members. And interaction is everything in a team , in order to ensure a harmonic and productive coexistence, in which everyone is happy and motivated. Team members need to openly communicate and try together to find solutions. This is why concrete and constructive feedback from everyone to everyone is a key element to overcome the difficulties and foster good collaboration and why every team should find a fitting context to apply it. I am a Data Scientist at commercetools and 1.5 years ago I took over a coaching side role. As a people coach, I support colleagues in my technical domain — and consequently my team — with their career goals, personal development and workplace happiness. This role gives me space to bring up and implement ideas for improved collaboration. My team has been relatively stable over time and therefore presents many perks and challenges that come along with a long-term relationship. Just like any tech team today, we regularly provide and receive snippets of feedback through our meetings, Slack chat, PR reviews and agile retrospectives. However this feedback is often implicit, tailored to a case or incident and sometimes too vague and subtle. Every team member might have a vague idea on where to improve but it is not always concretely communicated. And it isn’t only about areas to improve, but also positive feedback that gets lost in the daily routine. Under time and work pressure, it is easy to focus on flaws and things that need to be fixed, rather than acknowledging the effort and good work that went into making results happen. So this stream of feedback is not enough and it is quite prone to misinterpretation. Thus, roughly a year ago, we started experimenting with a more proper and direct feedback framework where team members could give one another constructive, detailed feedback on both areas they do particularly well and where to improve. It is like 360-degree feedback , but only within the team and conducted via regular feedback rounds and a progress evaluation game. As this started from scratch, there were many questions to answer in order to find the best fitting approach regarding whether it should be in-person, online, anonymous etc. After an iterative process of gathering each team member’s preferences while offering flexibility, I ended up with a Google Form consisting of two very straightforward questions. I replicated the form for each participant who wanted feedback and invited the rest team members to participate anonymously — participation was optional but highly encouraged. I did one for myself too and transferred the ownership of that form to a fellow colleague. The participants preferred to receive final responses exactly as they were written (instead of being summarized), which I also found better to avoid the risk of misinterpreting someone else’s words. Before we started, I also sent an informational email to highlight points like how this asynchronous form doesn’t leave space for clarification from the recipient’s end, so they need to be clear and use examples when possible. In the end I collected all the responses, rearranged the order, grouped similar topics together and handed them to each recipient. The outcome was helpful and motivating. Most team members participated, gave constructive feedback for improvements in a friendly and respectful manner and acknowledged the good work generously. It is hard to objectively say whether the collaboration improved significantly after this, but it did improve and I felt that people were overall more conscious regarding their “weaknesses”. Half a year later we repeated the process and, in the end, every member participated in at least one round — most of us in both rounds. Through the two feedback rounds, we all received some explicit recommendations on how to grow ourselves and improve the team work. But did we improve? Did we try? That part was not as clear and the whole process seemed somehow incomplete. And to complete it, we would need a more transparent approach. Up till now, feedback was anonymously given, but everyone would have to reveal it in order to get a feel from the entire team on whether they’re improving. That might cause some discomfort but it would only make sense and be fair if everyone was involved. Therefore I wanted something different, more pleasant than anonymous forms and perhaps even playful, so I tried to gamify it. This way it would feel less like a formal evaluation and more like a fun setting to communicate and discover more about ourselves and our interactions. I planned a meeting, explained in brief what it is about and asked everyone to pick 1–3 points of advice they received and for which they want to check the progress with the rest. To make things easier, I printed out the feedback and asked them to cut the respective snippets and bring them along. That was all they had to prepare before joining the meeting. The rest game was carried out with the help of a whiteboard, four printed memes and one magnet and blank piece of paper for each person. The memes were not necessary but provided casual and humorous context. Here’s how you can replicate what we did. The setting: On the whiteboard, draw two distinct areas: the points table and the response board. On the points table, write the names of the participants and prepare 6 columns for the points. On the response board, designate 4 small boxes or distinct subareas for the magnets and try to make clear what each represents and how many points it earns. Use memes to reflect the emotion of the response — and for fun — if you wish. Last, leave the magnets on the side for the voting part. On the table, prepare one blank piece of paper (and pen) for every participant. If possible, a small box would be handy but not necessary. Also, if you want to play with random order, prepare for it, e.g. write down in small paper lanes all the names to draw randomly one name every time. And don’t forget to put some sweets and snacks too! The rules: Secret vote : At the very beginning of the game, every player takes the blank paper note and writes down secretly the name of the person he/she thinks has showed the most improvement since feedback rounds started, then folds it and puts it in a box or along with the other secret votes. Note: you vote who you think has improved the most, not who you think will win. This will be saved for the end and will count as bonus points. Proceed with the rounds. The rounds : The main game consists of 3 rounds. Each has the same process but there are some variations in the points. In every round, with random order, every player is reading out loud one feedback snippet they received and asks the rest team members whether they improved on that. The rest will have to put a magnet on the response board area that reflects best their opinion among the given choices: You nailed it! -> You clearly improved [+2points] Not bad! -> I saw some progress but you can do better [+1 point] It wasn’t a problem -> I never saw this as a problem [+1 point] Oops … -> I didn’t see progress, try harder [-1 point] The votes on the response board define how many points each player will collect for the round. If, for example, there were 2 votes on “you nailed it” and 3 votes on “not bad” then the sum of points for that player and this round is 7. Every player must participate in the 1st round and can pass in the next ones, which means he/she doesn’t then read personal feedback and doesn’t collect points anymore but must still vote for the other players. The 3 rounds differ on how the points will be counted: Round A : the sum of the points for this round exactly as given and explained above Round B : the sum of points of the round plus 1 more point for the bravery of continuing to another round Round C : the sum points as given but, before playing, you have the right to brash off a not so good score from a previous round. The average of the rounds will be then counted only based on your left scores and so this is a chance for you to improve it. But remember: you have to decide this before proceeding with the round. Note that all team members decide together upfront whether the voting should be open or secret and only if everyone is comfortable with open voting, then you can go with that. After the end of the 3 rounds, you calculate the average and put it in a separate column and it is time now to reveal the secret votes. These will be 1 bonus point for every secret vote a player received. In the end the final score would be: Average points of rounds (exc. brushed off round) + Secret points. And tada, that’s our feedback on the feedback game! There were two reasons why I came up with these variations in the rounds. First, the idea was to incentivise playing multiple rounds. Feedback and communication is the ultimate goal, so improvement shouldn’t be the only rewarding action but also the eagerness to open up and ask for as much feedback as possible. Secondly, it’s a game so I tried to make it interesting with a chance for a turnaround and some space for strategy. And of course you can consider to modify or enhance it. Other alternatives I thought of, for example, was instead of brushing off your own points, to challenge another person to exchange points if you think you would end up with a higher score. I was anxious how this game would turn out and it was a great surprise to see it evolving into something much better and helpful than even I expected. We played it with full disclosure voting and everyone was motivated and participated until the end. Furthermore, it ended up in very constructive and friendly conversation with little attention to the game and points. For every feedback advice we discussed extensively, gave opinions, openly this time, and brought up lots of concrete examples. We asked questions, we answered questions. And all this in a very friendly and laughing atmosphere. And we had a winner, who got some candy as a reward, which he later shared with everyone. Because of the extensive conversation, even within 1.5 hours, we didn’t make it to the 3rd round, my favourite one. And since everyone planned to play in every round, the +1 point of the second round turned out a little pointless in our case. So, in a future repetition, I’ll make sure to allocate more time with a break or modify and adjust the number of rounds and their special rules. Overall everyone enjoyed it and it turned out to be very entertaining but, most importantly, very useful and enlightening, with concrete advice to consider and a much clearer picture of how our behaviour and work is perceived by the rest of the team members. If you’re interested to integrate a feedback framework in your team’s processes or to enhance an existing one, I highly recommend giving ours a try. Use it as is or be creative and customize your own version. Don’t spend too long looking for special feedback tools, it is easy to implement. And don’t wait for management or HR to bring you one because it’s not a meant to evaluate your colleagues’ performance. It is something to apply if you want to improve communication and collaboration in your team, and do so in a pleasant way that will help building a better ground for effective collaboration. And if you do try it out, please share your experiences and possible variations here, I’ll be looking forward to hearing them! *Thanks to my team for being so open to experiments for a better collaboration and to our agile coach for helping me out with the backstage preparation and some helpful tips. Looking under the hood of the commercetools platform 160 Team Feedback Workplace Culture Teamwork Team Culture 160 claps 160 Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-18"},
{"website": "CommerceTools", "title": "solving organizational issues like a product manager", "author": ["Andrea Stubbe"], "link": "https://techblog.commercetools.com/solving-organizational-issues-like-a-product-manager-f638a71a80e2", "abstract": "About We're hiring Working to carve out solutions that address the problems they’re meant to solve. At commercetools, we strive to have a working environment where everyone and every team feels, and is, valued/respected. To strive towards something also means that we’re never quite satisfied with the status quo, and constantly look for ways to improve. That leads to having plenty of ideas, and no way to work on all of them at the same time! And how do we know if an idea is actually effective at solving a problem? Sounds an awful lot like the day-to-day challenges which product teams have, right? Let’s explore if a product team’s way of working can be transferred to tackling organizational issues. What’s the value of solving the problem? Understand the problem, before jumping to solutions Build empathy and hear all sides Define and measure success Find solutions Start with deciding whether a problem is worth investigating further. All the following steps take considerable effort, and we only want to go there if the impact is high. How often was it raised in 1:1s, team meetings, postmortems, or informal chats? How badly does it affect people? Were there escalations? Does it impact company, product, or customers in a bad way? Other than when assessing the value of solving problems within the product, we haven’t seen a strong correlation between how many people bring up an organizational issue and the value of solving it. This might be different for you, so take good care of carefully interpreting the data at hand. Examples — High value Product : The API to add an item to a cart takes more than 1000 ms to reply. That is a long wait time, and the store notices a decline in sales. High value as 40% of customers abandon the site after wanting to add an item to the cart. Meaning all users of the Cart API are affected. Organizational : Handing over on-call duties between time zones leads to hiccups in incident handling. It doesn’t happen often — it’s been that way for many years and everybody is used to it — yet whenever it does, it leads to miscommunication with customers and longer recovery times. High value since it happens for an extended period of time, affects many people, and impacts customers. Examples — Low value Product : One customer, who is using a browser that has been deprecated by the vendor, cannot use the storefront and shops somewhere else. Low value as the number of affected customers is barely noticeable. Organizational : Support occasionally sends a ticket to the wrong team. The affected team brought it up in a handful of retrospectives, and things typically improve. But whenever a new person joins the support team, it starts happening again. Low value as it only happens around specific events, does not impact on customers, and is solved without escalations. After we decide a problem is worth looking into, we research it thoroughly. The human brain is wired to jump to solutions very early ( Thinking, Fast and Slow is a good read on that topic), and it takes discipline to stay in the problem space . The effort pays off, however, as only when a problem is understood, can we identify valid solutions. Otherwise, we’ll end up implementing an initiative that sounds like a great idea, but doesn’t perform well in solving the problem. To understand an organizational or product problem, look at specific examples. As many as you can find: Ask colleagues, dig through meeting notes and retrospectives from the past, and recall similar situations in other companies you worked at. Be aware of your biases and challenge assumptions , and take some time to understand the size of the problem. If the problem is defined too narrowly, you will miss out on making a fundamental improvement that would also help with other issues. It also gets a bit blame-y quickly. If the problem is defined too broadly, it is hard to find a solution that actually helps. Example: Team Product is focused on building a new feature. Everybody is excited and things go well until Team Project tells them that they need something else, now, or they will be delayed in delivering critical functionality for a customer project. Narrow: “Team Project interrupts Team Product’s flow and focus, which is bad” Broad: “Communication between departments has to improve” Middle: “We lack a way of handling dependencies that is not frustrating either party” So be sure to write down a clear problem statement! Examples: “Whenever responsibilities change during the handling of an incident, we see a decline in speed and quality” “It is hard for new hires in support to understand which team is responsible for which functionality” “Product Managers are worried about the impact a new architecture will have on the product and our ability to deliver value” For products, and especially in UX design, it is important to understand the situation and emotional state a user is in and design the product accordingly. Is the user stressed from performing a task? Make it plain and simple. Is the user scared, because small mistakes will have big consequences? Add review options and warnings for destructive operations, and reassurance for harmless ones. When it comes to organizational problems, emotions play an even bigger part . The simple act of talking about problems can challenge how people see themselves or may be perceived as criticism. Understanding the emotions involved helps you to phrase problems in a way that does not call people out or triggers defensiveness. Otherwise, your attempt to solve a problem may create additional ones. To understand the emotions and build up empathy, nothing beats observing and listening. Schedule interview sessions or group discussions with the main actors and people that came forward. Here are two quotes about dependency handling to show the two sides of the story: “Our team spends the vast majority of our time building something to unblock teams. We don’t make any progress on work we see as more important for the company.” “We are blocked for weeks, as we cannot find a team to build something that we need to proceed, and don’t have the expertise in our team.” Interestingly, both sides were frustrated about the same thing: Not being able to deliver what they perceive as most important is frustrating! Use those learnings as input for finding a solution that works for all sides. Before you’re ready to think about solutions, define what success looks like and how you can measure it. And then make it a habit to look at the numbers to see if initiatives should be kept, adjusted, or stopped. Examples: The number of support tickets assigned to a wrong team stays stable at a max of 5%, even when new members join the team Maintenance efforts are reduced to 10% within one year and stay there When building products, we often run into constraints for solutions we find: Do we have the knowledge and skills? Do we have a team to work on this? Is the solution aligned with the product vision, compliant with laws and regulations, can we scale it? For initiatives to solve organizational issues, the one big constraint is: Does it fit our vision of the company culture ? Just like with your product vision — you’ll want to think twice before implementing something that is not aligned with it. Example : Teams complain that they spend all their time performing maintenance, which they do not enjoy. Your product teams believe in a “you build it, you run it” culture. Consider these two solutions: Move maintenance to a specific team or another department. While this would make the problem go away quickly, the solution is not a good cultural match. Give teams the time needed to deliver high quality code in order to lower maintenance efforts in the future. While the results can only be seen in the long run, it is aligned with the culture. There are plenty of articles and frameworks about how to set up your teams to succeed. Spotify model, anyone ? Just like in product work, it is good to be aware of what your competitors are doing, but not simply copy them. It’s even more dangerous for organizations than for products: A feature that does not fit your customer’s needs can be adopted or rolled back. Solutions to organizational problems often entail changing habits, which are a tough thing to adjust . I’ve been trying the methods above for some months now, and we kicked off a handful of initiatives with clearly defined problem statements and OKRs. Those even made it on our internal quarterly roadmap, which we extended to also include organizational improvements. So far it’s looking good, but I’m sure there are plenty of other ways to approach this. I’d love to read “Solving Organizational Issues like a Presales Engineer”, “like a 3 Star Chef”, or “like the Tour Manager for a Punk Band” to discover how other teams and organizations are handling the aspects above. Looking under the hood of the commercetools platform 41 Product Problem Solving Product Management Workflow Teamwork 41 claps 41 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-20"},
{"website": "CommerceTools", "title": "easy steps to get obs running for high quality demo and presentation recordings", "author": ["Brad Soo"], "link": "https://techblog.commercetools.com/easy-steps-to-get-obs-running-for-high-quality-demo-and-presentation-recordings-a6eef311e56e", "abstract": "About We're hiring Earlier this year, the product team started conducting internal product update demos. Every other month, we would showcase the changes and new features from across the product team to the entire company. This would help keep everyone on top of the latest developments while giving them the opportunity to ask questions at the end of the session. At the rate commercetools has been growing, these demos are also excellent for newcomers to dive right in and for everyone to see the faces behind the product. We needed a way to record these demos. Not everyone has experience producing videos beyond hitting the ‘record’ button in meetings, but those aren’t always the highest quality. I put together this easy-to-follow guide to change just that. It’s a bit of a coincidence that shortly after we did our first demo session, we entered the age of lockdowns and extended period of remote working— increasing the number of virtual demos and webinars (and the need to record them). So, hopefully this guide really comes in extra handy. But regardless of time and what’s happening in the world, knowing how to effortlessly create high-quality videos is also a useful life skill! Here are examples of times when you’d want to produce videos by recording your screen: Demoing new product features Explaining a new technology, standard or initiative like GraphQL or CloudEvents Presenting changes and updates on existing features Providing a how-to guide to build or configure something Tutorial that combines one or more of the aspects above. Screenshots from OBS Studio v25.0.1 By now, you may have a question mark hanging over your head and ask why you’d ever want to mess with a screen capture/recording software? Why make life complicated when you can just hit ‘record’ in your favorite online meeting service and be off to the races? Offline recording . Record directly onto your local disk. No dependencies on upload speed or internet connection that might affect quality. Record any time. If it’s not a live demo, you don’t need to set up a meeting for one and going through the full motions of joining, presenting, recording and waiting for the download link. Just hit start locally in your software. Instant review. No waiting for a recorded file to be processed by the meeting provider and sent to your email. Control over resolution, frame rate, compression, etc. Most online meeting services only have one button: start/stop recording. High resolution: Using 720p (1280x720) is alright, but 1080p (1920x1080) is preferable. Majority of online meeting service providers provide a maximum resolution of 720p or lower, tend to apply very high compression (video looks pixelated) and may sometimes automatically reduce resolution. Screen recording software lets you specify the resolution and keep it consistent (you’re only limited by your computer hardware and many modern laptops can handle recording screens at 1080p 30 FPS just fine) First step is to get your favorite screen recording software installed. For most screen demos and work presentations, your screen recorder should be able to record: 1080p (1920x1080) resolution as a minimum At least 10 minutes per video clip (many freeware/trialware products have time limits) Watermark-free (many freeware/trialware products automatically mark videos with their own logo) If you don’t already have one, I highly recommend OBS (Open Broadcaster) because: It’s open source and free (therefore, no watermarks) It has powerful screen streaming and recording settings It’s cross-platform: works on Windows, Mac and Linux Great out-of-box compression (8-minute 1080p 60 FPS video = 300–400 MB) The only downside of OBS is its learning curve that’s higher than ‘easy one-click’ software. Since it’s geared towards power users, some might also argue it’s overkill for recording presentations and demos versus something like QuickTime’s built-in record feature. However, once you master it, you’ll find OBS to be significantly more versatile and powerful for a variety of screen-based audio-visual recording activities. There are a few things to keep in mind to record videos with the right quality and consistency (I’ll use screenshots from OBS, but please look for the equivalent if using another software!) Save location/folder — anywhere you want on your local disk File format — prefer mp4/mpeg4, avi, m4v, mov Resolution — 1280x720 minimum, 1920x1080 recommended Frame rate — 30 FPS Framing — Your presentation should occupy the entire screen! Also: Make sure you don’t have extra things being recorded, like your Spotify window lurking in the bottom corner. Audio source — use the highest quality possible (e.g. if you have a dedicated microphone, please use that over your laptop’s built-in mic!) Save location Go to Settings > Output > Recording tab > Recording path While you’re here, select “mp4” for recording format Recording options Go to Settings > Video Canvas — This is the area where you will framing the video and what will be recorded in it (think of it like a viewfinder of a camera). Aspect ratio should be 16:9 as it’s the most common for screens and video playback (let’s not get into that aspect ratio debate… I’m a 16:10 person myself). Keep in mind you might have to reframe the video within the canvas for certain laptop screens (Macbooks and some Dell XPS laptops have 16:10 screens, Microsoft Surfaces have 3:2 screens) and ultra-wide monitors that don’t have 16:9 ratios. See the next section on framing your screen. Canvas resolution — I recommend 1920x1080 to match output and make life easy. If canvas resolution is set lower than output resolution, then the video will be scaled up and might look pixelated. Output resolution — This is what the file produced will look like. For demos, go with 1280x720 (minimum) or 1920x1080 (recommended) FPS (frames per second) — 30 is the minimum and should be sufficient. No harm using 60 FPS, but keep in mind file size will be bigger and it will strain your laptop more (important for users of older laptops) Hotkeys (Optional) Helps improve quality of life, especially if you only have 1 physical monitor/screen — you don’t need to switch between windows to start/stop recording. After configuring settings, it’s time to set up framing of your screen. Step 1: In the Scenes box, click + and add a scene. Name it anything you want. Let’s name it “Screen section” as an example. Step 2: Click on the name of the new scene you just made (e.g. “Screen section”) Step 3: In the Sources box, click + and add a source that is “Window Capture”. Step 4: A dialog box will appear. Click “Create new” and give it a name (e.g. My web browser) Step 5: Under Window, select the program where your presentation will occur. This will be your browser if doing a product demo. This can be your web browser or PowerPoint if showing a slide deck. Step 6: What you see in this frame will be what appears in the recording. Frame your presentation within the capture area by moving/resizing what you see Full-screen, 16:9 monitor: Make sure your whole screen fits the frame (width and height) Full-screen, non-16:9 monitor: Match your screen to the frame (width) then move it (height) so the black bars on top/bottom when you present full-screen won’t be recorded in the video. Windowed is nice if you have a high-resolution 4K monitor or more than 1 screen, so you can have the recorder up next to your presentation and you can see what you’re recording: Resize both the presentation window AND the frame so they match each other. Do a quick sound check by making some noise — your mic is working and ready when the green-yellow-red bar lights up with the noise. Windows user tip: Mute your desktop audio within the audio mixer to prevent system sounds from being recorded (e.g. you will still hear notification chimes over your speakers, but they won’t be in your recorded video). MacOS is more forgiving in this regard as it stops one application from recording the sound from another by default. It also makes it a pain if you DO want to record sounds from within your system and other applications. But that’s out of scope for what we’re doing in this article. Please note there may be more than 1 mic/aux shown if you have multiple microphone sources (laptop mic, headset mic, and a USB mic all plugged in at the same time). Find the one you intend to use, and mute the rest. Self-explanatory. If your software has both streaming and recording functionality, be aware they aren’t the same thing! General guidelines Start with a quick introduction. You can fill in the blanks and use this canned intro: “Hello everyone, my name is {FULLNAME}. I am the {JOBTITLE.FUNCTION} (e.g. Product Manager) at commercetools. Today I will be talking about {TOPIC} (e.g. the latest product features from our team released in Q2 2023)” Start by showing a logo or title card as an introduction. Have a list of items and actions you will perform. If you slip up while recording, you can start again from the last action and edit that out later. Before presenting: Clean your desktop up to eliminate distractions, possible notifications, prevent leakage of sensitive data. Consider increasing your cursor size so it’s easier for watchers to follow your interactions: - Mac: Apple logo > System Prefs > Accessibility > Cursor size - Windows: Control Panel > Mouse Properties > Pointers > Scheme > Magnified Alternatively: Enable cursor finder and activate it to emphasize your actions: - Mac: Apple logo > System Prefs > Accessibility > Checkbox “Shake mouse pointer to locate” - Windows: Control Panel > Mouse Properties > Pointer Options > Checkbox “Show location of pointer when I press the CTRL key” Ensure minimal background noise both in your system (e.g. music playing) and in real life (e.g. neighbor’s kids screaming or construction work). When presenting: Only move the cursor around when necessary. Do not play with your mouse/trackpad for no reason. Verbally walk through what you’re doing on your screen — pretend you’re sitting next to the viewer and helping them. - What are you doing (I will now click on the Filters Dropdown) - What your action does (which opens up a filter drawer of different product criteria) - The effects/benefits of your action (so that I can narrow down the list of products) Consider whether showing your face is necessary — it can add a personal touch to your presentation but keep in mind that your shared screen will be smaller. If showing your face, ensure: - Your face is well-lit by good source(s) of light - Pro tip: Your monitor isn’t a light source, even if it’s 32 inches large - Your background is presentable (Having a bookcase behind you is OK, but your hanging laundry isn’t) - Ideally it would be a plain colored wall/backdrop. - You are dressed appropriately It really depends on content and discretion, but try to aim for one of the following lengths + examples on what kind of content will work for what length: 2 minutes: Explain a single concept, idea, topic in brief 10 minutes: Exploring a topic with examples OR how-to guide (usually focused on one area) 30 minutes: Discussing a group of topics OR explaining multiple concepts OR tutorial with larger coverage (e.g. how to set up products, being a power user of PIM search) 45 minutes: This really should be the longest you should go. This is the length of a full-webinar. Go for short-and-sweet; longer isn’t always better. Remember that the audience may have limited attention spans and schedules. I hope this guide was useful for getting setup to record product demos and presentations— congrats if you’ve managed to produce a video based on what you learned here. A few of our product people have already managed to get some great internal demo videos up after trying their hand at screen recording. Remember this is a pretty high-level, quick start guide to using what’s essentially a power tool in the world of screen recording. Reading, exploring and practicing are all key to being able to get the most out of a software with a high skill ceiling. Looking under the hood of the commercetools platform 5 Video Production Presentations Open Broadcaster Software Recording Software Demo 5 claps 5 Written by Product Marketing Manager @commercetools Looking under the hood of the commercetools platform Written by Product Marketing Manager @commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-12"},
{"website": "CommerceTools", "title": "serverless commercetools integration", "author": ["Nicholas Speeter"], "link": "https://techblog.commercetools.com/serverless-commercetools-integration-6e46596df0ea", "abstract": "About We're hiring One command deployments for your e-commerce Serverless code. www.serverless.com commercetools API Extensions and Subscriptions allow you to customize and extend e-commerce functionality based on your company’s unique business needs. The Serverless Framework allows for fast and painless development of serverless cloud applications by introducing easy to use testing, CI/CD, scaling, and monitoring. Serverless cloud functions are often used to add additional behavior through commercetools API extensions. A common use case is validating shopping cart contents. Serverless functions can also be used as asynchronous background processes in response to events on the commercetools platform through commercetools Subscriptions. One example would be to send a confirmation email once an order has been completed. Checkout the link above to a new Serverless Framework plugin that deploys and registers a function as a commercetools API Extension. This plugin can also be used to attach your Serverless code to a commercetools Subscription. The deployments work for both AWS and Google Cloud Platform. Step 1. The Serverless Platform allows you to easily build cloud applications. It is extensible with plugins. Install the Serverless Framework, create a lambda with the simple ‘serverless’ command, and then add the commercetools plugin : Step 2. docs.aws.amazon.com Set your cloud credentials via command line. Step 3. Setting up commercetools extensions/subscriptions is as easy as adding your client variables and configuration to the serverless.yaml and then running deploy. Note that the Lambda’s ARN is smartly assembled by the plugin itself so you do not need to add it to the post body. See the example serverless.yaml . github.com You now have a commercetools Extension and lambda function attached. With Extensions one can extend the functionality of the commercetools APIs. One could now create some carts for the case above using Postman to run your Serverless extension code. Please note that you can set a key on extensions and subscriptions. The key prevents the same extension or subscription from being deployed twice from CI/CD and manually deploys. docs.commercetools.com For Google Cloud Platform function deploys simple set the CTP_POST_BODY to use the HTTP destination. Add the deploy type and body to your client variables to the serverless.yaml and then run deploy. docs.commercetools.com commercetools.com techblog.commercetools.com techblog.commercetools.com The Serverless Plugin Directory features 1,000’s of plugins to extend your serverless development. Explore the directory below as you work with commercetools and the Serverless Framework. We are excited to see what e-commerce recipes you create. www.serverless.com Looking under the hood of the commercetools platform 2 E Commerce Solution Commercetools Serverless Ecommerce Cloud 2 claps 2 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-12"},
{"website": "CommerceTools", "title": "how we transformed the product search for the better", "author": ["María Barrena"], "link": "https://techblog.commercetools.com/how-we-transformed-the-product-search-for-the-better-7f8ac75dbeba", "abstract": "About We're hiring The product search functionality available in our Merchant Center lets merchants (and to be more specific, catalog managers) find products in their product catalog. Originally, we had a single search implementation catering to both catalog managers and shop customers. Over time, we noticed that their characteristics differed: storefront search queries are simpler and returning results super quickly is crucial. However, merchant searches are more complex. And optimizing for complexity and speed is complicated. By using a brand new feature as an example, we would like to show how the product team follows a user-centered design approach to provide the best solution for all kinds of needs. Despite sharing the same goal of finding products, the two groups of users have different needs and intentions. The principal end-goal for storefront customers is to make a buying decision. Merchants can customize the storefront search to promote certain products. But their principal need in a catalog search is to find products to check the data quality or update it, publish new products or manage inventory. The merchants principal need in a catalog search is to find products to check the data quality, publish new products or manage inventory. The accuracy of results returned in the Merchant Center catalog search was also a concern from some merchants. This was not ideal for them, whose more complex search needs also led to slow performance and a decrease of productivity. The existing visual interface in the product section of our Merchant Center was not suited for our heavy users (those that use it to a great extent). Some users even admitted they have never tried the built-in filters. For those using them, filtering by attribute data was especially cumbersome as they had to: Open a pop-up modal Find the filter attribute Search for or type the desired value Apply and close the modal In the most common screen resolution, 720p (1280x720 pixels), the results list would even move below the fold after applying a fourth filter. Definitely, the new filters needed to be more accessible. Catalog managers with complex data models need to view loads of information in the product list. But those with simpler data models don’t need much data displayed in the product list. For them, the search and filter functionalities are more relevant. By decluttering the page and making it more customizable we can accommodate those two different usage patterns. This lets Merchant Center users focus on what’s relevant for them in the new list. Let’s say we have a product catalog containing a product called “Perfect Blue DVD” and another one called “Nike shoes”. The latter contains a searchable attribute “color” with the value “Blue” selected. With the existing implementation, both products will show up in the results when a catalog manager types the term “Blue” in the search input. This behaviour might match the storefront customer’s intention. But the merchant doesn’t need to find all products that contain the term “Blue” on any field of their product information. Ideally, they would target which part of the product data they would like to look into (the product name OR the color field). This was the reason why some catalog managers reported inaccuracy of results. Decoupling storefront and merchant search into two separate entities solves the problem: keep the existing search API for the storefront search and create a new search adapted for merchants needs. That would mean merchants could perform their tasks without affecting the storefront search. By splitting the searches, we could now benefit from faceted search. The main search input would be the place to search for basic product data (names, descriptions or SKUs). For any other characteristic, we would provide with more advanced filters for each one of their attributes. For instance, if a catalog manager is looking for black accessories, the facets used are one in categories and another one in the attribute color. This solution brought usability questions: How do we help users leverage the power of filters when searching for attribute values? How do we encourage them to start using the filters again? What’s more relevant for merchants, see their entire product list or have filters always visible and available? The use cases might differ from one company’s data model to another. A way for each merchant to choose among those two options would be the best solution. The decision whether to move the filter section (and if we did, where) was one of the first UX/UI decisions taken into the project. Filters work the same way and are always placed in the same area of the page throughout the whole Merchant Center platform. Moving them to a different place on this single page would solve some problems but it would break the consistency in our UI. Oh, the old battle between consistency (one of the 10 usability heuristics of user interface design ) and efficiency that UX designers have been fighting many times! A common design pattern for filtering a list is a filter sidebar. Many online shops and travel websites (e.g, Amazon or booking.com) use that UI solution to accommodate their large number of filters (catalog manager’s situation as well!). The benefits we see implementing a sidebar in the product list for our particular case are: Have all filters in a single place (no more pop-ups needed) The applied filters won’t push results below the fold To cover the needs of every merchant, and by taking into consideration the progressive disclosure pattern, we came up with the idea of showing and hiding the filter sidebar on demand. The sidebar is hidden by default, so it does not overwhelm the user. When users don’t need to filter products by attributes, they can hide it and focus on their product list. As Katie Sherwin describes in her article User intent affects filter design, there are two dimensions to take into consideration during the design of filters: when to apply the filters to the result list and when to scroll to the top of the result list. There are two dimensions to take into consideration during the design of filters: when to apply the filters to the result list and when to scroll to the top of the result list. If the user’s intention is to start scanning the result list right after applying a filter, refresh the result list and scroll back to the top provides the most useful experience to the user. But what if the user is still browsing and applying filters? Taking them back to the top of the page would force them to scan the sidebar again, to find where they were before the disruption. We needed a way to keep the filter sidebar untouched when the product list refreshes. Independent scrolling was the answer. Once we figured out the page structure, it was time to focus on the sidebar content. Some stores have huge data models with hundreds of product attributes. Showing all these attributes in the filter sidebar by default would not be too useful for them, as the sidebar will become too cluttered and filters hard to find. They needed a way to edit the sidebar: choose what filters they need in that sidebar and remove what they don’t need. The first sketches included a filter sidebar with two tabs: We would put the basic filters in the default tab: those that any product catalog would contain. The second tab would contain the attribute filters (color, size, brand…), which were previously hidden in the pop-up. By initial interviews and shadowing some Merchant Center users, we found out that each catalog manager uses different types of data to find their products. Thus, we assumed first and by user testing then confirmed that the distinction was not even needed. Putting all filter attributes together in one place will improve their discoverability. Each type of filter required a different user interaction depending on the type of data merchants were looking for: Filters where the data is a list of items could be easily applied to the list by clicking on the item. Filters that allow any text or numeric value would need an input where merchants could submit any value and press enter to submit the filter. Filters for date and/or time ranges would contain two inputs to submit either a single value or a range. In some cases such ranges, submitting by pressing enter might not be intuitive for those less tech savvy users. We added a button for better usability instead. Despite the difference, we needed a standardized, easy way to apply and remove several filters from the list. Let’s imagine that a merchant needs to find all products from the category “accessories” created last month and today. This merchant would need to search for a date range and for a specific date in the “creation date” filter. The solution was to display any applied filter with a checkbox, allowing users to apply and remove each filter with one click. Last but not least, we wanted to take advantage of the information that facets provide. Airbnb’s use of facets in their price filter makes for a good example and source of inspiration for us. They use them to visually display the amount of places available by price range. This information can give Airbnb users a sense of what’s the average price in a location. But let’s not forget that these users are looking for one single item (in this case, accommodation), while the merchants need is quite different as explained before. Showing the average number of products per filter is not that relevant for their usual tasks. A solution found often in e-commerce is the use of numbers that represent the amount of products containing that filtered value. With this visual hint, merchants don’t even need to apply the filter to know how many products they will get. With this usage of facets, we follow the Poka-yoke principle and prevent them from getting zero results. Depending on the merchant’s need, suggesting values into each filter could be helpful. But for now, the only data we could get was the value included into the biggest amount of products. Without applying machine learning on each project, these suggestions might not be useful for merchants. As this thought was only an assumption, We still wanted to get feedback from real users about it. We decided to check if we were on the right path before implementation as the change in both page structure and functionality were too broad. The most effective way to do so is by testing prototypes with real users. Five product catalog managers, real users of the Merchant Center, were subjects taking part of the usability tests. There are many studies (like this one ) that prove that with five subjects you get close to user testing’s maximum benefit-cost ratio. With me in the role of the facilitator and a fellow UX designer as a note-taker (so we did not miss our user’s reactions), we visited them at their workplace. In order to replicate the user’s usual scenario, the product data we used was part of their own product catalog. The 15-minute test consisted of a short interview and a task using a clickable prototype. The three-part task was the following: You need to perform an update in all the antifreeze products under the “Kitchen and household” category between 2 and 5 liters. You have just realised that you just need to update the antifreeze products that are smaller than 2 as well. There are too many filters on your list that you no longer need. Is there any way to solve that? We asked participants to express their thoughts while performing the task. This method, called thinking aloud , gives qualitative feedback as the thoughts are shared in real-time. The goals of the usability test were the following: Learning if the filter sidebar is discoverable Verify if the visual representation of facets is understandable Follow the actions that users will perform to find products of a specific product type and from a certain attribute value range Confirm whether the “edit sidebar” feature was easy to discover and use. Our findings after testing the prototype were: 2 out of 5 test users tried to use the search input in order to fulfil all the tasks, dismissing the filters. They typed all the possible values that the task provided and tried to submit the search query. 1 out of 5 test users mentioned that they rarely need to search for product types or categories. This catalog manager focuses on filter attributes instead. Not distinguishing between basic filters and attribute filters was a good decision. None of the test users found helpful the suggested values on filters. We decided not to implement that as it would not bring any value for them. Launching a new interface, especially with a completely new API behind is always risky. Our team decided to release this new functionality as a beta to a select group of users first as well as those who participated in the usability tests. The usability tests provided good input, but we wanted to get feedback on the initial implementation, to learn and improve. Concurrently, a different group of UX/UI designers tried to replicate usual catalog managers tasks first and then ranked the usability breaks found as high, medium and low risks. Thanks to those, we were able to improve both copy and usability prior to the open release. This process is called UI risk analysis . A different group of UX/UI designers tried to replicate catalog managers tasks first and then ranked the usability breaks found as high, medium and low risks, following a UI risk analysis approach. After a few days, we had the chance to go through the new product search with some of these users. With their questions and their feedback, we were able to understand if the new solution fulfill their needs. If the users of your interface need to complete some training to start using it, it means that interface is not intuitive enough. We needed a way to highlight the new features in order to encourage all users to interact with them and become aware of their value added. Based on the previous feedback, this was especially applicable for filters. We added a quick “what’s new” message that pops up during the first time that a user visits the page. By monitoring the first few weeks of usage, we found that users were divided into two groups: One was using purely the search input without filtering their list. The other had a more balanced usage in both search input and filters applied. The investigation led us to the reason: it had to do with the demos we conducted. All users from the first group did not participate in any demo. Users from the second group took part of at least one of them. This meant that the “what’s new” message was not enough to change user behavior. To improve awareness, we targeted new places in the UI where we could inform users about it. A good place to start was the “no results found” message. If that group of users were performing so many searches, they were landing on that page often. We substituted “Try reducing your search term and apply less filters to get better results” to what you can see in the screenshot below: Within 3 months, we’ve launched our new product search, available for all our users. This release is a great symptom of the maturity that the Merchant Center is approaching. And that would not be possible without our users’ help. Thanks to the input and user’s feedback we collected throughout the process, we are confident the new search will improve the search experience for every catalog manager. ______ Thanks to our testers, beta users, designers, developers, product owners and anybody who helped building this feature. And special thanks to Andrea, Brad, Freddie, Adnan and Yann for their contributions to this article. Looking under the hood of the commercetools platform 348 Thanks to Freddie Karlbom . UX Search Filters Case Study UI 348 claps 348 Written by UX / UI designer based in Munich (DE) Looking under the hood of the commercetools platform Written by UX / UI designer based in Munich (DE) Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-19"},
{"website": "CommerceTools", "title": "migrating commerce apple watch app to graphql", "author": ["Nikola Mladenovic"], "link": "https://techblog.commercetools.com/migrating-commerce-apple-watch-app-to-graphql-cc76ffa64eac", "abstract": "About We're hiring In today’s constantly-connected era, we see many commerce avenues available across all existing platforms: from traditional websites and mobile apps that we’re used to, to voice assistants, IoT buttons, and last but not least — wearables. Even though not all platforms bring equal traffic and revenue, for many players it is important to have a presence on as many devices as possible. With that, it’s important to take full advantage of what every platform / device family has to offer. While a simple IoT button is really useful for reordering a detergent, an app on a wearable device can do more⁠ — such as letting customers update order statuses, perform quick actions on past orders, get recommendations, new products, and maybe even a quick add-to-cart action in case something catches customer’s eye. If you’re looking for a popular wearables platform to get started with, Apple Watch might be the right choice, mostly because the last year’s reports stated that Apple Watch is inches away from having a 50% market share in wearables. We have included an Apple Watch app along with our demo iOS app, Sunrise , which serves as a showcase of how you get easily get started with commercetools API and the Swift SDK . Initially, the watch app used the same REST API as its iOS counterpart. You can check out the beta version via Apple’s TestFlight app. Even though the app was fully functional, some API interactions took longer-than-ideal times to complete and decode the data received into model objects. It turns out that many entities from the e-commerce domain tend to be bigger in size, mostly because of the business logic they carry. A good example is a product projection, which carries information about all product variants, pricing conditions for each variant, tax info, reviews and ratings, etc. Even though the design for the watch doesn’t include many of these items, there is no simple way to exclude them when using a REST API. That’s when we decided to migrate over to GraphQL API⁠⁠ — commercetools has supported GraphQL since 2016 and was the first commerce platform to have done so. We’re working on getting it out of beta, though it has already been used across many production applications today. The first step is to decide whether you would like to use a watchOS framework like Apollo , which can help with generating models from your queries, and provide caching support if your application needs it. For the Sunrise watchOS app, we were already using the commercetools Swift SDK, which provided the models needed out of the box, so we decided not to include another dependency to the project. In this case, the trade-off is to at least write the top-level models matching the queries, and then use the structs from the SDK for lower-level fields. For encoding and decoding, and performing network tasks, we used the commercetools Swift SDK, which was already a part of the project — a good choice since it meant fewer changes. However, when starting from scratch and using an API which doesn’t provide an SDK, you might want to use features provided by the GraphQL framework, like Apollo. The improvements are noticeable even without looking at the numbers. Since the newest devices like the Apple Watch series 4 and 5 have better hardware, we decided to compare the app on the older series 2 watch, running watchOS 6.1.3. This way, we’d be able to observe even more pronounce differences in performance. We loaded the ‘My Orders’ screen, which shows the 5 most recent orders. With REST API implementation, we had to retrieve 5 Order objects, which contain a lot of information the watch doesn’t need, and averaged a loading time of 2000ms. This query that the new GraphQL version uses resulted in a load time of just 600ms (3 times faster!), since it retrieves only the necessary data we want. These timings include executing the network task, processing, and decoding the response. Similarly, for the product overview screens, where we retrieve five products at a time, the loading time (including decoding) was significantly faster with GraphQL — averaging around 3100ms with REST API but clocking under 1000ms with GraphQL. We’ve seen great GraphQL adoption over the years and many production products are successfully using it these days. However, it’s still underrated in wearables and lower-end devices which have probably the most to gain from it. In our case, and generally for majority of commerce apps, where the nature of business logic requires big domain objects, the improvements are definitely noticeable. If you would like to find out more about the demo app you can head to the Github page , or try it out using the TestFlight invite . Looking under the hood of the commercetools platform 205 Smartwatch GraphQL Apps App Development Ecommerce 205 claps 205 Written by iOS Software Engineer Looking under the hood of the commercetools platform Written by iOS Software Engineer Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-27"},
{"website": "CommerceTools", "title": "its done or is it", "author": ["Andrea Stubbe"], "link": "https://techblog.commercetools.com/its-done-or-is-it-9fe91b9a6981", "abstract": "About We're hiring commercetools has been growing a lot lately, and we want to continue doing that. Not only do we have new product owners, UX experts and developers joining, we are also growing areas around the core platform teams. More sales, more training, more support, more marketing; in offices all around the world to support the growing product we’re making. Being strong believers in independent teams, we thought about how we can ensure delivering a great product while: Minimizing disruption and interference with how the platform teams work Simplifying collaboration for all the other departments that contribute to the whole product Our starting point to this is having a common Definition of Done (DoD) for features, which describes what has to be in place to call a feature “done”. And for the customer, a feature isn’t just about code that’s deployed, but the entire package. That can be all of the software’s inherent qualities , but also additional assets living next to it, like training material or documentation. Lastly, a feature is not done, if it will be a mess to maintain in the future. Don’t confuse done in this context with complete . Most features start minimal and then keep evolving based on feedback and learnings. Done means: Whatever was defined is finished. And it’s good. The common definition of done for features brings us three benefits: Alignment on everything that’s customer facing. Consistency in APIs, documentation, wording, UX, and knowing what to expect makes their work so much easier, and leaves them with a warm and fuzzy feeling of trust . Helping colleagues to get their job done. Documentation, support and training, customer success and professional services, marketing — they all contribute to the product. In the end, how much is your feature worth if nobody talks about it, understands it, or recommends it in implementation projects? A Definition of Done is like a playbook for helping everyone understand what’s expected from them (and what they can expect). Setting us up for sustainable development and growth. Coming up with a Definition of Done is a fairly straightforward task, but still surprisingly time consuming. Having a common DoD for all teams lets you focus on more fun things, and at the same time ensures we all do have a shared understanding of what is required for a feature to be “done”. How does this relate to a team’s own DoD? If a team has their own DoD (for stories, sprints, or whatever you want), it goes deeper into details on how to achieve certain things like beautiful code, or having no (known) bugs. A company wide Definition of Done is not there to replace any of the team’s DoDs, but supplements it with a focus purely on qualities and assets visible to the outside world (that is, not the team). “Non-functional requirements are requirements, which, if not fulfilled, make your product non-functional.” The software we build & deploy has to be of good quality, to be useful. Those qualities are also called non-functional requirements. Making sure those are fulfilled is the responsibility of the platform teams. It’s the responsibility of the product owners to check the quality as part of accepting the story, but they can delegate to specific quality-checkers. Delegating makes a lot of sense for topics which require expert knowledge, such as security or legal requirements. The feature solves the defined problem Happy, sad and bad cases work as they should UX, design, and wording are consistent and good APIs are consistent and good Error messages are helpful and understandable Performance is okay No security and privacy issues No negative impact on platform stability When is the best time? A common concept for security and privacy is to have those built-in, by design , instead of as an afterthought. You can do the same for many other qualities: When defining the feature, you can think about performance implications, limits, API / user interface consistency and usability, architecture and modularisation, and all those things. Which is not to say you should come up with a fixed master plan before you start building, but it often does make sense to give those topics some thought. A similar approach is the Scrum-ish Definition of Ready . Customers are not looking for the most beautiful commerce APIs, they are looking for a product to solve a problem they have. This includes support and documentation, training and consulting on implementation practices, and (obvious, but often forgotten) knowing the feature exists and is exactly what they need to help them out. Taking care that all those things around a new feature are in place can be a bit tricky, because the people creating assets around a product feature aren’t always the ones who developed it. Being involved first-hand in feature development, product teams can help everyone a great deal — that includes giving input, answering questions, or even conducting internal training sessions. All that takes time, and it makes sense to see it as part of what you need to do, to get a feature “done”. Release notes published on all channels Documentation published or updated Tutorials are published Training material is updated Professional services knows about the feature, and best practices for using it in projects Support knows enough to support the new feature Sales and marketing are aware and can explain the new functionality Product marketing is aware and ready to evangelise the feature Customer success is aware, can explain the new functionality and update specific customers that asked for it Is it a good topic to blog or talk about? Do it! The last point that is covered in the feature DoD are not for customers, not for colleagues in other departments; they are for the teams. Taking care of those while still working on a feature makes work in the future easier and faster, and is an investment well worth the efforts. Automated tests to notice when bugs sneak in later (regression tests) Code is easy to understand and clean Architecture is clean, especially modularity and dependencies Logging / monitoring is in place Documentation for on-call to know what to do when things break Analytics for usage, performance, stability Sometimes it is better to be fast than perfect, and the benefits of going live with a new feature early outweigh the downside of not covering that one edge case, inconsistent UX, or documentation that’s still being updated. And that’s okay. The feature can still go live — just be transparent to your team and company that you’re not ‘done’ with work on it. Looking under the hood of the commercetools platform 63 2 Agile Growth Product Development Product Management Product 63 claps 63 2 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-03"},
{"website": "CommerceTools", "title": "adopting changesets for release and changelog automation", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/adopting-changesets-for-release-and-changelog-automation-cbdc36bbdc10", "abstract": "About We're hiring When working on Open Source Software (OSS), documentation is a crucial part of the project. It helps to ease adoption by providing in-depth descriptions on how to get started, how to use advanced functionalities, to communicate best practices with examples, and so on. However, documentation does not end with an initial release. It continuously evolves during the life-cycle of the project. New versions can contain new features, bug fixes, improvements, or even breaking changes. All of that needs to be documented and communicated properly. Traditionally, people expect to find this information in a file called CHANGELOG.md , or in GitHub Releases . A release can include different things: links to commits or Pull Requests related to the new version, contextual information and code examples, emojis, and so on. At the end of the day, it’s just Markdown and it's up to the maintainers of the project to provide necessary and useful information. Certain parts of the release notes often require manual content and review, while other parts can be automated. Ideally, the release notes for your project should have some kind of consistency. This is where tools and automation can come in to help. This becomes very useful particularly in large projects, or in monorepositories (or monorepos for short) with a lot of packages. For instance, the release process including generating release notes can be automated and seamlessly integrated into your Continuous Integration (CI) system. Some popular examples include Semantic Releases , Lerna with Lerna Changelog , and many other community projects. These automation tools can provide by default a lot of context about the release, such as references to Pull Requests, commit messages, list of contributors, etc. At commercetools , we have a bunch of open source monorepositories and as such, we needed a way to make the process simple and effective for all the contributors. So far we’ve been using Lerna and Lerna Changelog . The tools work great and they are very helpful to manage and release multiple packages within the same repository. However, certain steps have to be taken during the release process to ensure that the release notes are properly documented. For instance, merged Pull Requests that should be included in the release notes must have a label assigned to them. This allows Lerna Changelog to reference the Pull Request and group it appropriately in the generated release notes. After running the changelog command, we need to manually add the output to the CHANGELOG.md file and in GitHub Releases. We don’t want to have this step fully automated, as sometimes we need to provide more contextual information — which requires manual writing. While it’s very helpful and nice, the publication of release notes isn’t fully automated and relies on the user knowing what to do. This could be a bit of an issue for contributors who aren’t very familiar with the release process or are afraid to mess things up, so they end up getting discouraged from doing it. Recently I’ve been looking into another interesting project called Changesets . It’s described as: A way to manage your versioning and changelogs with a focus on monorepos It takes a bit of a different approach and it’s designed to work within a monorepository, which is very important for us. There is a nice documentation page about the motivation and design principles behind the project. I recommend to go check it out. What I like about the project is that you document the changes as you develop them. Those changes then dictate which packages will eventually be released. Additionally, each package gets its own CHANGELOG.md file, which is also nice and helpful when searching for changes of a specific package. Using the changesets command-line interface (CLI) on its own is still not super useful. However, combine it with the Changesets GitHub App and the Changesets GitHub Action , and suddenly you have a powerful killer feature. The new workflow roughly looks like this: You open a Pull Request and include a changeset file with a proper documentation of the changes and the desired semantic version bump ( major, minor, patch ). Merging the Pull Request does not trigger any release, instead a new Pull Request named Version Packages is created by the Changeset GitHub Action and includes the release plan. The release plan bumps the versions of all packages and dependent packages affected by the changeset files present in the repository, as well as the updated CHANGELOG.md files in each package. When it’s time to trigger a release, you simply merge the Version Packages Pull Request. The Changesets GitHub Action takes care of doing the actual release, by publishing the packages to NPM, creating the git tags and the GitHub Release notes. The steps mentioned above assume that you have installed the Changesets GitHub App and configured the Changesets GitHub Action . The Changesets project gets many points right, compared to how we previously worked with versioning and release notes, and it fits perfectly with the monorepository workflow that we have. It also simplifies the process of releasing and documenting version changes significantly, making that much more accessible to contributors of our projects. We can only recommend to try it out! Looking under the hood of the commercetools platform 107 Thanks to Tobias Deekens . Open Source Changelog Documentation Release Notes Automation 107 claps 107 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-27"},
{"website": "CommerceTools", "title": "from slack to production", "author": ["Malcolm L"], "link": "https://techblog.commercetools.com/from-slack-to-production-e25d1c214376", "abstract": "About We're hiring We maintain UI-Kit, our open-source component library outside of our product repositories. It has its own Github repository, it’s own CI setup, and it’s own testing suite. Today I want to go through the steps we follow to get a feature from code to production in UI-Kit. The steps are as follows A change is requested A developer makes the changes to the code They open a pull request and request reviews from design and development. They update the changelog and cut an NPM release. Yesterday a designer on my team messaged me on Slack. He found a visual bug in one of our UI Kit components. The margin between inputs in our LocalizedMultilineInput was off by a few pixels. Here is the message they sent me. After receiving the message, I opened up s torybook and took a look at the component. I saw that we show a button when there is more than one line of text. We show an “Expand” button when the component is open, and a “Collapse” button when it’s closed. The component also supports passing in an errors object. This way we can display messages for each individual language. As you can see from the screenshot, the errors and the collapse / expand button displayed on the same line. The problem was being caused by our Spacing component. It was applying a margin to each of its children. When there was no error message, and no collapse / expand button, we were rendering an empty div. This empty div received a margin from the Spacing component. This created the visual bug. To fix the problem, I added the following CSS code to the parent div of the error and collapse button. When the parent div has no children, we remove the margin. We needed to use !important because our spacing component uses !important to apply margin to its children. After checking my fix again in storybook, I pushed my branch to Github. Next, I opened a pull request. I tagged a dev reviewer and a design reviewer. CircleCI ran our linters and tests. As expected, our visual tests caught a diff and Percy’s GitHub integration flagged it as an error. Then I took a look at the visual diff that Percy found. It looked like we made the fix correctly. As you can see, there is less margin between the inputs when there is no expansion control button and no error message. To learn more about how to set up your own visual testing setup, check out the article below. techblog.commercetools.com The designer who caught the regression approved the changes in Percy and our Github checks went green. I merged the pull request. Now our fix was in the master branch. But not in our apps. To get the fix into our apps, we need to cut a new NPM release. To create a new release, we start by updating our changelog and our package.json version. I opened CHANGELOG.MD and added an explanation of the change. I also opened package.json and updated version . Next, I opened a PR. This time to get the changelog and package.json changes into master. Once approved, I merged it. Now came the last step, to cut the release. We have a job in our CircleCI setup that runs the following command every time a new tag is pushed to Github. So I ran the following commands to create a git tag and push it to master. Pushing this tag to Github triggered CircleCI to run our jobs and cut the release to NPM. Now we can update our apps to the latest version of UI-Kit, and get the fix into production. That’s our full component development lifecycle — from Slack message to Production. For more information on the what and why behind our component library, check out the following techblog.commercetools.com Looking under the hood of the commercetools platform 34 Programming JavaScript Software Engineering Work UX 34 claps 34 Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-12"},
{"website": "CommerceTools", "title": "integrating siri in existing e commerce apps", "author": ["Nikola Mladenovic"], "link": "https://techblog.commercetools.com/integrating-siri-in-existing-e-commerce-apps-addd338ac6ab", "abstract": "About We're hiring Voice-activated assistants and digital assistants are becoming more and more present in eCommerce industry. In fact, trend watchers estimate that 30% of browsing will be screenless by 2020. In 2018, Apple announced that there were 500 million devices in the world that used Siri voice assistant. That has a huge potential for any brand offering an iOS app for their customers. Being able to quickly reorder items they frequently buy, or add something to a shopping list, without having to use the screen and open the app, provides more convenience for customers, and revenue for brands. With iOS 12, third party apps got access to some of the Siri capabilities. Apps are now able to use predefined intent categories to allow users to record custom phrases, and use them with Siri to perform frequent tasks. Currently available e-commerce related categories are: order , book , and buy . When thinking about adding Siri support to an existing iOS app, a good first step is to think which tasks make sense to be integrated with voice commands. Frequently used tasks are a good candidate for voice integration. An e-commerce related example we chose to add to our Sunrise iOS app is reordering items customers need on a regular basis. The first step is to include the Add to Siri button on an order details page. The customer can pick an order from the order overview screen, and then create a custom shortcut. Actions triggered by a customer using Siri (known as intents ) are executed in a separate process from the associated iOS app. That means it doesn’t have access to the same sandbox and data. As a result, the process launched to handle the Siri request won’t be able to access customer’s profile, create an order, or even know if the customer is logged in. In order to solve this issue, the commercetools Swift SDK supports keychain sharing. If it’s turned on for both the main app target, the Siri Intents and Intents UI Extensions, you can easily configure the SDK to use the shared keychain. That way, the extension running the order task will be able to access customer’s tokens, and to communicate with the commercetools API to get previous order details, and create a new one, the same way the main app would. When defining custom intents and responses, you’ll notice that user confirmation is mandatory for any of the e-commerce categories. It’s important that a customer is aware of actions their voice commands will make, and a final confirmation before placing an order definitely helps. To make it easier for customers to understand details like what items are being ordered, how much does it all cost, etc, a good practice is to add another target: Siri Intents UI Extension. The UI Extension can handle one or more intents, and it will show a custom UI on customer’s phone before the action (in this case order) is confirmed, as long as the voice command wasn’t issued using HomePod (which obviously doesn’t have a screen). Voice assistants are definitely getting more spotlight in e-commerce domain. For brands already using the commercetools platform, it’s fairly easy to start offering convenient voice commands like ordering, adding items to a shopping list, checking shipping statuses, and others. Swift SDK offers convenient integration options with Siri Intents and many other iOS app extensions. Feel free to check out the actual implementation details referenced in this post at Sunrise iOS / watchOS GitHub repo . You can also get a beta demo build including Siri reorder integration using this link. Looking under the hood of the commercetools platform 53 Thanks to Freddie Karlbom and Malcolm L . Apple iOS E Commerce Software Commercetools Siri 53 claps 53 Written by iOS Software Engineer Looking under the hood of the commercetools platform Written by iOS Software Engineer Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-02"},
{"website": "CommerceTools", "title": "five practical tips when using react hooks in production", "author": ["Tobias Deekens"], "link": "https://techblog.commercetools.com/five-practical-tips-when-using-react-hooks-in-production-990a79745229", "abstract": "About We're hiring Since React hooks were introduced in early 2019 with React 16.8.0, we at commercetools were early adopters and have since been continuously refactoring our code base to make use of them. React hooks allow the use of state and other React features without writing a JavaScript class. By using them you can “hook into” the underlying lifecycle and state changes of a component within a functional component. React hooks helped us to simplify our code base by extracting logic out of components and composing different functionality more easily. Furthermore, adopting React hooks resulted in many lessons for us, such as how to structure our code better through the use of constant iterations on existing functionality. We are sure we will learn more tricks and improved ways to use React hooks, but we’re curious what yours are! Find us on Twitter and GitHub . Readability: via a smaller component tree by avoiding render props and Higher-Order Components (HoCs) Debuggability: using improved visual representation and additional debug values provided in React Dev Tools Modularity: an easier way to compose and build reusable logic due to the functional nature of hooks Separation of concerns : React components focus on the visual representation while hooks expose encapsulated business logic It is quite easy to start using React hooks in your Functional Components. We quickly dropped in a React.useState here and a React.useEffect there and moved on. This however does not fully utilize all advantages we could see from adopting hooks. By extracting a React.useState into a small use<StateName>State and a React.useEffect into a use<EffectName>Effect , we were able to: Keep our React components even leaner and shorter Allow reuse of the same hook across multiple components Provide more descriptive naming within React Developer Tools (for instance, State<StateName> over just State which becomes unorganized with multiple usages of React.useState within a single component Extracting hooks also makes reusable and shared logic more visible across different parts of the application. Similar or duplicated logic is harder to spot when only inlining hooks. The resulting React hooks can be small and contain little logic such as a useToggleState . On the other hand, bigger hooks like a useProductFetcher are now able to contain more functionality. Both cases helped us simplify our code base by keeping React components leaner. The example below illustrates building a small React hook to manage selection state. The advantages in encapsulating this functionality quickly become apparent when you realize how often a selection logic occurs inside of an application, for example to select a set of orders from a list. The built-in React.useDebugValue is a lesser known hook which can help with debugging and can be useful in shared hooks. Shared hooks are custom defined hooks which are used by multiple components in your application. However, it is not recommended for use in every custom hook as built-in hooks already log default debug values. Imagine building a custom React hook to evaluate if a feature should be enabled or not. The state backing this system could come from various sources and would be stored on a React context accessible through React.useContext . To help debugging, it would be helpful to know what feature flag name and variation was evaluated in the React Developer Tools. A small React.useDebugValue can help here: When now looking at the React Developer Tools we would see the following info about flag variation and flag name , and the resulting status of the feature: Note that whenever a built-in hook, such as React.useState or React.useRef , is used in a custom hook, it will already debug its respective state or ref value within the React Developer Tools. As a result React.useDebugValue({ state } , is not incredibly useful. When we started adopting and using more React hooks, we quickly ended up using about 5–10 hooks within a single component . The type of hooks we use varies a lot. We might use 2–3 React.useState hooks, then a React.useContext (for instance, to get information for the currently active user), a React.useEffect and hooks by other libraries such as react-router or react-intl . The pattern above repeated and small-ish React components turned out to be not so small after all. To avoid this we started to extract these individual hooks into custom hooks, depending on the component or feature. Imagine building an order creation feature. This feature is built using multiple components as well as hooks of different types. These can be combined into custom hooks, making their consumption more convenient. We often resorted to React.useState as the default hook to keep the state in our components. However, over time the component state might need to get more complex , depending on the new requirements like having multiple state values. In certain cases, using a React.useReducer hook can help to avoid multiple state values and simplifies the state update logic. Imagine managing a HTTP request/response state. This could require multiple state values for isLoading , data , and error . You can instead have the state managed by a reducer and have specific actions to update the state permutations. This ideally also guides thinking of the states within your interface as a state machine . A reducer passed to React.useReducer is similar to a reducer in Redux , where it receives the current state of an action and is meant to return the next state. The action contains the type and payload to derive the next state. In a contrived counter example a reducer could look like this: This reducer can be tested in isolation and then used within a React.useReducer with the following function: We can further apply what we learned in the previous three sections by extracting everything into a useCounterReducer . This improves our code by hiding the action types from the view. As a result preventing lacking implementation details into the views while also enabling additional debugging. This may seem a bit counter-intuitive at first but bear with me. Over time code bases adopt different patterns . In our example, these patterns include HoCs (Higher-order Components), render props and now hooks. When migrating code it is not desired and unfeasible to change everything at once . As a result we needed a migration path towards React hooks without requiring large rewrites. This can be challenging as changes naturally tend to grow size and concerns — and that’s something we try to avoid with our adoption of hooks. Our codebase uses Class Components and Functional Components. No matter which type of component was used, we wanted to share logic through our React hooks . We first (re)implemented the logic in hooks, before we exposed small HoCs that internally use the hooks and expose their functionality Class Components. As a result we ultimately have the logic in one place that can be used by different kinds of components again . The example above injects the functionality of a useTracking hook into the wrapped component. This incidentally allowed us to also split adopting hooks and rewriting our tests in older parts of the system. Still offering an early migration path towards hooks in all parts of our code base. Those are some ways React hooks helped with our quality of life, and can hopefully do the same for you in your development workflow. If you have any tips or questions, reach out to us on Twitter or GitHub ! Looking under the hood of the commercetools platform 629 2 React React Hook Hooks Development Coding 629 claps 629 2 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-09"},
{"website": "CommerceTools", "title": "ideas on writing a code generator", "author": ["Beniasaad Achraf"], "link": "https://techblog.commercetools.com/ideas-on-writing-a-code-generator-b7f474e13427", "abstract": "About We're hiring While in most industries, there is fear of automation and the impact it might have on the job markets, software engineering is the total opposite. The whole purpose of software engineering is to automate tasks that can be done by the machine, then come the question, isn’t writing code just another task that can be automated? 🤔 In general, no. If your code executes some kind of complex logic it’s already difficult to write, maintain and make sure it does what it is supposed to do. However, most of our code is just simple and void of logic. We spend a lot of time writing entities and config classes whose sole purpose is to carry data. Since they don’t have logic they can be generated, the only constraint is that their structure should be documented somehow somewhere. While the concept of code generation itself is widely spread among developers, I still think that people underestimate the power of it, or overestimate the effort involved in writing code generators. So I’m gonna take a specific case where information about the code you want to write already exists in a structured format (easy to parse), such as JSON, Yaml, CSV… In this case, I’ll build the following very simple chain to generate the code. Let’s take a developer for example, “Dave”, in some project he has a couple of properties, that are already defined in some property file, such as the following: To access these properties from a java project, he uses something like: This code wasn’t safe enough by Daves standards, since with each refactoring some property may change the name, and some code trying to retrieve it somewhere might fail silently. 😢 To fix that Dave wanted to opt for a simple solution, which is writing a java file that we update each time we update the properties file. It would be something as follows: This way any change in naming would result in a code that doesn’t compile anymore. Pretty good, huh! Except that there are dozens of these properties files throughout the app, and some are huge. So he starts thinking; wouldn’t it be better to just write a piece of code that does that for him? Does the code have a structured format(can be parsed)? => YES ( Properties file have a structure that makes it easy to associate keys with values ) Is it possible to write a parser for this format? => YES (in this lucky case the parser is included in the JVM) That means that in this easy case (for demo purpose), we already have the first two blocks of our chain. In other cases, it might be that you have to build a parser which is not always an easy task. If you use popular formats (JSON, Yaml… ) to store your data however, you can find open-source parsers. All you have to do is to implement the tree traversal logic. So at this point you have the data already parsed, now we should evaluate what templating engine we should use to put our data in the file format (Java class for this example). There are plenty of templating engines, such as Handlebars , Mustache , etc… They do a wonderful job but still have plenty of weaknesses. Most of them lack IDE support, they also contain placeholders that expect some model (Beans in the case of java for example) which is tightly coupled to the template and soon become difficult to change on demand. One other issue that arises is that some of them support some DSLs to write simple functions (for example to format strings…), which are separate languages, and the IDE can’t do much to help you there either 🤷‍♂ These reasons were enough for me to avoid these templating engines at all costs. If you want to write code in a single language like java or Kotlin, there are projects such as javapoet and kotlinpoet , that make writing code generators easy and safe. The only thing I have against these tools is that they are too verbose, it takes a lot of code to generate simple code blocks, also it isn’t guaranteed that you’ll find a file generating libraries for every language. I used some of these in the past, but while working in a polyglot environment where we had to generate code for java, typescript, etc., these tools were limited to only some of the languages we wanted to generate. The solution I like most for templating is using Kotlin multiline string for templates. I like it because it’s simple yet powerful because Kotlin strings allow Kotlin expressions inside which gives you access to all the language features Kotlin offers inside your template. Still, even Kotlin wasn’t perfect for code generation and here is why. let’s take a look at the following Kotlin code The code above generates the following config class. In case your OCD didn’t help see what’s wrong, the indentation isn’t added correctly, this might work just fine in case of java, but in languages such as Python, this is unacceptable since indentation itself has a semantic. So how can we fix that? To fix the indentation issue, I wrote a small utility function that will help us keep the indentation, just surround your block with < > characters, and your code will be indented correctly. Here is an example: …and here is the resulting generated code Beautiful, isn’t it? ❤️ Now if you have blocks inside other blocks you can rest assured that the indentation is kept in the generated code. Now we have a valid code generator, but since you want these files generated at a build step, we will evaluate how we can associate the code generation to a phase of your build process. At this point, we can already extract data from a specification file (the properties file in this case) and put it in a template. All is left is to wire the input and outputs, you can do this via a CLI. But in the java world you usually end up with a fat jar for this CLI, and usually, they don’t play very nice with build tools since they don’t have life cycles. This is why I prefer Gradle plugins. There are plenty of resources of how you build a plugin, but it’s out of the scope of this article, however, I already built a demo plugin. For those who are curious https://github.com/brainya/typesafe-config/tree/master/tools/gradle-plugin It’s not published so don’t expect it to work in your project, but the functional test already shows how it works and verifies its outputs. …and that’s it! Every time you build your project, your files are generated, and you have your type-safe properties in the form of java classes. 🥳🥳🥳 So in this article, I took a very simple example where we extract transform and generate files, also I gave a sample code of how you can build a Gradle plugin. To make this task part of your build script, the example can be generalized for other use cases. For example here at commercetools , we use Raml specs instead of the properties file as a source of data to generate Rest API code. This allow us to ensure consistency in services since you have the spec as the source of truth for both documentation and code. If you are interested in this use case, please check https://github.com/vrapio/rmf-codegen Sample code for this article at https://github.com/brainya/typesafe-config Like what you read? follow me on Twitter , LinkedIn Looking under the hood of the commercetools platform 124 Kotlin Gradle Programming Software Development Android 124 claps 124 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-05"},
{"website": "CommerceTools", "title": "adding consistency and automation to grafana", "author": ["Holden Omans"], "link": "https://techblog.commercetools.com/adding-consistency-and-automation-to-grafana-e99eb374fe40", "abstract": "About We're hiring Many DevOps operations utilize and rely on Grafana as an essential tool to monitor production environments in real time. However, mission critical dashboards for monitoring are often made “by hand” and need manual configuration if and when new environments are added. At commercetools , we support multiple cloud environments and regions for our platform. So when rolling out a new environment, it can mean new data sources being added and multiple dashboards needing to be configured. Depending on the task, this can be time consuming for a team member or can lead to inconsistencies. One way to limit the time it takes to make configuration changes is to leverage automation. This can help to keep things organized and consistent to make everyone's life easier. Plus, consistency keeps graphs looking nice and sometimes that's just as important. 😉 First I will summarize some of the available methods and tools, then provide a quick example. At commercetools we leverage Terraform to help maintain our deployments. The terraform team maintains a Grafana Provider which enables the configuration of data sources and dashboards via terraform. The Grafana Provider can manage the following resources: Dashboards Data Sources Folders Organizations Alert Notifications An example terraform configuration taken from their provider page : Using terraform to manage your grafana instance will allow you to spend more time on other tasks and reduce the difficulty of deployment. Grafana allows the import/export of dashboards via JSON data. This is quite nice, however, it can be cumbersome to modify and write by hand. There are a couple different libraries/tools that allow the generation of these JSON definitions via code. This can make it easier to perform complicated logic or provide looping over multiple data sources. Most of the libraries perform the same way, so most look and feel the same. They provide an abstraction layer to generate the needed dashboard JSON. Choosing one over the other can sometimes come down to language preference or features needed. Here are some of the more promising ones I found: Below you find an example from Crafana, a Crystal-Lang shard which allows building of dashboards. For those of you who don’t know what Crystal is, its a great low level scripting language. Focused on speed and readability its tag line is: Fast as C, slick as Ruby Check it out at https://crystal-lang.org/ First we want to require crafana and make an array of the data sources. We will use this to iterate through and make rows for each source. Crafana makes building new dashboards easy via the Crafana::Builder class. Adding a new dashboard is simple. All we need is the name to begin with, we will also add some tags and set the default time range. Within this loop we want to make a new row for each of the data sources we defined above Now we have a row for each data source that we can define sub graphs for. Graphs can be added via the add_graph or add_single_stat method. When you are done configuring the panel and dashboard, calling to_json for the dashboard creates output that can be imported directly. Here is the full script: Here’s what the JSON output creates: Now to add a new data source, we just update the initial array to add Prometheus Staging to it. Re-running generates new JSON for us to import, which now will include the new staging data source! This is just the beginning, but scripting these dashboards and panels allows you to bring the power of automation (and consistency) to your Grafana dashboards. Once again here is the list of some great libraries you can use to achieve this: Grafana Scripted Dashboards (JavaScript) GrafanaLib (Python) Crafana (Crystal) Grafana Dash Gen (Python) Thanks for reading! Looking under the hood of the commercetools platform 168 1 Thanks to Malcolm L . Grafana Crystal Lang DevOps Automation Metrics 168 claps 168 1 Written by Experienced DevOps Architect, Individual Contributor, and Crystal-Lang enthusiast with a history of working in the information technology and services industry. Looking under the hood of the commercetools platform Written by Experienced DevOps Architect, Individual Contributor, and Crystal-Lang enthusiast with a history of working in the information technology and services industry. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-25"},
{"website": "CommerceTools", "title": "reverse image search with machine learning", "author": ["Mohit Nalavadi"], "link": "https://techblog.commercetools.com/reverse-image-search-with-machine-learning-92786a07c142", "abstract": "About We're hiring The Machine Learning team at commercetools is excited to release the beta version of our new Image Search API. Image search (sometimes called reverse image search) is a tool, where given an image as a query, a duplicate or similar image is returned as a response. The technology driving this search engine is called computer vision, and advancements in this field are giving way to some compelling product features. To build an image search system, we first need a way to find meaning from image data. To a computer, an image is a 3-D matrix consisting of hundreds of thousands of numbers, representing red-green-blue (RGB) pixel values. But to a human, an image is an arrangement of semantic patterns — lines, curves, gradients, textures, colors — all of which integrate into some meaningful idea. A relatively new computer vision model, called a Convolutional Neural Network (CNN), can be used to bridge this gap between man and machine — the CNN extracts latent meaning from images. The most powerful advantage of using a CNN over other machine learning techniques is that it models the fact that nearby pixels are often locally correlated with one another (if one pixel in an image is part of a cute dog, odds are the surrounding pixels are as well). Albeit a simple idea, this is an incredibly powerful algorithm in practice, since previous image recognition technologies were constrained by detecting only color or very low level shapes for features. The CNN decides which features are important automatically in its training process, where it learns what is statistically significant vs. irrelevant from millions of example images. CNNs are so powerful that some form of them have been the winning algorithm of the the largest image recognition contest in the world, ImageNet , every year since 2012. You will find CNNs as the eyes behind self-driving cars , facial recognition on your phone, cancer detection software from CT scans, and even in Google Deep Mind’s Alpha Go . A CNN is a series of layers, where each layer is a function which takes the output of the previous layer as an input. An image is input into the first layer, and each time it passes through a new layer, the most meaningful features are extracted and passed forward. After each layer, the semantic representation of the new image becomes more dense. Generally, the first layers of the network extract low level features, like edges, and later levels combine these features into more abstract, and often meaningful shapes or ideas — like that wheels always go at the bottom of a car. Figure 2, below, shows that the first layers identify sharp edges in the image, the middle layers combine these shapes into wheels and the car body, and the final layer makes a coherent assertion that this is an image of a car. In practice, a CNN may have dozens of layers. Each layer of the network applies a filter to the input, and what these filters do to the image is part of the alchemy of deep learning. Each filter is a set of unique weights, and those weights are multiplied by the pixel values within a small window of the input image, generating the new image (this process is called a convolution). The new image is “deeper” in the z-dimension (as opposed to the length and width), and this depth encapsulates learned features. Was it Einstein who said , “If you can’t explain it simply, use a GIF.”? Figure 3 below shows the application of a filter to the input, the blue image, compacting it into the green image. A 3x3 window of the input is multiplied by the filter weights, and a single value is output. So the information within a 5x5 image gets mapped to a more dense 2x2 version. Additionally, the resulting condensed image is sub-sampled with another filter which simply finds the maximum or average value in the window, to further reduce its size. The final layer is collapsed into a single dimensional vector, an array of numbers, representing the features extracted from the image. This feature vector is the basis for our similar image search. And voila, we have a basic CNN! The values of the weights in the filters are determined during the model training process, where thousands, or even millions of images are passed through the network. Each image has a label of what is in it, like “cat” or “dog”. Every time an image is passed through the network, its feature vector is mapped to the labels, and a probability score is generated. The CNN might say “there is a 75% chance this is a dog, and a 25% chance this is a cat.”. With each pass, a calculation is made about which direction we can push the weights of the filters to generate a more accurate probability score — we want the model to be more confident in its assertions. Once we’re satisfied with the score (usually when it stops improving, after several thousand passes), we end the training process, and use the model only for predictions on new, unseen images. So far we’ve trained the network to see images the same way we see as relevant. But we still need a mechanism for the computer to compare the extracted feature vectors for similarity. If you or I were to decide if two images are similar, our brains would probably determine how similar certain features are within each image. If two images both contained furry, four-legged animals, we would be more likely to pair those together, relative to a third image of a smooth, two-legged reptilian animal. Similarly, for the CNN model, we compare resulting feature vectors against each other by measuring the distance between their vectors. Image feature vectors with small distance scores suggest their underlying image contain similar contents. If two images both contained furry, four-legged animals, we would be more likely to pair those together, relative to a third image of a smooth, two-legged reptilian animal. Euclidian distance (the length of a line between two points), although simple, fails as a good metric in this context. One reason for this is that for our feature vectors, it is better to measure the correlation between features, rather than the features themselves. The cosine distance does this by measuring the angle between two vectors. We can expect a feature vector to between between 100 and 4000 dimensions, so lets simplify things to the 2D world. Figure 5 below illustrates the distinction between euclidean vs. cosine distance. The dog vectors point in similar directions, meaning their two features vary together more, relative to the dinosaur vector (angle θ1 is smaller than θ2). Despite this, the linear distance between the blonde dog and the other two animals is still roughly equal. Rather than identifying vectors with values which are of similar magnitude (Euclidian), in image search we prefer finding vectors with the most similar patterns (cosine). Great, we have a way to generate feature vectors from images and compare their similarity, but how do we serve this application to users, and scale? As is the case with all production machine learning projects, the next step is to build software engineering infrastructure around our predictive model. In practice, this is a matter of vectorizing hundreds of thousands of product images, indexing those feature vectors into a relational database, and serving it as an API. To index a customer’s project’s images, we first run an asynchronous Python celery process for them. This process iterates through all unique images within all of the project’s product variants and creates a reference for it’s unique product and URL information. We vectorize each unique image URL using a CNN model created with Python’s deep learning library, Keras . The model is hosted on Google ML Engine to achieve rapid and responsive scaling for handling changes in demand. After evaluating image retrieval performance of several CNN models, our team decided to use a pretrained VGG16 network, which achieves 92.7% accuracy on ImageNet, a dataset of over 14 million images belonging to 1000 labels. As we aggregate feedback, we plan to improve our model using more e-commerce specific data. To allow for efficient storage and access of a large and growing amount of image data, we used a PostgreSQL database. We store each feature vector and its URL/product reference in two separate tables, since there can be a many-to-many mapping of products to images. A great feature about SQL is the ability to perform basic arithmetic within a query. Since cosine distance in this context is a linear combination of column values, we perform that operation within a SELECT query when comparing all image feature vectors to a new input image vector. This allows us to only return the closest vectors from the database as a response, reducing I/O bound latency. Once a customer’s project is indexed, he or she sends an image as an HTTP request ( see docs for an example request) to us, the image is vectorized and queried against our database for similar images, and the top results are returned as image URLs in a JSON response. At this point, we are bound by our own creativity. The similar image response object could be used by a shop owner to make a more convenient app, where a customer could snap a photo of a product they like, and see if the shop carries something similar. He or she could also check for duplicate images in their database, or to verify if someone else is stealing images from them. From product recommendations to image authentication and more, our new Image Search API opens a host of new opportunities to explore, and we’re excited to see how our customers put the tool into practice! Looking under the hood of the commercetools platform 275 Machine Learning Convolutional Network Ecommerce Image Processing Neural Networks 275 claps 275 Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-21"},
{"website": "CommerceTools", "title": "kubecon cloudnativecon europe 2019 in barcelona", "author": ["Marina Ilyina"], "link": "https://techblog.commercetools.com/kubecon-cloudnativecon-europe-2019-in-barcelona-9720aad2aa61", "abstract": "About We're hiring T his year we’ve attended KubeCon + CloudNativeCon 2019 Europe in Barcelona. It’s the largest Kubernetes conference, and this year more than 7000 people attended. The conference is so big, and the number of submitted talks about was so overwhelming, that another conference Cloud Native Rejekts was held on May 18–19, with the talks that didn’t make it to the KubeCon. Something we can consider for the future, the list of talks looks promising, but, unfortunately, the conference didn’t publish videos or slides. This year Kubernetes has its fifth birthday, and it’s really awesome to see that technology became the new industry standard. There was a party to celebrate, have a beer and share old stories about how the pioneers were evaluating and adopting Kubernetes even before it reached 1.0, and what a long way we all did in 5 years. This time though we got a feeling that the development of Kubernetes itself is slowing down, focusing on stability more than on new features. So if the previous Kubecon was mostly about new features in it, this year was more inclined towards the ecosystem around the Kubernetes. One of the most interesting parts for me were the operators for managing stateful apps. Those are now versioned, and SDK for writing them with Golang or Ansible is here. We highly recommend listening to Neo4j database journey to the custom operator by Johannes Unterstein: And great news, a wild open source version of Elasticsearch Operator appears! A lot of other community maintained operators are available at https://operatorhub.io/ . The hottest topics in the surrounding ecosphere are, of course, service mesh, tracing, and security. As more service mesh technologies arise, it’s easy to become locked with one implementation provider. So the industry players collaborated and created Service Mesh Interface specification which is a set of traffic API for the Kubernetes use to operate. The full article about it can be found in Microsoft blog: https://cloudblogs.microsoft.com/opensource/2019/05/21/service-mesh-interface-smi-release/ And while we are at it, a very interesting introduction to the zero-trust service-mesh, covering the theory as well as practical implementation using Calico policies using Felix agent and Envoy plugin Dikastes. O ne of the parts every DevOps, SRE or operations engineer are passionate about is observability , and it’s one of the topics which get the most attention at the conference. But before diving into every special topic, let’s keep in mind Tom Wilkie’s and Frederic Branczyk’s “ What Does the Future Hold of Observability? ” keynote talk. They’ve made three predictions, so let’s hold them to their word in a year or so! With Prometheus being an industry standard, there are still questions about long-term metrics storage. On the conference we’ve learned about two of them: Thanos — a set of components on top of existing Prometheus deployments. It provides a global overview of all metrics from connected Prometheus servers and unlimited storage space using every cloud object storage API. We highly recommend to see Autoscaling Multi-Cluster Observability with Thanos and Linkerd by Andrew Seigner and Frederic Branczyk. M3 — a distributed time series database from Uber, which also acts as a long-term metrics solution and universal query layer. It’s compatible with Graphite metrics, and the provided Kubernetes operator takes care of replacing the failed storage nodes and firing new ones. More information can be found in the Rob Skillington’s “ M3 and Prometheus, Monitoring at Planet Scale for Everyone ” Tom Wilkie and David Kaltschmidt from Grafana Labs started to work on Loki at 2018 with a goal to create a simple scalable solution for logs. We all know, that current logging stacks typically include full-text indexing, and therefore hard to scale and operate. So Loki dropped indexing logs content, adding Prometheus-like multidimensional labels and indexing them instead. Loki is also available as datasource in Grafana, which allows using one interface for full application diagnostic — metrics, logs and tracing at one place. Tom Wilkie’s talk Grafana Loki: Like Prometheus, but for Logs is an absolute must for everyone tired of Elasticsearch monopoly at the logs area. The project is currently in alpha, but going beta as soon as possible, with LogQL aggregations and alerts on them planned. With Grafana integration and low-cost maintenance, Loki has the potential to become the new logging and observability standard. Another famous in the community Grafana Lab employee, David Kaltschmidt summarised the best practices for building the dashboards and reducing tech pain and cognitive load in his talk Fool-Proof Kubernetes Dashboards for Sleep-Deprived Oncalls . We would highly recommend to see it if you are struggling to have an overview of your multicloud environments and different clusters. If you are not yet into functional programming, we would recommend “You Might Just be a Functional Programmer Now” by Cornelia Davis. She is also very charismatic and passionate about the subject. And Saad Ali from Google gave a talk about The Magic of Kubernetes Self-healing Capabilities , where he speaks in details about declarative APIs of various Kubernetes components. Spoiler alert: there’s no magic, but reconciliation algorithms, which can take quite some time, but still far more superior in the ever-changing cloud world. IBM open sourced Razee , a project to automate and manage the deployment of Kubernetes resources across clusters, environments, and cloud providers. Jason McGee gave us a glimpse at the day-to-day life of their SRE team, which managed thousands of clusters with 25 people. The talk is very inspiring, and that’s what we all in the industry should aim for: Sponsored Keynote: What I Learned Running 10,000+ Kubernetes Clusters . In the Sponsor’s Showcase, we’ve found several young interesting projects, which we would like to do tech time at some point and adopt them to our infrastructure. Here they are: Falco — Sysdig project for container visibility, intrusion and abnormality detection. Nuff said, security comes first. Open Policy Agent — general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack. It can run as a sidecar (so many sidecars!), host-level daemon or integrate with the application. It uses Rego as a configuration language for policies, and policies can be tested before applying: Unit Testing Your Kubernetes Configurations Using Open Policy Agent — Gareth Rushgrove, Docker . It was not the only talk about Open Policy agent, have a look at the schedule to learn more about it. We’re really excited to see how the security ecosystems thrive in the Kubernetes community nowadays. Fluent Bit — new and lightning fast written in C. It’s part of the Fluentd ecosystem and operates the same “input - parser — filter — buffer — output” model, but is written in C and therefore very fast and lightweight. It’s shipped with several C plugins and allows to write custom plugins with Golang. At their talk Fluent Bit: Extending Your Logging Pipeline with Go Warren Fernandes and Jason Keene gave a deep dive into the Fluent Bit Go interface and showed some neat examples. Pulumi — infrastructure as code, but this time the code is Python, Golang, Javascript or Typescript. They have an free version and support both the major cloud providers and Kubernetes services and abstractions. The project itself claims that it’s not a Terraform competitor, but aims to eliminate all the bash scripts it usually comes with and make it easy for developers to join the operations efforts. Mesosphere Kubernetes Engine — we were really surprised to see Mesosphere stand at the KubeCon, but once a competitor, they now offer a solution to install and maintain Kubernetes for different cloud provides. Unfortunately, we were not able to find an open source version. We know that there is a lot to take in, and we didn’t cover A LOT of topics presented at the conference. But take your time, and if you are interested in other themes, like CI/CD, tracing, networking or serverless and many more, give the full conference schedule a look, we’re sure that everyone in the modern IT community will find something interesting. Thank you, everyone, who participated, and hope to see you next year in Amsterdam! Looking under the hood of the commercetools platform 31 Kubernetes Kubecon 31 claps 31 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-13"},
{"website": "CommerceTools", "title": "virtual reality and commercetools", "author": ["Nicholas Speeter"], "link": "https://techblog.commercetools.com/virtual-reality-and-commercetools-79612cba61dc", "abstract": "About We're hiring The commercetools development lab does not have a modern home overlooking a forest. This experience exists only in an immersive state through two lenses and a small covered headset. Yet the house and landscape look real and can be traversed, interacted with, and you can even purchase objects throughout the environment. Virtual Reality (VR) creates and displays this totally immersive perception. VR and commercetools integrate together deftly to generate a 3D view of your products in the enveloping environment. VR is a new commerce medium experiencing electric growth. A customer can view a product, or even an entire custom virtual showroom, in interactive 3D from the store or their own home. Items can be carted with a virtual click and checked out later via mobile device or “in game”. commercetools carts are stored and easily accessible for multi channel use (web, iPhone, Android, tablet). Virtual Reality is an excellent way to display furniture, toys, electronics, cosmetics, and boxes with product information in a VR showroom. Creating a captivating VR experience increases the likelihood that a customer will keep the items they’ve carted and follow through with checkout, and also offers a unique interaction with your brand that encourages shoppers to keep coming back. A developer can augment menu and cart icons in the immersive view to provide a shopping experience. When an object in the environment is clicked, a popup appears with the product description, price, and an “Add To Cart” button. To develop the UI for the menus in your scene, try Unity’s UI Canvas . When the cart icon is clicked, a popup can then be manifested with the carted items. Create a commercetools project to associate with the VR application’s cart and items. Get started with commercetools . On object click, the matching commercetools product can be queried for from a commercetools project. Use of the commercetools http-api to get the matching product is recommended for the best VR development experience : https://docs.commercetools.com/http-api . Simply associate the clicked object with a product id in the VR application code and then use the /products/id endpoint to retrieve more information to display (price, description, variants, etc.). On Oculus rift one could use C# and HttpWebRequest to retrieve the product information from the commercetools project. The commercetools http-api requires a valid authorization token for all requests. So be sure to use the customer password flow explained in the following links: 1. https://docs.commercetools.com/tutorial-mobile-spa 2. https://docs.commercetools.com/http-api-authorization.html#password-flow On click of the object a menu pops up in VR. Click through the product’s variants and select one to add to cart. GraphQL can also be used to query the commercetools APIs: https://docs.commercetools.com/graphql-api Carts can be created and checked out “in game” from the VR cart icon, or the cart can be retrieved and checked out on another mobile device. The VR cart is saved to commercetools. The below example shows how to checkout the cart via mobile device with the commercetools Swift SDK: https://github.com/commercetools/commercetools-ios-sdk . In conclusion, it is very easy to integrate commercetools into an existing VR application utilizing the commercetools http-api . The UI can be as simple as popping up VR canvas menus when an item is in close proximity. Items can then be carted and checked out in game or via another channel. This commercetools integration will work with many VR Platforms. Looking under the hood of the commercetools platform 128 Thanks to Rob Senn . Virtual Reality Commercetools VR Ecommerce Oculus Rift 128 claps 128 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-02"},
{"website": "CommerceTools", "title": "bring your apis from good to awesome with ux research", "author": ["Sven Heckler"], "link": "https://techblog.commercetools.com/bring-your-apis-from-good-to-awesome-with-ux-research-15980ad18d24", "abstract": "About We're hiring When you hear about “UX research”, “user experience” or “user interface design”, the first things which might come to mind are “websites”, “user tools”, “software interfaces”, “intuitive interactions” and maybe “beautiful graphics”. All good and true, but you might have forgotten some of our most favourite “users” — developers! In the API space, we build something on a machine for a machine to use and this is wrong because there are people on the other side of API clients. — Ronnie Mitra , Director of Technology at Publicis Sapient The best way to create outstanding APIs is to write them with the user in mind. There is no point in designing an API only understandable to the developers who created it — after all, the real stakeholders are the developers who consume and interact with the API. Therefore, a good API must be easy to use for them. When working with an API-first product ( like the platform that we have ), you can’t forget UX research when designing APIs. With this blog post, you will learn how we at commercetools use UX research for our API solution and how this moves the API to the next level, including how we integrate the user research process into our API development. This article also discusses some methods and how to apply these to your project. This blog post is the first in a series of case studies where we will present the things we have tried and what has worked for us at commercetools . Understand and frame the problem First and foremost, you need to understand the needs and pain points of your API consumers. Only when you understand your users’ problems and are able to answer their questions you can create a strategy for a successful API solution. The best way to frame the problem is by doing research. Discover the problems Understanding the problem goes way beyond just asking users what they want or like. Remember, when people wanted faster horses ? Here are some ideas to get to actual root of issues and frustrations faced by users: Interviews Interviews are a great way to both get direct feedback and build good relations with your API consumers. Contact users from the target audience and ask them about their usage behavior and issues that they might have faced with your current or new solution. This will help you to gain better insights from the consumer’s perspective. Don’t forget to pay attention to their nonverbal reactions. Nonverbal communication often tells a different story and reveals how a consumer really feels about the solution — that frown you see can say more than anything they’ll tell you. This is important since some participants are too nice to give direct criticisms. With interviews, you gather a lot of qualitative data to allow you to get closer to discovering the needed solution. Qualitative Surveys Surveys are questionnaires that focus on a specific topic that you want to research. The better formulated your questions are, the more valuable the answers will be. After defining the questions the survey needs to be sent out to as many users of your key audience as possible. You’ll get to your users when they actually have the time and energy to share their thoughts. Plus, who likes scheduling meetings? You receive feedback as soon as the user has finished the survey. Afterwards, it’s really easy to translate the results into understandable infographics that summarize the outcome of the survey. Monitoring When you have an API solution that is already in use, you can track user interactions to get insights on their behaviors. For example, you can track which update action is performed the most on a specific feature, whether they prefer querying data by ID or human readable names, or simply whether a new feature is used at all. You can also elaborate on user flows to answer questions like: How much time did a user spend on the documentation before they asked support for help? While these statistics can help you make many decisions, keep in mind they will tell you only the who , where , and what … but not the why . Using this technique in combination with other research methods to build a data triangle will help you to achieve a comprehensive understanding of the whole topic, filling in the why’s and how’s. A great way to start and use these statistics is the UX Benchmarking process. The first step involves deciding what to measure and how it will be measured. Once the criteria is put into place and measuring is carried out and completed, evaluate the findings against industry standards, competitors or use the gathered information as a baseline to evaluate future improvements (e.g. the time spent on a task takes now 2 minutes less than before). UX benchmarking helps to demonstrate the value of UX research, and the metrics you gathered can be used when calculating “ return on investment ” (ROI) In the third step, analyze all the feedback that was gathered in the previous steps. There are many possibilities to do that, our favourites are: Thematic analysis This is a method used to analyze qualitative data. It’s mostly used on sets of texts gathered during interviews or directly taken from the interview scripts. The goal is to uncover themes within this data. To do so, you need to search for relationships or similarities within the sets of texts. When done correctly, you have clusters/themes of specific topics, e.g. paint points, consumer usage, behaviors, needs, opportunities. By color coding feedback by type such as pain points, opportunities or general feedback, you could create a board that is easily scannable by improvement topics. Rainbow spreadsheet tool Another tool that will help you analyze qualitative data is the rainbow spreadsheet. Introduced by Tomer Sharon , it can be easily created within Excel or Google Sheets, and can be used in many different ways. A good recommendation would be to use it for validation and visualizing key findings from e.g interviews, user observations, etc. Start by creating a table or a grid where you can list the unique findings. There should be one column to represent key findings, with one finding per row, while next to findings, there should be columns for participants (with each participant receiving their own column and a unique color code). After that, go through each row and highlight the cells for all participants who mentioned a specific finding, using the unique color of the participant. As a result, you can quickly scan for the findings that affect the most participants by looking at the rows with the most colored cells. These rows are the findings you may want to prioritize as the next “action to take” for your team. After uncovering your customers’ pain points, you should focus on possible solutions. Remember that steps 1–3 are quite important for the success of your solution and will only provide you with valid results if you run the tests well. Depending on the findings, there are many methods and prototype approaches that can be used to test the ideas. In terms of APIs, you need to think about the structuring of resources, methods, naming conventions or error messages displayed by the API. Also don’t forget the usage of your SDKs. Most of these things can be tested with appropriate UX writing techniques. Here are two examples of testing methods and how to use them: Cloze test Cloze test is a “gap fill” exercise for UX copywriting. The participants will be presented with a copy section, where they have to replace a missing language element. (e.g. words, characters, items). This test helps verifying how easily readable and understandable a section of text is. You could also abstract the approach to test naming conventions, proper error messages, or human readable descriptions. Prototype testing Prototypes serve as a first draft on the path to finalizing your solution. They help to elaborate if an idea will work and is usable by your user. When you have your draft ready, get in touch with your users and let them perform tasks associated with this prototype. Prototype testing isn’t just for frontend solutions, but can be used to test your APIs and SDKs too. Get in touch with a user (developer) and let them solve a task by showing your API/SDKs or a prototype of them. Observe the user while they are performing this task since it’s the best way to understand and see how your solutions are applied and if difficulties occur. An API prototype testing scenario could be done through Pair Programming (an agile software developer technique, where two developers work together on the same task simultaneously, often on the same device) If the previous 4 steps are followed correctly, you will have gained more and more confidence that you are building the right solution for your users. When none of your users have issues fulfilling their daily tasks with your provided solution, you will know you had done the right thing and you can start developing the feature. If you still see your users struggling to understand your solutions, go back to Step 2 and repeat the process until users are able to work smoothly. Having now given a brief guide of UX research processes, let’s go over some of our key learnings from past and current UX research processes conducted from our current API solution. Naming conventions Naming conventions within an API are quite important as those names will be used as requested. When the request speaks for itself it’s easier to connect the usage of this request to the dedicated outcome. One example from our product was “ImportSink”. The intended response to this request was to create templates for all your resource types needed to be imported (e.g. categories, products, product types, prices). The intended idea behind the name “Sink” was developed in a metaphoric way from a bathroom sink where you put a load of water (data) in at once. The sink is filled up, but asynchronously water goes into the drain and is sucked in (imported). This was a nice internal explanation from our side of the team, but no consumer would get this metaphor without providing them a proper explanation. This was the outcome of our research phase where we discovered that users will get the connection to the bathroom sink but not the intended idea behind it. On the other hand, many consumers thought of the word sink as “sync” from synchronization. This led to a lot of misunderstandings due to the similar pronunciation. We came to the conclusion to change the name for the resource into “ImportTemplates”. Always try to use names that are common in this specific type of field or listen to users and how they describe it. New word creations might not help your consumers or will confuse them. Troubleshooting — Unclear or missing information within the API Although we aim to provide the best development experience through our API and documentation, we know that there is always room for improvement. States and error codes are not sufficient to understand. As a consequence, users always contact our support to help them to solve their issue. We should always think about human readable and logical structured resources, methods and error codes and their related descriptions. Whenever the user needs to connect their systems to our API, it might happen that the API request is not behaving as expected. It could be that it was not possible to send a request, or the response that the user receives is different from the examples we provide in our documentation. This is the situation where the user needs to start debugging and troubleshooting. In best cases we can provide the users perfect status code and error messages that allow them to easily understand what went wrong. In reality this is not always possible as API errors can also occur in situations that are difficult to consider or test. Sometimes the user himself could provoke bad requests with mistakes or typos that our APIs won’t know how to handle. To avoid these types of trouble, we will provide our users with “best practices” and a variety of examples that they can easily copy and rewrite for their own needs. Always see your API from the users point of view Often breaking changes are made because developers create APIs without considering users. This means that they are creating something for themselves and not the actual needs of the API users. The API is a contract The most important thing is to treat the API interface as a formal contract. If you ever tried to change a contract you know how difficult and time consuming it is. Invest time in negotiating the contract, and limit the need for changes. That is basically what you do in your research: Negotiating until you find an acceptable API contract and design. If you miss an important part of your API solution and the API is out of beta, all your users need to deal with this. — — Do your UX research next time you change your API or build a whole new solution. It does take time and effort, and it may initially slow down your workflow but that’s absolutely worth it, as you will save a lot of effort, money, and time past that initial stage for the long-term future. As a bonus, your users will thank you! Not doing proper UX research before hand is like driving a Ferrari with a Renault twingo motor. If the engine is not right, you will never reach the full potential. One dedicated person doing the UX research will cost much less than a full development team delivering a couple of features during three sprints but having issues within the product from the lack of UX research. The earlier you can solve those issues within your product, the better. The best approach is to have at least 1 dedicated UX person for each team to help you with the required research and guide the team through the process! If you already have a dedicated UX team like we have at commercetools, get in touch. We are looking forward to exchanging ideas and tips or driving the next topic together ;) In the next part of this series on “Bringing your APIs from good to awesome with UX research!”, we will discuss the benefits of using UX methods such as: thematic analysis, close tests, prototype testing and, present detailed case studies about how we applied (UX Research/ Discovery techniques/ name it as you wish) to our API solutions at commercetools. Looking under the hood of the commercetools platform 442 UX Research API UX Development Api Development 442 claps 442 Written by Senior UX/UI Designer Looking under the hood of the commercetools platform Written by Senior UX/UI Designer Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-18"},
{"website": "CommerceTools", "title": "reflections on building and maintaining a react component library", "author": ["Malcolm L"], "link": "https://techblog.commercetools.com/reflections-on-building-and-maintaining-a-react-component-library-d6dc2a4d17", "abstract": "About We're hiring During my first six months at commercetools , I worked in a product team. I worked with five other developers to deploy user facing features. Then I changed teams. Now I work on UI Kit : our React component library that implements our design system. The biggest change was “who” I was developing for. Before I was developing features for our customers. Now I am developing components for my coworkers. It’s my job to deliver well tested components that my colleagues can build features with. Another big change was going from working in a vertical team, to working on a horizontal topic. All our frontend developers use UI Kit. Now I needed to communicate with all our developers. I needed to find out what they needed to build with UI Kit, and how UI Kit could make their jobs easier. I wrote two blog posts about this experience. One details our experience setting up visual regression testing. Another explains how and why we built our component library . Now that some time has passed, I wanted to write another post outlining some of the learnings we have made. Let’s face it: having a developer work full time on a component library is a luxury. Not all teams can afford this luxury. Building new components is only half the battle. The other half, is of course, implementing them. It’s easy to encourage developers to use them in new feature work. But not so easy to encourage them to go back and refactor existing features to use the new components. A component library won’t give your application visual consistency if you don’t apply it everywhere in your application. To make our applications consistent in look and feel, we strive to use our component library everywhere. If we create a form capturing a user’s name, address and age, we use UI-Kit inputs. If we refactor an old form, we replace the old inputs with UI-Kit inputs. This consistency helps both our customers and our designers. Now when they make visual changes, these changes propagate across our applications. We make our applications easier to use by providing a consistent visual experience. It also makes it easier for us to make visual changes. Without a component library, making visual changes is hard. You are often left changing a lot of hard coded css. With our component library, we could change our primary brand colour with one line changed in a JSON file. When one person is working full time on your component library, testing is important. Sure, they tag other developers to code review their pull requests. But these developers don’t have time to check every state of every component in Storybook. This can lead to visual regressions sneaking into your component library. The solution to this problem is simple. Delegate visual reviews to a testing service, not to your developers. We use percy.io and it has saved us countless developer hours. Here’s a concrete example: last week I added a new prop to our DateRangeInput: isClearable . If isClearable is false , then we don’t render a clear button on the input. Simple right? I made changes to two components: DateRangeInput , and CalendarBody . I forgot that CalendarBody is also used by our component DateInput . Now DateInput was no longer clearable. A colleague approved my pull request without noticing the error. The screenshots below highlight the problem. After seeing the failing visual tests, and the diff on percy, I made the required changes to fix the bug. This represents a real life example of the value of visual regression testing. It stopped us from introducing a regression into our component library. It prevented us from having to catch this bug in production, and then fix it and release the fix. Read more about how we built our visual testing setup . Now that our developers build their features on top of our components, they can write less code. Less code leads to less bugs in our applications. It also leads to code that is easier to refactor in the future. We first introduced react-testing-library into UI-Kit. Since then, we have introduced it into our application codebase. We first experimented with visual testing in UI Kit, and now use it in other repositories. A component library is the perfect place to experiment with new and better ways of doing things. We have spent time perfecting our CI scripts. We spent time getting our release publishing working. We spent lots of time setting up our testing suite. We wrote extensive documentation. We did everything we could to lower bus factor and increase knowledge sharing. Something happened when we pulled UI-Kit out of our application codebase. It became external in more ways than one. Now UI Kit is as external as our other external dependencies, like flopflip . This meant that developers started to look differently at UI-Kit. It became harder to get people to review pull requests. Now we need to wait for a release to npm instead of getting the changes immediately. This could also be a positive, as it means we make changes to our UI less often. If you want to try out an unpublished version of UI Kit in your feature branch, it’s not easy. You can either use yarn link , or npm link and continuously build UI Kit. Or you can copy and paste UI Kit’s dist folder into your application’s node_modules . Either way, it’s not simple to debug bugs with UI Kit in your application code. We need to maintain the tooling and build setup for UI Kit ourselves. We need to update dependencies. We need to investigate things like code splitting. We need to make sure we don’t blow up our bundle size. This all adds overhead. The time we spend on library maintenance and upkeep, is time we can’t spend developing new components. Working full time on a component library has been a rewarding challenge. Every day I can see our applications look and feel more and more cohesive. More and more developers are refactoring our applications to use the new components. We are seeing more and more pull requests that delete files, instead of adding them. Product and design are buying more and more into the benefits of a component library. Product teams are able to ship code faster. Finally, thanks to visual testing, we are breaking our UI less often. Check out UI Kit , our component library here . Looking under the hood of the commercetools platform 138 1 Thanks to amin benselim and Freddie Karlbom . Design React JavaScript Reactjs Components 138 claps 138 1 Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-14"},
{"website": "CommerceTools", "title": "building up a data science team from scratch", "author": ["Amadeus Magrabi"], "link": "https://techblog.commercetools.com/building-up-a-data-science-team-from-scratch-7a7b24ba9f2d", "abstract": "About We're hiring There are plenty of reasons for companies to incorporate data science and machine learning into their business. It can allow you to better understand and predict customer behavior, automate repetitive manual tasks, detect errors and anomalies faster, evaluate business decisions with data instead of mere intuition, get an edge over competitors, give marketing campaigns more punch, check a box for investors, attract more talent for the organization, or brag about it at conferences. Many companies face the challenge of building up a data science team from scratch and it can be hard to figure out how to start. In 2016, I was the first hire of a new data science team, with little infrastructure or strategy in place. Over the years, there were many different challenges for us to solve and mistakes to learn from as the team got more and more mature. This post is about what I learned about the process of building up a data science team, from both my own experience in the past years and conversations with other data scientists in a similar situation. In a nutshell, these are the points that I will argue for: Hire generalists instead of specialists Build an autonomous team Pick low-hanging fruits first Set the right expectations Avoid half-assed initiatives I am not claiming that these are golden rules to success that work for everyone, since every company has different goals and requirements that need to be considered. Instead, think of it as a checklist of topics for which I think companies should make informed strategic choices on, regardless of whether you reach the same conclusions as me. 1. Hire generalists instead of specialists Especially in the early stages of a company, you will not be able to predict exactly which use cases you will encounter and which skills you will need. If you have a choice between a candidate with deeper and one with broader knowledge, your organization will probably benefit more from the one with broader knowledge. Someone who knows a little bit about a broad range of topics (e.g. hypothesis testing, data cleaning, natural language processing, image classification, anomaly detection, clustering, time series forecasting, neural networks, object-oriented programming, databases, unit testing, containerization, REST APIs, cloud computing, distributed systems or data visualization) will have an easier time in a new data science team than someone who is an expert at specifically one of those things but naive about the others. Generalists might need more time to get to a good solution for a project, but will be more flexible, have a better eye to identify new business opportunities, and be able to communicate to stakeholders with more context. Even if the stars align and you happen to be a specialist for exactly what is needed for a project, you usually still cannot jump straight to the implementation. Just like the generalist, you typically still need an extensive research and learning phase to make sure that your knowledge is not outdated, because the state-of-the-art in data science and the frameworks to implement them change very quickly. What requires a specialist today, could be automated away with an elegant new library tomorrow. Besides, data science projects usually have very different requirements that demand unique solutions. So the overall benefit of being a specialist is typically lower, because even specialists will not be able to provide answers on-demand without more research on a particular problem. There are two pitfalls to avoid when looking to hire generalists. First, nobody will be an expert on everything, so do not try to look for a data science unicorn. Candidates should be able to explain the basic problem and approach behind a broad range of concepts, but you cannot expect them to have detailed knowledge on everything. Be relieved to hear “I have no idea” as an answer at some point in an interview, because it shows that candidates know about their knowledge gaps and are willing to admit them. This separates them from the huge majority that tries to hide their weaknesses by name-dropping a series of arbitrary technical terms when they are lost. Second, even though generalists will not know all the details on every topic, they should have deeper knowledge about something . Ask them to explain the specifics of past projects, implementation details of the technique they are most familiar with, or what the last complicated bug they fixed was. Otherwise you have no way of differentiating candidates who just talk about data science from the ones with hands-on experience. 2. Build an autonomous team There are a lot of things to figure out before a data science team can start working: Which projects should they work on first? What data is available and how can it be accessed? Does the team build software that runs in production or does it generate insights that guide business decisions? How should the results of the work be exposed to users and stakeholders? Which languages, frameworks or cloud providers can be used? The list goes on and on, and getting the right answers and setting up the first basic infrastructure requires a lot of communication and support from other teams in the company. Aligning your plans with the needs and goals of other teams is crucial. But once you know what you need to build, the team should have as much autonomy as possible to figure out how to build it, because hard dependencies on other teams can severely block the progress and slow down the momentum of a new data science team. The most important factor to make a team autonomous is to build a team structure with a broad and balanced skill set to cover data science projects from end-to-end. If you have a team that consists only of data scientists (with the core expertise of analyzing data to identify the best statistical models), then there will be a lot of friction with other teams, since they have to ask other departments every time they need help with something outside of their domain (getting access to new data, setting up databases, making software architectures more scalable, deploying applications to production, etc.). This will block the progress of a new data science team too much, because they will need help from other teams often, quickly and consistently to make progress. Instead, it is increasingly common for companies to adopt the framework that Spotify is known for and combine all the necessary roles within a single autonomous team. For example, if the main output of the team is software, the team could consist of: Data scientists who focus on data analysis and testing statistical models. Software engineers who focus on making the code efficient and maintainable. Data engineers who focus on managing databases and scalable infrastructures. Product managers who specify requirements and align them with other teams. It is not important that the resumes of the team members neatly fit into these job titles, but it is important that these skills are covered within the team to maximize autonomy. And as I mentioned in the previous section, this will work better by hiring generalists with a broader understanding instead of throwing several specialized domain experts with little common ground into one team. Autonomous and cross-disciplinary teams not only help to make the progress more smooth and avoid external blockers, they are also a great way to enable a productive learning culture in which team members can share knowledge and teach each other new skills. In the long run, this will for example give software engineers a better understanding of what they need to optimize for, and data scientists a better understanding of how to structure their work in a scalable way from the beginning. Apart from the team structure, another important aspect of autonomy is to give the team as much freedom as possible to choose the tools they are going to work with. Do not make it a company policy to only use Tensorflow and Python on AWS. Standardization can have several advantages, like making deployments and code reviews easier, but the landscape of data science tools is still very scattered and there is no one framework that can do everything. Having more freedom to switch between different tools gives the team more flexibility and avoids situations in which a framework has to be forced on a problem for which it was not designed. Also, especially in the beginning of a new team, it helps a lot if team members can choose tools they are already familiar with to get to first results faster. 3. Pick low-hanging fruits first Ideally, you already have a clear strategy in place of how exactly you will leverage data science for your business and what concrete projects are the most important ones before the first team member is even hired. More often than not, this is hard to determine a priori and you will have to revise your data science strategy iteratively along the way. But even if your plans are not set in stone yet, you will need to have better plans than “I heard it works” or “everyone else seems to be doing it”. Talk to the data science enthusiasts in the company and discuss what the biggest pain points in the company are and what kind of data you collect. If there is an overlap and data science seems like an obvious approach to improve a process, this is a good foundation for a potential data science project. Once you have some project ideas, you need to prioritize them and decide what the team should work on first. Say you have a choice between: An ambitious project that is experimental, innovative, uses sophisticated machine learning algorithms and could deliver a lot of business value if it works out. A simple project that is straightforward, standard and will only be a minor improvement to a process. What makes for a better first project? For a new data science team, it is very important to go for the simple and boring projects first. Nobody will be impressed by it, your data scientists will say that this is not what they went to school for, your marketing and sales team will be disappointed and the project will not have a big impact. But the project will allow you to learn as fast as possible what infrastructure and processes the team needs to complete data science projects from end to end. Figuring out how to bring any project over the finish line will already be incredibly complex and full of unexpected stumbling blocks. Do not increase the complexity even more by using the most advanced machine learning methods in the most experimental applications. You do not want to spend three months tuning hyperparameters for a deep reinforcement learning model with a custom Q-function, only to find out afterwards that you cannot actually deploy this model without another six months of data engineering work and that the use case is actually not quite as you thought it was. Instead, look for cases where a vanilla logistic regression, basic summary statistics or a simple visualization will already be useful. In the best case scenario, you will find a project that is both simple and highly valuable to the business. But if you have to choose, go for simplicity. The main goal of the first project is not to revolutionize the business, but to get a feel for what the company needs, who the stakeholders are, which skills are still lacking in the team and which infrastructure is required. To get to this point faster, make sure that the team is not trying to over-optimize the first solution. Data scientists tend to focus on improving model accuracy and paying attention to statistical details. Try to emphasize that simple implementations and fast iterations are more important than accuracy at this point. The earlier you finish your first project, the sooner you will be able to figure out what the team actually needs to be successful. 4. Set the right expectations Data science, machine learning and AI are still highly controversial topics and people have vastly different opinions on them. Common views are: Data science will transform every industry and solve all our problems. Data science is just a hype that will fade and is not actually useful. Data science is valuable for large tech companies, but small- or medium-sized companies are too far behind to get any value out of it. Data science is magic, creepy and will steal our jobs. If you do not start with data science now you will get steamrolled by your competitors in the future. Data what? As someone who has already read this post up to here, chances are that you are more on the optimistic and realistic side of the discussion. Before you can convince anyone else, you have to be clear about your own expectations. If someone asks me why companies need data science, my pitch usually goes something like this: The data that is being collected in the world grows exponentially and a lot of companies are becoming tech companies. On the one hand, this creates more opportunities to get something useful out of data, but on the other hand this also makes it harder to make sense of it without getting overwhelmed. Data science is the field that focusses on these topics: Making sense of business data and using it to make valuable predictions for optimization and automation. This can involve applying sophisticated machine learning algorithms, but in a lot of cases more simple statistical methods are already useful enough. Data science projects are inherently unpredictable and hard to plan, because they need a lot of trial and error and the processes and frameworks in the field are still largely unstandardized. But the sooner you start investing, the more mature your organization will be in terms of handling data, which will set the foundation to be more consistently successful in the future and become an effective data-driven organization. There are certainly companies which either do not have to care about data or which can rely on external consultants and third-party solutions. But this subset of companies is shrinking fast. To stay efficient and competitive in the long run, more and more companies depend on in-house expertise to properly manage and get value out of their data. No matter what your pitch is like and where you stand in this debate, there will always be people in your company who are more pessimistic than you, more optimistic than you or who simply do not know what data science is about. That is why it is very important to set the right expectations, otherwise it will be hard to work together, lead to a lot of misunderstandings and cause conflicts. Discuss with as many stakeholders as possible what you want to achieve and what they can expect from data science in your organization. Explain that it can take a very long time to build up a data infrastructure before there will be any results. Make it clear that projects need a lot of experimentation and can always fail, and that you cannot know in advance how accurate a model will be or how long it will take to build it. Debunk myths about the magical powers of AI and stress the importance of data quality. Convince skeptics about the long-term value of becoming a more data-driven organization. Be aware that different groups of people (e.g. engineers, managers, marketers or lawyers) will care about different aspects, and that you need to tailor your communication to them to bring everyone on the same page. Data science is still an extremely vague term to most people which makes it a breeding ground for misconceptions. Try to prevent this and do not let all the technical issues on your plate distract you from the importance of communication and setting the right expectations. It is far better to over- than under-communicate, especially in the beginning of a new team. 5. Avoid half-assed initiatives Companies that are just getting started with data science sometimes try to test the waters by hiring one or two data scientists, letting them do their thing and seeing what happens. The motivation behind this is understandable, because there is a lot of uncertainty around the question of how data science initiatives will work out, so companies do not want to risk too much at first. They want to invest a little bit, get a little bit of value out of it and learn from the experiences before investing more. Unfortunately, this is rarely a good idea. It will more likely give you zero instead of a little business value and you will have a hard time learning anything substantial. Your data scientists will not be able to properly finish projects, get stuck in endless prototyping and it will be impossible to set up an effective feedback cycle to iterate on and learn from. The only thing you can really learn at that point is that data science does not work without a solid foundation. Stop thinking about data scientists like Swiss Army knives who can do everything that is needed for data projects. Being good at statistics and machine learning is only a small part of the equation and you will need to fill other roles too. Depending on your goals, you might need dedicated software engineers, data engineers, product managers, DevOps engineers or UX designers. Apart from that, do not let the team become an isolated island in the company landscape, where nobody really knows what they are doing, but old men with grey beards sometimes tell stories by the fireplace about the magical things that are happening there. Instead, integrate the team with core processes of the company and let cross-departmental information flow in and out frequently. Get other teams involved as much as possible, facilitate communication and work towards shared goals. Not every single company needs to invest in data science. But if you do think that it is important for your industry, then go all-in and make sure you do it right. Data science is not a plug-in module that you can just add to a company by having a few seats in the building for data scientists. This will not lead to any value and only delay the learning curve to become a more data-driven organization. Final Remarks Of course there are many more topics to consider, like how to choose tools and frameworks, how data scientists and software engineers can best work together, or how to establish a continuous learning culture. I will save those for a potential part two. It is one thing to have a plan for how to build up a team, but putting it into practice effectively is an entirely different beast. I am still learning new things about this process every week and I am still far away from where I would like to be. The data science industry is still in a Wild West state and it will take a while until we have found a more robust recipe for effective data science teams and establish more standardized processes. Thanks for reading! Looking forward to hear about your experiences. Looking under the hood of the commercetools platform 787 7 Thanks to Tobias Deekens and Evi Lazaridou . Data Science Machine Learning Management 787 claps 787 7 Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-02"},
{"website": "CommerceTools", "title": "testing javascript applications with confidence", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/testing-javascript-applications-with-confidence-ff539c4a25f2", "abstract": "About We're hiring It is undeniable that a lot has changed in the last few years in the world of JavaScript. This should not come as a surprise. We are particularly excited to see a lot of progress in the way we write and think about testing in the JavaScript ecosystem. If we take a small step back, about 3–4 years ago the landscape was already promising. Test runners like Jest were already quite popular, providing a good Developer Experience, and testing React components was pretty much dictated by Enzyme . We at commercetools were pretty thrilled about the tooling we had at our disposal and we tried to establish some best practices in our team on how to write good unit tests. We wrote about it in this article: Testing in React: best practices, tips and tricks . However, if I reread this now, my reaction would be to disapprove most of what the article is preaching. Yes really, as this just shows how much things have changed since then. Luckily they have changed for the better! Let’s have a look at what exactly has changed. We mentioned before that we had good testing tools at our disposal. What we believe was lacking was the fundamental mentality on how we should approach writing and thinking about testing. Thanks to the community effort, and to the teachings of Kent C. Dodds , the way we test JavaScript applications has radically changed. The more your tests resemble the way your software is used, the more confidence they can give you. The key takeaway with this, is how we think about what needs to be tested. Traditionally the majority of us (we assume) wrote tests that also tested most of the internals, for example, of UI components, leading to frustration and problems when having to refactor code, as both the implementation and the tests had to be changed. After all, one of the main reasons to write tests is to be able to change to the implementation in the future with confidence. As a result, tests should only be concerned about what the end-user sees and can interact with . Therefore, tests are agnostic of the implementation details, allowing much more confident refactoring as only the implementation needs to be changed, not the end result and thus the tests. To help follow this mentality and enforce best practices, the DOM Testing Library & Co. were born. If you wish to know more about this topics, check out the following resources: https://kentcdodds.com/blog/introducing-the-react-testing-library https://testingjavascript.com/ https://kentcdodds.com/testing/ For us at commercetools this helped a lot to rethink and re-evaluate how we want to write tests. Most importantly we have seen the following benefits: We have much more confidence when refactoring existing code or implementing new features, as we can rely on stable tests. We don’t write tests for each component but instead more at the application level, giving us more depth coverage of what is being tested. This implicitly helps us to also test things like routing. The DOM Testing Library promotes writing and testing accessible components . Jest already provides a good mocking system. However, it leaves a lot of room for testing data fetching in components. We at commercetools primarily use GraphQL and Apollo, especially in our frontend applications. Apollo has a Mocked Provider component that can be used to mock the GraphQL queries. This is extremely useful when testing things at the application level and in turn plays very well with the DOM Testing Library approach. Still, mocking the GraphQL queries with the Apollo Mocked Provider has shown its own downsides. One reason being that matching requests can get quite verbose and different scenarios can be hard to distinguish, like having partial data, or different HTTP status codes. This is partly due to the nature of not being able to mock close enough to the network level, as it relies on Apollo itself. We can also argue that using the Apollo mocks requires to know the implementation details of the requests, which ideally is not what we should be aiming for (as mentioned before). Therefore, a better option would be to mock directly at the network level and thus removing the need for Apollo to dictate the mocking behavior. Using libraries such as xhr-mock we can intercept requests and handle them according to predefined matching rules. For example for GraphQL requests we can match GraphQL queries by their operation name. Apollo also offers tools to automatically create mock data based on the GraphQL schema. This approach is more inline with the mentality of testing how the application behaves rather than how the application is implemented. Luckily, to make it easier to mock requests at the network level, there is a new library called Mock Service Worker (MSW). By bringing the ability of Service Workers to capture requests for the purpose of caching, Mock Service Worker enables API mocking on the highest level of the network communication chain. It is the closest thing to a mocking server without having to create one. We at commercetools have been looking and trying out this library in the past months and so far we have been extremely happy with it. We have found that the test setup to mock the network requests is much simpler then, for example, using Apollo mocks. It also works for both GraphQL and REST requests, so the same mocking approach can be used for both. Lastly, MSW can be used in both the browser and a Node.js environment, removing the need to know and use different libraries. Migrating to use MSW was also pretty much seamless as all we had to do was replace the requests mock setup and the tests kept working as before. Furthermore, besides some of the nice features of MSW, the core idea of MSW follows the same testing principles that we discussed before: test how the application behaves rather than how it’s built . Instead of asserting that a request was made, or had the correct data, test how your application reacted to that request. However, we can still perform validations of incoming requests by sending an error response . We can only recommend checking it out and giving it a try. Another important aspect of testing is about the underlying test data. Oftentimes you end up writing the same “fake” data in a bunch of tests, maybe also omitting certain fields because they are not used in that specific test scenario. Plus, the data might not even cover real world scenarios, thus making most tests almost meaningless. This process is very verbose and error prone and we at commercetools have bumped into these issues more and more in the past years. With the increasing number of APIs in our platform, having to maintain tests in such a way is not scalable at all. So we decided to tackle this issue and started an initiative to have generated test data . In short, the idea is that every entity in our platform APIs is defined as a test data model. A model contains all the available fields for that entity, most of which are generated as random values using the faker library. We also have a concept called transformer where we can derive a different representation of the base model, for example for a GraphQL-like or REST-like shape. Combining this with the testing tools like DOM Testing Library and MSW results in a very powerful combo which makes the tests setup simpler allowing us to focus more on what matters: writing reliable tests with confidence. The approach you take on testing your application is crucial and most importantly the tests should help you and your team to have the right level of confidence rather than getting in the way. At commercetools we are very happy with how we continuously look to improve our testing approach, as it has proven useful a lot of times and has helped us be more productive and yes, more confident. Having a good level of confidence is what also enables us to do things like continuous deployments (deployment train). If most of this sounds new to you, we really recommend that you start rethinking how you can approach testing your application and UI components, and choose the tools suggested to help you write resilient and meaningful tests. Most importantly, you don’t have to rewrite everything at once. For example, we recommend rewriting the tests from scratch based on the existing test scenarios and remove all the previous tests related to that functionality, rather than trying to rewrite the previous tests. For instance, we still have parts of our codebase that use the “legacy testing approach” and that causes trouble from time to time. Whenever we have a chance to, we write the tests for that part of functionality as new tests and remove the previous test files. Happy testing! Looking under the hood of the commercetools platform 76 JavaScript Testing Confidence Mocking React 76 claps 76 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-29"},
{"website": "CommerceTools", "title": "how to reduce the distance when the ux designer works remotely", "author": ["María Barrena"], "link": "https://techblog.commercetools.com/how-to-reduce-the-distance-when-the-ux-designer-works-remotely-ffdc6afa77f", "abstract": "About We're hiring As my colleague Ida explained, the product team has embraced vertical, cross-functional teams. Each vertical team has its own UX/UI designer that focuses on the team’s area of responsibility. I am one of them. However, we are not sitting in the same office as the rest of our vertical team members. Even though the situation was initially inconvenient, our vertical team has overcome this difficulty. These are the steps we have followed to succeed on a better remote collaboration, with special focus on my role as UX designer. In this blog, we have already spoken about the importance of small teams . They create meaningful connections and build trust. But how do we establish these connections while apart? Building good human relations is harder when you are not in the same office. That’s why three weeks after I joined the vertical team, I decided to visit them. Starting the relationship in person sets up the solid foundation of your professional collaboration. Having the vertical team together in one place as frequent as possible has brought us a good outcome. Starting the relationship in person sets up the solid foundation of your professional collaboration. Once you are in the same room, there are many activities you can do to reduce the technical and personal gap. Here are the ones that I’ve tried out and have helped: The first point is quite obvious and you should apply it either you are in the same room or not. You need to get to know your team better and vice versa. Organise an activity all together outside of work, in a more relaxed environment. Knowing somebody more personally always creates empathy. It is also a great opportunity to refute wrong myths they might think about the role of a UX designer. Ask about their previous job experience and their experience working with designers. Even though the role of UX and UI designers are not new in the IT industry, we need to explain often the differences between those two to other stakeholders nowadays. Some people still believe that design is only about making a feature nice and happens at the end of the chain. This type of thinking is often associated with waterfall models of software development. It can get worse if all the UX/UI designers of a company are sitting together far from the rest. Teams can end up isolating that group as they will see the design department as an external agency. The sooner you start working towards changing such mentality, the better. Share with the vertical team any UX related task you would like to include into your design iterations and the team workflow. Explain what is it for and the benefits for each team member this can bring. It can be a certain way of doing research, a specific type of test or any other tactic. Giving examples of successful use cases can help you to convince the most skeptical ones. Showing the positive aspects of your area of expertise proves that you are there to help as well. If that works out, share the results of that task with the team afterwards. During my first few months working with the vertical team, I’ve suggested conducting usability tests. The goal was to verify whether the proposal we were discussing and working on was the right approach. As none of us were the real end user, it was a good task to perform. Once I performed those with real users, I’ve presented the learnings to the product team. This guarantees that the design decisions taken are user approved before implementation. It can also inspire other vertical teams to follow similar design strategies. Get a whiteboard and draw your proposals for the feature that you are working on. Then discuss it and brainstorm with the entire team . Inviting everybody to “mess-up” with your sketch shows a willing to listen and trust on their feedback. Don’t forget to explain what consequences their suggestions will bring to the user. That’s a great way to start spreading UX culture within the team. Although a bit more challenging, you can try this exercise remotely as well. But as it happens in usability tests, being able to see everybody’s physical reactions is as important as what they say. That’s why it is beneficial that the first time you try this out within the team, you do it in person. Since I started working in a web environment, I have read the same quote over and over again. “If you are a designer, you need to know how to code”. I believe a more accurate rule that UX and UI designers should apply is the following: If you are a designer, you need to understand each stakeholder’s complexities. That’s our job in a nutshell. There are many ways to get to know the technical limitations developers face. An effective one is sitting down with a developer and to observe how they work and which tools they use. This technique is called “shadowing” and it is used to train people in other industries. When new experts start working in an Apple store, they have to spend a week shadowing a senior specialist. That’s the way for them to learn more about the do’s and don’t of the job. If you are a developer reading this, bring the designer to technical meetings. The sooner you do it, the earlier they will start to become familiar with technical concepts. Terms that they will see in technical documentation when they start digging more on it. It can help designers to understand the complexities a design proposal can bring. UX designers will identify all these techniques as some of the best practices in user testing. What nobody in our industry dares to say is that we have more than one user to think about. It is clear that the tasks that we work on on a daily basis have the end user in mind. But our own team-mates are another type of user as well: the users of our design proposals. It’s to them, after all, that we present new ideas to and receive our deliverables from. A UX designer needs to understand the vertical team motivations and goals. You should never forget about all those when working on a task. Once you are back to your daily routine, the challenge is to make that bond that you have established strong. Being far away is not an excuse for the team to forget about who’s on the other side or not keeping a daily contact with them. At first, you might need to establish the habit by reminding you are still there. Ask to take part in the team meetings, even if they might sound technical at first. In those, you can learn the common shared language. Avoid that dev-design gap that happens often as soon as possible. Expressing design decisions using the same vocabulary will avoid confusion. Don’t be afraid to ask what you don’t understand. Most of the people are happy to explain to you what they know and do. After a few weeks, the habit of having a continuous contact will be established. The challenge then consists on being consistent with the bidirectional contact. Our vertical team follow a Scrum way to work without estimations. This type of structure has moved us away from a more sequential development. We do two-week sprints with sprint plannings, daily standups (with a physical board), retrospectives and grooming sessions. Grooming sessions are one of the most essential meetings we hold. Grooming sessions are one of the most essential meetings we hold. We go through the upcoming frontend tasks inside our backlog and refine them. We review the user flow and interaction we have already agreed upon and take a look at the proposed UI. Backend developers are responsible of checking the API implementation. They can confirm whether the backend allows us to build that or should be changed. Then, we write down the acceptance criteria with the help of the product owner. We also verify that the task does not need to be split into smaller tasks. In most cases, this is the first time that all team members are aware of what’s coming up. Everybody takes a more active role by discussing how something needs to be built. We also have separate meetings when we need to discuss a certain feature or task. We go over the design proposals with at least one frontend and one backend developers and the product owner. In those, we take meeting notes that we store so we can always go back to them. This type of structure helps us having a continuous communication within the team. But it can be complex if the technical setup is not a good one. The equipment ought to provide good video and sound quality. Avoid wasting time with technical issues as much as possible. As with any relationship, the effort of not losing the contact should come from both sides. Leaving somebody apart, even the far one does nothing but harm. Nobody should forget that a designer can help with their expertise on any of the steps of the project. By not conferring with the designer, you are communicating to the team UX is not relevant at all. Furthermore, you are giving the designer the impression that you don’t trust in their expertise. When you rely on internet connection, bringing the entire team to a meeting can be hard. Once of the skills that I’ve been improving a lot since I work on remote is writing proposals. When the conference call fails, the design decisions in written form is my backup plan. In those writings I provide a detailed explanation. They include the pros and cons for the user and describe the edge cases that I can oversee. Of course, those documents include pictures of sketches, screenshots and/or digital prototypes. Afterwards, I’ve shared that with the team. This has been a good exercise for me to get to synthesise ideas, being as clear as possible. This is quite essential when you need to present proposals on a conference call. Discuss something in written form is like having meeting note without the meeting. Giving and receiving rapid feedback being remote is not as easy as in person. The fact that I’m not in the same room has made me jump from paper prototypes to digital ones. I use digital prototypes during meetings to show the possible user flow and layout. Most of the times, the round of feedback happens during the meetings scheduled earlier on. This makes the design iterations longer than normal. But the biggest disadvantage in this case is the lack of useful feedback that happens often. Examples of unhelpful feedback sentences are: “We cannot build that”, “The answer is too technical for you” “It is easier to explain if I could go to your desk” No matter your role, always prioritise giving constructive feedback to your colleagues. No excuses. Writing a short message on a chat or making a quick video call to explain something takes less than 5 minutes. Even though the reasons might be too technical, get interested in those. It is your responsibility to learn more about the limitations the team faces. Your design iterations will get shorter. This becomes essential for the teams that have to deal with deadlines. More than one year has passed since we are working with such setup. I must say, I’m proud of the level of collaborative thinking the team have achieved so far. Nobody works solo and we always bring expertise to the table. I’m quite confident that my work wouldn’t differ much if I were sitting in the same room with my teammates. We keep gathering together from time to time to not lose the human touch. I’ve visited them once every two months while some of my teammates have visited me at least once. A few days ago, one of my colleagues told me that if he ever starts deciding on things on his own, I should let him know. This is the exact fear that everybody working on a vertical team should have. Together as a team we have reached a consensus of how we want to work. We find solutions to the problems together as a team. We have built trust between each other. Trust in providing feedback or discussing points of friction. And above all, we take care of not leaving the remote team member behind, no matter what their role in the team is. Looking under the hood of the commercetools platform 147 Thanks to Malcolm L . UX Remote Working Remote Work User Experience User Interface 147 claps 147 Written by UX / UI designer based in Munich (DE) Looking under the hood of the commercetools platform Written by UX / UI designer based in Munich (DE) Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-05"},
{"website": "CommerceTools", "title": "why you should build a react component library and style it with css in js", "author": ["Malcolm L"], "link": "https://techblog.commercetools.com/why-you-should-build-a-react-component-library-and-style-it-with-css-in-js-12397fd69c58", "abstract": "About We're hiring Mise en place is a concept from French cooking. It translates to “everything in its place”. It refers to the preparation before starting cooking. To make soup, you would start by chopping onions, carrots, celery, and garlic. Then with everything portioned and prepared, you start cooking. Mise en place is the religion of all good line cooks. — Anthony Bourdain The purpose of mise en place is to allow the cooks to cook in the most efficient way without having to stop. What if we applied the same concept to software development? When your React application gets big, break it into smaller parts. At commercetools , we started small. We moved our reusable components into a different directory in our mono repo. Our direction structure looked something like this. We used the components in the component folder to help us solve problems faster. When we built forms, we used our TextInput and DateInput components. When we built lists, we used our Table component. We were line cooks — building components was our mise en place. Implementing them was our cooking. We could now develop features faster, easier , and with better test coverage. We styled our components like our applications — with CSS modules and PostCSS. Our applications shared a single build system. It wasn’t possible to consume our components from outside of our application. Since we were the only developers working on our applications, this didn’t pose a problem. Fast forward a few months and the concept of extensibility came up. External developers needed to create applications that would run inside of our application. We decided to open source `ui-kit`, our component library. This would help us to ensure visual consistency across these applications. Move the code into a new public repository . ??? Profit! First we decided to use webpack to bundle our component library. We were already using it to bundle our apps, so we used our existing configuration. While webpack excels at building applications, it doesn’t support building ESM modules. We decided to go with another solution. We setup our build system using rollup. It allowed us to bundle our code in two formats: ECMAScript Modules (EMS) and CommonJS Modules (CJS). We used babel-plugin-rollup for our JavaScript and rollup-plugin-postcss for our CSS. Our entry file look like this: We had two options when it came to bundling our CSS with rollup. We could either include the CSS within our JavaScript bundle, or extract it a file. We decided to bundle it into our JavaScript to make it easier to consume our components. External developers could now use our component library in their applications. A few months passed, and ui-kit was being used to develop applications using React. Someone created a bug ticket. They used ui-kit with Gatsby, and noticed a flash of unstyled content. We made a quick demo repository to show the problem. Check out the GIF below that shows that flash of unstyled content in action. Because our JavaScript contained our CSS, our styles didn’t get applied until runtime. Our bundle contained calls to the function `styleInject`. This injected CSS into the document head when the application runs. We had never experienced this problem. This is because we render our applications on the client side. Gatsby is a static site builder. That means it delivers pre rendered HTML to the browser. This leads to this flash of unstyled content. The browser loads the page with the structure of the page, but not its styles . I know what you’re thinking. Wait a minute? Why don’t you extract a css file? Yes, we could do that. But, one of the reasons why we wanted to ship ESM modules was to support treeshaking. Tree shaking is a term used within a JavaScript context to describe the removal of dead code. source: https://developer.mozilla.org Extracting CSS into a single file breaks treeshaking. We only want to ship critical styles. This wouldn’t be ideal. We could do better. Supporting server side rendering with CSS in JS is simple. Emotion supports it out of the box. There are Gatsby plugins for emotion and styled components. We spent about a week refactoring our component library to use emotion. Thanks to our visual regression testing setup, it was easy. In conclusion, the ideal setup for our React component library looks like this. Bundled with rollup Transpiled with babel Styled with emotion Our component library supports treeshaking and server side rendering. In our books, that’s a huge win. Check out our React component library at github.com/commercetools/ui-kit Looking under the hood of the commercetools platform 200 1 Thanks to Tobias Deekens . React Design Systems Css In Js Styled Components Css Modules 200 claps 200 1 Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-13"},
{"website": "CommerceTools", "title": "graphql adoption at commercetools", "author": ["Yann Simon"], "link": "https://techblog.commercetools.com/graphql-adoption-at-commercetools-cab25a1d1f4a", "abstract": "About We're hiring It has been a while since we have written about GraphQL at commercetools. If you’re new to this topic, I’d suggest to have a look at our journey , our challenges , and how we model mutations . Over the last few months we have noticed that more and more customers are using GraphQL to access our e-commerce platform. The number of GraphQL queries, even though under the number of REST API requests, are nowadays an important part of our traffic: Our customers appreciate being able to select all information they need on one request (and only the information they need). Based on the GraphQL query, we are able to optimize database requests, providing better performance for simple queries. Our platform is API first. We have had a REST API since the beginning. When deciding to add GraphQL, we settled on the intention to have feature parity of REST and GraphQL. So that eventually all functionalities of our REST API are also available in through GraphQL. Over the last months, we have added GraphQL support to more and more resources. You can see the list on https://docs.commercetools.com/graphql-api.html . For example, the checkout processes can now use carts, orders and payments queries. We provide a GraphQL interactive console to access and manipulate data. ( US , EU ) For developers, we added the possibility to profile a GraphQL query. It is now easy to know which part of a GraphQL query does not perform well, and to optimize it consequently. We are putting a lot of effort into our Merchant Center web application ( US , EU ) to deliver all features needed in a modern e-commerce context. To enable this, we have structured ourselves into vertical teams, made up of backend and frontend developers. This helps us to avoid building a backend-only feature that would not be enabled in the web application. In our experience, GraphQL provides benefits in terms of performance, state management, maintainability and user experience for client-side applications. At commercetools, the main method of data fetching used for the Merchant Center is using the GraphQL API. The Merchant Center web application uses first-class Apollo Client support to connect to our public GraphQL API . When discussing new features, backend & frontend people tend to naturally use GraphQL-like schemas. This helps each group to understand each other. We always strive for our API to be backwards compatible. This is the mindset we also apply with regards to our GraphQL schema. But it does not mean that our GraphQL schema cannot improve. We use the deprecation feature to mark some fields as deprecated, and to document which new fields should be used instead. Deprecated fields are by default hidden and only shown when requested. A query using deprecated fields works as before, keeping backwards compatibility. The main advantage is that we can track who is using a deprecated GraphQL field: With that information, we can approach the customer using deprecated fields, helping them migrate to the new query. Once we are sure that a deprecated field is not used anymore, we can remove it. When experimenting with a new feature, we sometimes use deprecated fields in GraphQL to model the new data model. We use the deprecation notice to express that those fields are part of an experimentation. We also try out the new GraphQL schemas internally in the Merchant Center Backend before eventually adding them to our public APIs. Deprecation is not intended for this purpose. This usage can be controversial. But it serves us well. During our learning phase by starting implementing the feature, it allows us to change or to event remove the feature entirely. Looking under the hood of the commercetools platform 56 GraphQL API Apollo Client 56 claps 56 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-19"},
{"website": "CommerceTools", "title": "augmented reality commercetools in ar", "author": ["Nicholas Speeter"], "link": "https://techblog.commercetools.com/augmented-reality-commercetools-in-ar-a44853b5cab0", "abstract": "About We're hiring The commercetools development lab does not have a giant robot poster. It only exists in a superimposed state through a camera lens and video feed. Yet the poster looks real and can be sized, rotated, and placed on tap throughout the environment. Just as our brains render our own reality based on light reflected into our eyes, Augmented Reality (AR) creates and displays a perception, but with auxiliary entities. AR and commercetools integrate together deftly to generate a 3D view of your products in the present environment. A customer can view a product in their own home to become confident in a purchase before carting the item. Augmented Reality is an excellent medium to display furniture, posters, and frames in a living room. This real immersive view provides a personalized shopping experience. Clothing can be augmented in 3D, rotated, and even placed to walk around and try on in front of a mirror. Facial tracking allows earrings, necklaces, and jewelry to be augmented onto a user through the front facing camera. developer.apple.com Apple ARKit was used to develop the iOS mobile application for iPad and iPhone. The Swift programming language is utilized to display stored 3D models and retrieve product data. The commercetools iOS SDK integrates easily into an iOS AR project created in XCode to into interact with a commercetools project through API requests. github.com A Heads Up Display button in the bottom center allows the user to search and display products from a project. The product and price is placed in the left corner in the Heads Up Display. The cart button allows items to be added to the cart and ordered. AR world tracking can be turned on with a shake in order to detect walls and surfaces which then auto-place products in 3D. Yellow dots indicate the real time world tracking surface detection. The physicalSize property can be utilized in Swift in order to lock in the physical real world dimensions in meters. Apple Pay or another payment service should be implemented as required into the checkout flow. I found Apply Pay to be very straightforward to implement in swift for payment authorization. Once authorized through Apple Pay, Stripe is one top choice to handle the payment transaction. stripe.com Store your 3D Models in the art.scnassets folder. Here you can place and develop on .scn files, .dae files, and .obj file types. SceneKit, the Scene graph, and Xcode’s tooling provide an excellent experience to manipulate your models and scenes. I found the .dae files to be the most simple to work with, as you can simply drag and drop color images to specific parts of your model. Add world tracking, multiple objects, HUD elements, touch interactions, product search features, and payment processing to create your custom AR E-Commerce experience. As Augmented Reality continues to increase in popularity across hardware and software platforms, commerce will be increasingly important in the mixed reality landscape. Now that AR has exploded into the mainstream for Android, iOS, and headsets like Magic Leap, Google Glass, and other wearables, your commerce platform can embrace the technology. Checkout a commercetools AR integration by Threekit on the marketplace: https://marketplace.commercetools.com/integration/threekit-3d-augmented-reality-for-commercetools Looking under the hood of the commercetools platform 20 iOS Augmented Reality Commercetools Arkit AR 20 claps 20 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-28"},
{"website": "CommerceTools", "title": "improving data quality with product similarity search", "author": ["Evi Lazaridou"], "link": "https://techblog.commercetools.com/improving-data-quality-with-product-similarity-search-d037c7212071", "abstract": "About We're hiring Poor data quality is a bottleneck for every e-commerce business as it can undermine its own competitive standing and hinder the management of data. Product similarity search or commonly known as product matching can help merchants to eliminate this pain since it detects potential duplicate product entries and content. This is useful for both e-commerce stores and marketplaces. It’s a common issue in marketplaces as every time new products are imported by a new seller it is required to check if the items already exist in their catalog. This is not an easy and straightforward procedure. The data might differ in description, name, or even pictures but still refer to the same product. Universal identifiers like GTIN or EAN are not always available and sometimes even incorrect (more on the need of product matching in marketplaces in Product Matching in eCommerce using deep learning ). In the case of e-commerce stores, the same products might be imported by mistake with a different Id and same or similar data information. In addition to this, it is common for e-commerce stores to use similar descriptions for products of the same type for convenience, a practice that results in a lower ranking position in the search engines: It’s easy to take shortcuts with product descriptions on eCommerce websites, especially with similar products. However, consider that Google is judging the content of eCommerce websites similar to regular content sites. That alone should be enough to make a professional SEO realize that product page descriptions should be unique, compelling and robust–especially for mid-tier eCommerce websites who don’t have enough Domain Authority to compete with bigger competitors. Every little bit counts. Sharing short paragraphs, specifications and other content between product pages increases the likelihood that search engines will decrease their perception of a product page’s content quality and subsequently, ranking position. (source: Thin & Duplicate Content: eCommerce SEO ) Duplicate product content detection is the main use case that product similarity search aims to address with a few additional use cases. For example, content-based product recommendations could leverage product similarity to recommend alternative items with same characteristics for products that are out of stock. In addition, it can assist enhancing data information: If some product data lack certain attribute values, it is possible to suggest potential attributes that are found in its most similar products. Multi-project product similarity search, i.e. searching for product matching in two different inventories, is challenging because the product information most likely presents very different data structures and values. For starters, the available textual information might be in different languages which necessitates an automatic translation entailing some risk of inaccuracy. Product images might display the same product in completely different settings (different light conditions, background, packaging, depth, rotation, context, etc.). And when it comes to attributes, the differences might appear both in the attribute names (e.g. ‘ colour ’ vs ‘ color ’) and the values (‘ small ’ or ‘ S ’ or ‘ 34 ’). Scalability also becomes an issue when the store or marketplace contains a large number of products. Scanning an entire catalog to detect pairs of similar products requires comparing each single product variant with each other. This is mathematically translated into n!/2!(n-2)! pairs where n is the number of product variants. That means 499500 pair comparisons for just a small project of 1000 variants. Last but not least, data might be missing and noisy. Descriptions might contain formatting tags, many attributes might be meaningless to similarity measuring (e.g. internal codes) and attribute values may be sparse, i.e. many products contain a particular attribute but only a few products have some value. That requires a preprocessing step that needs to be adjusted to common standards, ignoring for example an individual business’s specificities in data. The Similar Products API provides the opportunity for a store to scan their entire catalog for products that are similar to each other in terms of their data information. It leverages diverse product information: product name, description, attribute values, price and variant count . These data sources comprise of different data types: Names and descriptions are textual data, price and variant count are numerical and attributes include mixed data types. Therefore the system consists basically of smaller independent components, one for each information source, and each component calculates the similarity for the respective information source. Users can choose which of these data sources should be considered and even specify which features should have a higher influence on the similarity score, since the final output is a weighted average similarity score for each pair of products in examination. Furthermore, it provides the option to limit the search to specific products or product types. The code is written in Python including methods from the most popular data science libraries: NumPy , scikit-learn , pandas , SciPy . Text similarity for names and descriptions : Product names and descriptions undoubtedly carry important information, but as with any NLP case, any text instance must be converted to a vector. This is done with the use of a text vectorizer like the tf-idf and the count vectorizer, which are typically used, but instead we decided for scikit-learn ’s Hashing Vectorizer. The Hashing vectorizer is similar to the count vectorizer but doesn’t keep a vocabulary. It also doesn’t take into account the inverse document frequencies like tf-idf, which penalizes the words that appear more frequently across the whole corpus. Therefore it provides shorter response times with an accuracy that is sufficient for our use cases. Numerical similarity for price and variant count: Product price may be a strong indicator of a mismatch but is not conclusive to detect a match alone. This is because identical products might have still diverse prices and non-identical products might have same price ranges. Variant count is similarly not conclusive alone but might be useful in certain use cases, thus it is more considered an additional feature, up to the user’s choice to leverage or not. Since both price and variant count data are numerical, the method is straightforward: we scale the values to a fixed standard range, typically [0,1] , and use a numerical distance metric to calculate how dissimilar two values are. It should be noted that products typically contain more than one price values (e.g. a normal price, a seasonal price, some discount price, prices for different countries etc.) and these are set to a specific currency as well. Thus for the price comparison, we convert all values to the same currency and use the median of all the prices as a representative for the product variant. Mixed data similarity for attributes : Product attributes are special cases of data because they have mixed data types. When the data is of the same type, e.g. numerical like the prices or the text vectors described above, a similarity metric can be directly applied. But if the data is represented by a set of features that are numerical, nominal, boolean and even multi-valued (i.e. every instance described as a collection of values), then there is no common similarity metric to compare all those types in a meaningful way. For a better understanding consider the following table representing a sample of attributes of wine products. We distinguish the following attributes : numerical: contents boolean: is_available nominal: color , country , acidity multi-valued: country_availability, foods Our implementation of a distance metric which is capable to handle different types of variables is based on the Gower’s distance metric¹: we calculate the distances between two instances differently for each variable type and combine those in a final weighted distance score. Two basic decisions had to be taken here: which distance metric should be used for every type and how the distance between missing values should be handled. For missing values, the assumption is that the distance between a missing value and any other value (even missing as well) should be the maximum, i.e. 1.0. The distance metrics for each data type are described in the following. For numerical attributes it is straightforward and similar to the price similarity case. We normalize the values and use the Euclidean distance . Boolean attributes can be converted to numerical values ( 0 and 1 ) and be treated in the same way with a numerical distance metric, which will eventually give maximum similarity for identical values and zero otherwise. Categorical (also called nominal) data, like country and color, must be compared in a similar way: identical values should have the maximum similarity and zero if they are different. One commonly applied approach is encoding the categorical values with numerical. However treating these values in the same way as numerical is not meaningful and a rather poor idea because the numerical counterparts do not have any meaning other than encoding. For example, the value encoded with ‘10’ should not be considered most similar to the value encoded with ‘9’ than to the one encoded as ‘1’ and the distance between these values carries no semantic information. Thus, a different metric is needed - one that basically checks whether values are identical or not. In this case we used the Hamming distance as a metric (for more information see SciPy’s cdist ). However, categorical data can also be handled as textual in some cases. For example for an attribute ‘ colour ’ and items with values ‘green’ , ‘light green’ and ‘red’ , a categorical approach will compute zero similarity between each pair and treat them as completely distinct categories, while text similarity will not ignore the similarity that exists between the two cases of green. This is hard to achieve without human supervision though and not meaningful in every case. Additionally, text similarity is computationally expensive. Therefore, we have also included a text distance similarity method for the nominal attributes. As it is computationally intensive, it is only enabled for small product sets and a limited number of nominal attributes. It is based on the Levenshtein distance between values from the fuzzywuzzy python library . The Levenshtein distance between two texts is a string distance metric that indicates the number of single-character changes required to convert the one string to the other. Last but not least, for the multi-valued attributes, where each item can have a set of values, we apply the Jaccard distance. The Jaccard distance between two sets of values is defined as the size of their intersection divided by the size of their union, i.e. the number of the values they have in common divided by the total number of unique values that appear in both sets. Handling attributes in this context is challenging also for reasons other than the mixed types. Without inspection of the data it is hard to automatically distinguish which attributes are meaningful to consider and which add noise or whether categorical data should be treated as text or as distinct values. Furthermore, when two inventories are compared, an overlap of matching attributes should be found and the case where the attributes match but the values are different (e.g. ‘ S ’ for size in the one but ‘ small ’ in the other) must be tackled as well. In addition to that, we wanted to weight the attributes so that the more important ones influence the result more, which requires a common metric of importance for all data types. To assess their relative importance and also decide which attributes should be used, we considered both the variance and density of their values as an indicator of their discriminative ability. However, there is no single variance metric applicable to every data type, and so as a variance counterpart, we used the entropy of the values. But this only after the attributes with entropy equal to 1 were removed, because that means that all items have unique values and thus the attribute is not adding any value in identifying similar items. Last, when two different inventories are compared and the overlap of matching attributes is small, as this accounts only partly for the similarity score, we make sure that this is reflected to the final score. The feature is currently provided in beta testing phase. To handle the high number of pair comparisons explained above, the total number of products analyzed in one request is currently limited. This can be improved by switching to a distributed framework like Dask . We are looking forward to receive customer feedback in order to get a deeper understanding of the use cases and identify potential for optimisation. ¹ Gower, J. C. (1971) A general coefficient of similarity and some of its properties, Biometrics, 27, 857–874. Looking under the hood of the commercetools platform 458 2 Thanks to Amadeus Magrabi , Freddie Karlbom , Matthew , and Joelle Katz . Machine Learning Data Science Technology AI 458 claps 458 2 Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Written by Data Scientist @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-07"},
{"website": "CommerceTools", "title": "product discounts in commercetools", "author": ["Evan Bonertz"], "link": "https://techblog.commercetools.com/product-discounts-in-commercetools-1272eecfa9c0", "abstract": "About We're hiring With the holiday season approaching quickly, and Black Friday on our doorstep, brands are looking now more than ever to offer creative and exciting deals for their consumers. What will catch a buyer’s attention? How can we attract new customers with special offers? How will these promotions influence our target audience and reflect on our brand? commercetools offers one of the most powerful promotion engines of any commerce platform ever. It’s driven entirely through cloud native APIs and relies on predicate language that allows you to define custom discounts and specific rules. The purpose of this article is give an overview of some the ways to use product discounts in commercetools. This article will not cover Cart Discount or Discount Codes but these use cases will be covered in future posts. Product Discounts The most common use for product discounts among our clients is to markdown a certain set of products. It could be a defined by brand, style, size or even entire categories of items. While there are limitless options for how you can build your discounts through the predicate language using the APIs, many of the options are already configurable and managed through the Merchant Center interface. To create , activate , deactivate , update or delete a product discount, go to Merchant Center and click the “Product discounts list” under Discounts in the toolbar. To define a new product discount click Add Discount and Select Product Discount. Example: 15% off all products in the Women’s Shoes Category Add Product Discount Form Required: Choose a Product discount name Optional: Add a description Required: Define a Rank (sortOrder) Only one Product discount can be applied per product and the sortOrder determines which one. The value of sortOrder is to be between 0 and 1, and the product discount with sortOrder closest to one will be applied. Required: Define a Discount Value to assign a type and amount. Relative is a percentage amount, absolute is a dollar amount. Define the valid to and from dates Create your Product Discount Rules using the rule builder Select which products to apply a Product discount to by selecting either: Apply to all products Apply to selected products If you have selected ‘Apply to selected products’ you must first choose a condition: If any of the following apply If all of the following apply If the following does not apply Define a Rule Type, Operator and Value for the rule. Click “+ OR” to add an additional rule. Product Discount Predicate If you would like to View or Edit the Product Discount Predicate click the </> button next to Editing Options. You can read the full documentation on the predicate language here . In this example, the predicate used to apply this Product Discount to any products that contain the Women’s Shoes Category is categories.id contains any (“306058bd-9be5–4ade-9d4b-82e7ac995664”) Exclusions Here is how you exclude certain products from the Women’s Shoe Sale Product Discount. Add an additional rule. Define type as SKU (or productId, or productKey) Define operator as Is Not Listed Insert Value The associated predicate for this would look like categories.id contains any (“306058bd-9be5–4ade-9d4b-82e7ac995664”) and sku not in (“PE013–101LT-5”) Activation Once you have saved the Product Discount you will also need to Activate it using the Status switch in the top right corner. You can, of course deactivate the discount the same way with the switch. Next to the Status switch you also have the option to copy or delete this discount. Discounted Price Representation To read the developer documentation for Product Discounts visit the commercetools HTTP API docs The discounted price is stored in the discounted field of the Price attribute on the affected product and variants in commercetools. When a discount is created, updated or removed, it can sometimes take a couple minutes to reference and update all the targeted products with the discount and reflect the new prices. In the JSON Response the prices will look like this. If you have more questions about the building or managing Product Discounts in commercetools please contact support@commercetools.com Looking under the hood of the commercetools platform 76 Retail Black Friday Promotion Discount Ecommerce 76 claps 76 Written by web developer and musician Looking under the hood of the commercetools platform Written by web developer and musician Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-16"},
{"website": "CommerceTools", "title": "mutating objects what can go wrong", "author": ["Islam Farg"], "link": "https://techblog.commercetools.com/mutating-objects-what-can-go-wrong-7b89d4b8b1ac", "abstract": "About We're hiring We all know that mutating objects can lead to unwanted side effects, and can introduce hard to trace bugs, and make code hard to test. In this article we are going to go through some of the issues that can occur because of the mutating side effects, and discover how code that may seem correct is actually not. We’ll begin with a mutability-related problem and iteratively improve the code until we fix the problem. Afterwards we’ll have a look at another real world example in the form of a React app. And wrap it all up we’ll learn how to avoid these kinds of issues. The code below is very simple: we want to get a user from our data store, update that user, and then persist the changes back to our data store. However, this code has an issue related to the user object. More specifically, the reference to a user object is mutated directly (lines 7–8) which means that the users list already contains the updated user. First let’s handle the obvious problem: mutating arguments #1 and #2. In JavaScript function arguments are passed by reference when they are not primitive values . This means we should be careful when working with them and consider them as immutable values. If you pass an object (i.e. a non-primitive value, such as Array or a user-defined object) as a parameter and the function changes the object's properties, that change is visible outside the function Let’s try to pass a copy of the user object and see if we can still mutate it without side effects. The code now is a bit cleaner and we’re cloning the user object so that we can mutate its values. However, we still have the same problem as before. The problem here is that we did a shallow copy which leaves deeper levels of the object referenced, unlike deep clone which creates a new object without any reference to the original one. In our case, deep clone would solve the problem, or just keep the shallow copy and use it to create new object with the new values. Let’s refactor the code to create a new object for the name and return the user with the new name. Cool, this works now! Let’s see another example with some visuals. In the example below, calling logFirstUser and logLastUser has a side effect of mutating the users in the state. It might have been easy to find the issue here, but similar issues can occur in more complex components which makes pinning down the root cause of the issue difficult. One solution is to use one of the great libraries that provides immutable objects e.g immutable-js or immer . If this difficult exercise caution and treat objects as immutable unless there is a good reason to mutate them. Always remember that function arguments are references to JS objects (except for primitive values). Be careful when using Shallow Copy vs Deep Copy, as explained before. Try to restrict the mutation of variables to the scope in which it was created, otherwise create new variables with the desired values When working with any method on an object that mutates it (for example any of the array’s mutator methods ), try to call it on a shallow clone e.g const sortedArray = [...originalArray].sort(); and if you intend to do it in-place it might be a good idea to not assign it to a new variable, so that you don’t give the impression that you didn’t mutate the original variable. Thanks for taking the time to read this article, issues related to mutating objects can be easily mitigated by applying best practices, and keeping it in mind during code reviews. Looking under the hood of the commercetools platform 162 1 JavaScript 162 claps 162 1 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-19"},
{"website": "CommerceTools", "title": "graphql and rest level 3 hateoas", "author": ["Freddie Karlbom"], "link": "https://techblog.commercetools.com/graphql-and-rest-level-3-hateoas-70904ff1f9cf", "abstract": "About We're hiring At commercetools, we’re often asked why we choose to offer GraphQL but not REST Level 3 (HATEOAS). The short answer to this question is that HATEOAS isn’t a good fit for most modern use cases for APIs. That is why after almost 20 years, HATEOAS still hasn’t gained wide adoption among developers. GraphQL on the other hand is spreading like wildfire because it solves real-world problems. Continue reading for a more detailed breakdown of our reasoning. Let’s backtrack a bit. APIs don’t come in one shape or form. However, from early protocols such as SOAP and XML-RPC, over time RESTful architecture principles have become a de facto standard for most common use cases. Even within what we usually call REST APIs, there are varying levels of commitment to the RESTful principles as described by Roy Fielding in his 2000 PhD dissertation, “ Architectural Styles and the Design of Network-Based Software Architectures ”. A common way to describe these varying levels of commitment is to use the Richardson Maturity Model (RMM), developed by Leonard Richardson and outlined in his book RESTful WEB APIs . Our Chief Product Officer, Kelly Goetsch, describes these levels well in his book Microservices for Modern Commerce . The following are excerpts from that book. This is how most begin using REST. HTTP is simply used as a transport mechanism to shuttle data between two methods — one local, one remote. This is most often used for monolithic applications. Let’s take inventory reservation as an example to illustrate each level of maturity. To query for inventory, you’d make an HTTP POST to the monolithic application. In this example, let’s assume that it’s available behind /App . The application itself is then responsible for parsing the XML, identifying that the requester is trying to find inventory. Already, this is problematic because you need a lot of business logic to determine where to forward the request within the application. In this example, queryInventory has to be mapped back to a function that will return inventory levels: This would be the response you get back: To reserve inventory, you’d do something like this: Notice how the only HTTP verb used is POST and you’re only interacting with the application. There’s no notion of an individual service ( /Inventory ) or a service plus function ( /Inventory/reserveInventory ). Rather than interact with an entire application, level 1 calls for interacting with specific resources ( /Inventory ) and objects within those resources ( product12345 ). The resources map back neatly to microservices. Here’s how you would query for inventory with level 1: Notice how you’re interacting with /Inventory rather than /App . You’re also directly referencing the product ( product12345 ) in the URL. And this is what you get back: To actually reserve inventory, you’d do the following: Notice how you’re only interacting with /Inventory/product12345 regardless of whether you’re querying for inventory or reserving inventory. Although this is certainly an improvement over dealing with /App , it still requires a lot of business logic in your inventory microservice to parse the input and forward it to the right function within your microservice. Level 2 is a slight improvement over level 1, making use of HTTP verbs. To date, all HTTP requests have used the POST verb, whether retrieving or updating data. HTTP verbs are built for exactly this purpose. HTTP GET is used to retrieve, HTTP PUT/POST is used to create or update, HTTP DELETE is used to delete. Going back to our example, you would now use GET rather than POST to retrieve the current inventory for a product: Reserving inventory is the same as before: HTTP PUT versus POST is beyond the scope of this discussion. To create a new inventory record, you’d do the following: Rather than a standard HTTP 200 OK response, you’d get back this: With the Location of the newly created object returned in the response, the caller can now programmatically access the new object without being told its location. Even though this is an improvement over level 1 in that it introduces HTTP verbs, you’re still dealing with objects and not individual functions within those objects. Level 3 makes full use of HTTP verbs, identifies objects by URI, and offers guidance on how to programmatically interact with those objects. The APIs become self-documenting, allowing the caller of the API to very easily interact with the API and not know very much about it. This form of self-documentation allows the server to change URIs without breaking clients. Let’s go back to our inventory example. To retrieve the inventory object, you’d call this: And this is what you’d get back: Notice how the response includes link tags showing how to perform all available actions against the inventory object. Callers of the APIs just need to know Inventory.reserveInventory . From that key, they can look up the URL ( /Inventory/reserveInventory ). Now that we have a clearer definition of the different levels of REST, let’s evaluate REST Level 3 in more detail. In most cases, APIs support up to Level 2 of REST, using HTTP verbs for CRUD (Create, Read, Update, Delete) actions on resources. REST Level 3 on the other hand is seldom seen in the wild. When you consider how fast Internet technologies have evolved and changed during the last 20 years, the fact that the adoption is so limited is a clear sign that REST Level 3 doesn’t add enough value compared to the effort of implementing it for the vast majority of web services. The reason behind this is simple. Roy Fielding himself lays it out in a comment on his blog : REST is software design on the scale of decades: every detail is intended to promote software longevity and independent evolution. Many of the constraints are directly opposed to short-term efficiency. Keeping a long-term perspective on software is important, but again: adoption of REST Level 3, even on a long-term timeline of 20 years, is low. From a technological Darwinist perspective, the potential long-term value doesn’t seem to outweigh the short-term inefficiencies. On the Internet, time has proven that solving real-world problems developers are facing end up eating dissertation-style theoretical purity for breakfast almost every time – remember XHTML2 vs HTML5 ? Furthermore, for commercetools as a platform, there are specific reasons why REST Level 3 is a poor fit for our customers. Our clients often only use a subset of our APIs à la carte for specific use cases. The frontend always needs domain knowledge that can’t be passed from our server, which removes the value of including inline documentation in the response, one of REST Level 3’s key selling points. If you just use commercetools for its product catalog features, why should the Product resource link to the commercetools “add to cart” update action when the customer isn’t going to use our cart? In heterogeneous environments which interconnect multiple backend systems — the standard in ecommerce today — APIs can be provided by any system. Coupling ends up confusing rather than helpful. Self-documenting APIs are primarily useful when called directly. This might happen during an initial exploration phase, but even there, as noted above, the returned documentation might end up being irrelevant for the customers’ use case. For any non-trivial API usage, they still have to refer to the proper documentation . For implementation and production use, our customers primarily rely on our SDKs or GraphQL API , as they are far more efficient and user friendly. This includes being less verbose and more streamlined, which reduces the risk of error and increases productivity. GraphQL also allows you to piece together the exact data you need in one call, instead of having to make multiple HTTP requests to individual resources to render a page. This greatly reduces load times and data that needs to be transmitted. If both server and client were to fully utilise HATEOAS, then perhaps each of those requests would depend on the preceding request finishing, as the next action to take would be included in the server response. Does this really sound like the way you want to consume APIs? REST Level 3 might somehow have ended up at a Request For Proposal (RFP) checklist for some clients, but the overwhelming feedback we get from developers working with our platform is that it’s too complicated and time-consuming to implement. The main values that REST Level 3 could potentially add to a project are covered better by our SDKs and GraphQL API. To put it bluntly: why would someone use REST Level 3 when they could use something better? After its introduction in 2015, GraphQL spread like wildfire among developer community because of its ease of use and technical prowess. Since we at commercetools launched our GraphQL API, the adoption has been similarly fast and continues to steadily rise . To be clear before we continue; REST Level 3 and GraphQL are not fully interchangeable as technologies. What we consider the most important practical aspect of REST Level 3 for most use cases though, API discoverability, is handled better by GraphQL. In REST, the client needs to know about a number of endpoints to fetch the data it needs. REST Level 3 tries to alleviate this by including lists of potentially relevant resources and actions included in the API responses. In GraphQL on the other hand, this complexity is avoided by just having a root endpoint from where any data can be fetched. The client doesn’t have to know the exact URL where each resource is located, just what data it needs. GraphQL IDEs allow for quick navigation through the available queries and mutations, and thus provides inline documentation and discoverability. On top of that, GraphQL means fewer API calls since you can fetch complex data types including multiple resources in one request, as well as surgical precision over which data is fetched via declarative data fetching. The combination of efficiency and flexibility is of great value when, as a platform, you support a variety of use cases. Since we can never be sure which data is relevant at any point to each of our customers, it’s better to let them specify the data they want. Similarly, this declarative data fetching allows for fast iterations during development of store frontends, where additional data quickly can be added on an as needed basis without having to add additional API calls. All this adds up to faster websites and website development, which in turn leads to better user and developer experiences. Calling APIs directly with an HTTP client always requires boiler plate code. Most of our customers prefer to use one of our SDKs to help with things like formatting parameters, type-safety, encapsulation and IDE auto-completion. When abstracting away the low-level details of the API implementation, though, REST Level 3 becomes even more irrelevant. commercetools provides five open source SDKs currently: Java, JavaScript (Node), .NET, iOS (Swift) and PHP . In addition, there are Go and Python SDKs available provided by one of our great partners! At commercetools, our firm belief is that the momentum GraphQL has will sustain its growth coming years. REST Level 3 on the other hand will, besides some niche use cases, remain relatively poorly supported and understood. If you thus are thinking of implementing REST Level 3, we strongly urge you to first investigate if GraphQL might provide a better fit for your use case. Looking under the hood of the commercetools platform 410 3 Thanks to Kelly Goetsch , Evi Lazaridou , and Christoph Neijenhuis . API Rest Api Ecommerce Hateoas GraphQL 410 claps 410 3 Written by Technical Product Manager specializing in data-intensive products and machine learning. Looking under the hood of the commercetools platform Written by Technical Product Manager specializing in data-intensive products and machine learning. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-01"},
{"website": "CommerceTools", "title": "keeping a react design system consistent", "author": ["Malcolm L"], "link": "https://techblog.commercetools.com/keeping-a-react-design-system-consistent-f055160d5166", "abstract": "About We're hiring Ensuring visual consistency is one of the biggest challenges of working on a UI component library. Since visual regressions in one component can lead to unwanted changes across an entire application, it becomes vital that we test not only our components functionality, but also their visuals. What follows is the story of how we introduced visual regression testing into our React component library that implements our design system. At some point, we upgraded the library we use for SVG transformations. This led to visual regressions (all of our icons lost their ability to be coloured 😱). Suddenly all of the icons in our applications were grey. Nobody was happy with us 🤦‍♂. If we had had visual regression testing, this regression would have been caught during the pull request, and this breaking change would never have been introduced into production. It’s hard to test visuals. We need to catch inadvertent visual regressions during code review, and we need an easy way for our designers to review and approve visual changes. We are trying to achieve these two things. Catch visual changes before merging pull requests. Provide a way for designers and other stakeholders to either approve or reject these visual changes. Let’s take a look at our TextInput component, and try to write tests for two of its visual states: pristine and hasWarning . Here are those states. At first, we used a combination of code snapshot testing , and integration tests . To test these two possible visual states of the component, our tests looked like this And their snapshot output Here we were able to ensure that the DOM structure of our components remained consistent, and that when the property hasWarning was passed to <TextInput> it was given the classname 'warning' . However, it doesn’t ensure that 'warning' actually changes the colour of the input’s border to orange. Refactoring the components structure results in a failed snapshot test, which is bad. These tests give us no confidence. We should be able to refactor the internals of a component, and as long as it looks visually identical to the User, our tests should pass. To read more about the drawbacks of this kind of shallow testing, check out this blog post by Kent C. Dodds . There is also no way for our designers to easily review code snapshot updates. Users don’t see classnames. They see visual elements. If a change is made to the styles for this component, it will not be caught by our tests. If a change was made to the warning design token, it will not be caught by our tests. We knew what we wanted: to be able to refactor our components, and to make changes to our tooling, and have the confidence that by making these changes, we weren’t introducing visual regressions in any of our components. We wanted to be able to take visual snapshots of our components under different states, and have our tests catch any changes by comparing those screenshots pixel by pixel. We used jest-image-snapshot to compare snapshots, our colleague Dominik Ferber wrote jsdom-screenshot to take the snapshots. Now our test code looked like this: This is already a whole lot better: because now we are comparing real images. Instead of asserting that a component has an error classname, we are asserting that when we pass an error state to that component, that it should have red text, and a red border. It also had some drawbacks as well 😢. We had to store images in the codebase. Screenshots would depend on the machine they were taken on (so we would have had to use docker to ensure consistency). Having to stop animations with CSS before taking screenshots to avoid false negatives. No easy way for designers to review visual differences. Test performance — each test would take 1–2 seconds. While we could have spent time developing solutions to the problems with our homemade approach, we found an easier way. We decided to use Percy . With Percy, we can take visual snapshots of our components, upload them to their service, and have their service analyze visual differences. We can integrate Percy into our build pipeline, and add a check that doesn’t allow pull requests to be merged if Percy detects unapproved visual regressions. Percy is a visual testing platform that makes it easy to get started with visual testing. It captures DOM assets to render full page snapshots. You don’t have to worry about uploading assets or about updating to the latest browser. To avoid false positives and stabilize snapshots, it handle things like freezing animations and video, managing font rendering, anti-aliasing, and more. Percy offers two SDKs relevant to React users: percy-storybook and percy-puppeteer . We decided to use percy-puppeteer because we wanted to take full control of our visual testing setup. We wanted to be able to interact with our components before taking snapshots, and we wanted to use the production rollup bundle of our library, something that isn’t possible with storybook. Visual testing with our final bundle gives us confidence that our visual baselines are done with the same code that is consumed by our consumers. Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol . Puppeteer runs headless by default, but can be configured to run full (non-headless) Chrome or Chromium. We use puppeteer to interact with our UI components before snapshotting them with Percy. Let’s take a look at how we integrated visual testing with Percy into our React component library that implements our design system. First we installed the required development dependencies. We already use jest to run our integration tests, so it made sense to use jest and jest-puppeteer to run our visual tests. In order to take snapshots of our components using percy-puppeteer, we needed a web server that renders our components in their different states. Our idea was that for each component, we would create a webpage that shows that component in all of it’s possible states. So for TextInput , we would want a page that shows a TextInput with a warning state, an error state, with a placeholder, without a placeholder, etc. To do this, we created a new React app called visual-testing-app . It’s pretty basic — just a simple React app and an express server. You can check out the code here . The idea was that for each UI component that we wanted to test visually, we would create two files: .visualspec.js and .visualroute.js . Our visual-testing-app renders every file in our codebase ending in . visualroute.js as a route. These files need to expose two named exports: component and routePath . This way, we can setup our visual test environments in these files, and access those routes in our . visualspec.js files. For our TextInput component, we created text-input.visualspec.js and text-input.visualroute.js . The former would make assertion on the HTML created by the latter. The relevant code in App.js to achieve this goal of one component per visualroute.js file can be seen below. We then created a basic webpack configuration to build our visual-testing-app , and an express app to serve it. Now we are ready to create our first visualroute.js file: text-input.visualroute.js . The code is pretty straightforward. We create two named exports: routePath and component . These exports are required because when we build our visual-testing-app , it creates a route for each routePath , and renders it with the specified component . First we build our visual-testing-app by running yarn visual-testing-app:build , which creates a route for each file ending in visualroute.js . Then we start the server by running yarn visual-testing-app:serve . Let’s check out what http://locahost:3001/text-input looks like To get up and running with jest-puppeteer, we need to setup some boilerplate. First, we create jest-puppeteer.config.js . Next we created a new jest configuration: jest.visual.config.js Finally, we add a new script to our package.json to execute our visual tests. Now we were are ready to create our first visual test! We create text-input.visualspec.js and add the following code. We can use the global page object (this comes from jest-puppeteer) to navigate to our visualroute , and we make a simple text assertion. This ensures that we don’t take any snapshots of our not found route. We can then run yarn test:visual to run our test. So far so good — our visual assertion passed, but Percy didn’t take any snapshots! To get Percy taking snapshots, we add the following script to our package.json . Now, after setting up Percy for local development (hint, add PERCY_TOKEN and PERCY_BRANCH to your environment), we ran yarn percy . This command runs our visual tests, and uploads all of our snapshots to Percy. We made two custom components: Suite and Spec . Suite sets up a few requirements for our components, like react-intl . Spec gives a min-height to each of our tests, to stop changes in a component’s height from causing any tests below it to fail. It also gives a bit of styling to each test, and outputs the props visually so it’s easier to review differences. Our visual route now looks like this Again, we run the following commands to build and serve our visual-testing-app. And navigate our browser to http://locahost:3001/text-input That’s more like it! Let’s start by making a change to the style of the TextInput component. We can change the background colour from white to pink, and the text colour to white. This should make our component look beautiful. 💯 Then we run the following commands to build our bundle, build our visual testing app, and run our visual tests. Our Percy project is public, so you can check out the diff yourself . Below you can see that Percy is highlighting visual regressions to our TextField component, as the TextInput now has a pink background. We can toggle the diff by clicking and see in red the highlighted diff. We can easily interact with our components with puppeteer before taking our snapshots with Percy. This is important, because we have a few components that have visual states that cannot be triggered with props. One such component is our DateInput . There is no defaultOpen prop, so the only way to take a snapshot of an open DateInput is by clicking on it. First, we create a new date-input.visualroute.js file. Again, we verify it locally by running Looking good! Next, we create our date-input.visualspec.js file. Finally, we run yarn percy to see the snapshots. You can see the Percy output here . Visual Regression Testing is hard . Making our own solution just isn’t feasible, and we are lucky that the good people at Percy provide such an incredible service. We were able to remove all tests that were asserting classnames, and we were able to abandon all code snapshots. We are very happy with visual regression testing in general, and we wouldn’t want to go back to a world without it. We feel a new found confidence when merging pull requests. To learn more about our setup, you can check out our visual testing README and our UI component repository . A special thanks goes out to Dominik Ferber for all of his help with this setup. Looking under the hood of the commercetools platform 594 2 Thanks to Dominik Ferber , Nicola Molinari , Celeste H. , Tobias Deekens , and Islam Farg . JavaScript Reactjs Testing Puppeteer Visual Regression Testing 594 claps 594 2 Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Written by JavaScript Consultant. Senior React developer. Always learning something new. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-09"},
{"website": "CommerceTools", "title": "prisma horizontal scaling a practical guide", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/prisma-horizontal-scaling-a-practical-guide-3a05833d4fc3", "abstract": "About We're hiring If you’re reading this, you probably heard about Prisma . If not, you will find some references about it at the end of this article. Prisma is a performant open-source GraphQL ORM-like* layer doing the heavy lifting in your GraphQL server. Prisma is the glue between your GraphQL API and database. Assuming that you are now familiar with Prisma and are eager to use it, you are quickly confronted with a question: “ Should I be hosting my own Prisma cluster or use the managed version Prisma Cloud? ”. We also faced this question and the answer depends on your company and project’s needs. We needed to use our own database while wanting to have full control over the deployment in our already existing Kubernetes cluster. Given this context we wanted to share our experience on the self-hosting part and how we manage it here at commercetools. When you get started with Prisma you want to run things locally first. Given that you have Docker installed, you run a docker-compose file that will spin up a database (e.g. Postgres) and the Prisma service from the official Docker image . That’s it! However, running in a production environment is a whole different story, as you need to ensure a scalable and available system . There is a GitHub Issue that describes the struggles with the setup for Horizontal Scaling . I bumped into this issue myself and, with some help from the lovely Prisma devs, managed to make it work. Other people are still confused though and are apparently still struggling with a proper setup. In retrospect, many of these aspects are not very clear at the beginning, hence the confusion. However, the documentation and tutorials are being constantly improved and hopefully things will be easier to grasp in the future. With that in mind, I’d like to illustrate how we run a production Prisma setup in our Kubernetes cluster and what parts you need to pay attention to. There are several resources out there that focus on the same thing: there is a Helm Chart for Prisma , there is a tutorial for running Prisma on Kubernetes and some others. Somehow though, they still lack of a proper way of scaling horizontally as also reported by one of the core developers of Prisma . I’ve also been asked to provide some sort of template to run Prisma on Kubernetes. I believe that having a single way of setting things up is not enough and makes things more complicated. More specifically, a production Prisma setup includes several components and the different variations and combination of those components is what makes it complicated to cover all possible cases. On top you might also have cloud vendors specific configurations. Therefore, I decided to write an article instead about how we at commercetools chose to do it to better fit within our infrastructure. Hopefully this will help other people making the correct choices given their setup and requirements. When we talk about “cluster”, we usually refer to a group of similar components that work together as a single unit in a distributed way. This is what allows things like High Availability (HA). When we run a service like Prisma on Kubernetes, we want to run it with at least a replica of 2 instances to ensure some sort of availability and to be able to scale up if necessary. The Prisma service itself is a stateful application and is composed of the following elements: the Prisma API: a server running on a JVM that exposes a GraphQL API plus a Management API ( more of that later ) the DB connector: traditionally for a relational database (MySQL, PostgreSQL) but other DB types are and will be supported as well the cache layer: holds a cache of the GraphQL schema to reduce latency on query execution the webhook/subscription layer: handles GraphQL subcriptions and webhooks As you can imagine, all those different parts need to efficiently work when the Prisma service is replicated over multiple instances. At commercetools, we run most of our services on GCP. We also live and work with a decentralized DevOps culture across our teams, meaning that besides the main Operations Team, each team is empowered and responsible of managing their own services. In particular, managing a Database with HA is not something you wish to be doing on your own. Luckily, most of the big cloud providers (Google, AWS, etc) offer several managed services that you are more than happy to pay for. In our case, we ended up with the following setup on Kubernetes: own Helm chart to deploy and scale the services managed Google Cloud SQL (Postgres) as the database official Helm chart for RabbitMQ as a dependency of our own Helm chart Google KMS to encrypt/decrypt our secret values The RabbitMQ component is essential for HA as explained in this comment . In our case, we were only interested in maintaining a synced cache layer and therefore opted for a simple Kubernetes installation of RabbitMQ managed by Helm. Depending on your requirements, you might opt for other solutions to ensure your RabbitMQ service is robust and scalable. NOTE: there is an open Issue about supporting other types of PubSub systems besides RabbitMQ, so different options might be available in the future. Once we settled on those initial requirements, we started implementing our own Helm chart. There is one very important thing to consider though: the Prisma Management API . This is a special API endpoint that the Prisma server and Prisma CLI use to perform administrative tasks, such as deploying schema changes etc. However, only one instance of the Prisma service is allowed to run with this API enabled . You do so by setting enableManagementApi: true in the PRISMA_CONFIG environment variable. When the Prisma service starts, it will try to get a lock, claiming to have the Management API enabled. This means that, across all the instances of the Prisma service, only one is supposed to work with the Management API enabled . This makes the setup a bit more complicated. Another small caveat is the PRISMA_CONFIG environment variable. This variable contains both sensitive and non-sensitive information which makes it a bit tricky to deal with. For instance, you want to ensure that sensitive information is kept encrypted and at the same time the normal configuration is easily accessible and reusable. In our case, as you will see later on, we decided to encrypt the entire PRISMA_CONFIG for security reasons (as you should do) and simply duplicate it into two Secrets to differentiate them from the enableManagementApi option. Hopefully the Prisma team will improve this in the future to make it simpler to manage. With the caveats and considerations described in the previous sections, we can now look at the Helm chart itself. The main peculiarity of the chart is that it contains two sets of Deployments and Secrets : one that does not have the Management API enabled and can be scaled up with multiple replicas . Those multiple instances are load balanced by a Service which can be exposed by an Ingress (depending on the requirements). one that has the Management API enabled and has a fixed replica of 1 . This Kubernetes resource consists only of a Deployment , no Service or Ingress attached to it. The chart contains the following files, which we are going to look into. To install the chart dependency, define a requirements.yaml and follow the Helm instructions to install it. The configuration for the RabbitMQ chart can be provided within the main values.yaml as long as it’s grouped by a key with the same name as the dependency chart rabbitmq . Sensitive information should be as always encrypted (in our case we define them in a secrets.yaml which gets encrypted with Google KMS). As mentioned before, we use two sets of Secrets . Notice that, besides the names, the only difference is the enableManagementApi value. Similarly, Deployments are also defined twice. NOTE: the Docker image for Prisma production is called prismagraphql/prisma-prod , which is different from prismagraphql/prisma ! ( source ) The only thing to note here is that it points to the “normal” Deployment , not the management one. Simply run the helm install command: And you should eventually see something like this: Just for the sake of completeness, when you want to deploy e.g. a schema change, you would need to connect to the prisma-management-xxx-xx pod and execute the prisma deploy command. In our case, the .env contains the following variables: Even though this guide is tailored to our setup at commercetools (e.g. Google CloudSQL), I hope it provides some useful information about getting a Prisma service up and running for production. I’m sure in the future there will be more generic solutions that might “just work” out-of-the-box. Even then, it might not cover all use cases and might not work for you. In that case, I hope this article to remain useful and to inspire you in setting up the different parts of a self-hosted Prisma cluster in detail. With that said, Prisma is great and you should definitely check it out and contribute back to its community. Prisma ( Official Website ) Prisma ( Blog ) Prisma Horizontal Scaling ( GitHub Issue ) Prisma Bindings ( HowToGraphql , Official Website ) Looking under the hood of the commercetools platform 553 3 Thanks to Tobias Deekens and Kelly Goetsch . Kubernetes Prisma Scalability DevOps GraphQL 553 claps 553 3 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-23"},
{"website": "CommerceTools", "title": "why seven is the magic team size", "author": ["Kelly Goetsch"], "link": "https://techblog.commercetools.com/why-seven-is-the-magic-team-size-8c94d1d9f4ba", "abstract": "About We're hiring The throughput of any development team is the collective throughput of the people on that team. People are happiest and most productive when they’re on a team that’s the right size. Two person teams are clearly bad, as are 100 person teams. So what is the right size? Software development is fairly unique among professions because close, trusting relationships are required to get anything done. Software development is an interconnected value chain, with each member of that chain contributing some work that is then consumed by another member of the team. Back-end developers build code fronted by APIs, which the front-end developers consume. Designers model user interfaces, which the front-end developers implement by consuming APIs. Both front-end and back-end developers must deploy code with the help of ops. It’s one big chain of interconnected dependencies. In many other professions, you can get additional throughput from a team by adding more members to that team. If you have a hotel full of rooms that need to be cleaned, you’ll double your throughput if you double your team size from 10 to 20. Since each cleaner works in their own room, you can parallelize the work as much as you’d like. You could go from a team of 10 to a team of 100 and get a perfectly proportional 10x improvement in throughput. Software development doesn’t work that way. As Fred Brooks said in his famous 1975 book, The Mythical Man Month , “Adding human resources to a late software project makes it later.” If you have 10 developers working on a project and double the team size to 20, it just slows it down further. If you went from 10 to 100 developers, the project would crawl to a complete stop. So what is the right size? Let’s look at a few considerations. Small teams are terrible for individual productivity and happiness. Two, three and four person teams suffer from a diversity of skills. Developing software requires many different disciplines including architecture, design, front-end, back-end, ops, security, etc with each person bringing years of experience. Many people bring experience from other disciplines. For example, it’s not uncommon that a designer started his or her career as a front-end developer, or that an ops person also has experience as a back-end developer. Good teams draw on the experience of many different individuals to come up with full stack solutions to problems that are inherently very complex to solve. Two, three and four person teams just don’t have enough skills to draw from. Another issue with small teams is instability due to interpersonal conflicts. If you have two people on a team, it can be like a bad marriage if the two individuals aren’t getting along. If you have three, it can devolve into a two versus one situation where someone feels left out. If you have four, you can have a two versus two situation, or even worse, a three versus one situation. Disagreements become personal much more quickly than they otherwise would. A “no” to your great new idea feels a lot worse if it’s coming from one or two other people than it does as the consensus of a larger group. Large teams, on the other hand, can be even worse than smaller teams. In creative professions like software development, important decisions must be continually made about what’s being worked on and how problems are solved. Are you going to re-factor now or in six months? Are you going to use React or Angular for your front-end? Are you going to work on supporting a new discount type or supporting purchase orders as a payment method? In large teams, there’s strong social pressure to be a team player and go along with the flow. People who speak up and say “Wait, why are we doing this?” are labeled as not being team players, or difficult to work with. In smaller teams, where each team member knows and trusts every other team member, it’s far easier for someone to break out of that group think. The problem of group think can extend beyond the boundaries of individual teams and extend to the rest of the organization. According to a Wall Street Journal profile of Amazon CEO Jeff Bezos : At an offsite retreat where some managers suggested employees should start communicating more with each other, Bezos stood up and declared, “No, communication is terrible!” He wanted a decentralized, even disorganized company where independent ideas would prevail over groupthink. Individuals within teams and teams themselves should feel completely empowered to question the status quo. Small, independent teams that can function autonomously are best for innovation. Another issue with larger teams is the sheer number of communication pathways that members have within teams. Software development requires that members within a team trust each other. When trust is lacking, you see fighting between factions of groups, the need for more documentation, and more process in the form of change requests, forms, approvals, etc. The formal term for this is “Cover Your Ass Syndrome” and is defined as : “the bureaucratic technique of averting future accusations of policy error or wrongdoing by deflecting responsibility in advance”. It often involves diffusing responsibility for one’s actions as a form of insurance against possible future negative repercussions. It can denote a type of institutional risk-averse mentality which works against accountability and responsibility, often characterized by excessive paperwork and documentation , which can be harmful to the institution’s overall effectiveness. The activity, sometimes seen as instinctive, is generally unnecessary towards accomplishing the goals of the organization, but helpful to protect a particular individual’s career within it, and it can be seen as a type of institutional corruption working against individual initiative. This lack of trust within and between large teams is why enterprise development is so painfully slow . It’s why those big multi-billion dollar IT projects in the private and especially the public sectors never succeed. Building trust within and across teams requires communication, which is also known as “social grooming.” Primates physically groom each other (hence the “grooming” in “social grooming”) by picking bugs out of each other’s fur. Humans, on the other hand, eat meals together and socialize outside of work. Both of these activities serve identical purposes — to bond, reinforce group membership, and build trust. We all do it and it’s what allows business to get done. Working in a purely transactional workplace would be terrible. The problem is that you can’t maintain meaningful connections with more than a few people. As groups add more members, the number of connections increases exponentially according to the formula [N * (N-1)]/2, where N = group size. A two person group has only one communication pathway. Person A can interact directly with Person B. A three person group has three communication pathways. A four person group has six. A 10 person group 45. A 20 person group has 190. Humans simply can’t maintain more than a handful of meaningful, trusting relationships at work. It’s a limitation of the human brain. Finally, the last issue with larger teams is social loafing. There’s a famous term for this in sociology called the Ringelmann Effect , named for a French agricultural engineer who in 1913 discovered that individuals exerted less individual effort as the number of people pulling on a rope increased. This effect has been replicated by dozens of subsequent studies across dozens of different tasks ranging from rowing a boat to assembling large pieces of machinery. As group sizes increase, members reduce their effort because they feel less responsible for the output of the group. A member of a five person group bears 20% of the outcome of the project, whereas a member of a 25 person team bears 4% of the outcome of the project. When the link between effort and outcome is severed, group members become demotivated and allow their colleagues to pick up the slack. People naturally want to own things. They want to be responsible. They want others to see their efforts and talents. They want people to tell their friends about the cool new feature they were responsible for bringing to market. Given the clear drawbacks of both small and large teams, what’s the right size? At commercetools, we’ve found the best size to be seven, plus or minus two members. It’s not a hard rule, but it is a range that we aspire to and often adhere to. Seven is a fairly standard number across many disciplines. Basketball teams have members on the court. Volleyball and hockey teams have six. Baseball teams have nine. The smallest unit of most militaries around the world is a squad, which has between six-10 members. Amazon has a famous “ two pizza team rule ,” meaning no team can be larger than can be fed by two pizzas. Most of the agile and microservice gurus call for seven person teams. Seven seems to be a magic number that has worked well for us at commercetools. It’s a large enough team that it can draw on the skills from many people, while avoiding the interpersonal conflicts that often arise from smaller teams. Each member of the team likely has a backup (or two), so that if one designer goes on vacation the other can take over, for example. Members of the team can build long-term, trusting relationships with each other. The whole team can go out to eat and sit at one table. The communication overhead is fairly minimal, with there are only 21 communication pathways (only six per individual) to maintain. In short, it’s a good size that works for all constituencies. Looking under the hood of the commercetools platform 17 1 Agile Commerce 17 claps 17 1 Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-26"},
{"website": "CommerceTools", "title": "best practices for handling api clients", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/best-practices-for-handling-api-clients-bb94ecfb6350", "abstract": "About We're hiring We recently re-designed our API Client UI from scratch. In this post, I want to share best practices on handling Access Secrets, and how these influenced our new design. Every micro-service, every frontend, every developer, and whoever else directly accesses your API should get its own API Client. This allows request correlation, and to revoke an API Client quickly when you find out it has been leaked. But most importantly, it is the basis for the principle of least privilege: For example, a micro-service importing products must not be able to read order data. Should this service be hacked and the API Client stolen, your customer data is still secure. For whatever reason an API Client isn’t being used anymore (e.g. a developer leaves the company, a micro-service is replaced with a newer version), you should remove it. If it falls into the wrong hands, no harm can be done anymore. You should rotate API Clients on special events, e.g. when a DevOps who handled API Clients leaves your company. But it’s even better if you also rotate them on a fixed schedule. Should an API Client leak, you at least limit the timeframe that it can be used. Truth to be told, our old UI was not optimized for security. Instead, it was focused on lowering the friction for new developers. As an unknown startup at the time, we had to convince them that our product was worth using, and losing them while setting up an API Client was a no-go. Therefore, an API Client with full permissions was setup with each project. A developer could just copy it, and get going. While that was great for onboarding, we found that this default API Client discouraged users from creating new clients, or clients with limited permissions. The biggest change that we’ve done is that API Client secrets are one-time-view only ! Everyone has to create their own client, and we don’t select full permissions by default anymore. While the new UX means a bit more work, it strongly encourages two best practices I’ve shared: An API Client per service, and minimal permissions. But we’ve tried to make up for it! We have a few permission-templates ready, and it is now possible to download the API Client in multiple formats . Instead of having to copy-paste multiple values (e.g. the project key, client id and client secret) into a configuration, you can simply download the full configuration. This is available for all of our SDKs, cURL, bash and Postman. The total time spent setting up a SDK has probably decreased. API Clients now also show the last time they’ve been used, making it easier to spot and remove unused ones. To assist with regularly rotating API Clients, we show the age of each client, and offer a convenient duplication button . Changing the API Clients UI may seem like a small thing — but it actually involved participation from nearly every team within the commercetools product organization! We had to change every tutorial, be it on our main website, in a GitHub Readme, or on a training slide. We worked with every SDK to have their configuration ready to be downloaded. We had to change our authentication service to be able to display the last time an API Client has been used. Our UX Designer had to do a deep-dive into OpsSec and Developer setups. Last but not least, we had to implement the UI itself! Thanks a lot to everyone involved. Our API Clients UI is a great case study how far along our product has come. We’ve moved on from an early-stage startup to an established product used by large enterprises. Our new UI is prioritizing security above everything else — but we still care about developers like on day one. Looking under the hood of the commercetools platform 43 1 Thanks to Tobias Deekens , Laura Luiz , Michael Schleichardt , Dominik Ferber , and Nicola Molinari . API Oauth Security Developer Experience 43 claps 43 1 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-20"},
{"website": "CommerceTools", "title": "why were investing in the cloudevents specification", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/why-were-investing-in-the-cloudevents-specification-fba3e5a91897", "abstract": "About We're hiring I’m thrilled to announce that we’ve released support for CloudEvents 0.1! The specification is an effort by the Cloud Native Computing Foundation. The goal is to provide a common way to describe events. Technical details of our integration can be found in the release notes , but here I’d like to lay out our motivation to invest in CloudEvents . The commercetools platform has always been a cloud-native, event-driven architecture. A couple of years ago, we’d be the only, or one of a few systems with such an architecture in our customer projects. Some of our clients chose us as the first step into this “new world”. In the customer projects starting today, an event-driven-architecture is becoming the new normal. There is still an abundance of legacy systems that need to be integrated with, but more and more of the systems we integrate with are event-driven. This is partly because other startups, and their event-driven products, are rising in marketshare, and partly because existing vendors (have to) offer event-driven architectures. But it’s also because more and more internal applications are being built with cloud-native architectures. This is also driven by the cloud vendors — the focus has changed from porting existing application architectures to VMs (like EC2) to enabling new architectures, built on top of kubernetes or serverless (like AWS Lambda). From the outside, those event-driven-architectures look the same. On a whiteboard, it’s easy to connect one to the other. In practice, it’s not so easy, at least when you start connecting services outside of your cloud provider. Let’s take AWS S3 as one example. You can subscribe to a single event ( s3:ObjectCreated:Copy) or a set of events ( s3:ObjectCreated:* ). You can also filter on events (e.g. only files with suffix .jpg ). Those events can now be pushed to SNS, SQS or straight to a Lambda function. Compare that with an external service. Let’s pick Stripe as a company which is generally known for providing a good API. With Stripe, I can either subscribe to all events, or to a list of specific events ( customer.created , customer.updated, customer.deleted ). I can not filter them (e.g. by currency). The events are pushed via Webhooks. Again, there is nothing wrong with Stripes implementation. Conceptually, it isn’t different from AWS events. The difficulties start when we combine events from AWS with events from Stripe. They are not only configured differently, but also error handling, monitoring and alerting are different. Put another way: If I add events from another AWS service (e.g. DynamoDB), the operational complexity stays the same, and the learning curve is quite flat. If I add events from an external service (such as Stripe), my operational complexity doubles, and I have to learn many implementation details. And any mid-size commerce project will add at least half a dozen external services like Stripe. To resolve this mess, we need a middleware that handles all events, regardless of origin. We need a middleware that accepts events from all services, routes them, provides a common error handling as well as monitoring and alerting. A big part of the value services like Zapier or IFTTT provide is that they provide such a middleware: Events are accepted from a popular apps and handled uniformly. This can currently only be achieved by painstakingly integrating with every single app. This Sisyphean task is necessary because there is no common way to describe events. Actually, there are a lot of ways to describe events (every Message Queue or Message Broker comes with one), but none has significant marketshare in the cloud. CloudEvents is a specification for describing event data in a common way. With a common way to describe events, event producers (like Stripe or commercetools) can publish events in a way that a middleware can understand it, route it, and provider common error handling and monitoring. The success of CloudEvents does not only depend on the quality of the specification itself, but also on industry adoption. Version 0.1 has been released, and the Serverless Event Gateway , Azure EventGrid and other middlewares are already supporting the standard. We’re proud to join these thought leaders. Not because it will make things easier today, but because we’re working towards better interoperability in the future. We like to believe that commercetools has played a role in establishing event-driven architectures in the commerce space. And we hope that we can play a small role in supporting CloudEvents, too! Looking under the hood of the commercetools platform 6 Thanks to Sven Müller , Amadeus Magrabi , and Dominik Ferber . AWS Cloudevents Cloud Computing Serverless Azure 6 claps 6 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-07"},
{"website": "CommerceTools", "title": "how commercetools hires product managers product owners and product marketing managers", "author": ["Kelly Goetsch"], "link": "https://techblog.commercetools.com/how-commercetools-hires-product-managers-product-owners-and-product-marketing-managers-178e7b14fb9b", "abstract": "About We're hiring I’m a product person through and through. While I spent the first seven years of my career as a developer and later as an architect, I never felt truly “at home.” I was always missing the “why” of what I was doing. Why did business need to publish a million price updates once an hour? Why was the store locator enhancement prioritized more highly than supporting a new promotion type? After making the transition to product management, I quickly came to understand the “why.” It was my calling and I’ve never looked back. Fundamentally, all forms of product management are about tradeoff management. Given a scarce number of people, budget, time, and always-growing list of business needs, what’s the most optimal way to satisfy the most stakeholders (with customers’ business problems being priority #1)? Before we get too far, let’s define those stakeholders (in no particular order). They include: Other business units within your organization — sales/marketing/professional services/support The executive management of your organization, including your CEO Internal development teams Organizations who buy your product End-shoppers who use the applications your customers build External systems integrators (SIs) and independent software vendors (ISVs) with whom you partner Your board members Your investors The broader society, depending on the type of product you have Essentially anyone involved in your company’s success or failure has an interest in what products you bring to market and what features those products have. You can’t satisfy all of your stakeholders. There are always tradeoffs. Let’s start with some historical context. In the early days of software development, a product manager would talk to all of the stakeholders and over the course of a few months or years compile an enormous document with thousands of individual requirements. Every individual feature was defined in excruciating detail, often including wireframes. When this document was finished, it was passed off to a project manager who would then be responsible for working with developers to actually build the product or feature. Before agile development really took off in the early 2000’s, this was almost universally how software was developed. When agile started to take off in the early 2000’s, there arose a need for a new type of product manager — a “product owner.” Agile calls for small, vertical, cross-functional development teams working on the same domain. With these small teams now more or less autonomous, and the work much more iterative, the old “throw it over the fence” style of product management no longer worked. Teams iterated constantly, re-prioritizing their backlogs as business needs dictate. In this model, product managers can’t be off somewhere else writing a static requirements document. Product owners are often called “product managers for agile development.” As a member of each agile team, they’re responsible for managing each team’s backlog, writing user stories, developing acceptance criteria, providing developers with context, mapping out dependencies between development teams, involving design, and bringing the voice of the customer back to developers. Product owners work most closely with their development team and their peers, rather than with other business units or external stakeholders. In some organizations, product owners are also known as “inbound product managers” because of their internally-facing orientation. Product owners should work closely with development managers but should never cross the line and assume those responsibilities. It’s a fine line, but a good product owner should motivate architects, designers, developers, and other members of the team by providing them with the “why” but not start demanding how they do their jobs. Similarly, a product owner is not the lead of the team. Developers should report to development managers, not product owners. Product owners are part of the team but not the managers of the team. Just because management often goes to product owners for updates, it does not automatically mean that person is the leader even though it may look like that from the outside. When we hire product owners, we look for emotionally intelligent people who are strong communicators. Emotionally intelligent people can recognize their own emotions and those with whom they’re communicating and use that understanding to resolve conflicts in a way that makes all stakeholders happy. “Happy” may mean getting your way, but it may also mean having your voice fully heard and having a good understanding of why you didn’t get your way. Additionally, we look for people who can speak intelligently to and be well respected by business stakeholders and by technical development teams. Professional services, for example, needs to know that you understand their needs enough to represent their interests when making decisions. The development team needs to know that you understand enough of the technical implementation details to represent their interests when making decisions. Blending business with technical knowledge is a difficult task that few can do well. Short of “managing” teams, it’s important that product owners can get teams to deliver. Teams that have a clearly articulated minimum viable product, well written requirements, and understand the “why” are able to deliver more than teams lacking that context. Product owners should also help to corral the more academic developers, or developers who are working on things that might be cool but not adding much business value. There’s a balance that needs to be struck between being part of the team and pushing the team to get features released as quickly as possible to satisfy various non-developer stakeholders. Developers should never see product owners as the managers of the team though. Finally, candidates must have the ability to develop a deep understanding of their team’s problem domain. At commercetools, we have small vertical teams that focus on checkout, product discovery, PIM, machine learning, etc. Each team has a dedicated product owner. There are probably 10 people in the world who are product owners at commerce platform vendors focusing just on checkout. We hire for aptitude, ability and attitude, not very granular domain expertise. That can be quickly learned over a month or two because each team has a fairly narrow scope. We do try to hire people with general commerce experience, whether on the vendor-side or on the customer-side. Often, successful candidates have had hands-on technical roles at some point but later went back to get an MBA or equivalent business-focused graduate degree. Product managers have a more broad set of responsibilities than product owners and are typically found in larger companies. Where product owners work most closely with their development team, product managers work most closely with the broader organization and other stakeholders, both internal and external. Within an organization, product managers often work with sales and marketing to define pricing and licensing terms of a product, with marketing to define how the product is taken to market, with professional services to ensure they’re properly trained on the product, with legal to work through any 3rd party licensing issues, with customer success and support teams to ensure that customers are fully supported. Many call product managers “mini-CEOs” of their products because of their cross-functional role in their product’s success. Externally, product managers evaluate the competitive landscape and broader market in which you compete, educate analysts from firms like Gartner and Forrester on your product, partner with 3rd party software vendors to fill gaps in the product offering, and build relationships with key customers. Product managers who focus more on the external-part of the role are sometimes known as outbound product managers or even product marketing managers, which I’ll cover shortly. Product managers are responsible for taking feedback from all of those internal and external sources, thinking of new business ideas, and producing a higher-level roadmap. Product owners are then tasked with turning the higher-level strategic vision into actual requirements for their individual vertical teams and then working with the development team to actually deliver. Like product owners, product managers are not the managers of teams. Instead, they’re at the nexus of internal and external stakeholders, working closely with product owners and development managers to get the product built. But they’re probably not in every feature clarification meeting, or looking at the progress of individual sprints. The ideal candidate for a product manager is quite similar to a product owner. Emotional intelligence and communication skills are of the utmost importance. However, a good product manager needs to have more business experience and acumen because the majority of their time is spent working with other business units and external stakeholders rather than with internal development teams. Additionally, we look for people with outside domain expertise. We want someone who can name our top five competitors and provide an analysis of where each fits in the market. We want someone who can talk about how a Digital Experience Platform differs from a Content Management System. Product owners don’t need to have that market context. At commercetools, we have a Chief Product Officer (yours truly) and a lead PO who are more focused on product management. We then have product owners for each of the vertical teams, though it’s more of a 40% external / 60% internal split, and not the 80% / 20% splits discussed earlier. A product marketing manager is someone who represents the product externally, sitting squarely between the product and marketing groups within an organization. Every day the product marketing manager should be working closely with product managers, product owners, architects and developers to find innovative features to market externally. Developers and product owners sometimes can’t articulate the novelty or business value of something they’re working on, but a product marketing manager does and will help to maximize the value of the feature by marketing it both within an organization and publicly. For example — commercetools has supported GraphQL since 2016. The benefits of GraphQL are crystal clear to anyone who has worked with a REST API. But what’s the business value of GraphQL to a Chief Marketing Officer who has never written a line of code in his or her life? Bridging that gap is the role of the product marketing manager. Additionally, a product marketing manager is often responsible for sales enablement, which involves training sales and pre-sales consultants on the product and giving them the collateral they need to effectively sell the product. Collateral can include PowerPoint presentations, white papers, fact sheets, demos, battle cards, and so on. Product marketing managers also keep a close eye on the competition and constantly talk with customers. This feedback is then given to product managers and/or product owners, depending on your organization. A product marketing manager is your company’s most externally-facing member of the product team, in contrast to product owners who spend the vast majority of their time with their development teams. Product marketing managers generally don’t build roadmaps, write requirements, or work with development teams to advance the product. Their role is to supply product managers and/or product owners with ideas based on what they’re seeing from the outside. They are still making tradeoffs because a product marketing manager may hear dozens of feature requests every day and not all of them have the same impact. In the early stages of a company, a product manager often plays this role. As the company grows, dedicated product marketing managers need to be hired who really know what they’re doing and can give this role the time and attention it deserves. At commercetools, our first product marketing manager will start in a few weeks and we couldn’t be more excited. Successful product marketing managers are often ex-product managers or product owners who have gained marketing skills, either through education or experience. As the role is half marketing and half product, good product marketing managers need to be able to credibly speak to both organizations. There’s a wide spectrum of product-related professions, ranging from product owner to product manager to product marketing manager, though all are fundamentally involved in satisfying stakeholders through tradeoff management. Communication and emotional intelligence are the two traits we most look for in potential candidates. Looking under the hood of the commercetools platform 12 Thanks to Ida Olsen , Christoph Neijenhuis , and Camilo Jimenez . Product Product Management Product Development Ecommerce SaaS 12 claps 12 Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-21"},
{"website": "CommerceTools", "title": "render props apollo and formik build and compose forms in react with ease", "author": ["Tobias Deekens"], "link": "https://techblog.commercetools.com/render-props-apollo-and-formik-build-and-compose-forms-in-react-with-ease-f79a594be239", "abstract": "About We're hiring Please accept my apologies for the clickbaity and buzzword heavy title. I promise the post to be practical from now on. I also hope you enjoy it. Let’s face it: forms are hard . Especially without a good form library doing the heavy lifting. Forms are easy to get wrong . State management easily becomes error prone. Maintainability of associated code is often brittle. Lastly, forms are often subject to overly eager abstractions in both library- and user land code which leads to a lot of hard to grasp and change “magic” and even worse: performance problems. In this post we will look at: How to get API data in and out of forms (from Apollo and GraphQL) How to split big forms into smaller building blocks How to validate form data during user input and before submission How to compose everything using Render Props It is recommended for this post to have a slight understanding of what Apollo and Formik are and solve. No deeper understanding of their internals is necessary. Let us assume we are tasked to build a discount code form . A form to create or edit a discount code which for illustrative purposes only consists of a name and description field. For completeness assume that a user can view discount codes from a list to get to an existing discount code. It is often best to split concerns into meaningful components before starting to write code. Based on our experience so far after trying different approaches, we finally started settling on a structure (or even architecture) we feel comfortable with. From a component-tree perspective, it looks like this. More detailed components implementations will follow. The <DiscountCodeDetails> receive the needed id from a router while the rest is composed using Render Props. Before we dive into the components in more detail let us discuss their concerns and responsibilities: 🦔 Create or Details: encapsulates the needed business logic and validation for creating or viewing/editing an existing discount code. It renders the form passing the initialValues 🌉 Connector: a component solely responsible for data fetching and offering mutations on it. Passing down so-called fetchers and mutators through its FaaC (Function as a Child) 🎹 Form: is a collection of fields and encapsulates the validation. It only receives initialValues and an onSubmit . It invokes onSubmit with what we call a draft 👩‍👦‍👦 Subform: a component grouping a set of related fields. It receives mostly only formik as a prop without abstracting anything additionally (like internal validation or a custom onChange callback) 🗒 Field: a combination of an input and its validation messages. It mostly operates on a single property of the form data (e.g. discount name) 🖋 Input: is either of a number input, string input, checkbox or radio. Often for us also quite “complex” input like a custom built MoneyInput We established these naming patterns across our teams which helps a lot when looking at a component as it becomes immediately obvious what purpose and reasoning stands behind it. Having a rough understanding of what components are involved we quickly realized that some utilities are needed to tie things together nicely: ⚗️ Validations: a small module exporting a validate function. Validation is a form level, not field (or input) level, concern. It will be passed to Formik as a prop. ⚙️ Conversions: a module to cleanly map data into and out of the form so that no field needs to parse or format its data. The module comes with two symmetric functions: responseToFromValues and formValuesToRequest . Having component hierarchy and split of concerns figured out we can finally write some code. Up until here, the post was text heavy, now it will focus on code. To ease understanding (hopefully) the order of components will be rearranged starting with the connector and the form. For this post, we will only focus on the <DiscountCodeDetailsConnector> with a <DiscountCodeCreateConnector> being very similar only that passes down a creator and not a fetcher and updater. Please glimpse over the render function: The <DiscountCodeDetailsConnector> passes down the fetcher and updater for a discount code. Note that the full component’s code is in this gist . The connector in our example uses GraphQL and Apollo but does not have to. One of its goals is to abstract over the data source for the consumers of the connector. Moreover, the example uses the graphql -HoC but does not have to. It could as well use the newer Query and Mutation components. However, the latter is also an implementation detail to the connector. It only standardizes what shape is passed down through children . Here the discountCodeFetcher could, in addition to isLoading and discountCode , also pass down a refetch function. The reasoning behind a form component is to be able to reuse it between the details and a create view. The form itself for this case only renders three fields and does not use subforms as logical grouping as it is not necessary yet: The Render Prop of the <DiscountCodeForm> passes down control over two form state flags in isDirty and isSubmitting . It also passes down the handleSubmit and handleCancel callbacks. Lastly and interestingly, it also passes down the form React element itself. All of this gives a lot of control to the consumer of the component to compose things together. An example use case would be to disable a header’s deletion button whenever the form is dirty. More on this later (yes I just snuck in this requirement — just like in real life). Knowing how the <DiscountCodeConnector> and <DiscountCodeForm> works we can easily tie them together in the <DiscountCodeDetails> . Again, the full implementation of the <DiscountCodeDetails> can be viewed in this gist . Lets see what happens here: We render the <DiscountCodeConnector> to get the discountCodeFetcher and the discountCodeUpdater We render the <DiscountCodeForm> to compose our Layout while conditionally disabling an <DeleteIcom> and showing a FormSubmissionStatusMessage We use our conversions module and responseToFormValues to map the fetched discount code into the form’s initialValues . In the gist we use the counter part in formValuesToRequest to map out of the form on submission and before making a request through the discountCodeUpdater The <DiscountCodeDetails> composes all components nicely. Rendering the <LoadingSpinner> and a <Redirect> becomes very declarative and is easy to test. The last component is the <DiscountCodeNameField> to illustrate how any field could be built. Here in the <DiscountCodeNameField> we: Render a <TextInput> which is used for any input of this kind and part of our Design System Wire up the onChange and onBlur of the <TextInput> to Formik while passing down an hasError and isDisabled (whenever the form is submitting) Render a <ValidationError.Switch> and <ValidationError.Match> which is a small React component allowing for more declarative rendering (instead of error.name && <ErrorMessage /> ) an <ErrorMessage> Even if often considered otherwise or even abandoned, validation is one of the most important aspects of forms. It is close to the user and drives and guides engagement with a form. Nothing is more stressful than a form not or falsely submitting without knowing why. There are different opportunities potentially triggering validation: Given input changes: the user is typing Given input loses focus : the user leaves a field (e.g. to the next field) Given the form submits: user attempts to submit the form Given API responds: the server answers The validate function covers the first three cases while the fact if validation is possible on each change (1.) also depends on the input. Some inputs can only be validated when loosing focus (2.). There are many approaches as to what is returned by validate . The Formik documentation has more examples and itself even offers a validationSchema . Our validations are built to: Return an object with boolean flags per field: { required: true, duplicate: false } Not return error messages to be rendered: “Name is required”. Rather to rely on react-intl ‘s <FormattedMessage> in a field Lastly, validate is where a lot of Functional Programming sugar could be added such as a Validated -type. However, this would affect how <ValidationError.Switch> works which is why we settled for boolean flags for now. The advantages of top level form conversions was a painful lesson for us. Before input level parsing and formatting was the norm. It turns out to be error prone by introducing a lot of accidental complexity. Essentially a form should keep the users original input in state until submitted. Moreover, the form should never have to carry out field/input level parsing. The example shows how we could deal with parsing and formatting string values or optional fields without the form ever having to be aware of them. Some additional cases We should deal with the fact that nullable GraphQL fields can be null in responses and set them to be empty strings. Same applies for any other optional property We should convert empty string values back to undefined or null or whatever the API’s contract is Inputs can export parsing and formatting helpers used here. Our internal MoneyInput for instance has a static MoneyInput.parseMoneyValue and MoneyInput.convertToMoneyValue to be used here Lastly, it should be pointed out that these functions are convenient to test and do not allow leaking any API data into a form . Maintainable forms depend on good component structure, an awesome form library like Formik and declarative data fetching like Apollo. Forms do not have to be painful. They can be fun to work on and with. Validation and conversions should be treated as a form level concern, not on each field or input. By keeping the separation and concerns of components narrow while using Render Props all parts compose naturally. If you want to learn how Render Props can easily be tested head over to our enzyme-extensions . Thanks for reading! Looking under the hood of the commercetools platform 565 3 Thanks to Marko Svaljek , Nicola Molinari , Dominik Ferber , and A. Sharif . React GraphQL Formik Apollo JavaScript 565 claps 565 3 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-29"},
{"website": "CommerceTools", "title": "google cloud next 2018", "author": ["Sven Müller"], "link": "https://techblog.commercetools.com/google-cloud-next-2018-416c9db1dc0f", "abstract": "About We're hiring We had the chance to visit Google Cloud Next 2018 which took place on 24–26 July, 2018 at the Moscone Center in San Francisco. It was a blast hanging out with so many developers and meeting people from all over the world with the same mindset and goal to contribute, share knowledge and learn more about Google Cloud Platform and its future. This year the conference hit an all-time high with 23,000 attendees. To be honest, the start of the conference felt a bit bumpy due to long queues almost everywhere, mostly due to miscommunication. For example, we had to wait to get access to the opening keynote and were told by the staff that there would likely be no free seats available, although the whole room was almost empty when we got in after waiting some more. During the first day, long waiting lines for our sessions were usual even though we reserved a spot for all of them. This and also the fact that the sessions were distributed across multiple buildings of the Moscone Center and also separate hotel rooms led to rushing between the sessions just to be there early enough to make it in. But maybe this is the way Google tries to keep conference attendees healthy ;) The event organization got better over time. We didn’t see any of the issues mentioned above on the following two conference days and could enjoy the event to the fullest. What we really liked was the variety of sessions covering in-depth themes that are interesting for almost all of us like Application Development, Collaboration & Productivity, Data Analytics, Infrastructure & Operations, IoT, Machine Learning & Artificial Intelligence, Mobility & Devices and not to forget Security. Most of the sessions we attended were well prepared and gave us many suggestions and points for reflections that are relevant to our business. Beside the Breakout Sessions, we also really liked the Hands-on Labs powered by QWIKLABS , which gave you the opportunity to try out things yourself and gain practical experiences. After we were done with the task, we received a rating as little incentive to maybe get better the next time. During the Hands-on Lab, there was always a friendly trainer available for us in case we got stuck or had technical issues (broken monitor, I’m looking at you!). Definitely, something we can recommend you to try out! If you are a developer, we also highly recommend watching Kelsey Hightower’s live demo held during the developer keynote on the last conference day. He created a weather app from zero to the actual endpoint including voice integration starting with the database (using PostgreSQL, Kubernetes, Envoy proxy, Stackdriver application tracing integration, Dialogflow for talking to the weather app). And all that only in roughly 15 minutes. That’s pretty dope! Of course, it wouldn’t be Kelsey Hightower if he would stop there ;) A ton of new technologies and solution were announced during the conference. In this blog post, we will only mention a few of them which seem to be useful for our needs. One of the highlights this year for us was the announcement of the upcoming managed Istio service, which is currently in alpha and will be released later this year. Interested customers can already signup to try it out themselves. Istio is an open-source service mesh which allows connecting and managing microservices in a secure way at scale. We are using Google HTTPS load balancers in front of our services (running in Kubernetes). One of the things we are missing is that the HTTP load balancer request logs do not contain the response time (latency) on a per request basis. To partly work around this, we are using a reverse-proxy like NGINX which can log the response time. But it doesn’t provide the full picture since it only logs the latency between the proxy and the backend. Istio comes in handy here since it would provide better observability and a break-down of response times for each involved backend service. Istio version 1.0 was released only a few days after the actual conference. Stackdriver Service Monitoring is a new view in stackdriver UI , which allows discovering service graphs in real-time, but it also allows seeing how the service graph changed over time. For example, in the case of a new issue with service “X”, you can find out if service “X” was already existing before the issue started or if it was deployed recently. You can create alerts for each deployed service based on SLO’s for availability and performance inspired by Google’s own Site Reliability Engineering team (SRE). The related dashboards are created automatically for you, showing SLI, Error Budget and SLO compliance graphs. In case of issues, this makes it really easy to drill down into your services and check metrics/logs etc. We at commercetools use machine learning behind the curtain for many things, e.g. for boosting product categorization to help improve our customers product data (see related blog post or talk held by Amadeus Magrabi from our machine learning team). That’s why we are very excited to see many announcements in the AI and machine learning domain this year. One of the interesting new services is BigQuery ML. The BigQuery ML service is currently in beta, and allows data scientists and data analysts to build machine learning models using simple SQL commands directly from BigQuery. This integration into BigQuery makes it really easy to create machine learning data models without the need of moving big amounts of data around. The following example is a simple SQL statement that creates and trains a linear regression model with name “bqml_tutorial.natality_model” from an existing BigQuery dataset and can be used to predict the birth weight of a child. Google Cloud Build is now in status GA and considered by Google as a fully managed continuous delivery (CI/CD) platform for building container and non-container artifacts. Until recently, we were missing a deep integration into our GitHub workflow and tooling, so the newly announced partnership with GitHub is a nice step in the right direction to improve the developer experience. To set up the Google Cloud Build GitHub integration, just head over to the GitHub marketplace and install the Google Cloud Build app for your GitHub repository. More detailed information can be found in the Google Cloud Build docs . Google Next 2018 in San Francisco was one of the best conferences I was able to attend so far. The number of interesting themes and sessions was overwhelming so that picking the right ones was really hard. The good news is that nearly all of them are already online or will be available sooner or later. If you have some spare time, go check them out! Looking under the hood of the commercetools platform 118 Thanks to Dominik Ferber , Anas Oubrahim , Camilo Jimenez . Google Cloud Platform Google San Francisco Cloud Services Cloud 118 claps 118 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-02"},
{"website": "CommerceTools", "title": "kubernetes dns and tls automation", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/kubernetes-dns-and-tls-automation-c3ce0919ff1d", "abstract": "About We're hiring NOTE: this article assumes that you have some basic knowledge with Kubernetes , DNS providers and TLS certificates. In the recent years, the DevOps community grew a lot and created new tools and ways to accomplish cumbersome tasks like managing DNS records or issuance of TLS ( previously SSL ) certificates. Nowadays, doing infrastructure work is like coding, thanks to tools such as Terraform , Chef , Serverless , etc. However, even with those tools, managing domains still requires a certain amount of boilerplate and management effort because it still remains a static configuration. What we want though, is something more dynamic that is easy to use and to manage. How nice would it be to simply tell what you want to have and let the system care about how is done ? With Kubernetes, we use yaml files to describe the resources that we would like to have installed/deployed. The yaml syntax is traditionally very declarative and easy to read, therefore a perfect match to what we are aiming to achieve. So how does this relate to “managing domains”? Well, we can declare what domain we want to be created and that we want a valid TLS certificate for that. In Kubernetes world, we can do that by using annotations , to allow other services to hook into the created resources through the Kubernetes API. This approach brings certain advantages: ✂️ almost zero management hassle ( = happier Engineers 🎉) 📜 declarative configuration 🚒 automatic renewal of expiring TLS certificates ( e.g. Let’s Encrypt issues certificates with short lifetime of 90 days , which means improved security ) To demonstrate how we can implement that and what we need to set up, we can imagine to deploy branches . This is a common use case when you are developing features that should be reviewed and tested by developers and non-developers. PS: if you look around, there are already awesome solutions out there to deploy a branch after opening a PR and that integrate very well with e.g. GitHub. On top of my head, I’d really recommend Netlify and Now . If that’s good enough for your use-case, go ahead and use one of those! (or both) 😉 Spoiler: the article will explore things from a “branch deployment” point of view. However, the final result can be used for any kind of application. Our requirements are quite simple: opt-in to deploy a branch after opening a PR (to reduce the number of used resources) use the same tools and setup of our staging/production environment have a unique “user-friendly” URL (e.g. pr-1234.example.com ) the URL must run on HTTPS The first two points are covered by the CI setup. For example, on CircleCI you can use an “approval” step to trigger the deployment whenever you want to. The challenge we are facing is about the last two points: having to dynamically manage a DNS domain and issue valid TLS certificates for running on HTTPS. To solve those problems, we’re going to look at two specific tools that integrate very well with Kubernetes: external-dns to manage DNS resources with DNS providers cert-manager to manage TLS certificates with Certificate authorities This tool works like a bridge between your Kubernetes resources and your existing DNS providers ( Google CloudDNS, AWS Route 53, etc. ). It will use the Kubernetes API to retrieve metadata from Kubernetes resource annotations and perform actions based on that, such as create or delete a DNS record. NOTE: we are going to install this tool in a Kubernetes namespace called external-dns using the related helm chart . At commercetools we use Google Cloud, therefore we’re going to look at the integration with the Google CloudDNS . The first thing to do is to create a DNS zone where all the DNS entries will be created. Once you have the DNS zone set up, you need to define credentials to access your CloudDNS. For that we need to create a service account with the role roles/dns.admin . Then, we need to generate a private key for that service account. The private key of the service account should be stored in a Kubernetes secret , which can be safely referenced by the external-dns service. The last step is to install the helm chart. Where the external-dns-values.yaml contains the following configuration: That’s it, the service now runs in the cluster and will start looking into Kubernetes metadata annotations to check which DNS entries it needs to create. 🎉 With the external-dns running in the cluster, we can manage DNS entries within the Ingress of the service that you want to deploy. In the example above, a new DNS entry foobar.example.com will be created in Google CloudDNS. When the Ingress resource will be deleted, external-dns will take care of removing the DNS entry. This tool works also like a bridge between your Kubernetes resources and TLS certificate issuers ( e.g. Let’s Encrypt ). It will use the Kubernetes API to retrieve metadata from Kubernetes resource annotations and performs actions based on that, such as provisioning and validating TLS certificates for a given domain and automatically renewing expiring certificates. NOTE: we are going to install this tool in a Kubernetes namespace called cert-manager using the related helm chart . At commercetools we use decided to use Let’s Encrypt , therefore we’re going to look at the integration with their service. Since Let’s Encrypt will be going to issue certificates for a domain, it needs to be authorized to do so . Therefore, we need to define a CAA record in our CloudDNS to trust Let’s Encrypt ( this assumes that you already have set up a DNS zone ). Note that you can optionally specify an email with iodef where you want to get error reports about the certificates. The cert-manager needs to access the DNS zone as well in order to perform a so called ACME DNS-01 challenge ( more on that later ). Therefore, we need to create a new service account with the role roles/dns.admin . Then, we need to generate a private key for that service account. The private key of the service account should be stored in a Kubernetes secret , which can be safely referenced by the external-dns service. We can now proceed on installing the helm chart. Where the cert-manager-values.yaml contains the following configuration: Now the chart is installed but it’s not enough, as we still need to configure the actual “issuer”. This is the crucial piece of the puzzle. An Issuer or ClusterIssuer represents a certificate authority from which signed x509 certificates can be obtained. The difference between an Issuer and a ClusterIssuer is that the first only works within a cluster namespace whereas the latter works across all namespaces. Depending on the setup of your cluster, you can choose the one or the other. In our case, we went with a ClusterIssuer to be able to use the issuer across our namespaces. Let’s break down a couple of things here: we define the ClusterIssuer resource kind to use the ACME protocol the ACME server points to Let’s Encrypt production API, however there is also a staging environment that you can use for testing the privateKeySecretRef is managed by cert-manager to store the private tls.key of the issuer account (account registration is done automatically, you simply need to define a valid email address) the dns01 configuration contains a list of providers that can be used to solve DNS challenges. A challenge is like a procedure of proving the ownership of the domain between the CA and the DNS provider the provider google-clouddns references the service account secret that we created beforehand With the cert-manager now up and running in the cluster, we can manage TLS certificates within the Ingress of the service that you want to deploy. In the example above, a new Certificate for the hostname foobar.example.com will be created by Let’s Encrypt and stored within the namespace of the Ingress . Note that it takes some minutes for the certificate to be issued and available. Managing DNS and TLS does not have to be hard and painful. More importantly it should be very easy for every developer to do so, not only for the DevOps team. Using a combination of those tools we were able to abstract away all the management hassles of DNS and TLS and focus on getting our application (for branch deployments) up and running with a declarative configuration. Looking under the hood of the commercetools platform 154 Thanks to Tobias Deekens and Sven Müller . Kubernetes DNS Tls Automation DevOps 154 claps 154 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-06"},
{"website": "CommerceTools", "title": "scaladays 2018 berlin takeaways", "author": ["Marko Svaljek"], "link": "https://techblog.commercetools.com/scaladays-2018-berlin-takeaways-f4f40f2fe925", "abstract": "About We're hiring I had a wonderful week around the ScalaDays 2018 in Berlin. Since I got a Scala overdose it made me feel like sharing this experience in form of a Recap with takeaways. Overall I learned much more than can fit into a blog post but I’ll try to focus on the key aspects of my experience. What I really liked about 2018 Berlin ScalaDays is that there were multiple interesting events organised around the ScalaDays themselves. If you ever get an opportunity to go to the ScalaDays do inform yourself about the accompanying events. I’m pretty sure you’ll find them very interesting. At Monday there was a GOTO night in Cooperation with the Scala UG Berlin. There was a very good talk about the migration to the Scala 2.13. One good news is that the process seems to be relatively deterministic, then again it might not be 100% automatic due to changes in the Collections library. You can have a look at the slides describing the migration. On Tuesday morning there was an Open Source Spree where you could approach maintainers of various Scala project and libraries and get their help in order to start contributing. If you ever wanted to contribute to Scala Open Source Project it’s probably the easiest way to start. From my personal experience, I can only recommend it. The venue was really nice and everybody was very helpful. If you make pull requests to a project and it gets accepted you get a Scala contributor T-Shirt. The Spree ended shortly before the opening of the ScalaDays. Also on Friday, there was a Typelevel Summit . Some of the talks there were really incredible and they covered more functional aspects than the ScalaDays. So next time you visit the ScalaDays take the accompanying events into account, they are definitely worth it. First conference day had a simple schedule. Registration and a Keynote talk by Adrian Moors and Martin Odersky . One of the funniest moments of the talk was when Adrian said he was very busy making the compiler slower by adding new features. The talk was focused on the future of Scala and especially on the Scala 3 with some quite good examples what’s going to come in Scala 3. The keynote was just the right balance between showing technicalities of new features to come with motivation for introducing them. If you are interested in the slides you can find them here: One of the talks that showed what will be possible in Dotty was from Miles Sabin . The focus of the talk was not so much on the Dotty and the talks goes a bit beyond the usual Scala Developer vocabulary with a lot of in-depth examples about using the Generalised Algebraic Data Types in Scala and there were many parallels how this is done in Haskell. What I really liked is that there were complete code examples showing how this really looks like and how it’s possible in previous versions of Scala but with relatively lots of boilerplate code. Basically, it’s incredible to see how Kind Polymorphism develops and will develop in Scala 3. One very important improvement in Scala 3 will be implicit function types. The works on this started some time ago but now it’s possible to have a look into this. There was one entire talk about this feature from Olivier Blanvillain where he provided many examples of how the code will now contain much less boilerplate and will often enable you to avoid using macros. Implicit function types are one of the features to improve developer ergonomics. In short, it looks like one of the main focus of Scala 3 is Developer ergonomics. In the opening keynote, Martin mentioned a couple of times that Scala is supposed to be a platform where Object Oriented and Functional programming meet so I find it very nice to focus a keynote of the second day on the functional programming in general. Tomas Petricek did a very nice keynote with live coding examples from F#. It’s always refreshing to see how this happens in other languages. A very inspiring talk that goes into exploring the of the Object Oriented and the Functional fusion was the one from Bill Venners and Frank Sommers. They explored a lot of Scala production systems out there and identified that every Scala app consists of three regions: Wilderness — IO libraries that don’t play nicely and even send nulls Pure Region — Referentially transparent code with pure function only Confined Zone — Mutable performance oriented code wrapped so that it seems referentially transparent to the outside Bill and Frank took quite some time to figure out the name for this pattern, but ScalaDays in Berlin helped them out with naming it. Basically, they think that every Scala app should be like a Berliner. Have very thin crust that deals with the outside world, nice and airy dough as a Pure region and jelly which represents the Confined Region. So the main idea is that when you are doing your application you should strive to make a Berliner with just the right ratio between the layers: It was very interesting to see the questions from the audience after the talk. This really triggered discussion among the people during and after the talk, it’s very inspiring to see variety of opinions what a Scala application should look like. On a number of occasions, I was really grateful for the type system Scala has. Basically whenever there’s refactoring of any kind the compiler will do the heavy lifting and you can always remain quite sure that the changes you are making to the code are what you actually intended. This aspect of programming in Scala is very important so it’s no wonder there were quite some talks about this. Since the conference was a four-track event I’ll just name a few here. Tamer Abdulradi had a very nice talk about Literal types and how they can help you in everyday development. In his introduction, he started from refined types and used examples from refined Scala library to demonstrate the importance of type safety in programming with Scala. Toward the end of his talk, he focused on Literal types and showed examples what will become easier when interacting with the databases in a more typesafe way with the new Scala features. One of the advantages of Scala running in JVM is the abundance of Java libraries that solve a lot of existing problems. However, the fashion in which those libraries approach the domain is not always the most idiomatic way for Scala. There are a lot of libraries out there for Scala that makes a certain task more functional or typesafe. Michael Pollmeier presented his fork of Apache TinkerGraph library and provided examples how his library is more typesafe than the original. The punchline was pretty similar to this class of talks, types are good and you should use them. Being a contemporary developer usually means staying informed about the right tools to do the job. One library that surprised me a bit was Spire , somehow it managed to stay below my radar until now. It solves a lot of mathematical problems, Vladimir Pavkin used this library in order to do complex time allocation logic. One of my first associations with his problem was the usage of DiscountCodes in commercetools platform . Basically, DiscountCode has a validity and his talk enlightened me that this problem can actually be represented very simply as a mathematical set. This library could come in very handy if we’ll ever have to level up validity intersections with other entities. Anyway, this library contains a lot of valuable mathematical algorithms and I can only say I’m sorry I didn’t know about it sooner. One other library I was aware but somehow didn’t get around looking into it’s benefits is sttp . I must say after a live demo from Adam Warski I’m totally convinced. It abstracts away a lot of the constructs I faced in similar http libraries. It enables easy switching between sync and async way of processing the requests and has very nice string interpolation capabilities for dealing with http parameters and it enables very easy implementation for following http redirects . Developers from non-Scala background often make jokes about slow compile times of Scala. Some even say that Scala stands for “Slowly Compiled Academic Language”. On one side we have type safety in Scala which as some researchers suggest reduces the number of bugs. On the other side, there is a noticeable compile time when compared to some other popular programming languages. So there is a lot of optimisations that go in the direction of compilation time improvement. One of the promising Scala development workflow tools is bloop . The main motivation for creating the tool is that there are simply lots of build tools out there like sbt, grade, maven etc. and they don’t integrate well with everything else that is out there. Plus in a lot of cases, the tools will start their own JVM which is then started up cold and so on. Basically, Jorge Vicente Cantero and Martin Duhem started a really promising project that might make our life easier in the future. bloop is in its essence a compilation server. Also, one very important work related to bloop is the Build Server Protocol. Now that the compilation server concept in Scala ecosystem is developing you also need a protocol to communicate with it. Just as a sneak peak during the Q&A session after the talk Speakers showed a demo of how fast IntelliJ can boot when using a BSP, I must admit I was amazed. Once again bloop is definitely something to keep an eye on in the future. Programming language concepts don’t always correspond to how hardware is performing the operations, this is not a Scala only problem. Basically no matter what high-level programming language you use there is always some cases where you have to choose between readability and performance. A very detailed talk from Zahari Dichev felt like a trip down the rabbit hole. He had some quite useful performance optimisation tips for the critical path in a Scala application. Some of them were Old but gold type of thing like Name Based Extractors where generating Options might put load on your JVM by instantiating a lot of instances of Some and another one was padding the data structures with additional bytes so that unrelated data gets processed in independent memory regions thus saving on cpu cycles and reducing cache missing. The talk was very detailed and filled with examples how to use the tooling like sbt-jol to investigate byte padding of the compiled code. Initially, I was very sorry that the author didn’t publish the slides. I wrote it in this blog post and I even made a public note to myself: “next time take photos of the slides”. Then Zahari read my post and here are the slides: Maybe not 100% Scala related but one of the revealing talks to me at the conference was the one about GraalVM from Vojin Jovanovic . I must say I’m really impressed by the performance gains GraalVM offers. It’s definitely something you should follow up on if you are working with JVM based technologies because performance gains seem significant. Vojin claims he got around 40% faster compilation times when compared to regular JVM. Also, he showed a lot of benchmarks when compiling various popular Open Source Projects like Akka to prove so. It’s definitely something to check out. ScalaDays Berlin 2018 provided a lot of insight to what’s ahead with Scala 3 and how we are going to get there. I really like how the road to Scala 3 got a timeline and how Scala 3 is further going to improve developer ergonomics. I would like to say a big thanks to all the organisers and speakers. It was really a nice and insightful event. Also a special thanks to my company commercetools for enabling me to attend the ScalaDays. Looking under the hood of the commercetools platform 51 Thanks to Oleg Ilyenko , Tobias Deekens , and Yann Simon . Scala 51 claps 51 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-28"},
{"website": "CommerceTools", "title": "continuously deploying elasticsearch on kubernetes", "author": ["Yann Simon"], "link": "https://techblog.commercetools.com/continuously-deploying-elasticsearch-on-kubernetes-57fdfba40670", "abstract": "About We're hiring In this article we would like to share our experience on deploying and managing an Elasticsearch cluster on Kubernetes, as it’s not a trivial task. At commercetools, our Products API offers search capabilities to allow merchants to provide a better shopping experience. After gathering feedback from our users, we came to the conclusion that this search was not optimal for merchants, and that we need a search that is specialized for managing products in a shop. With that in mind, we started implementing a new search API, still using Elasticsearch but with a different implementation approach. The new service runs decoupled from the core platform (some people might call such a component a micro-service 😉) and leverages our event-based architecture to update the indexed documents by listening to events . To manage this new service, we decided to try deploying the Elasticsearch stack on Kubernetes . In case we would encounter major problems with that approach, we would switch back to running Elasticsearch on plain VMs (no Kubernetes). To deploy on Kubernetes, we use Helm Charts. Helm is a package manager for Kubernetes. It can install and update Kubernetes components, like a deployment of a stateless application. We definitely recommend using it. The configuration of an application is described in several files, packaged together as a Helm chart . There is a community effort around Helm to propose official charts ready to be installed . Those charts can be labelled as stable, or can be in the incubator. As we started the project, we tried the Helm chart of Elasticsearch that was in the incubator . After some tests, we were not satisfied with the quality of those templates based on a very old version of Elasticsearch and decided to start with the clockworksoul Helm charts reflecting the pires/kubernetes-elasticsearch-cluster project. (Thanks Matthew Titmus and Pires for the awesome work!) Later, as elastic announced official support for docker images , and as the official incubator charts were being improved, we switched to the official incubator charts (the version of one pull request , plus this one , plus some internal changes) In general, we found that the process to contribute to the official Helm charts takes a long time. We are waiting for some pull requests to be merged before being able to propose some changes that we are using ourselves. We hope that this process will improve in the future. Alongside Elasticsearch, we deployed two more charts: Elasticsearch Exporter to export statistics to Prometheus, that we use for metrics purpose. Cerebro , a web admin tool for Elasticsearch. Helm charts for stateful services such as Elasticsearch, MongoDB, etc. are complex. Kubernetes is well designed for stateless applications but when it comes to stateful services there are many more factors to consider. However, as of now, Kubernetes has much better support for stateful services. A stateful service usually includes a Kubernetes StatefulSet controller that have access to a stable persistent storage , with some of extra configuration like pod anti affinity and bash scripts for bootstrapping. However, I was surprised to see that there were no tests, nothing to check that all this complexity is working well together. So the first thing we did was to create a suite of tests. We are Scala developers, so we used sbt , scalatest and the kubernetes-api java library from fabric8 . The test suite installs an elastic search cluster into a new Kubernetes namespace. By using the Kubernetes API, and the proxy for services , we were able to implement some tests like: creating an index and indexing some documents killing some data nodes (or all of them) checking that the cluster recovers, and that the indexed data are not lost. Having those tests helped us to gain confidence in the build infrastructure. It also allowed us to add tests next to a change in the Helm templates (like testing that pods are deployed on different hosts with a pod anti affinity configuration). The Helm charts are versioned in the same repository as the application. The complete component is contained in one repository.This includes the application source files, the Kubernetes/Helm configurations, the CI/CD pipeline configuration, and so on. The deployment pipeline deploys all components, from Elasticsearch to our application and we do that for each commit. Triggering a deployment for each commit is actually not a problem because Helm will detect if a chart has changed and perform the deployment only in that case. By having all the deployment configuration inside the same repository, we can check each pull request by deploying the complete application in a new Kubernetes namespace for each branch, and run all features tests on it. That way, we can detect errors as soon as possible, in a production-like environment. When we deploy a change in the Elasticsearch deployment configuration, we want to be sure that no data will be lost. For that, we can rely on the data replication offered by Elasticsearch . However, this is not always sufficient. Let’s say we have 4 data nodes (A, B, C, D), and that an index is replicated 2 times. The index exists on 3 data nodes, let’s say on nodes A, B and C. When the node A goes down, Elasticsearch detects the index is not replicated enough. So it starts replicating the index on the node D (by copying data from node B or C). If the node A becomes alive before the replication on the node D is over, then the deployment can shut down the node B. The full index exists only on node C. If the deployment process shuts down the node C before any replication is over, then we can even have a data lost. To avoid that, we use a pre-stop hook that asks the Elasticsearch cluster to remove all the data from the node to be shut down. The pre-stop hook looks like this: 1) first exclude the node A from the cluster. Elasticsearch starts replicating the data to the node D. 2) then wait for the node to become empty: When the node does not contain any data anymore, the pre-stop hook exits, allowing the deployment process to continue. The node is shut down and the new version is installed before the deployment process starts with the next node. With that hook, we ensure that the node to be shut down contains no more data, and that all indexes are replicated to the other nodes. Together with the pre-stop hook, we also had to add a post-start hook that allows the new started node to receive data again: With those hooks, we could update an Elasticsearch installation or update the Kubernetes cluster, and the cluster state always stays green. It is kind of magic to see a whole Elasticsearch cluster updating node by node. Something that is really working is that the deployment of Elasticsearch on Kubernetes is mainly implemented by the development team. Discussions and decisions are still taken together with the Operations Team. The implementation of the pipeline, the changes of the Helm Charts are mostly done by the developers. This was the first time I really experienced that the developers are committed to maintain and update the deployment infrastructure. I guess that using Kubernetes and Helm Charts have helped a lot here. The community around Kubernetes is also very developer friendly. All resources can be found on git repositories. Based on our new knowledge, we could propose changes and provide feedback to other parts. We encourage you to give this approach a try. We would love to hear your feedback on that and to see if your team productivity improved as well. Looking under the hood of the commercetools platform 202 3 Thanks to Jonas Adler , Marko Svaljek , Nicola Molinari , and Sven Müller . Elasticsearch Kubernetes Continuous Deployment Helm 202 claps 202 3 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-07"},
{"website": "CommerceTools", "title": "modeling graphql mutations", "author": ["Oleg Ilyenko"], "link": "https://techblog.commercetools.com/modeling-graphql-mutations-52d4369f73b1", "abstract": "About We're hiring GraphQL mutations provide a flexible way for a client to modify data on the server. On the surface level is a very simple concept: syntactically a GraphQL mutation resembles a normal query with one small difference — top-level mutation fields are never executed in parallel . This aspect helps to ensure that all subsequent sibling mutation fields always operate on the most up-to-date version of the data. Although you can put arbitrary logic in the mutation field resolvers, typically top-level fields are used to perform some kind of side-effect, like updating data in the database, sending an email, etc. So it comes as no surprise that these field names usually start with a verb, like create , delete , update , send , etc. This flexibility of GraphQL mutations begs a question: when you want to build a new GraphQL API, what design principles should you use when you are designing the mutations? Should it resemble a simple CRUD (Create, Read, Update and Delete) API where all updates operate on anemic data structures , or maybe there are other, possibly more robust, design principles that you can employ? It turns out that this question is not as trivial as it seems. In this article, I would like to explore one possible way how you can approach this challenge with examples from our own GraphQL API . Recently, Marc-André Giroux wrote a series of articles on this topic: medium.com I found them very inspiring and I can highly recommend to read them all. They also inspired me to share our approach of modeling the GraphQL mutations. I will try to reference relevant posts from Marc-André Giroux which you can read to get more information on a specific topic. At commercetools, our API is heavily influenced by the principles of CQRS (Command Query Responsibility Segregation) and DDD (Domain-Driven Design) . CQRS might sound scary at first, but in its essence, it’s a quite simple concept: it separates read and write data model. GraphQL makes a strong distinction between input and output object types, which must be defined separately. This makes it perfectly compatible with the CQRS concepts. In many cases, these types might look quite similar, but not necessarily the same. This brings a lot of flexibility: when we model mutation fields and their arguments (write model), we are no longer constrained by the shape and structure of the output types (read model). Given this flexibility, we decided to model all mutations as “commands” and “update actions”. An example of command would be createProduct , updateDiscountCode , mergeAnonymousCart , etc. In the GraphQL schema it is represented as a top-level mutation field. An update action represents a more fine-granular change on a specific aggregate root (you can think of an “aggregate root” as an entity with a globally unique ID, more on it below). All update actions are represented as GraphQL input types. To make it more concrete, let’s have a look at an example of such an aggregate root that we expose via GraphQL API: DiscountCode . In this example, DiscountCodeDraft as well as DiscountCodeUpdateAction and other *Draft input types represent our write model and slightly relate to the actual read mode ( DiscountCode output object type). This design goes hand in hand with the ideas described in “Anemic Mutations” article . Modeling every possible change as an update action (a separate input object) has a lot of advantages. With this approach we have a lot of control over the domain constraints and are able to represent and enforce them with a help of the GraphQL type system. Let’s look at the update actions of DiscountCode aggregate root. We are enforcing several domain constraints here. For example, there is no setCode update action (after its creation, the string code is immutable). When we are updating custom fields (with setCustomType update action), we are also able to provide several convenience fields to identify the related Type (another aggregate root) with either typeId or typeKey or full ResourceIdentifier . It would be hard to provide this functionality if we would choose the anemic data model. Moreover, this gives us a very good documentation and code auto-completion in the GraphiQL for all commands and update actions. Here is an example of how a client might update an existing DiscountCode : As you see, you can provide multiple update actions at once. You also must provide the last seen version of the discount code object. These aspects will become important as we will discuss the transactional boundaries and optimistic concurrency control. You probably also noticed that the current form allows for a possibility to provide several input fields in a single update action, e.g. {setName: …, changeIsActive: …} . This goes back to the way we represent polymorphic input types. In our Scala backend, all these update actions are modeled as a type hierarchy: Unfortunately, the GraphQL spec does not support polymorphic input types at the moment. To address this limitation, we model it as an additional input object DiscountCodeUpdateAction , that contains all of the available update action fields. To enforce a “single input field per update action” constraint, we implemented as simple run-time validation on the backend side. All the ideas I just described correlate nicely to Marc-André Giroux “Static-Friendly Mutations” and “Batch Updates” posts. So definitely check them out for more info. As a nice side-effect of this model, we are also able to go one step further and internally represent all data changes as events. In our API backend, commands and update actions are validated, then they execute relevant business logic and get persisted as set of events (one command might produce multiple events). This approach is called Event Sourcing . In practice, we have a set of listeners in the background which listen to all of the incoming events (we have a few queues in between) and either perform additional business logic or propagate and accumulate the data in the views. One such view is the read model you get as a result of successful mutation. “We will argue below why we conclude that atomic transactions cannot span entities. The programmer must always stick to the data contained inside a single entity for each transaction. This restriction is true for entities within the same application and for entities within different applications.” — Pat Helland As Pat Helland described in his whitepaper “Life beyond Distributed Transactions” , it is hard to achieve atomic transactional behaviour that spans multiple entities and still maintain a massively scalable system. For this reason we provide a quite narrow transactional boundary that is limited to a single aggregate root . In other words, all update actions in the list are applied as a single atomic operation (either all of them are successful and get persisted or none of them are applied). There is no such guarantee for commands though. In other words, there is no transactional guarantees between multiple top-level mutation fields. Also keep in mind the difference between the nullable and not-null mutation field types. According to the GraphQL error propagation rules , if an error is raised during the resolution of a not-null field, then the whole object is considered unresolved and the error bubbles up. Since mutation fields are always resolved sequentially, this provides us a mechanism to control how mutation query execution should react in case of a failed mutation field. In our case we decided to make all top-level mutation fields nullable. This means that if one mutation field fails for some reason, then subsequent sibling mutation fields will still execute. An alternative would be to disallow this behaviour and make all fields not-null. In this case query execution will stop on the first failed mutation field. Yet another alternative would be to allow client to decide the error handling behaviour and provide both field variations in the schema. When it comes to mutations, especially on mission-critical data, it is important to be aware of inherent concurrency. Let’s imagine a simple GraphQL client that: Loads stock information Makes an analysis of the stock entry and then makes a decision of whether to increment or decrement the quantity Sends appropriate mutation to the server It is important to realize that between steps 1 and 3 another unrelated GraphQL client might do the same and update the stock entry while the first client still makes a decision (see diagram below). This means that the decision might no longer be valid. In this scenario, the client should be able to detect such conflicts and react on them (for instance, the client can retry all steps from 1 to 3). By this point you probably already noticed the version argument which is present in update and delete commands. An aggregate root version provides a mechanism for a client to detect such scenarios. On this diagram, when a client performs a mutation, it must specify the version last seen on the stock entry. If the server is able to verify that it is still the most recent version, then the mutation succeeds. If the stock entry is mutated by some other client in meantime, then the server sees a more recent version and fails the mutation, thus allowing the client to detect concurrent modification of the data. Without this mechanism, client would be unaware of external concurrent changes and will always override the server data with potentially outdated information. I also would like to point that while this mechanism is quite helpful, it is not always necessary. In some cases client just wants to override the data on the server, regardless of the current state and version. To cover these scenarios, you might consider making version optional. Thanks for reading and I hope you would find this article useful! If you are still struggling with modeling of your GraphQL mutations, give this approach a try. It will provide a scalable, maintainable and future-proof solution. Also feel free to drop a comment below and let us know what you think! Looking under the hood of the commercetools platform 717 4 Thanks to Yann Simon , Dominik Ferber , Ifeanyi Oraelosi , and Marko Svaljek . GraphQL Api Design Scala 717 claps 717 4 Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-26"},
{"website": "CommerceTools", "title": "the order is a changin implementing order amendments using event driven messages", "author": ["Ida Olsen"], "link": "https://techblog.commercetools.com/the-order-is-a-changin-implementing-order-amendments-using-event-driven-messages-d2297202a349", "abstract": "About We're hiring Event-based architectures are sweeping IT architecture, and for good reason — turning events into messages offer the ability to connect different systems in a scalable, distributed, loosely coupled manner. At commercetools, our Subscription and Message API enable us to easily become the e-commerce puzzle piece of a wider ecosystem containing e.g. CRM or ERP solutions. In this article, we will illustrate how messages can be leveraged in a new manner within a specific context: implementing order amendments. Before explaining this in more detail, let us first quickly address the standard use case for messages. By calling the commercetools Message API , you can find out about changes to any entity (Product, Customer, Order, etc.). The API will return messages informing you of changes that have occurred to this entity. It can be a message about a small property change such as “customer email changed” or a brand-new creation such as “order created”. Instead of pulling the messages from the Message API, you can set up a subscription through the Subscription API . This allows the message to be automatically pushed into a queue of your choice (AWS SNS, AWS SQS, Azure Service Bus and IronMQ are currently supported). By listening to these queues, external components can take appropriate action within their own realm based on the message they have picked up from the queue. An “order created” message e.g. might trigger a transactional email service to send out an order confirmation email, or a “line item state transition” message could trigger a refund to be issued by a payment service. With this as backdrop, we embarked on investigating the requirements for supporting order amendments. At first glance, order amendments seem like an easy enough feature. Since our HTTP API allows updates in the form of partial updates, we might need to simply provide a new enhanced set of update actions for Orders . Doing so, however, would miss a crucial aspect of what editing specifically an order entails. By making changes to an order that has already been placed, you are essentially breaking the original handshake between merchant and customer. The agreement becomes invalid. This implies three characteristics that order amendments as a feature has to support: 1. Grouped Updates : The various actions performed on the order have to be grouped together and considered as parts of one order amendment. Once the amendment is ready to be applied, this must occur as one transaction. Although multiple updates will take effect on the order, it has to be possible to identify these as part of one overall action. 2. Safe-zone Before Taking Effect : An order amendment cannot blindly be applied to an order without an explicit consent. Often an amendment will need to undergo a review process where the customer will agree to (or reject) the new terms of the order. 3. Extract Change History : An order can have multiple amendments throughout its lifecycle and it must be possible to extract these from its history. It is not enough to support a simple binary logic of an order being either staged or published. The overall change history containing potentially many amendments must be kept and made visible if needed. Having to fulfill these feature requirements led us to discussing various implementation approaches. We initially considered the following options: Flip Order to Cart : When an order amendment is triggered, the order would be copied into a new cart where all the regular cart actions known from the checkout process would be supported. Once editing was done, the cart would be converted into a new amended order. Via persisting references to the original entity, an alternate chain of cart and order would be established allowing the change history to be supported. Stores Changes within the Order : The order would be opened up to become a mutable entity. For every value that is updated, the now outdated “old” value would remain inside the order with a reference to the version within which it was valid. Both implementation approaches could have been pursued, but our main objection to both of the approaches is the heavy burden that it places on the developer asked to integrate such an order amendment implementation. Imagine having to build a user interface for a customer service agent with either of these approaches. The customer service agent expects to have immediate in-screen guidance when making order amendments and also to have quick access to the overall history of the order. This quickly becomes a cumbersome and error prone user interface to build, since the integrating developer would need to keep a timeline of the versions of importance and constantly create diff views to identify the impacted parts. With the intention to provide the integrating developer a better Developer Experience , we looked to our existing message-driven architecture. What we identified is that our current Message API and Subscription API contain the content we are looking for: the changes of an update. The payload of a message delivers changes performed to an entity in a consistent and nicely formatted fashion. No diff views between versions are needed, no pin-pointing of old versus new values is needed either. But in contrast to the existing use case of the Message API and Subscription API, which is all about notification to the world outside commercetools, the order amendment feature needs the message to be immediate. So ultimately, we decided to combine the best of both worlds. When an order amendment is performed, the API response will include a) the resulting order as well as b) the order amendment specific message capturing all the changes that lead to the resulting order. No asynchronous requests to the Message API or message processing from a subscription are required. Instead, it will be delivered as one package in the API response. This, we believe, will optimally serve the integrating developer. We want to design an API that takes responsibility for not only performing an action, but also for responding with the contextual implication of that action. If we compare this usage of messages with the standard use case of connecting loosely coupled systems, we have a new interesting setup where the focus had been moved to enrich the communication between API and client. Arriving at this implementation approach has been a team journey and we are currently busy executing on what has been outlined in this article. In the coming months, we are looking forward to revealing more details about the full implementation of how to perform order amendments. Looking under the hood of the commercetools platform 60 Thanks to Kelly Goetsch , Nicola Molinari , Philipp Sporrer , and Michael Schleichardt . Microservices 60 claps 60 Written by Product Manager @ commercetools Looking under the hood of the commercetools platform Written by Product Manager @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-23"},
{"website": "CommerceTools", "title": "web security for single page applications great impact with little effort", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/web-security-for-single-page-applications-great-impact-with-little-effort-a7a506cec20b", "abstract": "About We're hiring As the web keeps growing, so do the challenges that we need to undertake in order to maintain a high level of trust and security for our Web Applications. Recently browser vendors have been implementing great new features based on the W3C specs to provide better and more advanced tools to allow us, developers, to keep up with those challenges. In this article, we are going to look at some of those tools and show you how much you can achieve with minimal effort. If you’re familiar with the Pareto principle (also known as the 80/20 rule), that’s exactly what I’m talking about. NOTE: even though this article is focusing on the aspects from the perspective of a JavaScript Single Page Application, many of the tools and concepts can be applied to any Website or Web Application. The article covers the following sections. If you are already familiar with some of those topics, feel free to “jump” to the section of your interest. Common vulnerabilities Protecting against XSS Security Headers (CSP & SRI, HSTS, HPKP) Securing sensitive information Protecting agains CSRF CORS Conclusions & References When we think about Web Security, probably the first thing that comes to mind is HTTPS , which is a protocol that describes and ensures that the data exchanged over the connection is securely encrypted. But is that enough? Unfortunately, it’s not. Two of the most common vulnerabilities for Web Applications are XSS and CSRF (I assume that you have a vague understanding of them, otherwise there are some useful references at the end of the article). When we talk about vulnerabilities and attacks from malicious parties, it’s all about data , more specifically sensitive data , and how we keep it protected . This can be anything like a session token, a user email, etc. The browser offers different ways to store and exchange data, each one comes with pros and cons. Here at commercetools my team and I are working on a Web Application called “Merchant Center”. From a technical point of view, it’s a JavaScript Single Page Application that runs on the browser and connects to a proxy API in order to consume different services within our platform. When implementing a Single Page Application, it’s common practice to store an authorization token in window.localStorage so that it can be retrieved when sending an API request. Unfortunately window.localStorage was never designed to be secure. Any piece of JavaScript can read and write from/to it , which opens up an XSS vulnerability attack (like a “malicious” script that reads from window.localStorage and sends data to an external “malicious” server). In the last weeks, we’ve been digging more into this security topic and made some simple changes to raise our security defenses to a higher level. Today we want to share this experience with you, hoping that you will feel more comfortable in understanding and implementing security measures with just a few steps. In a JavaScript application, an XSS ( Cross-Site Scripting ) attack is one the most malicious attacks. Fortunately, browsers offer and implement several features to help prevent those vulnerabilities. First, we’re going to look at some Security Headers . A security header is “just” an HTTP Response Header specifically designed for security purposes. There are a few of them which are worth mentioning. The CSP header describes a series of directives (or rules) to instruct the browser to allow or block resources to be loaded and executed. I would argue that this is one of the most important Security Headers to use. However, it can be quite challenging to properly set it up. Let’s have a look at how you can get started with it and implement a basic CSP. 1) Start with a default template 2) Use -Report-Only mode so that the policy is active but the browser does not block anything, instead it will send violation reports to the provided URI 3) Inspect policy violations (within the reporting tools of your choice, for example we use Sentry ) 4) Update the policy to white-list the domains and resources that you trust 5) Repeat steps 3 and 4 until you covered all the minimal rules to make your application work. To further help this process, 3rd party services like Intercom provide a list of directives to white-list all their resources . 6) Enforce the policy (replace Content-Security-Policy-Report-Only with Content-Security-Policy ) Once you have your base policy enabled you can start tweaking it. One thing I would like to focus on is related to the script-src directive. As you might have noticed, I used unsafe-inline and unsafe-eval which basically allows any kind of injected script to be executed . This might seem acceptable, depending on the type of application, but we can do better . In fact, I strongly recommend trying to get rid of those. Let’s look at a common example: Google Tag Manager. To set it up, you need to put an inline script in your HTML page. Of course, you can decide to put it into a JS file, load the file instead and white-list the URI where the file lives (at the cost of an additional network request). However, for the sake of the example let’s assume we want it as an inline script . In order to white-list that inline script, we simply need to hash it . The CSP supports 3 types of hash algorithms: sha256 , sha384 , sha512 . Once you have the hash, you can white-list it in your CSP. That’s it, simple as that. NOTE that the hash is static and can be hard-coded in your CSP. If the inline script changes, you obviously need to update the hash (the browser will “remind you” of that 😉 because it simply won’t execute the script). In rare cases, you might need to dynamically generate the hash on each request, based e.g. on some environment variables. If you have control over the HTTP server you can still do that, otherwise, you need to find a different solution. UPDATE: As an alternative of the “sha-hash”, you can also use a cryptographic “nonce-hash”. The difference is that the “nonce-hash” is uniquely generated by the server on each request and does not need to hash the script content. This is useful for more traditional Web Applications where there might be a lot of inline scripts on different pages and hashing each one of them becomes complex and tedious. Using a “sha-hash” will allow to be more strict in which inline scripts are allowed. Using a “nonce-hash” will provide more flexibility to allow any kind of inline script rendered by the server and prevent unknown script injection. Well, that’s fairly easy: do not use eval 😇 Seriously, it’s really discouraged! Furthermore, inspect your 3rd party dependencies and look for the usage of eval() or Function . If you find any, open an issue at the related library to address the problem. There are different versions of the CSP spec, the current one being version 2.0 with the upcoming 3.0 (still in draft). All major browser vendors support most of the features , with the exception of IE (it only supports 1.0 with the sandbox directive, nothing else 🙃). What happens when a browser does not support a directive? Nothing, really. The unsupported directives will simply be ignored. This is a good thing for us developers because we just need to maintain one policy. On the other hand, if such an “old” browser is used, the user will simply not benefit from the security features. We just looked at the Content Security Policy as a way to white-list resources that we trust to be loaded and executed. Some of those resources might come from a CDN . We might trust the CDN domain or URI but can the files be fully trusted ? 🤔 If an attacker is able to modify the e.g. jquery.min.js file content that is stored on a CDN, our application will still load and trust the file but it doesn’t know that the content changed and that it might contain malicious code. In order to prevent that, we can use another important security feature: Subresource Integrity . Using SRI enables browsers to verify that files they fetch (for example, from a CDN) are delivered without unexpected manipulation. Again, this is also pretty simple to implement. We hash the file and reference the hash in the script file. Then we enable the require-sri-for script directive in our CSP. Content Security Policy is a powerful feature . It gives us a lot of options and flexibility to instruct the browser to understand the context of our application or website and take necessary actions. In other words, it allows you to control pretty much everything that’s going on in the browser (scripts, fonts, images, network requests, etc). In this article we just looked at some of the features of CSP. However, there is much more to it and I recommend to follow the reference links to dig deeper. What else can we do? The HTTP Strict Transport Security is another HTTP response header that tells the browser that it should only allow HTTPS requests on all subsequent requests. Enforcing this header highly reduces possible MiTM attacks. If you want to know more about it, I really recommend reading this article from the well-known Security Researcher, Scott Helme . This one is more of a “nice-to-have” header. It’s a very powerful one but quite delicate to handle at the same time. You probably want to look at this only in cases where you need a high level of security , such as a bank website. So what is it? It stands for HTTP Public Key Pinning and has the ability to instruct the browser to verify a set of given cryptographic public keys against the requesting website. Enforcing this header also highly reduces possible MiTM attacks with forged certificates. However, there are some important trade-offs to be aware of before using this. Again, you can read more about this in this article from the well-known Security Researcher, Scott Helme . As we saw in the previous sections, Security Headers do not take much effort to implement and offer a built-in high level of protection from the browser. However, in the case that an attacker still manages to bypass those security protections, our sensitive data might still be at risk. What else can we do to store that data more securely? The answer is: use HTTP Cookies , specifically in Secure and HttpOnly mode. The Secure flag ensures that the browser sends the cookie only over a HTTP connection. The HttpOnly flag prevents any JavaScript code to read from it. Simple as that…or is it? 🤔 Well, using HTTP Cookies opens up another vulnerability, namely CSRF. A CSRF attack ( Cross-Site Request Forgery , also pronounced as “ sea-surf ” or “csurf”) has been around longer than XSS and traditionally applies to normal Web Applications (using server-side rendering and forms). It’s the ability to perform unwanted actions on an authenticated user’s behalf To understand how we can prevent this within a Single Page Application, we need to look at CORS first. CORS stands for Cross-Origin Resource Sharing and it outlines a policy that defends against one origin from “stealing” another origin’s data. This is a common thing to encounter if you develop a JavaScript application that makes network requests to other origins. In order for the request to “succeed”, the second origin server needs to consent the request via the Access-Control-Allow-Origin header in the response. When does a request need the consent from the server? Well, it depends whether the browser is forced to check if the CORS protocol is understood by the server or not. The check is performed via a “ preflight ” request with the HTTP OPTIONS method. The check logic is determined by the “kind” of the request: Simple or Non-Simple . A simple request is a request that does NOT trigger a “CORS-Preflight” and it must fulfill the following requirements: Method: GET , HEAD , POST Headers: Accept , Accept-Language , Content-Type (*), … Content-Type (*) values: application/x-www-form-urlencoded , multipart/form-data , text/plain Simple requests allow CSRF attacks in case the affected origin performs authenticated requests using credentials stored as e.g. HTTP Cookie or Basic Authentication. So how can we protect from CSRF attacks? We use Non-Simple requests! A non-simple request is a request that TRIGGERS a “CORS-Preflight”. As opposed to a simple request, it fulfills other requirements: Method: PUT , DELETE , OPTIONS Headers: same as for simple requests, plus any custom header (e.g. X-Something ) Content-Type values: all other values that are not included in simple requests (e.g. application/json ) As you might have noticed, requests to a JSON API will send the Content-Type: application/json header, making it a Non-Simple request. JSON APIs are automatically protected against CSRF attacks, given that the API enforces Content-Type: application/json . Securing your Web Application is NOT hard and it takes only a few steps to produce a big impact. I think it’s important that every developer, especially those working with the browser, is aware of the possible security threats and understands how to minimize the risk of an attack. I hope this article gave you good insights on the steps you can take to further secure your website, or at least to increase your knowledge level in case you were already familiar with those topics. Nevertheless, I strongly recommend to read and inform yourself more about security , as it’s a very broad subject. Below there are some useful and interesting links that you can start with. In general, I really recommend following and reading articles from the well-known Security Researcher, Scott Helme ( https://scotthelme.co.uk/ ). In particular: Modern Web Security Standard ( by Scott Helme ) https://www.youtube.com/watch?v=j-0Bj40juMI Protect site from Cryptojacking CSP + SRI ( by Scott Helme ) https://scotthelme.co.uk/protect-site-from-cryptojacking-csp-sri/ Migrating from HTTP to HTTPS. Ease the pain with CSP and HSTS ( by Scott Helme ) https://scotthelme.co.uk/migrating-from-http-to-https-ease-the-pain-with-csp-and-hsts/ Authoritative guide to CORS for REST APIs ( by Derric Gilling ) https://www.moesif.com/blog/technical/cors/Authoritative-Guide-to-CORS-Cross-Origin-Resource-Sharing-for-REST-APIs/ Understanding CORS ( by Bartosz Szczeciński ) https://medium.com/@baphemot/understanding-cors-18ad6b478e2b Content Security Policy with Sentry ( by Geller Bedoya ) https://medium.com/sourceclear/content-security-policy-with-sentry-efb04f336f59 Tools for setting up security headers https://report-uri.com/home/tools https://securityheaders.io I’m harvesting credit card numbers and passwords from your site. Here’s how. ( by David Gilbertson ) (Part 1) https://hackernoon.com/im-harvesting-credit-card-numbers-and-passwords-from-your-site-here-s-how-9a8cb347c5b5 (Part 2) https://hackernoon.com/part-2-how-to-stop-me-harvesting-credit-card-numbers-and-passwords-from-your-site-844f739659b9 And last but not least, I recommend watching this funny and very informative talk from Dominik Kundel from the last JSConf 2018 in Iceland. Enjoy 😎 Looking under the hood of the commercetools platform 463 2 Thanks to Tobias Deekens , Philipp Sporrer , and Dominik Ferber . JavaScript Security Web Applications Xss Attack Csrf 463 claps 463 2 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-03"},
{"website": "CommerceTools", "title": "enjoy our commercetools api with httpie and jq part 2 of 2", "author": ["Michael Schleichardt"], "link": "https://techblog.commercetools.com/enjoy-our-commercetools-api-with-httpie-and-jq-part-2-of-2-b26429f2d71c", "abstract": "About We're hiring Go to part 1 . In this second part of how httpie and jq can be used with the commercetools API, we will show how the tools can be used together to perform a checkout process. To do so we need to do the following steps: get an access token for an anonymous shop user search a product and obtain its ID create a cart containing the product add the shipping address create the order The access token we obtained in part 1 will not allow us to create a cart, since it can only be used for SPAs. We will therefore fetch a token for an anonymous shop user . First, a http request is made to fetch an anonymous token. The response body is then saved in the variable “userAccessTokenResponse”, which is sent to jq to extract the access token. The “-r” flag gives the raw JSON output without quotes, so that we have g2WIeVlL33aaf6YTEbUpmCUV7n8y3CVT as content of the token instead of \"g2WIeVlL33aaf6YTEbUpmCUV7n8y3CVT\" with the double quotes. To find a product for our purchase, we search for a product with the name “SAPPHIRE” and then just grab the ID of the first product found. Since we are now working with a user, the session key is different and for the first request, we will add the user access token to this new httpie session. When a product is placed in a cart, it is called a line item. Creating a cart with a line item actually needs very little data so we can declare the JSON properties as parameters of httpie. The fields, declared with := are treated as raw non-string JSON fields and the input is added to the request object. The actual body we send looks like this: We have seen now different name value separators like : for headers, == for query parameters, = for raw string parameters for JSON or forms and := for JSON fields that are not strings (object, array, boolean, number, null). Of course you can add query parameters which do not need to be encoded directly to the url like e.g. POST $authUrl/oauth/token?grant_type=client_credentials&scope=manage_project:$projectKey. Before we can order the cart we need to add a shipping address. In order to update the cart, we need the version of the cart in the body for optimistic concurrency control . Setting the address in the httpie request body with := would be too messy, so we will show different approaches to create a readable body with the current cart version number. One solution could be to put the static settings into a file and then use the addition feature of jq to add the version field: Alternatively, it is possible to use the shell heredoc feature with string interpolation directly in the shell (script): Either way the update command will contain the cart version number: The actual command is sent by echoing the cart update command to httpie: The resulting cart will look like this: In order to finalize the checkout process, the cart needs to be turned into an order. This is done with a simple request: Above, we illustrated best practices within a specific five step checkout process. Many more best practices exist, and in this section, we want to bring attention to some of our favorites. The flag “ — check-status” instructs httpie to exit with an error when the HTTP response code is greater than or equal to 400. This is useful when you want to fail the shell script on invalid or failed requests. A similar fail fast behavior can be configured for jq with the flag “-e”: To pretty print the whole output from curl with jq just use a dot: curl $url | jq . . Instead of reusing a httpie session with --session=ctp consider to use --session-read-only=ctp to not accidentally add new headers to the session. Do not forget the brackets for arrays when specifying a path with jq, use jq .lineItems[].id instead of jq .lineItems.id, otherwise an error “Cannot index array with string “id” is raised which may give the impression that the field coming after the array in the filter DSL is faulty. It is possible to create more than just JSON with jq. The following example shows how to extract correctly quoted comma separated values: One last tip: quoting is not easy and sometimes there is the need to execute a shell command for each element of an array of objects. With the base64 filter of jq , each element can be base64 encoded, and all items as a multiline string can be piped to a consumer. In the following example, a JSON document with a results array property is sending each result object to a local elastic search server: Httpie and jq are very useful tools to discover our REST API and to perform requests in a simplified manner. The tools could for example be of great benefit for support teams who quickly need to try and reproduce reported issues. Another example where the tools become very handy is for API examples and tutorials. Thanks to the tools, it is possible to ensure that the JSON/http examples work and are nicely formatted with only the fields that are relevant to this tutorial. For creating tutorials check out this shell script which we used to create the JSON files for our tutorial on a recent release which added support of multiple shipping addresses for one cart . Looking under the hood of the commercetools platform 15 Thanks to Ida Olsen . API Jq Httpie Commercetools Json 15 claps 15 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-19"},
{"website": "CommerceTools", "title": "vertical teams and collective thinking", "author": ["Ida Olsen"], "link": "https://techblog.commercetools.com/vertical-teams-and-collective-thinking-4d4943112588", "abstract": "About We're hiring At commercetools we are organized into vertical cross-functional teams dedicated to specific feature areas (such as the checkout process, prices and products), as opposed to horizontal teams which focus on specific technology layers. The vertical team where I am Product Manager, the Checkout team, thought it would be fun to start a reading club. We decided to spend an hour every Monday morning discussing an article that we take turns picking out beforehand. We have no real constraints on the subject matter. We have so far read about and discussed Microkernel Architecture, GitHub documentation, the danger of light-weight OMS solutions and many other topics. One article was about the importance of Deep Work and how difficult it can be to achieve concentration and focus in this day and age. This is a recurring topic at commercetools: how to create an environment and a process that allows individuals to reach a state of mind where Deep Work can be achieved. In our discussion, I spoke up for not forgetting a type of work that gets less attention: Collective Thinking , i.e. a collaborative version of Deep Work where multiple brains come together and output value. As important as Deep Work is, I find that Collective Thinking is a prerequisite for software development of quality and that it is a precursor for having something to apply Deep Work to. This is even more important when working with agile methodology frameworks. In the waterfall days, a Product Manager would work much more in solitude on a verbose Product Requirement Document with detailed paragraphs leaving no stone unturned. Once the document was given the status of complete, it would be handed over the fence to Developers who would execute the implementation according to the document. When trouble occurred, and a Developer would utter “that wasn’t specified”, the Product Manager would proudly refer to paragraph 5.3.1.1 on page 55 to argue her case that it indeed was. The Product Requirement Document became the product instead of the actual product. Days like these are luckily long gone. In an agile environment where continuous delivery is expected, Minimal Viable Products are to be defined, implemented, tested and iterated upon, Product Managers and Developers cannot rely on written documents being thrown over a fence in monthly or quarterly intervals. Instead, they have to come together and through dialog find common ground and reach a shared understanding of the Why, What and the How of an implementation. When reading about roles within a development team, the consensus is clear that the Product Manager owns the Why. The Product Manager needs to understand the business domain, the use cases, the problems, the opportunities, the goal, the motivation. It is also clear that the Developer owns the How. The details of how the solution is orchestrated and the code is written is the realm of the Developer. The What, however, is a territory that both Product Manager and Developer own. The What is the actual thing that will be delivered with the implementation. It is deciding which entities will be involved or introduced, which properties they will have, and which actions you can perform on them. When you move from the phase of exploring to grooming, you have to get to the point where you nail down the What. In doing so, it is easy to fall into the trap of procrastination, pushing the What ahead of yourself. Meeting upon meeting can be left with the conclusion that “this is very complex”, “we could go many routes”, “not sure we know enough to make any decisions”. I am not arguing that a team should fast forward to locking down a solution, but rather as part of loving the problem , it is crucial to reach that stage in the process where a leap is taken and ideas are suggested and the problem is vetted from all angles. Here Collective Thinking can be a tool that propels a team forward. Achieving Collective Thinking is, however, not something that just happens. In the past, I have experienced many interactions with Developers where I was aiming at Collective Thinking, and instead was met with a puzzled look, as they seemingly wondered if I was trying to test them or trick them. Needless to say, such fairly awkward moments do not bring any product feature forward. What I find interesting though is that ever since we introduced vertical teams at commercetools, I have experienced and enjoyed Collective Thinking with Developers much more frequently and with much more ease. For a thorough explanation of what Collective Thinking is exactly and what it entails, I recommend reading Dialog: The Power of Collective Thinking . This article is from a completely different domain than software development, but I find it accurately outlines the difficulties and the phases of Collective Thinking between Product Managers and Developers. In trying to extract what it is about vertical teams that bring about conducive conditions for Collective Thinking, I found three key aspects: 1) Safe environment 2) T-shaped profiles 3) Collective pain. Let’s explore each of them individually. Safe Environment Product Managers and Developers are in natural opposition to each other due to the way their roles view the product through different lenses, but they do have to come together and find mutually satisfactory solutions. When working in a small vertical team, sitting next to each other and dealing with bugs, stories, customer support, documentation and test specification pertaining to the same feature domain day in and day out, a mutual trust is naturally formed. The team members start acting as allies and not as adversaries. When mutual trust exists, each team member can dare to spit out ideas that may or may not be good ones, can dare to go off on tangents to test whether or not a concept is water-tight, and to oscillate between details and the big picture. When defining the What, Product Managers and Developers will, and should, encroach on each other’s turf, vetting concepts and determining their consequences from all sides. For such interaction to be fruitful, and not end up becoming a defensive trench-war, all team members have to experience the team as a safe environment. T-shaped Profiles Vertical teams with a defined feature domain focus allow both Product Managers and Developers to become T-shaped profiles, i.e. they possess deep skills in a few areas while having high-level skills in a wide set of areas. For Collective Thinking to output quality, the foundation has to be knowledge. If a group of generalists come together knowing a little bit about every product feature, you are left skimming the surface and never going to the depth that is ultimately required. Because a vertical team has a clear and relatively narrow focus, expertise in a feature domain is possible. As the Product Manager for the Checkout vertical, I am a generalist when it comes to non-checkout related topics such as the product category tree or shop search, but when it comes to line items in a cart, tax calculation logic or order amendments, I have a thorough understanding which qualifies me to join Collective Thinking on anything related to carts, orders and payments. If organized horizontally, where job function instead of topic is the segregating principle, expertise is simply not possible and consequently Collective Thinking and ultimately the product features would suffer. Collective Pain When a vertical team has a defined focus, ownership results, or perhaps better put: collective pain results. A clear focus will set boundaries around what a team cares about. If a mountain of product features is distributed more or less randomly among horizontal pools of Product Managers and Developers, it is a) easy to perceive each feature implementation as just another output from a feature assembly line that one feels estranged from and b) easy to withdraw from or constantly procrastinate decision making on complex issues that need to be addressed. The “tragedy of the commons” problem occurs where the quick turn-around of individual tickets is implicitly incentivized, hurting the overall comprehensibility and sustainability of the product. Instead, what you achieve with vertical teams, is a group of people that choose to inquire, choose to decide and choose to improve. This not necessarily because Product Managers and Developers become better or more conscientious people by being organized in vertical teams, but because flight is not an option. They know that the consequence of their implementation choices will be a reality which the next feature implementation has to deal with. This experience of collective pain entices both Product Managers and Developers to engage in and commit to Collective Thinking. Working at commercetools, it is a process of learning to continuously try and optimize the touch points between agile development and organizational team structure. We continue to tweak our vertical team model to ensure that it supports and promotes agile development. In this article, I have focused on Collective Thinking being one of such touch points. Collective Thinking has really emerged as a by-product of our organizational setup and it supports us greatly in our aim to be agile and to deliver features of high value. Looking under the hood of the commercetools platform 92 Thanks to Laura Luiz , Michael Schleichardt , and Nicola Molinari . Agile 92 claps 92 Written by Product Manager @ commercetools Looking under the hood of the commercetools platform Written by Product Manager @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-16"},
{"website": "CommerceTools", "title": "gss grid application layout in production", "author": ["Philipp Sporrer"], "link": "https://techblog.commercetools.com/gss-grid-application-layout-in-production-f60c65a05cfa", "abstract": "About We're hiring CSS Grid is one of the biggest enhancements for layouts the web has seen in a long time. We are finally able to create native two-dimensional layouts in the browser. It makes your HTML more concise and your CSS more robust. In this article we’d like to share how we were able to reimplement our application layout in CSS Grid, what benefits we saw and how we were able to even support IE11 🎉. Our application is an admin UI for merchants managing their enterprise commerce businesses. The layout we use is the classical holy grail layout and consists of: App Bar: With a fixed height of 45px while taking up the full width and not scrolling with the page content. Menu: Should be below the app bar, have a width of 200px and take up the full height, and should also not scroll with the page content. Content: Should take up the remaining width and height, and should be scrollable. Implementing the above described layout in CSS Grid is easy as 🍰. Let's take a look at the new CSS properties we used. If you are already familiar with those feel free to skip to the \"Making it work in IE11\" section. First of all we use display: grid; to make the .app element a grid container. As soon as we do this all direct children of .app become grid items . Now we are ready to define our layout. It consists of two columns and two rows: grid-template-colums: 200px 1fr; defines that we want two columns. The first will be 200px wide and the second will take up one fraction unit (fr) of the remaining horizontal space. Fraction units work just like flex-grow values in flexbox. They represent a fraction of the available space in the grid container. Similarly, grid-template-rows: 45px 1fr; defines that we want two rows. The first row will be 45px tall and the second row will take up the remaining vertical space. In CSS Grid columns and rows are also referred to as grid tracks . The lines that surround the tracks are called grid lines . These grid lines also get numbers assigned automatically: Grid tracks are used for defining columns and rows. Grid lines are used for placing grid items in the grid. By default CSS Grid will automatically place each item in one grid cell. If you want one grid item to take up more than one cell—like our App Bar that should take up two cells horizontally—you need to explicitly position the grid item using grid-column and/or grid-row . Looking at the image above we can identify the grid lines we need to tell the App Bar to stretch from and to. This tells the App Bar to start at grid line 1 and go until grid line 3. Notice that in the full code above we are telling the app bar to go until the grid line -1 instead of 3. -1 always refers to the last grid line. This makes the App Bar always take up the full width no matter how many columns we add later 🎉. If you want to learn more about all the new terminology and properties head to the MDN CSS Grid Layout documentation or The Complete Guide to CSS Grid Layout . These are great place to get started 📖. Note that we are not explicitly placing the App Bar in the first row, though. Also we are not explicitly placing the Menu and Content. CSS Grid automatically places these items for us ✨. We have now seen how simple it is to create layouts using CSS Grid. Although browser support for CSS Grid is already great in general, there are a few extra steps we need to take in order to make our layout IE11 compatible. Here is what we did. First of all, the IE implementation relies on the -ms vendor prefix. Using Autoprefixer with grid mode enabled will take care of most of the work. Sadly, the IE10 implementation of CSS Grid — which is also used in IE11 — doesn’t support auto-placement: There is no auto-placement behaviour in IE10 implementation. This means that you need to position everything rather than use the autoplacement ability of grid. – Rachel Andrews This is a bummer! We need to fallback to explicitly placing everything on the grid by using grid-column and grid-row . This works fine for our use case since we only have a couple of items to place on the grid. If we were to dynamically fill our grid with items we would be out of luck with the IE implementation. During autoprefixing the grid-column property gets transformed to IEs -ms-grid-column-start property. However, compared to grid-column it only supports the starting value—like the name suggests. In IE we need to explicitly define how many columns the grid item should span using the -ms-grid-column-span property as you will see in the code sample below. Another quirk we’ve encountered in IE11 is that for some reason the <main> element is not supported as a grid child. The solution is to fallback to a <div> with role=\"main\" , which keeps the HTML5 semantics and allows us to place the main container in the grid in IE11. Here is how the code looks after we take these insights into consideration. Notice that we are not doing any manual prefixing. The only real IE specific part is stretching the App Bar using ms-grid-column-span : Take another look at the HTML structure. It’s pure semantic markup . No unnecessary wrapper div s. With this we have a fully functioning application layout that is also IE11 compatible. But this was a bit too easy, wasn’t it? Let’s step it up a notch 🔝. To give our tablet users a wider content area we want to shrink the nav to 64px whenever the screen gets narrower than 992px. How do we get the left column to be either 64px or 200px depending on the screen width? We don’t. We get it to automatically be as wide as its content and get its content — the nav bar — to be responsive. This separates the concerns nicely. The nav bar is concerned with changing its width responsively. The app layout is concerned with giving the nav bar as much space as it needs. Here are the changed CSS rules: Being able to implement the responsive nav bar this easily blew my mind 🤯. No absolute positioning. No calc() . Just layout 🤩. Now there is just one more thing to the existing layout. Global notifications that push down the content below them. Global notifications are displayed at the very top above the header and are used to inform the user about things like cookie usage. Implementing this with old school layout hacks like absolute positioning would have been a nightmare. With CSS Grid it’s a breeze of fresh air. All we have to do is add another auto sized row above the header row bump up all grid-row values by one add the notification container This is what the full code looks like: In total that’s only 45 lines of CSS and 9 lines of HTML. CSS Grid is awesome and it is ready for production 🚀. You don’t need to wait until no one is using IE11 anymore to use CSS Grid for basic layouts. You also may not need to fallback to flexbox or float based layouts on IE11 like we’ve seen in this article. Use Rachel Andrew’s article about the IE implementation of CSS Grid as a reference to figure out if you really need to fallback to alternative layout methods for IE11. CSS Grid helps with making your HTML more semantic and massively reduces the complexity of your CSS. There are very high-quality online resources that can help you learnings CSS Grid like basically everything on Rachel Andrew’s site and the free CSS Grid course by Wes Bos . CSS Grid has already been around for a while and it is supported in all major browsers—even the IE version is not too bad. Let’s do layout like it’s 2018 💅. Looking under the hood of the commercetools platform 1.4K 6 Thanks to Tobias Deekens , Nicola Molinari , Dominik Ferber , Ifeanyi Oraelosi , Florian Dietrich , Lukas Sporrer , and Luis Gomes . CSS Web Development Web Design Html5 Grid Layout 1.4K claps 1.4K 6 Written by Frontend Engineer @Commercetools. Looking under the hood of the commercetools platform Written by Frontend Engineer @Commercetools. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-19"},
{"website": "CommerceTools", "title": "reduce the gender gap in programming", "author": ["Laura Luiz"], "link": "https://techblog.commercetools.com/reduce-the-gender-gap-in-programming-991da98ab8d", "abstract": "About We're hiring Today is the International Women’s Day , and millions of women from all around the world are protesting in different ways against the wage gap and sexual discrimination that their societies suffer. As a female developer, I would like to join them and highlight the gender gap present in the technical sector, especially in the software development area where I come from. Probably you are already familiar with this gap, most likely you can confirm it just by taking a look around you. Nonetheless, the Organisation for Economic Co-operation and Development ( OECD ) has published several studies about gender equality in education that will help us to understand why it exists and find ways to fight it. The most recent study , dated from 2015, stated that on average less than 5% of girls and nearly 20% of boys contemplate pursuing a career in engineering and computing . The same study makes it clear that there is absolutely no evidence that such gender disparity comes from innate differences in ability, but rather lies in a negative perception that girls have of their own capabilities in mathematics. Indeed, when comparing girls and boys with same level of performance in mathematics, girls’ self-efficacy and self-concept are considerably lower. It is this lack of self-confidence in their own abilities in mathematics that holds them back from considering careers in which this field of study plays an important role. So, if this perception is not based on reality, where does it come from? Unfortunately, there is no definitive answer to that question, but the data presented in the study suggest that girls may be influenced by certain factors in their environment, such as the family context. For example, as the following table reports, parents tend to expect their sons, rather than their daughters, to work in a STEM field , even when both have the same performance in mathematics and science . Or another relevant figure: it is 8% more likely that boys use computers before the age of six than girls. The good news is that we can all contribute today to change this tendency. By introducing girls to computers and programming we can boost their confidence in their technical skills . At the same time their parents’ expectations may shift once they realize their daughters are not only perfectly capable, but they also show the same interest in programming as any boy would do. This has been precisely my experience during the REWE Digital Hackdays4Girls ’18 , in which I participated as one of the mentors for girls from 7 to 17 years old, guiding them to program with Scratch and a Calliope Mini device. Both projects use visual programming languages in which you combine script blocks together to implement the desired behaviour . Despite their apparent simplicity, they provide a wide range of possibilities and they proved to be an excellent way for the girls to develop a programmer mindset. Our aim was to let them explore everything they could use so that they would be motivated to continue at home . For example, using all basic Calliope features (i.e. microphone, speaker, all LEDs and buttons, temperature and brightness measurement, as well as its accelerometer and compass) all girls had the chance to implement functionalities such as displaying the room temperature or playing music. Furthermore, each team unleashed their creativity by programming a combination of interactive stories and games with Scratch. It was fascinating to see those girls, especially the younger ones, so immersed in their projects, showing such enthusiasm for each functionality they implemented. Some little girls discovered on their own how to use logical operators, all because they had run out of Calliope’s switches where to attach more functionalities, so they came up with the idea of allowing combination of switches, such as “Press A and C at the same time”, to trigger some action. And when our time was up, two 9-year-old girls made my day when they asked me with a saddened tone “Can we continue programming?” . After that day, we started receiving feedback from the parents describing how excited their daughters were on their way back home and how they eagerly explained in detail everything they had implemented. Even a month later we still receive messages from grateful parents with pictures of the girls programming their newly purchased Calliopes. We hopefully made a difference on 29 girls. And fortunately we are not the only ones, actually many companies, institutes and organizations around the world have been promoting similar initiatives for some years already. It is in the best interest of our society to enable students to choose their professional career based on their abilities and interests, and not conditioned by stereotypes . It is therefore important to continue reducing the gender gap until it disappears from the reports once and for all. Looking under the hood of the commercetools platform 325 Education Girls In Tech Gender Gap Scratch Gender Equality 325 claps 325 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-08"},
{"website": "CommerceTools", "title": "enjoy our commercetools api with httpie and jq part 1 of 2", "author": ["Michael Schleichardt"], "link": "https://techblog.commercetools.com/enjoy-our-commercetools-api-with-httpie-and-jq-part-1-of-2-ac270b3239fd", "abstract": "About We're hiring There are so many ways to consume our commercetools (REST) API: in different languages, with different SDKs, with the API playground or not even using REST but our GraphQL API instead. In this blog entry, we want to show you some best practices to use our REST API within interactive shells or bash scripts using the HTTP client httpie and the JSON processor jq . First, we show our favorite features for each tool and then we simulate a checkout in part 2. For running the examples the following commercetools project variables shall be given: These are real credentials to create a SPA (single page application)/mobile app. You can view but not change products, categories and project settings. You can create anonymous sessions to add products to a cart of that anonymous user. Httpie is a command-line HTTP client with an intuitive syntax, JSON-formatting support, syntax highlighting for JSON as well as HTTP headers right out of the box unlike other tools. The first example uses the Client Credentials Flow to create an access token. As you can see by default, the response headers are printed prettily as well as the JSON response body which is originally sent minified by commercetools. The “-a” parameter with the subsequent username and password sets the headers for basic authentication. The two key-value-pairs “grant_type” and “client_credentials” are HTTP query parameters which are url-encoded automatically if necessary. Thus, “scope”, which contains the OAuth scopes separated by space characters, will be encoded. With httpie, you are not restricted to work with JSON data. You can, e.g., download binary files like wget with the “-d” parameter: Httpie can do more than just downloading. It supports streaming, continuing downloads and shows the progress of the download. With the previous OAuth request we got an access token and let’s assume the token has been stored to a variable called “accessToken”. The next requests enable us to load the commercetools project settings for which we are authenticated: The response body will look this: As you saw in the project-get request, headers are declared by key value pairs using a colon: headerName:headerValue . For subsequent requests we don’t need to specify the “Authorization” and “User-Agent” header if we use `-- session=$projectKey`. The session feature reuses headers including cookies from previous requests. So if we now want to fetch the project data again, the command would be as simple as this: You don’t need to use a shell variable as name for the session (maybe just use --session=ctp ) but doing so makes it easier to work with different projects. At this point, we are authenticated and headers are set up. Let’s now query products! Products can be searched either with a GET request or with a POST request: The result, which we stored in the file “snowboard-equipment.json”, contains only two products but it is quite verbose: Let’s make this JSON more approachable by just extracting the fields that interest us. We can achieve that with jq. Jq J q processes JSON. It means it transforms JSON to other JSON documents, like removing fields, adding fields, updating fields, rearranging data and even calculating with values. It is also possible to create strings like CSV from JSON. Jq gets the JSON from stdin, so you can either pipe directly the httpie output into it or read the JSON from a file: Either way the output will contain a JSON document we created with jq (the curly braces within the quotes are for object creation ) containing only the properties “count” and “total” from the input document: It is not necessary to stick with the original field name. You can rename them: Jq’s features are not limited to the top level properties and object. You can also can create JSON arrays and create projections of array data, like the following command with the pipe operator that extracts the slugs of the products: For this project we have only the English locale, so we could put the slug value directly to the “slug” property: The same applies for “name”. With the extraction of other interesting fields like the “price” and “image” we can reduce the “snowboard-equipment.json” with This reduced representation of “snowboard-equipment.json” is formatted by jq and contains only the fields we are interested in. The filter, however, is quite long and barely manageable in an interactive shell session. An improvement could be to store the filter in a file: Then we can use the “-f” parameter of jq to read the filter — not the JSON — from a file: This concludes part 1 which intended to give you an introduction to the capabilities of httpie and jq. You have learned how to do basic requests with httpie and the basics of processing JSON with jq. Coming up in part 2, we will show you how to do a checkout with our API and give you some best practices to use both tools in bash scripts. Part 2 Looking under the hood of the commercetools platform 47 Thanks to Oleg Ilyenko , Stefan Meissner , and Ida Olsen . API Jq Httpie Commercetools Json 47 claps 47 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-19"},
{"website": "CommerceTools", "title": "new api extensibility feature", "author": ["Kelly Goetsch"], "link": "https://techblog.commercetools.com/new-api-extensibility-feature-5b3b8d83da2d", "abstract": "About We're hiring No off-the-shelf product will exactly suit your business needs, especially in the commerce platform space. We at commercetools are excited to offer an entirely new approach to extending our out of the box APIs with your custom business logic. Let’s look at a handful of hypothetical extension use cases: Allow customers to purchase only one pair of the latest hot basketball shoes Inject custom tax amount Calculating loyalty points earned for a shopping cart Add mandatory shipping insurance for expensive products Real-time inventory check with back-end ERP system during checkout Today, we offer essentially three extension options. Extend data model by allowing customers to add custom attributes and objects. For example, you could add “shoeSize” as an attribute of the customer profile Subscribe to events in commercetools and then asynchronously execute custom applications or serverless functions. If you subscribe to order-related events, for example, we’ll publish an “Order Submitted” event to the queue of your choice. That event can then be executed by a standalone custom application or a serverless function, like AWS Lambda. In this case, we’d want to send the customer a “Thank you for your order”-type email asynchronously Create a proxy layer that inserts custom code. Your front-ends call a little application you build (“AcmeCorpProduct”) which then calls the corresponding out-of-the-box commercetools API (“ Product ”). This is historically how legacy non-API based commerce platforms were extended the past few decades The event-based model is great but it’s only for asynchronous events. Proxies work as well, but you’re then left managing the lifecycles of dozens of these little wrapper applications. Using proxies also makes it hard to use our SDKs from your front-ends, as commercetools SDKs work with commercetools APIs. Now that we have some context on the status quo, I’d like to introduce our new approach. Essentially, we’re now allowing you to synchronously call back to one or more serverless functions or arbitrary URLs from within the commercetools platform. The benefits of this are simply revolutionary. You can simply call our out-of-the-box APIs and still have your custom code executed behind the scenes. This allows you to use our entire developer ecosystem, including SDKs. Your custom code can be run as independent little serverless functions, rather than entire applications whose lifecycles must be managed. Let’s take an example to illustrate how this works in practice. Say you’re selling furniture and have a business requirement that caps the number of items in a shopping cart at 10 due to delivery truck size constraints. You’d build a little Google Cloud Function that performs this check: Next, using our extensions APIs , you define your extension endpoint and bind it to all create/update calls to the cart API. At this point, you’ve bound the extension to the cart API. Let’s see what happens when you try to add 11 products to the shopping cart: You correctly get the “You can not put more than 10 items into the cart” error. If you look at the entire error response, you can see which extension was responsible for throwing the exception: The caller of this API can then gracefully report to the end-customer. There’s a nice tutorial for this functionality at https://docs.commercetools.com/tutorial-extensions.html . Internally, the way it works is pretty straight-forward. The client (SDK, REST client, etc) calls a commercetools API. The out-of-the-box commercetools code generates a client-agnostic JSON representation of the object being touched (e.g. cart, customer profile, etc). But instead of immediately persisting that JSON to the internal datastore and returning it to the client, the platform now looks which extensions are registered to the particular API for the particular action being performed. Each extension is passed the JSON object. It can request changes, which the API applies (like it does from the regular client) and the modified JSON object is then persisted to the platform’s datastore and the response is then returned to the client. Alternatively, the extension can return an error, which is returned to the client and nothing is persisted. We’ll wait two seconds for each of the extensions. If it takes longer than that, a timeout error is thrown and the client is free to re-try or pass that failure back to the customer. While this functionality is in beta mode for the carts API only, we expect to roll it out to the entire platform in the next few weeks. We will heavily invest in this functionality over the next few quarters, releasing functionality such as: Declarative reference expansion, allowing the extension to receive objects it would not normally have access to with the API the extension is bound to. For example, you might want to bind an extension to the user’s cart that’s dependent on the user’s profile. The extension can declare it wants the user’s profile, thus saving an additional HTTP request Declarative dependencies, allowing for extensions to be run in a prescribed order. Today, all registered extensions are executed in parallel at the same time. But there are use cases where it would be great to define an order of execution Predicate-based triggering. Today the extension is always called if it’s bound to a particular API for a particular action. For example, a cart extension bound to the “update” action will always execute when a cart is updated. Using our predicate language , we’ll support being able to call extensions selectively. Sample predicates include: For more information about commercetools extensibility, have a look at: techblog.commercetools.com Thanks to Christoph Neijenhuis , Ida Olsen and the rest of the checkout team for making this a reality! Looking under the hood of the commercetools platform 30 Thanks to Michael Schleichardt . Serverless API Lambda 30 claps 30 Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-20"},
{"website": "CommerceTools", "title": "takeaways from serverless conf nyc 17", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/takeaways-from-serverless-conf-nyc-17-724e297c9e65", "abstract": "About We're hiring The 5th Serverless Conf took place in New York last week. It was the first one I attended (most likely not the last one, though!) and I was impressed with the overall quality of the talks, the enthusiastic crowd, and the many bigger production apps demoed. These are my key takeaways. The scope of apps built with functions (such as AWS Lambda) has come a long way. Generally, they have become larger and therefore more complex. Startups have left the MVP, “move fast and break things” stage. Their teams have become larger, and the product is approaching product-market fit. Enterprises use functions not only for glue code and background processing anymore, but in the core of their application. This poses higher requirements for operating an application, serverless or not. A lot of talks focused on how to monitor serverless apps, and general tooling for operating them. For larger teams and code bases, Infrastructure-as-code becomes vital along with a good CI/CD pipeline. These tasks are usually associated with DevOps. Turns out, we can’t (with good consciousness, at least) claim that serverless is NoOps. However, compared to what we’ve traditionally called DevOps, operating a serverless app is easier in many ways. A lot of the heavy lifting that would traditionally be done internally (like running Kafka, tuning a DB cluster etc.) is done by the cloud platform and services are leveraged heavily. Serverless developers are in general very careful of designing their system as simple and maintenance-free as possible. This results in not needing a dedicated DevOps role, or Ops team, but everyone on the team is able to — and responsible for — doing Ops. The term #DiffOps was proposed, let’s see if it catches on! Some talks discussed migrating from a legacy app to serverless, and other talks discussed migrated from a cloud-native background. It’s interesting to compare the experience. A cloud-native dev already has the mindset in place to go serverless, and a lot of knowledge working with their chosen cloud platform. It’ll be more of an evolution than a revolution for someone coming from a container-based microservice architecture, and they get up to speed quite quickly. However, a legacy app dev has the most to gain! They’re most likely looking at a major refactoring or complete rewrite when going for a modern architecture anyway. If they go serverless, they can leapfrog previous cloud technologies like VMs and Containers. Similarly, most of Africa telecoms never built landlines, but invested right away in mobile networks instead. The take-away for me is that I’ll have to pitch serverless quite differently, depending on the background of the person I’m talking to. For someone working on legacy technologies, my pitch should be 80% generic cloud and 20% serverless. For a cloud-native dev, the pitch should be really focused on FaaS and on using truly “serverless” services (i.e. not a DBaaS where the user is responsible for scaling up/down). The motivation behind microservices is an organizational one, based on Conways Law: As the dev team grows bigger, communication gets harder and productivity goes down. Splitting into independent teams with independent(ish) services allows each team to have a high productivity again. That however flies in the face of how we’ve traditionally built web apps (with Ruby-On-Rails, Spring, or whatever). Today, microservices is more associated with an architectural change than with an organizational one: Suddenly, one has to deploy many services instead of one, have an event-based architecture to communicate between services, monitoring needs to trace requests between multiple services etc. What is a “serverless monolith” anyway? In terms of organization, it means that every developer can touch any part of the code base, and is responsible for all functions. When moving to microservice-teams, we don’t need to change the way we deploy and monitor, or re-design our architecture from the ground up. The move from monolith to microservice for a serverless app becomes mostly an organizational change. This is great news, because enforcing a microservice architecture on a small (2 to 4 person) dev team often meant they’d perform worse than with a monolithic architecture. But if the project takes off and the team size increases into two-digit ranges, moving to a microservice architecture could be a huge time investment. Yes, really. See for yourself: On a more serious note, AWS (but also Azure and IBM) are working hard to push their FaaS into the core of big apps. Tim Wagner of AWS will publish a book on Serverless Design Patterns next year, Azure demoed Durable Functions (a simple way to do a fan-in or wait for input), IBM presented Composer (orchestrates functions and data-flow), and Lambdas will support canary and blue/green deployments. All of this is clearly targeted at large apps, and at having their core run on FaaS. We’ve clearly come a long way from running only cron jobs and background processing! A SaaS, especially when targeted at Enterprise-level customers, needs to be customizable. Salesforce, maybe the prime example for SaaS in general, has built an execution environment early on into their CRM. Many APIs today provide webhooks. SaaS that want to add extensibility today often base them on a serverless tech, e.g. Twilio Functions are built on top of AWS Lambda. Compared to plain webhooks, Auth0 has seen a higher retention rate for customers trying out their product. It has also allowed persons not traditionally thought of as developers to customize their API. They’ve kept the entry level low by having a code editor in the browser and offering many templates. That is, for semi-technical people, a much easier starting point than having to set up their own server to receive webhooks. Serverless is maturing quickly. For us at commercetools, as a cloud-native platform, this is fantastic! Our customers have an easy way to deploy and run their code. We take away many of the Ops-headaches of the past associated with running a legacy commerce system, and a FaaS is a perfect counterpart for the apps our customers built on top of our platform. We already integrate tightly with AWS, Azure and Iron.io for background processing . I‘m looking forward to extend the integrations, and bring serverless technologies even closer to our platform! I had a blast at the Serverless Conf. There is so much more talks and themes that I didn’t touch on in this post, from security to debugging to multi-cloud setups to performance tuning to cost savings that I can only recommend you to watch some talks from the conference as they come online. I’m really excited, both about building purely serverless apps, and about integrating serverless technologies into our platform. Looking under the hood of the commercetools platform 63 1 Serverless AWS Lambda DevOps Cloud Computing Microservices 63 claps 63 1 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-18"},
{"website": "CommerceTools", "title": "seven patterns by example the many ways to type radio in react", "author": ["Tobias Deekens"], "link": "https://techblog.commercetools.com/seven-patterns-by-example-the-many-ways-to-type-radio-in-react-bfe14322bb6f", "abstract": "About We're hiring A guided tour through seven patterns in React explaining different ways of implementing a radio group as a React component. It will cover patterns such as: compound components , context , prop getters , render props and Function as a Child (FaaC). But first let us take a step back by discussing the problem we are trying to solve. Update: A section about using the new Context API was added contrasting both the oder and newer API. An <input> element of type radio is often used as a radio group which, as the name implies, groups related options . Radio groups should be used when only one option can be selected at the same time by a user. An example would be selecting the size of a T-Shirt of either M , L or XL . Implementing these in React without wrapping them into a component is fairly straight forward. In its simplest form it could look something as this: The example is implemented as a controlled component which means nothing more than that the size data is handled by a React component rendering the input while the input has a change handler to notify our React component about changes. To solve the problem above we would render three of these inputs each with a different value . There are however already some downsides to this approach: Leaky abstraction: onChange passes an event to handleChange from which we need to extract the value (from event.target.value ) Not DRY: to ensure that checked is correctly set we have to repeat the comparison of props and the value on each input field itself which is error prone and might break easily Not accessible: each input should be wrapped by a label or have an adjacent label with a for prop. Clicking this label should also trigger the RadioOption . This should abstracted away to ensure that a label is always present and users of our RadioOption do not have to worry about it. It is worth mentioning that the same applies to aria-roles . However, these are are out of scope for this article. Lastly, the input likely needs some custom styling to fit into the corporate identity of the project. This can be achieved by one or more className s supported by for instance classnames . However, this will also be repetitive when using the input and when forgotten or changed it leads to inconsistent UI. With all that in mind we can for instance imagine our inputs needing the following look: With the requirements and problems being defined, lets finally jump into some code which will build up different proposed solutions. We can solve most of the goals stated above by defining a RadioGroup which receives an options argument. You would then use this argument to pass the complete configuration for all the RadioGroups as a complex data-structure. The options would be of type array and should contain a value and an label to distinguish between the visual representation and the actual value of each option. This method is quite widely used for instance in react-select and would look as follows: The responsibility of the RadioGroup is to render an input per passed option while computing and forwarding the provided prop to the inputs . Lets look how this solves our problems in more detail: The RadioGroup can internally pass down an improved onChange to the underlying input extracting the value from the event The RadioGroup can contain some custom styling through e.g. CSS and custom SVGs and abstract those away The RadioGroup can pass checked to the input comparing value and option[n].value Even though this feels like a good solution already we can still improve our component to solve some of the following previously unthought of issues: Improve the declarative nature of our component (one of React’s strengths) by defining what we want to render not through an options argument Slightly improve performance of the application as options will have a new reference on each render triggering a wasteful render (for completeness: we can also use memoization or an immutable data structure for this) Enable to potentially influence how the actual inputs are rendered to e.g. change their layout With these in mind we can continue to our next solution. We can continue tackling more of the discovered and newly created problems by abstracting the input into an RadioOption being wrapped by an RadioGroup . The latter will start to show its advantages a bit later. The envisioned way to declare a group of radios would be: This nicely iterates on our newly found problems of our previous solution: We declaratively render RadioOptions as child-elements of RadioGroup defining the intended state of our view We will not trigger wasteful renders by passing in referentially different options We can use any kind of layout (e.g. custom div ) component in between or around the RadioOptions The RadioOption can still pass down an improved onChange to its input The RadioOption can still contain custom styling The RadioOption also passes checked to the input comparing value and selectedValue However, we can still do better and remove for instance the need to specify the selectedValue and onChange on each RadioOption . Compound components are a pattern in React in which the given children are re-assembled by providing or injecting custom props which the parent (in our case RadioGroup ) is able to compute and provide. Using this pattern eases the usage of our RadioGroup and RadioOption to the following: So how do the props from RadioGroup end up in RadioOption and ultimately in the input elements? Through React’s Top-Level API which has functions such as React.children.map and cloneElement . For a more profound deep dive of these and other functions please also head to “A deep dive into children in React” . In our specific case we would map over the children checking their type to be of RadioOption to cloneElement each providing the additional props . Note that for the given example the RadioGroup will likely be an React.Component we just wanted to keep things short for the code sample. Anyway, this pattern is able to solve all of our problems stated above. It is declarative, encapsulates styling, is performant and serves a good and concise API to consumers of RadioGroup and RadioOptions . There are however some downsides with it: Prop-type validation happens at element creation time to produce more useful errors. However, we cloneElement the RadioOption already implying that it has been created before. This results in the fact that we can not mark any of the compound props from RadioOption as isRequired Direct child : means that each RadioOption has to be a direct child of RadioGroup . We can not have any wrapping components around a RadioOption . The latter might be a bit of an “artificial” problem and also a good constraint to consumers of our component. However, react-call-return plans to solve this problem. Allowing a more natural composition of components in future versions of React. Until then we can, if needed, resort to another solution. In general context enables us to build an API where parents and children can communicate across other component’s boundaries. Without going into too much detail: using context is a bit controversial in general. It might change with an RFC in process . A good guide to when to use it is laid out in an older Tweet by Dan Abramov. Assuming that we are sure to use context we would: Note that React introduced a new Context API in version 16.3 in by which examples might be slightly outdated. However same concepts still apply and the older Context API will continue to work for now. Keep on reading to understand how the new and old Context API differ and how code becomes drastically simpler to understand with the new API. In this example RadioGroup functions as a provider for the context while RadioOption is a consumer. This replaces the need for cloneElement as the RadioGroup has to manually wire itself up by reading the respective values of the context . As a result, using context decouples RadioGroup and RadioOption as expected allowing nesting and intermediary components in between the two. For a consumer of our two components not much changes as context is an implementation detail depending on the requirements we need to fulfil. Using context also somewhat nicely solves our previously discovered problem with missing ability to perform rigorous prop-type checks with isRequired due to the cloneElement for compound components. It should also be noted that it is advised to abstract the usage of context in the attempt to guard against potential breaking changes in its API. We can do so by e.g. using Higher Order Components (HoC) offered by recompose . Using withContext in RadioGroup to define the childContextTypes and getChildContext and getContext in RadioOption would allow us to achieve just that. There is another major downside which is oftentimes forgotten when working with context in React. Whenever any of the intermediary components returns false from shouldComponentUpdate a change in any of the context values will not be propagated to the RadioOptions . All of the edge cases above are taken care of with the new Context API. Also the code becomes easier to follow as there is less manual wiring needed. When using the new Context API (React 16.3 needed) the code would look roughly as follows: When creating a new context a Consumer and Provider pair is generated. The Consumer can only be rendered as a child of a Provider anywhere deep down a component tree. The call to createContext also allows specifying initial values which in our case are not known at that time. Furthermore, we memoize the context value generation to not trigger wasteful rendering whenever the Provider renders but none of the values (passed down API) actually changed. The Consumer pair then receives the context value in a Function as a Child (more also on that in Solution 5. ). We can now render a Provider as far up the tree as possible and the Consumer as far down the tree as needed. Both communicate through the created context. The Context API not being supported in older versions of React may sound like a problem. However, libraries such as flopflip work around that constraint by using something like create-react-context which acts as a polyfill for the Context API allowing you to even use it today whenever you did not manage to upgrade to a later version of React. Moving away from a controversial pattern, to allow for nesting, to a more “standard” way empowering this, we can explore the power of the Function as a Child pattern in React. As a result children of RadioGroup would become a function receiving arguments for the RadioOptions such as onChange , name and the value . With this explanation the code would roughly look as follows: Apart from the attempt to be fancy and using React.Fragment this solution should show that we could use any other intermediary layout components between RadioGroup and RadioOption . We could also use a render prop instead of children or offer both. A decision which is up to the team to decide upon. Anyhow, there are some slight downsides with this, as with any, approach namely: Wiring up RadioOptions is left to the consumer of our component making the API a bit more noisy to work with Without extracting children onto some instance variable it will be referentially different to RadioGroup during reconciliation triggering wasteful renders just as options With increasing distance between RadioGroup and RadioOption and more nesting it might become unclear where onChange comes from. It might even yield silent errors by shadowing any of the props such as onChange which will break the component However, none of these stated problems are unsolvable which brings us to another possible solution. Prop getters are a pattern in React which recently gained prominence but theoretically was possible for longer. Its use and visibility increased significantly with the release of downshift . With it, we would pass down a prop getter from RadioGroup which would look like: The example shows how all props are passed through the FaaC of RadioGroup to RadioOption and are contained in a simple function when invoked returning the needed props for the RadioOption . This pattern quite significantly decreases the amount of manual wiring we had to carry out in the previous solution. Often the prop getter itself accepts additional arguments which influence its returned props. We could for instance pass in a related prop such as className which would look like getOptionProps({ className: 'custom' }) . This serves already as a quite concise and nice way to implement both RadioGroup and RadioOption . There is however another pattern to explore. This might be the most rarely seen solution which might make it seem a bit exotic. We add a little twist to the previously mentioned compound component pattern by mixing it with the FaaC solution. This gives us a RadioGroup which passes down “patched” RadioOptions through its children prop. It would look something like: This gives us the ability to nest just as with our context and FaaC based solutions without imposing any of their stated problems. The pattern has been discussed in a tweet and is also used within an older version or react-radio-group . It mixes both the FaaC and compound component patterns to an extend where the RadioOption is dynamically assembled in the RadioGroup for use in the children function only needing the value . I hope you enjoyed the little walk through different ways of implementing an good old type=\"radio\" in React. I am sure there are even more ways of about everything mentioned before. It is important to keep in mind that none of the outlined solutions is meant to be the best or goto approach. All seven seven patterns come with their own set of advantages and disadvantages which should be contrasted with the given requirements. More importantly all these patterns do not only apply to radio buttons but to almost any component you might be building in the future. We at commercetools went with the compound component solution for now while accepting a vertical or horizontal layout prop on the RadioGroup to work around the nesting and requirement for different layouts options. Looking under the hood of the commercetools platform 706 5 Thanks to Florian Dietrich and Philipp Sporrer . React 706 claps 706 5 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-28"},
{"website": "CommerceTools", "title": "gzip on aws lambda and api gateway", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/gzip-on-aws-lambda-and-api-gateway-5170bb02b543", "abstract": "About We're hiring gzip is a compression format widely used over HTTP for text-based files like JSON, HTML and CSS. Depending on the repetition in your data, the gzipped file is usually 2 to 10 times smaller than the original. For a user on a slow (e.g. mobile) connection this can make a huge difference. I’ll show to generate gzip responses on AWS Lambda. In a previous post, I discussed how an existing API that we don’t control can be customized by proxying the traffic through the API Gateway and (selectively) through Lambda, so I’ll also show how we can pass already gizpped responses through. Unfortunately, the API Gateway is currently oblivious of gzip. If we’re using a HTTP proxy, and the other HTTP endpoint returns a gzipped response, it’ll try to reencode it, garbling the response. We’ll have to tell the API Gateway to treat our responses as binary files — not touching it in any way. Since all my API responses will be gzipped, I’ve done so for all content-types (using */* ) in the screenshot on the left. But you can also limit it (e.g. application/json if you only want to gzip JSON files). With that change, the HTTP proxy will successfully pass the gzip response from the origin server on to the client. Now that we’ve enabled binary support on the API Gateway, our Lambda function will receive its input differently. The API Gateway expects binary data, but it passes a String to the Lambda function. Therefore, Athe Lambda will receive the binary data in a base64-encoded String. If the client is POSTing a JSON file, we can’t parse it directly anymore, but first have to decode it. In a previous post, I had a Lambda function modify the JSON of a request, send the modified request to an API and pass on the response to the client. What if the API returns a gzipped response? First, we have to collect the binary data. Again, the API Gateway does not take binary data directly, but needs it base64-encoded. We’ll also have to set a special flag when we call it back with a base64-encoded response. This will work for both gzipped and regular responses. A small optimization is to check if the Content-Encoding header is set in the response. If not, we don’t have to base64-encode the response. Creating our own gzipped response First, we should check if the client can decompress gzip. If so, the Accept-Encoding header should contain “gzip”. If it’s present, we’ll use zlib to compress our response, and use base64-encoding as above. We’ll also have to set the Content-Encoding header, so that the client knows what to expect. The above gist is kept short for clarity, you probably want to set additional headers. Headers are case-insensitive, you may want to normalize them first. Especially when writing tests for the Lambda functions, it becomes a bit cumbersome as one has to base64-encode the JSON request first. Also, the performance gains will be limited if the request only originated from within AWS (e.g. from another webserver) and not from an outside client (like a mobile app). To disable it on the API Gateway, you can overwrite the Accept-Encoding header. Unfortunately, you have to do so on every HTTP proxy that you create. Similarly, the Lambda-function should unset the Header. Proxying or returnig gzipped responses with the API Gateway and Lambda is not as straight-forward as one would expect. Hopefully in the future, the API Gateway will honor the Content-Encoding header when proxying requests, and gzip (or otherwise compress) responses on its own. In the mean time, with a few lines of code, we can make it work. It’s also not hard to add other compression formats, such as deflate. You can find complete Gists for the proxy scenario here: Gateway , Lambda . Looking under the hood of the commercetools platform 65 2 Thanks to Tobias Deekens and Stefan Meissner . AWS Lambda AWS Aws Api Gateway Serverless Compression 65 claps 65 2 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-16"},
{"website": "CommerceTools", "title": "kotlin and the commercetools jvm sdk", "author": ["Michael Schleichardt"], "link": "https://techblog.commercetools.com/kotlin-and-the-commercetools-jvm-sdk-649d1bcf8fd8", "abstract": "About We're hiring Kotlin is a neat language to interact with the commercetools Java tooling. This will be shown in a brief introduction and illustrated in Java and Kotlin code samples. Commercetools provides an omnichannel commerce JSON HTTP API which can be consumed by different programming languages. The commercetools JVM SDK binds our API to the Java™ Platform, SE 8 and therefore can be used with Java, Scala and Kotlin. At the moment the majority of Android devices does not support Java 8 which makes the JVM SDK not suitable for Android. Kotlin is a statically typed programming language which can be used to build applications for the JVM (6–8), Android, browser and even native applications. Kotlin can be compiled on the command line with a standalone compiler, Maven, Gradle and the IDEs IntelliJ IDEA and Eclipse. Sure, there are other, older players which compile to Java byte code like Scala, Clojure and Groovy. But Kotlin achieved some massive milestones, like the integration into the Spring Initializr , a code stub generator for Spring applications. A second big milestone is the support of Kotlin to configure Gradle builds . The third milestone is that Google made Kotlin a first class member of Android development next to Java. Kotlin is used by famous brands like Pinterest, Coursera, Evernote, Uber, Basecamp and Atlassian. In the following section I will compare Kotlin and Java based on some examples in the commerce domain. The Kotlin language provides type-inference, but to make the examples more expressive we annotated the types. To showcase the awesome features of the Kotlin language we show a Java 8 code snippet first for each example followed by a Kotlin snippet. Kotlin enables you to use getters and setters as if they were properties while those methods are still used internally (see the second line) which improves readability of the code. The Elvis operator (?.) is a short expression to make null safe method calls by calling the function on an existing object or return null directly. This way no NullPointerExceptions will be thrown and no if-else statements are necessary. The “?:” operator provides a default value if the left statement is null. The following example within the context of commercetools shows how to update an existing address with the country or creating a new address with the given country in case the address does not exist yet: In the JVM SDK we annotate getters which may return null with the Nullable annotation from JSR 305: With this annotation IDEs can show a warning that a call may result in a NullPointerException: Kotlin goes a step further and distinguishes between nullable references and non-null references. Assigning null to a non-null reference yields to a compile error as well as for accessing directly a method of a nullable reference without a null check. Nullable references are marked with a question mark at the end of the type annotation. So, for example, sku is a nullable reference of type “String?” and variantId is a non-null reference of type “Int”. Kotlin can infer automatically that the sku is a nullable String since it is annotated as Nullable in the JVM SDK. Kotlin not only provides String interpolation — it is just enabled for all String literals by default! (You may need to escape $ with a backslash if it is literally a dollar sign.) The JVM SDK uses polymorphism for the CartDiscountValue interface which can be either an absolute or a relative discount. The Java example shows how any CartDiscountValue is formatted for the given currency and locale. In Java it is necessary to perform instanceof checks and then cast before you do some business logic, like finding the absolute discount with the correct currency. Whereas the Java example looks complicated and you feel tempted to skip this example, the Kotlin snippet is quite brief by using some simple pattern matching with the when expression. For each case it is checked that value is of type AbsoluteCartDiscountValue or RelativeCartDiscountValue while the right hand side value is being automatically casted to the corresponding type. In addition to the smart cast, also the collection extension method first is used in this example which finds the first item in a list matching a predicate or throwing a NoSuchElementException . Update actions express in what way an entity, like a product or a category should be updated. In the JVM SDK there are a lot of cases where the simple class names of update actions occur several times, like ChangeName that exists for products as well as for categories. To distinguish between the update actions for different entities used in one class a fully qualified class name needs to be used in Java whereas in Kotlin it is possible to import elements by giving them an alias name with the keyword as . All in all this is only a small subset of Kotlin’s features we demonstrated here, there is also multiline strings, collection extension methods, no checked exceptions, inline functions, data classes and more. Want to start working with Kotlin and the JVM SDK? On GitHub we provide an example you can simply run or extend. Looking under the hood of the commercetools platform 11 1 Thanks to Oleg Ilyenko , Nikolaus Kühn , Yann Simon , and Tobias Deekens . Kotlin 11 claps 11 1 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-07-11"},
{"website": "CommerceTools", "title": "boosting product categorization with machine learning", "author": ["Amadeus Magrabi"], "link": "https://techblog.commercetools.com/boosting-product-categorization-with-machine-learning-ad4dbd30b0e8", "abstract": "About We're hiring Product categories are the structural backbone of every online shop, but it can be quite a nightmare for e-commerce managers to make sure that all products are assigned to the correct categories. The set of available categories is typically large (Amazon has listed over 50000), changes constantly, and new products have to be added on a daily basis. Mistakes can be costly, because miscategorized products not only look confusing and unprofessional, they also cannot be sold when customers are not able to find them. To improve the process of product categorization, we looked into methods from machine learning. Our goal was to develop a machine learning system that can predict which categories fit best to a given product, in order to make the whole process easier, faster and less error-prone. In this blog post, I am going to walk you through the problems we faced on the way and how we decided to solve them. From the perspective of machine learning, there are some unique challenges to the problem of predicting product categories: The class set is very large. Machine learning applications typically only have to predict between a few selected classes (e.g. classifying an email as spam or no-spam), but in e-commerce there are often hundreds or thousands of categories that need to be classified. To train robust models for these cases, you need a particularly large amount of training data. Product data is diverse and unbalanced. One product can have a detailed set of attribute data, which is completely missing in another product (e.g. color, size or material of a t-shirt vs. expiration date, fat content or volume of a milk). Taking into account all available product variables would lead to an explosion of missing values, which makes model convergence that much harder. To handle this problem, we decided to keep it simple and only use product names, images and descriptions as our predictor variables, because they are available for the majority of products and arguably carry the most important information. Every online shop has a unique category structure. At commercetools , we offer a cloud-based API to manage commerce platforms (dealing with processes related to products, orders, customers, carts, payments, etc.). Our API is designed with flexibility in mind, so our customers operate in very different industries (such as fashion, groceries, agriculture, home supplies, winter sports or jewelry). When building our machine learning system, we need to make sure that only categories relevant for a specific shop are being recommended. One way to handle this problem would be to train separate models for the products and categories of each online shop, but there are a bunch of problems with this approach: Some stores might not have enough product data for models to converge, classes are more likely to be unbalanced (e.g. a fashion shop might have a lot of data for the category “t-shirts” but only a few cases for “bandanas”), models need to be retrained frequently to account for newly added categories, and the infrastructure to handle all these models and their different versions in production can get quite complicated. For these reasons, we took a different approach and trained one general-purpose model that covers a broad range of categories and can be used by all of our customers. To map these general categories to store-specific categories, we use a separate machine learning model that quantifies similarities between words (i.e. to identify that the general category “jeans” corresponds to the category “Fashion > Men > Jeans” in store A, but “Clothing > Pants” in store B; more on that later). With this approach, we lose some accuracy for categories that are very customer-specific, but we gain more flexibility, more data to handle unbalanced classes, better support for smaller stores, and less maintenance costs. To recap our goal: We want to build a machine learning system that predicts a broad range of product categories from names, images or descriptions. These categories are then mapped to store-specific categories through a separate machine learning model. Our main coding language to build this system is Python. Defining a good class set for this problem is a bit of an art form. If the set is too small, you might miss some categories that are important for a specific store. But if the set is too large, prediction accuracies will drop significantly. We tested both larger and smaller sets and ended up with a set of 723 categories in our current version. The set is composed of rather broad terms, typically consisting of just one or two words. Here are some examples: When we started, we tried to predict as many categories as we could imagine to ensure that we have a good coverage. But it is important to realize that our customers are not an evenly distributed group across the entire category landscape, but are more like clustered islands of particular industries. Even though it is against the very nature of every data scientist, we found that it is actually a good idea to “overfit” our models to these islands, since these are the actual use cases of our customers. When we get customers from entirely new industries, we can still adapt the class set, so that our models can evolve together with the needs of our customers. Now that we know what we want to predict, we can start looking at the variables we want to use for the predictions. When it comes to image classification, convolutional neural networks are undoubtedly the gold standard, mainly because of their ability to identify and combine low-level features (lines, edges, colors) to more and more abstract features (squares, circles, objects, faces). We find a similar mechanism in the visual cortex of the human brain, so this algorithm must be doing something right. With the main breakthrough of convolutional neural networks in 2012 , researchers have consistently improved their architecture in the context of the annual ImageNet competition, which is something like the olympics of computer vision. In 2017 , the winning contribution reached an impressive top-5-accuracy of ~97.8%. Remember when I told you that we are going to need a particularly large amount of data to train a robust model for all these product categories? We are going to cheat a little here with an approach called transfer learning . Training a state-of-the-art convolutional neural network from scratch takes a lot of time, data and computational resources, so we are just going to take a neural network ( Inception v3 ) that has already been pre-trained on a large image dataset. This model has already learned a lot about extracting and combining image features, but it does not yet know which categories we want to have predictions for in our particular use case. To bend the network to our needs, we simply cut off the final classification layer, add a new layer with 723 units corresponding to our product categories, and then retrain only these weights with our own dataset, while all the other model weights are frozen. This allows us to build robust, customized classifiers with relatively little effort and data, in our case with a dataset of ~130000 images (~100–200 for each category). We used the library TensorFlow in Python to implement this approach and ran model training on the Google Cloud ML Engine (with modified code from this example). The code we wrote on top of that mainly deals with downloading images from urls, converting them to jpeg, rescaling them, sorting them into subfolders for each category, removing duplicates or invalid files, and uploading bottlenecks and trained models to Google Storage . There is still quite some manual work involved in double-checking all the images, because unfortunately not all images assigned to a product are representative images for a category that you want your network to learn (images of usage instructions, generic company logos, low quality images, etc.). Next, we built our classifier for product names. After cleaning up duplicates, uninformative names, and balancing our dataset, we ended up with~230000 samples (~300 for each category). To make the names easier to deal with and reduce their dimensionality, we first run them through a preprocessing pipeline (mainly using the libraries re and spacy ): Lowercasing all letters. Removing punctuation and special characters (like * , | or . ). We keep hyphens to preserve information in cases like “t-shirts”. Removing stopwords ( the , and , in , etc.) because we do not expect them to have much predictive value. Lemmatizing words (≈finding word stems) to remove variance from word inflection (i.e. we want our model to know that “apples” and “apple” refer to the same thing). We experimented with removing very short words (1–3 letters) and automatic spelling correction, but excluded these steps in the end because they did not lead to a better performance. After that, we want to convert our preprocessed text samples into numbers, because this is the only language that machine learning models can work with. We tested several methods to accomplish this: Bag-of-words : Each sample is converted to an n-dimensional vector corresponding to the set of unique words in the dataset, with values of the respective word frequencies in the current sample. Easy to do, but ignores syntax and leads to very sparse vectors (≈high-dimensional space with a lot of zeros) which complicate model training. TF-IDF ( term frequency-inverse document frequency ): Similar to bag-of-words, but weighs word occurrences in a text sample higher when the words are rare in the rest of the dataset, since these words are likely to be more descriptive of the sample. Further, words with a high overall frequency in the dataset can be excluded from the lexicon. As a result, both the impact of non-informative words as well as the dimensionality of the vector space can be reduced. Word2Vec : Solves the sparsity problem by training a two-layer neural that predicts the context for a given word (i.e. the word “Nike” will be more often next to the word “shoes” than “bananas”). This is more complex to compute, but manages to create a low-dimensional text representation that encodes subtle semantic similarities between words and is easier for classifiers to train on. We achieved the best results with TF-IDF. Even though Word2Vec definitely outperforms TF-IDF in tasks that include complex semantic relationships between text samples, it is an overkill for our use case, since product names are rather simplistic and have barely any syntax in them. After preprocessing and vectorization, we can finally build our actual text classifier. We tested prediction accuracies for a range of machine learning models in the library scikit-learn : Naive Bayes , Logistic Regression , k-Nearest Neighbors , Random Forests , Support Vector Machines and Gradient Boosting . Logistic Regression with TF-IDF vectorization performed best, which shows yet again that you do not always need the most sophisticated and complex techniques to achieve the best results. We used the same setup to develop a classifier for product descriptions. Even though descriptions have more complex syntax than product names, the same combination of Logistic Regression and TF-IDF achieved the highest accuracies. Now, we have classifiers for images, names and descriptions that generate probabilities for our 723 categories. How do we integrate the predictions from the different models? You can get more fancy here and train a so-called ensemble model , which is basically a higher-order machine learning model that takes as its input the output of other models. But for our purposes, it was enough to compute the mean of the class probabilities of each model to generate our final predictions. We can get general category predictions now, but we still need a mechanism to match these categories to store-specific categories, such that we do not bombard our customers (i.e. different online shops that use our API) with category recommendations that they have not defined. If our model says this product belongs to the category “bracelets”, we need to know to which categories in the store that made the request this fits, since there can be quite some variance. For this task, we used the library gensim to train a Word2Vec model on a large corpus of Google News articles, which is commonly used to estimate word similarity. Since it can take a while to compute the similarity between all model categories and all store categories with this model, we precompute these similarities and store them in a database, which is updated every night. Like the class probabilities, category similarities are also scaled to the range between 0 and 1, and we count every value above 0.6 as “similar enough” to match a model category to a store-specific category. To account for the variance in the similarities between matches, we multiply the probabilities of our class predictions with these similarity scores to quantify our confidence in the final category predictions. We achieved our highest accuracies above 90% for stores in the fashion or jewelry industries, whereas our lowest accuracies of 70–80% were for stores in the grocery and home supplies industries, which mainly stems from the fact that the latter industries have a significantly larger category set and a lot more diversity in their data. To expose our application, we wrote a HTTP API in the library flask . We have two endpoints, one for general model predictions and one for store-specific predictions. The general endpoint is mainly used to test the behavior of our classifiers for different images, names or descriptions: In contrast, our store-specific endpoint has the following workflow: First, it takes as input parameters the project key of the store and the id of the product that needs category recommendations. We then look for data on images, names or descriptions in our database and pass it to our machine learning classifiers. After that, the model predictions are matched to the store-specific categories and the ids of the most likely categories are returned. Confidence scores can get rather low due to the large class set and the similarity matching procedure, but the important part is the relative difference between confidence scores to identify the most relevant categories. We use the store-specific API to generate recommendations in our user interface called the merchant center , where our customers have access to a range of features to manage their commerce platforms. The feature is currently in the beta testing phase and the API documentation can be found here . We are looking forward to iteratively adapt the feature through feedback from our customers, since there are many knobs in our development pipeline that we can fine-tune to improve the application. Looking under the hood of the commercetools platform 372 8 Thanks to HBA and Evi Lazaridou . Machine Learning Data Science Retail E Commerce Software 372 claps 372 8 Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-02"},
{"website": "CommerceTools", "title": "embracing real time feature toggling in your react application", "author": ["Tobias Deekens"], "link": "https://techblog.commercetools.com/embracing-real-time-feature-toggling-in-your-react-application-a5e6052716a9", "abstract": "About We're hiring Update: We released a flopflip@5.0.0 which brings a SwitchFeature component for multi-variate toggling, a rebranded ToggleFeature , flag selectors for react-redux , noticeable performance improvements and many helpful minor changes. Update: We released a flopflip@3.0.0 which brings custom adapters such as localstorage , in memory , LaunchDarkly or split . You can also roll your own to integrate with another service or API. Configuration and examples are in our readme . Toggling features within a code base is a technique to influence the behaviour of software without having to redeploy. It is often used in conjunction with Continuous Delivery enabling alternative branches for different user groups within the interface a web application. These branches should optimally be triggered independently from the main code base . We at commercetools like to have small iterations on features gathering feedback from customers in the process. Without feature toggling we struggled to target specific customers and often had to make assumptions about the scope and shape of features during development before releasing them. Using feature toggles allows us to engage with our customers and shape features with them by targeting only specific stakeholders involved. Often times feature toggling can be used to avoid long-living feature branches. A given feature can be integrated and released multiple times without being visible to the general public. This reduces the risk of having infrequent, big, impactful and hence potentially breaking releases for customers. The state of a feature toggle can be bound to any context of a given user. Differentiations can be made for different clients, locations, staff, roles, or any type of user grouping. This allows a feature to be validated quicker and safer by controlling the impact of a features availability. For example, we can start making features available to a select group, allowing them to participate in testing and provide feedback, before sharing it with everyone else. A/B testing (bucket tests or split-run testing) is a controlled experiment with two (or also more) variants, referred to as A and B. The goal of A/B testing is to identify changes which maximise an aimed for outcome. This outcome can be any success metric such as conversion rate or general interaction with a feature. Feature toggles allow for this experimentation within the UI by having multiple variations (not just on or off) represented in a toggle. An example would be two variations of a signup form which are triggered by a multivariate toggle targeting different groups by e.g. location. The scope of the variation can differ from button colours to swapping an entire section of a page. Lets leave the theory behind and dive into code and some examples. We decided to use LaunchDarkly as a service for our feature toggles with minimal effort. It supports multiple projects and environments while empowering us to target users based on information we provide. As the integration within our React application we chose a library called flopflip which offers a set of Higher Order Components (HoCs) and other helpers to maintain the state of all feature toggles. In our root component we simply configure flopflip by wrapping our app component This behind the scenes connects to LaunchDarkly, fetches all toggles and instruments any components further down the component tree of our application. Now we are ready to toggle any component in our application. The easiest way to toggle a feature is to wrap the component within the ToggleFeature component Here we also specify a component to be rendered in case the feature is disabled. This helps to mitigate layout issues or just generally helps to communicate that a feature will be there in the future for the excluded target group. Another more advanced component offered by flopflip is the branchOnFeatureToggle HoC. It enhances any components in our application by component composition This pattern nicely integrates with any other HoC by composing them all together. A real world example would be to compose connecting to our store while adding routing and internationalisation and a feature toggle When working with the above-mentioned multivariate flags the branchOnFeatureToggle HoC offers to pass an optional variation key which determines which variate of a feature this component represents. The component will then only render when the user is targeted for that specific variation. Whenever even more fined grained control within a component is needed to trigger custom logic and not just conditionally render, the injectFeatureToggle HoC allows for just injecting the demanded feature toggle’s value onto the props of a given component. In addition, the injectFeatureToggle HoC also has a sibling component in injectFeatureToggles which allows to make decisions within a component based on multiple feature toggles. The integration of flopflip with the js-client of LaunchDarkly comes with the power of real-time updates of flag values within the frontend. As a result, merely turning off a feature or changing its targeting will automatically live-update any React components attached to it within a deployed and running application without refreshing the browser. Lastly, flopflip offers a non-Redux integration in flopflip/react-broadcast with the same API and components. Switching between both is nothing more than changing imports from flopflip/react-redux to flopflip/react-broadcast . We like to share some tips and tricks we learned in the process of dipping our toes in feature toggling parts of our frontend. Feature toggles should be short lived and be removed from the code base as soon as possible. Maintaining a code base with a lot long lived feature toggles makes predictability of the software harder. After all, problems or even bugs within the program can be a result of different combinations of flags. Creating an engineering process around feature toggles and their lifecycle can help to manage their usefulness and avoid dead code within the application. This includes keeping tasks to remove feature flags, a dashboard to view all available flags and the habit of testing the UI with different plausible combinations of available flags. As feature flags are often integrated through another independent system, either self-written or a service, it is inevitable to provide fallback behaviour in case of that system not being available. This can be achieved by a sane default for every flag. This ensures that certain elements of the interface do not accidentally show up or that a basic view is always present even with the flag service or provider being down. This is quite easy to achieve using flopflip by just passing another prop called defaultFlags when configuring the library. Furthermore, the initialisation of an adapter can be deferred by controlling the so called deferAdapterConfiguration prop on ConfigureFlopflip which defaults to false triggering the immediate initialisation respectively. Finally, using feature toggles should not be used as an excuse to develop big features behind them. Trading a long living branch for a long living feature toggle will likely not provide as much customer value as possible. Toggled features should still be split into small and independent parts, some of which might not need toggling and can provide customer value alone. Generally, small bits can be iterated more easily upon with little assumptions being made beforehand. Customer feedback can then drive further improvements and the cost of failure remains low until the feature has been collaboratively developed and the toggle is removed. This feedback focussed culture needs to be established and cultivated among all parties involved and time for iteration needs to be reserved and communicated. Feature toggling is a powerful technique not only for backend applications to for instance control server load and feature rollout. It allows features within a frontend being integrated often and early already during development. It reduces risk of big releases and embraces customer feedback. However, as anything it is not a silver bullet and requires discipline and lightweight processes across all parts of the organisation. Knowing when and how to apply feature toggles is crucial. We definitely recommend to consider adopting feature toggling in your team, especially with powerful tools like LaunchDarkly together with flopflip . Looking under the hood of the commercetools platform 315 JavaScript React Feature Toggles Frontend Engineering Culture 315 claps 315 Written by Looking under the hood of the commercetools platform Written by Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-10"},
{"website": "CommerceTools", "title": "how to extend commerce apis", "author": ["Kelly Goetsch"], "link": "https://techblog.commercetools.com/how-to-extend-commerce-apis-f6b3b59da9e7", "abstract": "About We're hiring Your business is inherently unique and no software vendor will be able to offer you APIs that exactly match what you need them to do. As a result, you’ll have to extend the APIs you consume. Common extensions in the eCommerce space include: Sending notifications when an event has occurred, like sending an email when order has been shipped Capturing additional properties on objects, like capturing a customer’s shoe size on registration Validating data, like checking user-submitted data for SQL injection attacks Performing real-time data checks, like making sure inventory is available during checkout Adjusting the behavior of the API, like changing how prices are calculated In this article, I’ll explain the three different approaches to extending APIs, highlighting which approach is best for which type of extensions. If you were consuming a legacy commerce platform, you’d essentially be getting two things: A framework A bunch of libraries The framework, platform, or whatever you want to call it often includes some type of extensibility mechanism, allowing you to plug your custom code inside the framework. This is often called Inversion of Control . The libraries are self-explanatory — JAR files, NPM packages, or whatever your programming language offers. These are immutable, pre-compiled pieces of functionality. Libraries are pretty similar APIs, with the only difference being how the functionality is consumed. With a library, you’re embedding the vendor’s code in your application. With an API, you’re still embedding the vendor’s code in your application but rather than executing locally, it’s executing somewhere else. The sage Martin Fowler draws the distinction between frameworks and libraries as follows : Inversion of Control is a key part of what makes a framework different to a library. A library is essentially a set of functions that you can call, these days usually organized into classes. Each call does some work and returns control to the client. A framework embodies some abstract design, with more behavior built in. In order to use it you need to insert your behavior into various places in the framework either by subclassing or by plugging in your own classes. The framework’s code then calls your code at these points. If you’re buying a legacy commerce platform, your extension model is a function of the framework your vendor has defined. It’s extremely prescriptive — you follow your vendor’s documentation. As I’ve written about before , commerce platforms are no longer just something you deploy off to the side of your business. Commerce is your business. Small, vertical teams are building and exposing granular pieces of functionality to the rest of your business, often as microservices. In this model, there is no longer a single packaged commerce solution providing both the framework and the libraries, with a vendor telling you how to extend out of the box functionality. Let’s explore three different approaches to extending API-based commerce platforms. Many customizations are simply a matter of collecting additional attributes or defining custom objects. If you sell shoes, you’ll want to capture the shoe size of your customer upon registration. If you sell auto parts, you’ll want to capture the make/model/year of the customer’s cars. These are all fairly standard requirements that any API-based commerce platform should be able to easily support. At commercetools, we offer the ability to add attributes to existing objects. Check out our comprehensive tutorial for further guidance on how to add attributes to objects. We also allow you to define custom objects and interact with them through APIs. The objects are stored in our datastore in our platform. Many customization use cases can be solved through the use of events. An event is basically a message with a payload, often a JSON or XML-based representation of an object, like an order or a customer profile. What differentiates an event from a message is volume. Traditionally, messaging was strictly limited to passing important bits of data (orders, customer profiles, etc) around between systems. Messaging often used heavyweight protocols like JMS and relied on expensive commercial products. Eventing is a central characteristic of modern software development, especially microservices-based development. Everything is represented as an event. Lines in log files, small changes to orders/customer profiles/products, container instantiations, API calls, etc are all represented as unique events. It’s not uncommon to have millions or tens of millions of events per second in a microservice-based application. An event is often produced for any activity or change of any kind. At commercetools, we offer the ability to subscribe to the internal events that our individual microservices produce. You can push those events out to IronMQ , AWS SQS , AWS SNS , and Azure Service Bus . Once an event is on a topic, it must be passed to some custom code that can process it. Here, you have two options. The first option is to write a small application. Using a small framework like Spring Boot, Play or Node.js, you can very quickly write a small application whose sole responsibility is to pull events from a topic and do something. The “something” may be connecting to your back-end CRM system and updating a customer record. It may be sending an email to a customer. It may be charging a credit card. While these applications can be easily built, they must be maintained over years or even decades. The development framework you use will need to be upgraded. You’ll inevitibly have to upgrade your continuous delivery pipeline. Docker will continue to change as it matures. Maintaining applications is difficult. The second option is to use serverless. AWS Lambda , Google Cloud Functions , Azure Functions , and others allow you to essentially route specific types of events back to arbitrary functions/methods. Let’s take an example. A fairly routine requirement is to send an email to the customer when the order has been successfully sent to the OMS. You’d start by defining your AWS Lambda function: Then you’d write your code: Then, you’d bind your function to your topic, so that your code is executed whenever a message is published to that topic. You’d follow a similar approach with the other cloud vendors and their serverless offerings. Serverless works great because it’s simple and because it allows different pieces of the application to be changed and deployed at different times. You can change your order confirmation email template without re-deploying your entire monolithic application, which is pretty revolutionary for enterprise-level commerce. To summarize, it’s best to use event-based extensions for: Asynchronously synchronizing data between systems, often from your commerce platform to legacy back-end systems Sending notification emails Logging important data for audit purposes Computationally heavy activities, like generating product recommendations Rather than the provider of the API publishing events to a topic, your provider may additionally or instead offer webhooks. Webhooks are URLs that the vendor of the API posts data to. For example, you can often register webhooks for when an order is placed, a product is added, or a customer record is updated. In the case you register a webhook when an order is placed, the vendor of the API will post the entire order and maybe the customer’s profile to the URL that you define. It’s essentially the same model as events, with the following exceptions: HTTP requests are often made synchronously, whereas events are often posted asynchronously Rather than pulling messages from a topic, you have to provide your vendor with a URL to post data to An event-based model allows any number of consumers to pull the message from the topic, whereas you’re often to define just one webhook URL The vendor has to explicitly provide hook points. This often results in fewer hooks being defined. Events are just emitted from the application One advantage of webhooks is that it is theoretically possible to post data to back-end systems without having to write an intermediary application or serverless function. The problem is that these back-end systems have different authentication and authorization schemes. Sometimes they require VPNs. It’s hard to do a direct post from your API to another system without that intermediary layer. Going back to the previous example of using events, you can also define URLs that trigger serverless functions. This is an example of how you’d do it with AWS Lambda and AWS API Gateaway: In this example, you’d register https://xatp7l47qh.execute-api.us-west-2.amazonaws.com/prod/SendOrderConfirmationEmail as the webhook URL for the “placeOrderWebhookURL” property or whatever your vendor defines. At commercetools, we use events rather than webhooks. Our own Christoph Neijenhuis goes into detail at https://techblog.commercetools.com/webhooks-the-devil-in-the-details-ca7f7982c24f . While events and webhooks solve a wide range of use cases, there are a class of extensions that are inherently synchronous. Examples include: Validating data, like checking user-submitted data for SQL injection attacks. Performing real-time data checks, like making sure inventory is available during checkout Changing how the API works, like changing how prices are calculated All of these examples require synchronously executing custom code before or after an API call is made. The process for this flavor of extensions is exactly the same as you’re used to, at least conceptually. Let’s take inventory as an example. Let’s say you want to perform a real-time inventory check when inventory dips below 100 units. With a traditional commerce platform, you’d use their Inversion of Control (IoC) framework to extend the out of the box inventory code. Out of the box, “InventoryService” would normally return an instance of com.bigcorp.inventory.InventoryService or whatever the out of the box class is. You’d then create com.yourcorp.inventory.CustomInventoryService with a method as follows: Now, “InventoryService” resolves back to instantiations of com.yourcorp.inventory.CustomInventoryService. Simple. The difference is that APIs-based commerce platforms don’t have an IoC framework. You’re consuming APIs from your commerce platform vendor, from 3rd party vendors (payment, tax, product recommendations, etc), and from custom applications/microservices that you build in-house. As we discussed earlier, there is no more single platform — it’s just a bunch of APIs from different sources. To perform this simple extension you’d start by using your API vendor’s SDK. Then, pick a small framework like Spring Boot, Play or Node.js and build a standalone application that queries the API using your vendor’s SDK: This example is based on Spring Boot. Once you’ve defined your application, deploy it behind your API gateway. Clients would then query inventory by accessing https://api.yourcompany.com/InventoryService and passing productId and variantId as HTTP GET arguments. Using these three basic approaches, you can implement just about any requirement you can think of. And if you can’t, just build a brand new API backed by a new microservice to accomplish your requirement. Looking under the hood of the commercetools platform 22 Thanks to Philipp Sporrer , Nikolaus Kühn , Martin Möllmann , and Valentin Sauer . AWS Lambda Commerce Ecommerce Software API 22 claps 22 Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-30"},
{"website": "CommerceTools", "title": "10 takeaways from css js react and graphql europe conferences", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/10-takeaways-from-css-js-react-and-graphql-europe-conferences-d1974d922bdf", "abstract": "About We're hiring In the last couple of weeks, we had the privilege to take part in 4 amazing conferences: CSS Conf EU , JS Conf EU , React Europe and GraphQL Europe . It’s been quite a long journey and an amazing experience in many different ways. In this post we would like to share our thoughts and our takeaways. Out of the three conferences, CSS/JS Conf was the largest one. The location was astonishing and boy, such delicious food 🌮🍜. The audience gathered from all over the world, with lots of attendees and speakers travelling all they way from e.g. US and Australia. For many of them this was considered one of the most important conferences of the year . Nonetheless, React and GraphQL Europe were as just as good, considering the smaller size and locations. What struck us the most across all conferences was first and foremost the people. I guess we kind of knew it before, but really: we have such a great community! We had plenty of interesting conversations with many different people, from different countries, with different backgrounds. And everyone was really nice and helpful and keen to share their knowledge. This was kind of a recurring topic across all conferences. Optimizing your website/application, does bring value and a much better experience to users. Time is money 💸 It relates to “performance” as mentioned earlier and it’s an important architectural decision of many applications. Rollup is for libraries, Webpack for apps ( Here’s a very good article about “ Predictable long term caching with Webpack ”) Also known as the “ Pareto Principle ”, it’s a principle formulating an “80/20 rule” that can be applied to many different fields (science, economics, software, sports, etc.). We can apply this to our everyday job in which we try to focus on delivering e.g. a feature with 20% effort while aiming for an 80% result instead of 100%. Having this in mind will effectively speed up your delivery time. (video is not up yet) Again, somehow related to the aforementioned, it’s an interesting concept to have in mind when developing e.g. new features, a library, etc. Simplicity above all else. Simplicity leads to popularity. Popularity leads to contributors who will fix the missing pieces. (video is not up yet) Obviously this is a crucial part of development lifecycle and of a product itself. How do you know if the feature XYZ is used? Do you know which parts of it are used the most? How many people are using it? All those questions are not only important to measure the performance of a feature but also to determine how to evolve your product (e.g. what can be deprecated). Those who are using GraphQL with the Apollo Client, can and should try out Apollo Optics . It is now free for 10.000 requests per month 🎉. (video is not up yet) Writing more Higher-Order Components and composing them together can really clean up your code. Just look at this example taken from Nik Graf ’s talk: One of the themes of CSS Conf was CSS-in-JS ( of course 😜). Mark Dalgleish and Glen Maddern gave good tips about how we should separate our concerns with often overseen benefits of CSS-in-JS . Mark also published a blog post version of his talk if you don't want to wait for the video to be uploaded 👀. Lin Clark gave two amazing talks about WebAssembly . One explaining what it is and how it works: And another one about what it means for React: WebAssembly it is the future. But it will take us a couple more years to really benefit from it. Some things that still need to be implemented in browsers: Threads with shared memory, so that JavaScript and WebAssembly can share data and interact with each other Direct DOM access for WebAssembly (right now WebAssembly has no DOM access at all) Integration with the browser’s GC This paradigm was mentioned in a couple of talks at GraphQL Europe and it is successfully used at companies like GitHub and Graphcool . Schema Driven Development means having people that write and consume an API sit together in one room and define the schema for a new feature. This ensures that all client needs are met and that the knowledge is shared across different teams, including frontend, backend, native. How is this schema defined? Using the GraphQL IDL . Does it require more effort? No, because instead of having all iOS developers sitting in one room and all backend developers sitting in the other room, you have a couple of developers from each group sitting together. (videos are not up yet) The conferences were really interesting for a variety of reasons. The most enjoyable part and key for us was to have good discussions with a lot of different people and to come back home with new ideas and inspirations . See you all at the next conferences! 🎉 https://medium.freecodecamp.com/what-ive-learned-from-react-europe-2017-c433468890d6 https://hackernoon.com/react-europe-2017-551961487403 https://medium.com/@miduga/jsconf-eu-2017-the-awesome-the-bug-and-the-ugly-6d4267701566 https://medium.com/@kotzendekrabbe/what-an-a-m-a-z-i-n-g-weekend-65ad099453f7 Looking under the hood of the commercetools platform 29 Thanks to Philipp Sporrer , Luis Gomes , and Tobias Deekens . GraphQL React JavaScript Conference Community 29 claps 29 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-02"},
{"website": "CommerceTools", "title": "creating an aem commerce connector", "author": ["commercetools"], "link": "https://techblog.commercetools.com/creating-an-aem-commerce-connector-99f17ab72b3b", "abstract": "About We're hiring Adobe Experience Manager (AEM) is one of the leading Content Management Systems (CMS) for the enterprise market today. Out of the box functionality includes Mobile Content Management(MCM), marketing campaigns, multilingual support, dynamic media and an interface so simple non-technical staff can easily implement changes(see their capabilities page for even more features ). AEM has a tremendous presence in the CMS space, as Forrester’s Q1 2017 report has outlined them as being the top in its field. Similarly, BuiltWith which tracks and analyzes CMS usage across the top 10,000 websites on the internet, lists AEM with roughly 8.9% of the marketplace (I’m including CQ as it is the predecessor to AEM). A CMS, in our view, has to be e-commerce ready because the competitive landscape requires it to be. That means that any client of any given CMS expects either an out of the box solution or an easy way to integrate an existing e-commerce solution. In AEM’s case some parts of e-commerce functionality are out of the box. With AEM’s existing functionality paired with an e-commerce SaaS platform anyone can create a best-in-breed hybrid solution that leverages both platforms in the most performant way possible. For instance, both AEM and commercetools provide a CDN for your assets which allows your content-rich pages to load lightning fast. With AEM you can make the entire page shoppable, that means text, images and video can all be customized so that a single click on the content immediately adds it to a shopping cart. AEM is competing in an environment that includes Hippo , Acquia and Magnolia . Where AEM shines, in our opinion, is in its flexibility thanks to its technology stack. One of the strongest areas of AEM’s platform is its ability to be extended. Adobe is a huge contributor in the open source world including many Apache projects that AEM is built on top of. These open source projects are what allow AEM to thrive in the enterprise — your project is easily extended, manipulated and sculpted to fit your needs. From a development stand point, getting started with AEM is a bit overwhelming. There are many fundamental technologies AEM is built on top of, many of which Adobe maintains or supports through the Apache foundation. That said, getting from couch to connector is not as simple as we originally anticipated. That is mainly because of two domain-specific technologies we at commercetools didn’t have experience with. Apache Felix is one such technology that AEM relies on which helps with OSGi related technologies. OSGi was new to us and while we read everything we could on the subject, the core concepts of it were foreign to our team. After many nights of reading and watching various videos, this infograph seems to be the best representation of its lifecycle. OSGi is important because when you start writing your AEM connector you actually compile it as an OSGi bundle and inject it at runtime [as a bundle]. This is nice feature because the AEM instance you’re running locally doesn’t need to be restarted as you develop. For instance, as you go through any of the AEM tutorials you’ll notice you’re running Apache Maven profile commands such as: These profiles are the way you recompile your code into OSGi bundles, check in changes into the running AEM instance and like magic, OSGi does what it does best, injects dynamic components. I think OSGi’s architecture describes this wonderfully: The OSGi technology is a set of specifications that define a dynamic component system for Java. These specifications enable a development model where applications are (dynamically) composed of many different (reusable) components. The OSGi specifications enable components to hide their implementations from other components while communicating through services, which are objects that are specifically shared between components. This surprisingly simple model has far reaching effects for almost any aspect of the software development process. As we continued to iterate on our connector, implementing more and more functionality, the benefits of OSGi started to show. We were able to checkin code quicker, verify changes then rinse and repeat. It wasn’t all easy though, while we have an existing JVM SDK it wasn’t built with OSGi in mind and because of the transient dependencies that our sdk relies on, it took a lot of man hours to get it running as an OSGi bundle. The second major technology that had to be wrestled with was Apache Jackrabbit Oak . We hadn’t run across JCR before so reviewing this project took some time. For instance, on the Concepts and Architecture Overview page there is a note at the top that says: TODO: Explain concepts: MVCC, Oak modules and plugins, key APIs, … Any time an overview page has a TODO you can assume you're going to be dealing with incomplete documentation. Dealing with incomplete documentation on a project that is new to everyone on the team is likely going to cause something I call 'getting-started-pain'. We did our best to review in detail JCR’s architecture which is showcased as such: JCR sits on top of Oak, which talks to to the NodeStore, who builds hierarchical trees to store in Tar files, MongoDB or a RDBMS. This was really confusing because AEM 6.2 doesn’t ship with MongoDB or a RDBMS and we didn’t see any Tar files when we booted up the Author instance. So where are properties, such as product data, being served from? Furthermore, when we edited a product in Author view, shut down our local AEM instance and then rebooted our changes were still there. We knew the data was being persisted but we didn’t know how or where. That’s when we learned about JCR and the CRXDE Lite console. CRXDE Lite is an application/console to browse the JCR object database. After digging through the tree and finding content related to Geometrixx and later for We.Retail we discovered the persistence layer we were after (Geometrixx and We.Retail are demo sites that ship with AEM). Here’s an example of the folder tree in CRXDE: When you traverse the JCR tree you start to understand more of the underlying components and configurations used in AEM. While we only focused on a commerce connector there are many ways to extend AEM through this configuration including content, mobile applications, forms and more. Apache Sling is the last major component of AEM that I want to mention. Sling is thankfully not as foreign to the team here as it’s a simple Java web framework that has 5 major points of interest: REST based web framework Content-driven, using a JCR content repository Powered by OSGi Scripting inside, multiple languages (JSP, server-side javascript, Scala, etc.) Apache Open Source project Some of those terms should look familiar, but most importantly it’s powered by OSGi and serves content through a JCR repository. We’re happy to be able to offer commerce functionality to AEM users, we hope it allows our customers another enterprise CMS solution while also providing Adobe customers with a great e-commerce solution. As mentioned earlier, the power of leveraging different systems for what they are great at is the heart of what we offer at commercetools and truly the heart of microservices . Thank you for reading and if you’re interested in more, please refer to our Adobe connector page . Looking under the hood of the commercetools platform 16 Web Development CMS Aem Adobe Experience Manager E-commerce 16 claps 16 Written by commercetools provides Commerce-as-a-Service. One Platform. One API. All Channels. Looking under the hood of the commercetools platform Written by commercetools provides Commerce-as-a-Service. One Platform. One API. All Channels. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-17"},
{"website": "CommerceTools", "title": "testing in react best practices tips and tricks", "author": ["Nicola Molinari"], "link": "https://techblog.commercetools.com/testing-in-react-best-practices-tips-and-tricks-577bb98845cd", "abstract": "About We're hiring Working in a UI environment is not that straightforward anymore, especially in the modern era dominated by complex front-end applications. And it’s not always fun. With the “arrival” of React, things changed a lot, especially in the way of thinking about UI in terms of components. But what about testing? Wait, testing JavaScript? Who does that? Well, that might have been the case 10 years ago, nowadays it’s a completely different story. As we all know testing such complex UIs and applications is very crucial. And stop with excuses such as “I had a tight deadline and couldn’t find the time for writing tests”. Writing tests is usually the first thing you should start with, even if you don’t strictly follow TDD. Enough with the jibber-jabber, let’s have a look at some tips and best practices we have found to be useful when testing UI / React components. Because at the end, testing should not be a pain or a burden, it’s a necessary piece of software development and you should feel comfortable as well as having fun doing it. First of all I’m going to use Jest as the test runner. Most of the things described in this article can easily be done with any other test runner, but some of them are possible because of Jest features. If you’re not using Jest, we strongly recommend to switch to it. It really steps up in terms of “ T esting e X perience” and it has a lot of awesome powerful features. I’m also going to assume that you use Enzyme as a React test library. I think this speaks for itself. At this point you’re ready to start writing tests. If you do TDD that’s what you do anyway to start with. It’s a good practice to “ describe ” your test steps and assertions, just like you do when you write a check list. It’s good for having a mental picture of what you want to test as well as for preparing the different test setups and simply to have a list to follow. You are going to do that using “ describes ” and “ its ” (or whatever the test runner of your choice provides). That’s good, but we can do a lot better. For example, the main describe “ Core::Buttons::LinkButton ” might be useful to “uniquely” identify this specific test in the test runner output. Fortunately with Jest (and probably with other test runners as well) this is not necessary anymore. Jest groups all tests by their filenames. If a test fails you will then get the “path” to the failed assertion, as well as a lot of useful information. That’s one of the things I love the most about Jest, they really did an amazing job. As you can see having targeted test assertions is really helpful. It not only makes the test easier to read but also easier to identify the specific assertion. So don’t be afraid of nesting different “ describes ” and to split your assertions across multiple “ it ” blocks. We found that is beneficial when each “it” block only contains one assertion. This allows to properly document each test using the “it” block instead of using the assertion message. What we also do is to group the main “ describes ” by some sort of context , which usually looks something like this: In general, you should structure your test steps by nesting them in a way that they are readable . This implies nesting “ describes” on many levels. Don’t be afraid of that, it’s perfectly fine to do it, even more so if we consider the point above of having one “ it ” block per assertion. This approach is called RDD (Readme Driven Development). “ describes ” are meant to explain conditions, whereas “ its ” are meant to explain the expected output. Look at this example: It’s very easy to read and to understand what’s going on. It also allows to have specific setups in each nested “ describe ” (e.g. rendered component props, mocked imports, etc) using “ beforeEach ”. Speaking about “ beforeEach” , it becomes really helpful when you are testing different states of your component. Look at this: prevent order-of-execution errors — be careful not to mix code sitting next to “ describes ” and “ its ” with “ beforeEach ” hooks. Because hooks are async, Jest will execute the synchronous code first instead of the “ beforeEach ”, as you would expect. prevent dependent tests — make sure that each “ it ” block can be run in isolation. The latest Jest version provides also an easy command in “watch” mode to run specific tests within a test suite. If use use iTerm, you can “cmd + click” on the file path in the stacktrace, in case the test fails, to jump into the file on the specific line. Go to “Preferences > Profiles > {profile} > Advanced > Semantic History” and provide (in case you are using Atom): You’ve got your test structure set up. You can now start testing your React component. If you are just getting started with testing in React you probably face one dilemma: how to render the component for testing it? Enzyme provides a “ shallow ” and a “ mount ” function. Which one should you use? This is really easy to answer: use “ shallow ” ! 99% of the time it’s what you want. There has to be a damn good reason to use “ mount ”. You could even see it as a code smell . Why should you not use “ mount ”? Because unit tests should be isolated. When testing a component we only want to test the logic of that specific component and that component only. If you use “ mount ” you are automatically exposed to the logic of all components in your render tree making it impossible to only test the component in question. Additionally, “ mount ” requires JSDOM and therefore slows down your tests dramatically. Even if you want to test a component in its mounted state you don’t need to use “ mount ”: This is more of a best practice we’ve adopted rather than a rule. Instead of passing each single prop to the component we are testing, we define a function that creates test props. Then you just spread the props when rendering the test component. This approach allows us to clearly see the common props that the component expects, it keeps the test setup small and is flexible enough to easily add / override those props for special cases. Writing unit tests means testing something in isolation . When we test React components, we test them in isolation using shallow rendering . We are not interested in what sits outside of the component we are testing, therefore we define mocks to abstract away the real implementation. Furthermore, you want to check that all your functions passed as props are being called throughout the tests. To do that it’s a good practice to always provide a spy for props that are functions. You can do a lot with spies. With Jest when you call “ jest.fn() ” you can pass an implementation of that function that does / returns whatever you need it to do. In the example below, the “ fetchData ” function provides a callback that is called when the data fetching is done. We can use “ jest.fn()” to simulate the success of the data fetching by executing the callback manually. This allows us to test whatever code runs as a result of a successful data fetching (e.g. show a notification, update the state, redirect to another route, etc). If you don’t use Jest, you should probably use sinon as your spy/mocking library. If you do use Jest, you have all of this out of the box. Additionally, Jest provides a really powerful module mocking system built-in. What about snapshot testing? Couldn’t we have simply tested all of the rendered components with a snapshot test? True, but that’s not always what you want. Snapshot testing is great and we should use it but we should do so with care and only when it makes sense. With great power comes great responsibility In general, I would say that we should be explicit on what elements are rendered or what props are passed to those elements — even if it’s a bit verbose. In the example below, we want to make sure that the “ Button ” is disabled if a certain limit is reached. If we explicitly test for it, we see how the output log helps us understanding the context (“ rendering > <Button> > when limit is reached > should disable the <Button> ”). If we were to test the entire component solely using a snapshot test , it would still fail (given the same circumstances) but you would lose the context related to the failing test (“ rendering > outputs correct tree ”). You don’t know anymore why the “ Button ” should be disabled. Nevertheless, in many simple test scenarios, it’s more than enough to just use snapshot testing. Snapshot testing is great when you want to validate the structure of something like a component or an object. Even better is to use them in combination with normal test cases: you get the benefits of snapshot testing as well as the fine-grained control of targeted assertions. lose the ability of doing TDD — well, you can’t have a snapshot of something that doesn’t exist yet lose the ability of doing RDD — the test file no longer describes what is important to be rendered in the component don’t know what the actual output “should be”, you only know what it “was” before. There are cases when the component your are testing renders a component that has a prop in the shape of a React element or a function that returns one. So how would you test those props? In the example above, the component renders “ Table ” or “ Button ”, but because of shallow rendering everything beyond that won’t get rendered. This is how you can solve it — it just requires a bit more configuration. You simply shallow render it as a normal react component. The same thing applies for the prop being a function, you just pass the arguments based on whatever it is that you want to test. You probably already heard about this: Function as Child components. If you don’t there are plenty of articles explaining what this is / does. Or you’ve seen this before but you didn’t know it was called like this. Let’s take a simple example: The “ Collapsible ” component basically manages the toggle state and returns a “ isOpen ” boolean flag to indicate the current state as well as a “ toggle ” function to, you know, update the state. Imagine wanting to test a “ CollapsiblePanel ” component which uses the “ Collapsible ” component. Since you’re testing the “ CollapsiblePanel ” component with shallow rendering, the child function of “ Collapsible ” won’t be executed. How do you test this then? This is how you can solve it: extract the child function into a function on the component instance, then test the function directly. Now all you have to do is to call the “ renderPanel ” function directly and test the returned value as a normal React component. Testing React components is not difficult. If combined with a test runner like Jest it provides a really powerful experience. Furthermore, if you follow some guidelines and have a proper structure, people in your team will write tests more consciously, resulting in higher quality, faster iterations, less bugs and more productivity. I hope the tips in this article will help you achieve that and improve your development time. Looking under the hood of the commercetools platform 1.4K 4 Thanks to Dominik Ferber . React JavaScript Testing Jest Tech 1.4K claps 1.4K 4 Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Written by Software Engineer @commercetools, Dad, Technology Enthusiast. I ❤️ building things. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-03"},
{"website": "CommerceTools", "title": "version or evolve apis why not both", "author": ["Kelly Goetsch"], "link": "https://techblog.commercetools.com/version-or-evolve-apis-why-not-both-a19995a64d21", "abstract": "About We're hiring With APIs now clearly now at the center of commerce , how do the vendors that produce and host those APIs evolve them over time? We at commercetools are constantly pushing updates to production, often multiple times a day. We offer a library of >300 (and growing every day!) commerce REST APIs as a service . For example, we offer a shopping cart , a discount engine , products , and more — all as REST endpoints that we host for our customers. Our customers simply get URLs or native clients , if desired. APIs are never static. Continual updates are such an important part of our company culture that new developers end their first day by pushing an update to production. If you want APIs that never change, there are always 20+ year old on premises platforms to choose from. Traditionally, commerce applications have forced all clients to use the same API and implementation versions. In practice, this meant the monthly or quarterly release to production would require the clients to be updated at the same time, leading to a long weekend for the ops team. When the only client was a website, this was just fine. When it was mobile and web, it became more difficult because an update to the core platform meant you had to deploy both mobile and web at the same time. But in today’s omnichannel world, there could be dozens of clients, each with their own release cycles. It is not possible to get dozens of clients to push new versions live at the same time. Each client must evolve independently, with its own release cycle. There are two basic approaches that producers of APIs can take to APIs: evolve and/or version. Let’s start with evolvable APIs. Many APIs simply do not change that much, especially if they’re well designed up front by people who understand the domain extremely well. For example, most of the external tax calculators have static APIs. Have a quick look at Avalara’s tax API as an example. The underlying tax rates and sometimes the formulas change, but the actual API you call is fairly static. The response you get back is also fairly static. The U.S. could adopt a VAT-style tax system and the APIs still wouldn’t change. Most APIs you interact with on a day to day basis are like this. Inevitably, APIs need to evolve — but not necessarily change. Let’s say you’re building a customer profile API that allow simple CRUD operations. Let’s pretend this is the JSON object your API accepts to register a new customer. Now let’s say that your business users want to start capture your customers’ shoe sizes, so they can be targeted with better product offers. Your JSON object would now look like this: This is an evolution of your API, which should be easily supported without versioning the API. If the client doesn’t specify the shoeSize parameter, the server shouldn’t break. This goes back to Postel’s law , which states that you should be “ liberal in what you accept and conservative in what you send. ” When applied to APIs, Postel’s law basically means you shouldn’t do strict serializations/deserializations. Instead, your code should be tolerant of additional properties. If shoeSize suddenly appears as an attribute, it shouldn’t break your code. Your code should just ignore it. For example, the serializer we use allows for the following annotation: If you adopt a strict approach to serialization, any difference in the client and server is going to break the client: The approach of having evolvable APIs goes back to Bertrand Meyer’s open/closed principle , which he documented in his Object Oriented Software Construction book (1988). In it he said that software entities (especially APIs) should be “open for extension but closed for modification.” He went on to further say: A module will be said to be open if it is still available for extension. For example, it should be possible to add fields to the data structures it contains, or new elements to the set of functions it performs. A module will be said to be closed if it is available for use by other modules. This assumes that the module has been given a well-defined, stable description (the interface in the sense of information hiding). The majority of APIs you have will fall into this category. Simply add properties where you can and don’t break existing functionality. The major advantage of this approach is that the APIs remain fairly static, allowing clients to code to them more easily. It’s one less dimension for developers to care about. The supplier of the API only has one version of the codebase to support in production at any given time, dramatically simplifying bug fixing, logging, monitoring, etc. The disadvantage of this approach is that the APIs are fairly locked in from the start. Vendors who solely adopt this approach lose the flexibility to radically change the APIs. Which is perfectly acceptable in many cases. For APIs that change more radically, versioning is the preferred approach. With versioning, the provider of the APIs deploys more than one major version of an API to the same environment at the same time. For example, versions 1, 2 and, and 3 of the pricing API may be live in production all at the same time. All versions can be serving traffic concurrently. While there are many flavors of versioning, a common approach is to guarantee API compatibility at the major version level but continually push minor updates. For example, clients could code to version 1 of an API. The vendor responsible for the implementation of the API can then publish deploy versions 1.1, 1.2, 1.3, and beyond over time to fix bugs and implement new features that don’t break the published API. Later, that team can publish version 2, which breaks API compatibility with version 1. Clients (e.g., point of sale, web, mobile, and kiosk) can request a specific major version of an API when making an HTTP request. By default, they should get the latest minor release of a major version. This is often done through a URL (e.g., /inventory/v2/ or /inventory?version=v2) but can also be done through HTTP request headers. This is great for vendors who are rapidly innovating. It allows them to release minimum viable products. When enough is learned, they can fork the codebase and then offer the old version 1 and have an entirely new breaking API as version 2. The vendor isn’t “locked in” to a specific API, as is the issue with evolvable APIs. The major challenge that SaaS vendors have with versioning is the persistent data. Here, there are essentially two approaches: You can have one one datastore per major API version or you can have one datastore per environment. If you have one datastore per major API version, then you need to migrate or continually synchronize the data between major versions. If your client was using version 1 of the order API and then you start using 2, you need to physically move or synchronize the data from version 1 to version 2. You can’t just seamlessly switch over to version 2, for example. This is hard to do when you have multiple clients because it requires that you cut all of your potentially dozens of clients over to the new version of the API at the same time. Facebook has gone so far as to offer an upgrade utility , to help developers transition from one version to another. If you have one datastore per environment, with all API versions hitting the same datastore, you have the problem of “evolving” the objects. Your point of sale system could write an order object using version 1 of the API and 5 seconds later your iOS application could try to amend that order using version 2 of the same API. Any API version can write an object and any API version can update that order at any time. This is by far the most common approach but it’s hard to do. Given all of the challenges with versioning, why do vendors choose to do it? They do it because it offers more freedom to innovate, especially in a fast-changing environment. Evolvable APIs are easier to support but both the clients and the vendors offering the APIs tend to get locked in over time, slowing the pace of change. We at commercetools have favored evolvable APIs, though we are not opposed to versioning in the future. To date, we have never forked a single of our 300 APIs. From an API standpoint at least, eCommerce is a well known problem domain. A shopping cart is a shopping cart. Most of the changes to our APIs tend to be additional fields rather than fundamental differences in functionality that would necessitate the introduction of an entirely new version of an existing API. We do have a deprecation policy for specific fields, but it’s used very rarely. Our company is staffed with eCommerce industry veterans who understood the nuances of modeling eCommerce APIs. It’s a different problem than, say Amazon having to model APIs for Alexa, or for Google modeling its maps API in the early 2000’s. We’re certainly open to versioning but again we just haven’t had the need to. Our formal policy states: No values or fields will be changed or deleted in the same API Version Every change within the same API version only includes additional information: — new fields and values within an existing object — new messages for the Messages API — new API endpoints — required fields becoming optional Evolvable APIs makes it easy to write clients for our APIs. As a developer, you don’t have to worry about versions. On a daily basis, we push out new incremental versions of our APIs. As a consumer of the APIs, you just get new functionality. If you liked this post, we have plenty more on https://techblog.commercetools.com . Have a look! Looking under the hood of the commercetools platform 32 2 Thanks to Nikolaus Kühn and Philipp Sporrer . API Web Development Microservices Restful Software Development 32 claps 32 2 Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Written by Chief Product Officer, commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-26"},
{"website": "CommerceTools", "title": "webhooks the devil in the details", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/webhooks-the-devil-in-the-details-ca7f7982c24f", "abstract": "About We're hiring Webhooks seem to be such a simple solution for callbacks: Send an HTTP request. Receive an HTTP request, do something, and return 200. Seems pretty straightforward, right? Once we leave the happy path, we see that it’s not so simple — neither for the sender, nor for the receiver. One can ignore many of these issues if the cost of failure is low, e.g. if a CI build is not triggered, or a message was not posted into a Slack channel. However, when looking at commerce use cases for callbacks, many of these have a high cost of failure, e.g. if your food is not delivered within the promised time, or if a refund is not triggered. Let’s have a closer look at the details and at what can go wrong: What should the sender do if it can’t deliver the Webhook because the destination server or the network isn’t available? Turns out: It depends. If the callback is supposed to trigger a refund on your credit card, it should be retried ad nauseam. However, if the callback is supposed to trigger a push notification with “your food delivery is on the way”, it should be dropped after a couple of minutes as the customer has received his meal already. Bugs hide everywhere — especially in software. A bug in the receiver may only be triggered by a specific payload. How can the developers of the receiver retrieve this payload? A nice sender would offer a dead letter queue or similar that can be accessed via a UI and/or an API. How many HTTP requests can be send in parallel before overloading the receiver? 5? 50? 500? Workloads in commerce are often spiky. Your frontend needs to scale, but for many of the background processes, a viable strategy is to be asynchronous and buffer tasks. Webhooks are not a good buffer, because the sender pushes to the receiver. If the receiver can pull the work, he can consume it at his own pace. As a workaround, the receiver can refuse to accept Webhooks under high load, and trust the sender to retry the delivery. However, the receiver would refuse incoming requests randomly. If the sender doesn’t maintain a global FIFO queue (which is quite hard to scale), some tasks may be stuck for hours while others get processed on the first try. Even if the sender has addressed all of the above problems, it is still vital that the system can be monitored and that automated alerts can be send out when deliveries fail, a message is added to the dead letter queue or the buffer size grows. This should obviously nicely integrate with the system the (Dev)Ops are using already for monitoring the rest of the infrastructure. Fortunately, there is already a lot of software out there that is built to solve all of these problems — Message Queues! They allow you to define retry policies, dead letter queues, and are often integrated with your favorite monitoring software. They can be consumed via a pull-API, and many can be integrated with auto-scaling workers/lambdas. With a Message Queue, the delivery is clearly separated from the processing of the message. It allows the sender to hand it off with a much higher chance of success, because neither bugs nor performance issues in the receiver will have an impact on the delivery. On the other hand, the receiver has control over the messages. Not only can the policies be defined, it’s also possible to manually manage the state of the queue. Often this is possible via a UI, which may also allow you to peek at messages. If you are integrating with a system that only offers Webhooks, fear not: It is quite easy to just accept the message and put it into the Message Queue of your choice. A few Message Queues have an interface to act as a webhook-receiver themselves, one example being IronMQ . I’d argue that it’s better to use a “serverless” worker (i.e. AWS Lambda, Iron Worker or Google Function) to process messages, but most cloud-based Message Queues do support pushing messages via Webhooks ( IronMQ and Pub/Sub do this natively, AWS has SNS ). The advantage over “plain” Webhooks is that you are still able to configure retry policies, a dead letter queue and monitor the system. When designing our API and platform, we’re giving our best to not only make it easy to learn and develop for, but also easy to maintain once an application is in production. When we started designing our callback API , we first looked deeply at Webhooks, because everybody knows them, and they are, hands-down, the easiest thing to implement. But when we started designing for maintainability in a production environment, we found ourselves re-inventing the wheel and building yet another message queue. We decided to take a step back. While it surely would have been fun to build a Message Queue, our mission is first and foremost to design a great commerce platform! Therefore, we decided to let our callback API stand on the shoulder of giants: Messages are put into a Message Queue of your choice. We currently support SQS on AWS, Pub/Sub on Google Cloud and IronMQ of iron.io. Just like with programming languages, you can choose the queue that fits your needs best. Webhooks allow you to quickly glue two systems together. However, for business-critical callbacks it is necessary to be in control of edge cases and monitor the health of the system. An application consuming Webhooks can gain scalability and enable monitoring by forwarding the calls to a Message Queue. An application sending Webhooks can address these concerns, but may end up building yet another Message Queue. Looking under the hood of the commercetools platform 133 2 Thanks to Amadeus Magrabi , Oleg Ilyenko , Nicola Molinari , and Dominik Ferber . AWS Webhook Message Queue Sqs Google Cloud Platform 133 claps 133 2 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-07"},
{"website": "CommerceTools", "title": "customizing a baas api serverless", "author": ["Christoph Neijenhuis"], "link": "https://techblog.commercetools.com/customizing-a-baas-api-serverless-bb80d08fd126", "abstract": "About We're hiring A hosted Backend-as-a-Service can be a great time- and cost-saver, because one doesn’t have to do Ops. However, unlike in a self-hosted scenario, we can’t edit or extend the code. If we need to write custom code, we don’t want to self-host that code either. In this blogpost we want to explore how to write a wrapper with custom code around the API without doing Ops. We’ll use the commercetools API and AWS Lambda functions. The pattern is applicable for other BaaS APIs and Serverless technologies, too. We use APIs to super-charge our development, because those APIs implement a lot of the business functionality we need. But usually they don’t cover 100% of what we need. Often multiple clients use a Backend-as-a-Service API. For example a web server, an Android app, an iOS app and a chatbot can use the commercetools API. We want to implement our business functionality independently of those clients. Doing so would be a lot of overhead, and we can’t force all clients, such as mobile apps, to update. Some examples for applicable use cases we see at commercetools: Add data before saving. E.g. select the closest warehouse to ship the product from. We’ll use this example for the blog post. Validate data before saving. E.g. a certain product category like alcoholic beverages can only be purchased in limited quantity. Replace data with a custom calculation. E.g. a different service calculates prices, which is also used in physical stores. We want to inject arbitrary pre- and postprocessing code into the API calls. However, API client applications shouldn’t notice. At commercetools, we offer SDKs and a template shop which should continue to work. Additionally, we don’t want to update the API clients when we change the business code. The pattern applies for slight modifications and additions to an existing API, not additional APIs or the replacement of an API. We only discuss direct responses to API calls. There are different patterns for asynchronous events . We use the AWS API Gateway only for HTTP routing. In our example, we want to intercept updates to a cart and send those to the Lambda function. We want to forward all other calls to the commercetools API. The Lambda function either works with the request or the response. It can validate the request, and if the validation fails return an error in the API error format, or modify the request, e.g. by adding a field. The Lambda function can also work with the response, and may do another API call based on it. It will return the final API response to the caller. E.g. it can recalculate the discounts and total price for a modified cart, save the result via an API call, and return that final response. In our example, we want to modify the request and add the closest warehouse to ship the product from. The commercetools SDKs allow us to specify the base URL of the API (e.g. api.commercetools.co ). They construct the path (e.g. /<project>/carts/<id> ) for each request. The API Gateway URLs have the format {api_id}.execute-api.{region}.amazonaws.com/{stage_name}/ . If stage_name is the same as the name for our project, the SDKs will construct a correct URL if we specify {api_id}.execute-api.{region}.amazonaws.com as the base URL. Otherwise, we’d have to setup a new URL via Route 53. For the API forwarding, we save the project name as a Stage Variable. We need to setup two rules: One for the base path, and one for any other URL (a proxy resource, in AWS terminology). We intercept POST requests to /carts/id and proxy them to our Lambda function. The AWS Swagger export is available here . We want to forward the request — sometimes with a small change — to the commercetools API, and return the response. We’ll need to forward the HTTP method, path, body and headers (which includes the authorization). The event object contains those. The Host header is overwritten by the API Gateway, but the commercetools API will reject requests with invalid Host headers. Now we need to modify requests that add something to the cart without a warehouse. We look for addLineItem update actions that don’t have the supply channel (=the warehouse) set. To keep the code simple, we add a default warehouse. The rest of the request stays untouched. Finally, we return the response (also any error response) to the client. We have to callback the API Gateway with the HTTP status, headers and the body. The full gist is available here . Our Sunrise template shop is available as a web and a mobile app. We deploy the web app to Heroku in the EU. Instead of the default URL api.commercetools.co we configure the API Gateway ourid123.execute-api.eu-west-1.amazonaws.com . We do the same for the mobile app. Both frontends keep working without further changes! Whenever we put a product into the cart, the warehouse is added by the Lambda function. How does this architecture influence the overall performance as seen by the client? To keep network latency low, we’re using the same AWS datacenter for the web frontend and the Gateway/Lambda. We measured both the overhead of HTTP routing only, and hitting the Lambda function. For the HTTP routing, we found that the API Gateway has a high deviation. While some requests have an overhead of only 20ms, some add more than 100ms. On average, we saw an overhead of 26ms. Updating the cart added an average overhead of 35ms, but was more consistent in its response times. The 35ms includes both the API Gateway and the Lambda function, which itself adds an overhead of less than 20ms. In our opinion, the overhead is acceptable. We actually think the Lambda function performs quite well. We see more room for improvement on the API Gateway side, especially because a typical frontend will use it way more often. We’ve shown how to add business functionality to a BaaS API without modifying its code. Existing clients continued to work without any code changes. In the AWS ecosystem, the configuration and code is straightforward. The performance loss for API calls we don’t intercept has room for improvement, but is acceptable. Like the BaaS API, the Serverless implementation will scale and we don’t have to worry about other Ops tasks. Many thanks to Nikolaus Kühn for helping me research this blogpost! Looking under the hood of the commercetools platform 13 1 Thanks to Nikolaus Kühn and Nikola Mladenovic . AWS AWS Lambda Serverless API Cloud Computing 13 claps 13 1 Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Written by Tech Lead @ commercetools Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-20"},
{"website": "CommerceTools", "title": "notification event processing in the serverless world", "author": ["Nikola Mladenovic"], "link": "https://techblog.commercetools.com/notification-event-processing-in-the-serverless-world-6376c696fd85", "abstract": "About We're hiring It’s no secret that the future will be serverless. Computing is becoming more granular. From monolithic applications, to microservices, serverless is the natural evolution. It is a good and effective way to build scalable and cost efficient components, helping you decouple the code. Those components essentially run in event triggered stateless containers, and therefore represent a good solution for the notification processing. Handling events in a complex project can be a real challenge. In this blogpost, we are combining a BaaS (Backend-as-a-Service) with two different serverless platforms: AWS and Iron.io. In our example, we want to send a Push Notification when a reserved product is available for pickup in a store. The BaaS delivers event messages concerning the reservation, which are processed by an FaaS (Function-as-a-Service), and in our example passed onto another component. For those of you who have dealt with direct integration with the Apple Push Notification service (APNs) and Google / Firebase Cloud Messaging (GCM / FCM) in the past, know it can be a bit messy to keep it as a part of your core platform’s codebase. You have to manually handle queueing, and processing events which can trigger a notification. Events usually carry additional information depending on their type. The service receiving them has to map that information into an appropriate platform specific notification payload. Finally, based on the user’s device type, the payload is delivered to the notification server. Definitely sounds like a service which should be separated from the complex system using it. And it is. All major serverless app platforms place notifications on top of their solutions list. This sounds like a difficult question. But is it really? The answer mostly depends on whether you are building some mission critical component, and have to worry about reliability, where every uptime permille counts. It also depends on whether you have a specific language / framework preference, or are open to adjusting your tech stack to what the ecosystem supports. When it comes down to push notifications — they’re by definition not mission critical. We have decided to try out a couple of solutions which integrate well with commercetools , our core platform behind the mobile application in this example. As a test example, we used the scenario where our mobile shop customer reserves certain product for the in-store pickup. For the sake of simplicity, we are expecting the user to get the confirmation notification right after placing a reservation. We used two products from the suite of Amazon Web Services : Simple Notification Service along with AWS Lambda . The first step is to create an SNS Topic, which will serve as a message queue for all events sent from the core platform. The best and easiest way to achieve that with commercetools is using subscriptions . Once we have the subscription for OrderCreated events in place, any time a reservation is made, a new message is delivered to the SNS Topic. SNS provides an easy way to create a platform application , used to help you around delivering notifications to a specific platform. For an example, you can directly upload your certificates for APNS, a GCM API key, etc. No need to keep them in your project files, just use a simple GUI provided by AWS console. The final step is to write the AWS Lambda function, which will be triggered by every message the Topic receives. You can currently choose among 5 runtimes, depending on the language and dependencies you plan to use. The code used in this example can be downloaded from GitHub . It consists of a Groovy script, and a couple of dependencies managed by Gradle. The task the Lambda function is performing here is constructing the APNS-specific payload from the SNS event message, which is essentially the message delivered from our core platform to the Topic. The second task is obtaining a token for the customer referred by the event message. Finally, the function uses an Apple iOS Prod application created as a part of SNS services, to send the payload to the Apple’s servers. Another suite of products, in this case from Iron.io , provides us with a serverless ecosystem. We used IronMQ along with IronWorker for the test. Our core platform also provides an option to subscribe and deliver messages to IronMQ. IronWorker will be in charge of processing event messages, constructing payloads, obtaining customer’s tokens, but also used for sending out notifications directly to Apple’s servers. IronWorkers are built and distributed using Docker images. Once the Docker image of your worker is pushed to the Docker hub , you can register it with Iron. A slightly modified script and dependencies from the previous example are also available on GitHub . After the worker is registered with Iron, a unicast queue can be created in the Iron.io dashboard. The subscriber URL has to match the newly created worker. Now we can setup the subscription from our core platform, and have a message pushed to the IronMQ every time a customer places a reservation. The IronMQ subsequently triggers workers, passing the event message along the way. After trying out both solutions, we were able to notice certain startup time differences. Notifications implemented the Amazon way were received almost instantly, while the Iron.io way gave us a certain delay. A slightly outdated article from couple of years ago comparing Lambda and IronWorker services pointed out the startup time difference, but no numbers were given. In order to quantify this difference, we tried to measure the time between the order creation (the moment when the commercetools platform returns a successful response for the create order request), and the moment when the device receives the notification. The best result of the AWS way was slightly above 2 seconds, while the best time achieved with the Iron.io way took around 9 seconds. It is important to note that the delay we get with the Iron.io way isn’t solely caused by the startup time difference. The execution times on the IronWorker were longer, and inconsistent — the variation between samples was a few times higher than the best time itself. In many aspects both ecosystems we have tried are very similar. For those thinking about reliability, Iron.io claims to be pretty rock solid . AWS SNS also promotes redundancy and reliability . Depending on your specific use-case, you may find startup time differences more or less important. If your language preferences do not align with runtimes available on AWS Lambda, you might opt for the IronWorker solution, providing much more freedom, due to their approach using the Docker images to package and execute tasks. As a Swift engineer, I am personally looking forward to trying out Swift code under a Docker container, and pushing it to IronWorker. It is always important to avoid supplier lock-in. In this particular example, the JVM runtime is supported by both AWS Lambda and IronWorker, making a migration possible. An obvious difference between the two scripts is the entry point, i.e the way to consume the message sent from the queue. While SNS features come in handy to simplify the codebase, moving away from AWS would require additional effort to implement those functionalities. On the other hand, if you initially start with a language that is not properly supported by the AWS runtimes, migrating away from the Docker images supported by the IronWorker, to the Lambda, can be a challenge. Looking under the hood of the commercetools platform 41 2 Thanks to Christoph Neijenhuis , Philipp Sporrer , Kelly Goetsch , Yann Simon , and Stefan Meissner . AWS AWS Lambda Ironworker Apns Serverless 41 claps 41 2 Written by iOS Software Engineer Looking under the hood of the commercetools platform Written by iOS Software Engineer Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-07"},
{"website": "CommerceTools", "title": "advanced data tables in react", "author": ["Philipp Sporrer"], "link": "https://techblog.commercetools.com/advanced-data-tables-in-react-dbe33f8345ab", "abstract": "About We're hiring The web development community has left behind the days where tables were used for layout or weren’t used at all in favour of trendier layouts. We are finally using tables how they were intended to be used: For visualizing data . In this article, we will take a look at some data table libraries in React, how they compare to each other, which one we chose, and suggestions which one you should choose depending on your context. We will also show some storybook demos at the end of this post. Our team at commercetools is building a web application for managing the company’s e-commerce platform, which enables merchants to efficiently conduct their daily business. We visualize data like products and categories in a flexible and customizable way in order to cover several business cases. In order to do that we have been using a data table library for a while, but recently we hit its limitations and had to reconsider our options. We decided to not reinvent the wheel by writing our own data table but to pick one of the existing table libraries. In order to find out which one is most suitable to our needs we sat together and identified requirements that a modern data table should fulfill in our context: fixed (sticky) and scrollable columns resizable columns horizontal scrolling custom cell rendering flexible column width and row height should be a react component In React there is a handful of options for building data tables. Here are the ones we looked at: react-data-grid react-virtualized FlexTable react-datagrid fixed-data-table ag-grid In our case flexible column width and row height is very important since there is a lot of related information the user wants to see at once in an e-commerce admin UI. For example listing associated categories per product: The image above shows that a product can have multiple categories. Thus every row in the table could have a different height. The problem with this is that long category paths will wrap across multiple lines, making it hard to compare paths to each other. We also want to give our users the option to make the column as wide as the longest category path, so no category path will wrap to the next line: Now that we know our requirements and which table libraries to look at, let's see the results: These are all great libraries, but none of them was able to tick all the boxes. We have already been using FixedDataTable for quite a while and would have loved to just stick with it, but it does not support the flexible cell dimensions requirement described above: FixedDataTable does not provide a layout reflow mechanism or calculate content layout information such as width and height of the cell contents. The developer has to provide the layout information to the table instead. - FixedDataTable Docs While evaluating React Virtualized (RV) we came across its CellMeasurer component that “ automatically measures a cell’s contents by temporarily rendering it in a way that is not visible to the user ”. But this feature has to be used with care since it has some performance limitations: “Measuring a column’s width requires measuring all rows in order to determine the widest occurrence of that column. The same is true in reverse for measuring a row’s height. For this reason, it may not be a good idea to use this HOC for Grids containing a large number of both columns and cells” - CellMeasurer Docs For us, this is not an issue, since all our large tables use pagination and are limited to 200 rows. What's also great about CellMeasurer is that it's a so-called “Function as Child” (FaC) component. A Function as Child component accepts a function where you would normally have child elements or components. In the case of CellMeasurer , this function will be called with two callback functions to get the height of a row and the width of a column. Function as Child components abstract away state “without making demands on how the state is leveraged by its children” . This means we can easily compose CellMeasurer and FixedDataTable to address exactly the limitations of FixedDataTable we mentioned above 🎉. If you are unfamiliar with Function as Child components and when to use them, this article is a great place to get started. Since FixedDataTable needs to know the dimensions of rows and columns as static values while CellMeasurer provides async callbacks to get these dimensions, we can't directly compose them together. So we had to build a component that uses CellMeasurer to generate the dimensions of each cell, collect these cell dimensions and provide them as static values. We call it TableMeasurer. We already mentioned that Function as Child components are great for abstracting away state without making implications on how the state is used. TableMeasurer is another great use case for them since it should not make any implications on how FixedDataTable uses the calculated dimensions or how FixedDataTable renders. Let’s take a look at how exactly we can compose CellMeasurer , TableMeasurer , and FixedDataTable : This is how our list of product now looks like with the list of each product's categories: Since all the tools we used to build the table are open source, we figured we’d give back and also share our code. You can see it in this GitHub repo . We also deployed a storybook with some demos 👀. We have been using this setup for a couple of months now and it has been working nicely. Sadly, FixedDataTable is not maintained anymore and Facebook engineers advise to use React Virtualized instead: React Virtualized is looking very promising and is covering all the basic data table requirements. Basically, the whole library is just Function as Child components that you can compose together to create a data table that exactly does what you need it to do and not more. It also enables you to extend the table's features easily to make it even more flexible and customizable. For us, it is just the ScrollSync c omponent that is holding us back from replacing the described setup with React Virtualized because at the moment it is not possible to scroll the table while hovering over a fixed column or row. We already have some ideas of how to make it work, but it requires a reimplementation of the ScrollSync component. There are many great data table libraries when it comes to React. We decided for a combination of React Virtualized and FixedDataTable because we needed both fixed/scrollable columns and dynamic column widths and row heights. As a next step, we will try to help to reimplement ScrollSync so we can migrate to React Virtualized in the near future . If you are on the search for a data table React library right now you should probably already choose React Virtualized. Looking under the hood of the commercetools platform 254 18 Thanks to Josh Bones , Nicola Molinari , Dominik Ferber , and Manuel Küblböck . React JavaScript Tech Datatables 254 claps 254 18 Written by Frontend Engineer @Commercetools. Looking under the hood of the commercetools platform Written by Frontend Engineer @Commercetools. Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-23"},
{"website": "CommerceTools", "title": "building an ecommerce graphql api the challenges", "author": ["Oleg Ilyenko"], "link": "https://techblog.commercetools.com/building-an-ecommerce-graphql-api-the-challenges-6d652a95f478", "abstract": "About We're hiring In a first part of this article I described some of the reasons why GraphQL caught our attention and how it helped us to overcome so of the challenges that we were facing with the RESTful approach. techblog.commercetools.com Implementing a GraphQL server has challenges on it’s own though. Next I would like to describe some of them and show how we approached these issues. As I mentioned before, some of our entities are pretty big in size, like products for instance. GraphQL already helps a client to reduce amount of data sent from the server, but we went one step further and optimized communication between a backend and a database itself based on the same GraphQL query. This turned out to be a very effective technique. Just to demonstrate this approach, here is a simplified example of how we define list-based fields (here we are using sangria — a scala GraphQL implementation): Calculating a total number of results can be pretty expensive, so we would like to avoid it if possible. In this example, with Projector we are getting all the nested field names (just one level deep) and conditionally calculate a total if the query asked for it. My colleague, Yann Simon , recently gave a talk on this topic at Berlin’s first GraphQL meetup . Here are the slides of his presentation: As you have been reading about GraphQL and how we are using it, you have probably started to wonder: isn’t it dangerous to allow clients to request an arbitrarily complex queries that even may include recursive data types, like Category ? Indeed, it can be dangerous unless you have some safety mechanisms in place. In our GraphQL API we are putting a cost/complexity metric on every single field within a schema. Here is, for instance, the same category list field definition that you saw before, but focused on a complexity argument: As you can see, the complexity is just a number based on the field’s own complexity (50 in this case) and the complexity of all of its children/nested fields. In order to make this an even more fair and safe complexity estimations, we also take the limit into consideration — this represents the worst-case scenario where the amount of available categories is the same as or greater than a limit. The complexity of the query itself is just an aggregation of the complexity of every single field used in the query. Given this complexity definition, we can estimate the complexity of a query without even executing it! Static query complexity analysis takes place after query validation and before a query is executed. In our case we are doing it with a query reducer like this one: This will not only measure a query’s complexity, but also reject all queries that are above specific query complexity threshold. The commercetools platform supports user-defined types and fields in the form of product types . Recently we integrated product types in a GraphQL schema by generating parts of it based on the project’s product type definitions. Since it’s pretty expensive to load all of the product types from a database on every single request, we decided to go with a hybrid approach where we are generating and storing GraphQL schema extensions in advance. Then on every client request we are loading these generated schema extensions and use them to extend a normal, statically defined, schema (which is the same for all tenants). The whole process looks something like this: Schema extensions are formatted as a string in GraphQL IDL format: When we are processing a client request, we just need to extend our static schema like this: CustomSchemaBuilder generates all of the necessary logic for extension types and fields. This approach provides a pretty flexible and efficient solution to this challenge. We are still able to define static parts of the schema in the scala code and generate dynamic bits on-the-fly. This one is a classic N+1 problem when loading data from a database. With GraphQL, the client decides the structure of the query, so for a backend it becomes a bit harder to optimize the execution of the whole query, especially if every query field is resolved in isolation. As I mentioned earlier in this article, we already have a reference expansion mechanism in place which we are using for our REST API. In order to reuse the same mechanism in the GraphQL schema, and optimize the query execution plan, we are using deferred value resolution mechanism . In the backend we have defined Deferred values like this one: Which we then use in the resolve functions in order to defer the retrieval of particular reference/entity instead of loading it eagerly. Here is an example of Category ancestors field: When the query is executed, all deferred values are collected and given to a DeferredResolver responsible for loading them all at once in one or several efficient batched queries. This mechanism is pretty easy to use and it massively reduces the load on the database since many queries are batched together. All in all, we are very happy with GraphQL so far and are planning to continue working on it in the future. We already have clients and applications using it, but we are still at the beginning of this journey. I recommend you to check out our GraphQL API docs and try out some queries in the GraphiQL . If you have some questions or would like know more about particular aspect of what was discussed here, just leave a comment below! Looking under the hood of the commercetools platform 84 GraphQL Scala Ecommerce 84 claps 84 Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-17"},
{"website": "CommerceTools", "title": "top 5 machine learning applications for e commerce", "author": ["Amadeus Magrabi"], "link": "https://techblog.commercetools.com/top-5-machine-learning-applications-for-e-commerce-268eb1c89607", "abstract": "About We're hiring With the explosive growth of data, it is one of the most important challenges of modern businesses to develop data-driven infrastructures. Scientific breakthroughs in artificial intelligence (AI) have opened the door for a broad range of applications, which can leverage vast amounts of data into real business value. Leading AI researcher Andrew Ng says that AI is the new electricity as it will fundamentally change all industrial sectors, Forrester predicts that AI investments will grow by 300% in 2017 , and Barack Obama notes that his successor will govern a country being transformed by AI . Initially, the hype around big data and AI was quite overwhelming and companies were not really sure how to react. Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. — Dan Ariely, 2013 By now, most companies have adopted more realistic expectations and understand that AI will neither solve all of their problems nor can it be ignored. One area that is particularly affected by this development is retail. The number of purchases made online is steadily increasing , which allows companies to gather detailed data on the whole customer experience: what they look at, in what order, for how long, on which day, which questions they might have, what they eventually buy, or how they rate and review products. With AI, this data can then be used to improve this experience to make it easier, more efficient, more engaging, and more adapted to personal needs. Innovations in this area are based on methods from machine learning , which is the subfield of AI that develops autonomous learning algorithms. This article will introduce five of the most influential machine learning applications in the e-commerce domain. Product recommendation is typically the first thing people have in mind when they think about machine learning for e-commerce. Features like “ if you like product x, you will probably also like product y ” have been shown to work remarkably well, and they can serve as a valuable tool to guide users through the ever-increasing masses of options available to them. Traditionally, recommendations have been added by hand based on hard-coded product categories, but this is extremely time-consuming, error-prone, and quickly out-dated. Modern recommendation algorithms are separated into two categories: collaborative filtering and content-based filtering . In collaborative filtering, recommendations for a given customer are based on what similar customers have chosen in the past. Simplifying somewhat, if customer A has bought product X and Y, and customer B has only bought product X, then product Y could be recommended to customer B. On the other hand, in content-based filtering, recommendations try to match the content of customer profiles (e.g. gender, brand preference, or age) to the content of products (e.g. category, price, or color). eBay has listed more than 800 million products in their catalog. Numbers like these make it apparent that it is increasingly more important to provide efficient search algorithms, because no matter how high the quality or how low the price of a product, it cannot generate sales if customers are not able to find it. Machine learning can assist with features like search ranking , which allows sorting search results by their estimated relevance. This estimation can take into account frequencies of specific search terms as well as the particular customer profile (e.g. age range, previous product views, phrasing habits, or previous search terms). In short, search algorithms become less about listing all products that match a given sequence of letters, and more about predicting what customers might actually want to see, even when they might not know it yet. Another important feature is query expansion , in which the most likely search term completions are suggested while the customer is still typing. Apart from typical text-based search, image-based search is becoming an increasingly viable option. Scientific advances in image recognition through deep neural networks now provide the technology to use pictures of products to find similar items online. In addition to that, these methods can be used to classify facial expressions and recognize emotions. But even though the idea of dynamically adapting commerce services to the current emotional state of a customer certainly seems valuable, companies still have not quite figured out how to put this into practice. The more data you have, the harder it is to check for inconsistencies. One way to handle this problem is automatic anomaly detection. The idea is that an algorithm can identify patterns in the data to learn what is ‘normal’ and then send alerts as soon as data points exceed that range. From the machine learning perspective, the main challenge of this problem is to train a robust model despite having a heavily imbalanced dataset, since there are far fewer cases labelled as ‘anomaly’ than ‘normal’. A popular e-commerce application of this approach is fraud detection. Retailers frequently have to deal with abusive customers that use stolen credit cards to make excessive orders, or customers that retract payments via their credit card company once products have already been delivered. Besides cases of fraud, anomaly detection can also be used to ensure a high level of data quality for product information. Large databases in the e-commerce sector often contain errors like incomplete product titles, missing images, or products sorted in the wrong categories. Detecting these cases quickly and efficiently can therefore save companies a lot of time, money, and effort. When you have trouble with a service, trying to get help can often be quite a frustrating experience. Customers frequently complain about exceedingly long waiting times, having to explain and re-explain their problem multiple times, unqualified advice, or stressed out employees. Given the high amount of resources that are required to provide reliable customer service, it is not surprising that these issues can occur. Machine learning can help to automatize this process through robots that can answer phone calls. Whereas previous systems were only able to deal with a narrow range of problems and had frequent misunderstandings, recent advances in speech recognition and natural language processing via deep learning have made it possible to have a more flexible and natural interaction with robots. Crucially, these methods have shown improvements in taking contextual information into account. Instead of analyzing a speech sound or a single word in isolation, modern approaches take information from the whole input into account and compare it against frequently occurring patterns, which has boosted the accuracy of machine learning models. Apart from phone calls, machine learning can also add to other support channels, such as automatically answering emails, categorizing emails (e.g. complaint vs. question vs. request), or providing support via chatbots. Chatbots in particular have inspired a variety of AI startups that want to revolutionize communication channels for marketing, consulting, or recruiting. Airlines were among the first companies to embrace the concept of automatically adapting prices. On the basic level, this can simply mean to increase prices when the demand is high and decrease them when the demand is low. But there are plenty of other variables that can also be used to estimate optimal prices, such as prices of competitors, time of day, warehouse stock, or season. However, pricing algorithms cannot be painted with a broad brush and need to be adapted for specific products to accommodate factors like marketing strategies (e.g. whether to give particular competitor prices more weight than others, or whether or not to keep prices low after introducing a new product to boost sales). From the perspective of the customer, these techniques can have positive as well as negative effects. Some customers will regret their purchase and stop using a shop when they see the price dropping only minutes later. For other customers, dynamic prices can turn out to be an exciting game in which they can try to hunt the best prices. It remains to be seen what the large-scale effects will be, but if the rate at which retailers adopt these new techniques is any indication, then dynamic pricing is here to stay. The applications mentioned above are only a small selection of what machine learning can do for e-commerce, but there are plenty of other options, such as: Customer Segmentation : Identify systematic groups of customers to make marketing more precise. Product Categorization : Automatically sort products into categories to speed up inventory management and improve customer navigation. Churn Prediction : Predict when customers will stop using a service to analyze potential reasons and allow for countermeasures. Sentiment Analysis : Evaluate the public perception of a product based on sources like social media. Inventory Forecast : Make production and distribution more efficient by predicting market demands. Anticipatory Shipping : Ship a product before an order is placed. It is quite apparent that advances in machine learning have opened up a broad range of options for the commerce sector, and with the increasing availability of open source packages and easier access to distributed cloud computing, these techniques are not just available to large retail giants with armies of data scientists anymore. Based on the vast number of possible applications, it is also important for companies to define strategic priorities to get the most value out of machine learning. At commercetools , our machine learning projects are focussed on enabling personalized and engaging customer experiences as well as optimizing product information management. It will be interesting to see how the continued advancement of AI, especially in the areas of text- and image-processing, will continue to shape the future of the commerce industry. Looking under the hood of the commercetools platform 303 Thanks to Josh Bones , Bert Stevens , Nikolaus Kühn , and Christoph Neijenhuis . Machine Learning Artificial Intelligence Ecommerce Retail Data Science 303 claps 303 Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Written by Lead Data Scientist, interested in data science, machine learning, Python and decision sciences. twitter.com/amadeusmagrabi Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-11"},
{"website": "CommerceTools", "title": "building an ecommerce graphql api our journey", "author": ["Oleg Ilyenko"], "link": "https://techblog.commercetools.com/building-an-ecommerce-graphql-api-our-journey-bf5d83f236d2", "abstract": "About We're hiring Shortly after the initial announcement of GraphQL we realized that we can provide a lot of value for our customers by providing a GraphQL-based API for the commercetools platform (CTP). Here I would like to share our journey with you and describe the reasons why we chose GraphQL and how we implemented it alongside our REST API. From the very beginning the commercetools platform was API-driven and for a while we have been using a RESTful approach to expose it. We put a lot of attention in the design of our resources and their semantics. The resources are modeled based on Domain Driven Design (DDD) principles and patterns. For instance, every resource (like product, order, discount, etc.) represents an aggregate root and generally defines a transactional boundary. As we iterated on our API design based on the user feedback and our own discoveries we faced quite a few challenges which led us to GraphQL and which I would like to share with you in more detail in the next sections. The first challenge is the amount of information we give back to our clients. Let’s look at a typical e-commerce application. It requires a lot of information to render just a single page. On the other hand we have other clients that do not necessarily need all of this information, but are very limited in terms of number of requests and amount of data they can transfer/process in a reasonable amount of time: It’s a tough challenge to design a single model and resource structure that would fit the needs of all these different client types. In order to address some of these issues we have introduced reference expansion in our REST API. This helped clients to get most of the needed information in a single request, but still results in huge amount of data over-fetching. For instance some of the bigger projects contain huge products and variants. In these projects just a single product may have several megabytes of data. What makes situation even worse is that most of the clients need only a small portion of this data which is relevant to the application. Even after we introduced the reference expansion mechanism for all of our resources, there are still a lot of use-cases that require clients to make several consequent API requests because they are unable to fetch all of the necessary information in one go. Given a series of independent client requests it becomes challenging to correlate these requests together and to optimize data retrieving/caching strategy. There is an implicit correlation between many of the requests to our API, but there is no way for a client to communicate this information and help us perform optimizations based on it. A while back I described some of the challenges of API evolution in this article: medium.com Our API evolves very fast: we learn new things, get feedback from our customers, implement new ideas. It is important for us to keep the pace and change the API without breaking existing API clients. Unfortunately this is challenging with the RESTful approach. There is no (standard) way for client and server to communicate the deprecation information. So far we did our best to not introduce versioning in our REST API since maintaining different versions would require a lot of maintenance effort and may slow us down. At the moment we are maintaining our REST API documentation in a set of static markdown files. These markdown files are external to the code and it is pretty tedious to keep documentation in sync with the actual API. I strongly believe that modern API documentation should not only provide structured information, but also be interactive. Ideally new API users should be able to discover API capabilities as they make requests in an iterative environment. We looked at RAML and Swagger and made several proofs of concept. After evaluating these tools we still could not find a consensus on which tools we should use and which approach will require the least maintenance. Even though these tools do provide an ability to document the API in a structured way, one still needs to manage it externally. JSON schema also does not map very smoothly in the way we structure and define our API. We also faced quite a few issues with different tools around JSON schema, which didn’t make our decision easier. There are a number of API consoles available for RAML and Swagger, but in my opinion, none of them provide the same degree of interactivity and polish as GraphiQL . GraphQL helped us address these challenges. The request/response cycle of a typical GraphQL query looks very similar to normal HTTP GET or POST request: In this example the client asks a server for a name and description of a particular product. And this is precisely what the server gives back to the client in it’s response. I will not go into detail on the GraphQL basics in this article, there are a lot of resources already available on this topic. Instead I would like to focus on how it helped us building our API and address challenges that I described earlier. One can see a GraphQL query as a way for a client to communicate its data requirements: Which also means that one can treat different versions of the same client application as different application with slightly different data requirements. GraphQL itself provides a native way of deprecating fields and communicating this deprecation information to the client. The client, on the other hand, is always required to specify its data requirements in form of a GraphQL query. This provides us with a way to evolve our API very naturally without breaking existing clients or resorting to versioning. On the server we always know which objects and fields existing clients are still actively using. If we decide to remove or change existing fields, we can go through a deprecation process where we first deprecate existing fields, wait until all clients are migrated, and then remove these fields with confidence. By its nature, GraphQL addresses issues with under-/over-fetching of the data. Project data is no longer available as a set of independent resources, but instead represented as an object graph. This not only provides a very natural data model for a client, but also allows it to fetch all necessary information for a particular view or bit of business logic in one request. At the end of the day it’s all about making colleagues, who are using your API, happy ;) In this particular case we exposed product categories in our GraphQL API. It is an interesting scenario because both server and client benefit greatly from using the GraphQL API. After the new feature was shipped, the client was able to fetch several levels of categories with product counts with a single GraphQL query like this one: The server, on the other hand, does not receive all these 150 independent requests anymore. Instead it gets only one single request with a GraphQL query that describes very precisely the client intention and requirements. Just by having this information we are able to optimize our internal queries to MongoDB and Elasticsearch and retrieve the requested three levels of categories very efficiently. In terms of documentation and discovery GraphQL has a lot to offer as well. The type system and introspection API provide all of the necessary information to generate structured documentation and build tools that help users discover and learn an API. I think this demonstration of GraphiQL tool speaks for itself: The GraphQL provided us with a very compelling set of tools to overcome the challenges we faced with the REST API. The journey to GraphQL was not without challenges on its own, though. Stay tuned for a second part of this article where we will go into detail on challenges we faced as we were implementing the GraphQL API and the way we addressed them. techblog.commercetools.com Looking under the hood of the commercetools platform 101 Thanks to Sven Müller , Philipp Sporrer , Melanie Niermann , Nikolaus Kühn , Roman Zenner , Josh Bones , Christoph Neijenhuis , Yann Simon , Nicola Molinari , and Peter Gerhard . GraphQL Scala Ecommerce 101 claps 101 Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Written by Passionate software engineer on his journey to perfection Looking under the hood of the commercetools platform Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-17"}
]