[
{"website": "Kickstarter", "title": "supporting open source our donations for 2018", "author": ["Natacha Springer"], "link": "https://kickstarter.engineering/supporting-open-source-our-donations-for-2018-d69a6114bf89", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! At Kickstarter, we use a lot of open-source software. We pledge in our PBC charter to support a more creative and equitable world, and that our operations will reflect our values. As part of that ethos, we try to do our share to support the open-source ecosystem by making a financial contribution to select projects. To choose these organizations, we surveyed all of our engineers asking for nominations. They were more than eager to help, and as a result we split our yearly donation budget between two open-source projects and four organizations that foster learning or the improvement of coding skills within the open-source community. Babel : Babel is a toolchain that is mainly used to convert ECMAScript 2015+ code into a backwards-compatible version of JavaScript in current and older browsers or environments. Babel allows us to use the latest Javascript technologies while making Kickstarter accessible to creators and backers all over the world. Henry and the rest of the Babel team have done an amazing job growing and improving the Babel toolchain. Webpack : Webpack is the tool we use to bundle our Javascript here at Kickstarter, and we’re grateful for the community that builds and maintains this tool! Advent of Code : For the last two years our engineers have enjoyed sharpening their problem-solving skills by working together on Advent of Code puzzles during the holiday season. Eric Wastl has done an amazing job turning complex computing problems into whimsical holiday stories! Rails Girls Summer of Code : Rails Girls Summer of Code is a program that helps women and non-binary individuals get involved in open-source projects. More diverse teams build better software. We value diversity on our team and chose to support Rails Girls Summer of Code to encourage diversity in the wider tech community. Code Nation : Code Nation equips students in under-resourced high schools with skills, experiences and connections to create access to careers in technology. One of our senior engineers spends time each week mentoring students through Code Nation, so it made sense for us to support them a bit more. Code Cooperative : Code Cooperative teaches formerly incarcerated individuals the skills needed to be users and creators of technology. The work of addressing the systemic inequity of the prison system is vast, and we’re excited to support an organization equipping this community to address issues important to them using technology. Huge thanks to all of the many people who put time and effort into these projects! Want to help select the projects and organizations we support next year? Join our team ! We’re always hiring passionate, creative, and collaborative engineers to help bring creative projects to life. The engineers who build and run Kickstarter share a… 145 1 Open Source Engineering Leadership Engineering Solutions Kickstarter Campaign Engineering Job Openings 145 claps 145 1 Written by Engineering Manager, Payments and Identity The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Engineering Manager, Payments and Identity The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-11"},
{"website": "Kickstarter", "title": "a guide to mindful communication in code reviews", "author": ["Amy Ciavolino"], "link": "https://kickstarter.engineering/a-guide-to-mindful-communication-in-code-reviews-48aab5282e5e", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Code reviews are a core part of being a software engineer. Most of us review code every day. Technical skills are needed to review code thoroughly and quickly. But beyond that, reviewing code is also a form of communication, teaching, and learning. Either as the author or the reviewer of code, being mindful in your communications can make code reviews more valuable for everyone. First of all, what does it mean to be mindful in your communication? Mindful communication is a term that means to listen and speak with compassion, kindness, and awareness of yourself and others. This perspective is the basis of most of the tips in this guide, so let’s dive into it a bit more. Here are some general guidelines for practicing mindful communication: Put yourself in the other person’s shoes. Listen to hear what’s actually being said. Let go of what you think the outcome should be. These things are easier said than done! So I’ll break down how these guidelines apply specifically to reviewing code and having your code reviewed. A crucial skill to learn for practicing mindful communication is a self check-in. This means taking a moment to see how you’re feeling and take a step back from what you’re communicating. When you’re doing code reviews, you’ll want to check in with yourself to see if you’re following the guidelines. Good times to check in are right before you start or reply to a review, if you’re feeling frustrated, or right before you hit “submit” on that comment. When you’re checking in, also consider how you’re feeling in general. Giving kind and considered feedback takes time and energy. If you’re hungry, tired, in a hurry, have a lot of meetings, etc., don’t review code or send out a review. You need to fix those things first. If you don’t care for yourself, you can’t take care of others. Here are some good questions to ask yourself when you check in: Why did this person write this code the way they did? Is the tone and wording of my feedback something I would feel good hearing? Do I fully understand where they’re coming from? Do I think I know the “right” way to write this code? Am I allowing space for a solution I haven’t thought of? Do I have all the information I need to understand this issue? Have I eaten lunch? Do I have enough time right now to give thoughtful feedback? Am I feeling upset/nervous/anxious about something else? Assume the author did what they did for a reason. Put yourself in their shoes. When you make a code change you likely consider a few options, and go with one that fits the needs and constraints of the problem you’re solving. They likely did the same thing. Instead of telling the author they did something wrong, try: “What about this other option? It might make xyz better!” “What happens in abc situation?” This also hits on letting go of the outcome. You might not have the right answer. You might not have all the information you need to understand why the author took the approach they did. If you step back and try to understand where they’re coming from, it might surprise you! Listen! Don’t assume. If you think something is 100% off the wall, check in with yourself. You respect your co-workers, so ask them to explain their thinking. You might learn something, or get a glimpse into someone else’s thought process. Try these formats: “This part is a little confusing. Could you walk me through what it’s doing?” “I’m having a hard time following this section. How does it work?” “What other options did you consider for this part?” There’s no need to call out nits or small things that maybe aren’t your style. Linters are great for catching small issues or keeping a code base consistent, so there’s no need for a human to call those sorts of things out. If a linter could catch what you’re about to point out (even if your current setup doesn’t), think hard about whether it’s worth everyone’s time to discuss. Labeling a comment a “nit” is also to be avoided because it minimizes your feedback. Your input is valuable, so leave out the caveats. If you don’t feel your comment is worth adding without a caveat, consider leaving it out completely. Avoid starting comments with these: “Nit: …” “This is a nitpick …” “Tiny nit: …” “This is slightly nit-picky, but…” (All pulled from real PR comments 😱) So you’ve left out the nits, but there are still things you think could be improved that aren’t strictly necessary. Let the author know you don’t mind if they’re updated in a later or follow-up PR. You might not be aware of a deadline for the team or that there are a lot of changes still coming. Here are some phrases that acknowledge iterative work: “This could cause x problem later, but I think it’s fine to fix in the next iteration” “Can we make a ticket to come back to this section? It might cause x issue down the road!” “Have you thought about how this will generalize to what we’re working on next?” “How are you planning to handle this other situation that’s coming up?” Our work is iterative and subjective so don’t be a gatekeeper. Try to approve reviews even if you disagree with the style or approach. People write code differently, and come from different perspectives. Though I advocate for not blocking PRs on reviews, often the author must get your approval before moving forward. This creates a power imbalance. Avoid abusing that power and approve it unless you’re worried a change will cause major problems. If you’re that worried about something, another option is to go talk in person or use some of the open questions from my earlier tips. An exception to this is if the change is very large or includes an architectural decision. When a decision is being made that will have long-term consequences, it likely warrants a larger conversation than a code review. After that conversation the review will likely be a quick 👍. Particularly when reviewing code for someone more junior, giving a specific example in the review comment can be helpful. This saves them time, and makes it really clear what you’re proposing. Also, you already thought through it, so why make them figure it out on their own? Here are some examples: “This section might be more clear if you switch the order like this: if (true) { return “this is an example”; }” “You might be able to replace this part with a utility we have! I think this would do the trick: const x = utily_thing(y, { a: true })” Linking to documentation or files you referenced shows the author where to find the information next time. This is particularly good to do if you looked it up while writing the comment. You already have the link, so you might as well. Sharing is caring! But acknowledging that you needed to look something up can also help fight imposter syndrome . Calling out that you learned something new in the review can have a similar effect. If you come across a new method or syntax that you need to look up, leave a “TIL” comment and include the link that explains what was new to you. Ultimately the author of the code will be making and owning the change. If you have a more fundamental or big piece of feedback, end the comment with “What do you think?” This goes back again to letting go of the outcome. Maybe you don’t have all the context — maybe the author already thought about the approach you’re suggesting and there are issues with it that you missed. You can also ask for history about how the decision was made. If you weren’t part of conversations about the approach, getting some history can be helpful before suggesting a different one. Sometimes junior engineers are following the advice of another more senior engineer, so if you disagree it likely makes more sense to talk to the senior engineer. Hopefully large or fundamental changes aren’t needed by the time code is reviewed, which leads me into tips for the author. If you review code, you likely have your code reviewed also, so think about what makes a PR easier or harder to review. It is so much easier to review small changes — the smaller the better. Smaller changes are less risky, and take less time to review. Reviews with many hundreds of lines changed take much longer to review than several small ones with isolated changes. If each PR maps to one piece of work, it’s easier to understand what the code is doing. The reviewer doesn’t have to pull apart which changes are needed for each task. Sound familiar? This is a tip for both reviewers and authors! Linking to documentation in your review description or comments can make it easier for the reviewer to understand what’s being changed or implemented. If you’re implementing an interface or using a new library, link to the documentation you referenced while doing your work. This is particularly helpful for seemingly small changes in things like configuration. Have you ever received a one-line change and had no idea what it meant? A link to the documentation is so helpful in these cases. Even if a reviewer follows all the tips above, they can still make mistakes or get careless. Try to keep in mind that we’re on the same team and they likely had good intentions behind whatever feedback they gave. It’s easy to get defensive, but remember, as the author you have to let go of the outcome just like the reviewer. Maybe the reviewer has some knowledge or expertise you were missing, or saw a blind spot you missed because they came from a different perspective. A major exception to this is if the reviewer legitimately doesn’t have the best intent. If someone is being actively malicious or hostile in a code review, that’s harassment and you should speak to your manager and HR about it. Your diff itself is a kind of communication. Make sure it’s well written! Here are some things to think about when cleaning up a diff: Don’t refactor in addition to new changes. Check that your PR doesn’t have unnecessary changes or unrelated files. As mentioned above, each PR should be one logical unit of work. If you find yourself leaving explanatory comments on your own code review, those might be better as code comments. Sometimes diffs are more easily viewed with no white space. You can see a diff without whitespace changes on GitHub by checking “Hide whitespace changes” under “Diff settings” or by adding w=1 to the url. On the command line you can use git diff -w. You can call out that a particular change might be better to view that way in your PR description. Having a quick chat about some implementation options with someone who’ll review your code is a great way to avoid stress when the review comes around. Reviews are usually not the time to work out architectural decisions since the work has already been done. If this all sounds like a lot of hard work, that’s because it is. Like I said earlier, giving kind and considered feedback takes time and energy. But luckily it gets easier. You can build up muscle memory of actively giving kind feedback and it will become natural. At first it might feel uncomfortable to ask a question when you think you can just tell someone the answer, or it might feel slow to write out a code example. You might have to stop, check in with yourself, and do it anyway. This is a learning process and learning new things is uncomfortable. Lastly, keep in mind that every team is different, so these specific tips might not all apply to your team. If you come up with other ways to apply mindful communication to code reviews or building software in general, I’d love to hear about them! This post is adapted from an internal talk I gave a while back. At our biweekly Engineering team meetings, anyone on the team who has something to share can give a talk. The talks cover everything from explainers about parts of our infrastructure, to a new Javascript pattern, to how teams talk about their feelings. We also start each meeting with an “opening act” that’s usually someone from the team playing a little guitar or showing off some weird internet art. If you enjoyed this post, come work with me! We’re looking to add a whole bunch of folks to the team . Thanks to Natacha, Mark, Janel, and Corey for reviewing this post. ❤️ The engineers who build and run Kickstarter share a… 842 1 Code Review Mindfulness Software Development Communication Programming Languages 842 claps 842 1 Written by Engineering manager @kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Engineering manager @kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-02"},
{"website": "Kickstarter", "title": "oncreate ing kickstarters android app", "author": ["Lisa Luo"], "link": "https://kickstarter.engineering/oncreate-ing-kickstarters-android-app-90f567846750", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! O nce upon a time, Kickstarter existed on the web and iOS . Creators and backers rejoiced! But as our creative community blossomed, Android users were left in the dust. “Why is there still no Android app?” they said, “Kickstarter should run a Kickstarter to build an Android app!” they said. Finally on January 21, 2016, six years after the launch of Kickstarter.com and three years after the iOS release, Kickstarter for Android 1.0 entered the Play Store . We’d like to share the the story now, nearly two years later, the story of how the native team built the Android app, why we made the architectural decisions we did, and what we learned. This will be from the team’s perspective, but much kudos to Kickstarter alum Christopher Wright and Brandon Williams for masterminding our base FRP architecture and pushing our team to open source both native apps in their entirety . RxJava has been a core design dependency since the very beginning of the project. Our team liked how functional reactive programming (FRP) was rooted in mathematical principals, primarily the framework of thinking in functions: modular, composable relations between inputs and outputs with no side effects or mutability. Why is an Android or iOS app a good candidate for FRP? Unlike web development, native app development includes implementing the lifecycle of the native device’s operating system. In short, every app is a jungle of lifecycle callbacks, memory management, and asynchronous tasks which are a breeding ground for mutable variables and state. Mutability often makes code hard to read and impossible to test, but by designing our app in a Model View ViewModel (MVVM) pattern using RxJava, we were able to test our app’s business logic easily — more on that soon. We designed the app in an MVVM pattern with the hope of keeping all business logic in ViewModels . The theory was to keep our ViewModel classes agnostic of their View classes, and vice-versa: each BaseActivity had a ViewModel instance and was responsible for pinging the ViewModel ’s input functions with the data as defined and using the output values as described. We named our outputs very explicitly, e.g. startProjectActivity , so both the View and the developer would know exactly what the output should do. If you’d like to see more specifics on our ViewModel style, read this . This input-output ViewModel structure allowed seamless data flow between a View (an Activity , ViewHolder , or Fragment ) and its ViewModel class via explicitly defined pipelines. In the ViewModel class, the resulting business logic code is often quite readable: RxJava’s library of Observable types were the key to bridging the world between the imperative View and the functional ViewModel . We fed our input PublishSubject s with the appropriate Java types and subscribed to output Observable s or BehaviorSubject s, which allowed for us to observe the history of their values emitted and poll the latest values to live-update our View s. Extracting our logic from the View by separating inputs from outputs not only allowed for us to break down and remove if / else chains and async networking tasks, used too fondly in Android development, from our View s, but also allowed us to write lightweight unit tests for each output using Robolectric . The moment we wrote our first ViewModel test solidified the good that we had stumbled upon with RxJava and MVVM design. Our theoretical beliefs and proof-of-concept white boarding sessions paid off right then: the reactive architecture we felt we had hammered into our Android app not only worked, but also made testing both feasible and fun. We used trusty ol’ Robolectric and RxJava’s TestSubscribers to test our outputs , with mock data including Model factories and a MockApiClient . What’s so fun about this? Well, aside from the sheer excitement of having thorough tests in an Android app, tests that read clearly like scripts for each screen’s interactions, they were fast to write, fast to run, and helped us develop in a TDD manner even under deadline pressures. During our 1.0 development we seldom merged a new ViewModel pull request without accompanying tests; the ViewModel looked lonely, and investing the small cost of a test file upfront has accounted for our <0.01% crash rate, which to this date our small Android team is very grateful for. In fact we love testing so much that we’ve given several talks on how we do it, check them out ! Although RxJava has been the backbone of both our Android app and this blog post, we did make many non-FRP design decisions that have served us well. We used Dagger from the start for dependency injections, and have since created a global Environment parcelable class to holds our global values including the current user, api client, and currency formatter. The Environment is provided to base ViewModel s to reduce scattered dependency injections, but View s still use Dagger to unwrap its required dependencies. Can a generalist Android post be published without mentioning fragments? Well we have one for our discovery’s sorting ViewPager , we love her, and it was a joy including Fragments into our MVVM design. We simply added a BaseFragment and a base FragmentViewModel modeled after our BaseActivity and ActivityViewModel , with a few more lifecycle callbacks and arguments() instead of an intent() . ☺︎ A long long time ago, back in 2014, building an Android app from scratch with a deadline using RxJava was a challenge in itself. The minimal online resources and having to relearn how to debug RxJava operators in Android Studio resulted in many ViewModel iterations to feel solid with our architecture— in the beginning we named our logic classes Presenter s but later switched to ViewModel and MVVM from our not-quite MVP design. We realized only after releasing 1.0 that by binding ViewHolder s to the lifecycle, they could also instantiate their own ViewModel classes. We’re still working on modernizing many of our classes with these iterations, with the help of our open source community ! Luckily our team of two iOS and two Android engineers agreed from the start that functional reactive programming was well worth the daunting learning curve. From a non-mathematical background, I went in with the attitude of “ anything is better than the current Android development standard, I’m in!” Despite our late nights dissecting extraneous API flatMap calls only to learn that hot and cold observables were a thing , and moments of “let’s hope our fancy functional code will be…functional,” writing tests, finding patterns, and drawing debugging diagrams solidified our team’s understanding of and passion for FRP. Our architecture became a solid, organized codebase that enabled our team to ship the app in about ten months with localization, test coverage, and accessibility. After shipping Android, we realized that we liked the project’s MVVM architecture, ease of development, and testing so much that we were inspired to rewrite our iOS app in Swift with the same MVVM, Rx, and TDD design. Check out our ios-oss and android-oss repos and see how similar our ViewModel s are — write your business logic once, reuse everywhere: why limit that to one platform?? Today the Android team continues to develop core features and drive design. We hope to continue improving and modernizing our MVVM and RxJava infrastructure, make the app feel alive with animation, add screenshot testing, write more Kotlin code — whatever it takes to deliver a fantastic native Android experience to backers and creators alike! If any of this sounds cool to you, we’re hiring . The engineers who build and run Kickstarter share a… 196 2 Android Rxjava Tdd Mvvm App Development 196 claps 196 2 Written by software engineer, by day The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by software engineer, by day The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-13"},
{"website": "Kickstarter", "title": "fabric for open source android", "author": ["Rashad Cureton"], "link": "https://kickstarter.engineering/fabric-for-open-source-android-7743db969d5d", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! So you’re thinking of working on an Android application that will be open source or you join a codebase that’s open source. There are many things to keep in mind when you’re working on an open source project. Here at Kickstarter we don’t have that many external dependencies in our Android application, but for the ones we have we need to keep our credentials secret from the public. We recently added Crashlytics , a kit available through Fabric , to our app so we can receive better crash reporting and track real-time analytics. The process for adding Crashlytics to your Android application is very straight forward but in our case it required a couple extra steps. In order to integrate Fabric into our Android application you will need to add the proper gradle dependencies. After you add the dependencies, there are two ways to add the apiKey so that the SDK can read it. You can add the key to your Android Manifest which looks like this: Fabric creates a fabric.properties when the app is done compiling. In advance of that we can create our own fabric.properties file in the app directory of the project and store our apiKey and apiSecret there like this: Once you add all the necessary Fabric code to your application class the app should build successfully and Crashlytics should be enabled. Everything works so we should be good, right? WRONG! Just because we’re open source doesn’t mean our credentials should be, too! We should focus on making sure the app successfully builds when people want to contribute to it while hiding our credentials. In our project we need to create a fabric.properties.example file. Our Ruby script will copy the mock apiKey value into this file from the fabric.properties file that we’re going to generate with our gradle script. Since our open source contributors do not have access to our secret repo we will default them to the example file and copy this value below. Now the Fabric SDK will read this value and compile so that they can build the app without this dreadful error: Lastly we need to add this script to our gradle file to generate this fabric.properties if it doesn’t exist. That’s it! Now when open source contributors clone our project and build it they will be able to run it successfully without issue and our Fabric credentials aren’t exposed to the world so that people can create thousands of fake crashes 🙃. The engineers who build and run Kickstarter share a… 70 2 Thanks to Janel diBiccari . Android AndroidDev Crashlytics Fabric Open Source 70 claps 70 2 Written by Anything of value has a level of difficulty attached to it. Android Developer @Kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Anything of value has a level of difficulty attached to it. Android Developer @Kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-16"},
{"website": "Kickstarter", "title": "mvvm and kotlin", "author": ["Julie"], "link": "https://kickstarter.engineering/mvvm-and-kotlin-617ed4dbea7", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! If you’ve browsed the code for our open source iOS or Android apps, you may have noticed that we try to follow Functional Reactive Programming (FRP) patterns wherever possible. For an introduction to FRP, I recommend this explainer from Dan Lew. It’s a paradigm that we have followed in our mobile apps for years now and has helped us write clear code with minimal bugs. One of the main benefits of this pattern is it allows us to push all of our mutation and complex logic into ViewModels, which are then thoroughly tested . We are able to keep our views very simple and test as much of our code as possible while avoiding a reliance on manual UI testing. Gina Binetti and Lisa Luo, who have contributed to both Android and iOS apps at Kickstarter, gave a great talk on the benefits of FRP at the 2016 Functional Swift Conference: Which brings us to this year! I joined the Android team earlier this year after working for over two years on the backend and payments teams at Kickstarter, and the discipline from our mobile engineers in following FRP has allowed me to ramp up on the codebase very quickly. In addition to FRP, the Android team at Kickstarter is pretty excited about Kotlin, the new programming language from JetBrains that runs on the JVM ( as well as elsewhere! ). We have already started adding a few Kotlin classes to our Android app, but we haven’t converted any core functionality to Kotlin because of some worries about our dependencies. I was also fortunate enough to speak at Functional Swift Conference 2017 in Berlin, Germany. For my conference talk I decided to try to push our use of Kotlin a bit further by adding a simple screen to our Android app using our established FRP patterns. Warning: it’s a lot of live coding, but hopefully it can be a good introduction to FRP and/or Kotlin. Enjoy: The engineers who build and run Kickstarter share a… 119 Thanks to Lisa Luo and Stephanie Coleman . Kotlin Open Source Videos 119 claps 119 Written by software engineer, cook, texan. The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by software engineer, cook, texan. The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-06"},
{"website": "Kickstarter", "title": "functional swift conference 2017 brooklyn", "author": ["Brandon Williams"], "link": "https://kickstarter.engineering/functional-swift-conference-2017-brooklyn-69341a22645c", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! On April 15th Kickstarter hosted the fourth ever Functional Swift Conference , which is the third time we’ve hosted it at our office! It’s an all-day event with 10 speakers (4 were Kickstarter engineers!), 90 attendees and lots of discussions around what can we learn from functional programming to write better Swift code. If you find any of this interesting, we’re hiring and we work out in the open ! Konrad describes the power of value types and recursive data structures as a means to very simply describe complicated objects. He applies this to KD-trees and shows off their amazing performance properties. Matt makes the case that test doubles aren’t helping you as much as you think they are. He shows how you can instead describe side-effects instead of performing them in order to gain more testability and understandability. Lisa, an Android and iOS engineer at Kickstarter, describes her journey moving between Android and iOS, and how functional programming provided a base language for how she interacted with two very different platforms. Christella, also an iOS engineer at Kickstarter, describes what it has been like to learn functional programming after having just learned object-oriented programming. Rahul discusses domain specific languages and uses GraphQL as an example, which a little bit of live coding at the end! Brandon gives a talk about algebraic laws that often lurk behind our abstractions, and why they are important. Brandon, another engineer at Kickstarter, continued other Brandon’s theme on monoids by showing some interesting constructions of monoids and then applying it to the case of predicates and sorting functions. Stephen, yet another engineer at Kickstarter, describes applicative functors, which are a generalization of monads, and how they unlock functionality that cannot be seen from the monadic world. Chris continues the theme on applicatives by live coding a simple API that generates its own documentation for free! Finally, Robert gives a whirlwind of a tour through type systems and why type inference is so hard. He also describes how the Swift compiler team uses graph theory to attack problems of type inference in a practical way. The engineers who build and run Kickstarter share a… 15 Swift iOS Functional Programming Conference Videos 15 claps 15 Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-28"},
{"website": "Kickstarter", "title": "why you should co locate your xcode tests", "author": ["Brandon Williams"], "link": "https://kickstarter.engineering/why-you-should-co-locate-your-xcode-tests-c69f79211411", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! The default structure for Xcode projects has a directory to hold implementation files and another directory to hold test files. There is no standard for where files are placed in each of the directories, but often we try to create a mirror version of the source files in the tests directory. We propose that co-locating the test files with the implementation files provides a more seamless approach to development in an environment where a lot of emphasis has been put on testing. By default, an Xcode project with unit tests has two targets, one for implementation files and one for test files, and two corresponding directories to hold the files. The two directories are purely for organizational purposes as any file in the project can be associated to any subset of targets in the project. This style is also popular in “convention over configuration” environments, such as Rails and Maven. We find this approach to have many downsides. Maintaining a mirrored directory structure for tests is thankless work that feels like a chore. It can be difficult to quickly navigate between implementation and corresponding test file. It is quite common to browse code on GitHub and other code hosting sites these days, and the separation of test files makes it hard to see how well-tested a codebase is. Tests tend to become second-class citizens, often having a lower standard of code quality than implementations. We propose to co-locate the test file in the same directory as the implementation file, helping to solve some of these problems. This immediately doubles the number of files in a directory, but makes it obvious where to find the tests for a particular file. Even though the test file has been moved, we still maintain the target it is associated with. No more hunting for where to find files or put files. Navigating between implementation and test files is becomes much easier now. More interestingly, the test files are kind of analogous to header files, except they are also living code that describe how the API is expected to behave. Browsing large amounts of code on GitHub is quite common these days, and co-locating the test files makes this much easier. People can easily jump between implementation and test, and are practically encouraged to do so now. It also aids in reviewing pull-requests since the implementation and test files will alternate instead of having them lumped together. Tests should have some coding standards as implementation. Where we used to have a separate SwiftLint configuration for tests, we now use the same configuration for all code. There’s not always a one-to-one correspondence between implementation and test. Some tests cover code in multiple files (e.g. integration tests or UI tests). Tests of that type should structured in the more traditional way, inside a separate directory. We could even go further and say that those types of tests should even have their own target and be separate from your unit tests. We are now all onboard with this test co-location style. We have opened multiple pull-requests (by the way, we’re open source! ) to bring each of our first-party dependencies ( link , link , link ) and the main app ( link ) into this world, and so far we’re loving it! If any of this sounds interesting to you, we’re hiring ! The engineers who build and run Kickstarter share a… 109 8 Thanks to Stephen Celis . Testing Xcode Swift 109 claps 109 8 Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-30"},
{"website": "Kickstarter", "title": "namespacing actions for redux", "author": ["Sarah Groff Hennigh-Palermo"], "link": "https://kickstarter.engineering/namespacing-actions-for-redux-d9b55a88b1b1", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! L ike many other companies, we here on the Kickstarter front-end team have been rewriting our site as a React app, with Redux to handle application state. This post is about our investigations into and ultimate solution for one issue we ran into in our work: namespacing actions. I’ll cover why we wanted to namespace our Redux actions, the variety of available approaches, and the specific constraints we were working under before detailing our solution. (But feel free to scroll down to that last section first.) The code examples in this post for both the problem and the solution are available in their totality on CodePen. With Redux, you can use combineReducers to create nested reducers that only operate on a slice of state, but all reducers still respond to all actions. Often this is the point—a component can affect another component just by dispatching an action. But when we started creating multiple instances of the same component, we created a system where every instance responded to action meant for just one. Consider these instances of a div that changes color on hover. The intention is that just the instance being hovered over should change. But that’s not how it works. Why not? When you use combineReducers with Redux, you are creating a nested reducer where the keys match your state keys. So for instance, something like this: results in a reducer with a shape like this: When an action is dispatched, each reducer is called with that action and the slice of state which corresponds to its name. In this case, boxOne is called with state.boxOne , and boxTwo with state.boxTwo . This means that if an action is dispatched by one version of a component, something like: then it is responded to by both components. You get behavior you don’t want. We began with an initial research pass. In this stage, we took a look at commonly suggested solutions from the community: using local state and three approaches to manual namespacing via action type string. One common solution in a case like this is to give each component a local state and to let it handle its own interactions. However, our current front-end setup uses only functional components—functions that take props and return JSX components; no class ... extends React.Component . This choice made it easier to move to React for folks with less of a Javascript background. We were free from the this keyword and from ES6 class idiosyncrasies. Pure components were simpler to test. In addition, using redux-loop for our middleware means both state updates and side effects are triggered by the reducer. A typical reducer condition with redux-loop would look like this: We are therefore very incentivized to keep all changes funneled through the reducer. If it can change independently, bugs can be harder to track down and behavior harder to explain. For the last few years, the first approach for namespacing actions without local state has been to namespace the strings manually . This can take a few forms, for instance using the feature name as a namespace: This can be augmented or replaced by placing all your action constants in a single big file so that they cannot clash. Though in this case, it would be possible still to assign the same string to different constants in a big file, for example: This can be addressed by using a unique string to be sure that even if constant names are reused across files, the action is namespaced: Unfortunately, these are manual interventions. But what about cases where we wanted to add an arbitrary number of components to a page — say a shipping country select to each reward in a project, which could range from zero to nearly infinite — what then? In addition to eschewing class-based components and manual solutions, I was particularly interested in a solution that complemented the component architecture my feature team was working with—what we called amalgamated components . These were higher-level components that encapsulated a feature unit, like a payment form or a custom select element: something that would exist at the molecule or organism level in atomic CSS. It comprises the display component, which may take any number of event handlers and a wrapper component that takes state and dispatch and binds the default events. This way, if someone later down the road needs to use their own reducer and handler functions, they may grab the inner component, but in general, other teams instantiating the component should have a very low-surface-level API to work with. They would be able to import the component, its reducer, and its default state into the top-level file for the mount node, use combineReducers , and otherwise not have to fiddle with the component. The preferred solution would work with this approach and allow us to keep things as encapsulated as possible for easy instantiation. In this way, the first level of research allowed us to flesh out what requirements a successful solution would support. We needed a way to namespace actions that: Did not involve using local state Worked with combineReducers Could be applied programmatically Had a low–API surface area, which is to say, did not ask to much of others using the resultant component With these constraints in mind, we identified three solutions: nested reducers, higher-order reducers, and the module pattern, and wrote up an RFC for the front-end team to consider. The first solution we tried was to use nested reducers, which was inspired by the Elm architecture. At the time of the RFC, it had been implemented in a few locations in our codebase. As this example makes plain, however, this approach contravened the goal of simple instantiation. While it’s likely possible to write generator functions in order to avoid manual instantiation, the path to that is definitely not straightforward. Many members of our team also felt this approach seemed unneccesarily complex. Focusing on reducers that could be more straightforwardly be generated for namespacing, brought us to higher-order reducers . This approach centers on a reducer generator function that returns a reducer that only executes when called with a named action. It would be paired with a higher-order action creator: This solution hit the programmatic constraint pretty well, but broke down amid the low–API surface area desires. In the film Hidden Figures , there is a part where the mathematicians and engineers are struggling to figure out the best way to compute a trajectory and Katherine Johnson comes up with the solution by going back to “old” math. Though this is definitely a silly way to put it and almost certainly came from the pen of a screenwriter and not the mouth of a mathematician, it also aptly describes the final pattern we considered — a relic from old Javascript. The module pattern was a popular way of namespacing functions back in the “old” days of ES5 and worked by creating an immediately-invoked function expression (IIFE), which would use the power of closures to create functions that would not clash with one another. For instance, with this example counter, the variable num can be operated on by the functions in the returned object, but it will not clash in case the same name is used elsewhere in the code. Applying this to our problem brought us to this suggestion: which was very promising. It: Did not involve using local state Worked with combineReducers Could be applied programmatically Had a low–API surface area: it required only state and dispatch to be instantiated, like the non-namespaced version. The obvious downsides to this approach were the need to wrap the entire component in the scoping function and the reliance on string interpolation. The latter became more than a downside when we tried to apply the pattern within our concurrent TypeScript experiment. Namespacing the action strings interfered with the team’s ability to declare action types as a union type on the action’s string. But we forged ahead. And we succeeded! In the end, we settled on a solution that combined elements from higher-order reducers and the module pattern. Instead of using string interpolation, we added a namespace value to our actions and applied the higher-order reducer pattern. In terms of the module pattern from above, the change results in a module that looks like this: In order to mitigate the other drawback — being forced to work inside the closure—we added a set of utility functions to add the namespaces in. This way, a developer could implement the namespacing elements in her module as she saw fit. The only requirements were that reusable components should provide a namespacing function that could be called with a namespace string but otherwise would default to a uuid. This function would return: namespaced actions with the structure { type, namespace, ...otherPayload } a namespaced wrapper component that can be instantiated with state , dispatch , and optional configuration parameters a namespaced reducer that handles passing the namespace through all actions, even those returned by redux-loop the namespace itself (useful if users want to autogenerate namespaces but refer to them) In addition to this function, the component is expected to provide an initial state with appropriate defaults/blank state, suitable for being folded into the top-level state. The utilities also include an all-in-one module namespacing function that allows a developer to abstract namespacing to the factory functions: It is worth noting that the namespacing function provides for a number of arguments, not just the namespace itself, but also the component, reducer and actions. This was put in place to address cases where components might need to be wrapped before being passed to the namespacer, for instance a themed component. In that case, a component might be a curried function that takes its theme as the first argument and then returns the wrapper component. To namespace this, the themed component could be passed as part of the options, supplanting the usual version. The primary challenge we have encountered with this approach so far has been keeping actions namespaced as we pass them through reducers. Whenever the result of a dispatched action is to further dispatch other actions, those actions need to maintain their namespace. We’ve chosen to assure this by binding actions to reducers, usually through partial application on initialization: In a one-step initialization, as above, this works well. We have run into a few instances of complex composition, however, where the binding has gotten lost and caused errors. The other drawback to this approach is that instead of relying on shared scope for actions returned from promises, we need to pass the namespaced version as an argument. The verbosity is a reasonable trade-off here, though, since the handler code needs only to be written once, but components will be instantiated multiple times. And despite the kinks, we have found this approach to work successfully for most cases, including compound components that wrap a base reusable component with greater functionality, like a base uploader that can be wrapped to become an image uploader or a video uploader or a custom select that can have async option fetching also added in. One larger concern with this approach is that at some point there may be too many reducers and that will result in a performance hit. There are a few ways to approach mitigation here, from libraries that help single out subreducers to ignore to reconstituting reducers as hash-maps instead of case statements. But we don’t need to solve problems before we hit them, so we’ll be sticking with our adapted modules for now. thanks to Logan McDonald , Ryan Closner , and Will Duffy , for their help with this post & to David Peter and Will Duffy for investigating with me in the first place The engineers who build and run Kickstarter share a… 1.5K 6 Thanks to Ryan Closner , Logan McDonald , and Lisa Luo . React Redux JavaScript 1.5K claps 1.5K 6 Written by 💻 ➕ 🎨 The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by 💻 ➕ 🎨 The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-20"},
{"website": "Kickstarter", "title": "always be learning", "author": ["Jeremy Salfen"], "link": "https://kickstarter.engineering/always-be-learning-f4a496dc3085", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! One of our core values at Kickstarter is to “always be learning.” Employees here regularly hold teaching sessions — called “ABLs” — to share knowledge about anything from oil painting to functional programming. On the Data team, we’ve incorporated continual learning into our regular routines in a couple of different ways. We have monthly reading groups open to everyone at Kickstarter. These sessions tend to focus on a data-related paper or technique , but we’ve also read about and discussed issues like the ethics of algorithms in judicial sentencing . In addition to the monthly reading group, we’ve also done more focused studying in pairs, working through a textbook in a small group to gain a deeper understanding of a subject. This post will focus on our experience with the latter and offer some takeaways for those curious to try pair learning for themselves. Last year, two of us decided to work through Hadley Wickham’s inimitable textbook Advanced R . We had both been programming in R for a while but wanted to gain a deeper understanding of why it is the way it is. Every two weeks we’d read one chapter on our own. Then we’d meet for an hour and a half to discuss it and work through the exercises together. We might have absorbed more if we had spent more time on the exercises, but it would have made the process longer. We think it was an acceptable tradeoff. We just finished the last chapter a few weeks ago. We’ve been really happy with the experience, so we’d like to share some of what we learned about pair learning: It’s good to learn with another person. Having a study buddy can help motivate you and offer new perspectives on the subject at hand. Sometimes you just need to try explaining something to another person for it to click with you. Pace yourself and don’t bite off more than you can chew. The goal of covering only one chapter every two weeks seemed like a low bar at first, but we found that spreading out the work over time made it easier to stay engaged and finish the entire book, even longterm. Apply what you learn to a domain you care about. Toy examples in textbooks are useful for understanding how something works, but we found the underlying concepts easier to grasp when applied to Kickstarter data. Take notes on things you’ll likely want to remember or share with others. We created an internal R guide to document what we learned so the entire Data team could benefit from our studies. Here’s a brief snippet from that guide — a quick example of R’s approach to lazy evaluation: It’s totally fine if you don’t remember everything you’ve studied. We’re already a little hazy on the details of the rarer object-oriented systems in R (e.g., S4 and reference classes), but we know they exist, and we know where to go if we need a refresher. There are pros and cons to working as pairs rather than a larger group. One of the big downsides of pair work is that you miss out on other perspectives that you might otherwise get from more people. It’s also hard not to be inclusive if more folks want to join. On the other hand, a two-person meeting is much easier to schedule and keep on track. You also can’t slack off when it’s just the two of you — you’ve got to do the work and be ready to talk. We all come to our jobs with different experiences and baselines of knowledge. We learn new things from each other and on our own when our work calls for it, but sometimes it can be useful to strike out into the unknown and learn something new as a team. Working through a text with a partner or small group has helped us to get sharper at the skills we need for our jobs, but it also has spurred our intellectual curiosity and our hunger to always be expanding our minds. You should try it! Jeremy is Director of Data and Jeffrey is a Data Scientist at Kickstarter. If you’d like to learn more about our approach to continual learning or about how we use R at Kickstarter, you can find us at the New York R Conference in April. The engineers who build and run Kickstarter share a… 18 Programming Data Science Self Learning Data 18 claps 18 Written by Data at Atom Finance The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Data at Atom Finance The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-03"},
{"website": "Kickstarter", "title": "kickstarter kotlin", "author": ["Lisa Luo"], "link": "https://kickstarter.engineering/kickstarter-kotlin-ca8768ef8f8f", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! An exploration of our first three Kotlin classes in our Android app and how they were inspired by Swift. In the early phases of developing our Android app at Kickstarter, we poked around the Kotlin documentation and dreamed of ways we could use this new JVM language in our app. We were, however, a small team of new Java engineers with a hefty deliverable so our Kotlin dreams stayed in the pipes — the syntax was just too different from the Java 6 we were getting used to, and we had already challenged ourselves to master the RxJava framework to build a functional, testable app. One Android release and one Swift rewrite later, our native engineering team grew the wiser from embracing a cross-platform workflow. Kotlin began to look more familiar to us Android engineers after working in Swift as we had now experienced the benefits of a language with first-class functional programming support. Christopher Wright even took a crack at implementing functions from Learn You A Haskell For Great Good! in Kotlin and reveled at the power of the language. Our dreams of having Kotlin in the Android app started to become a tangible reality, and a year after the Android 1.0 release we introduced our first Kotlin class to our repo. Aside from the aforementioned first-class functional programming support (built-in lambdas, higher-order functions, operators), we felt nostalgic about Optional s, let s, and default parameter values when switching back to Java from Swift. Having safely unwrapped values and concisely declared immutable values not only soothed anxiety while attempting to write code with no side effects, but also sped up the time it took to review files e.g. not having to keep track of @Nullable values and if they were handled correctly via reviewing a pull request. Nope! As JetBrains intended, we have found Kotlin to interop quite nicely with our existing Android codebase. Our first Kotlin setup commit was painless and we have continued developing in Kotlin right in the mix with our Java files. We work now with the mindset of using Kotlin as we see fit to solve specific problems that Kotlin is better equipped to handle than Java 6. Let’s dive into the first three problems we were able to solve with our Kotlin caps on. The first problem I encountered when building a feature for Android — comments on project updates — was how to refactor our CommentsViewModel in such a way to be configured with either a Project or an Update model. We had comments enabled already for projects, and since the comments UI was the same we wanted to reuse the same ViewModel logic for updates. We solved this problem in Swift by creating an Either type, which represents a choice between a Left or a Right value of different types: a project or an update. We passed this projectOrUpdate Either type along to the ViewModel, which then used the Either class’s helper methods to process the data accordingly. Yep, this stank of Kotlin potential, so we added Either.kt as our first class, with tests. github.com The main takeaway from Either.kt was deciding to use a sealed class over a companion object for our tagged Left and Right union. This was important because we needed to restrict our Either type to take only a Left or a Right value, not both, and a sealed class prevented even the private ability to do so (h/t Stephen Celis ). Another nice benefit from writing Either in Kotlin was the ability to use higher-order functions to create useful operators, e.g. We were even able to take advantage of Kotlin’s when syntax here to make a beautifully expressive function. 💅🏽 How did we use this new Either type back in Java? Check out the diff here . The most practical and least intimidating use of Kotlin I found was adding a utils method with a default parameter. Scattered throughout our codebase were repetitions of this block, which creates and sets the Animation for our WebView loading indicators: This is quite a bit of repeated code that contains the same 300L magic number for animation duration. I know, we probably should have made a helper for these animations a while ago, but we can fix it now using Kotlin to provide us with an animation of a default duration, if otherwise not specified. The Kotlin nicety to notice here is the @JvmOverloads annotation which allows the Java compiler to see the method as essentially two methods: one with the default parameter, and one without. An easy win! Pull request here . We really missed first-class enum s from Swift — the type safety, the type inference, the speed. enum s in Java are also handy for types, but they are quite expensive and in most use cases their values can be implemented, with resource in mind, using static constants. Koala, our event tracking class for analytics, often provides a context with an event name to let us know from where an event was triggered. In Swift, using enum s for a new string type was a no-brainer since it provided type checking: How can we provide a similar implementation using Kotlin? Well, Kotlin has an enum class out of the box that has the type checking we crave for our context parameters. By nature I was skeptical about performance so I did some research and manual benchmarking to see how enum class performance would hold up to static class es in Java (the classic Java approach) and sealed class es (a more complex but maybe faster Kotlin approach; see “ Swift Enums are more powerful :” of this nifty article ). An example of a sealed class KoalaContext implementation is: An example of a static class KoalaContext implementation is: An example of an enum class KoalaContext implementation is: In the end, benchmarking via measuring the time it took to print each sealed, static, and enum Comments.Project context value 100_000 times resulted in the following: Yep, the sealed class approach was technically the winner, but we chose to favor readability and stuck with enum class — it’s faster than using static Java constants anyways in this case. For more detail and team discussion check out the pull request here . We are lucky to be a small team of functional programming believers who have the opportunity to work on an open sourced app day in and out. Whether you’re Kotlin-curious or a Kotlin-enthusiast follow along with or even participate in our open source repo as we continue adding all sorts of Kotlin fun ! If any of this sounds interesting to you, we’re hiring and I would love a new friend. The engineers who build and run Kickstarter share a… 138 2 Thanks to Stephen Celis and Brandon Williams . Kotlin Android Kickstarter Swift Functional Programming 138 claps 138 2 Written by software engineer, by day The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by software engineer, by day The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-19"},
{"website": "Kickstarter", "title": "event sourcing made simple", "author": ["Philippe Creux"], "link": "https://kickstarter.engineering/event-sourcing-made-simple-4a2625113224", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! tl;dr: Event Sourcing is to data what Git is to code. We’ve implemented a minimal event sourcing framework at Kickstarter to power d.rip . It’s simple and it has made our life so much better! Read on! Most software developers use a tool to keep track of code history. Git is a fantastic example that’s used widely across the industry. Type git log and you can see all the changes made to a codebase. Who made the change, when it happened, what the change was (that’s the commit title), why the change was made (that’s a good commit description) and how the change was performed (well, that’s the diff). Git is also a time machine, that allows you to go back in time and see what the code looked like back then ( git checkout @{12.days.ago} ). You can also replay history and play what-if scenarios: go back in time, checkout a new branch, commit a change, and replay all the events commits that happened after that. When something goes wrong, you can find how a bug happened and when it was introduced. And thanks to all that, you can generate useful reports: number of commits per month, hotspots… and superb visualizations: Think a second about how life would be (and was) without Source Version Control (git, svn, cvs, mercurial…). We would have to annotate files by hand, copy files to have some sort of backups and share code via… ftp? Looks painful. Not fun. Could you work without a tool managing code history ? Nope. Now, look at your database. Does your database manage data history ? Unless you’re using Datomic or libraries like Papertrail.rb , the answer is very likely to be: Sigh. The tedious hand crafted comments you see above are very similar to what we do to keep (some) data history. We add attributes like: updated_at , updated_by_user_id , accepted_at , destroyed_by_admin_id . We backup our database hourly. And even then it’s quite hard to know “how we got there”. “Why is this subscription marked as inactive here but active on the payment platform? The customer is still getting charged for it!” “Was this post re-published at some point?” “Which posts had the category we just deleted?” These questions could be answered in seconds if we had a full history. So in this post we’d like to talk about Event Sourcing . We’ll go over a high level introduction to Event Sourcing where we will highlight the four components that make a (minimal) Event Sourcing system: Events , Calculators , Aggregates and Reactors . We will then talk about how we implemented a (minimal) Event Sourcing Framework at Kickstarter for d.rip . And finally we’ll reflect a bit on the ah-ha moments and the challenges that we’re going through with this approach — 9 months after having started to work on d.rip and 4 months after launch. Martin Fowler defines Event Sourcing as: “All changes to an application state are stored as a sequence of events.” Let’s illustrate this with an imaginary e-commerce platform. User action, API calls, callbacks (webhooks), recurring cron jobs can all generate Events . Events are persisted and immutable. Here are some events generated as a customer placed an order on the platform: These events are this order’s history. We know when they happened, who triggered them and what they were. Note that the events above hold various pieces of information: product id, user id, order id, parcel tracking number, truck identifier etc. By going through these events, we get a sense of what the current state of the world is and how it came to be in that state. It would be nice not to play all events every time we want to build application state. That’s the role of Aggregates and Calculators . Aggregates represent the current state of the application. Calculators read events and update aggregates accordingly. In the diagram below, the little blue circle are calculators and the green sticky notes are aggregates. The calculator reads the sequence of events and updates the order accordingly: it adds and removes items, updates the total and marks the shipping and delivery dates. You can create as many aggregates (and calculators) as you need. For example, an aggregate could read through the same set of events to generate a Daily Sales report. Now that we have the current state of the world, we also want to do things when that state changes. Like it would be sweet to send our customer an email confirmation when their order has just been shipped. We need something to “react” to events. Good news, there is such a thing. It’s a Reactor . Reactors “react” to events as they are created. They trigger side-effects and might create other events in turn. The reactor on the right hand side listens to the “Shipped” event. Whenever a “Shipped” event is created, it sends an email notification to the customer. The reactor on the left hand side has a local state. Whenever a Cart has two articles, it displays a promotional offer and creates an event to keep track of this. This is actually where the “Promo displayed” event comes from. So those are the four components of a minimal event sourcing system: Events to provide a history Aggregates to represent the current state of the application Calculator to update the state of the application Reactors to trigger side effects as events happen Having a full history of the events is one of the main benefits. We can know how we got there which helps with a lot of customer support tasks and debugging sessions. Being able to Replay Events unlocks very neat features. You can go back in time by replaying all events up to a certain point. Replay all events up until Oct 31st… and you get what the application state was on Halloween Day. Spooky! All software has bugs. So when a calculator has a bug, you can fix the calculator, replay the events and get back to a valid state. Finally, adding columns to an aggregate and backfilling the data is quite simple: - 1. Add the column. - 2. Update the calculator. - 3. Replay the events. - 4. The data is backfilled! On a “regular” relational database, the data you store is the data you read. With event sourcing, the data you write (events) is decoupled from data you read (aggregates). So you can design your aggregates for the current needs of the application. Not having to “future-proof” aggregates for eventual future usage and data needs is quite nice — and avoids a lot of “gut feeling based debates”. Aggregates can also be optimized for various usages which comes in handy in read-intensive applications: orders’ summary (for list views), orders’ details (to display one order), orders’ daily reports (for business), etc. You could basically get to the point where your aggregate fields match one to one your UI or report fields. That. Is. Fast reads! And finally, Event Sourcing is a great pattern for distributed systems that tend to be asynchronous and have various services or serverless functions. Services can listen to events they are interested in to update their local state, perform actions and publish other events in turn. Drip is a platform to support creators’ practice. Creators publish content (comics, podcasts, behind the scene videos, etc…) that supporters get access to by subscribing to the creator’s Drip. We launched the first version of Drip on November 15th — roughly 6 months after the first line of code was being written. The Back-end is a Ruby on Rails application offering a GraphQL API. The front-end is React based. We were a couple of engineers to suggest that we experiment with “Event Sourcing” when we started to work on Drip. It was pretty easy to convince the rest of the team to give it a try since it would address a lot of the pain points that most apps (including Kickstarter) run into after a couple of years (or months) of existence. The deadline was pretty tight (6 months to launch) so the Event Sourcing experiment had the following requirements: It should not slow down development (too much) It should be quick for an engineer to learn the concept and be proficient If the experiment fails, it should be easy to rip-out and rollback to a regular Rails / MVC pattern. Based on those requirements, we decided to make the Event Sourcing framework an implementation detail of the back-end. The event sourcing implementation is not surfaced to GraphQL. The client application consuming the GraphQL API is not aware there is some Event Sourcing going on behind the scene. We wanted the Aggregates to be regular ActiveRecord models that follow patterns that you’d find on a regular Rails application. This way, we could remove the Event Sourcing framework altogether and replace it with in-place data mutation: create!, update! and destroy! calls. We looked at various Event Sourcing frameworks written in Ruby but most of them were actually too complex for our needs or would store data in a way that was too different from your regular Rails app. So we decided to build our own minimal framework. It’s about 200 lines of code. And it’s been good enough so far. Aggregates and Events are stored in a relational database. Each Aggregate (ex: subscriptions) has an Event table associated to it (ex: subscription_events). Events are created and applied to the aggregate synchronously in a SQL transaction. We avoid situations where events only partly applied and we don’t have to deal with the complexity that asynchronicity introduces. Relying on database transactions to keep the data consistent requires almost no effort on our part. All Reactors respond to the call method and take an event as an argument. The Dispatcher connects Reactors to Events . Let’s talk about the Subscription model. When a user subscribes to a Drip, we create a Subscription. Sample content: Notice that this model and its attributes are very similar to models you would come across in any Rails application. The only difference is that we have access to the history via has_many :events . All events related to an aggregate are stored in the same table. All events tables have a similar schema: We rely on ActiveRecord’s Single Table Inheritance mechanism to store all the events related to the Aggregate in the same table. Active Record stores the event classname in the type column. Being specific to each event, event data and metadata are stored as json. Below is the “Subscription Activated” event. Like all events related to “Subscriptions” it inherits from the “Subscription Base Event” . data_attributes defines setters and getters for the attributes passed in. They will all get stored in the data column. The apply method is the actual Calculator for this event. Most calculators are embedded into the event code to simplify things. A couple of events delegate apply to external calculators when the calculation is complex (international taxes, I’m looking at you!). apply takes an aggregate and applies changes to it. You might notice that activated_at is set to the event creation time — not the current time. That’s because we don’t want that timestamp to change when we replay events. Replaying events should be idempotent. As a rule of thumb, the calculator ( apply ) should only use constants (here: “active”) or attributes defined on the event ( stripe_key and created_at ) and events should embed all the information necessary to update aggregates. Below are the entries for “Subscription Created” and “Subscription Activated” events: Looking at the metadata, you might guess that the “Created” event is triggered by a user while the “Activated” event comes from a webhook notification. When an event is created it is automagically applied to the associated aggregate. The following would create the events and update the aggregate: Here are two reactors that react to the “Subscription Activated” event. They both queue up an email for delivery. The first one sends a confirmation email to the subscriber, the second one a notification email to the creator. We subscribe reactors to events in the Dispatcher. Most reactors are triggered asynchronously (notice the async keyword above) and a couple of reactors are triggered synchronously using trigger: instead of async: . We tend to run synchronously reactors triggering events that update related records. For example, only one post can be pinned at a time. On “Post Pinned” the dispatcher triggers a reactor that will unpin any other pinned posts by creating a “Post Unpinned” event. We want all those changes to happen atomically to keep things simple and consistent. While not part of the mechanics of an Event Sourcing framework, on Drip we use an additional layer called “ Commands ”. They are responsible for: Validating attributes Validating that the action can be performed given the current state of the application Building and persisting the event Below is a command that activates a subscription. It includes the “Command” mixin which provides some validation capabilities, syntactic sugars to define attributes and default behavior. The command above will be a noop (it won’t create an event) if the subscription is already activated. It will raise an exception ( ActiveModel::ValidationError ) if the stripe_key is missing. Commands are triggered via call : Drip is currently in Public Beta. As of April 2018, we’ve invited 85+ creators to the platform that are supported by 7000+ active subscribers. Code wise, we have: 12 Aggregates 90 Events 35 Reactors 50 Commands And data wise: 25,000+ aggregates 150,000+ events Replaying events is awesome! Whether we replay events to add a column to an aggregate, fix a bug in a calculator or restore deleted aggregates (yes!) it always feels magical and powerful. No need to write a custom script to backfill or fix your data. Just update the calculator, replay events, and you’re done! You’re in a safe place where you cannot lose data. It’s like when you delete code or files and know that you’ll be able to get that content back anytime if needs be. You get reporting and charting for (almost) free. All the codebases I’ve worked on are cluttered with code that sends hand crafted events to an event bus or a third party service like Google Analytics, Mixpanel, Intercom, etc. It’s tedious to maintain, often inconsistent, not tested and you need to add more and more event tracking as the application gets more mature. Events being a first class citizen in event sourcing, you can create one Reactor to forward them all to your favorite analytic platform(s). Obviously understanding “how we get here” by looking at the history makes tracking bugs a breeze and helps tremendously the customer success team. We also thought that versioning events would be hard. So far, we’ve only had to add new attributes to events. When that happens, there are two scenarios: Either the attribute value was “implicit” before it was added. For example, if the “currency” attribute is not defined on an old record of an event, we can assume it’s “USD”. If there is no “implicit” value (ex: subscriber country), you can persist “backfilling” events (“CountryGuessedForBackfilling”) that use various data sources to guess the country (e.g. user address, credit card company, etc) Naming is hard. And there are so many immutable events and attributes to name. The names you choose now will be the ones stored forever. So take a good dictionary and make sure that you nail down names that are explicit and future-proof. Destructuring one action (GraphQL mutation in our case) into multiple commands and events is quite complex. It is actually the most complex part of the system. There are lots of combinations, so we (should) rely on generative testing to ensure that all combinations result in valid states. Take the mutation to update a post. All the attributes are optional, so you can call or The first call should only update the title. The second one should only publish the post. Why? Because the title and the description are unchanged. They have the same value as the ones persisted in the database. Here is (a subset) of the attributes, commands and events that the updatePost mutation is destructured into: We put together a simple implementation of the Event Sourcing framework. There are 4 components : - Aggregate (regular Active Record models) - Events - Calculator (built into events) - Reactors The data is persisted on a regular SQL database . Updates are Synchronous and Atomic . Home-made “framework” (about 200 line of code). Yet, it brings a lot of value. Full History , Audit Log Updating aggregates and backfilling data is easy Fixing bugs is easier There is less risk of “losing” data Events can be sent to your favorite analytic platforms with (almost) no additional code So, is it like git for data ? Pretty much yeah! We definitely encourage you to evaluate event sourcing for your next app, next feature or any existing feature that’s mission critical. Our Event Sourcing implementation is available on Github for educational purpose . Original presentation “Event Sourcing Made Simple” given at Conf & Coffee, in Vancouver BC on April 15 can be found there . Recording coming soon. Martin Fowler gave an excellent talk where he highlights that Event Sourcing implementations don’t need to be asynchronous. It made us feel good about putting together such a simple implementation. We encourage you to look at the Ruby frameworks that we’ve evaluated. They were a great source of inspiration and they might fit your needs better than that 200 lines long gist. Event Sourced Record , Rails Event Store , Sandthorn and Sequent . Thanks to Natacha, Amy, JJ, Brian and Mark for their feedback and special thanks to Janel meticulously reviewing this post. 💚 The engineers who build and run Kickstarter share a… 4K 16 Thanks to Janel diBiccari . Software Architecture Event Sourcing Ruby Rails Data 4K claps 4K 16 Written by Healthy Ruby & Rails @kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Healthy Ruby & Rails @kickstarter The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-02"},
{"website": "Kickstarter", "title": "upgrading kickstarter to rails 5", "author": ["Logan McDonald"], "link": "https://kickstarter.engineering/upgrading-kickstarter-to-rails-5-e8203f93df55", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Fall is here! Change is in the air. And as of last month, Kickstarter runs its two biggest applications on Rails 5: our payments app, Rosie, and our main app, Kickstarter. In this post, we’ll concentrate on the Kickstarter Rails 5 upgrade. We update our apps because we want to ensure the security and performance of our site — but getting to the next major Rails version on a nine-year-old legacy application was no easy feat. We’d like to share some of the lessons learned during this upgrade by breaking down our upgrade process and discussing some of the issues we ran into along the way. First, we’ll discuss the planning and scoping by taking a look at how we defined success and how we organized the team. Then, we’ll walk through the twelve steps that got us to Rails 5. Timeline At the outset, we scoped about three months for this upgrade. We’re happy to report that we successfully hit that mark. Team #upgraderz This project was scoped for one person to lead and implement, but included invaluable help across our Ops, Platform, and other engineering teams. Across upgrades, we refer to ourselves as the #upgraderz and have a Github team and Slack channel to communicate any issues that arise during the upgrade. Without this team of #upgraderz, a major app upgrade would not have been possible. Success metrics During the upgrade process, we upgraded Rails versions for both our payments and Kickstarter apps, as well as the Ruby version of the Kickstarter app. We based success metrics on: A timely upgrade Limited downtime due to 4.2-compatible upgrades Limited downtime due to the upgrade itself Cross-training and education Process We’ve broken our process down into twelve steps to replicate on future upgrades. The first step we took was to try to release everything but the major version. We upgraded Rails from 4.1.x to 4.2.x back in April 2016 and developed a good team that understood some of the potential challenges we might run into when upgrading to Rails 5. Then, over the summer, we kept up with minor patch upgrades. Before tackling 5, we upgraded Rails 4.2.8 to 4.2.9. Finally, we took this opportunity to upgrade Ruby as well. At the start of the project, we took about half a week to upgrade Ruby 2.2.5 to 2.4.x. The benefits of this were twofold. First, gem dependencies would sometimes bump to a version that supported Ruby 2.4 and Rails 5, and if we didn’t have both, this would be problematic. Secondly, this upgrade provided good practice for releasing a new version. One lesson we learned, for example, was to always upgrade to the next minor patch. We learned this after upgrading to 2.4.0 instead of the most recent patch, which resulted in a developer struggling for a day on a bug that ended up being due to random errors in MRI 2.4.0 caused by def-delegators . Lesson learned! Both the Rails upgrade guide and release notes are well documented. This allowed us to start building a good project outline. We took the release notes for each class and broke them down into three categories — non-master–compatible change, master-compatible change, and deprecation change — based on our knowledge of the codebase and reading of the upgrade docs. Careful documentation of our work throughout this process was integral to its success. We kept a master Trello card on the Ops Engineering board with tasks broken into categories: dependency upgrades, master-compatible upgrades, non-master–compatible upgrades, and non-necessary upgrade TODOs. We also carefully labeled pull requests associated with the upgrade into pre-rails5 , rails5 , and post-rails5 . For bigger changes necessary in the upgrade but not compatible in master, we opened pull requests off the Rails 5 branch itself. After documenting our way forward, we tried to just install the gem! We added the new version to the Gemfile and… immediately ran into a million dependency issues. Not only were there gem changes for dependencies where we had to track down changelogs, but we had to open many PRs in our custom gems or gems we relied on to make them compatible. It took a week just to get the Rails 5 gem properly installed with no dependency errors. In the process, we went through our entire gem list and read every single changelog to make sure we were on the best possible version. Ready4Rails was a really helpful site for navigating these gem upgrades, but any Rails developer will know that a nine-year-old Rails app’s Gemfile can be a treasure trove of mysterious custom dependency conflicts. Once we’d upgraded the gem, we were able to run Rails’ handy upgrade script , which gave us a diff of all the Rails 5 config changes. We opened an initial PR against our Rails 5 branch for the team to review, since many of these changes would be the most central to our infrastructure and being aware of any challenges early on would be helpful. Merge any low-hanging fruit in master. This is all the stuff that got missed in previous upgrades or easy fixes for deprecation warnings in Rails 5. Things like switching to assert_nothing_raised in Rails 5 were easy and digestible fixes. This was the longest part of the process. After merging all low-hanging fruit to master, we embarked on turning our CI green. We took a snapshot of code coverage before tackling these failures to give us a sense of what making the tests pass would get us. We started with nearly 4,000 unit and integration test failures in our CI pipeline with a code coverage of about 70 percent. This was really helpful for knowing how confident we could be in our test suite. Again, we approached this task with solid documentation. Many of the test failures could be grouped together and patterns identified. We kept track of all the moving test failures and which test fixes created even more failures. This documentation process took the form of namespacing PRs (i.e. pre-rails5 ) and spreadsheets sent to the team. We tried to treat fixing tests as a good opportunity for refactors as well. If we found problematic tests or logic, we took a little extra time to alleviate this tech debt along the way. Once the test suite was completely green, we looked solely at the changelog for deprecated features of the Rails API and tried to fix as many of those as possible. We also used our unit tests to surface a lot of these. Some of them are hard to miss. For example, the new Rails 5 controller tests use kwargs in ActionController::TestCase and ActionDispatch::Integration HTTP methods . This means that every line that had an HTTP request spit out an error. There are thousands of these across hundreds of files in our test suite. This was definitely the most tedious step in the process, but it was incredibly rewarding to see clean test runs. After the first six steps, we were about halfway done with the process. Since tests do not cover everything and also often mock out critical flows, we started to smoke test some of the major flows and ran into several problems. Over the last nine years, we’ve developed quite a few patches and “creative” fixes that rely on Rails logic that is subject to change. We tried to backport as much as we could, including Rails 5-specific patches we wanted input on. We did this using tags and namespaces to the Rails 5 major version. This allowed us to stick with incremental changes and debug any challenges that came up before the entire gem was updated in master. We merged over thirty pull requests to master before even opening the Rails 5 pull request. Once there was nothing else to merge to master, we opened a PR and heavily documented it for reviewers. We included documentation about why the change needed to be made, as well as a justification for why it couldn’t be made in master. In the end, the change was 300 files across our app code — a sizable change. However, most of the changes were due to test files that needed to be updated because of the keyword arguments change. By breaking down the context of the change, we were able to specifically direct reviewers to a subset of the files to review. In the weeks before we opened the PR, we reached out to various feature teams and asked for a representative to help review it. We recruited them as smoke testers and reviewers to look through the site and test the major flows that they were the most familiar with. When we opened the PR, we charged these engineers with specific responsibilities, ensuring that the PR wouldn’t sit open indefinitely. Finally, when we were confident in our flows and test suite, we decided to take a day when no one was deploying and deploy our branch to staging. Our staging is slightly different from our development environments and deployed development environments in a couple of ways. First, it connects directly to a sandbox payments system. We were getting a lot of false timeouts on payments deployment development environments that went away in the staging environment. Secondly, our staging config is much closer to our production config than the development config. Another successful idea was to deploy to staging on the Sunday before we deployed to production. We were able to get out of the way of developers and were free to test things on staging without disrupting the development workflow. Finally, deploying to staging allowed us to practice the worst case scenario: a rollback. The ability to perform a rollback to a previous Rails version illuminated a critical flaw. We had forgotten to change the cache version for Rails 5 to avoid Marshal conflicts. Ecstatic that we caught this early, we were able to implement a fix for production and practice the rollback on staging successfully. The Monday morning after our Sunday staging deploy, we merged and deployed Rails 5 to production. We ensured Rails subject matter experts were around in case anything broke. Immediately after the deploy, things started to break. We quickly triaged the things that were breaking and made calls on what needed to be fixed and in what order. Our careful planning and contingency plans allowed us to work calmly in the face of these issues. Our cross-training and preparedness allowed us to spot issues quickly and identify fixes on the spot. Overall, we suffered no major downtime and very few user-facing issues. After a successful merge and deploy, we continued to keep track of any issues that arose and worked closely with our community support team and support engineers to ensure that any Rails 5-related bugs were escalated to the team as quickly as possible. We concluded the project with a Project Retrospective with the entire engineering team, as well as a representative from community support, to discuss issues and reflect on this process. In this retrospective, we revisited our success metrics as a team: A timely upgrade Success! We hit our three-month timeline. Limited downtime due to 4.2-compatible upgrades Minor outages, but handled with no rollbacks. The worst bug was probably upgrading to Ruby 2.4.0 rather than the most recent patch. Limited downtime due to the upgrade itself We did not have to roll back any part of the upgrade, but we did experience some problems with untested bugs, which resulted in a spike in Zendesk tickets. We could have done better on this metric by developing a better plan for triaging the most important user-facing bugs as they came in. Cross-training and education The team learned a ton, and having the retrospective at the end ensured that we carried lessons learned over to the next upgrade. While we tried to be as thorough as possible in fixing bugs before merging Rails 5, there are always going to be things you don’t anticipate. The key is documentation and planning. The biggest problem we experienced throughout the process was with a relatively straightforward change in the release notes: ActionController::Parameters no longer inherits from HashWithIndifferentAccess . Our reliance on ActionController::Parameters inheriting from a Hash is incredibly widespread in ways that were hard to systematically track down without reading each file. Fortunately, unit, integration, and smoke tests caught most of them. But there were production-specific issues like tracking events that were unknown unknowns to us. The day of deployment, this was definitely our most pervasive remaining issue. Another unknown unknown was how our API changes were going to impact our mobile applications. While we had extensively smoke tested flows on our web application, we did not have any member of the native team on our #upgraderz team. In hindsight, this was a mistake. Because the leaders of the project were less familiar with the native applications, it was not prioritized. While only a few small bugs specifically impacted mobile applications this time, next time we will prioritize getting input from members across the engineering team. We are already behind where we could be: 5.1 is here! 5.2 is coming! However, we feel we are in a good place for now and are proud of our efficient ascent to this next major version of our app. I learned a ton during this project and could not be more proud of our team. While we did spend many days hitting our heads against technical walls trying to dream up ways around various bugs, our countless hours of planning, cross-team communication, and documentation made this project a great success. Overall, our biggest piece of advice to anyone looking to upgrade a legacy application is to take that into consideration early on and actively communicate with stakeholders during all phases of the project. I hope you enjoyed and learned something from this post. If you have any questions, don’t hesitate to reach out. Always be upgrading! The engineers who build and run Kickstarter share a… 776 5 Rails 5 Ruby on Rails Upgrading 776 claps 776 5 Written by Site Reliability Engineer @buzzfeedexp The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Site Reliability Engineer @buzzfeedexp The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-06"},
{"website": "Kickstarter", "title": "open sourcing our android and ios apps", "author": ["Brandon Williams"], "link": "https://kickstarter.engineering/open-sourcing-our-android-and-ios-apps-6891be909fcd", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Today the Kickstarter engineering team is open sourcing our Android and iOS apps, and we’re excited about a future of working in the open. The native team at Kickstarter is responsible for building and maintaining features for Android and iOS. We focus on writing well-tested code built with parts that can be easily understood on their own, and to do that we’ve adopted many functional programming techniques. There have been a ton of benefits to this, some of which we couldn’t have predicted at the start. We are excited to share these findings with the developer community. We started by organizing the Functional Swift Conference . We hosted the first two conferences in 2014 and 2015 at the Kickstarter office, and in 2016 it moved to Budapest. We wanted to give a space for Swift developers, new and old to functional programming, to come together and share ideas. Building on the conference, we wanted to take our sharing a step further. The idea of open sourcing our apps came up around the time that Kickstarter became a Public Benefit Corporation , putting a commitment to the public good at the core of its company charter . We started thinking about how we could embrace that principle as engineers. We believe that open sourcing will not only give a new level of transparency into our engineering culture, but will also provide useful resources to the global developer community. It would be impossible for any organization to open source their iOS application without mentioning the monumental contribution Artsy has made in this area. We were very inspired by Artsy’s open sourcing of Eigen and their open-by-default policy in general. They have provided the community with a great service, and we are thankful for the initiatives they have taken. Here’s a quick glance at a few things we’re particularly proud of: The Screenshots directory holds nearly 500 screenshots of various screens in every language, device and edge-case state. For example, a backer viewing a project in French here , or a creator looking at their dashboard in German and on an iPad here . We use view models as a lightweight way to isolate side effects and embrace a functional core. We write these as a pure mapping of input signals to output signals, and test them heavily, including tests for localization, accessibility, and event tracking. We use Swift Playgrounds for iterative development and styling . Most major screens in the app get a corresponding playground where we can see a wide variety of devices, languages, and data in real time. Browse our collection of playgrounds here . Our first two pull requests on the open source repositories: updating the design of the backer activity feed and updating the design and functionality of the user profile. The native engineering team at Kickstarter is coming off a hectic period of development. A year-and-a-half ago, we were only two iOS engineers: Brandon and Gina, who worked on a four-year-old Objective-C codebase, the majority of which was written by Brandon in the early days of Kickstarter. We didn’t even have an Android app . One winter day, Chris, a backend engineer, expressed interest in building the Android app. He moved over to work on it full-time. Along the way, we hired Lisa, a former summer intern, to help out with Android. Brandon and Gina also jumped in and made great contributions to the project. In less than eight months, four engineers who had never written production Java code, let alone production Android code, shipped a 1.0. It was a grueling but rewarding experience. The final product was well-received, stable, and a great way for our backers to explore projects. One of the tools we employed from the start in our Java code was functional programming. Regardless of how difficult it was to write truly functional code in Java, we knew that if we embraced immutable data structures and pure functions to the best of our ability, we would be able to avoid a lot of complexity that creeps into applications. We were naturally led to using RxJava for wrangling UI interactions in a declarative fashion, and we came up with a simplified way of separating the pure logic from the side-effects in our code, very much in the spirit of the Functional Core, Imperative Shell . The result was a highly testable code base most of which could be fully understood, in isolation, with no concern for how it interacted with the rest of the system. We were quite proud of our code! But once the dust settled from releasing Kickstarter for Android 1.0, we were left with the sinking feeling that we still had this — now four-year-old — Objective-C codebase to poke back alive. Luckily, our team was quite familiar with Swift and where the community was headed. Brandon had been co-organizing the Functional Swift Conference and Gina co-hosts the Brooklyn Swift Meetup . We not only had extensive knowledge of how we could leverage Swift’s type system to better express the problems we wanted to model, but also, with our Android experience, had the knowledge of how to build an application, from ground up, in a way that allowed us to piece together functional parts with side-effects at the boundaries. Thus, we embarked on yet another formidable journey: rewriting our iOS app in Swift and creating something that the entire team could share in ownership. We amplified all of the things we learned from our work on Android, many times over. We had an unwavering eye on writing as much pure functional code as possible, writing it in such a way that enhanced testability, and seeing what new things it would unlock for us at every turn. We were so excited by these developments that we gave two separate talks at this year’s Functional Swift Conference about the cool things we were doing ( here and here ). As a native engineering team, we often look back at functional programming as the most important tool that empowered us to work the way we do. We were a team of mixed skills, backgrounds and experience levels, and we became a team that could write understandable, well-tested code on both Android and iOS platforms. We found a common philosophical basis of development through functional programming. Most importantly, we tried not to over-identify ourselves as engineers of a specific platform, but rather look at software development as a more holistic discipline. There’s no “I” in open source software. This would not have been possible without the efforts of the entire native squad: Brandon , Chris , Christella , Courtney , Katie, Gina , Lisa , Maggie, Stephen . We’re excited be working in the open and able to share the cool things we come up with more freely, and the challenges we face more honestly. There are quite a few things we look forward to in 2017, including writing our first Kotlin code in Android, exploring new ways of writing declarative views and, of course, the big Swift 3 migration. If any of this sounds interesting to you, we’re hiring ! Sincerely, The Kickstarter Native Team The engineers who build and run Kickstarter share a… 914 13 iOS Swift Open Source Android Functional Programming 914 claps 914 13 Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Functional Believer. Previously lead iOS and Android @Kickstarter. Working on https://www.pointfree.co The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-14"},
{"website": "Kickstarter", "title": "leveraging functional programming aws lambda to drive chatops", "author": ["Logan McDonald"], "link": "https://kickstarter.engineering/leveraging-functional-programming-aws-lambda-to-drive-chatops-4b269558d3fb", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! In Operations Engineering at Kickstarter, we aim to ensure the health and sustained development of our website. A large part of that is making sure our engineers are getting the right information in a timely matter. For this reason we have started to embrace ChatOps. In this post, I will detail how Kickstarter is leveraging our use of AWS Lambda and embracing functional programming to drive our ChatOps infrastructure, and through that infrastructure drive development through conversation. Our developers often use AWS CloudFormation templates . We had the idea to use these templates to spin up infrastructure and then send the events as those resources are created into Slack, where team members could watch and talk about their progress. We also rely on alerts about our RDS instances. In fact, we have many different alerts we’d like to be informed about and discuss in real time. What if every time one of these alerts were triggered, they magically appeared in dedicated Slack channels where our developers could discuss them? That was the problem we set out to solve. ChatOps can be thought of as conversation-driven development. Chat rooms that use ChatOps can either be a combination of tools and humans talking, or they can be “bot-only” rooms that provide convenient logs of information. As a log, ChatOps can also help coworkers see what commands have been run recently that impact infrastructure and watch the result of that command together. This is how the desired ChatOps flow for CloudFormation notifications in Slack works: a user creates, updates, or deletes a CloudFormation stack that is configured with the SNS role for our function. If the stack’s assigned policy is configured to that notification role, a function subscribed to this notification will be triggered. When the function hears the incoming event, it processes information and sends a POST request to the Slack webhook for the appropriate channel (in this case, #cloudformation-events). Then the Slack webhook starts receiving requests and posting to the channel where users now see logs of the impact of the event on their stack. So how do we achieve the magic above with as simple and flexible code as possible? We do it “serverless,” using AWS Lambda and pure functional programming (or, at least, as pure as we can get). The serverless movement attempts to abstract users away from servers and infrastructure. Of course, serverless infrastructure is a bit of an oxymoron. The point of the movement is to focus on the compute time and logs for single functions or processes, without thinking about the server at large. While many people are hopeful for a future where all infrastructure is serverless, we are starting with a few components of our infrastructure that can be broken out into functions. There are many single-purpose services where we could apply serverless infrastructure. One of those is sending alerts through chat. Lambda is an AWS service that launched at the end of 2014. Through Lambda, we can define functions that perform small operations and have AWS provisions and run them. When thinking about whether a certain task is fit for Lambda, a person should ask themselves if what they need is a simple one-off function that will reliably run in the same way and use the same small amount of resources every time. In the same way we use AWS S3 for asset uploads or auth management services for our credentials, we can use Lambda to manage our functions. For us, that means functions to fetch and post data. Lambdas are simple: you define a function that performs an operation, and AWS provisions and schedules that function. Here is how we organize our function for ChatOps: first, we define the function and its dependencies in function.json (necessary for our deployment process) and package.json, respectively. We control the crux of our function in an index file in the root. You can contain all the code for your function in this file. There are a couple ways to deploy your Lambda. We use a service called Apex , which allows us to deploy from the command line easily and offers several options for environment variables. You can also deploy your function code straight from the AWS console. Once we have our Lambda configuration setup, we can start to break up our code, test it, and make it efficient. Functional programming is a prime candidate for this. Before joining the Ops team, I had never used functional Javascript or AWS Lambda. Coming at them simultaneously worked incredibly well for my brain. Functional programming is based on the premise of computation of data. Data comes in, data flows out, there are no side effects. This incorporates well with Lambda’s push/pull model. An event producer directly calls the function and pulls the update from the data stream to invoke the function. Pure functions and immutable variables also allow for lower complexity of programs. Here is a part of our function that formats the incoming data: As the data flows through, we apply filters and maps to functions that format and push the data to the next function. The concept of using functional programming to process events seems natural. The definition of a pure function is a function that, given the same parameters, will always return the same result. So when we have technology that processes incoming events (either triggered or scheduled) over and over, functional programming is a perfect fit. Testing is necessary for any function! However, when depending on calls to webhooks like Slack, it can be difficult. In order to test our various functions, we can use dependency injection. Instead of using hardcoded dependencies, we can pass the dependencies (mocked out) into our module. That makes testing much easier. For example, we don’t have to deal with this POST to the webhook repeatedly by doing this: Then in defaultDependencies, we simply require the postToSlack method: Now we can test the other functions without worrying about implicit execution of external code. Another benefit to this is that the postToSlack becomes agnostic to which channels or what data it is posting. We can thus send payloads with different information for different channels. Using one function, we can process CloudFormation events, RDS events, and many more without duplicating any code. It’s simple and, given the same input, guaranteed to work the same way every time. There are several steps to moving forward with ChatOps. We are currently working on the end-to-end user experience for ChatOps so it is fluid with our use of Cog by Operable. For more information about what we are doing with Cog, check out Operations Manager Aaron Suggs’ conversation with Operable from last month. We are also working on adding tagged alerts so that only developers in charge of certain infrastructure get alerted in these channels. Finally, we are looking for more ways to embrace functional programming throughout our alerting systems and code bases. The engineers who build and run Kickstarter share a… 16 2 Thanks to Brandon Williams , Aaron Suggs , and Natacha Springer . AWS Serverless DevOps Functional Programming 16 claps 16 2 Written by Site Reliability Engineer @buzzfeedexp The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Site Reliability Engineer @buzzfeedexp The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-14"},
{"website": "Kickstarter", "title": "the relaunch of our engineering blog", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/the-relaunch-of-our-engineering-blog-2f29be89f453", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! We, the Kickstarter engineering team, are happy to announce our latest migration: our engineering blog, from self-hosted to Medium. In 2013 we launched our blog to share our creativity, our mistakes, and our open source projects. We’ve had many great posts and discussions thus far, and today we move to Medium to engage more deeply with the community. For individuals and engineering teams alike, Medium provides a clean and easy way to publish, read, and share ideas. We are excited to make our new home as the Kickstarter Engineering blog, formerly known as Backing and Hacking. Our old posts have made it here safe and sound, and we cannot wait to continue sharing and engaging with you. In fact, here are two new posts for you already! Read about our journey to open sourcing our native apps and how we leverage functional programming and AWS lambda to drive ChatOps . The engineers who build and run Kickstarter share a… 28 Thanks to Lisa Luo and Stephanie Coleman . Engineering AWS Blog Open Source 28 claps 28 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-14"},
{"website": "Kickstarter", "title": "the value of reliable developer tooling", "author": ["Aaron Suggs"], "link": "https://kickstarter.engineering/the-value-of-reliable-developer-tooling-e94791d1482e", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Kickstarter recently rewrote the scripts we use to manage local development environments with a focus on improving their reliability. I want to share the benefits of that project. The main Kickstarter app has several system dependencies. Dozens of people work in the codebase each day. We had a script/bootstrap that theoretically managed all the dependencies and configuration to get a working development environment. But in practice it was neglected, confusing to use, and rarely fixed the problem blocking a developer. In my past as a sysadmin, I often installed software by running ./configure && make && sudo make install . Those familiar with this process are used to having thousands of lines of logs fly by like a Hollywood hacking montage. When the process finishes, it’s not always clear if it succeeded or if you need to scroll up to the cryptic error message you’ll need to google. That’s how our old script/bootstrap was subconsciously designed. I thought it was helpful to be so verbose! In retrospect, this was quite wrong. Having too much detail made problems harder to solve: team members would paste lines of output that they thought included the salient error but actually didn’t. People avoided running script/bootstrap because they worried it would break more things. That skepticism was understandable because the output didn’t make it clear what the script was doing. As we embarked on a mission to improve developer happiness and productivity with a next-gen bootstrap script, we had a few goals in mind: Make it fast : we want people to be comfortable running this as part of their daily workflow. Make it debuggable : if there’s a problem, point people in the right direction so it’s easier to address it. Make it reliable : running it should result in a working development environment the vast majority of the time. The core of our solution was the kickstarter/laptop script to install common dependencies across Kickstarter projects. While the script is not designed to be useful outside Kickstarter, we kept it open source to acknowledge the Thoughtbot script it’s based on and to serve as an example for others. We use the laptop script within particular Kickstarter projects by calling it from a bin/dev script. These bin/dev scripts can also perform additional project-specific bootstrapping. These new scripts have been quite successful, slashing the amount of time developers spend debugging problems with their local environments. Here’s what it looks like to run bin/dev on our main codebase: We printed the timestamp for ambient performance context. By making it easy to see how long each step takes, it keeps our focus on making the script fast for developer productivity. The whole script took 12 seconds. Here’s what it looks like when the script crashes: When things inevitably break, we give several pointers for what to try next next. The reset option clears caches and re-installs some historically flaky tools. bash -x is a nice trick to get verbose output that’s helpful for particularly gnarly edge cases. We also point you to the appropriate Slack channel for extra help. We made several tweaks to more robustly handle edge cases. As an afterthought, we added some emoji to the output for fun. Incidentally, we made a consistent, branded output format. That format has become familiar and trustworthy. Output that doesn’t end with the green checkmark is more clearly an error. The big green success indicators and red error indicators don’t leave people guessing if it worked. In retrospect, having clear, consistent, terse output for the script was the single biggest improvement needed to gain the trust of developers using the tool. It gave me a deeper appreciation for the value of design and consistent UX to make even a CLI tool more trustworthy and useful. Special thanks to Thoughtbot for their laptop script and to Mike McQuaid for sharing how GitHub replaced Boxen with Strap . The engineers who build and run Kickstarter share a… 87 1 Thanks to Stephen Celis . DevOps Programming 87 claps 87 1 Written by Principal Engineer at @Glossier. The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by Principal Engineer at @Glossier. The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-03"},
{"website": "Kickstarter", "title": "functional swift conference 2016", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/functional-swift-conference-2016-e54aeb7ba266", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! On October 1, 2016, a group of engineers from Kickstarter’s native team presented at Functional Swift Conference 2016 in Budapest. Gina Binetti and Lisa Luo gave a great talk called Putting the Fun in Functional Reactive Programming: Leveraging MVVM & TDD for a better life: Brandon Williams also spoke on Finding Happiness in Functional Programming: We’re looking forward to hosting the next Functional Swift Conference in 2017 at Kickstarter’s NYC office! Written by Christopher Wright . The engineers who build and run Kickstarter share a… 3 Video iOS Swift Functional Programming 3 claps 3 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "ecs vault shhhhh i have a secret", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/ecs-vault-shhhhh-i-have-a-secret-40e41af42c28", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! As Kickstarter moves from a monolithic application towards a service oriented architecture, we needed to develop a fast and secure way for a service to programmatically retrieve API tokens, passwords, keys, etc. We are currently managing our containers using Amazon’s ECS (Elastic Container Service), which we like for its seamless scalability of Docker containers, and really wanted to secure the application’s access to sensitive information. We looked at a few tools and even built some ourselves, but we were instantly drawn to Hashicorp’s Vault . Not only is Vault an open-source project that is well maintained, it is also very secure: it uses AES-GCM-256, which is considered to be state of the art for data encryption and TLS 1.2 for data in transit to client. Since the project is entirely open source, it also benefits from the developer community’s scrutiny. One of the problems we faced was to figure out how a container would authenticate itself with our Vault server and automate the entire process. It was quite a relief to see that Hashicorp released a new AppRole authentication backend less than a month ago geared towards machines and services. An AppRole represents a set of login constraints, and the scope of the constraints can be completely customized for each application with specific access control lists (ACLs). We decided to go one step further and also added another constraint: the ECS task role , which can be queried from within the container using the following command: We built an AWS Lambda whose sole job is to manage these AppRoles in Vault when a resource is created, updated, or deleted with CloudFormation. Once a Docker container starts up, we set up an ENTRYPOINT script that uses the credentials set by the Lambda function to retrieve a Vault token and access all the app-specific secrets in Vault. To make our lives even easier, Hashicorp built a nifty tool called Envconsul (initially built for Consul key/value pairs but now also with added Vault integration). Envconsul provides a convenient way to populate secrets inside the container, and with one line in our entrypoint script we securely set all our secrets inside the container: Here is a schematic representation of our current workflow: We hope this pragmatic approach to secret management within containers is helpful for your infrastructure, and as always, please don’t be shy and feel free to email me at natacha@kickstarter.com with any questions! Special Thanks to teammate Kyle Burckhard for bringing his Docker expertise into the equation. Written by Natacha Springer . The engineers who build and run Kickstarter share a… 118 1 Infrastructure AWS DevOps 118 claps 118 1 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "a bug free downtime free major version upgrade of elasticsearch", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/a-bug-free-downtime-free-major-version-upgrade-of-elasticsearch-7091cb7edca2", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Some parts of your software stack can be tricky to upgrade. In our case, we upgraded to Elasticsearch 0.9 over two years ago, and since then it became unsupported, had a CVE announced that affected developer machines, and our Java 6 runtime had several CVEs . On top of all that, search is a complicated feature and difficult to test. We decided to bite the bullet. But what was the upgrade path? We approached the upgrade as an experiment, with the following hypotheses: ES 1.7 searches would be faster and more stable/reliable than on ES 0.9 A Java 8 runtime would also give us a performance boost over Java 6 As part of our philosophy of continuous delivery, we also required there be zero downtime during the switch. Launch a new ES 1.7 cluster with the same settings and number of nodes Index data into both 0.9 and 1.7 clusters Switch our search features to 1.7, one by one Test our hypotheses by comparing response times and mismatches, using Github’s scientist gem The scientist gem calls itself a “Ruby library for carefully refactoring critical paths.” It’s similar to feature flags , but adds metrics and can run multiple code paths in the same context. An experiment with scientist for our ES upgrade looked like this: We were able to switch some search features over to ES 1.7 very quickly. Our FAQ search is infrequent, but the experiment results were enough to show that ES 1.7 was slightly slower on average: But on the bright side, we didn’t see any mismatches between ES 0.9 and ES 1.7 results! We found more issues with other features, such as our project search tool. Performance was often slightly better: But we saw mismatches in about 15% of the results: As it turned out, when we looked into the mismatches, the results contained the same results — they just occasionally had slightly different orders! The change in sorting was an acceptable difference for us. Another search feature’s experiment mysteriously showed occasional mismatches. After investigating, we found that it stemmed from some missing documents in the ES 1.7 cluster. These documents had been rejected during our bulk indexing because of a limit on the bulk index threadpool size in ES 1.7. Ironically, that limit had been added just one patch version above the old ES 0.9 version we were running. :D After we completely switched our search features over to ES 1.7, we found that our two hypotheses were wrong: ES 1.7 running on Java 8 didn’t perform better than ES 0.9 on Java 6. The difference was marginal though, so being on the latest supported version was worth the upgrade. If we use the scientist gem again in the future, it’ll probably be with a smaller set of changes, since correctly analyzing the results of an experiment can take time. If you need to do something similar, this gem is worth checking out. We’re very happy that this upgrade was done with no disruption and we’re now on a current version of Elasticsearch. Written by Tieg Zaharia . The engineers who build and run Kickstarter share a… 3 Infrastructure Elasticsearch 3 claps 3 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "our sql style guide", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/our-sql-style-guide-8a1c24a6fe0f", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! From beginners working towards their first commits to experts trying to ease into a new codebase, style guides represent valuable investments in helping your team work together. Since much of our Data Team’s day-to-day work involves querying Redshift using SQL, we’ve put time into refining a query style guide. Many of the recommendations in our guide have been unapologetically lifted from previous guides we’ve encountered at past jobs, but much of it also stems from things we’ve discovered collaborating on thousands of queries. Here’s a sample on how to format SELECT clauses: Align all columns to the first column on their own line: We’ve got other sections on FROM, JOIN, WHERE, CASE, and how to write well formatted Common Table Expressions. Checkout the full guide here . Written by Fred Benenson . The engineers who build and run Kickstarter share a… 18 Sql Data Style Guides 18 claps 18 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "this is the story of analytics at kickstarter", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/this-is-the-story-of-analytics-at-kickstarter-2c45b9be67c9", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! If you’ve built a product of any size, chances are you’ve evaluated and deployed at least one analytics service. We have too, and that is why we wanted to share with you the story of analytics at Kickstarter. From Google Analytics, to Mixpanel, to our own infrastructure, this post will detail the decisions we’ve made (technical and otherwise) and the path we’ve taken over the last 6 years. It will culminate with a survey of our current custom analytics stack that we’ve built on top of AWS Kinesis, Redshift, and Looker. In late 2009, the early days of Kickstarter, one of the first services we used was Google Analytics. We were small enough that we weren’t going to hit any data caps, it was free, and the limitations of researching user behavior by analyzing page views weren’t yet clear to us. But users play videos. Their browsers send multiple asynchronous JavaScript requests related to one action. They trigger back-end events that aren’t easily tracked in JavaScript. So to get the best possible understanding of user behavior on Kickstarter, we knew we would have to go deeper and start looking beyond merely looking at which URLs were requested. While GA provided some basic tools for tracking events, the amount of metadata about an event (i.e., properties like a project name or category) that we could attach was limited, and the GA Measurement Protocol didn’t exist yet so we couldn’t send events outside the browser. Finally, the GA UI became increasingly sluggish as it struggled to cope with our growing traffic, and soon our data was being aggressively sampled, resulting in reports based on extrapolated trends. This was particularly problematic for reports that had dimensions with many unique values (i.e., high cardinality), which effectively prevented us from analyzing specific trends in a fine-grained way. For example, we’d frequently run into the dreaded (other) row in GA reports: this meant that there was a long tail of data which GA sampling could detect but couldn’t report on. Without knowing a particular URL to investigate, GA prevented us from truly exploring our data and diving deep. In early 2012, we heard word of a service called Mixpanel. Instead of tracking page views, Mixpanel was designed to track individual events. While this required manually instrumenting those events (effectively whitelisting which behavior we wanted to track), this approach was touted as being particularly useful for mobile devices where the page view metaphor made even less sense. Mixpanel’s event-driven model provided a solution to the problems we were encountering with Google’s page views: we could track video plays, signups, password changes, etc., and those events could be aggregated and split in exactly the same way page views could be. Even better, we wouldn’t have to wait 24–48 hours to analyze the data and access all our reports — Mixpanel would deliver data in real time to their polished web UI. They also allowed us to use an API to export the raw data in bulk every night, which was a huge selling point when deciding to invest in the service. In May of that year we deployed Mixpanel, and focused on instrumenting our flow from project page to checkout. This enabled us for the first time to calculate such things as conversion rates across project categories, but also to tie individual events to particular projects, so we could spot trends and accurately correlate them with particular subsets of users or projects. For many years, Mixpanel served us incredibly well. The data team, engineers, product managers, designers, members of our community support and integrity teams, and even our CEO used it daily to dive deep on trends and analyze product features and engagement. As our desire to better analyze the increasing volume of data we were sending the service grew, we found their bulk export API to be invaluable — we built a data pipeline to ingest our Mixpanel events into a Redshift cluster. We were subsequently able to conduct even finer-grained analysis using SQL and R. The flexibility of Mixpanel’s event model also allowed us to build our own custom A/B testing framework without much additional overhead. By using event properties to send experiment names and variant identifiers, we didn’t have to create new events for A/B tests. We could choose to investigate which behaviors a test might affect after the fact, without having to hardcode what a conversion “was” into the test beforehand. This overcame a frequent limitation of other A/B testing frameworks that we had evaluated. As Kickstarter grew, we wanted more and more from our event data. Mixpanel’s real-time dashboards were nice, but programmatically accessing the raw data in real time was impossible. Additionally, we wanted to send more data to Mixpanel without worrying about a ballooning monthly bill. By 2014, granular event data became mission-critical for Kickstarter’s day-to-day existence. Whereas previously event level data was considered a nice-to-have complement to the transactional data generated by our application database, we began depending on it for analyzing product launches, supplying stats for board meetings, and for other essential projects. At this point we started reconsidering the Build vs. Buy tradeoff. Mixpanel had provided incredible value by allowing us to get a first-class analytics service running overnight, but it was time to do the hard work of moving things in-house. As we loaded more and more data into our cluster thanks to Mixpanel’s export API, Redshift had become our go-to tool for our serious analytics work. We had invested significant time and effort into building and maintaining our data warehouse — we were shoving as much data as we possibly could into it and had many analysts and data scientists using it full time. Redshift itself had barely broken a sweat, so it felt natural to use it to anchor our in-house analytics. With Redshift as our starting point, we had to figure out how to get data into it in close-to-real-time. We have a modest volume of data — tens of millions of events a day — but our events are rich, and ever-changing. We had to make sure that engineers, product managers, and analysts had the freedom to add new events and add or change properties on existing events, all while getting feedback in real time. Since the majority of our analytics needs are ad-hoc, reaching for a streaming framework like Storm didn’t make sense. However, using some kind of streaming infrastructure would let us get access to our data in real time. For all of the reasons that distributed logs are awesome, we ended up building around AWS Kinesis, Kafka’s hosted cousin. Our current stack ingests events through an HTTPS Collector and sends them to a Kinesis stream. Streams act as our source of truth for event data, and are continuously written to S3. As data arrives in S3, we use SQS to notify services that transcode the data and load it into Redshift. It takes seconds to see an event appear in a Kinesis stream, and under 10 minutes to see it appear in Redshift. Here’s a rough sketch: This architecture has helped us realize our goal of real-time access to our data. Having event data in Kinesis means that any analyst or engineer can get at a real-time feed of their data programmatically or visually inspect it with a command-line tool we whipped up. While work began on our backbone infrastructure, we also began seriously investigating Looker as a tool to enable even greater data access across Kickstarter. Looker is a business intelligence tool that was appealing to us because it allows people across the company to query data, create visualizations, and combine them into dashboards. Once we got comfortable with Looker, it dawned on us that we could use it to replicate much of Mixpanel’s reporting functionality. Looker’s DSL for building dashboards, called LookML and their templated filters provided a powerful way to make virtually any dashboard imaginable. This made it just as easy to access our data in Looker as it was in Mixpanel — anyone can still pull and visualize data without having to understand SQL or R. As we became more advanced in our Looker development we were able to build dashboards similar to Mixpanel’s event segmentation report: Most significantly, we were able to take advantage of Kickstarter specific knowledge and practices to create even more complex dashboards. One of the ones we’re most proud of is a dashboard that visualizes the results of A/B tests: Owning your own analytics infrastructure isn’t merely about replicating services you’re already comfortable with. It is about opening up a field of opportunities for new products and insights beyond your team’s current roadmap and imagination. Replacing a best-in-class service like Mixpanel isn’t for the faint of heart, and requires serious engineering, staffing, and infrastructure investments. However, given the maturity and scale of our application and community, the benefits were clear and worth it. If this post was helpful to you, or you’ve built something similar, let us know! Written by Fred Benenson and Ben Linsay . The engineers who build and run Kickstarter share a… 92 1 Infrastructure Analytics 92 claps 92 1 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "the kickstarter engineering and data team ladder", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/the-kickstarter-engineering-and-data-team-ladder-96996c3b327", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Over the last year, we’ve doubled the size of the Engineering and Data teams at Kickstarter. Prior to that growth, our teams’ structure was very flat, and titles were very generic. Now we’ve got folks who have differing levels of skills and experience, we need a structure to help us organize ourselves. We decided we should build an engineering ladder and define roles across the teams. Deciding to design and implement an engineering ladder can be tricky. It needs to start right, exert flexibility as we evolve, scale as we grow, and the process needs to be as consultative and inclusive as possible. Thankfully, earlier in the year, Camille Fournier, then CTO at Rent the Runway, shared her team’s Engineering Ladder . It was enormously influential in guiding our thinking around how Engineering should be leveled and structured. (We should also thank Harry Heymann, Jason Liszka, and Andrew Hogue from Foursquare, who inspired Rent the Runway in the first place). We took the material and ideas we found in Fournier’s work and modified them to suit our requirements. We then shared the document with the team and asked for feedback and review. After lots of discussion and editing, we ended up with roles that people understood and were excited to grow into. We’ve now deployed the roles — and in the spirit of giving back to the community that inspired us to do this work, we wanted to share the ladder we created. Junior Software Engineer Software Engineer Senior Software Engineer Staff Engineer Principal Engineer Data Analyst Data Scientist VP of Data Engineering Manager Engineering Director CTO You can see the full details here . If you’re in the process of thinking through how you organize your team, we hope this can be of some help. And if you use this as a starting point for building your own ladder, and tailoring it to your own needs, we’d love to hear about it! Written by James Turnbull . The engineers who build and run Kickstarter share a… 5 Engineering Mangement 5 claps 5 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "introducing mail x smtpapi a structured email header for ruby", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/introducing-mail-x-smtpapi-a-structured-email-header-for-ruby-317703a6946", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! At Kickstarter we use SendGrid to deliver transactional and campaign emails, and use SendGrid’s X-SMTPAPI header for advanced features like batch deliveries and email event notifications. Developing the first of these features went well — but the second and third features became entangled when we tried to share an unstructured hash that was unceremoniously encoded into an email header at a less-than-ideal time. Our solution was to add first-class header support to Ruby’s Mail gem . This gave us a structured value object that we could write to from any location with access to the mail object, allowing our mailing infrastructure to remain focused and decoupled. Today we’re announcing our open source Mail extension, appropriately titled mail-x_smtpapi . With this gem you can write to a structured mail.smtpapi value object from anywhere in your mailing pipeline, including a Rails mailer, template helper, or custom mail interceptor. Here’s a basic example from the gem’s README to get you started. This variation from the Rails Guide gives you extra detail in SendGrid’s email event notifications: We hope you find this as useful as we did, or find inspiration here to develop header classes for your own custom uses. As always, we love feedback, especially in the form of pull requests or bug reports. If you take delight in discovering simple solutions to stubborn code, why not browse our jobs page ? We’re hiring! Written by Lance Ivy . The engineers who build and run Kickstarter share a… 2 Ruby on Rails Open Source 2 claps 2 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "introducing cfn flow a practical workflow for aws cloudformation", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/introducing-cfn-flow-a-practical-workflow-for-aws-cloudformation-8891afb3bfeb", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! If you’re looking for a simple, reliable way to develop with AWS CloudFormation, check out cfn-flow on GitHub . As an Ops Engineer, I’m always seeking better ways to manage Kickstarter’s server infrastructure. It can never be too easy, secure, or resilient. I’ve been excited about AWS CloudFormation as a way to make our infrastructure provisioning simpler and replicable. Some recent greenfield projects provided a great opportunity to try it out. We quickly found we wanted tooling to consistently launch and manage CloudFormation stacks. And each project presented the same workflow decisions, like how to organize resources in templates, where to store templates, and when to update existing stacks or launch new ones. I built cfn-flow to reflect Kickstarter’s best practices for using CloudFormation and give developers a consistent, productive deploy process. Two especially helpful constraints of the workflow are worth highlighting: cfn-flow embraces the red/black deployment pattern to gracefully switch between two immutable application versions. For each deployment, we launch a new CloudFormation stack then delete the old stack once we’ve verified that the new one works well. This is preferable to modifying long-running stacks because rollbacks are trivial (just delete the new stack), and deployment errors won’t leave stacks in unpredictable states. Since deployments launch and delete stacks, templates can only include ephemeral resources that can safely be destroyed. For our apps, that usually means a LaunchConfig, an AutoScalingGroup, and, optionally, an ELB with a Route53 weighted DNS record and an InstanceProfile. Resources that are part of your service that do not change in each deployment are considered backing resources. These include RDS databases, security groups that let both new and old EC2 servers communicate, SQS queues, etc. We extract backing resources to a separate template that’s deployed less frequently. Backing resources are then passed as parameters to our app stack via our cfn-flow.yml configuration. cfn-flow is a command-line tool distributed as a RubyGem. You track your CloudFormation templates in the same directory as your application code, and use the cfn-flow.yml configuration file to tie it all together. Check out the cfn-flow README for details and examples. We’ve been using it for a few months with great success. It gives developers good, easy affordances to build robust services in AWS. I encourage anyone else interested in CloudFormation to give cfn-flow a try. If it’s not making your job easier, please file a GitHub Issue . Written by Aaron Suggs . The engineers who build and run Kickstarter share a… 9 DevOps AWS Infrastructure Cloudformation 9 claps 9 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "introducing kumquat a rails template handler for rmarkdown", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/introducing-kumquat-a-rails-template-handler-for-rmarkdown-b32dbc430802", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! At Kickstarter our data team works extensively in R , using Hadley Wickham ’s essential R package ggplot2 to generate data visualizations. Last year, we began developing internal reports using knitr , a tool to render R output and images into HTML and PDFs. The reports looked great, but we wanted a way to both automate them and also integrate them within the Kickstarter Rails application. Our requirements would be to build something to replace our daily-reporting infrastructure which could: Get rendered and manipulated by Rails Connect to our data warehouse, Redshift Be capable of generating graphs via R and ggplot2 So, as part of our month of Open Source, we’re announcing Kumquat , a Rails Engine designed to help integrate RMarkdown (and therefore anything R can produce) into Rails. At its core, Kumquat is a Rails template handler and email interceptor that uses R to send rich data reports. For example, consider a typical render call to a partial: This partial is _a_knitr_report.Rmd, a regular RMarkdown file stored in your app: Which yields: For more technical details, head over to the README on GitHub . I presented an early version of Kumquat at a meetup at Kickstarter HQ in April. If you’d like more background about how and why I built Kumquat, check out my slides: Written by Fred Benenson . The engineers who build and run Kickstarter share a… Data R Language Markdown Ruby Open Source Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "kickstarter data driven university", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/kickstarter-data-driven-university-35d5ae8b39b8", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! The Kickstarter Data team’s mission is to support our community, staff, and organization with data-driven research and infrastructure. Core to that, we’ve made it our goal to cultivate increased data literacy throughout the entire company. Whether it’s knowing when to use a line chart or a bar plot, or explaining why correlation does not equal causation, we strongly believe that basic data skills benefit everyone: designers and engineers, product managers and community support staff, all the way up to our senior team. During my time working at LinkedIn on their Insights team, our leadership helped establish a program called Data-Driven University (DDU). DDU was a two-day bootcamp of best practices on working with data: tips on how to communicate effectively using data, how to use data to crack business problems, and how to match a visualization with the right story to tell. It was a transformative experience for me; I witnessed leaders of some of the largest business units discover techniques to help their teams make better decisions with data. When I joined Kickstarter’s Data team last year, I saw an opportunity to use the same approach with our own staff. Our intention was to create a series of courses that was open to everyone, not just a select few; hence, Kickstarter Data-Driven University (KDDU) was born. First, we surveyed the company on a number of voluntary data-related sessions taught by our team. Analyzing the themes in our survey response data led us to settle on offering three sessions: Data Skepticism (how to think critically using data), Data Visualization (how to effectively present data visually), and Data Storytelling (how to communicate compellingly with data). After several weeks of prep work, we held four classes (including an additional workshop on conducting A/B tests). The results were encouraging: more than 50% of the company attended at least one class, and our final Net Promoter Score was 73 (taken by survey after KDDU wrapped up), on par with the Apple Macbook . Not bad! We also heard positive feedback directly from our staff, such as the following: Broke down complicated terms/jargon and offered real-use cases to help the audience better grasp how data is analyzed/presented. The Data team had such a good time presenting KDDU internally that we volunteered to give the seminar two more times. So in July, we partnered with New York Tech Talent Pipeline (NYTTP) for their Beyond Coding program and gave a slightly modified version of KDDU to their new grads and students looking to build skills before entering the workforce. Today, we’re making those slides available for you to leverage with your own teams to help increase data skills and literacy: Here are some of our takeaways from teaching data skills to our colleagues: We could have talked about our favorite Data team subjects: our infrastructure, the nuances of Postgres 8.0.2 , or our favorite R packages … but we knew we had to keep data approachable for a broader audience. We decided to focus on giving our audience a set of simple rules and principles that would help them work with data more effectively in their day-to-day. We sent out a brief survey to see what topics our coworkers wanted to learn about most. This both made it easier to decide on which topics to present, and also meant we knew the topics we chose would be interesting to our audience. Within the individual presentations we focused on selecting examples that would resonate with our audience , highlighting trends from actual Kickstarter data, insights into past A/B tests we’ve run, and other familiar and relevant stats. As an old boss used to say, if you can’t measure it, you can’t manage it . So after we completed KDDU, we sent out a second brief survey, this one to collect feedback on the overall selection of courses and the individual lessons. This data has helped refine our approach for a second round of KDDU sessions that we’re considering offering as our company grows. We couldn’t be more excited to share our experience with you, and hope you find it valuable to increasing data-driven decision-making and skills at your organization! Written by Kevin Showkat . The engineers who build and run Kickstarter share a… 15 Data Teaching 15 claps 15 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "joining rubytogether", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/joining-rubytogether-280f99fb22a4", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! At Kickstarter, we’ve built our platform on top of Ruby on Rails. It’s the core technology we use, and we’re very proud of the platform we’ve built with it. We’re also contributors to the Ruby and Rails communities, both in terms of open-source contributions and engagement via conferences and talks. This week, we’re happy to announce we’ve also become members of RubyTogether . RubyTogether is dedicated to funding and helping the awesome volunteers who maintain the glue of the Ruby ecosystem: tools like Bundler, RubyGems, and the like. We’re thrilled to be giving more back to the Ruby community, and we strongly encourage other large Ruby or Rails-based platforms to become members , too. Written by James Turnbull . The engineers who build and run Kickstarter share a… Ruby Community Open Source Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "introducing telekinesis a kinesis client for jruby", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/introducing-telekinesis-a-kinesis-client-for-jruby-f317001f4da7", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Kickstarter exists to help make it easier for people to create new things. And when it comes to code, there’s one very simple way to help others create — by sharing the things we’ve already built. That’s why, over the past month, we’ve been open-sourcing a new library each week. Today’s is called Telekinesis, and, well … we’ll let Ben explain it. At Kickstarter we use a variety of AWS services to help us build infrastructure quickly, easily, and affordably. Last winter, we started experimenting with Kinesis, Amazon’s hosted Kafka equivalent, as the backbone for some of our data infrastructure. After deciding that we needed a distributed log , we settled on using Kinesis based on cost and ease of operation. Kickstarter is all about Ruby , so it made sense for us to do our prototyping in Ruby. Since the Kinesis Client Library (KCL) is primarily built for Java, we quickly decided that building on top of JRuby was our best option. We already have some Java expertise in-house, so we also knew that running and deploying the JVM would be relatively straightforward. It’s been going so well that we haven’t looked back — despite Amazon’s announcement that they officially support Ruby through the Multilang Daemon . As part of open source month, we’re releasing Telekinesis, the library we’ve built up around the KCL. It includes some helpers to make using a Consumer from Ruby a little more idiomatic. It also includes a multi-threaded producer that we’ve been using in production for a couple months. Head on over to Github for a closer look. Looking for more of our tools? Just poke around Backing & Hacking , or see our open-source projects on GitHub ! And if you’re excited by what you see, you might be even more excited to know that we’re hiring … Written by Ben Linsay . The engineers who build and run Kickstarter share a… 12 Ruby Open Source Data Kinesis 12 claps 12 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "open source month week one caption crunch", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/open-source-month-week-one-caption-crunch-38b9f8571b0a", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Kickstarter exists to help make it easier for people to create new things. And when it comes to code, there’s one very simple way to help others create — by sharing the things we’ve already built. That’s why, all August long, we’ll be open-sourcing a new library each week. I’m David, and this time around, I’ll be your gracious host! (Here, let me take your coat.) We think creativity is for everyone, and part of living up to that belief is making sure our website works for everyone, too. We recently announced a feature that lets creators add subtitles and captions to their project videos, and watched creators use it to make their stories available to more and more people, across all sorts of different languages and levels of hearing — a win for accessibility and for bringing together a global community. Today, we’re excited to share a chunk of that feature with creative people all around the web: a tool we call Caption Crunch. Our site allows creators to type in their own subtitles and captions, but we wanted them to be able to import subtitle and caption files, too. (That’s helpful for creators who use a service to subtitle, caption, or translate their videos.) In order to import files, we need a parser to take the files apart, read them, and understand them. Glancing at RubyGems and GitHub, many parsers for subtitle files exist. However, their code either didn’t parse appropriately, didn’t have great test coverage, or had stylistic issues. We believe in supporting other open source projects, but in this case, we decided to make our own. Hence Caption Crunch , a Ruby parser for subtitle files. If you need to import caption files into your app, consider trying it out! Currently, only VTT files are supported, but we’ve designed the gem with extensibility in mind — the adapter enables you to add new types. You can code your own parser, and we’d love to see how you’ve tailored the gem to your own needs. Feel free to open an issue or pull request! Add this line to your application’s Gemfile: And then execute: Or install it yourself as: Using the CaptionCrunch.parse method, you can parse a subtitle file or string. The result of the parse is a bunch of Ruby CaptionCrunch::Cue objects within a larger CaptionCrunch::Track object. With them, you can insert the CaptionCrunch::Track and CaptionCrunch::Cue properties into your own database, or manipulate them however you need. Contributions are welcome! If you’d like to see a certain subtitle type, post an issue on GitHub. We hope the library is useful! Care for some tea on your way out? This is only the first of our weekly open-source libraries — don’t forget to check Backing & Hacking for more, all August long. Or just see our open-source projects on GitHub ! And if you’re excited by what you see, you might be even more excited to know that we’re hiring … Written by David Peter . The engineers who build and run Kickstarter share a… Open Source Ruby Captioning Accessibility Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-15"},
{"website": "Kickstarter", "title": "were going to full stack fest", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/were-going-to-full-stack-fest-daf5554c895e", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! Being an engineer at Kickstarter isn’t just about writing and deploying code, iterating on product features, and brainstorming new ideas. It’s also about being a part of the larger developer community: hosting meet-ups, sharing information, and finding ways to both learn from and support each other. One way we do that is by attending engineering conferences — and we’ve been busy this season! Here’s a little snapshot of what we’ve been up to: At RailsConf , Aaron ( @ktheory ) talked about teaching “ GitHub for Poets ,” Kickstarter’s approach to helping everyone in the organization safely make changes in our code. Our VP of Engineering, James ( @kartar ), presented at FOSDEM , Config Management Camp , and FluentConf . Suz ( @noopkat ) took on Norway, “ babbling with merfolk ” and presenting at NodeConf and Web Rebels . And Rebecca P. ( @rebeccapoulson ) travelled to the Sunshine State to discuss “The Junior Jump” at Ancient City Ruby . Next up, we’ll be at Full Stack Fest in Barcelona. The weeklong programming event features a host of cool workshops and talks, plus MC Liz Abinante ( @feministy ). We’re also super excited that Full Stack is introducing live-captioning this year in an effort to make the conference more accessible. (On that note, did you know we launched our own subtitles and captions feature on Kickstarter recently?) If you want to learn more about Full Stack Fest, what they’re all about, and their take on building an inclusive conference, check out their website . Maybe we’ll see you in Barcelona! (Psst! Full Stack is still looking for sponsors . Full disclosure: we’re totally helping sponsor the live-captioning! ;) Written by Sid Orlando . The engineers who build and run Kickstarter share a… Conference Ruby Ruby on Rails JavaScript Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "a b test reporting in looker", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/a-b-test-reporting-in-looker-bf4869f6b52", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! One of the Data team’s priorities this year has been improving the Kickstarter A/B testing process. To this end, I’ve been been focused on making it easier to set up, run, and analyze experiments. This will make it more likely we’ll use data from experiments to inform product design. Until recently, we monitored A/B tests in an ad hoc way. We use our event tracking infrastructure to log A/B test data, so while a test was running, a Product Manager or Data Analyst watched the number of users in the experiment’s event stream until it reached the required sample size. At that point, we ran the numbers through an R script and sent out the results. Kickstarter recently adopted a business intelligence tool called Looker to support data reporting and ad hoc analysis. Looker connects directly to our Redshift cluster, which is where we store the raw event data from our A/B tests. This made me wonder whether we could use Looker to monitor experiments and report results. One feature of Looker we like is the ability to save and schedule queries, with the results delivered via email. If we could find a way to analyze A/B tests via SQL, then Looker could handle the rest. How can we do statistics in SQL without access to probability distributions? There are methods for generating normally distributed data in SQL , but this approach seems like overkill. We don’t need to recreate a standard normal distribution on the fly. The values don’t change. My aha moment was remembering my old statistics textbooks with look-up tables in the back. By adding look-up tables for probability distributions to Redshift, we can get good approximations of power, p-values, and confidence intervals for the typical A/B tests we run. Although this means we’re simulating a continuous distribution with a discrete one, we don’t rely exclusively on p-values to interpret our tests, so a difference of a few thousandths of a point won’t make much difference. As an example, I’m going to use a common type of test we run — a hypothesis test of the difference of two proportions. (If you’d like to learn more about the statistics behind this test, this is a good place to start ). To make this concrete, let’s say we’re testing a new design of the Discover page, and we want to know whether it affects the number of users clicking through to project pages. To generate a test statistic for this type of test, we need a standard normal distribution. I generated a set of z-scores and their probabilities in R and loaded this into Redshift as standard_normal_distribution. The table looks something like this: Now let’s say we’ve already calculated the results of our experiment for two groups: control and experimental. For each group, we have the number of unique users n who visited the Discover page, the number of unique users x who clicked through to a project page, and the proportion p = x / n . This can all be done with a query. In the sections below, I’ll use the output of that query to calculate the sample proporution, standard error, and other sample statistics using subqueries called common table expressions (CTEs). If you aren’t familiar with this flavor of SQL syntax, you can think of CTEs as forming temporary tables that can be used in subsequent parts of the query. Using a CTE, we calculate p_hat , the pooled proportion under the null hypothesis: Next we calculate the pooled standard error under the null hypothesis: This allows us to calculate an exact z-score from the data: Then we find the nearest z-score in our standard normal look-up table and use that to calculate a p-value: Having a p-value is a good start, but we also want to generate confidence intervals for the test. While we’re at it, we’d also like to conduct a power analysis so the test results only display when we’ve reached the minimum sample size. To do that properly, we need some details about the test design: the significance level, the power, and the minimum change to detect between the two variants. These are all added to the query using Looker’s templated filters, which take user input and add them as parameters. Unfortunately, Looker cannot simply add an arbitrary value (e.g. 0.05) to any part of a query. To get around this, I filter a table column with user input and then use the resulting value. For example, in the following section, the query takes user input as significance_level, matches it against the probability column of the standard_normal_distribution table (after some rounding to ensure a match), and saves that value as alpha: Note Looker’s syntax for what it calls templated filters: If the user input is 0.05 for the significance_level filter, Looker converts this to: See the appendix below for the entire query. Admittedly, doing all this in SQL is kind of preposterous, but it means that we can add it to Looker as the engine of an A/B Test Dashboard. The dashboard abstracts away all the calculations and presents a clean UI for taking user input on the parameters of the test design, allowing people without any special engineering or data expertise to use it. Now that it’s built into Looker, it’s part of our larger data reporting infrastructure. After taking input about the test design, the dashboard calculates the number of users in each variant, their conversion rates, and the minimum sample size. If the sample size has been met, the dashboard also outputs a p-value and confidence interval for the test. The dashboard can be scheduled to run daily, and we can even set it up to email only when there are results to report. Now when we implement a new A/B test, we add it to Looker so we can get daily status emails, including statistical results when the test is complete. This can be done by someone on the Product or Engineering teams, freeing up Data team resources to focus on designing experiments well and running more complex tests. This kind of dashboard pushes Looker to its limits, so naturally there are some drawbacks to doing A/B test reporting this way. It separates the implementation of the test from the implementation of the reporting, so there is some duplicated effort. Furthermore, it only works for specific types of tests where the math can be handled by a SQL query and a static probability distribution. On the other hand, we’re happy that Looker is flexible enough to allow us to prototype internal data tools. The A/B Test Dashboard has automated what was a very manual process before, and it has reduced the dependency on the Data team for monitoring and reporting the results of common types of tests. This all means we can run more experiments to create a better experience for our users. If this kind of thing gets data juices flowing, you should know we’re hiring for a Data Scientist ! Head over to the job description to learn more . Our query in full: Written by Jeremy Salfen . The engineers who build and run Kickstarter share a… 63 Data Ab Testing Sql 63 claps 63 Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"},
{"website": "Kickstarter", "title": "beyond coding a summer curriculum for emerging software developers", "author": ["Kickstarter Engineering"], "link": "https://kickstarter.engineering/beyond-coding-a-summer-curriculum-for-emerging-software-developers-6e7897e16efb", "abstract": "Ruby Infrastructure Data Open Source Video We’re hiring! With nearly five open jobs for every available software developer, the need for qualified technical talent is higher than ever. Here in New York City alone, there are 13,000 firms seeking workers with highly sought-after tech skills: web development, mobile development, user-interface design, and more. So when Mayor Bill de Blasio called companies to action in support of the city’s Tech Talent Pipeline efforts, we teamed up with five other NYC-based companies — Crest CC, Foursquare, Tumblr, Trello, and Stack Overflow — to find ways to support emerging developers as they join the local tech ecosystem. Together, we’re introducing Beyond Coding , a new, free summer program that aims to equip emerging computer programmers with the skills to help them succeed at their first coding jobs. The goal? Provide New Yorkers who have a passion for technology with access to the mentoring, training, and support they need to really grow as developers. The curriculum is designed to address areas where junior-level technical talent might need an extra boost: it’ll cover professional networking, effective strategies for communicating technical ideas to a variety of audiences, how to prepare for an interview, and ways to gain programming knowledge outside the classroom. Beyond Coding will be open to anybody in the NYC area who is currently looking for a job as a software developer (or something related), and has experience and knowledge of programming — but doesn’t have access to tools, resources, or a professional network of support. Eligible students will receive a formal certification of their course completion at the culmination of the 10-week program, and will be introduced to tech companies in New York City who are actively hiring junior-level developers. Any students interested in participating in the Beyond Coding program this summer can register at beyondcoding.io . Employers interested in attending the hiring fair with certified students can email employers@beyondcoding.io for more information. Written by Sid Orlando . The engineers who build and run Kickstarter share a… Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Written by The engineers who build and run Kickstarter share a behind-the-scenes look at their work, from approaches to open source and code review to feature releases. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-28"}
]