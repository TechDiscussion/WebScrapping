[
{"website": "Buzzfeed", "title": "finding and fixing memory leaks in python", "author": ["Peter Karp"], "link": "https://tech.buzzfeed.com/finding-and-fixing-memory-leaks-in-python-413ce4266e7d", "abstract": "Latest Posts Events Apply To BuzzFeed One of the major benefits provided in dynamic interpreted languages such as Python is that they make managing memory easy. Objects such as arrays and strings grow dynamically as needed and their memory is cleared when no longer needed. Since memory management is handled by the language, memory leaks are less common of a problem than in languages like C and C++ where it is left to the programmer to request and free memory. The BuzzFeed technology stack includes a micro-service architecture that supports over a hundred services many of which are built with Python. We monitor the services for common system properties such as memory and load. In the case of memory a well-behaved service will use memory and free memory. It performs like this chart reporting on the memory used over a three-month period. A microservice that leaks memory over time will exhibit a saw-tooth behavior as memory increases until some point (for example, maximum memory available) where the service is shut down, freeing all the memory and then restarted. Sometimes a code review will identify places where underlying operating system resources such as a file handle are allocated but never freed. These resources are limited and each time they are used they allocate a small amount of memory and need to be freed after use so others may use them. This post first describes the tools used to identify the source of a memory leak. It then presents a real example of a Python application that leaked memory and how these tools were used to track down the leak If a code review does not turn up any viable suspects, then it is time to turn to tools for tracking down memory leaks. The first tool should provide a way to chart memory usage over time. At BuzzFeed we use DataDog to monitor microservices performance. Leaks may accumulate slowly over time, several bytes at a time. In this case it is necessary to chart the memory growth to see the trend. The other tool, tracemalloc , is part of the Python system library. Essentially tracemalloc is used to take snapshots of the Python memory. To begin using tracemalloc first call tracemalloc.start() to initialize tracemalloc , then take a snapshot using tracemalloc can show a sorted list of the top allocations in the snapshot using the statistics() method on a snapshot. In this snippet the top five allocations grouped by source filename are logged. The output will look similar to this: This shows the size of the memory allocation, the number of objects allocated and the average size each on a per module basis. We take a snapshot at the start of our program and implement a callback that runs every few minutes to take a snapshot of the memory. Comparing two snapshots shows changes with memory allocation. We compare each snapshot to the one taken at the start. By observing any allocation that is increasing over time we may capture an object that is leaking memory. The method compare_to() is called on snapshots to compare it with another snapshot. The 'filename' parameter is used to group all allocations by module. This helps to narrow a search to a module that is leaking memory. The output will look similar to this: This shows the size and the number of objects and a comparison of each and the average allocation size on a per module basis. Once a suspect module is identified, it may be possible to find the exact line of code responsible for a memory allocation. tracemalloc provides a way to view a stack trace for any memory allocation. As with a Python exception traceback, it shows the line and module where an allocation occurred and all the calls that came before. Reading bottom to top, this shows a trace to a line in the socket module where a memory allocation took place. With this information it may be possible to finally isolate the cause of the memory leak. In this first section we saw that tracemalloc takes snapshots of memory and provides statistics about the memory allocation. The next section describes the search for an actual memory leak in one BuzzFeed microservice. Over several months we observed the classic saw-tooth of an application with a memory leak. We instrumented the microservice with a call to trace_leak(), a function we wrote to log the statistics found in the tracemalloc snapshots. The code loops forever and sleeps for some delay in each loop. The microservice is built using tornado so we call it using spawn_callback() and pass parameters delay , top and trace : The logs for a single iteration showed allocations occurring in several modules tracemalloc is not the source of the memory leak! However, it does require some memory so it shows up here. After running the service for several hours we use DataDog to filter the logs by module and we start to see a pattern with socket.py: The size of the allocation for socket.py is increasing from 1840 KiB to 1845 KiB. None of the other modules exhibited this clear a trend. We next look at the traceback for socket.py . We get a stack trace from tracemalloc for the socket module. Initially, I want to assume that Python and the standard library is solid and not leaking memory. Everything in this trace is part of the Python 3.6 standard library except for a package from DataDog ddtrace/writer.py . Given my assumption about the integrity of Python, a package provided by a third-party seems like a good place to start investigating further. We find when ddtrace was added to our service and do a quick rollback of requirements and then start monitoring the memory again. Over the course of several days the memory continues to rise. Removing the module did not stop the leak. We did not find the leaking culprit. So it’s back to the logs to find another suspect. There is nothing in these logs that looks suspicious on its own. However, ssl.py is allocating the largest chunk by far, 2.5 MB of memory. Over time the logs show that this remains constant, neither increasing nor decreasing. Without much else to go on we start checking the tracebacks for ssl.py . The top of the stack shows a call on line 645 of ssl.py to peer_certificate() . Without much else to go on we make a long-shot Google search for “python memory leak ssl peer_certificate” and get a link to a Python bug report . Fortunately, this bug was resolved. Now it was simply a matter of updating our container image from Python 3.6.1 to Python 3.6.4 to get the fixed version and see if it resolved our memory leak. After updating the image we monitor the memory again with DataDog. After a fresh deploy around Sept. 9th the memory now runs flat. Having the right tools for the job can make the difference between solving the problem and not. The search for our memory leak took place over two months. tracemalloc provides good insight into the memory allocations happening in a Python program; however, it does not know about the memory allocations that take place in packages that are allocating memory in C or C++. In the end, tracking down memory leaks requires patience, persistence, and a bit of detective work. https://docs.python.org/3/library/tracemalloc.html https://www.fugue.co/blog/2017-03-06-diagnosing-and-fixing-memory-leaks-in-python.html To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 875 2 Python Memory Leak Software Engineering Buzzfeed Posts Debugging 875 claps 875 2 Written by Staff Software Engineer @BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Staff Software Engineer @BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-17"},
{"website": "Buzzfeed", "title": "building great products by building great relationships", "author": ["Sami Simon"], "link": "https://tech.buzzfeed.com/building-great-products-by-building-great-relationships-7d36d4027929", "abstract": "Latest Posts Events Apply To BuzzFeed By making cooking accessible, fun, and social, Tasty has become one of the most beloved and popular food brands on the Internet today. Tasty started as a Facebook page that pioneered the top-down time-lapse recipe video format, and has since grown into a cross-platform phenomenon and a massive business. Today, Tasty now reaches over 500 million people every single month, and we just hit our all time highest day for engagement and traffic this past Thanksgiving on both the Tasty App and Tasty.co, making it very clear that we’ve really become an important part of our users’ lives in the kitchen. As the product lead for Tasty, it’s my goal to ensure that we are solving the most important problems for Tasty users and the Tasty business by ensuring we’re working in lockstep with the dozens of people who jointly make Tasty successful. This year, the Tasty leadership team held our first two-day offsite with teammates from departments spanning sales, production, editorial, culinary, social, data, tech, and operations. I left the offsite feeling more energized than ever, and felt our team was better positioned to succeed than ever before. I wanted to take a moment and share a few lessons I learned from this cross-functional working session that I think any tech leader can benefit from. This may sound obvious, but it’s important for your cross-functional group to find a few uninterrupted days to take a step back and discuss the most important problems that need to be solved and to then propose a way forward. Everyone went into the offsite with a clear set of objectives, and we used our time together to align on those objectives.We also leveraged each department’s unique perspective to suggest and evaluate ideas — from new frameworks, to new processes, to new products. People, teams, and company goals are constantly evolving. A strong organization does its best to always pivot to meet these changes instead of keeping things the same for the sake of consistency. So, if you think something is important, check in with your partners across the company every so often to see if there is an opportunity now to try it out. Pitch experimental approaches to make it easier to transition if people aren’t totally sure about your idea. No matter your approach, just avoid assumption and speak up when you see an opportunity. We are often asked to brainstorm ideas or think through how to solve problems because we bring subject matter expertise or knowledge that is required; however, there is something to be said about bringing a diverse bunch of just really smart people who all care about the same thing in one space and to just work on ideas together. For example, even though I’m not a social media strategist, I found myself getting really excited around the idea of how to make our recipes more successfully monetized on Facebook and YouTube. Getting the opportunity to think outside of your comfort zone or regular day-to-day is not only fun, but can make you think about your own job in new ways. This type of inspiration that encourages outside the box thinking which leads to innovation. By getting to spend two days exploring the challenges of creating a social strategy or creating product awareness on our Tasty branded items, my eyes were opened to all of the other challenges and opportunities my partners were facing across the company. While we go to talk to stakeholders when we need something from them, it is not often we can talk through what is challenging in their world that may not have anything to do with tech. By understanding the struggles of another team helps to build empathy and make you feel more connected. Open communication and understanding are essential to any relationship, but particularly in a company setting, they allow us to do better work and not feel isolated when finding the right solution. Sometimes, it can feel really challenging to create an ongoing rapport with the stakeholders to your group if all you have to ever discuss are updates. While keeping each other in the loop is important, it can sometimes feel inorganic or something that could probably be handled over an email. However, when you actually have a goal or a mission that you and your stakeholders and partners are trying to achieve, the quality and frequency of conversation increases. Having everyone rally around the goal of improving recipes has made it so we stay better in touch and provide each other interesting updates more often. At the end of the day, creating this relationship and alliance can be more important than the actual outcome of the offsite itself. While creating new ideas is great, creating strong relationships and trust is invaluable to building a better Tasty. I’m in awe of how much all of the Tasty teams evolved and improved how we work together. Tasty and BuzzFeed gives their staff opportunities to take this incredibly well-loved brand and make it an awesome product that has become an essential part of our audience’s lives. I’m beyond proud everyday that I get to help take the well-known top-down recipe video and make it a way that people can feed their families on a busy night or make a special dessert as a way to connect with friends. Even moreso, I’m proud I work at a company that encourages self-reflection, trying new things, collaboration across a myriad of skill sets, and pivoting to always find the right opportunity. P.S. Download Tasty on iOS and Android or check out Tasty.co today! Have thoughts on cross-team collaboration when building great products? We’d love to hear them! Send us a comment below. BuzzFeed Tech is hiring. If you are interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 102 Product Management Teamwork BuzzFeed Food Technology Media 102 claps 102 Written by Senior Product Manager at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Senior Product Manager at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-27"},
{"website": "Buzzfeed", "title": "how were building superpowers for social publishers at buzzfeed", "author": ["Swara Kantaria"], "link": "https://tech.buzzfeed.com/how-were-building-superpowers-for-social-publishers-at-buzzfeed-ac24fad5ae11", "abstract": "Latest Posts Events Apply To BuzzFeed Over the past year, my definition of a “good” internal workflow tool has evolved significantly. I once believed a “good” tool eliminated repetitive manual tasks and helped BuzzFeed employees finish their work more quickly, with fewer mistakes. A “good” tool was easy to use and filled with features that users loved. In the past year, I realized my definition of “good” was accurate but incomplete. Workflow tools aren’t only about making people more efficient. They should be your business’s competitive advantage. They should incorporate innovative technology to help grow your company in the most meaningful ways. A “good” tool can help retain your most talented staff. I had this realization after I started leading product for a relatively new team at BuzzFeed called Distribution Tools. My team is responsible for building tools & services that distribute BuzzFeed content everywhere it needs to go to reach ( and bring immense delight or groundbreaking news ) to our audience. This essentially means we build products that send all of our content to social media platforms (like Facebook, YouTube, Twitter and Instagram), OTT destinations (like Roku and Pluto) and to syndication partners. Over the past year, our product approach evolved from building easy-to-use publishing tools that let people work more efficiently, to building artificial intelligence and machine learning models that let people work more intelligently. Today, I want to share some details of this evolution with you. Before I dive deeper into talking about our work, there are few facts you need to know for context: BuzzFeed creates 1200+ new pieces of content every week. This includes: videos, quizzes, listicles, breaking news posts, memes, .gifs and so much more. Our social media team distributes this content to our diverse audiences across 400+ social accounts on 10 different social platforms (such as Facebook, YouTube, Instagram, Snapchat, Tumblr and Twitter). While BuzzFeed’s site and app have a large user base, a significant portion of our audience discovers our content on social platforms. The distribution workflow becomes challenging for social publishers as we increase the amount of content we produce and grow the number of social destinations. The team is stretched really thin trying to make sure every piece of content is being distributed to all the right places. When I walked by their desks they looked something like this: When we first formed as a team, we were obsessed with figuring out how tech can could improve the social publishing team’s existing workflow, which relied on several third-party tools. We started by launching a new proprietary distribution tool, PubHub, to make it faster for publishers to find content (videos, images, posts) and publish it to social platforms. We automated the process of collecting crucial data, so publishers didn’t have to manually add data to spreadsheets to power reporting. We shipped features that saved time, like auto-generating YouTube descriptions or setting default monetization settings. We launched automated Slack notifications so content creators didn’t have to bug the social team to find out when their work was going to be published. People were thrilled about what we were building! The response to most feature launches looked something like this: Within a year, we had laid a strong foundation for distribution tooling at BuzzFeed. PubHub quickly became a beloved internal tool and was used on a weekly basis by 220+ people. The tool allowed the social team to distribute more content than ever. In spite of the fact that we had built a tool that the social publishing team found increasingly indispensable, we knew that we had only streamlined their workflow. We hadn’t fundamentally changed it. We started to wonder if tech could offer our users something better… something they hadn’t imagined yet . We had this lingering question: Are we sure each piece of content we produce is reaching its full potential and reaching our most loyal fans? We have a lot of data about the content we produce and we have data on what performs best on our various social channels. We were curious to learn if Tech could make social distribution more effective through data-powered publishing recommendations, enabling social publishers to focus on various more impactful tasks. We began by discussing this idea with our social media team, my engineering team and our data science leads. Everyone was excited about working together to see if data-driven publishing had potential. We then started building a machine learning model that suggested content to publish to a page based on previously performed well on that page. Our team tested the model in a lightweight way by surfacing the recommendations in Slack for users to test: Our users found the initial recommendations useful and we’ve continued to expand data-science investments: We built a model to help social find trending older content to repromote when new content output is low. We built similarity recommendations to help social find content that’s similar to high performing posts and keywords for their page. We’ve worked on a scheduling model to suggest an optimal publish time for a specific post based on past performance of similar content. With each new model we built, we partnered with social publishers to test it and make sure it actually worked. Now, we’re starting to build new tooling that will incorporate this data-science work and allow social publishers to: Quickly schedule recommended content to their pages by dragging content from a recommendation queue to a page’s publishing schedule. “Dismiss” recommendations, so that our data science models can learn from human feedback and become smarter overtime. Easily reference useful data (like progress towards social engagement or output goals) to help them make informed decisions on what to publish. We’re excited to see how this new tooling helps the team take BuzzFeed’s social media game to the next level and deliver all of our best content to our audience. Our work in data-powered workflow tooling is still evolving, and I’m learning new things every day. I’ll leave you with my top lessons learned: Using artificial intelligence and machine learning to improve workflows is not about replacing jobs. It’s about giving smart, creative people more time to focus on work that computers aren’t good at. We’re trying to build a future where social publishers spend less time on tedious tasks like finding the content we’ve already made and more time on impactful tasks like collaborating with creators to make better content, experimenting on new platforms like IGTV or growing new audiences and channels. Avoid using data science buzzwords when collaborating with users. We noticed using words like “machine learning” and “artificial intelligence” in user conversations can be confusing. Instead of saying, “We’re going to build a machine learning model to make recommendations for posts with a high confidence score for your social page.” Say, “We’re imagining using data to generate a queue of recommended posts for each page. It would look something like this… [show people a wireframe or design.]” When exploring something experimental like improving workflows with AI or ML, it’s important to test in the lightest way possible. When we start to work on a new model, we often surface our recommendations in third-party software like Google Sheets, Mode reports, or Slackbots at first. This allows us to move quickly, focusing most of our time on building and refining the models. We don’t want to build new tools (or even integrate the models into existing tools) until we have confidence that our models work. It’s important that the data science models serve realistic user workflows. When we started working on this project, we discussed two approaches: (1) go to a piece of content and see suggestions for where it should be published or (2) go to a page and see suggesting for content that should be published to that page. The first option seems like a logical place to start, but it doesn’t work with how our social teams are organized. So, we went with the second option. It’s important to build solutions that are realistic to your company’s internal workflows… otherwise, no one will use them. Don’t abandon pragmatic wins when working on experimental projects. Just because we’re working on more experimental data-driven curation recommendations doesn’t mean that we’ve completely given up on delivering pragmatic workflow improvements. We still love to work simple, crowd-pleasing features and enhancements into the product roadmap. Do you work on products that use AI models to improve workflows? What have you learned along the way? Send us a comment below. BuzzFeed Tech is hiring. If you are interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 610 Product Management Machine Learning Artificial Intelligence Internal Tools Buzzfeed Posts 610 claps 610 Written by Senior Product Manager @ Twitter, previously built products @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Senior Product Manager @ Twitter, previously built products @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-22"},
{"website": "Buzzfeed", "title": "unleashing the", "author": ["Shraya Ramani"], "link": "https://tech.buzzfeed.com/unleashing-the-a6a1a5da39d6", "abstract": "Latest Posts Events Apply To BuzzFeed Today we are open-sourcing sso , our single-sign-on authentication proxy — known internally as the S.S. Octopus — used to secure access to internal services at BuzzFeed. This post will cover an overview of the history of authentication at BuzzFeed, why we wrote sso , and a little bit about how it works. We think sso is a useful solution to a problem that many organizations face. Because we’ve benefited so much from the open source community, we’re excited to give back by making it available for anyone to use! BuzzFeed’s software ecosystem is comprised of hundreds of microservices that interact with each other in a variety of ways. A subset of those services are applications exposed on the public internet which must only be accessible to privileged users. As BuzzFeed’s global employee base grew, the need to expose tools to the internet for internal use became more apparent, which created an equally growing need to secure those applications with user authentication. To establish a single source of truth for identity, we standardized on how we protect each of those applications by using Bitly’s open source oauth2_proxy service, which is a reverse proxy that uses third party OAuth2 providers (Google, GitHub, and others) to authenticate and authorize requests. Google’s identity aware proxy , which embodies their Beyond Corp philosophy , was an inspiration for us. It’s a generally effective pattern for microservices because it allows the developers to focus on their services’ primary functionality instead of reimplementing authentication every time. BuzzFeed’s use of oauth2_proxy allowed developers to rapidly grow the number of internal applications deployed on the platform. For a while, using oauth2_proxy in front of services was an easy drop-in solution for developers creating services; however, as the number of services grew more rapidly over the years, the solution was not as scalable as we had hoped. As operators, managing the proliferation of boilerplate auth proxy services proved difficult. Critical security fixes required 100+ patches and deploys, since each protected microservice had its own auth proxy service to go with it. Auditing and controlling access across those services was also an ongoing challenge. Scalability issues were not exclusive to the operators and developers. End users were required to sign into each application separately, which could be frustrating and confusing. These separate logins also prevented the development of seamless workflows between related tools. Finally, this had the unintended side effect of training users to blindly click through the OAuth2 login flow, instilling bad security habits. Our solution to these pain points is sso , which allowed us to replace every individual oauth2_proxy service with a single system providing a seamless and secure single sign-on experience, easy auditing, rich instrumentation, and a painless developer experience. sso is an OAuth2-friendly adaptation of the Central Authentication Service protocol (CAS). The CAS protocol uses a “federated” approach, where all authentication is handled by a centralized service, instead of individual applications. Our implementation is comprised of two services, sso-auth and sso-proxy , that cooperate to perform a nested authentication flow and proxy requests: sso-auth is the central authentication service, which directs a user through an OAuth flow with a third-party provider (e.g. Google). sso-proxy ensures all requests are authenticated and authorized according to sso-auth before proxying them to upstream services, and signs requests to allow verification that the requests originate from sso-proxy Both sso-auth and sso-proxy store user session information in long-lived, encrypted cookies, but sso-proxy transparently re-validates the user’s session with sso-auth on a short, configurable interval to ensure quick propagation of authentication/authorization changes. When an organization would like to secure their services behind sso, they create a wildcard DNS entry *.sso.pacworld.com which points at their deployment of sso-proxy * They want to use sso to secure their service, which is deployed at ghost-land-internal.pacworld.com So, they add ghost-land to their sso-proxy configuration** file: 4. Now, their employees can securely access Ghost Land at ghost-land.sso.pacworld.com * In practice, we usually create a more user-friendly domain like ghost-land.pacworld.com that points to ghost-land.sso.pacworld.com . ** sso upstreams are defined using a static config file, and ‘service discovery’ is handled by DNS. When a user visits an sso -protected site for the first time , they are redirected to sso-auth and prompted to authenticate with an authoritative third party provider (Google) before proceeding to their destination. When Pinky visits a different sso -protected site for the first time, their browser will be redirected to sso-auth , and will immediately be redirected back to sso-proxy because they have already authenticated, logging them in automatically and transparently. Every step along the way, sso-auth ensures that the user is authenticated against the authoritative third party OAuth provider, and sso-proxy ensures that the user is authorized to access each specific upstream based on their email address and group membership. With sso , the process for adding authentication to service is now much more straightforward — just a simple configuration change. Maintaining the security of our services is also much simpler; a security bug fix now only needs to made in one place, rather than 100. Users love it too — they only have to login once to be able to access all of the services behind sso , rather than having to login many times! We have clear visibility into sso because of the rich instrumentation baked into the system, including statsd metrics and structured logging, that allows us to have a better understanding of our internal services. As mentioned above, sso is built on top of Bitly’s open source oauth2_proxy , which has been community verified and hardened. Throughout its development at BuzzFeed we have made sso a priority target for penetration testing by researchers on our bug bounty program — we’ve paid bounties for a number of reported issues! In preparation for open sourcing we also engaged with Security Innovation, a widely respected agency who count Microsoft, Symantec, and Amazon as clients, to do a more in-depth, week long assessment, with full access to source code and design documents. This found no major issues, which gives us the confidence to open source sso today. However, being mindful that the security landscape changes rapidly, we will continue to make sso available for BuzzFeed’s bug bounty program and encourage responsible disclosure of any security issues there! Here is the link to the GitHub repo and quickstart guide. We’d love your feedback, so please try it out and open some issues (or pull requests)! First of all, this project would not exist if it weren’t for Justin Hines , who developed the central ideas and helped bring it to life with our original founding team, Michael Hansen , Will McCutchen , and myself. We’d also like to thank Andrew Mulholland , Dan Katz , Dan Meruelo , Eleanor Saitta , Logan McDonald , Lystra Batchoo , and Matt Reiferson , for their work on open sourcing this project. Thanks to Kelsey Scherer for our amazing octoboi logo, which we love very much. Finally, we extend a huge thank you to the BuzzFeed organization as a whole for valuing open source work and supporting our team throughout this process, especially the Infrastructure squads! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 1.6K Thanks to Matt Reiferson , Logan McDonald , Andrew Mulholland , Caroline Amaba , and Will McCutchen . Security Open Source Oauth Software Engineering Buzzfeed Posts 1.6K claps 1.6K Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-27"},
{"website": "Buzzfeed", "title": "i interned at buzzfeed and you should too", "author": ["Brandon Choi"], "link": "https://tech.buzzfeed.com/i-interned-at-buzzfeed-and-you-should-too-3057455712f3", "abstract": "Latest Posts Events Apply To BuzzFeed In the summer of 2016, I spent ten incredible weeks as a software engineering intern at BuzzFeed. On my first day, I arrived awkwardly early and waited at one end of the block, staring down at the bold red letters of the company’s infamous logo. As someone who grew up in an age pervaded by the Internet, I had inevitably known BuzzFeed for its addicting quizzes and hilarious videos. I walked in for my first day nervous and sweaty from the New York heat, with no idea for what the months ahead had in store for me. Every summer, BuzzFeed hires interns across multiple disciplines in the tech organization: engineering, product design, data science, and product management. For ten weeks, the interns work closely with their respective teams to build and ship products that power everything from the distribution of BuzzFeed’s content to the landing page of BuzzFeed itself. Moreover, they are granted the opportunities to meet and hear from the company’s top executives including Jonah Peretti and Dao Nguyen. Since my time in 2016, BuzzFeed has continued to invest in its internship program. We have almost doubled the number of tech interns and the benefits and perks we offer them have increased as well. The internship wouldn’t be a true BuzzFeed experience without its random celebrity sightings and bizarre events. During my internship, I got to take a selfie with Stephen Colbert, go to a red carpet premiere, march in the NYC Pride Parade, and meet some of my favorite BuzzFeed stars . This past summer, interns hung out with Crazy Rich Asians actor Henry Golding , wrote their own BuzzFeed quiz , helped plan summer happy hours, watched Jesse McCartney perform in our office, and took part in our annual Hack Week. Even as a current full-time employee, I still admit that there is never a dull day at work. After switching between countless majors, minors, and certificates in college, I ended up studying computer science and creative writing. I was particularly interested in tech but I knew I wanted to be at the intersection of creativity, storytelling, and technology. I naturally gravitated towards BuzzFeed because I saw it as a perfect opportunity to do just that. Most importantly, I wanted to work at BuzzFeed because I saw it as a platform that amplifies the voices of those who wouldn’t normally be heard. While the internship application process has become more streamlined, my path to the BuzzFeed internship was a bit unconventional. A film instructor in college connected me to a managing editor, who then forwarded me to the technical recruiter. I applied as an eager sophomore in college who hadn’t taken many computer science courses or had done any challenging projects. In the end, I was politely rejected and so I spent the summer in a study abroad program but had my sights on the internship for the following year. With more experience under my belt, I applied again in the fall of my junior year and received an offer a few weeks later. I spent my summer working in an internal tools team called Video Tools. We built tools that streamlined the production process for video producers and allowed the social media team to publish them easily across our channels. On my first day, I was briefed on my summer project — the BuzzFeed Motion Pictures Database. Given BuzzFeed’s prolific video production, there was a dire need to properly credit creators and allow fans to learn more about them. I decided to shorten its long name to BFMPDB, which in retrospect was an annoyingly long acronym. The internship was obviously off to a impressive start. Because BuzzFeed’s tech organization is only around 200 employees, interns are trusted with responsibilities on par with those of full-timers. I was tasked to spend my summer building BFMPDB (cringe) by myself from scratch by utilizing the team’s existing services and collaborating directly with product, design, and QA. My manager and team sounded confident in my abilities but in reality, I was freaking out. Luckily, my team was extremely supportive in helping me understand the existing video tools ecosystem. They helped me break down my project into smaller tasks. While the end goal may have seemed daunting, my team’s week-by-week approach made focusing on individuals steps easier and more approachable. In addition, I learned how to work with people in different disciplines. To kick off the project, I met with my product manager to understand the reasons behind its initiative, its key stakeholders, and goals. During development, I collaborated closely with our designer to fine tune the mocks before implementing them and learned to work with QA for regression testing and stress cases. As a first-timer in the tech industry, I learned how to work on a product effectively to properly ship it to production. I was able to finish the minimum viable product (MVP) of BFMPDB before ending my internship and the service is still running today! You can find the links like this in every one of BuzzFeed’s YouTube video descriptions. To do BuzzFeed justice, I wrote this portion in list form! 1. You will not know how to do a lot of things and that is perfectly normal. The goal of an internship is to learn more about the role and industry. Interns are not expected to be experts in their field but should be curious and eager to learn. If so, you will learn A LOT. 2. Set personal and professional goals for the summer. Ten weeks will fly by. Setting professional goals with your manager and personal ones on your own for what you want to gain from the internship can help ensure you have a rewarding summer. Personally, I wanted to learn if software engineering was a good fit for me since it was my first internship ever. (Spoiler alert: I’m still an engineer right now!) 3. Try, try, and try again. While your team is there to help, persevering through strange bugs or obstacles yourself will make you into a better problem solver. Certainly reach out if you need assistance but don’t immediately give up after your first attempt fails. 4. New team, who this? One of the most rewarding experiences was sitting down with my product manager and lead designer to learn more about what they do. Hearing about others’ careers as well as how and why they ended up working at BuzzFeed may reveal other opportunities you didn’t even know existed. 5. Don’t be glued to your desk. While I learned a ton about software engineering that summer, I learned even more about media and tech from the people at BuzzFeed. Value your relationships because those will certainly last beyond your time at the company. A few months after my internship ended, I signed my return offer in November of my senior year. After graduating, I packed my bags and moved from North Carolina to New York City. I decided to return to BuzzFeed for the same reasons that I had initially wanted to intern there. It was an ideal place and rare environment to combine my diverse interests. Moreover, while the rapid dynamics of the media landscape can be frustrating, I found it exciting to be on the forefront of such an elastic and innovative company . Since starting as a full-time employee, I have been working in Distribution Tools, a team that branched out of Video Tools. The bulk of my team has remained the same since my internship but we have grown and expanded our tools to do some pretty neat stuff . Beyond my day-to-day work, I have become an active member of A*Family, one of BuzzFeed’s many employee resource groups that focuses on the Asian American community and its initiatives, like A*POP . Through this group, I have gotten to host author Kevin Kwan , bring in creators from the Tribeca Film Festival, and partake in an epic Diwali celebration . Oh, and I am now finally old enough to drink at our happy hours . If you are interested in the BuzzFeed Tech Internship, you can find the listings and apply here . You’ll have the opportunity to work with an impressive group of passionate people who are pushing media forward. Finally, here are a few additional posts that can provide more information about our internship program: We Were The 2018 BuzzFeed Intern Class And Now We’re Crying In the Club Because Summer’s Over 11 Ways To Have An Awesome Internship At BuzzFeed What It’s Like to Intern As A Product Design Intern At BuzzFeed I’m constantly impressed with the growing number of impressive candidates that we continue to attract and look forward to meeting the next cohort. Hope to see you around next summer! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 391 Internships BuzzFeed Tech Media Careers 391 claps 391 Written by human @ buzzfeed Sharing our experiences & discoveries for the betterment of all! Written by human @ buzzfeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-12"},
{"website": "Buzzfeed", "title": "why i believe a few photos and the best ballad of 1991 can change the face of collaboration at", "author": ["Max Brawer"], "link": "https://tech.buzzfeed.com/why-i-believe-a-few-photos-and-the-best-ballad-of-1991-can-change-the-face-of-collaboration-at-af2f588d9f17", "abstract": "Latest Posts Events Apply To BuzzFeed A few months ago, I rotated into a special project outside of the People Team — to work on business intelligence with our advertising org. Soon after, our squad released a semi-secret document to pose the following question: Everywhere we looked (literally, in each division) someone was asking us if we could help map out the finer points of the organization — the stuff that existing directories don’t cover (current projects, skills, what I work on in detail). Knowing who to go to for what and why was the most common need info-wise, and so #whowhatwhy became the battle cry of our team. In parallel, my switch into HR was motivated in part by the belief that ‘fuzzy’ exercises, like knowing a fun fact or having a coffee chat with a coworker leads to great work. So I got to thinking, how can we innovate the tech we have to be more empathetic and educational? The inspiration came from a conversation with one of our most senior executives. “I know every single person by name in my division,” he claimed. “And I try to recognize them in org-wide emails, but I’m not sure if my leadership team will know who’s who.” Suddenly, really heart-warming emails started to bother me. For example, this from an exec: “Special thanks to Beau Kiniry, who organized the channel launch from the publishing side. And big thanks to Summer Burton, Andrew Gauthier, Michelle Kempner, Lauren Dolgen, Katie LeBlanc, Stacy Noble, Mireille Keuroghlian, Kevin McShane, Mahira Dayal, Cheska Bacaltos, the META team, Kari Koeppel…and so many others!” Like you, who may not work with me, I don’t know who these people are. I’m glad they are being recognized , but I don’t recognize them myself . We have so much readily accessible people data on hand that it’s criminal to be so in the dark about who your coworkers are. So I learned how to make an add-on for Gmail (a brand new feature of Google’s) and finished a simple prototype in about a day. Then I did what I do best: make it complicated by filming and recording an entire music video to promote the product: https://www.youtube.com/watch?v=3AJZ8N6cVEo So what’s actually happening here? Whenever an employee name appears in the body of an email, you are served a card that contains their Workday photo, helpful basics, and special info like Fun Fact and what they work on Via Slack, you can now edit personal information in our internal API. This is a big deal because it puts the onus on the individual to keep some of their meaningful but transitory info up to date In Docs, you can also put a face to a name in one click (for every name that appears in your email) in a nicely formatted signature There’s a lot more potential down the line: we did this in 3 days. I’d love to see the data become searchable, not just the people: for example, “who can I talk to about…” will reference all the user-submitted specialities and help us build internal networks. But let’s put the geeky stuff aside. This project happened because I joined an Employee Resource Group and met a singer. And then met a few more on his team. And because I advertised my musicianship, I met a new lead vocalist in the nick of time. And because I advertised this project to the heads of the Tech team, I met three engineers from the UK who put their limited time into building real, ready-to-use tools. And because I joined an analyst’s skill sharing group, I met a production lead. And because I met him, I got a professional video shoot at the zero hour. I love this video so much because the medium is the message: because I know the people in my organization and what they are capable of, near and far, I have the harmony of a lifetime to show off for Hackweek 2018. Author’s note: I lied for the headline’s sake — the best ballad of 1991 is “ To Be With You ” by a nose. Max is Head of People Analytics at BuzzFeed and loves to write, share and consult on his work. Reach out: inquiries@maxbrawer.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 154 Hrtech People Analytics Gmail Workday Buzzfeed Posts 154 claps 154 Written by Head of People Analytics @Twitch, formerly @BuzzFeed, Google, Nielsen | Support articles like these by buying Max a coffee @ https://ko-fi.com/Z8Z11TR87 Sharing our experiences & discoveries for the betterment of all! Written by Head of People Analytics @Twitch, formerly @BuzzFeed, Google, Nielsen | Support articles like these by buying Max a coffee @ https://ko-fi.com/Z8Z11TR87 Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-03-25"},
{"website": "Buzzfeed", "title": "how we personalized tasty tag search results", "author": ["Jarvis Miller"], "link": "https://tech.buzzfeed.com/how-we-personalized-tasty-tag-search-results-17e8a0618ea0", "abstract": "Latest Posts Events Apply To BuzzFeed The Tasty app just celebrated its first birthday! There are many great things about the app, but there is definitely room for improvement. For example, when searching for recipes via our tags, the results are ordered by the most recent recipes posted. If I usually click on healthy smoothies while searching on the ‘drinks’ tag, I might be a bit annoyed if I search ‘drinks’ again and the first ten results are all cocktails just because they were the most recently posted. Ideally, my search results would be personalized by taking into account my previous interactions. Personalization of results can help improve search metrics in a couple of ways: Decrease the time spent scrolling until you find a recipe you want to click (minimize impressions until a click) Decrease the proportion of times one searches for a recipe and exits without clicking anything (minimize the exit ratio) This article will introduce the technical steps taken to achieve this goal, how Tasty data was prepped to build the model, and how to interpret the results. When trying to build a model, it’s important to figure out how much data you have as well as the quality of that data. To personalize search results based on each user’s previous interactions with your content (in our case, recipes), you should have an idea of which items a user interacted with, and how they felt about that interaction. There are two kinds of data available for how a user felt about an interaction: explicit and implicit feedback. Explicit feedback is related to interactions such as rating a show on Netflix or giving a recipe a thumbs up. Explicit feedback is much richer as we have a concrete way of whether the user had a positive or negative experience, but the data is often very scarce. I’ve personally used Netflix and Amazon for years but have never rated anything! Implicit feedback involves interactions such as clicking on a link, purchasing an item, or viewing a video. Implicit feedback is less scarce but it can be more difficult to infer whether I liked a movie or recipe just because I viewed it. The Tasty app only recently incorporated tips and ratings so explicit data would definitely be too sparse to use, thus I had to rely on implicit feedback. The implicit feedback used is the number of times someone clicked on a recipe in a given time span. This isn’t the end of the world, as companies like Hulu also realize the abundance of implicit feedback and utilize it in their recommendations! Since the data represents the number of clicks for each user, recipe interaction, it’s a large MxN matrix denoted R where M is the number of users and N is the number of recipes. Below is a toy example of what it might look like Each entry represents the number of times clicks for that user — recipe combination. With utilizing click data from the past 30 days, the matrix had about 1 billion entries and a large percentage of these entries were 0. Thus, the implicit Ratings matrix was very large and mostly empty meaning the computer is allocating a lot of space for information that isn’t super helpful for personalizing tag search. Recall that the goal is to personalize tag search based on previous click history. Thus, for each user, we somehow use the non-zero ratings to predict an implicit rating for recipes that the user did not interact with. Then, we can sort recipes by the predicted ratings. To make better use of this large, sparse matrix, I use Matrix Factorization , which consists of decomposing a large matrix into a product of smaller matrices, to get latent/hidden features of each user and each item. This can approximate how a user might interact with an item they’ve not previously engaged with. As previously mentioned, our implicit Ratings matrix is large and sparse, meaning we’re storing a lot of values that aren’t helpful for personalizing tag search. Instead, we could reduce our MxN matrix into an MxK matrix, U and KxN matrix V such that R ≈ U*V. Instead of one sparse matrix, Matrix Factorization will yield two dense matrices including a number of latent features K for each of our items and users. We can then approximate the user f’ s interest in all recipes via the f ’th row of the product of U and V To get these user and item vectors, we must minimize a cost function that contains MxN terms. This huge number of terms prevents most direct optimization techniques such as stochastic gradient descent. Thus, Alternating Least Squares is used instead as it allows us to solve one feature vector at a time, which means it can be run in parallel! To do this, we can randomly initialize U and solve for V . Then we can use the current approximation of V to solve for U . The patterns repeats until we get a convergence that approximates R as best as we can. Ben Frederickson, a software developer in Vancouver, released an amazing packaged called implicit . Getting the user and recipe feature vectors by scratch is non-trivial and his package simplifies things. Here is a brief explanation of the parameters used when initializing the model: Factors: The value of K, which represents the number of latent features for the user and recipe vectors Regularization: Gives a tradeoff between a biased model and model w/ high variance. Increasing this value may increase bias but decrease variance. Iterations: The number of times to alternate between solving for the user feature vector and recipe feature vector in alternating least squares. Model Evaluation Usually in Machine Learning applications, you train your model on training data then evaluate it on data that has never been seen. This can be done by randomly subsetting the data to create a training and testing set. The setup looks a bit like this. Our application needs a bit of a different setup because we need all of the user/recipe interactions to properly factorize the matrix and get our feature vectors. Instead, we hide a certain percentage of randomly selected user/recipe interactions from the model during the training phase. The testing set will be a binary version of the original Ratings matrix — whether a recipe was clicked or not. After we get our simplified matrix, we check during the test phase how many of the recommended recipes (utilizing the feature vectors from the training matrix) were actually clicked on using the testing set. The setup looks a bit like this Now that we have our training set, test set, and model, what can we compare it to? Common practice says a good comparison is between the model recommendations and always recommending the most popular recipes. We did this for the top four searched Tasty tags, meaning we recommend all recipes within a clicked tag then compare it to the most popular recipe in that tag. The metric we use to compare is the area under the Receiver Operating Characteristic (ROC) curve. A greater area under the curve (AUC) means we are recommending recipes that end up being clicked near the top of the list of recommended recipes. Note that since the recipes are masked randomly, there are a number of cases where masked recipes are filtered out because they aren’t under the specified tag. For example, a user had one recipe masked that was under the ‘dinner’ tag, so when I get the mean AUC for predictions under the ‘dessert’ tag, that user will not have any clicked recipes in the test set since their masked recipe was filtered out for not being dessert. For this reason, I also calculate the proportion of cases (denoted prop_non_nan ) where users don’t have their masked recipes filtered out so there is a sort of ‘trust’ in the comparison between AUC values. The first column represents the AUC for the recommendation system and the second represents the AUC for recommending the most popular recipes first. Note that the photo below is a preliminary evaluation on a sample of users and doesn’t reflect the true evaluation metrics. Now that we know our model at least improves upon recommending the most popular recipe within each tag, let’s explore how this can get utilized in the app. I learned a lot during this section since I did not much data engineering experience prior to this internship. We now have our user and recipe feature vectors that will be updated every day to take into account incoming data as users continue to click and new recipes continue to be published. We utilize these to get a recommendation vector: What to do with the recipes that a user has already clicked? There might be a chance that those recipes consistently resurface to the top which defeats the purpose of the project. For now, I get all the indices in rec_vector_scaled that correspond to previously clicked recipes and turn those values to 0 such that they are at the bottom of the barrel. If a user wants to revisit an old recipe, they can use the ‘Recently Viewed’ tab in Tasty or ‘like’ the recipe and effectively save it. Now that we have our recommendation vector, we can create a dataframe with the recipe IDs and their recommendation scores, and then merge the dataframe with another dataframe containing tags for each recipe. This allows us to subset to only contain recipes with the tags the user is searching. Here is a small example of how things work: The previous clicks from the user can be categorized into ‘easy weeknight dinners’. This is captured in the recommendations as well. Perfect! Given an arbitrary user_id and set of click tags, we can generate a recommendation of recipes utilizing that user’s previous interactions! I thought my project was good to go, and it could get integrated into the Tasty app. I talked to my mentor and she mentioned that I’d have to brainstorm what should be precomputed and stored in a database versus what should be computed and served to the user while they interact with the app? If I had to compute a recommendation vector, merge to a dataframe, subset to recipes with the clicked tags(s), then serve those recipe ids each time someone added or removed a tag, they might get a significant delay of a few seconds. A solution I thought of was to precompute a recommendation lookup table for every user that includes the top 500 most recommended recipes. It would look something like this These recipes will probably be split among a few tags with some overlap (e.g., 200 of them have the dinner tag, 300 are vegetarian, 100 are vegan, etc) but this number was justified because the majority of users tend to search primarily among a few tags and they rarely scroll anywhere near the 500th recipe. Now, everytime a user clicks a tag, we can query Elasticsearch (a distributed, RESTful search and analytics engine) to get all recipes with the tag, join with the recipes in the lookup table, then serve the results. Since our lookup table is quite small, the join is quick and the users are served the top recipes almost immediately. The next steps involve questions I never thought about until this internship. How much improvement will justify the operational cost of the integration (Big Query, AWS clusters, etc)? How much improvement do you expect to see? To try and answer these questions I talked to a senior product manager to get an idea of the metrics the team would want to see improved. It’s always good for a data scientist to better understand how the product should be impacted by their work. With these metrics in hand, I can now run an A/B test to see if the recommendation system had a significant and practical impact on the ways people use the app! Additionally, there are a few things to do that could potentially improve the recommendations such as augmenting the score based on recipe historical performance (is this a recipe that people actually watch if they click on it?) or based on how recent the recipe was published. For example, people who use the app frequently might be looking for the newest thing to cook for dinner. So there might be a way to promote recently published content while not sticking with reverse chronological posting order. Lastly, this utilized vectors of latent features for both users and recipes. These could be useful for lots of other tasks, such as which recipe should be the first one featured in the “Discover” page. Can we add new carousels (a row of recipes in the app with a theme such as ‘steak dinner’) utilizing which recipes our users tend to click on, etc. So many possibilities and fun ways to utilize this project! I learned a ton working on this project and had fun amidst the bugs and frustration! I hope you all have fun using them to discover your next favorite meal! Interested in becoming an intern yourself? Keep an eye out for internship postings on the BuzzFeed jobs page here! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 388 Machine Learning Tasty Software Engineering Data Science Buzzfeed Posts 388 claps 388 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-27"},
{"website": "Buzzfeed", "title": "paying tech debt forward", "author": ["Plum Ertz"], "link": "https://tech.buzzfeed.com/paying-tech-debt-forward-d4c14d96daac", "abstract": "Latest Posts Events Apply To BuzzFeed It started as most projects in tech generally begin — with the desire to break things while eating sheet cake. Or maybe that’s just a “BuzzFeed” thing. Or possibly just a “me” thing. Something that is definitely not just a “BuzzFeed” thing is tech debt. We leave it all over the place, and we promise ourselves that we’ll definitely take care of it next sprint. Except for that other project we want to get done by the end of this month — but definitely right after that. Right? Cut to three months later, and we have internalized and accepted the fact that a certain variable representing an element container is called “conteiner”, even though we misspell it (or, I guess, spell it correctly) every time we try to use it, and we have to comment out three lines of that one test to work locally because reasons . Everyone has those skeletons in their tech closet: the ghosts of the past that haunt us at night, reminding us of the pain we have caused one another in the name of just getting it done and the grim future we face if we don’t clean it up soon. If you’re thinking, “that sounds a lot like A Christmas Carol, specifically the 1992 motion picture starring the Muppets… oh god, 1992 was over 20 years ago, I should rewatch that,” then you are demonstrating one of the many reason tech debt lingers — distraction . There’s always something more interesting to do than clean out old unit tests. If you’re thinking, “the Ghosts of Christmas Past would be a great name for a tech-debt-based-initiative during the month of December”, then you’ve followed our exact train of thought. With December being generally quiet, why not make a dedicated effort to clean up the tasks we’ve been pushing off for months, and start the year off with a clean conscience? When it comes to the end of the year, there are plenty of reasons why not to take on that project: overlapping vacations, back-to-back three-day weeks, and six seasons of The Great British Bake Off on Netflix. “Clean up tech debt” is a great idea in theory , but in practice, it’s no Mary Berry. If we were going to sweeten the deal, we’d need more than just a sheet cake. We could offer swag, karma, or maybe an extra holiday bonus, but it all seemed so selfish during a season of giving, especially if we were already giving ourselves the gift of a stronger code base. That is when we had our Christmas Day revelation (it was actually in early November, but you get the point): BuzzFeed would donate a sum of money to charity for every tech debt task completed. The more work we did, the more others would benefit. We circulated the idea among the team, and the response was overwhelmingly positive (by BuzzFeed standards): Management was also 💯on board and provided us with financial backing for the donations from our BuzzFeed tech budget. A small planning committee was formed and put together the general outline of how the event would run: The event would last six days, spanning over the last short week of December and the first short week of January (got to love those Tuesday holidays). It would be open to all BuzzFeed offices, as well as all those working from home or while traveling to visit family. In the weeks leading up to the event, teams used a shared spreadsheet to plan out developer availability and the scope and effort level of their tech debt. Code review breaks can be beneficial during extended family stays! Developers would claim tech debt items in the second sheet, and submit a pull request tagged with ghosts_of_christmas_past for us to track on GitHub. Any pull request that was at least under review, if not merged, would “count” towards our donations. The amount of money that would be donated to charity for each completed item would be based on both the priority and level of effort of the task. More important tasks, or tasks that would take more time to complete, would earn higher rewards. Based on our budget and the number of items claimed, we would award money according to this matrix: A short list of tech-focused charities would be selected as the main beneficiaries of the event. When an item of tech debt was completed, the individual who completed it would select which of those charities would receive their tech debt bounty (and were welcome to name an alternate charity if they so desired). And, because it wouldn’t be a tech event without stickers, we printed up a spooky mascot by the name of GhostyGhost: With all of the planning done well in advance of the holidays, the only things left to take care of during the Ghosts of Christmas Past Event itself were to spam Slack with :ghostyghost: emoji and bust some ghosts . You thought you were going to get through this without a Ghostbusters reference, huh? We tempered our expectations going into the event — lots of tech debt items were on the docket to be completed, but would we really spend the time over the holiday to do them? You bet we would. In six days, our team cleaned up 46 tech debt items, raising $1,460 distributed across 7 charitable organizations in the process, including Girls Who Code and OUT in Tech. And this wasn’t just deleting some old comments ( // @todo — delete this ), this work included: Increased test coverage Updated library versions Broke down large chunks of code into manageable files Cleaned up some long-standing console errors I tell this holiday tale of joy and charity not only to bask in warm fuzzy feelings but also to demonstrate just how easy it was to turn a typically slow part of the year into a productive and collaborative two weeks that had a substantial impact both inside and outside of BuzzFeed. It’s a bit early to start my wish list for 2019, but I already know what I want: The Ghosts of Christmas Past to return to BuzzFeed for another round of coding for charity (don’t worry, we’ll have more tech debt by then) Other tech organizations to host their own events and join us! Interested in facing your own ghosts next year? Discussion is open in the comments! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 380 BuzzFeed Buzzfeed Posts Charity Tech Debt Software Engineering 380 claps 380 Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-20"},
{"website": "Buzzfeed", "title": "11 ways to have an awesome internship at buzzfeed", "author": ["Logan Wilson"], "link": "https://tech.buzzfeed.com/11-ways-to-have-an-awesome-internship-at-buzzfeed-ef28bd3a3dfd", "abstract": "Latest Posts Events Apply To BuzzFeed We asked the 2018 Tech interns for their tips on how to have a great summer internship at BuzzFeed. Once we sifted through all the gifs, they actually had some pretty good advice! Listen during meetings and volunteer to take on tasks. There is always lots to do at a growing company like BuzzFeed and more things your team wants to accomplish in a shorter period of time. Finding things that need to be done in your team, even if it’s not explicitly assigned to you, is a great way to gain ownership and learn more during your internship. — Alice Pham (Cornell University), Product Management Intern, Distribution Tools There’s so many ways to get involved in BuzzFeed outside of direct work responsibilities. Helping plan a happy hour or volunteering for our global week of service are great opportunities to have fun and meet all sorts of great people you wouldn’t normally see during regular work. — Michael Yang (New York University), Software Engineering Intern, Data Pipeline Don’t wait until the end of your internship to show off what you’ve been working on. Sign up for a short presentation at one of the many weekly meetings to get feedback early and often! — Melanie Tosik (New York University), Data Science Intern, Distribution Tools Have an intern project that could use a small new feature? Ask if you can add it! There’s no harm in taking the initiative to making a project even better. — Mary Huerta (The University of Texas at Arlington), Software Engineering Intern, Tasty BuzzFeed has offices all over the world — the Minneapolis office has a dog and a salad club! — Hannah Dettmann (St. Olaf College), Software Engineering Intern, App Services Ask for clarity if anything is confusing, your co-workers will be happy you did! — Raymond Berger (Eckerd College), Software Engineering Intern, Data Infrastructure There will be tons of lingo employees throw around because they’re used to everyone knowing what they’re talking about. Or they’ll explain something too briefly without realizing the concept they’re explaining only makes sense in the context of another confusing concept. Ask away so you don’t get frustrated googling things! — Jarvis Miller (University of Michigan), Data Science Intern, Tasty The Tasty kitchen in the NY office runs out of food fast, so keep a stack of plates and forks at your desk to optimize your time! — Emily Koagedal (UC Berkeley), Software Engineering Intern, BuzzFeed News Be open to learning about everything happening around you! An internship is a great opportunity to learn about a company in depth. Talented people at BuzzFeed are super open to talking about their work and their experience in the industry, so you will learn a lot from speaking with them! — Tommy Bai (New York University), Data Science Intern, Ads Team It’s easy at a fun company like BuzzFeed to get lost in all the fun events and perks and forget the goals of what you wanted to get out of your internship. My recommendation for future interns is to always keep track of your long- and short-term goals, whether that’s in 1-on-1's with your manager or something you keep track of in a journal! These goals can be about skills and software that you want to learn, or getting coffee with someone new each week! — Kellie Dinh (Bryn Mawr College), Software Engineering Intern, Commerce / BuzzFeed Originals BuzzFeed is filled with amazing talented people who’ve had incredible journeys to reach where they are today and each of them can provide a lot of insight, in ways that you wouldn’t expect! — Nikhil Sethi (Georgia Tech), Product Design Intern, Video Creation Tools / Growth Don’t be shy! Everyone at BuzzFeed is awesome so get to know them! — Nicholas Gervais (University of Minnesota Duluth), Software Engineering Intern, Distribution Tools I asked Jonah Peretti, the CEO of BuzzFeed if he liked potatoes and then we spent some time talking about leadership. Okay, so most of your questions will be more relevant, but regardless people are very open and eager to answer them, whatever level they are at. I get it — it’s sometimes hard to talk to people. It’s okay to be scared or shy. It’s worth getting out of your comfort zone. It might be awkward, it might be awesome — who knows, you even might get a new potato recipe! — Agata Grdal (Warsaw University of Technology), Software Engineering, Video / Growth Interested in taking advantage of some of this advice? Keep an eye out for internship postings on the BuzzFeed jobs page here! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 430 Thanks to Jessie Wu . Internships BuzzFeed Technology Summer Internships Buzzfeed Posts 430 claps 430 Written by Data Science Intern at BuzzFeed - https://www.linkedin.com/in/logan-wilson/ Sharing our experiences & discoveries for the betterment of all! Written by Data Science Intern at BuzzFeed - https://www.linkedin.com/in/logan-wilson/ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-09"},
{"website": "Buzzfeed", "title": "identifying trending topics for buzzfeed news", "author": ["Logan Wilson"], "link": "https://tech.buzzfeed.com/identifying-trending-topics-for-buzzfeed-news-d1759fac888e", "abstract": "Latest Posts Events Apply To BuzzFeed How does BuzzFeed know what’s trending? On every page of BuzzFeedNews.com , you’re greeted with a big red circle labelled “Trending,” followed by a list of tags for stories that are, apparently, “trending.” One might assume that the editors at BuzzFeed consult an All-Knowing Oracle who would tell them that Ariana Grande was getting a lot of buzz that day. And that would be correct! Well, mostly — the Oracle isn’t really All-Knowing, but rather a combination of data pipelines and analytics. During my internship with BuzzFeed this summer, I helped build this Oracle to recommend content for the trending bar for BuzzFeed News. Its name is TrendyBot! How can we determine if 2 stories are about the same topic? At BuzzFeed, editors assign various tags to their posts to help with categorization. But what happens if the tags aren’t consistent? For instance, one post might use the tag “immigration,” while another post about the same topic might instead use the tag “immigrants.” We explored some recent posts and found that a single topic can have as many as 10 unique associated tags! While all of these tags may certainly be relevant, they can make it very difficult to identify what posts belong together. We can solve this problem with a modeling technique known as word embeddings . Implementing a word embeddings model developed by Facebook’s AI Research Lab, known as fastText , we convert each tag into a vector. We can then calculate how semantically similar one tag is to another by calculating the cosine similarity of their respective vectors. The model was trained on a corpus of text from Wikipedia to recognize that words like “dolphin” and “porpoise” often appear in a similar context and should therefore have similar vectors. Once we’ve vectorized every tag for every post and applied principal component analysis to reduce the dimensionality, we can calculate a single vector to represent each post as the weighted average of its associated tag vectors, assigning higher weights for tags flagged as “primary” and lower weights for tags appearing in a large number of posts (since very common tags like “social news” are not especially useful). Now that we have a vector representation of each post, we can perform clustering. Essentially, we want to assign an arbitrary label to each post such that posts with the same label are similar and form natural groupings — in our case, each group should represent a topic. Typically with most clustering problems, the process is something like: However, news changes fast! With topics shifting by the hour, it would be infeasible to be constantly re-calibrating clusters. Fortunately, we can programmatically analyze several cluster configurations at once by applying an algorithm called “ hierarchical agglomerative clustering .” Suppose we plot 6 post vectors (labelled A through F) in two-dimensional space like so: Posts E & F are closest together, so we join them together first into EF. Posts A & B are the next closest together, so we join them together into AB. Then we join D to EF, then join C to DEF, and then finally join AB to CDEF. We can visualize this process in a dendrogram: Now suppose we “cut” the hierarchy at the dotted line like so: At this level we have 4 clusters: AB, C, D, and EF. We can compute the average silhouette score of each post in this configuration — this is a measure of how similar each post is to its own cluster relative to all other clusters. We can repeat this process for each level of the hierarchy, cutting and computing silhouette scores. The optimal level of the hierarchy should maximize the average silhouette score. At this level, clusters are as cohesive and unique as possible. On a larger scale, this looks something like this: Here, we examine a 48-hour window consisting of 59 posts. After applying the algorithm, we have 36 clusters, many of which consist of only a single post. This is consistent with what we’d expect: BuzzFeed News covers a wide variety of topics, some of which warrant multiple posts, but most of which can be comprehensively addressed in a single post. Which of these topics are important enough to feature in the trending bar? This process is highly subjective — even within BuzzFeed, there’s no consensus about the quantitative definition of “trending.” For this use case, we calculate “trendiness” through the following process: Suppose in one hour, a particular post receives 10,000 views, while all of BuzzFeed receives 100,000 views. That post’s hourly view proportion is 10 percent. Performing a similar calculation for subsequent hours, we define that post’s trendiness to be the maximum hourly view proportion over the given timeframe. We can then score each topic cluster by taking a weighted sum of metrics aggregated from its composite posts, including trendiness, recency, and silhouette score. This ensures that highly ranked topic clusters are popular, recent, and cohesive. To label these topic clusters, we score each tag in each cluster through a similar process, taking a weighted sum of metrics such as cluster similarity, TF-IDF, and recency. This ensures that the recommended labels (assigned to the highest ranking tag for each cluster) are both relevant and unique to their respective cluster, while also reflecting the most recent post in the cluster. So how can we repeat this process multiple times a day, every day? To keep up with the breakneck pace of news, we need to implement this entire process in an efficient, logical manner with an intuitive interface. At this point, we’ll need to put on our software engineering hat to really pull the whole project together. We frame the service as an API developed in rig , BuzzFeed’s internal development platform. This allows anyone at BuzzFeed to send a GET request to the service with their desired parameters, such as timeframe, metavertical, and various weight parameters to control sorting and labelling. The API acts as the “glue” connecting several data pipelines together, allowing the service to query BuzzFeed’s internal data sources for posts, tags, word embedding vectors, and view counts. After the data has been crammed together and processed into a reasonable format, the clustering and scoring engine described above is applied. Finally, the user receives a JSON response of the labelled and sorted topic clusters. The whole process can take a few seconds depending on the size of the data being processed, so we implement a cache with Redis to store the results for future requests with the same parameters. Of course, an API isn’t the most user-friendly interface, so we connect the service to a Slack bot — remember this guy? TrendyBot uses some basic natural language processing to interpret requests, query the API, and format the results. So anyone at BuzzFeed can talk to TrendyBot like so: By default, TrendyBot shows the top 6 topics for the past 48 hours, but can handle more complex commands, like different timeframes or metaverticals. TrendyBot and its underlying clustering and scoring engine are very much a work in progress! In testing, we found that some recommendations don’t always make sense. For instance, unrelated posts may be grouped together, or some topics may be mislabelled. That’s okay! That’s why they’re just recommendations — the editors at BuzzFeed still have total discretion in populating the trending bar. The entire process is dependent on the quality of the data — if editors tag their content in an illogical or inconsistent manner, the clustering process may fail completely. Unsupervised learning problems are notoriously difficult, since there’s often no objectively “right” answer. In this case, the best content to populate the trending bar is whatever best reflects the stories editors want to tell and users want to read. As the methods used for clustering and scoring topics are fine-tuned, the interface will likely improve as well, potentially being embedded directly into the publishing platform to reduce friction in implementing the recommendations. Developing the algorithms and interface for TrendyBot was a fascinating opportunity to better understand how trending news is curated, and to see a product move from an idea into production. Next time you’re reading about whatever Ariana Grande is up to on BuzzFeedNews.com, be sure to give TrendyBot a wave! Interested in becoming an intern yourself? Keep an eye out for internship postings on the BuzzFeed jobs page here! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 184 1 Data Science Trending News Topic Modeling Unsupervised Learning Buzzfeed Posts 184 claps 184 1 Written by Data Science Intern at BuzzFeed - https://www.linkedin.com/in/logan-wilson/ Sharing our experiences & discoveries for the betterment of all! Written by Data Science Intern at BuzzFeed - https://www.linkedin.com/in/logan-wilson/ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-30"},
{"website": "Buzzfeed", "title": "gender pay gap data hack", "author": ["Ola Sendecka"], "link": "https://tech.buzzfeed.com/gender-pay-gap-data-hack-537dca55adf8", "abstract": "Latest Posts Events Apply To BuzzFeed And What We’ve Learned About Being a Host for Tech Events On 22nd and 23rd June, the BuzzFeed UK office hosted a unique event: a Gender Pay Gap Data Hack. We collaborated with Ellpha , Codebar and AI Club to see what happens when you invite 50 data scientists for a day and let them work with some interesting salary data. This was the first time BuzzFeed UK has hosted an event for the benefit of the wider tech community, and we were excited to be picked as the venue of choice! We want to share our learnings about hosting events, in the hopes that you will also feel inspired to lend your work space to the community. But first things first. What is the Gender Pay Gap and how did attendees get the access to this data? In April 2018, new legislation in the UK came into effect that requires any employer of 250 employees or more to publicly publish the annual gender pay gap figures . This information is now available through the gov.uk Gender Pay Gap API . Gender Pay Gap is not the same as unequal pay (which is illegal in UK). “The gender pay gap is the difference in the average hourly wage of all men and women across a workforce. If women do more of the less well paid jobs within an organisation than men, the gender pay gap is usually bigger.” ( source ) We love data at BuzzFeed, so we needed no extra encouragement to get involved in the event. On top of publicly available data, attendees of the Hack had access to data from Indeed , which meant that we could look at some interesting topics, such as how job listing language relates to the reported gender pay gap. Our UK team has quite a lot of event organizing experience (various conferences and workshops), but we have never been on the side of a host venue before. Running this event has given us a lot of insight about how to be a good host! Before the event started, we had prepared the space adequately, by estimating and anticipating people’s needs. We’ve all been to events where we get lost, the WiFi goes down, or you spend agonising time watching people try to get their laptop to connect to the projector properly, so we were keen to avoid this friction if possible. As well as making sure we had enough tables and chairs (which was not trivial!), here are some other things we thought about ahead of time: Connectivity: BuzzFeed UK has an excellent data connection in our office, but the standard guest WiFi network is bandwidth limited — so we liaised with our IT team to set up a temporary Data Hack network which enjoyed a full gigabit of bandwidth. We also had a trolley full of network switches and cables in case the WiFi fell over, but it was rock-solid on the day. We prepared enough sockets to keep people charged, and had plenty of spares ready to go. Inclusivity: Like many companies, BuzzFeed UK shares office space with other tenants in our building — so we gained clearances from the office management to prepare the space to be welcoming to everyone. This included clear signage to the event, coordination with security to help people sign in by their preferred name, and setting up gender neutral bathrooms. Being Welcoming: We wanted everyone to feel at home in our space. We are lucky enough to have a wellness room in our office, where people can take a break from the main event. It has comfy seating, some blankets and a drinks fridge — but most importantly, an open/closed sign on the door to let people know when it is occupied. It’s a small thing, but people appreciate an option to retreat back from an event if they choose to. We also made sure our photography policy was clear, and gave people the option to easily not be filmed at all (we used colour coded ribbons!). Adapting To Challenges : We made sure the event had a clearly communicated code of conduct which people knew how to approach, and could be assured would be enforced. You want to know that the event is making sure every attendee can feel safe and welcomed. The event was two days long: on Friday evening people formed the teams around the topics they were passionate about, while on Saturday each team worked on their selected topic and to present their findings by the end of the day. The proposed topics were: Text Analysis — what can we learn from job ads? Sentiment behind all tweets in #PayGap hashtag, etc.. CEOs & Boards — composition of the board, gender of CEOs. Deep Dives — companies that pay women more on average, analysing charity sector, women in finance, company culture & policies Pay Gap Chatbot — building an interactive way for people to get advice about gender pay gap and help them to negotiate salary better. Media Review — reviewing analytical methods used to draw conclusions about Gender Pay Gap in mainstream media. Each topic was very broad and one day was not enough to get to the bottom of each area. That said, the data scientists still managed to learn some interesting things . View the Twitter moment to see how the event went. Spoiler: it was ✨ AMAZING ✨ . There is a giant difference between allowing an event to happen in your space and actually being an active part of the event. Don’t be afraid to get involved: sponsor the food if you can afford it and help ordering it to your space, make sure to respond to all possible questions as soon as possible, help to spread the word about the event. All these make you someone who people would love to collaborate in a future bvy allowing organizers to focus on merit of the event and provide the best positive value to the attendees. Offering your office space and bandwidth, which you have already paid for, is a valuable way to give back to the community. We hope that this post can inspire you to do the same at your company — there are many small steps, but they add up to make a big difference! We are honoured that BuzzFeed could be a part of the Gender Pay Gap Data Hack event and we are looking forward to collaborate with Ellpha , Codebar and AI Club in a future. Written by Ola Sendecka and Paul Curry . To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 109 1 Gender Pay Gap Data Hack Gender Minorities Buzzfeed Posts Buzzfeed Events 109 claps 109 1 Written by Python enthusiast & community activist. Staff Software Engineer @ BuzzFeed, Django Girls co-founder, conference organizer, Python Fellow. 💖✨ Sharing our experiences & discoveries for the betterment of all! Written by Python enthusiast & community activist. Staff Software Engineer @ BuzzFeed, Django Girls co-founder, conference organizer, Python Fellow. 💖✨ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"},
{"website": "Buzzfeed", "title": "better docs", "author": ["Ola Sendecka"], "link": "https://tech.buzzfeed.com/better-docs-8fccb8ebd6a4", "abstract": "Latest Posts Events Apply To BuzzFeed How We’re Building a Culture of Documentation Every year at BuzzFeed, we run a Global Employee Survey that tells us the areas where we did great and not so great in the past year. In the 2017 survey, there was one area in particular where our tech department felt we did very poorly: technical documentation. This survey triggered the creation of the Better Docs initiative: a group of people who were willing to do something about this problem. One year later, we managed to change the perception of technical documentation drastically from only 29% of the team being happy with the state of our documentation to 70%! Even though there is still loads to be done, this is a huge success, and we want to share what we’ve learned in the past year on how to build documentation into our culture. As a first exercise, we believe it’s important to answer four very basic questions. Your answers will depend on your tech stack and company’s size, goals, and culture. It’s good to be as candid as possible when answering these questions, and try not to fall into the trap of wishful thinking about what is possible or not for your team. What should be documented? How should it be documented? Who should document it? How can you make your documentation discoverable? Let’s take a look at each one in more detail. You might be tempted to say, “Let’s document all the things!” But the reality is it’s not only hard but often suboptimal. Here are the things we decided to document. To make sure we cover crucial elements for our services, we created a standard that our service’s README files should meet. Here’s what the standard template looks like: # _name of the service_ Each service needs to have a section explaining what the service is doing and why. ## Point of contact and Slack channel Then we need to mention who a point of contact is: a team responsible for this project, concrete names of people who would help you with questions, and a Slack channel. ## Usage Usage explains how the service is used. If it’s an API, what do we call it? We try to give real-life examples a curious developer could try when interacting with the service for the first time. ## Running the service Next, there is Running a service, which tells you how to start the service and the ways to access it. ## System Lists all the dependencies and technologies the service uses. Does it sit behind a CDN? Is it Python/Go, etc.? We try to highlight particularly critical or nonobvious dependencies. It’s important to understand — especially if your infrastructure consists of microservices — how things relate to one another. ## Runbook Links to the document that is a compilation of routine procedures and operations the system administrator or anyone in the tech team could carry out when something goes wrong. The person who knows the most about the service isn’t always available when there is an outage, and having a runbook in place helps anyone be empowered to help. ## Monitoring This section covers links to any existing monitoring: logging, dashboards or alarms that help us notice outages as soon as possible. ## Documentation Finally, in this section link to any additional documentation: architecture diagrams, architecture docs, proposals, any business logic documents, and so on. So, in fact, what we require for each service’s documentation is quite limited: only eight points . These eight points cover the most crucial things we need to know about a service from the code and project level. They help us maintain certain standards for our services. Just by looking at what we are required to document in the README file, we will be reminded of questions such as these: Are we actually logging the right things? Have we added necessary instrumentation to our service, and do we monitor any problems and performance issues in the given service? Are we prepared for our service to break? What would be the impact on other parts of our infrastructure? Since BuzzFeed Tech is distributed across multiple time zones, our goal for this document is to make sure anyone, no matter which team they’re on, can understand why the service exists, how it fits into our existing architecture, and how to set it up locally without asking anyone for help. We have shared our README guidelines here if you are interested in more details. Apart from code and project documentation, we also keep system architecture documentation and proposals. Having them documented allows for better collaboration and gives people an equal chance to participate in technical decisions no matter where they are based. One of our Tech Leadership principles at BuzzFeed states that leaders in BuzzFeed “practice generosity,” which means that helping other people grow and broaden their knowledge and skills is something crucial to being successful at BuzzFeed. Documentation is one of the tools we use to share our experiences and be generous with one another. Without it, the impact we have on our company culture would be strictly tied to location and size of our office. That’s why we document things like best practices, processes, workflows, runbooks, postmortems, how we test, how we deploy, deployment checklists, onboarding documents, and so on and so forth. Anything that helps us be a stronger team is a good thing to be documented. Once we know what kind of things we want to document, it is time to decide how it should be done. For code and project documentation, we wanted to keep documentation as close to the code as possible. We decided to use markdown files inside our Github mono repository. Each service requires one top-level README file that should meet the criteria mentioned previously. Any additional code-specific documentation should live in the top-level /docs directory in the service. This ensures that once you fetch your repo, you have documentation about your service in place. For system architecture documentation, proposals, best practices, and workflow documents, we decided to use a shared Team Drive on Google Drive because Google docs allow you to collaborate, comment, suggest, and share documents easily (and you can collaborate with anyone, not only tech folks!); Team Drive allows shared ownership of the file — no more issues with permission to the doc, and the doc is not gone if the owner leaves the company; and it improves transparency and allows people to see how other teams are working. Something you might find curious when looking at our documentation is that everyone in the tech organization has the ability to edit the content. The decision to give everyone edit access was a turning point in building our documentation culture. Trusting that people won’t abuse this power sends a very powerful message: We are all responsible for the state of our documentation. To make this work even more efficiently, we formed a Documentation Team (Better Docs), which sets standards and helps keep documentation consistent; advises upon and suggests solutions for documentation; gathers feedback, finds patterns, and identifies pain points in our documentation; educates and promotes a culture of documentation within the company; and works in a transparent way, making it easy to join and contribute at any point. We created a “Tech Docs and Where to Find Them” Google doc. It compiles the most crucial information about documentation: what we document and how — all the things mentioned when discussing the questions “What should we document?” and “How should it be documented?” We also tried a couple of programmatic solutions (i.e., writing a documentation Slack integration ), but that turned out not to work as well as we had hoped. It turned out that fixing a human problem with technical solution is not necessarily the best approach. What worked better for us in the end was simply reminding people about the documentation standards through email and Slack and by answering questions from the team members and organizing documentation events. Starting a documentation culture in your company might seem like an overwhelming task. If documentation has no clear owner in your company and there are many services missing proper documentation, the amount of work might seem terrifying! Here are some tips we have for you. Gather people who are passionate about the topic and form a working group. Your experiences and needs when it comes to documentation might be completely different as someone else’s, so make sure you invite a diverse crowd to participate. See what you have in common, and share ideas on what could have the biggest impact in your company. Set a great example by encouraging the addition of documentation in your own team. You could start small and use your team to show that you could improve your work by having good documentation in place. Make it obvious where people can go to chat and share ideas about documentation. Where should interested people find advice? Otherwise, everyone will be trying different things to solve their own problems without a chance to share their approaches with a broader audience. And finally — and this is one I love so much: There is nothing more exciting than to build a brand and a community around it. And it’s easier than it seems: logo, stickers, and a memorable name can help a lot with that. It somehow makes it feel more serious and professional. And, in a way, pretty fun… Make some time for everyone to be able to focus on documentation. In January, we ran a one-day event (Doc Day) where our whole tech team was encouraged to work exclusively on documentation. We formed teams, set goals, and tried to make it as fun as possible. We had a tasty breakfast, lots of stickers, and the feeling of a common goal. It was super successful and made people excited about writing documentation, encouraging them to start writing documentation in their projects. Don’t be afraid to bribe people with a pile of cakes or a tasty breakfast if it will motivate everyone to fix the documentation problem in your company. ;) Ultimately what worked for us in BuzzFeed was not writing all the docs ourselves as a small team but creating a culture of documentation where everyone is encouraged to make a difference. In order to make this possible in your workplace, there are some things to keep in mind: Is your company actually ready to support a culture of documentation? I’m sure that if loads of companies were asked if they care about documentation, they would answer, “Yes!” without hesitation. But do they really? If you decide to work on documentation during your work hours, are you sure your work will be noticed and appropriately rewarded? Is it taken into account during performance review and promotions? Is it valued as much as writing actual code? If not, how can you expect anyone to invest time in writing documentation if it is not progressing their career and is not recognized? Does tech leadership support documentation initiatives, and how? (Examples of the support we got for Better Docs in BuzzFeed included public praise, clear communication that tech leadership values the work we were doing, taking our documentation work into account during promotions, and allowing 200+ people in our tech team to stop working on their current projects for a whole day to work on documentation during Doc Day.) Is your company willing to spend money, time, and resources to improve documentation? Are you sure documentation is not one of the invisible tasks people from underrepresented groups end up doing without ever getting recognition for it? If you are able to say that your workplace is able to support you in building a culture of documentation, first: Congratulations, you are very lucky ! And my next bit of advice for you? One of the big things that helped people notice Better Docs was creating a brand: events, stickers, funny but informative emails (full of GIFs), or things that make documentation process a fun experience (like these diagram monsters — thanks, Rowan and Eddie!). I can’t even tell you how excited people were when the monsters were introduced. It’s all about making the process an enjoyable experience. Is it possible to create a culture of documentation in your tech team? Absolutely! It’s not easy — it’s a hard fight to win — but it can be fun and rewarding. And to quote my colleague Alice DuBois (from the email she sent to encourage her team to actively participate in Doc Day): Think back to some time that you asked a colleague for documentation on a service you needed to use and that colleague immediately sent you to a doc that was clear and up to date. HOW MUCH DID YOU LOVE THAT PERSON? Well, you could be loved that much. To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 427 2 Documentation Technical Documentation Better Docs Buzzfeed Posts Google Drive 427 claps 427 2 Written by Python enthusiast & community activist. Staff Software Engineer @ BuzzFeed, Django Girls co-founder, conference organizer, Python Fellow. 💖✨ Sharing our experiences & discoveries for the betterment of all! Written by Python enthusiast & community activist. Staff Software Engineer @ BuzzFeed, Django Girls co-founder, conference organizer, Python Fellow. 💖✨ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-06-20"},
{"website": "Buzzfeed", "title": "have fun storming the castle", "author": ["Logan McDonald"], "link": "https://tech.buzzfeed.com/have-fun-storming-the-castle-8093d19e49ad", "abstract": "Latest Posts Events Apply To BuzzFeed Securing the perimeter through dynamic security group rules for our bastion hosts As developers working in the cloud, we always face an interesting tradeoff: we want our systems to be as secure as possible, but also available for inspection anywhere in the world. Essentially, we need to be able to observe our systems. On the BuzzFeed Site Reliability team, we recently found an example of this tradeoff in how we access our clusters through SSH. We use bastion hosts to proxy to our cluster’s AWS EC2 instances. A bastion host is a special, minimal configuration EC2 instance designed specifically for this purpose. These bastion hosts need to be accessible for ingress traffic from any place in the world. What if an admin is out at a public café and the site goes down? We use a variety of tools and approaches to observe our systems from a distance, so rarely need to login. During critical situations it can be useful to debug directly on a host.. While we want access to the castle, we don’t want to give out the keys without some protection! As an extra layer of defense, our administrators must go through bastions. This is especially important for our remote engineers; however, leaving our ingress traffic open to a wide block of addresses is not a secure practice. Often the solution is for an organization to provide a VPN to access bastions and other internal systems, but that brings its own drawbacks. This blog post will explain how we recently leveraged the dynamic nature of our cloud environment to securely access our bastions. Previously, the security group for ingress access to the bastion allowed traffic from 0.0.0.0/0 , so that on a network level anyone with proper credentials could access them. This brought the risk that a vulnerability in our hosts or the SSH daemon could result in unauthorized access. We wanted to remove this permissive default, so we started looking at how rules could be created dynamically through the AWS API’s authorize-security-group-ingress call . Also, it turns out that since we had implemented our original default rule, AWS had introduced a very handy new feature: descriptions for security group rules . This allows us to add descriptive text to each of our security group rules. We realized that we could employ these descriptions to add some helpful metadata about the user accessing the bastion. You know, use AWS APIs like a database — doesn’t everyone do that? 😉 Another important property of our infrastructure is that our hosts are immutable once provisioned, which allows us to easily autoscale and rotate them frequently. Given the ephemerality of these hosts, we created a helper script that makes it easy for admins to identify and ssh to one through its bastion. Our first step was to update this script. First, we added a new security group to each cluster with no pre-existing ingress rules. We added logic to punch a hole in these security groups by creating a new ingress rule permitting the user’s current IP address. Then, using the new description field, we record metadata of the user accessing the bastion and the timestamp the rule was created. Now, instead of one broad rule, we have many tighter, more descriptive rules for our security group. Given the ephemeral nature of these new user-specific rules, we needed a way to clean them up — a lambda function was a prime candidate! We use Terraform to manage AWS resources, including our lambda functions, so we provisioned a new lambda function to run every minute, looking for security rules for our bastions that were older than our defined TTL. This is where that description field we previously added came in handy. We then use the AWS API to revoke the ingress rules that had descriptions with timestamps that had expired. This worked great! Now we had tighter perimeter security, but, we foresaw a problem: What if the AWS API goes down and we no longer have access to authorize new rules? If the API is down, chances are high that there are other problems we’ll want to have access to our infrastructure to debug. Thus, we decided to provision a default rule that would never get cleaned up, but only allowed ingress traffic from a specific set of IP addresses. Now, in the event of an outage we are guaranteed access while also allowing global access to authorize new rules under normal operating conditions. In the future, we want to add even more improvements to our perimeter security. First of all, while we’ve secured our SSH script for rig , we still have the remaining legacy infrastructure using our old practices and we want to update those as well. Next, we’d like to add an additional element of security and visibility to this process. It would be great for transparency if every time we created a new authorizing rule for the security group we got a notification in Slack. Beyond visibility, eliminating the need for whitelisting any IP blocks would be ideal. In this world, we would only have a single, dynamic security group. Finally, our team is looking forward to implementing SSH certificates, an optimization which will make onboarding and offboarding much easier. In conclusion, dynamically inserting firewall rules to permit an admin to login gives us the perimeter protection we want, with little overhead. Leveraging the programmatic nature of the cloud allows us to tighten our security while also considering a variety of edge cases to keep us both safe and available. If you liked this post and are interested in providing feedback about our security practices or joining our bug bounty program , please contact security@buzzfeed.com . Thanks for reading, and have fun storming the castle! (It would take a miracle.) To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @ BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 311 AWS Security Cloud Security Ssh Buzzfeed Posts 311 claps 311 Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-25"},
{"website": "Buzzfeed", "title": "what is it really like interning at buzzfeed", "author": ["Noshin Begum"], "link": "https://tech.buzzfeed.com/what-is-it-really-like-interning-at-buzzfeed-578d06a72fc6", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed — one of the biggest digital media companies out there. Our videos are lit, the quizzes are addictingly fun, so surely it must be the dream place to intern at. Right?! What did the 2019 BuzzFeed Tech Interns think? Editor’s Note: Responses have been edited for clarity. I worked on Distribution Tools, a team in BuzzFeed Tech dedicated to building software to streamline the process of distributing content across various outlets across the Internet: ranging from BuzzFeed’s owned and operated platforms to its numerous social media channels. My specific project was to build a service that would gather content that is currently trending on BuzzFeed and make recommendations to social curators to post them on various social media platforms. It also marked the recommendations as trending, so social curators could prioritize posting these. The NYC Summer Tech Outing at the Brooklyn Barge! We had a beautiful view of Manhattan and it was great to meet the interns from other offices whom I had only seen before on video calls. Had a great time hanging out with the rest of BuzzFeed Tech 😄 One of the biggest obstacles I faced in my project was getting my service to send recommendation messages that successfully went through the Pipeline Infrastructure. This was not always easy, because there were a lot of services that I had to get familiar with and sometimes even tweak the code for in order to make sure that my service would work alongside the existing infrastructure. I think I’ve learned a lot about what it means to work on a team. I’ve also gained a greater understanding of the types of practices a tech company can engage in in order to make sure their code can be written, tested, and deployed in the most efficient way possible. I’ve gained more experience in writing backend software, React, Python, DataDog, Software Integration Testing, and honestly, the list would keep going on, so I’ll cut it off there lol. Years ago, I saw a good Medium post from Gilad Lotan analyzing partisanship around Israel and Palestine on Twitter. When I saw a job listing for a BuzzFeed data science internship (on Gilad’s team), I decided to apply! I have been working on two projects at BuzzFeed. The first project is focused on BuzzFeed’s comment analysis tools, which use data science techniques to help the organization better understand what users are posting on the site. For instance, BuzzFeed might want to understand if a user is posting a personal story in the comments section. Identifying such comments could give the editorial team ideas for a full BuzzFeed post. I’ve also been working on a second project that is focused on detecting nicknames in BuzzFeed comments. For example, when users post about Ariana Grande they don’t always use her formal name; they might use the name “Ari” or “Princess of Pop”. I am working on a project that tries to detect such nicknames using computer science tools so that BuzzFeed can better understand what our community is saying about particular celebrities. During the year, I’m a graduate student in a research lab, studying a subfield of computer science that is connected to linguistics. It has been really interesting for me to see how the skills and perspectives I have learned from the research world both apply and do not apply in a more practical setting. For example, computer science research often emphasizes novelty: the focus is on developing new methods or applying existing tools in new domains. In a practical setting, such novelty can provide technical advantages, but new methods also incur much higher maintenance and documentation costs. Bridging the gap between theoretical research and practical data science. Lots of hands-on experience with real-world natural language processing. I’m a double major in computer science and theatre, so writing code to bring some sort of entertainment to the world has always been a dream of mine. I’m also a fellow with hackNY , so when we began the matching process, I really wanted to come work at BuzzFeed! I feel so lucky to be here. I’m building an internal tool with John, another intern on the Site Team, to help editors change the navigation bar on the site without having to ask Tech to do it for them. I’ve been working on the front-end portion of the tool in React, which has been awesome. The best parts of BuzzFeed for me have definitely been working with my awesome manager and team, getting to know the other interns, and learning a ton! This is my first software engineering internship (I’ve previously only worked with Girls Who Code), so I kind of started at level zero. It was really tough at first, but it’s awesome to be able to really see how far I’ve come since. I’ve learned so much! Lots about software engineering, for sure, but I’ve also had such great mentorship from my manager, team, and really everyone I’ve met. I’ve also made great friends in the other interns! I wanted to work for a company that wasn’t just a tech company. I’m also super interested in how technology shapes Internet culture and humans in today’s world, so BuzzFeed was an obvious choice. On the first day, we had to take a picture to sign in as a visitor, and I wasn’t tall enough to get my entire head in the picture and I thought I was going to get a picture of my forehead on my badge for the whole summer! BuzzFeed flew out my whole team from all the offices for a week-long planning meeting and as a bonding exercise, we went to Spyscape and made burgers on my manager’s rooftop! I will definitely miss my team the most when I leave! The people I work with are so inspiring, smart and just straight-up cool. They have made me feel so welcome for the summer. I’m working on a project that supports the continuous delivery of configuration files that are edited often. Currently, only core-infra has access to deploy, so this will reduce interruptions in their workflow. So much! It’s been an amazing experience to see how the back-end of the site runs, and how thoughtful everyone on infra is of the developer experience and codebase as a whole! I’ve been able to sit in on meetings where the infra team talks about how to create a culture and system where developers are always innovating which has been a super valuable experience. I’ve always been interested in interdisciplinary fields that involve computer science! Computer science is so powerful, and it’s exciting to see how my programming skills can help solve problems in other industries. I applied to BuzzFeed because it’s a digital media company, so I knew that I could potentially meet and work with people in tech, journalism, marketing, entertainment, design, and so much more! I was also curious to learn about the different ways that software engineering helps a digital media company since digital media is something that I did not know much about prior to my Internship. BuzzFeed has an ERG (Employee Resource Group) for Asian Americans called A*Family, which secured tickets to an early screening of The Farewell with Awkwafina! I really enjoyed seeing a movie with so much Asian representation! 😄 Also, the people on my team from Minneapolis and LA flew to NYC for a few days to hang out and go on a #distro-devs outing! We rode a ferry along the East River and went to a really good pizza restaurant (Keste)! This summer, I’m part of the Distribution Tools team. We build internal tools that help social media strategists distribute BuzzFeed content. Our main tool is PubHub , which automatically publishes content to various platforms like Twitter, Facebook, etc. PubHub has a queue that recommends articles and videos to be published, and my main project this summer is to build a seasonality filter for the recommendation queue. The filter will prevent illogical seasonal content from entering the queue. For example, it would prevent Halloween content from being recommended in July. I also plan on building a small filter that prevents content with not safe for work (NSFW) text from being recommended. I was really astonished by the power of Slack in the workplace! I had a lot of questions throughout my internship, but my team has a variety of Slack channels for different topics. I could get quick feedback or suggestions through Slack. My manager was also incredibly helpful. They were very responsive on Slack and willing to meet with me for a code review or pair programming. I now have a newfound appreciation and excitement for software engineering. Since my team builds internal tools for the company, we often meet with our stakeholders (e.g., social media strategists) to discuss their pain points when using PubHub. That way, we can brainstorm ways to solve their problems. I was also able to shadow two social curators while they were using PubHub. One of them was monitoring the Tasty page and kept having to dismiss Thanksgiving turkey recipes. At that moment I thought, “Wow, my project can actually have an impact on someone and make their lives a little easier!” I grew up watching BuzzFeed videos and interacting with BuzzFeed content, so when I was admitted into hackNY and learned that I could work for BuzzFeed, I jumped at the opportunity! I was also really excited to sharpen my web accessibility and front-end engineering skills on the Site Team here, both of which I’ve been lucky enough to do. Plus, leftovers from the Tasty kitchen didn’t sound so bad either! 😄 I’m building an internal tool with Emily, another intern on the Site Team, to help editors change the navigation bar on the site themselves. I’ve gotten to work on API development, local storage and caching mechanisms, and loads of other nerdy stuff during this project. After the project is done, I’ll be working on creating some components for BuzzFeed’s internal component library! We had a happy hour that was entirely Shrek themed, complete with a mural of Shrek and Donkey, green face paint, and multiple bloomin’ onions from Outback Steakhouse. My jaw dropped the second I walked in, and I was floored by everyone’s commitment to making it outrageously awesome. To say that this company has a sense of humor is an understatement. Making sure that I was meeting enough people! It was super easy to keep my head down and work all the time, but I knew that I needed to learn more about the organization. I had to muster up the courage and time to meet a lot of people, but it was seriously worth it! Navigating a huge codebase and trying my best to not break buzzfeed.com were also huge obstacles, but ones that my team helped me immensely with. I learned a lot about React, Node, and JavaScript in general, but the most important thing that I learned is that BuzzFeed Tech will let you take over their Twitter if you ask nicely. 😄 I’ll probably never be verified in my lifetime, but the brief week that I was verified was the most power I’ve felt in my life. My memes have never had more reach, and for that, I am truly thankful. I cannot remember exactly why I applied here. My career goal is very clear and straightforward: product designer. What’s unclear to me is work for what kind of company and product. What BuzzFeed attracts me is that it is a consumer product; its users are mostly young people. The design critique. The design team has 3 design critique sessions every week, which is very helpful for me. I learned a lot from it. The biggest project in my intern period was to redesign the comment moderation tool. It’s a large project with a lot of domain knowledge, like the design for performance, design for machine learning tools, and emotional design. The whole design team gave me a lot of support on this, answering my questions, giving me feedback on my research and design work, etc. Too many things: receiving and giving feedback; writing documents; communication; domain knowledge of online communities and digital media; design for the application of machine learning; design to improve the efficiency of a tool; design to increase the engagement of an online community; learning to define a problem and solve it! The culture and people here. I applied to BuzzFeed because I wanted to learn back-end development and saw it has a really great culture. I am primarily a front-end engineer, but backend development and Software Engineering were two skills I wanted to improve on. I am glad I did apply because I have gotten plenty of help from my team on those two fronts. I did have first day nerves. This is my first paid engineering internship. I did not have an idea of how I would fit in BuzzFeed’s work culture, the formalities, and the standards I had to hold myself accountable for. I have learned a lot about office environments since then. I am on the Data Infrastructure team which controls the flow of data throughout BuzzFeed. My project is an enrichment service that adds geolocation metadata to the data flowing through the pipeline. The Interns went out to eat, and I got to know what they were doing and found anime fans with me. Also, free food on the 16th floor will always be the best. Food is the companion of life and the essence of the soul. I think my inexperience with back-end development. Thankfully, my team is filled with many experienced individuals who criticized my work and helped me write high-quality code. Weight, from all those free snacks! I gained 5 pounds, which is a tiny step towards my goal of 180 pounds. I have also gained social skills, communication skills, and a clearer mindset towards things. I’ll miss my mentor, the Interns, and the people here. I have received a lot of help and joy. I attended a Prime Time Intern Tech Talk event last summer, and I was really impressed by how well-rounded the employees were. All the engineers had interests that weren’t just engineering, and that resonated heavily with me. It can be suffocating when people talk about tech things 24/7, but the BuzzFeed employees had a really great work-life balance and were comfortable talking about lots of different things. I was a little nervous just because I had never met a BuzzFeed intern before, so I didn’t know what to expect. I wasn’t sure if everyone would be decked out in BuzzFeed gear or only speak in text abbreviations. But they were all friendly and welcoming! Meetings are surprisingly informal and everyone, regardless of title, is taken seriously and encouraged to speak up. I was able to give insight on some designs for my project and the other people in the meeting were very supportive. Also, a perk is we get private concerts every so often which are AMAZING. I saw Chris Carrabba from Dashboard Confessional and my inner emo kid was revived. I actually ran into him after the show and got to talk to him for a little bit. I’ve run into a few of my favorite video producers too! They were so kind and welcoming, it was definitely a once in a lifetime experience. I’m on the AdsTech team, and my focus right now is front-end development. I’m building a new ad format that plays a video, and then a playlist of BuzzFeed videos. Different advertisers will be able to purchase the format. The biggest obstacle that I faced was working with a large codebase. This is always a challenge when getting situated at a new company, but at BuzzFeed, it was much easier because most people who worked on the code were a few desks away from me. I’d ask for help regularly whenever I was stuck on something that I really didn’t understand, and usually, the person who wrote the code could point me down the right path! One of my goals was learning how to communicate with a big team. I’ve been able to practice this with our weekly standups, and other meetings. We talk often about the projects we work on and solutions that we’re considering. This has pushed me to be as clear and concise as possible when sharing updates about my project and asking for help. I wanted to experience how Data Science is applied in diverse fields, especially in the online news, media, and entertainment industry. There’s always nerves on your first day at a new job. You’re unfamiliar with the people and the company culture, plus there was the added nervousness of potentially running into a celebrity at any point. I saw Ellen Page on my first day in the office! This is my first time working at a relatively new company, so the best part has definitely been the insight gained into the startup tech company culture. I’m writing an algorithm to detect anomalies in product metadata that is continuously streamed in (hourly page views, engagement, revenue, etc.). My biggest obstacle is using Google Products. Working with Google BigQuery (why is LegacySQL the default?), Google Collab (their version of Jupyter Notebook) and the Google Cloud Platform in general. I’m more familiar with other platforms, and so using these new tools added a little bit of a learning curve for me. Visiting BuzzFeed back in 2015 confirmed that I could 100% merge my love for tech, community, and design — and I left feeling so inspired about that and passionate about the work happening at BF. Fast forward a few years later and I met Essence Gant (BF’s Beauty Director) who told me about the summer internship program! I applied and the rest is history! TBH, I was so nervous! The great thing about BuzzFeed though is that it’s such a welcoming environment. Starting off with a huge intern class helped too and made me even more excited to be at such a cool place like BF! I’m on the Tasty team and have been working on a super dope video-centered inspiration feature designed to help users who don’t know what to cook™ 🤔 become users who cook IRL™ 👩🏽‍🍳 . This project has included lots of competitive research, strategy meetings, and a pending A/B test! We’re also currently building out a recipe linking feature that allows food editors to easily link new recipes (like vegan chocolate cake) with already posted recipes (like vegan chocolate frosting). This feature will save our editors lots of time and also keep users within the Tasty ecosystem for longer (yay!). So many things! Being able to work with such brilliant and fun people! Kicking off projects that are gonna transform the Tasty user experience! All of the laughs during team outings! The fun work and the good laughs are what make this place so great. Other fun times include: helping prep for the Juneteenth Happy Hour with BIO (Buzzfeed’s Black ERG), taco Tuesday with the interns, hosting a community tech talk with the interns that over 150 people attended (!!!), and seeing Lil Nas X performed LIVE (I’m still freaking out). I knew coding was hard, but dang y’all — haha! I’ve learned so much about Rig (our internal tool), GitHub, queries, pull requests, and code review thanks to Tasty’s engineering team. When I deployed my first little piece of code, them (and all of Tasty Tech) were also there to cheer me on! It was such an exciting win! I’ve also learned a lot about high-level strategy. Getting in the weeds about a project is SOOO FUN, but also taking a step back and examining all opportunities is just as important! During my interview rounds, I knew I wanted to work with Tasty, but had to build up some major courage to even ask (spoiler alert: I’m glad I did haha). A huge theme for me this summer has been learning how to find my voice and use it! I’ve learned a lot and I’m excited to pay this good lesson forward for someone else! An alum from my college who now works for BuzzFeed shared the opportunity with my school. BuzzFeed seemed like a super personable, empathetic company so I gave it a shot. I was mostly nervous I’d mess up someone’s name. I’m terrible with names. Other than that I just smiled and nodded my way through. I really enjoyed all the Design Club presentations. I definitely look forward to those every week. Those little bursts of pride I get every time I deploy some relatively insignificant change have also been highlights of my experience. So far, I’ve spent the summer working to make the Tasty App feed more engaging by interleaving different groups of recipes alongside the recent recipes. This basically involves taking either trending recipes from the past week or historically popular recipes and crisscrossing them between the recent ones. Controlling my free snack intake was tough for a bit. I’ve successfully limited myself to a granola bar and like two bags of nuts a day. Way more than I realize now, I think. A much better understanding of GitHub (and fear of pushing to master). I also know now how contributing to projects happens at tech companies with multiple offices. Working with people from different locations with different backgrounds has been really great. The Minneapolis office. It’s such a homey location (and it helps that everyone here is very welcoming). I wanted to dip my toes back into the private sector and take a quick break from school (I’m doing a Ph.D. in Computer Science). A ton! I missed my subway stop and ended up being late! I also was the only intern being on-boarded at the LA office, so I got high-quality treatment, but just meeting a barrage of new faces on my first day was definitely a bit daunting. Lunches, just hanging out with the LA crew on and off hours, and Summer Drinks/tea time! Image classification magic on BuzzFeed.com, and bringing those results into the recommendation system. It was a lot of time in the sun. I made friends with a rad bunch of nerds and got to work with enterprise-scale stuff. I also get to see Hollywood on the way to work every day. Also, found out how Rig actually worked and made a fool of myself on slack (still am). The open and exciting atmosphere in the LA office. There’s always something going on. #tastyleftovers I walked into this beautiful building through the posh spinning doors. Signing into reception and waiting to go up into the office was the scariest time ever! I was a nervous mess until I walked into the office and saw a sign that read: “Welcome to BuzzFeed UK Damola and Noshin!” I have been working on BuzzFeed’s Internet points, and it has been so fun! The whole process allowed me to experience every stage in creating a new feature. I got to study the designing process (amazing experience for my creative side), building the front-end of the badge, and then looking at all the back-end and API! It’s so amazing and inspiring seeing how much work is put into these features. When deployed onto BuzzFeed was the coolest (brag-worthy) highlight of my Internship! I worked on other smaller parts, like updating different pages or sections, and I even thought it probably had not much significance it was THE COOLEST EXPERIENCE deploying! There were so many highlights! But meeting and hugging PrettyMuch and their private performance really stood out and pretty much breathing the same air as Eric Nam… Fracturing my arm, so I was working slower than usual but everyone was so patient. Whenever I found something difficult everyone was SOOOO helpful. They helped by demonstrating, sending helpful links, and sending docs to read. I feel like I have gained so much knowledge, new skills, and tricks. This is what I wanted out of this whole experience, I wanted to leave as a better student and person. The obstacles and problems allowed all these smart experienced people around me to share and show all their tips and tricks which I’m pretty sure changed my academic life. I feel more grown academically, as an employee, and as a person. I know how hard I’ve worked, but despite the challenges I’ve been through and the doubts I’ve had, this whole experience taught me that challenges are not life-ending problems; there are people there to help and support you. I feel optimistic and excited about the future. I’m so happy to have met all of the kind, talented, and helpful people around me. Everyone had so much patience when teaching and answering my millions of questions! What I gained here is something I know no other company will ever offer. I also earned my PR llama stuffed toy and named her “Queen”! Everyone in Tech at BuzzFeed London deserves a special shout out: I swear they’re the most helpful, funniest, and kindest people. That’s what I expected and wanted out of BuzzFeed, and I’m so glad that I got a chance to work alongside them. I was nervous about working for a big company. I was worried that it would be like Lord of the Flies, but with super-smart people, so I was pleasantly surprised to find a welcoming and supportive environment. My first project centered around easing the configuration of the recommender system API through separate .yml files for different platforms. The changes made it easier to control the behavior of the API across iOS, Android, and Web platforms. Additionally, the changes made testing the configuration of all the different routes easier. My second project has been about improving ABeagle’s UI/UX, which is BuzzFeed’s internal A/B testing tool. My project was aimed at improving the usability and functionality of ABeagle, by adding new fields and features and redesigning the UI and API to take advantage of said features and fields. The overall impact is users will have all the details of their experiments/feature flags all in one place, instead of having some details in Google docs or JIRA tickets. Additionally, users can choose to see only experiments that their team is conducting! There were so many amazing highlights, there are three that come to mind. My first highlight was when I got to meet Big Shaq, I was so excited because I loved his “Mans Not Hot” song and could not believe I got to meet him. The second highlight for me was meeting my favorite Nigerian Musician Adekunle Gold, I was fanboying so hard because I sing/listen to his songs every day! The final highlight for me was also the social events with all my co-workers, it made me feel right at home and gave me the impression that I could approach anyone! I had imposter syndrome early on, but that went away as I got things merged into master. Merging into master provided me with tangible proof, that I can contribute to the team, and also complete tasks that have been assigned to me! The little voice in the back of my head that was telling “You are not good enough”, suddenly disappeared into the ether never to be heard from again. I learned how to program in Go, which is a language that I was not too familiar with. It was an interesting experience to go from zero knowledge to having code in production. I learned to develop a growth mindset, as opposed to the fixed one I had about my circle of competence and abilities. I have also learned how to properly function within a team, in terms of adapting to a new social dynamic and interaction well with orders. It was apparent from the tasks that were assigned to me and the already existing workflow, that I would need to lean on the other engineers on the site team. This pushed me out of my comfort zone, putting me in situations where I had to communicate clearly with other engineers to get feedback on my ideas or my PRs. Interested in becoming an intern yourself? Keep an eye out for internship postings on the BuzzFeed jobs page here! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 572 Internships BuzzFeed Experience Learning Tech 572 claps 572 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-30"},
{"website": "Buzzfeed", "title": "a night of monitoring and observability with buzzfeed and datadog", "author": ["Chloe (Rowan) C."], "link": "https://tech.buzzfeed.com/a-night-of-monitoring-and-observability-with-buzzfeed-and-datadog-a7e97b10424e", "abstract": "Latest Posts Events Apply To BuzzFeed As you can probably imagine, we have a lot of things to monitor and observe at BuzzFeed. From how long it takes for an article to load to how much CPU that loading is asking for from the server, we watch it all with help from DataDog , who live up to their tagline “modern monitoring and analytics.” Since we’re so into what they’re doing, and since we make such good use of it, we asked them to co-host an event with us about, you guessed it, monitoring! So on April 5, 2018, we threw open the doors with them for this event. First up was Mark Hintz , a software engineer working in Data Visualization for DataDog, to talk to us about how DataDog “dogfoods” their own product by using it internally to dashboard. They don’t just monitor their own systems: they monitor some of the systems that surround them, such as the MTA (check out MTAServiceChecker to see if your train is delayed!) or a marathon being run by a coworker. My personal favorite external system checker is Is Pokémon Go Down or Not , which tells me whether I need to bother putting on sneakers to go “catch ’em all!” Mark’s talk really set the stage for the other talks in the evening, by walking us through how DataDog handles some of the challenges that come their way: ensuring redundancy and backups, making sure that scaling capacity is seamless, and making sure that if something breaks…solving it is as close to a single button push as possible. After Mark, we all took a quick breath before Kenton Jacobson , Director of Engineering for Vogue, Glamour, and GQ at Condé Nast, shared with us how low-noise, actionable metrics are difficult to achieve but invaluable. Using graphs from Condé Nast, including one showing the incredible traffic spikes around the Met Gala (confession, my browser is one of those data points 😉), we were able to understand how the work he had done around refining metrics really helped to drive engineers to the solutions to their problems. After the guests, BuzzFeed took the stage, with a series of three lightning talks about monitoring and observability at BuzzFeed. Felicia Cippoletti, a member of the SRE team focused on databases, spoke about being familiar with day to day trends in our metrics. While having metrics is great, if we’re not familiar with what a “green” or “up” state looks like, it can be more difficult to track down the source of problems. Felicia discussed the importance of knowing what your metrics “should” look like, so you know when they’re telling you something’s wrong After Felicia, we were treated to a bit of a “war story” from Dan Meruelo. Dan shared a time when some of our tagged metrics went missing at deploy time. He walked us through how he discovered that a code interaction between our in-house code and the DataDog metrics agent conflicted, thus leading us to think we had problems. After that dive in, he showed us what the metrics should look like and explained how to get there. To close out the talks, I spoke about the four principles I used in developing a proposal for how to move forward with observability at BuzzFeed. I particularly wanted to share how we were using human-focused ideas such as: reducing the burden of on-call shifts, empowering service owners who aren’t familiar with the ops domain to drive the decisions we make about code, and about how to observe our systems. In addition to all the great information shared, it was a fantastic night where we got to open our doors to the community. Everyone got to meet new folks as well as spending time with friends and colleagues. The crew from DataDog was fantastic, and so was everyone who joined us to learn about observability. We’re already looking forward to the next one! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 8 DevOps Buzzfeed Events Tech Talk Monitoring Observability 8 claps 8 Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-02"},
{"website": "Buzzfeed", "title": "we were the 2018 buzzfeed intern class and now were crying in the club because summer s over", "author": ["Michelle Chiu"], "link": "https://tech.buzzfeed.com/we-were-the-2018-buzzfeed-intern-class-and-now-were-crying-in-the-club-because-summer-s-over-dba1dcb85e0a", "abstract": "Latest Posts Events Apply To BuzzFeed From partying it up during the tech offsite boat outing to teaching our teams how to make pierogis from scratch, us BuzzFeed interns can tell you that we’ve shared an unforgettable summer together full of new memories… and lots of free food. We came from colleges, universities, and programs around the globe to work in New York, Minneapolis, and London offices across a variety of disciplines: software engineering, product design, data science, and product management. We’ve also built and designed awesome technologies for audiences that touch BuzzFeed’s products every day, internally and externally. Keep on reading to learn more about our backgrounds, the impact we’ve made during our time here, and some fun facts we’ll probably use in an upcoming ice-breaker. Cornell University, 2018 • LinkedIn What’s been the highlight of your BuzzFeed experience? Getting hands-on industry experience! BuzzFeed’s culture is amazing and learning directly from super smart, driven people was an invaluable part of my BuzzFeed experience. What did you work on? I worked on a couple different projects on the Distribution Tools team, the team that content curators use to get content to different platforms: YouTube, Facebook and Instagram. I worked on using data science to improve our auto-publishing service, introducing an activity feed and commenting feature into our internal tools, introducing new features for our Instagram and YouTube publishing tools like user tagging and image cropping, launching auto-publishing to some of our biggest pages like BuzzFeed UK (with over 1.7M followers!), and running design exercises for a new tool we’re building out. What’s your favorite piece of BuzzFeed content? I loved BuzzFeed’s Queer Prom series and I really think it shows why BuzzFeed is so great. BuzzFeed gives marginalized communities a voice and provided an opportunity for these awesome teenagers to have an incredible prom experience that most people have when they’re in high school. University of Birmingham, 2020 • Instagram , LinkedIn What’s been the highlight of your BuzzFeed experience? The amazing tech family in the London office! The London office is a lil bit smaller than NYC and is a really friendly environment so I got to know all the super talented people working in different parts of tech. In my second week, we had a phenomenal Bob Ross and wine night where we painted along with Bob and everyone produced surprisingly successful results. What did you work on? My first project was building a tool to allow writers across all the BuzzFeed brands to add newsletter signup boxes anywhere in their posts. After this I worked on an offline page for the news site that caches a roundup of today’s biggest news for the user to read offline. How do the snacks in London compare to New York? In my opinion, London has a stronger snack game than New York although I did miss the froyo machine deeply when I left NYC and reading the daily tasty kitchen leftover alerts from the other side of the Atlantic is a very tough experience. Georgia Institute of Technology, 2018 • LinkedIn What’s been the highlight of your BuzzFeed experience? It’s a tie between having to gather research on quizzes by playing them OR meeting all the people behind Cuppy, @thegoodadvicecupcake . What did you work on? I helped increase quiz result shares by launching a test that added percentiles (i.e. how hard it was to get your score) in the trivia quizzes. I also helped make BuzzFeed landing pages (like Animals and Community) more addicting [I’ll take all that unproductiveness and make your breaks longer mwahaha]. What was the best thing/event/moment that happened that you least expected? So once, BuzzFeed threw a full-blown 90’s party. On a Tuesday. UC Berkeley, 2020 • Instagram , LinkedIn What’s been the highlight of your BuzzFeed experience? I would have to say working on the BuzzFeed News team and being part of the process leading up to the launch day of the new website ! It meant a lot to work on the trending tags bar because it was a very meaningful project that I could point to on the website and say “I contributed to that!”. BuzzFeed has such an amazing community of people so I would also say all the cool events like The Women of BuzzFeed panel and the awesome designers who came and talked at Design Club! Also…I can’t forget the amazing Tasty Kitchen for their constant amount of food throughout the work day! 🙌 What did you work on? I created a new service that updates the articles associated with the current trending tags on BuzzFeed News. This is on a scheduler that checks the trending tags every 5 minutes and updates it in the database. I also created a slack bot called TrendyBot, that sends a notification to the news curators every time this update occurs. TrendyBot also gives them a summary of the current trending tags and their associated articles if they send “@TrendyBot trending” in the slack channel. For Hack Week I also created a robot cat called “Lucky Cat” that waves its arm every time someone “@here”s in the #nyc-leftovers channel, which is where the Tasty kitchen posts when they have food for us. 🐱 Favorite BuzzFeed quiz? Build A Man Out Of Meat To Find Your Introvert/Extrovert Percentage . I got ambivert! St. Olaf College, 2020 • LinkedIn , Instagram What’s been the highlight of your BuzzFeed experience? The community! Working in the Minneapolis office meant our entire office often went on outings. I got to know everyone really well and it felt like the whole office was grieving when my bike wheels got stolen. What did you work on? I built a Twitter API that was used for retrieving tweet data from Twitter. When you see tweets in articles or posts, Twitter Fresh is responsible for keeping the tweets up-to-date and accurate. Favorite BuzzFeed quiz? Which Melancholy Vegetable Matches Your Personality? I got “The Pensive Broccoli.” University of Michigan, 2018 • Twitter What’s been the highlight of your BuzzFeed experience? I was interviewed by a couple of BuzzFeed employees to help create a “What’s It Like To Intern At BuzzFeed” video! I even got to do some vlogging while I worked throughout the week. It’s used internally at the moment but it might get posted on YouTube someday! What did you work on? I started off doing a few things for our product team. I created a slackbot that reports out-performing recipes daily and made a couple reports tracking user adoption of certain features on the app. Then I started my larger project, which involved building a recommendation system to personalize results when searching for recipes via tags in the app. Favorite celebrity to visit the office? The one and only Mason Ramsey. We stan a young king. Your fave yodeler could never! ICONIC! Bryn Mawr College, 2019 • Twitter What’s been the highlight of your BuzzFeed experience? The best experience I’ve had at BuzzFeed is being a part of this amazing community and meeting so many talented and cool people from all different divisions and squads! Through A*POP events, summer Happy Hours, Global Service Week, and the #nyc-coffee-buddies [Slack channel], I met people I would have not otherwise interacted with during my internship. The best part about BuzzFeed is the people, and one highlight this summer was having my desk decorated with snacks, ribbons and oil blotting sheets (my one true love) on my birthday! What did you work on? I worked on two teams, Commerce and BFO which gave me the unique opportunity to learn many different sides of BuzzFeed Tech. On Commerce, I helped build front-end components to advertise Prime Day that were shipped to BuzzFeed.com and seen by hundreds of thousands of people! I also helped build a unit to display trending stories on As/Is articles. On BFO, I worked on Quizzes, everyone’s favorite way to spend time on BuzzFeed. I helped build an internal feature to help the Editorial and Creative teams view the result cards for the quizzes they make. By being on these two teams at BuzzFeed I got to see how fast things move and are shipped to users first-hand! If you could recreate any BuzzFeed video, which would it be? Any of the Worth It food videos, but in particular I would love to create the buffet episode or the episode comparing fried chicken! UCLA, 2020 • Instagram , LinkedIn What’s been the highlight of your BuzzFeed experience? It’s all been a highlight tbh :’) I’m gonna pick two highlights: one, the amazing learning opportunities available at BuzzFeed and two, the amazing, supportive, and talented community that I get to work with everyday. What did you work on? As an iOS Engineering Intern, I worked on creating the iPad version of Tasty from scratch. It launched just at the end of July, and it was absolutely incredibly to think about how, just like that, my work was in the hands of millions of users. Favorite BuzzFeed quiz? Plan A Party And We’ll Tell You If You’re More Of A Flamingo, Peacock, Or A Swan . Lmao I got swan. Northwestern University, 2018 • LinkedIn What’s been the highlight of your BuzzFeed experience? SO. MANY. SNACKS. Seriously, froyo on demand & Tasty leftovers were more than I could have ever asked. Also, I guess the people I worked with were pretty alright too. What did you work on? I developed an algorithm for identifying trending topics in the news using natural language processing and agglomerative hierarchical clustering. I also built an API to implement the algorithm to recommend content to editors for the trending bar on the BuzzFeed News site. What’s the best thing you learned this summer? Courtesy of Tasty, I learned that 100 crepes can, in fact, be stacked into a single incredible cake, and attempting to eat said cake will result in being too full and sleepy to get any work done for the rest of the afternoon. The University of Texas at Arlington, 2019 • Instagram , LinkedIn What’s been the highlight of your BuzzFeed experience? The highlight of my experience has been working with an amazing group of people on the Tasty team! They are some of the hardest working and smartest people I’ve met. What did you work on? As a software engineering intern on the Tasty team, I developed an internal dashboard for moderating Tasty’s new tips and ratings. Users on Tasty are able to submit ratings, tips, and photos for any recipes, which which will be viewed by thousands of people. This dashboard allows moderators to view, sort, edit, hide and publish the several thousands of tips a day. I also demoed a user profile page that allows moderators to view and edit a specific user’s information, tips, and status. What’s your favorite Tasty recipes? Any of the Tasty 101 recipes, they are so helpful when starting to cook! NYU, 2019 • Twitter What’s been the highlight of your BuzzFeed experience? All of the data! BuzzFeed is the largest company I have worked for to date, and it has been an incredible experience to have a massive number of text data sets readily available (and easily accessible)! I also really enjoyed working for a company that is tech-savvy but not necessarily tech-first; it was fantastic to be part of a highly cross-functional team that ultimately enables social strategists to source and schedule high-quality content even more effectively. What did you work on? I worked on the Distribution Tools team, which is focused on building products and services that help BuzzFeed grow our off-site revenue and engagement. For my project this summer, I implemented a machine learning model that provides related content recommendations to an input query or list of previously published BuzzFeed articles. The primary goal of the project is to surface more targeted, relevant content to the users that follow BuzzFeed’s Facebook pages, as well as to reduce the manual work that content curators have to do to discover the right content for their topic-based pages. Why work at BuzzFeed? Over the years, BuzzFeed has managed to build an incredibly diverse tech organization. During my time here, I continuously felt inspired and encouraged by a culture that is thoroughly welcoming and stimulating across all verticals! NYU, 2020 • Website What’s been the highlight of your BuzzFeed experience? Racing down to the Tasty kitchen from the 16th floor; all the friendly people; making boba for the A*POP happy hour; debating the latest #froyo flavor on Slack; building an IoT lucky cat for Hack Week. Where did the time go 😤😪 What did you work on? I built out the data warehouse monitoring tool, which we use for Slack alerting on the health of our data warehouse importing processes. I also participated in the Better Docs working group to help improve tech onboarding and documentation. What is your favorite BuzzFeed quiz? “ Would You Be A Better Date Than My New Man? ” Apparently I’m no match for Percy :-( Georgia Tech, 2018 • Website , Twitter What’s been the highlight of your BuzzFeed experience? Getting to know everyone that’s a part of the BuzzFeed family! In addition to effortlessly rocking the trendiest outfits and dropping high quality memes on our Slack channels, the people here constantly inspire with their humor and dedication. Also, being able to work on a digital product that touches the lives of so many people has been so fulfilling! It still boggles my mind to know that millions of people will potentially see the designs I create for Tasty. What did you work on? I had the opportunity to design how nutrition information would be displayed across Tasty platforms! Also, I worked on designing a new experience for Tasty.co users to lean into video-binging. If any of you reading start watching more of our recipe videos, it’s partially my fault oops 🤭. Outside of my main projects, I was also able to help create promotional artwork for our apps and redesign BuzzFeed’s jobs page for Hack Week. What’s your favorite moment from the summer? I’d have to say when Jesse McCartney visited the office and sang “Beautiful Soul.” All of my teenage dreams came true at that moment! Also, my goal of being featured in a video was fulfilled — be on the lookout for a “What It’s Like To Intern At BuzzFeed” clip on the site soon! University of Minnesota Duluth, 2019 What’s been the highlight of your BuzzFeed experience? My highlight was the fact that I got to work with a super talented and friendly team! I’m honored to have been surrounded by so many amazing engineers who taught me how to truly out smart and innovate my way through challenging problems. Additionally, working at BuzzFeed allowed me to inquire and engage in my burning interests of software engineering and data science. What did you work on? As a part of the Distribution Tools team, I was able to work on few different things including Pubhub’s API, syncing Pubhub’s drafts with Facebook, and creating a new API to provide recommended times to schedule drafts. Also for Hack Week I messed around with machine learning models training on video data. I am so thankful for the software engineering and data science experience I gained through these projects and for everyone who taught and worked with me on the way. Thanks BuzzFeed! What show on Netflix did you binge watch embarrassingly fast? With all my spare time during freshman year of college I binge-watched The Office a little too quickly, but it’s the greatest show ever so it was time well spent. Georgia Tech, 2019• Twitter What’s been the highlight of your BuzzFeed experience? BuzzFeed is filled with amazing people that make every day interesting. The design team spends a minimum of three hours together a week during crit, and it’s always exciting to work in a place where you look forward to meetings because of how fun everyone can be. In particular, I want to shout out my design buddy Elaine for always providing amazing guidance and cool historical context about nearly everything and mourning Blockbuster Video with me. What did you work on? I worked on a couple of different features for Vidder, our internal video editing tool . I mainly focused on creating a better experience for exporting and previewing videos and making clip manipulation such as captioning and trimming. I also worked on A/B tests for the content pages and spent some time trying to improve messaging within our design system, Solid. Which BuzzFeed quiz do you resonate most with? “Make Carbonara With All The Wrong Ingredients And We’ll Reveal What Kind Of Energy You Give Off” by Sarah Wainschel is a cursed quiz that exemplifies what is great about BuzzFeed and reminded me that I’ll never die. PS : I’m very active on Twitter! Feel free to DM me any questions you may have about design at BuzzFeed, or about anything else on your mind. Eckerd College, 2018 • LinkedIn , Twitter What’s been the highlight of your BuzzFeed experience? Probably the best part of my BuzzFeed experience is that I got to live in NYC for the first time. I’ve never lived in a big city before so coming here, working at an exciting company, and spending the summer surrounded by awesome people was definitely the best thing for me! What did you work on? BuzzFeed has an internal service that runs jobs on a schedule. Previously if anyone wanted to add a new job they had to go into GitHub, manually create a file, and request that it be added in. I created an API so this process could be automated and a UI so that it was easy for anyone to do! What was the most fun thing you did with BuzzFeed? Everyone in tech got to go out on a boat ride for two hours with lots of food and drinks! Rutgers University, 2019 What’s been the highlight of your BuzzFeed experience? Hands down, the people (shoutout to my team and fellow interns!). This summer I’ve been able to both challenge and enjoy myself to the fullest, from banging my head over code (whether it be alone or with others) to my team’s outing, or even our only-partly-successful intern “brunch”. Many thanks to the people around me who inspire me every day, and especially to my mentor for all of her guidance! What did you work on? Most of my efforts this summer have been focused toward building a new buzz format, an interactive infographic, as well as its builder tool within our content management system. I even had the opportunity to do some product design work for my project during Hack Week! I also worked on two Slack bots, one of which allows BuzzFeeders to quickly and easily check the versions of our internal packages. Since we use a lot of internal packages and versions are constantly being updated, it’s convenient to be able to check versions and make sure our services are using the most up to date packages with a single Slack command. If you could recreate any BuzzFeed video, which would it be? I would love to recreate “ I Learned How To Dance in 30 Days ,” my all time favorite BuzzFeed video! Ashly is so relatable and her progression throughout the video is really inspiring! She talks about her journey in gaining self-confidence through dance and learning to just let loose and have fun. I started dancing just over a year ago, so I’ve been there (and honestly, I still have a long way to go). And how could I pass up the opportunity to learn and dance with Kyle Hanagami, Haley Fitzgerald, and Emma Hauser?! NYU, 2019 • LinkedIn What’s been the highlight of your BuzzFeed experience? It was an amazing internship! The highlight of this summer was working with a boss who values my work and always gives me constructive advice on my project as well as career development. What did you work on? I worked on analyzing Adx programmatic data. The goal of the project is to optimize the BuzzFeed’s revenue from running programmatic ads. I conducted in depth analysis on the programmatic data and developed machine learning and deep learning algorithms to predict ads prices (effective cost per thousand impressions) and identify leading variables. Through this internship, I obtained domain knowledge in programmatic ads on publisher’s side, and obtained hand-on experience of applying data science power to benefit business! Interested what I did? Plz go to https://medium.com/hetian-tommy-bai . What was the best moment during your summer at BuzzFeed? Team outing in Brooklyn, Go Crabbers!!! Interested in becoming an intern yourself? Keep an eye out for internship postings on the BuzzFeed jobs page here! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 332 Thanks to Caroline Amaba , Nikhil Sethi , and Jessie Wu . Media Internships BuzzFeed Tech Learning 332 claps 332 Written by Product designer at MongoDB (155K+ views). Prev. Microsoft, Apple, DoorDash, BuzzFeed, Wish, & Frog Design. Tweets found at www.twitter.com/michelle__chiu Sharing our experiences & discoveries for the betterment of all! Written by Product designer at MongoDB (155K+ views). Prev. Microsoft, Apple, DoorDash, BuzzFeed, Wish, & Frog Design. Tweets found at www.twitter.com/michelle__chiu Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-01-27"},
{"website": "Buzzfeed", "title": "more data more problems", "author": ["Matt Semanyshyn"], "link": "https://tech.buzzfeed.com/more-data-more-problems-3c585b8bc84d", "abstract": "Latest Posts Events Apply To BuzzFeed How BuzzFeed Scaled its Data Operation Data has always been integral to BuzzFeed’s success. It allows team members to build data-driven products, evaluate how our content is performing, and ask questions to more deeply understand our audience — all to ultimately inform BuzzFeed’s overall strategy and create the best experience for our users. Our data originates from many sources and covers a large footprint, including anonymized first-party tracking, third party analytics (Google), platform APIs (Facebook, YouTube, Instagram, etc), and internal applications (content metadata from MySQL databases). Where we have control of this data, we’ve worked hard to improve it at the point of creation. Our first-party tracking, for example, was recently redesigned and reimplemented to employ a modular schema design, ensuring consistency and flexibility across all our products. To meet the increasing demands of these data sets, our Data Engineering group invested significantly in our data infrastructure over the last couple of years. We migrated our data into Google’s BigQuery and reworked our ingestion pipeline to import new data into the warehouse in near real-time. With this foundation in place, we are now ingesting tens of thousands of records per second, totaling nearly 2 TB of data per day. This process is fairly unopinionated; by simply specifying a schema, relevant database dumps or event stream log files get ingested into BigQuery without any transformation. While the availability of this data in BigQuery in near real-time unlocks a multitude of ways in which it can be leveraged, we quickly realized more data also leads to more problems. In this post we’ll detail these challenges and how we ultimately worked past them to empower our organization to scale its use of data while also simplifying our data infrastructure. Transforming the Data BigQuery, while effective at storing large data volumes (totaling over 2 Petabytes across all of BuzzFeed’s datasets), requires special considerations when querying it. All BuzzFeed BigQuery queries share a fixed pool of 2,000 slots — units of computational capacity required to execute the query. BigQuery calculates the number of slots required by each query based on its complexity and amount of data scanned. Inefficient or large queries will not only take longer to execute but can also potentially block or slow other concurrent queries due to the number of slots it requires. Table JOINs in particular can become computationally expensive because of the way that data needs to be coordinated between slots. As such, BigQuery is most performant when data is denormalized. Since our data is imported into BigQuery in its raw form, we needed a way to optimize it into representations that capture common query patterns and transformations. (So, for example, we want to do things like aggregate individual page view events into hourly totals or create a denormalized representation of our core content metadata.) To achieve this, we’ve built a “Materialized Views” system. On its surface, the system is fairly straightforward: given a SQL query, run it periodically and save its results in a new table that can be queried independently from its source data. On closer inspection, however, you’ll see a much more complex system that tracks dependencies, schedules and triggers full and partial rebuilds of tables, orchestrates rebuild execution to balance job priorities against the fixed BigQuery slot allocation, provides tooling for creation and validation of views, and enforces change management rules to ensure reliability for downstream consumers of the resulting tables. With over 200 views in production, the tables created by the Materialized Views system have become the primary data access point for data in BuzzFeed, supporting over 80% of our reporting. Standardizing the Data Given the varied nature of BuzzFeed’s data, understanding what data is available and how it relates can be difficult. To lower the barrier of entry for working with this data, we’ve introduced the “BuzzFeed Data Model” (BFDM for short). Built by leveraging the Materialized Views system, BFDM provides a standardized and consistent set of tables designed to support a majority of common business use cases. It considers the entire landscape of our raw data and how the various sources relate to one another to provide: Consistency in data granularity Regardless of the source, BFDM provides metrics precomputed at hourly, daily, and lifetime granularities (where applicable). Consistency in terminology and naming By standardizing naming conventions across BFDM, it is easier to find relevant tables, understand what data is available within, and query across them. Clearer relationships Tables are broken out into one of three types: entities, relationships, and metrics. By understanding a set of common fields available on each, any sets of data can easily be JOINed together. Centralization of business logic (i.e. content categorization, relationships, and grouping rules) Data enrichment, clean-up, and error remediation This set of tables makes it easier for teams to work with data, simplifies and optimizes queries, and provides a “sanctioned” source of truth for BuzzFeed’s core metrics. Team members are able to seamlessly query different tables without needing to acknowledge a long list of “gotchas” about the data. Creating a Single Source of Truth Through the years, various differing (and sometimes redundant) approaches were introduced to BuzzFeed’s data infrastructure: Spark jobs aggregated raw page view events into hourly aggregates to be imported into our data warehouse (Redshift prior to BigQuery) Looker Persistent Derived Tables transformed data for its own use A Redis-backed API served transformed and aggregated data to some internal dashboards A Cassandra-backed API served real-time time-series page view aggregates to other dashboards Not only has each of these legacy pieces increasingly become an operational burden, but they have also allowed for potential inconsistencies. While our move to BigQuery introduced one more potential source for inconsistency, it also has provided the key components to allow us to decommission each of the legacy systems in favor of one consolidated approach. Remember, we now can import data into BigQuery in near real-time to be transformed by the Materialized Views system into our standardized source of truth, the BuzzFeed Data Model. With this, the same BFDM tables can be used for ad-hoc queries or within BI tools like Looker. By introducing one more system — an API that runs lightweight queries against BFDM — our internal dashboards can be powered by them as well, guaranteeing consistency across all points of data access. (Not to mention reduced technical debt from each of our decommissioned systems!) Looking to the future These various efforts have left BuzzFeed on a strong footing to continue leaning into its data-driven culture. However, to continue to succeed into the future, our data-powered approach must be understood, valued, and supported throughout the organization — teams need to use our infrastructure effectively, properly instrument their products with tracking, and help BFDM evolve. To help achieve this, the Data Group built out Data Governance processes, resources, and organizational structures: Comprised of a set of “Data Stewards” representing each engineering team at BuzzFeed, a “Data Governance Council” disseminates established best practices in a scalable manner, opens up channels of communication to evolve these best practices in a way that properly represents each team’s practical needs, and facilitates knowledge sharing and collaboration across the engineering organization on data initiatives. A data review process to be completed at the start of any new user-facing initiative helps ensure that the data needs of the project are considered as a first-rate product requirement. A data resource center highlights best practices and centralizes documentation for use across the organization. This work has been a collective effort across BuzzFeed Tech and enables us to explore many new and exciting data-driven initiatives! If you’d like to join us, BuzzFeed Tech is hiring! To browse openings, check out buzzfeed.com/jobs. You can also follow us on Twitter @ buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… 138 Data Model Buzzfeed Posts Database Operations Data Management 138 claps 138 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-22"},
{"website": "Buzzfeed", "title": "how we made your ai generated lover with a gan", "author": ["Max Woolf"], "link": "https://tech.buzzfeed.com/how-we-made-your-ai-generated-lover-with-a-gan-bdcd347b0038", "abstract": "Latest Posts Events Apply To BuzzFeed And you can find your AI-Generated Lover too! BuzzFeed is all about exploring new ideas for content, no matter how outside the box. Last December, BuzzFeed CEO Jonah Peretti posted an ambitious idea into the company’s Slack (paraphrased): It could be fun to create a quiz called ‘Who’s Your Future Partner?’ where the BuzzFeed user inputs what they want in a partner, and the result is the image of a fake person with the caption ‘Congrats, hope you have a great life together!’ Always up for a challenge, a number of BuzzFeed writers, engineers, and myself (as a data scientist) decided to experiment if this more advanced quiz format was feasible. It felt like an opportunity to learn something new and create an even more engaging experience for our audience. Besides, I knew how to best generate fake images. And I couldn’t pass up the opportunity to play with GANs! A GAN , a generative adversarial network, is nowadays the most popular way to generate believable fake images. I won’t go into detail on how GANs work as there are many, many other articles that do it better, but the tl;dr is that they are a neural network architecture that simultaneously trains both a fake image generator and a discriminator to tell real input images from fakes ones. As the GAN trains, fake images become progressively more “real” over time. We can then use the generator to create fake images for fun! The process isn’t perfect though. GANs can occasionally go off the rails and include hilarious artifacts. The Twitter account Normal Cat Pics is a good novelty account illustrating generated cats from a GAN trained on cats: However, good GANs are very computationally expensive to train, and are still very much an evolving area of research. The most notable breakthrough was StyleGAN by NVIDIA . The pretrained weights for that model have been released for anyone to use. The most notable demo of StyleGAN is thispersondoesnotexist.com , which has a self-explanatory URL and simply shows a StyleGAN-generated person, no frills. But it’s effective: how would you tell that is not a real person? The hard part about using a GAN is setting up a system capable of running it. Larger GANs like StyleGAN require an NVIDIA GPU to generate images in a reasonable timeframe, CUDA to interface with the GPU, and a deep learning framework like TensorFlow or PyTorch to construct the model. It can be time-consuming and expensive to set up by hand. Fortunately, Google released Google Colab , which allows anyone to instantly set up a free virtual machine with a compatible GPU + deep learning framework with just a click, then share those notebooks with others who can tweak it if necessary. Recently, NVIDIA released StyleGAN2 ADA , which further improves StyleGAN architecture and solves some artifact issues from the generated images using adaptive discriminator augmentation. This network is trained on a dataset of high-quality faces from Flickr. This Colab Notebook by Jeff Heaton at Washington University includes the code and boilerplate necessary to run StyleGAN2 ADA, and you can run it too! The notebook also contains more detailed information about how GANs work. The latent vector is a 512-element vector of random noise that is passed to StyleGAN2, with a specified seed used to make the random noise reproducible. For example, here’s what you pass a latent vector generated with a seed of 1234: Pretty humanlike, isn’t it? The StyleGAN2 images are so high quality I actually had to shrink and compress them before saving to avoid using too much storage. (You can do that in Colab by installing and using imagemagick and pngquant respectively.) After generating and presenting proof-of-concept AI-generated images to the team, I got the go-ahead to run some more experiments with StyleGAN2. Because GANs are computationally expensive, most websites that use large GANs like This Person Does Not Exist precompute thousands of images instead of generating them in real-time. We ended up doing that as well, but for other reasons I hadn’t expected when starting this project. There were two issues immediately apparent: the StyleGAN had a tendency to output images of caucasian faces with relatively few images corresponding to racial minorities (this is, unfortunately, a consequence of the inherent bias of the input Flickr data), and StyleGAN2 had the possibility to generate images of children, which is a massive problem when you are creating a dating quiz. A team member created a mini-webapp to allow a panel of diverse human BuzzFeeders to classify a batch of generated images with their presented gender, age bucket, whether the image is problematic and should not be used, and whether the image is good or an artifacted trainwreck. Another problem is that StyleGAN2 is too good and didn’t create enough trainwrecks. Trainwreck images of half-rendered or fake-looking faces are funny, especially when the quiz is supposed to generate an ideal romantic partner. In the end, several thousand GAN-generated images were classified. The logic was that if a given user takes a quiz more than once (which we expected), the odds of them getting the same random AI-generated lover from the pre-generated set had to be very low. Another aspect of the AI-generated lover is how they would introduce themselves (a la a dating app) and how they describe themselves. For those who know me outside my work at BuzzFeed, I specialize in AI text generation and have written open-source tools to help make it easier for everyone. Unfortunately, my attempts at training an AI to do this went poorly, plus we would have similar real-time generation issues as noted above with StyleGAN. We decided to compromise, and BuzzFeed staff created templates of flirty text to capture the tone and sentiment of many, many different types of theoretical people. The templates were populated randomly from a spreadsheet of prewritten possible answers and became part of the user’s quiz results. This more controlled approach also avoided risking the AI-generated model outputting incoherent or grossly offensive, uncurated output. Although some argue that powerful AI text generation models such as GPT-3 could replace the writers on a site like BuzzFeed, we’re confident that won’t happen. Our editorial team’s unique voice and perspective is essential, and will only continue to be more effective and powerful when layered with AI to yield new innovative formats. Note: Even GPT-3 has issues with pickup lines! After the quiz went up on a Friday morning, we promoted it via our usual social media channels and it became a featured Twitter Moment. The main BuzzFeed Twitter account tweeted a link to the quiz, asking for others to reply with their own AI-generated lovers: …and indeed they did, by posting screenshots along with their reactions to their new life partner. In the end, we received over 1 million views of the quiz. User reaction was overwhelmingly positive, both in response to the generated text and generated images. The tweaks and constraints for both images and texts worked out and created a super engaging experience for our audience! In the few months since the original quiz was published, research into GANs has yielded exciting projects, including being able to generate an image from text alone , and even editing existing images to style them according to a text prompt ! This experiment was an effective illustration of BuzzFeed’s scrappy creative culture that enabled us to move quickly, think critically, and collaborate across multiple teams to innovate. We took a signature BuzzFeed format and powered it with new technology to create an enhanced experience for our audience. Given the very positive feedback to the quiz, we’re eager to expand new ideas using AI in the creative process. We’d also like to experiment with building our own models or work with specialized partners. We’re always testing and experimenting with new content formats at BuzzFeed, so keep an eye on this space for what we’ll pioneer next! This work has been a collective effort across BuzzFeed Tech and enables us to explore many new and exciting data-driven initiatives! If you’d like to join us, BuzzFeed Tech is hiring! To browse openings, check out buzzfeed.com/jobs . You can also follow us on Twitter @ buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… 68 ML BuzzFeed Buzzfeed Tech AI 68 claps 68 Written by Data Scientist at @BuzzFeed in San Francisco. Creator of AI text generation tools such as @aitextgen and gpt-2-simple. I am the data. Sharing our experiences & discoveries for the betterment of all! Written by Data Scientist at @BuzzFeed in San Francisco. Creator of AI text generation tools such as @aitextgen and gpt-2-simple. I am the data. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-14"},
{"website": "Buzzfeed", "title": "accessible buzzfeed", "author": ["Jack Reid"], "link": "https://tech.buzzfeed.com/accessible-buzzfeed-2e1f3f94f352", "abstract": "Latest Posts Events Apply To BuzzFeed Last month, external accessibility experts certified buzzfeed.com as compliant with the best accessibility practices for the web . That simple statement, ripped straight from the headlines of a boilerplate internal email, does not do justice to the two-year process that brought us to that point. Nor does it embody what the achievement means to our team, especially myself, on a personal level. In 2004, I watched my grandad build a remote control plane from scratch in his garage. I sat cross-legged at his feet on the grubby floor, ten years old. He was an engineer his whole life, a car mechanic, an ex-steel worker. His garage was filled with tools and machinery and he’d mastered all of it. In 2016, I watched that same man try to shop online, an iPad in his lap. By then, diabetes had taken his foot, much of the sensation in his fingers, and left him with only a narrow, milky porthole for his vision. He manipulated the screen with a stylus attached to a lanyard. Even with that affordance, I watched him open Safari and make multiple attempts to select the address bar. That done, he slowly tapped in “ebay”, then made a few stabs at tapping the top result in the Google results. Presented with the homepage of ebay.com, he tried to pinch-to-zoom to find the search box, inadvertently activating a link to a promotion for a lawnmower. A lot of people will have a story like this, perhaps less severe, perhaps worse. Statistically speaking, you’re pretty likely to have a similar anecdote, either about yourself or a loved one — after all, 1 in 4 Americans has a disability that may affect how they interact with screens. As the world fills more and more with those screens, it is going to shrink for those people unless the architects of those experiences take them seriously. For me, it’s incredibly difficult to separate these personal moments from my day job as one of the people who builds BuzzFeed. Building things for the web is an invisible avalanche of tiny decisions influenced by a mix of implicit and explicit motives: make it fast, make it fun, make it profitable, make it maintainable, make it measurable. We occasionally spend some time talking about one of those things in isolation: a meeting about maintainable architecture, about adding the right metrics to our software. We then hope that those things stick around in the morass of motives in our colleagues’ minds when they’re building stuff. Sometimes it does stick, and sometimes it fades out in favour of bigger, newer, shinier priorities. We take great care to measure and report on the stuff that’s important to us as a business, but not all of the variables can be measured easily. The true quality of the experience we provide to a user with accessibility needs can’t be meaningfully quantified. Instead, maintaining and monitoring accessible experiences takes sustained effort from everybody who works on the product. In the winter of 2018, we at BuzzFeed decided to take some serious action about making buzzfeed.com an accessible experience for everybody. We knew anecdotally that we weren’t doing well on the accessibility front. Engineers familiar with accessibility standards raised their concerns regularly, and fixes were made here and there; we had an #a11y Slack channel. This time, though, we would be taking a more thorough and holistic approach to accessibility, not just a series of patch jobs. We would contract with a specialist accessibility auditor, Accessible360, who would produce a full report of all of our issues, and review our fixes as they went in. The end goal: an accessible experience accredited by the experts. The initial audit report landed in our inboxes in January 2019, and our fears were confirmed: we had completely failed to provide a viable experience for users with accessibility needs. The issue tracker showed in excess of 400 problems with the site, and some of them sounded insurmountably difficult to solve within our current technological and organizational structures. There was no obvious single group to own all of the fixes, nor was there any one team big enough to get through the issues in any reasonable amount of time. It was going to have to be a holistic effort. The issues could be categorized into two general buckets. The first bucket was smaller fixes — missing button titles, content presented as a list but not marked up as such — and could easily be divided among the teams who maintained those portions of the site. The issues in the second bucket, however, were going to take more dedicated effort and product thinking to resolve. Here’s one example of an issue that needed to be tackled more strategically: “These images lack alt attributes, thus it is not clear what they contain.” BuzzFeed is full of images. We tease our articles with carefully selected and cropped thumbnails. The articles themselves are often adorned with gloriously art-directed banner images, stacks of red carpet photos, and the latest celebrity apology in a series of screenshots of the Notes app. In all of these cases, our content creators are putting a lot of work to tell stories with a visual medium that is inaccessible to users who can’t see the screen. To be accessible to somebody using a screen reader, those images need an alternative way to be experienced — in this case, descriptive alt text. When our writers have uploaded images to our content management system (CMS), we’ve never required them to provide descriptive alt text — or even provided them with the option to do so. First, we tried to avoid changing our editorial workflow. We assessed different machine learning services that offered to describe the content of images programmatically. None of them could describe everything, let alone describe it well. We quickly realised that there was only one solution: to consider alt text as integral a piece of content as any piece of visible text in an article. That meant it would fall to our writers and editors to understand the importance of alt text and to write great image descriptions. So, while our engineering team whittled away at hundreds of issues that could be fixed in the background, we worked with our editorial team to put on workshops that demonstrated how screen readers experience our content. After establishing that understanding, we discussed the best alt text for all our different types of image content: from red carpet photos to meme screenshots to shopping items. The editorial team didn’t just learn how to write alt text to pass the bare minimum standard. They brought alt text into their style guide, making sure the BuzzFeed voice shone through regardless of how someone experienced the content. The members of the team who attended multiple workshops became experts in good alt text, and now they help their colleagues write it every day. Andrew Ziegler, an editorial colleague who came to many of these workshops, remarked that the burden of writing good alt text was far less than expected: “You’re encouraged to keep it short … you’re not supposed to be redundant and repeat yourself, and you actually have a ton of freedom with it.” Furthermore, these workshops validated that our writers and editors were best placed to provide these descriptions: “Instead of robotically spewing out what’s in the image, you treat it like any other written part of the post and you’re free (and even encouraged) to carry the tone of the post into the alt text itself.” We have a dedicated Slack channel where they exchange tips and establish practices for new kinds of content that present challenges for describing things at the appropriate level of detail. It’s inspiring to see that this enormous problem could be solved by our team by providing the right information and resources, and that they dove into the subject without hesitation once they understood how much the problem impacted our users. When we embarked on the task of making BuzzFeed accessible to all, we set ourselves a tentative and extremely ambitious task. Over a hundred individuals have worked on fixing one of those 400+ issues in some way or another. The biggest shift over the course of the last year goes above and beyond just a pile of bug fixes — changing our site also changed how we think about building our site. The consequence of having so many contributors to the technical side of our accessibility initiative is that nearly all of the front-end engineers working on the website have now made targeted accessibility fixes, and can identify and prevent additional instances of them in the future. This influence extends past engineering as well: product managers, designers, and QA engineers are now all more attuned to our accessibility concerns. We’re starting to hear “it’d be a huge accessibility win” in the arguments for a new feature or change. I think it’s becoming part of our DNA, and that makes me incredibly hopeful. We’re trying to harness that new wave of motivation by supplying the best tooling and documentation. So far, we’ve used a mixture of the great assets already out there, such as accessible component recipes, as well as making some home-cooked guides on how patterns common to BuzzFeed have been made accessible in the past. Accessibility has also become a foundational part of the ongoing design language project. The arrival of our letter of accessibility compliance from our external experts marked the end of an enormous push to fix years of inaccessible feature development. Our website is finally accessible to all. Whoever wants to read an article, take a quiz, leave a comment — they can do it. The work that is ahead of us now is making sure that accessible culture is a mainstay of how we build and write content at BuzzFeed. I’ve noticed a real push lately to make digital products accessible to everybody. That’s probably partly due to an uptick in lawsuits under the Americans With Disabilities Act (ADA). However, I also think that there has been a recognition that we’ve replaced significant services and products that used to be physical, tactile things — like newspapers, television sets and telephones with dials and buttons — with digital applications. If you want to buy something particularly obscure, an electric motor for a remote control aeroplane, for example, you’re best off doing it online, particularly during a pandemic. People who want to take a quick picture usually aren’t reaching for their big manual cameras, but their little touch-screen phones. If the digital device is to be the mediating layer between us and the world, as it increasingly is, it must work for everybody. We have finally reached a place where buzzfeed.com provides an accessible experience for all. My grandad passed away, but I can now imagine him asking about my job and being able to skim through articles on our homepage with VoiceOver on his iPad, as easily as he could roll his wheelchair down the ramp into the garden to listen to the aeroplanes overhead. To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 210 2 BuzzFeed Accessibility Buzzfeed Tech Web Accessibility A11y A11y 210 claps 210 2 Written by Web Person Sharing our experiences & discoveries for the betterment of all! Written by Web Person Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-27"},
{"website": "Buzzfeed", "title": "best of hack week 2020", "author": ["Reina Hoshino"], "link": "https://tech.buzzfeed.com/best-of-hack-week-2020-42df16471d0d", "abstract": "Latest Posts Events Apply To BuzzFeed We recently wrapped up our 2020 Hack Week , appropriately themed “Hacking from Home”. Even though our tech teams are 100% remote, that didn’t stop us from working together to design, build, and (mostly) ship our hacks within a single week! We saw a wide variety of projects from site features to internal tools to hardware. In total, 16 groups completed and demoed hacks to the entire company in a 100+ person Google Hangout. Here are some of the fan favorites! If you weren’t already addicted to BuzzFeed.com, we’ve now got a gadget that dispenses delicious juice to reward users for interactions on our site. We built, programmed, and tested the juice delivery mechanism in hopes of increasing user retention and engagement. Even if this feature isn’t released to the public yet, rest assured we will be using it internally to reward our teams for all of the cool things we’ve been building. Project Team: Paul Curry & company We were inspired by recent social justice movements to create an organized database of crowdsourced videos to call attention to police misconduct. Chances are, you’ve seen videos of police misconduct in your social feeds, but what do you do about it? With Blue Check, you can upload and browse videos of police misconduct in your location, provide and see information about who your representatives are, and steps you can take to push your elected leaders to take action! Project Team: Rico Moorer, Tami Olafunmiloye, Gabe Campo, Mireille Keuroghlian It has become a custom for our tech team to gather for an online game of Pictionary every Friday. However, we had several qualms with our Pictionary platform of choice (including limits to number of players and lack of Windows 98 vibes), so we decided to build our own. Introducing BF Paint — endearingly named and themed after MS Paint. Some of our favorite features include the Painty (Clippy’s canonical second cousin) avatars, persistent leaderboard and game records, AI-powered image scoring, and text detection! Project Team: Plum Ertz, Caroline Amaba, Maria Enderton, Judith Leng, John Phillips, Estefania Reichel The days of virtual pets are no more. Introducing Digital Plant, a virtual plant that lives on your very own BuzzFeed.com user profile! Given lots of love, water, and fertilizer, it will evolve into different types of larger plants! Project Team: Sam Thurman, Devin Argenta, Jigna Lad, Lisa Maldonado, Lizzy Grillo, Milan Samuel, Malcolm Mitchell, Reina Hoshino A team of designers and engineers collaborated on a tiered and component-based design system to allow for code consistency across products but also encourage visual flexibility. Sometimes we have sponsored homepage takeovers that require design and dev work, and this token based system would make it super easy to just plug in the variables and go! BuzzFeed.com dark theme, anyone? Project Team: Suleiman Shakir, Jonathan Ginter, Angela Medina Arma- git -ton is a retro 2D shooter arcade game where you take down hostile files straight from our GitHub repository! Face off against files of all types from Python to JavaScript to Go to Perl. And the best — or maybe the worst — part? Every file you take down is deleted from our repository. Play this game too much, and we’ll all be out of jobs… Project Team: Liz Frost Find out what else BuzzFeed Tech is up to by following our Twitter, @buzzfeedexp ! Also, we’re hiring! If you are interested in browsing openings, check out buzzfeed.com/jobs . Sharing our experiences & discoveries for the betterment of… 34 BuzzFeed Buzzfeed Tech Hack Week 34 claps 34 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-17"},
{"website": "Buzzfeed", "title": "hacking from home", "author": ["Plum Ertz"], "link": "https://tech.buzzfeed.com/hacking-from-home-c6bae35b124b", "abstract": "Latest Posts Events Apply To BuzzFeed If you look at the to-do lists and notes apps of BuzzFeeders in tech this week, you might be a little confused. A few folks are growing virtual plants, someone is making pancakes, and Paul appears to be checks notes “improving site retention with a juice-based rewards system delivered via mechanical pump.” Has quarantine cabin fever finally taken over? Nah, it’s just Hack Week. Hack Weeks and hackathons are relatively common concepts among tech organizations at various companies, and come in a lot of different flavors. Some places will limit total working time to certain hours of the day, or compress all the work into a shorter but nonstop (read: no sleep) sprint of two days, or set limits on technology or theme. BuzzFeed’s directives are relatively freeform: Build stuff. Be weird. Present (if you want) on Friday. Oddly enough, despite this week being a proverbial single-arm-table-sweep clean of all regular teams, meetings, roadmaps, and projects, quite a few of our previous “hacks” have ended up in production. Multiple of our internal tools and a few parts of buzzfeed.com that our users see every day grew from engineers’ wandering minds during Hack Week. Despite our best efforts to be as minimally useful and productive as possible, we still somehow manage to produce good work. We can’t talk about Hack Week this year without touching on the question du jour of all tech blog posts in 2020: how has it been impacted by the current ongoing (despite what Mickey Mouse wants you to believe) global pandemic? The honest answer is, it really hasn’t changed too much. Our teams are already distributed across multiple offices and homes both in and outside of the US, including both full-time and contract employees, so the majority of pre-planning and coordination already happened asynchronously. We’re using many of the same planning sheets and channels that we have in previous years. (Pro tip: Save planning documents in a drive folder. Go back to said drive folder a year later when you panic and realize you need to figure out what to do for Hack Week again. Thank your past self for sharing.) The one area we’re missing out on this year is in the free food department. Previous Hack Weeks have included home baking from our Tasty tech team, happy hours, and bountiful bags of McDonald’s breakfast sandwiches. No pandemic can stop BuzzFeed tech from doing what it does best, though: making stickers. (They’re h o l o g r a p h i c.) Stay tuned this week and follow BuzzFeed Tech on Twitter as we move fast and break things! Sharing our experiences & discoveries for the betterment of… 11 Hack Week BuzzFeed Software Engineering 11 claps 11 Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-27"},
{"website": "Buzzfeed", "title": "continuous deployments at buzzfeed", "author": ["Logan McDonald"], "link": "https://tech.buzzfeed.com/continuous-deployments-at-buzzfeed-d171f76c1ac4", "abstract": "Latest Posts Events Apply To BuzzFeed At BuzzFeed, our Core Infrastructure team is always looking for new ways to improve developer productivity. Recently, we implemented continuous deployment of every merged change to our production environment. This has saved developers time, reduced operator stress, and improved the resilience of our services. In this post, we want to talk about why we invested time in implementing continuous deployments, what they look like for our ecosystem, and how we did it. Our homegrown platform, rig , powers a microservice ecosystem of over 600 apps including HTTP APIs & UIs, queue readers, one-off jobs, and more. We use a monorepo structure for development , and rather than using more formal, semantically versioned releases, our deploys are tied to a specific git revision (or commit). Every revision is deployable if it passes our automated tests. Before the advent of automated production deploys as described in this post, each deploy required an intentional step by a developer: Choosing the app, revision, and target environment through a deploy UI (and deploys to pre-production environments still work this way). Last year, a few of us interested in learning more about the discipline of “Release Engineering” decided to form a working group to investigate what could be improved about releasing software in our org. This small group wanted to learn more about continuous delivery as a practice and how we as an organization could reduce cost, time, and risk in our release process. We adopted a motto from Rolf Andrew Russell : “Continuous Delivery means minimizing lead time from idea to production and then feeding back to the idea again.” We took a page out of Google’s SRE handbook Chapter 8 on release engineering and conducted user interviews with a variety of BuzzFeed developers using Google’s categories: self-service release process, high velocity for changes, hermetic builds, and clear policies and procedures for releasing. Through these interviews we found a consistent problem that impacted every category: Even though we preached a “master is golden” approach, we sometimes failed to embody it. In BuzzFeed’s monorepo, every change merged into master must be ready to be deployed to production at any time. It’s the responsibility of every developer to ensure that their changes are validated before merge, because the nature of our development process means they do not necessarily have control over when it gets deployed after that point. An important aspect of this approach is ensuring that every change merged into master is deployed ASAP , to minimize the number of changes to debug when something goes wrong. Though we preached this “master is golden” approach, we suspected that developers were sometimes failing to deploy every app after a change was merged, because it is not uncommon for a single pull request in our monorepo to change many apps at once, leading to a lot of manual clicking to trigger individual deploys. (Some enterprising developers attempted to mitigate this manual clicking by using our annual Hack Week to create a Chrome extension, affectionately called Deploy Rocket, that tricked our deploy tooling into doing multi-service deploys. While an effective hack, when used for large numbers of deploys it wound up causing issues with rate limits and disrupting other developers.) Step one, as always, was measuring to confirm our suspicion that our services were lagging behind master, and the results were a little scary. There were services that hadn’t been deployed in years. Even worse were those builds on production that were never merged into master. We found cases where feature branches had been running in production for over 500 days. It had become far too easy for changes to be merged and never deployed. We had the idea that we could address these issues while also improving the resilience of our microservice ecosystem. While every deploy is an opportunity to introduce problems, a more aggressive, automated approach would ensure that we were always deploying the smallest possible change. So this became our mission: Find a way to safely and reliably ship more consistently and more frequently, to ensure production is always running the latest code. And that’s how continuous deployments at BuzzFeed were born. Last year, our team developed a new HTTP API to power the rig platform, which took our platform to the next level, allowing us to simplify our admin services, improve operational security, and build features faster. This API empowered us to programmatically deploy services in the rig ecosystem, and thus, to continuously deploy those services. To keep it simple, we first set out with some requirements and non-requirements for what continuous deployments at BuzzFeed should look like. Our broad mandate was this: when a pull request is merged, all affected services will be deployed to production. Requirements involved in that were: Continuous deployments would only handle our production environment Each merge would trigger a deploy of all applications modified We would present developers with an opt-in period to gain confidence in the new system and then switch to an opt-out approach once it was ready for widespread adoption Quality of service controls, like rate limiting, backoff, and retries, would have to be built-in To help developers adjust to this (potentially scary) new approach, they would receive clear indicators when a change would be deployed automatically on merge Non-requirements were: Control over the order of deploys Automatic rollbacks of broken deploys Delivery of any non-master branch to a pre-production environment Given those requirements, we ended up with an architecture that looks like this: The flow works like this: A user merges a pull request on Github, which is sent to Builder, our CI system. For each app affected by the merge, Builder produces two deploy artifacts: A docker image, pushed to ECR , and metadata needed to deploy that revision, pushed to S3 . When a new docker image is pushed, ECR sends a PutImage event to an SQS queue . A new deploy_worker app receives those events and, after some validation, makes an HTTP request to rig_controller_api to trigger a deployment. Finally, deploy_worker updates the originating pull request with the deploy result: Meanwhile, the API notifies developers in Slack: And, tada! An “automagical” deployment is created. (And, yes, we even continuously deploy the apps that power continuous deployments). At this point, we have moved to an opt-out phase for continuous deployments, with only a few services currently choosing not to participate. In the past week 85% of our deploys in production have been automated, with the remaining being mostly validation deploys. That’s 307 deploys that humans did not have to do. Just for fun here were our deploys last Friday: Feedback from our developers has shown that continuous deployments have not only made their lives easier, but have made them feel safer about deploying. They always feel confident about what state the service is currently in for production and because the services get deployed more often, they feel less likely to break something with their change. Now they can get back to the important work… optimizing the ways we tell you how well you see blue . Thank you to Dan Meruelo and Will McCutchen who worked on this project (extra thanks to Will for the art and design for the UI!) Thanks in addition to all of Core Infrastructure, and especially Justin Hines, Jack Ussher-Smith, Dan Katz, Clem Huyghebaert, and Andrew Mulholland for their work on the rig controller API. Sharing our experiences & discoveries for the betterment of… 668 DevOps Continuous Delivery Continuous Integration AWS Buzzfeed Posts 668 claps 668 Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-24"},
{"website": "Buzzfeed", "title": "sso is one year old", "author": ["Jack Ussher-Smith"], "link": "https://tech.buzzfeed.com/sso-is-one-year-old-de40e6326423", "abstract": "Latest Posts Events Apply To BuzzFeed SSO was initially created to solve a problem here at BuzzFeed — we needed a way to reliably secure our 600+ microservices (check out this blog post for a look back in time and a more detailed overview of SSO). Not only has it continued to be a great solution for microservice authentication for us, but it’s also been utilized by a range of companies and individuals outside of BuzzFeed; had ongoing support from the open source community, and had a whole host of improvements and bug fixes added. This Fall, we celebrated SSO’s one year anniversary. We wanted to take this opportunity to reflect back on what we’ve done with SSO since we open sourced it, as well as thank everyone who has contributed to the project — especially our open-source contributors! Perhaps one of the largest changes is that SSO now supports Okta as an identity provider in addition to Google . Okta is used extensively here at BuzzFeed, and adding support for it in SSO is something that had been frequently requested by the open source community, so we saw a great opportunity to have a wide-reaching impact. Internally for us, this was a big deal. Google had experienced several outages that affected our ability to authenticate through SSO. The introduction of another provider gave us something very important: it gave us options (that didn’t involve a potentially questionable change like drastically increasing the SSO session lifetime). We are no longer tied to and reliant on any single provider, and in the event of an outage with either provider, we have the option of switching to authenticate via whichever one isn’t in the middle of a meltdown. It also meant that we could push more internal systems behind Okta — something that our IT team had been working towards for a while. It was, however, a big undertaking to migrate all of our 140+ SSO-protected upstreams (multiplied by multiple environments!) that originally used the Google provider to use the new Okta provider. We needed to work out a way to go through this process that didn’t require a hard change of provider for all upstreams at once — ultimately minimizing the risks that are typically associated with such large scale migrations. We decided SSO should be able to: Use multiple identity providers at once Allow each individual upstream to easily set which provider to use (with as little as a one-line configuration change) This meant we had much better control over the rollout: we could move upstreams to a new provider in batches while having full confidence that no other upstreams would be affected. Implementing this functionality had its own set of hurdles to tackle. The idea of using multiple providers at once was entirely new to SSO, so we needed to modify and split up large parts of SSO’s code: Authenticator logic needed to be changed to be on a one-to-one basis with upstreams, rather than one-to-many. Routing needed to be restructured to support multiple provider routes. Google has a unique way of providing the group data that we use for group-based authorization. Okta handles this very differently, so we needed to implement a new mechanism for caching these groups. A new configuration variable mechanism needed to be introduced to support configuration for multiple, and different identity providers. The UX while authenticating needed to be molded to allow this new flow with different providers and to understand a new set of possible error scenarios and states a request may come through with. It often pays to put in the extra work and time to make migrations like this more manageable. By doing so we achieved an extremely ‘quiet’ rollout with no downtime to the services being migrated, minimal disruption to engineers and no one being woken up in the night by alerts. We’re also looking to add support for more third-party identity providers in the near future! The use of configuration variables and how they were being managed across the codebase was rigid and limiting. It involved a large ‘options’ object being initialized at runtime and passed around as needed. The limits of this method were evident when we needed to support the configuration of multiple and different identity providers at once. With a ground-up rewrite of the configuration mechanism, we introduced a whole new backend for managing configuration variables within the SSO Authenticator component (SSO Proxy still to come!) using the open sourced library go-config . This gave us the flexibility to maintain multiple configurations at once, and also opened up the doors for providing new ways of defining these variables; such as by CLI flags, from files or even something remote like Consul . Tests are always a vital part of developing and maintaining a healthy codebase, but it can quickly become difficult to properly understand the usefulness of a set of tests and what is actually being tested. A good starting point to improve in this area is by having a mechanism to monitor the test coverage across the repo. We integrated CodeCov into the repo and configured it to provide useful test coverage stats and details on pull requests. It helps us and other contributors gauge the impact of new tests and gives us detailed information around what is lacking tests. Using the data CodeCov gives us, we can now confidently say that test coverage has increased overall! Originally SSO hadn’t really been tested on Kubernetes — there existed no I also don’t guides, configurations or quick-start steps for integrating the two together. Once again, thanks to open-source contributors (specifically @while1eq1 ) SSO can officially run on Kubernetes! Check out this blog post @while1eq1 wrote for more information! SSO started out by using gpm — an outdated package manager for Golang. The leap (a +243,015 / −165 line change leap, to be precise) was taken to migrate over to use go mod and take that much-needed step into the future, and in this case…it is greener on the other side! Thanks, @slyr ! We’ve been fortunate enough to give a whole multitude of talks and presentations on SSO over the last year in various formats and locations. AWS re:Invent 2018 — Security Microservices at Scale QCon London 2019 — Security Services using SSO The Secure Developer — Using SSO Octopus Apple — Token Security Podcast How We built it (meetup) — The process for open sourcing SSO Velocity San Jose 2019 — Securing services using SSO The women in tech show — Securing Services Increment magazine — Security Edition — Open sourcing of SSO Medium — Unleashing the SSO Seeing the interest that SSO has gotten at these events, feedback from its adopters, and community engagement on the repository has really helped us keep the momentum going. “SSO drastically simplified an important part of our stack while also making it more robust. Our team depends on it daily and delivers in spades.” — Daniel McGrath, Director of Engineering @ Button The SSO Docker image has been downloaded over 500k times 1.5k 👏 on its original release blog post. ⭐ ️by ~2.2k people on GitHub Used in production by numerous other organizations such as Button , CommonBond and The Markup 12 community contributors have committed a total of 29 commits The repository receives thousands of views each month, with hundreds of unique visitors It goes without saying but this progress wouldn’t have been possible without the help and support of everyone at BuzzFeed, and all of our open-source contributors. Huge thanks to all that have been involved! BuzzFeed Tech is hiring. 👋 If you are interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! You can also find out about some of the new things we’re launching at BuzzFeed Tech by following us on Twitter, @buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… 104 Open Source BuzzFeed Buzzfeed Tech Oauth2 Microservices 104 claps 104 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-04"},
{"website": "Buzzfeed", "title": "micro frontends at buzzfeed", "author": ["Ian Feather"], "link": "https://tech.buzzfeed.com/micro-frontends-at-buzzfeed-b8754b31d178", "abstract": "Latest Posts Events Apply To BuzzFeed The definition of what constitutes a “micro Frontend” perhaps hasn’t yet reached consensus. The smart folks at DAZN consider it to be a series of full pages managed by a client-side orchestrator. Other approaches, such as OpenComponents , compose single pages out of multiple micro frontends. BuzzFeed’s use case fits somewhere in between the two. I wouldn’t say we have a micro frontend architecture; however, we do leverage them for a few parts of the page. We consider something to be a micro frontend if the API returns fully rendered html (and assets) but not an <html> or <body> element. We have three micro frontends: the header component, the post content, and our interactive embeds. Each of these adopted the micro frontend approach because they presented real and distinct business problems. Why? Component Distribution This is the buzzfeed.com header. It has a light layer of configuration as well as a reasonable amount of code behind it: certainly enough that it merits an abstraction as opposed to duplicating it in all of our services. Originally, we made this abstraction and extracted it into an npm package, which services imported as part of their build process. This allowed us to remove duplication as well as have the service bundle the header as part of its own build process (meaning we could easily deduplicate common code and libraries). With only two or three services, this technique works really well, but we have more than ten rendering services backing buzzfeed.com. This meant that every time we wanted to make a change to the header we had to make the following changes more than 10 times: Update the code in the header Make a Pull Request Merge and publish to npm Update the service package.json Make a Pull Request Merge and Deploy the service This became extremely time consuming and led to teams avoiding header changes because of it. Sure, there are ways in which we could have improved this workflow (e.g. using loose semver and just rebuilding the service, automating the update and creation of service PRs) but these still felt like the wrong approach. By moving to a micro frontend pattern, we’re now able to distribute the header instantly to all services and the workflow to update it on all of buzzfeed.com is now: Update the code in the header Make a Pull Request Deploy the header Why? To maintain a contract with the CMS We have a few different “destinations” (e.g., BuzzFeed and BuzzFeed News) for our content yet each one is powered by a single CMS. Each destination is its own service (or multiple services) which connects to our content APIs. This means that we have the ability to render the same content in multiple destinations; however, in practice we choose not to. The same content rendered in three different BuzzFeed destinations. This also means that we have to maintain a contract between the CMS / Content APIs and the rendering services. To illustrate this it’s easier to focus on an example. When an editor wants to add an image to the page, they select the image “subbuzz” in the CMS and upload it. They then have the option to add extensions to that image. One such extension is the ability to mark the image as showing Graphic Content. The intention of adding this extension is that the image would be blurred out and the user would have to opt-in to see it (this is particularly important with sensitive news content). As far as the CMS cares though, all this means is a boolean value stored against an image. Because the CMS depends on the rendering services to add a blurred overlay we end up with an implicit coupling between the two. If a destination failed to support this feature then users would be exposed to graphic content, and we would have failed to uphold the editors’ intentions. So what does this have to do with Micro Frontends? We could choose to abstract these subbuzz templates into an npm package and share them across the destinations; however, when we change support for something in the CMS we need the rendering services to be able to reflect this immediately. The CMS is deployed in an un-versioned state, and the content APIs only expose major version numbers. Coupling these with npm packages using semver and deployed via a package would make it harder for them to stay in sync. By moving the subbuzzes behind an HTTP API, we can update the rendering-cms contract across all destinations immediately and guarantee that each destination supports the latest CMS features. Why? Independence from the platform Maybe the most clear use case for Micro Frontends: the Embed. We host a ton of embeds (Instagram, Twitter, etc.), including first-party embeds. We call these BFPs which stands for Buzz Format Platform, and they can be anything from a newsletter signup to a heavily reusable quiz format or a bespoke format supporting an investigative story. The entry point for an embed is typically an iframe or a script element, so it doesn’t really qualify as Micro Frontends themselves. We break that mold (where possible) by rendering them server-side and including the returned DOM directly in the page. We do this so that we can render the embeds in distributed formats (like our BuzzFeed Mobile App or Facebook Instant Articles) as well as expose the content to search engine crawlers. BFP provides independence from the platform and gives engineers the feeling of working on a small component without having to consider the wider BuzzFeed ecosystem. This feeling is one we always try to get to when creating developer environments and Micro Frontends certainly provide that opportunity. A micro frontend architecture can give you a great developer experience and a lot of flexibility, but they don’t come for free. You trade them off against: Larger client-side assets or tougher orchestration We compose our micro frontends in the browser which means there’s no singular build process that can optimize and deduplicate shared dependencies. To achieve this at the browser level, you need to code-split all dependencies and make sure you use the same versions — or build in an orchestration layer. Higher risk when releasing updates Just as we’re able to distribute new changes instantly across many services, we’re also able to distribute bugs and errors. These errors also surface at runtime rather than at build time or in CI. We use this heightened risk as an opportunity to focus more on testing and ensuring that the component contract is maintained. There has also been criticism that micro frontends make it tougher to have a cohesive UX, but this is not something we’ve experienced. All of these micro frontends inherit design patterns and smaller components via shared packages. Overall the micro frontend pattern has worked well for BuzzFeed Tech in these use cases and has been well tested over the last one to two years. There is definitely an inflection point where having many more of them would require more work to offset the first trade-off but, we don’t think we’re there yet and don’t anticipate being there any time soon — abstracting components to shared packages work well for the majority of our cases. Where it doesn’t, it’s nice to have another architectural pattern to reach for. BuzzFeed Tech is hiring. If you are interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! You can also find out about some of the new things we’re launching at BuzzFeed Tech by following our twitter, @buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… 611 Micro Frontends Frontend Frontend Architecture BuzzFeed Buzzfeed Tech 611 claps 611 Written by Engineering at BuzzFeed, mostly writing at ianfeather.co.uk Sharing our experiences & discoveries for the betterment of all! Written by Engineering at BuzzFeed, mostly writing at ianfeather.co.uk Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-17"},
{"website": "Buzzfeed", "title": "my first product launch buzzfeed product manager edition", "author": ["Carey J"], "link": "https://tech.buzzfeed.com/my-first-product-launch-buzzfeed-product-manager-edition-297700957c4c", "abstract": "Latest Posts Events Apply To BuzzFeed It was mid-spring when I got the call that every millennial dreams of — I got extended an offer to intern at BuzzFeed! A few weeks after receiving the call, I found out that I’d be working as a product management intern at BuzzFeed Tech on the Tasty team. I was so excited about the opportunity and a little nervous, too — after all, BuzzFeed was going to be my second role in product ever and I wasn’t quite sure what it looked like to launch products at BuzzFeed and do it well . To my excitement, halfway through my internship, I successfully shipped my first feature (you’ll read more about that below). What is product management, anyway? The truth is that it’s completely different everywhere you go! BuzzFeed has a deeply people-driven culture. To that end, I saw product managers encourage and inspire their teams just as much as I saw them lay down product goals and spearhead company objectives. In textbooks, product management is typically defined as “ the practice of strategically driving the development, market launch, and continual support and improvement of a company’s products .” but as someone pretty new to the role, I’d probably define it a little something like this : But the question remains: what’s it like to launch your first product at a company like BuzzFeed? To get a good answer, I asked five BuzzFeed Product Managers about the wins, challenges, and experiences that come with a first product launch! Keep reading to hear what they had to say. Responses have been lightly edited for clarity. Then: Product Manager for BuzzFeed App (October 2017) Now: Senior Product Manager for Tasty Tech Team What was BuzzFeed like when you first started as a product manager? BuzzFeed was definitely an established company by the time I got here, and one I had been excited about for many years before! I came at a time when we were trying to figure out how to diversify the company’s revenue — so there was definitely this energy of new beginnings and excitement. What was the first product you launched? What was your role in the project? The most significant was the Tasty Action on Google, which was a voice experience where users could use their Google Assistant to help them find and cook Tasty recipes. I was the product manager, but I did a ton of work in partner management and anything else that needed to be done. I wrote a blog post about it for Google if you want to read more! How has your approach to product changed since your early PM days? I think early on, I felt like I needed permission to make many of my product decisions. What I’ve come to realize is that my instincts are solid and that I know when to ask for guidance when I need it. Those two things combined make me very capable of making great product decisions. I think it’s also important to always remember things can be iterated on in production — nothing has to be perfect at launch! Then: Associate Product Manager for Post Experience Team (July 2016) Now: Staff Product Manager for Community Team Was BuzzFeed pretty different back in 2016? Yeah, back in 2016, BuzzFeed was transitioning from being a startup to a mid-stage company. The business model was a lot simpler then: the sole revenue stream was native advertising (a.k.a., creating and distributing branded content for advertisers). BuzzFeed’s culture of curious, creative, down-to-earth, and incredibly friendly people hasn’t changed one bit though! What was the first product you launched? What was your role in the project? My first large project was re-platforming the post page. The post page had different versions for desktop and mobile, was served by legacy code, and had accumulated years of tech debt. The goal was to help build a single, responsive page that could serve all of our different post formats (articles, lists, quizzes, etc). What obstacles did you face while launching your first BF product? What did you learn? Keeping the re-platforming project within a reasonable scope was challenging. On top of deprecating legacy code, we also wanted to make design improvements to our post pages. But due to our rollout plan, we couldn’t test each improvement individually (which was risky because even the smallest of changes could have knocked our user metrics — and been disastrous for BF). We ended up only making the improvements that we had high conviction around and saved everything else for future iterations. Overall, I learned a ton about how to prioritize and sequence complex projects and I developed a ton of relationships with people across the organization! Then: Technical Project Manager for Growth (September 2015) Now: Staff Product Manager for BuzzFeed Shopping What was BuzzFeed’s big goal back in 2015? Back when I joined BuzzFeed as a technical project manager in 2015, the big focus was to launch BuzzFeed in new international markets. My first project was to launch our site and app in Japan . My team was also responsible for working on new workflow tools to support our international growth. For example, we launched a new translation tool (called Bento) that allows editors to easily request posts for translation, localize translated posts, and then publish them on our site. It was a fun time filled with lots of growth and learnings about international expansion! What was the first product you launched? What was your role in the project? After a little bit of time working on project management at BuzzFeed, I started taking on more product responsibilities. My first official product launch (as a product manager) was for a new consumer product called BFMPDB (which stands for Buzzfeed Motion Pictures Database, basically BuzzFeed’s version of IMDB for our original videos). I was responsible for creating two new pages: the contributor pages and the video pages . We use these pages on buzzfeed.com to give credit to all the talented people who contribute to making our videos. What advice would you give to someone jumping into their first PM role? As a Product Manager, you play an important role in building a competitive advantage for your company by adding real value for your product’s users. Early in your career, it can be easy to get overwhelmed by the various inbound stakeholder feature requests thrown your way. Try to stay focused on the problem you’re trying to solve for your users. Staying rooted in real user problems will give you clarity on what to prioritize and will ensure you’re building products that serve your users in the most meaningful ways! Then: Lead Designer (August 2006) Now: Principal Product Manager on the Site Team Imagine we took a time machine back to the early beginnings of BuzzFeed — how would you describe it? The Internet of 2006 would barely be recognizable today. This was before the iPhone, before Facebook’s newsfeed, Twitter was still where you shared a funny status about what you ate for lunch, and YouTube was mostly just a place you went to watch clips ripped from TV shows. When I started at BuzzFeed we were a tiny team: myself, Mark Wilkie as the engineering lead, Peggy Wang as editorial lead, and BuzzFeed founder Jonah Peretti. There were also a few advisors like Jason Kottke , Duncan Watts, and early investors. Jonah was BuzzFeed’s first “product manager.” It was a very experimental time — we were more of a tech R&D lab and not yet anything resembling a media company. As a team, we were very collaborative and would just talk through ideas together until they made sense — our process was very fluid and organic. Jonah has a million ideas, and the early days were mostly about experimenting and trying things out. But it always felt like Jonah had a vision we were working towards, rather than just experimenting randomly. What was the first product you launched? What was your role in the project? The first project I helped launch was BuzzFeed.com. I created BuzzFeed’s branding, the visual design of the site, as well as all the HTML and CSS markup. We also built a custom content management system for the site. Part of BuzzFeed’s success is that we started as a tech company so our key tech has always been built in-house for our needs. This has helped us evolve our publishing in new innovative ways and also stay ahead of trends like social, mobile, and video, rather than do what everyone else was doing at the time. What’s a product management motto you live by? “Be the glue.” As product managers, we have to be leaders, but we also need to be the glue that fills in the holes on our team. I like to work very collaboratively, and I try to adapt myself to be whatever the team needs to do the best work. This also means that everyone on the team should be a part of idea generation and decision making. Then: Technology and Political Columnist (February 2015) Now: Senior Product Manager, Content Systems February 2015 was over four years ago! What was BuzzFeed like at that time? When I joined, BuzzFeed had just finished a major hiring spree and was like a small company walking around in a big company’s body. Tech had just grown by something like 1000% and was trying to adjust its process and tech stack to match. Having a design process and product KPIs were new concepts at the time. Code deploys happened once a day, and they were an enormous production that involved the bundling-together of many disparate branches from around the organization. As often and not, the deploy would then get frantically rolled back after something in the bundle caught fire and took the whole system down. In 2015, distributed platforms were on the horizon: Facebook Instant Articles was rolling in, and Apple News was a top-secret project. BuzzFeed was swinging towards a distributed strategy where our platforms came first and our owned and operated (our own mobile platforms) came second. It paid off in many senses, but I’m glad that our O&O platform is front and center again because it allows us to do so many fun things we can’t do on the platforms. What was the first product you launched? What was your role in the project? In my very first days, I got very bound up trying to create a working auto-complete drop-down (with keyboard shortcuts!) for bylines in the CMS, which was finicky but stood the test of time. My first giant product was an overhaul of the CMS interface: make it easy for editors to edit BuzzFeed lists by clicking in and out of fields instead of opening up and saving data in modals. What led you to product management? What made you stay? My career has alternated between political journalism and content management systems, which, in hindsight, is weird. After a stint as the editor of a couple of student newspapers, I wrote my own CMS and sold it to other college papers (this was before WordPress was such a thing). I switched to journalism for a decade, but when the opportunity to join BuzzFeed Tech came up, I jumped. I’ve stayed because, honestly, it’s a great place with great people, and a mission I believe in: to spread news and cats, for the betterment of all. Last Spring: Strategy, Tech, & Design Student @ University of Oklahoma MBA Now: Product Management Intern for Tasty Tech Team (June 2019) What was it like to intern at BuzzFeed? It was so amazing! For context, BuzzFeed is broken up into agile teams and squads(like Tasty, BuzzFeed.com, Internal Tools) that are working on some exciting projects. Walking through the BuzzFeed office you can often find interns coding, designing, and launching all sorts of projects with their teams — which I think totally speaks to BuzzFeed’s culture of having all hands in . During my time, I also had a blast attending Shrek-themed happy hours, seeing Lil Nas X perform live, and soaking up five seconds of fame by starring in a BuzzFeed News video ! Most of all, I got to build great products with an incredible team that I learned so much from along the way (shoutout to Tasty Tech!). What was the first product you launched? What was your role in the project? My first official™ product launch was Tasty’s recipe linking feature, which allows food editors to easily link newly posted recipes (like pumpkin cupcakes) with previously posted recipes (like vanilla frosting). This new linking feature means that our food editors no longer have to copy and paste the same vanilla frosting recipe 500x and it also gives our users easier access all of the homemade frostings, sauces, and crumbles that are necessary to complete their recipes — bringing them much closer to the Tasty brand. As the product management intern, I kicked off the recipe linking project by setting goals and KPIs, leading a competitive audit, walking through user-flows with our designer, and nurturing our conversation with the Food Team — a major stakeholder in the project. You can catch the feature in action here ! How can I prepare for a role in product management if I’m new to product management? There are honestly so many ways to jump into product! Even though I pursued a product internship, my personal roadmap into the field actually began with entrepreneurship. In college, I spent a lot of time launching tech projects with friends (like Roomswap and BLOOM ), which gave me foundational knowledge in strategy, UI/UX design, and team-building. These experiences also taught me how to launch projects fearlessly, which I think is a super important muscle for newer product managers to strengthen anytime we can. In terms of applying for positions, I recommend publishing online case studies of your work — it not only is a huge application boost, but it’s a great conversation starter that has personally helped me connect with all sorts of amazing new friends and teammates! Have a story on your first product launch? We’d love to hear it! Send us a comment below. BuzzFeed Tech is hiring! If you’re interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp ! Sharing our experiences & discoveries for the betterment of… 179 Product Management BuzzFeed Media Tech Product 179 claps 179 Written by Product @BLOOM. Previously @BuzzFeedTasty. Passionate about community-centered design, cultural documentation, and black + indigenous futures. Mvskokē ties. Sharing our experiences & discoveries for the betterment of all! Written by Product @BLOOM. Previously @BuzzFeedTasty. Passionate about community-centered design, cultural documentation, and black + indigenous futures. Mvskokē ties. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-23"},
{"website": "Buzzfeed", "title": "buzzfeed techs 2018 diversity update", "author": ["Swati Vauthrin"], "link": "https://tech.buzzfeed.com/buzzfeed-techs-2018-diversity-update-f269ef4c8a4f", "abstract": "Latest Posts Events Apply To BuzzFeed Our mission at BuzzFeed Tech is (still) to create a diverse and inclusive culture at every level and across every discipline in Tech . We need to continue to invest in programs and initiatives in order to fulfill this long-term mission. These efforts are focused both on hiring a talented, diverse team and retaining and growing that talent here at BuzzFeed. Below are 2 charts (of many) that we are proud of that reflects our progress: You can take a look at the rest of our charts and details on our continued efforts here: www.buzzfeed.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 102 Diversity In Tech Diversity And Inclusion Diversity Buzzfeed Posts 102 claps 102 Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-16"},
{"website": "Buzzfeed", "title": "buzzfeed x tech ladies presents beyond together summit", "author": ["Swati Vauthrin"], "link": "https://tech.buzzfeed.com/buzzfeed-x-tech-ladies-presents-beyond-together-summit-c4f6dfc92008", "abstract": "Latest Posts Events Apply To BuzzFeed Our Mission: This summit aims to bring women and nonbinary people together to celebrate what we are doing today and reflect upon how we got here. We will look ahead to where we want to take our workplace, culture and technology and discuss how to make those ideas a reality. When : Friday May 18, 2018 All Day Where : BuzzFeed NYC Our lineup for the day : Ally Schneider — Let’s talk about AI — Gender & Ethics Everyone is talking about AI and what it is going to mean to the world that we live in. But what roles to do gender and ethics play in how we build, design, and engage with AI assistants and bots? During this presentation, Ally Schneider will speak to her experiences working in the AI field and the challenges that both AI companies and consumers are up against when it comes to building and engaging with these new technologies. Ally has been working in the AI space for the past 5 years. She currently manages enterprise sales at x.ai which makes an AI personal assistant who schedules meetings for you. She joined the x.ai team after various roles at IBM Watson. She worked in enterprise sales representing the cross industry portfolio of Watson solutions and later in business development working with start ups who were building off of the Watson APIs. She is incredibly passionate about women in tech initiatives and the NYC tech ecosystem. Ally has a bachelor’s degree from the University of Notre Dame. Andrea Mares — Gender Diversity in the Workplace In a world where our language and conceptions around gender are expanding, workplaces are becoming aware of the need to lift industry standards. From using appropriate gender pronouns to ensuring inclusive facilities to creating an HR policy for transitioning employees, this panel will connect engaged business leaders with inclusion trailblazers and tangible resources. Andrea is BuzzFeed’s Senior People Team Coordinator. As the first employee to transition on the job, she has helped co-create standards and hold space for other trans and gender non-conforming employees. Thanks to efforts from Andrea, the People Team, and leadership, all BuzzFeed employees have an e-training available to them on the topic of “Gender Diversity and Inclusion.” Anil Dash — Fireside chat with Shani Hilton, VP of News & Programming, BuzzFeed News This fireside chat will be a conversation on how to lead with values and build inclusive workplaces (for real) Anil is an entrepreneur and activist known as one of the most prominent advocates for a more inclusive and ethical technology industry. He is CEO of Fog Creek Software, the creators of Glitch, the friendly community where anyone can create, discover and share the cutting edge of the internet.The New Yorker described Dash as a “blogging pioneer” for his Webby-recognized personal website which began in 1999, and for his seminal work in helping create some of the first blogging and social media publishing tools. As a public speaker, Dash has taken the stage at events ranging from the Obama Foundation Summit to the Aspen Ideas Festival to SXSW. He has guested on media and podcasts ranging from Vice’s Desus and Mero to Krista Tippet’s On Being, and collaborated with “Hamilton” creator Lin-Manuel Miranda to create one of the most popular Spotify playlists of 2018. Time named @anildash one of the best accounts on Twitter, and it is the only account ever retweeted by Bill Gates, the Obama White House and Prince, a succinct encapsulation of Dash’s interests. Brianna Wu — Fireside chat with Davey Alba, Sr. Technology Reporter, BuzzFeed News “The Things Left Unsaid.” In this Fireside chat, Brianna Wu will discuss the hidden biases that sabotage women’s careers in tech. Being talked over in meetings, double standards, the need to constantly prove yourself, the lack of support for parents, and more. Brianna is a candidate for US Congress in MA-08 and a software engineer. She is also a nationally recognized leader on women’s rights in the tech industry — and is best known as one of the primary targets of Gamergate. Bo Ren — How to tackle redesigns with shared humanity. Redesigns are hard to do. Tackling big redesign projects are challenging given their scale and cross-functional impact. How do you approach such a monolithic task? Rooted in user empathy, Bo draws upon early days of working in customer service to solve product problems. Using a combination of behavioral science, design-thinking, and curiosity, Bo will explain how she approached the Facebook Notes redesign and Tumblr mobile redesign. She will cover learnings from launching and growing Facebook Notes and how the Notes playbook informed the Tumblr mobile redesign of new post forms. Bo is an investor, product manager, and writer in NYC. She believes in humanizing tech and building a more diverse and inclusive future. She previously worked for Facebook, Instagram, and Tumblr as a product manager. She launched Facebook Notes which democratized long form publishing and led the Tumblr mobile redesign. She is currently building a venture fund for underserved founders in NYC. You can find her on Twitter at @bosefina and Medium. Camille Fournier — Keynote speaker — Looking for the Upside It is easy as engineers to get caught up in the negatives. In this talk we will explore ways to identify the upsides in systems, and the strengths in people, to expand our scope of thinking and ultimately create better teams and software from the components we have at hand. Camille is the head of Platform Engineering at Two Sigma, a financial company in New York City. Prior to joining Two Sigma she was the Chief Technology Officer of Rent the Runway , a transformative brand that offers unprecedented access to designer fashion, disrupting the way millions of women get dressed. She is an open source contributor and project committee member for both Apache ZooKeeper and the Dropwizard web framework. Prior to working for Rent the Runway, Camille served as a software engineer at Microsoft, and most recently, spent several years as a technical specialist at Goldman Sachs, creating distributed systems for managing risk analysis and firmwide infrastructure. She has a BS in Computer Science from Carnegie Mellon University and an MS in Computer Science from the University of Wisconsin-Madison. Camille is a well-respected voice within the tech community, speaking on a variety of topics such as engineering leadership, distributed systems, scaling teams, and technical architecture. In 2017 she released her book, “ The Manager’s Path: A Guide for Tech Leaders Navigating Growth and Change .” Caroline Amaba — How We’re Balancing Quantity & Quality of Programmatic Ads Last summer, BuzzFeed made a huge decision to add programmatic ads to the website. Now that we have these ads, how do we balance native advertising, programmatic quantity, and programmatic quality to ensure the ad units generate revenue. This quick talk will give a high-level overview of how BuzzFeed Tech is approaching this balancing act. Before joining the Ads Group at BuzzFeed Tech as a software engineer, Caroline was in the advertising space for five years working as a full-stack web developer at digital and social advertising agency, VaynerMedia. She went from putting clients’ ads on sites, building microsites, and working with social media APIs to helping BuzzFeed solidify the dot com (and other owned properties) as a prime advertising platform. Caroline also puts her social media skills to use helping with the @buzzfeedexp Twitter and editing the tech.buzzfeed.com blog. Caroline has a Bachelor of Science Degree in Computer Science and a Bachelor of Arts Degree in Visual Arts & Technology from Stevens Institute of Technology. Christina Vuleta — Mentor Mingle: Mentor. Exchange wisdom. Find your squad. Interested in mentoring, finding a mentor….or simply sharing some wisdom? Mentors are all around you — the first move is just surrounding yourself with amazing people. This interactive meetup will provide guided networking to connect with people who can help you take your next step or vice versa. All you need to bring is your expertise and your ask. Christina is the VP, Women’s Digital Network at Forbes Media where she led the launch of Women@Forbes. She brings her expertise in generational insights, trends and strategic planning to building a business first platform for millennial-minded and entrepreneurial women on the rise. She oversees all editorial and strategic direction for the channel with the mission to help every woman take her next step forward. Christina is also founder of 40:20 Vision, a resource for women to start conversations and facilitate wisdom exchange between generations and cofounder of 40 Women to Watch Over 40, a community for women disrupting and innovating after age 40. Dheerja Kaur — Building Products for a Loyal Audience theSkimm has a loyal audience of 7M readers…also known as “Skimm’rs”. The fun part? Building for them. Learn how theSkimm developed its subscription app, how it thinks about integrating into Skimm’rs routines, and how to take a truly audience-first approach to building products. You’ll also hear how this methodology has scaled to a unique, innovative, and fast-paced product culture at theSkimm. Dheerja is head of product and design at theSkimm, a company that focuses on delivering news and information into the routines of female millennials. They have an audience of millions who engage with them daily via their newsletter, app, and audio/video products. Since joining theSkimm in its early years, Dheerja has helped grow the company to 70 employees, almost half of whom are in product development across product, design, engineering, and analytics. Prior to theSkimm, Dheerja was an engineering and product leader at ESPN, building ESPN.com and ESPN’s flagship mobile apps, and was the first engineer to build Grantland. Lyle Smith — Defining a new metric for success Social Lift is a metric at the core of BuzzFeed’s culture. For a long time, Lift brought light to content that was reaching a new audience and truly going viral, but changes to BuzzFeed’s distribution strategy made the metric less and less useful. Lyle will be discussing how she worked with editorial to define a new metric for success, and how we’re using our product suite to put that number in front of people. Lyle Smith is a Data Science Manager at BuzzFeed where she co-leads the Reporting Tools Team in addition to the Data Science Team. The Reporting Tools team is responsible for building tools and dashboards that track the health and performance of BuzzFeed’s content and properties. They also have been working on broader initiatives to improve company metadata, introduce anomaly detection and develop post metrics that better tie to ‘viral’ performance. Before starting her career in data science, Lyle worked as a consultant at Activate, a boutique media strategy consulting firm. Meghan Heintz — A case study on how to think about data products Building great data products isn’t just about throwing whatever the cutting edge algorithm at a problem. This lightning talk will be a short case study on how to think about using your data to build new features or products centered around Tasty’s trending recipes algorithm. Meghan is a senior data scientist at BuzzFeed, primarily working on Tasty. That’s right, you can blame her for those cheese stuffed top down cooking videos that thwart your Paleo diet plans. She previously worked at Zynga on dynamic difficulty tuning and predictive modeling. Before she was in data science, she worked as an environmental consultant on river and wetland restoration projects. She holds a B.S. in Environmental Resources Engineering from Humboldt State where she focused on fluid dynamic modeling. Natalie Fratto — The Adaptability Quotient: Why AQ is More Important than IQ & EQ in Tech. In today’s tech-driven world, the job market is changing quicker than ever before. As a result, adaptability may be the most critical success factor– both for individuals and for organizations. Natalie, SVB VP and writer on the topic of AQ, will explore how to assess your own AQ score and how to improve it. Natalie is a Vice President at Silicon Valley Bank. In this role, she supports the growth of Seed and Series A companies and manages SVB’s strategic partnerships with New York-based VC funds, accelerators, universities, and angel investors. Prior to joining SVB, Natalie went through Y-Combinator leading operations for Vive, an on-demand services company and worked as a strategist for IBM Watson. She started her career in IBM’s Mergers & Acquisitions practice focused on M&A within the semiconductor and medical devices industries. Natalie is a contributing writer for publications including Fortune and Fast Company. She can be found on twitter at @nataliefratto. Sara Menker — How data helped predict the most expensive weather disaster of 2018 Gro is a data product that enables the discovery and analysis of hundreds of trillions of data points in the global agriculture industry. Sara Menker and team members from Gro Intelligence will present how they used these data points to predict the 2018 drought that has cost Argentina $4 billion thus far. They’ll discuss technical challenges associated with data processing, building complex real world system models and the importance of data visualization in translating the data into decisions. Sara is founder and CEO of Gro Intelligence, a technology company that is bridging data gaps across the global agriculture sector, empowering decision makers and creating a more informed, connected, efficient and productive global agriculture industry. Prior to founding Gro, Menker was a vice president in Morgan Stanley’s commodities group. She began her career in commodities risk management, where she covered all commodity markets, and she subsequently moved to trading, where she managed a trading portfolio. Menker is a trustee of the Mandela Institute For Development Studies (MINDS) and a trustee of the International Center for Tropical Agriculture (CIAT). She was named a Global Young Leader by the World Economic Forum and is a fellow of the African Leadership Initiative of the Aspen Institute. Menker received a B.A. in Economics and African Studies at Mount Holyoke College and the London School of Economics and an M.B.A. from Columbia University.” To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 280 Women In Tech Nonbinary Technology Conference Buzzfeed Events 280 claps 280 Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"},
{"website": "Buzzfeed", "title": "tech and news working group", "author": ["Logan McDonald"], "link": "https://tech.buzzfeed.com/tech-and-news-working-group-7dabaaa38e45", "abstract": "Latest Posts Events Apply To BuzzFeed Reporter: Is anyone around to take a quick look at some code on codepen and help me understand what it does? This question — and many, many others like it — arose from an initiative called “The Tech + News Working Group,” a BuzzFeed News collaboration between reporters and nearly every role across the Tech department — including SREs, SWEs, data scientists and product managers. The BuzzFeed Tech team is afforded “7% time,” or half a day each week, to learn something new. For those of us on the Tech side of the working group, that 7% time is dedicated to bringing our technical expertise to the newsroom. A primary value of BuzzFeed Tech is fostering an experimental and collaborative workplace; One way that’s come to life is through the Tech + News Working Group. The foundation for the group was laid by former BuzzFeed Tech SRE Sri Ray , who provided countless acts of support to the newsroom throughout his time here. Reporters began reaching out to people on the tech and IT teams to help understand terminology, dig into tips, or verify the security of a file they had received from tipsters. The partnership allowed the Tech team to learn from reporters and consider the wider implications of our work. In turn, as insiders in the tech industry, the newsroom benefitted from our tools and perspective while reporting on technology, security, and business stories. Like other award-winning newsrooms, BuzzFeed News has a dedicated data journalism team that uses data and programming to aid in their reporting. But a year ago, we realized that there is a place for the Tech team to help with reporting, too. We decided to expand and formalize our collaboration, with an open Slack channel that includes members from both our Tech and News organizations to support day-to-day consulting on stories and work on larger ongoing projects, along with a new mission statement: Our mission [for the Tech News Working Group] is to act as an elevating force for the newsroom’s work through guidance and assistance using our technical skills. Our News team at BuzzFeed strives to shine light on the most important issues facing us today. Currently, this means a growing focus on the impact of the internet and technology on a range of topics covered by News. As a team of technical experts, we are uniquely equipped to assist in this realm. Since then, we’ve worked with the newsroom to develop reporting tools, including a project called Tubeviewer which took us down the YouTube rabbit hole . Through Tubeviewer, we found that the Up Next algorithm occasionally pushes users toward hyperpartisan videos that include divisive, conspiratorial, and sometimes hateful content. Tubeviewer simulates the viewing experience of real humans watching YouTube videos given a certain set of search terms. Turn it on, walk away, and come back to a full picture of your viewing experience. We’ve used tools like this to audit algorithms across tech platforms. We’ve also developed bots that send digests of patent information to Slack or collect data from filing systems like NYSCEF. We’ve developed patterns for doing network analysis through Wireshark and done visualizations for articles with the data . We’ve received hundreds of requests ranging from assistance with finding data from a website to explaining how authentication tokens are used: Reporter: working on a story about token based authentication vulnerability in a video game that uses google/fb sign on — and looking for someone to comment on the limits/risks of using token based authentication in general Sometimes we get requests for help with something that would require a lot of manual labor, were it not for a quick and easy tech solution: Reporter: this is a weird request that i apologize for but can someone count the number of stories under this tag if there is an easy way to do that… Engineer: can do with JS, count the number of item-list classes When we created the group, we didn’t know how popular the channel would be, but these requests are a fun break from our day to day “real” work. Sometimes the Tech team gets way more invested in the answer to a question posed by a reporter than the reporter intended. Once, we spent an embarrassing amount of time trying to determine what software Jessica Simpson used to create the graphic for this tweet (loads of EXIF digging later, we still couldn’t figure it out and the reporter had completely moved on). In the end, we’ve helped contribute to a bunch of stories and scoops, and in doing so created a bond of trust amongst reporters and technologists across our organization. Reporter: I credit this scoop to asking the group here, so thank you The mentality of a good engineer is similar to that of a journalist; both need to ask good questions and seek out accurate answers with persistence even when there might be bugs in a system. Over the past year, it has been fun and rewarding to build a stronger bridge between the News and Tech teams at BuzzFeed, and we only expect this work to continue. Through the collaboration, both teams thrive: members of the Tech team have the privilege of working hand-in-hand with journalists, while the newsroom is able to report on technology with more authority. If any of these things interest you, we’re hiring in both tech and news . Tips are welcome at tips.buzzfeed.com . Sharing our experiences & discoveries for the betterment of… 730 Journalism Data Journalism Technology Technology Reporting News 730 claps 730 Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Written by Site Reliability Engineer @buzzfeedexp Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-04"},
{"website": "Buzzfeed", "title": "hackweek 2019", "author": ["Plum Ertz"], "link": "https://tech.buzzfeed.com/hackweek-2019-f5a6c89296d2", "abstract": "Latest Posts Events Apply To BuzzFeed Once a year, BuzzFeed Tech is allowed to run rampant in our codebase*, with a simple directive: work on anything except work . The ambitious, scrappy, and — dare I say it — actually functional projects that spring from our Hack Week are a snapshot into the tech minds that power your daily dose of quizzes and memes. Remember, as you peek through the highlights below, that all of these projects were designed and implemented in under a week , and presented to the company (including our CEO, Jonah Peretti) that Friday! *In an isolated cluster, for all the SREs who may be concerned. Not working on work means no work, right? Not for our engineering team. Over the course of five days, our team contributed a total of over 150 pull requests and spun up 23 new services to support our Hack Week projects. Not bad for a weeks’ work! With absolutely no prompting, five different groups focused their efforts on Tasty, producing: The Tasty app has already introduced some great new features for users to share their baking and cooking experiences with tips and ratings. Tasty & Chill takes it a step further by introducing user profiles, so you can see the full collection of ratings and reviews a particular user has left. If a particular user always offers great advice, you can follow them within the app to see more of their tips and favorite recipes. Project Team: Suleiman Shakir, Graham Wood, Randy Karels, Malcom Mitchell, Joshua Walker, Will Kalish, Maria Enderton Want to get great Tasty recipes without having to type lots of words? Tasty Emoji Search understands food emoji as well as text, allowing you to save precious seconds when looking for your favorite mushroom recipe. Some emoji also come with punny results page titles as an added bonus! Project Team: Maria Enderton Tasty food isn’t just about the taste — it’s about the nutrition! The Tasty meal planner takes recipes a step further than just a single meal by letting users plan out an entire week of breakfasts, lunches, and dinners using Tasty recipes. Nutritional information and macro counts are included to keep track of a users’ specific dietary needs, and a week’s worth of recipes can be easily combined into a shopping list for easy prep. Project Team: Emily Ji, John Philip AI is the future of technology, so it was only a matter of time before someone built an AI to write Tasty recipes. The AI was trained with gpt-2-simple , and can take user-input suggestions for title and ingredients to produce new and sometimes… unconventional recipes. This one could probably take a pointer from our Tasty chefs. 😉 Project Team: Max Woolf One of the top-requested features for the Tasty app is Spanish translation. ¿Por qué no lo haces? This hack uses an ML Kit for Firebase to perform on-device translation (no network required!) for recipes. Project Team: Sufei Zhao (But not in the creepy, “Big Brother is always watching” way.) It’s exactly what it sounds like — emoji reactions to individual images/sections of articles on BuzzFeed! Who needs likes when you can have the love llama? Project Team: Caroline Amaba, Jess Kustra, Angela Medina, Xu Zeng, and Andrew Paulus An ephemeral chat format that will appear for a limited period of time on popular posts, so multiple commenters can chat in real-time. Think Snapchat, but less snap and more chat. Project Team: Clem Huyghebaert, Aaron Goldberg, Ben Stockwell, Allison Krausman, Elaine Dunlap, Chris Johanesen A one-stop-shop for managing BuzzFeed service deployments, checking statuses, and viewing documentation ( we’ve talked about rig before on this blog ). It was built as an upgrade to our current deployment service (geared mostly towards engineers and just shipping code) so that the purpose and status of one of our microservices could be understood and managed by anyone , not just the engineers who built it. Project Team: Ian Feather, Kevin Ushijima, Edgar Sanchez, Artyom Neustroev, Jack Reid, Agata Grdal, Dang Vang When things go wrong at BuzzFeed, we’ve got bots to help get our team on the scene. This project updated our current SlackBots (originally built during a previous Hack Week) that assist with managing incidents and contacting on-call engineers to give it the power to start on-call incidents, create and mark off items on checklists specific to the incident, and clean up lingering/abandoned incident channels. Project Team: Raymond Wong, Marc McDonnell, Jack Ussher-Smith Writing docs is great, but who documents the documentation ? Go Links is an internal short-linking system that gained popularity at Google and proliferated across many other tech companies . The team used an open-source fork of the project to allow anyone at BuzzFeed to quickly set up short links to important dashboards and files. Not only was this project a great collaboration across multiple BuzzFeed offices, but it also set up a great long-term system to quickly increase discoverability of internal documentation! Project Team: Arushi Bandi, Edgar Sanchez, Felicia Cippoletti, Kate Zasada, Lauren Zhang, Logan McDonald, Andrew Mulholland, Nicholas Gervais, Sami Simon, Shraya Ramani Two of the projects presented at our Hack Week Demo Day this year were 100% powered by our tech interns ! In addition to the Tasty Meal Planner (see #2 — powered by Emily Ji and John Philip), our UK interns Noshin Begum and Oyindamola Aderinwale built an entire customizable 2D game engine for BuzzFeed posts. Introducing: A Queer Eye Themed 2D side scroller game engine that you can build with custom artwork and embed into posts. Select your Fab Five member and background, and you’ve built a game in seconds ! BuzzFeed tech isn’t just about software and websites — we know our hardware as well. A few folks went above and beyond during Hack Week to bring the fun offline: Ads engineer David Zhao navigated the perils of Raspberry Pis, romset versions, and getting things delivered to Manhattan in a timely fashion to construct a fully-functional arcade machine (now part of the permanent collection of the BuzzFeed New York office!) Core infrastructure engineer Chloe Rota schooled our NYC office engineers on locksport (after making us all promise to never use this knowledge for nefarious purposes). With her detailed instruction, we were picking (very simple) locks in minutes. The class was capped off with Insomnia Cookies, a staple of BuzzFeed NY snack-related events. Our Minneapolis office took some time to rest their hacker-typing fingers with some home-baked treats — Lemon Meringue Bars and sugar cookies baked in the shape of Minnesota. What happens to these projects now that the sun has set on Hack Week? Some will rescind back into the void of “If only I had time to work on…” until next year, while others may be coming to a site or app near you. Which projects would you be most excited to see rolled out to production? Comment below to let us know! You can also find out about some of the new things we’re launching at BuzzFeed Tech by following our twitter, @buzzfeedexp ! Also, BuzzFeed Tech is hiring. If you are interested in browsing openings, check out buzzfeed.com/jobs . We have roles in Los Angeles, Minneapolis, London, and New York! Sharing our experiences & discoveries for the betterment of… 153 BuzzFeed Tech Hack Week Tasty 153 claps 153 Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Written by UI developer, baker of noms, casual economist. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-28"},
{"website": "Buzzfeed", "title": "code sharing a recipe for how we quickly cooked up the tasty app", "author": ["Graham Wood"], "link": "https://tech.buzzfeed.com/code-sharing-a-recipe-for-how-we-quickly-cooked-up-the-tasty-app-55e7bcb1e1eb", "abstract": "Latest Posts Events Apply To BuzzFeed There was a time when there was one gigantic Xcode project that housed all of the functionality that you see in the BuzzFeed app. This made it a “clone and own” job to create another app that may want to share a lot of the same business logic. The advantages of cloning and owning an existing app is that it enables you to start with a complete set of functionality that can be easily tailored to your requirements, and you don’t have to worry about maintaining compatibility when making changes to existing code. The disadvantages of this approach are that any bugs found need to be fixed in multiple places, and new features that may benefit both apps are built independently. This would be a good approach if the short term goal to get an app to market outweighs the benefit of shared code, but in our case we need to be able to maintain and build upon each app continuously. The BuzzFeed News and BuzzFeed Video apps had to take this approach for two reasons: Swift was new and because the existing BuzzFeed iOS app was largely written in Objective-C. These new apps were built with common frameworks embedded in their projects using Carthage. The BuzzFeed iOS app also started using this framework approach to share code across targets. For example, the BuzzFeed iOS app for iPhone/iPad and tvOS app for Apple TV share the same video player code. This solved the problem of duplicate copies of code, improving our process, but it didn’t quite get us to a mono repo. Here is a timeline of our iOS git repo history: Early 2010: The initial iOS git repo was started with the BuzzFeed app Xcode project Fall 2012: Hurricane Sandy takes out our data center and iOS repo, and a github repo was created from the repo with the latest pull from master (yay for distributed version control!) Early 2015: The team started working on BuzzFeed News and BuzzFeed Video Apps in Swift (before primarily in Objective-C) in separate repos. Mid 2015 : The BuzzFeed app dropped iOS 7 support and started making common code in Swift frameworks. Mid 2015 to Early 2016 : There was continued effort to move common code from apps over to frameworks. Any new code that had the possibility of being shared was developed in frameworks. April 2016: The iOS mono repo was created. Early 2017: The Tasty app was created in the mono repo. July 2017: The Tasty iOS app was launched on the App Store. Updating code in shared frameworks contained in different repositories is a mundane task. Debugging apps in separate repositories requires extra work, too. Once a framework is updated and ready for release, it must be versioned and tagged. At that point any dependent frameworks and apps must make sure to pull in the updated version in order to stay current with the latest update, which could contain an important bug fix. This can be less obvious if the update happens down the dependency chain. If the build system is not set up to automatically pull in all updated frameworks, this process is quite prone to human error. For example: Framework A is dependent on Framework B, and Framework B is dependent on Framework C. If the app only has a direct dependency on A, you would not get the update for a fix in C unless B and A have pulled in the update to C. Our web services at BuzzFeed transitioned to using rig with a mono repo . While the apps teams don’t have the need to deploy continuously, we thought the use of a single git repo for all of our iOS apps would alleviate some of the issues that we were having. The mono repo at BuzzFeed does not consist of all of our code across the organization. While discussions have happened on the pros and cons of that approach, we felt it best to keep our mobile application repos separate from our web service mono repo, so we created our own apps mono repo! The only drawbacks we foresaw with an iOS mono repo were larger build times and less ability to plan for updates (if one app updates a framework, others have to use that or branch). Since changes to shared frameworks are immediately built in all apps, it was a bit more effort up front to implement good test coverage to ensure that nothing breaks when a framework is updated. These cons have been fairly painless and have definitely been outweighed by the pros of having the mono repo organization in place. We utilize common code for the following things across apps/targets in our mono repo: Common UIKit and Foundation extensions/utilities and theming A/B testing framework Playing video Collection view helpers Common models Handling network requests Sharing activities Analytics When the plan came along to build a Tasty app, we needed to make a decision on whether we should spin up a new repo with copies of our frameworks to be maintained separately, or learn to work in the mono repo. It was an easy decision to use the mono repo for many reasons. It had been had been in use for nearly a year and the build and automated test system was already well established. During the time that the Tasty app was to be developed there would have been many changes in the shared frameworks that would have been either missed by Tasty or had to have been manually pulled into Tasty’s repo. The only lifting that was needed to get a Tasty app up and running with our shared frameworks was mapping the Tasty data models to work with our collection view rendering framework (the framework that takes article or recipe JSON from our API and displays it in a UICollectionView) and tweaking the layout code to match our design. You might think that collection view rendering is simple enough: this isn’t that big of a savings in effort versus building from scratch; however, our collection view rendering framework handles all different types of cells (the basic UI component of a collection view). Some example of cells the framework can render are autoplaying video cells and horizontally scrolling carousels of content (recipes in our case, breaking news stories in the case of the BuzzFeed app). With these cells also comes any impression and interaction analytics that we can use to determine what content and features are most successful. This allowed us to focus immediately on building our new app features, specifically the recipe page and the step by step instruction flow. We were able to use some common framework code here and even contribute back to it. Every architectural decision is visited with the question of whether the code should live in a common framework in each app’s specific code base. There is a lot of clone and owned code that does not make sense to put in a common framework (or at least didn’t for the Tasty iOS team), such as our deep linking logic and utilities for navigation to different areas of the app. Any code change in a common framework must be regression tested on BuzzFeed and Tasty apps and must not break any of our tests. Our engineers have had to make a habit of building and running multiple configurations (BuzzFeed app, Tasty app, test targets for each app, and test targets for any common framework changed) before pushing these changes up to github and having the build server find they broke the build for another target. Since the build server takes a long time to build all of the targets, bugs and regressions aren’t usually discovered for some time, and usually this results in variations of the following gif being posted to our slack channel: This has just moved the work to the developer of the breaking change, where previously the impact of the change would likely not be known until another app or framework tries to update to the new version of the framework. Swift migrations and Xcode updates have been painful, because everyone must agree on a plan so that we can move all of our code over at the same time.We also all share some of the effort of updating our common code base. As time goes on and Swift matures these migrations have been less painful, especially since frameworks have been allowed to build with different Swift versions. The Tasty app has been live in the App Store for almost nine months. As we add new features, we continue to explore opportunities for adding shared code and reducing tech debt in our common frameworks. Our iOS engineers meet weekly to discuss any common work that is in progress or coming up in near sprints so that we can coordinate these changes. We’ve discovered that some of our day to day pains can be used to our advantage. If we need to staff up an app for a sprint/release or two to crank out a feature, it is much easier to do so given their understanding of the common code base. Our transition to XCode 9 was smooth: we could all update as soon as our changes were merged into our master branch, thus putting behind us the pain of coordinating the update across all of the frameworks and repos. We don’t have surprises during regression testing from integrating a version of a framework that we neglected to pull in updates for. While it can be painful doing more testing on each feature, the benefits far outweigh the pains, and we’ve got a much stronger shared code base that we can more easily base new features off of! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 144 1 Thanks to Swati Vauthrin . iOS Technology Swift Buzzfeed Posts 144 claps 144 1 Written by iOS Engineering Manager at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by iOS Engineering Manager at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-11"},
{"website": "Buzzfeed", "title": "scalable request handling an odyssey part 2", "author": ["Mark McDonnell"], "link": "https://tech.buzzfeed.com/scalable-request-handling-an-odyssey-part-2-ad2433b2f6ed", "abstract": "Latest Posts Events Apply To BuzzFeed One of the primary functions for any website is the ability to serve content based on a given user request. This is the second article in a three part series. Last time we looked at BuzzFeed’s architecture and discussed how user requests were being handled, as well as figuring out what a new service (Site Router) would look like and why it needed to exist. In this segment we aim to explain the design of our Site Router service layer, its configuration interface, and some examples of the key features. So pour yourself a 🍹 and let’s get ready for some fun 🎉 To understand what our architecture looks like now, let’s first look back at what it used to be: In this flow diagram we have a user, who makes a request for some content. That content request first ends up at our CDN layer, is inspected (possibly manipulated), and finally proxied through to an appropriate origin server, which provides the content that is sent back to the user. This diagram is a simple ‘cold-cache’ scenario: the CDN hasn’t already cached the origin’s response, so the request flows through to the origin server. Once that request happens, the CDN will be able to cache the content so that future requests don’t reach our origins. In reality, when additional requests come in, we have lots of caching strategy logic to prevent origin servers from becoming overloaded (in addition to load balancers and auto scaling groups to handle the distribution and scaling of our microservices). Something to be aware of though, is that due to the distributed nature of the web, a user in the UK will find they will be routed to a different POP than a user located in the US for example. This means a cached request in the UK won’t necessarily be available to a US user (as their POP will have its own set of caches). There’s also multiple nodes within a POP, meaning multiple UK based requests can fluctuate between seeing a cache HIT vs a cache MISS (depending on the node those requests are routed to). Now, as far as our new routing layer is concerned, there is only a small change to this request flow (if this is all new to you, then go checkout Part 1 ): We have added an additional network hop into the mix: Site Router. Our routing logic has now been extracted from the CDN and moved into a separate “router” microservice. We do still have some basic routing logic within the CDN layer (generally anything that makes sense to be handled at the network edge — i.e. being nearer to our users), but the vast majority of logic is now sitting within the Site Router layer. The benefits of the new Site Router service outweighs the negligible overhead of an additional network hop, and through end-to-end load testing, the introduction of this service did not significantly degrade performance. On the other hand, the developer experience has improved massively: we received immediate positive responses from teams engaging with the service informing us that they have been able to easily add new routes whilst being able to A/B test specialised behaviour (as well as utilising other types of features we’ll cover in more detail below), all via a simple YAML configuration file which Site Router provides as part of its consumer interface. Site Router was also designed to be run upon our Rig platform , which facilitates easy deployment of the service to each of our ECS environments (test, stage and production) allowing developers to verify integration behaviours with other services before sending changes out to production. The Site Router service was originally built upon the very popular and highly performant open-source web server NGINX , but it has evolved over time to utilise the commercial version NGINX Plus instead as we came to realise that our usage was complex enough to justify using some of its specific features: DNS monitoring, service discovery, advanced metrics, live activity monitoring and tech support to name a few. Due to its asynchronous event-driven architecture NGINX is commonly used as a reverse proxy , which is exactly the pattern we were looking to implement for our Site Router service. The ubiquitous nature of NGINX meant that more engineers were already familiar with the product, as well as NGINX already having a stable and proven track record of managing high traffic workloads. This made it the right choice for BuzzFeed. Although NGINX is well known, and easy to pick up quickly, we decided we wanted to implement a thin abstraction layer on top of it so that our Site Router service would allow teams to not only add new routing requirements easily but to do it without having to worry about best practice configuration. This meant that we could avoid engineers accidentally modifying some subtle behaviour that would impact our overall performance goals. This was one of the primary concerns which made us want to change our VCL setup in the first place. In order to achieve these goals we designed a small configuration-based interface using a simple YAML file. By using this configuration abstraction we are able to dynamically generate the required nginx.conf file (used by NGINX) with the help of the Python programming language . Each of our mono repo’s microservices already has a config.yml defined. This is something that is provided as part of the rig platform we utilise for testing and deploying our services. We decided that the interfacing layer should use the same configuration file. It’s probably worth me saying now, that describing our configuration file is actually a lot more complicated than actually using it is 🙂 . That being said, before we see an example configuration file for our Site Router service, it would be best to start with an empty config.yml file, as that will help to introduce the concept of configuration on a ‘per-environment’ basis: Notice in the above configuration that we have various environments (dev, test, stage and production), but we also have something referred to as ‘default’. Default serves as the fallback for any non-existent keys in a specific environment configuration. For example, we commonly utilise the default block for setting debug log levels to “info” while overriding it to “debug” in the development environment. Our YAML configuration file holds quite a bit of information related to NGINX and our origin services for which we can define routing behaviour against. Breaking apart this chunk of configuration, we can see we have the following sections: upstreams and timeouts : upstreams are the servers that return the relevant content for incoming client requests. Engineers can add new upstreams simply by specifying their internal hostname (internally this uses NGINX’s upstream directive). Timeouts allow engineers to define global timeouts for each upstream. Engineers can also override these upstream specific timeouts on a per-route basis, which internally uses NGINX’s proxy_read_timeout directive. fallbacks : we allow engineers to define a failover upstream to handle a particular request if the initial intended upstream happens to error (internally this uses NGINX’s error_page directive). The value assigned must map to an existing upstream key. log_format : this lets engineers define additional variables to appear in NGINX’s request log output (internally this uses NGINX’s log_format directive). locations : this is the most important section for engineers as it’s where all our routing logic is placed, although it doesn’t actually appear in the main config.yml file exactly like I’ve shown above. Don’t worry, I’ll explain what I mean by that in just a moment. We lean heavily on some YAML specific features such as “ Anchors and Aliases ” and “ Merge Key ”, which allows us to reuse configuration across environment blocks and to easily override specific keys without repeating huge chunks of configuration. We use this approach primarily for our ‘upstreams’ and ‘timeouts’ sections, as we usually want to override those values to be environment specific. Elsewhere in our configuration we define a dns_resolver_ttl_override key that allows us to define the TTL for name server DNS resolution. This setting internally uses NGINX’s resolver directive and we’ll cover why we do this in part three of the series, where we discuss the various technical challenges we’ve encountered whilst designing and building Site Router. This section concludes the general configuration model that is provided as part of our Rig platform. In the following sections we’ll start to dig into the Site Router specific configuration model and cover the various features our abstraction provides. In the above example routing configuration you can see how just a few simple keys are able to offer engineers a lot of control over how incoming requests are handled. Let’s take a look at the individual keys and understand what they represent: url : mandatory key using a regular expression to find matches on an incoming request. redirect : returns a 301 Moved Permanently status code to the new path. upstream : defines the backend that would handle the requested path. description : provides more context to the regex and purpose of the route. example : links to online regex testing tool for quick verification purposes. So at this point we’ve seen two fundamental pieces of our configuration model: the foundational pieces (such as defining upstreams and timeouts for those upstreams) and the routing configuration. Let’s now review a small selection of the key abstractions we’ve built into the Site Router service. One of the most common things we need to do (other than define a route) is to override that route’s behaviour based upon details within the incoming request. This is important as it allows engineers to do things such as testing a new service backend, as well as handle path rewriting for users based in specific localities, which will be further detailed below. In the following example route we have defined some behaviour that states the path /foo should be proxied onto the “foo” upstream, but if we find the request includes a query param of service=new_thing then we should change the upstream that was going to handle the proxied request to be the “bar” upstream instead (you can imagine how engineers use this feature to verify a new internal service is routed to and functioning correctly in production). The variable key accepts any valid nginx variable (of which there are many! ). The match key utilises regular expressions to help find a match. The symbols used at the start of the match pattern ( ~* ) are NGINX specific and indicate that the regex pattern specified will be case-insensitive. Failovers are where we wish to try a different upstream in order to ensure we’re serving some form of content in the case of our chosen upstream failing to return something useful. In the following example route we have defined an upstream for the path /foo and if that particular proxied request to the “foo” upstream returns a 404 Not Found status, then we should retry the request using the monolithic upstream instead. The reference to monolith is actually a mapping back to a key defined within our ‘fallbacks’ section of our main configuration file, which ultimately was a pointer resolving to monolith.buzzfeed.com . Engineers can define their own status codes for which they expect a fallback to be applied by using the intercept_codes key, which is a space separated set of codes to listen out for (e.g. intercept_codes: 404 500 503 ). An example of this fallback is where we previously had rolled out a new service responsible for rendering our primary article pages, but not all of the data that drives the pages was supported, and so we needed to rely on our original monolith service for rendering the content for those specific pages that the new service wasn’t yet ready to handle. Path rewriting allows us to change the intended request path from what the user specified. It can be useful in a number of scenarios, such as when users make a common misspelling for a particular resource. Engineers use the path key, at the top-level of a location block, to trigger a rewrite. The standard use case is simple enough, but let’s look at a more detailed example where we needed to change the path within an override block instead: What the above example demonstrates is that a user can make a request for something like /uk/foo , but if they were using a QA host (e.g. qahost1.buzzfeed.com), we’ll override the upstream whilst also rewriting the path. The reason we rewrite the path in this example is because the upstream that is being set as the override doesn’t support the original path, e.g. the foo upstream supports /<edition>/<path> , whereas the override only supports /<path> . Below are some example paths that would match this location block, and if using a QA host we can see what they would be rewritten to when within the override block: /uk/foo → /foo /us/bar → /bar /de/baz → /baz To help us achieve this behaviour we require the use of named capture groups to define explicit segments of the incoming path. So we can see we’ve split the path into two pieces: Edition : (?<edition>(?:uk|us|de)/) Page : (?<page>\\w+) When rewriting the path (within the override block), we can now reference the ‘page’ named capture group like so: ${page} and thus omit the ‘edition’ portion of the path. Similarly, in the override block, we’re also able to utilise named capture groups to help us identify a specific host (e.g. qahost1, qahost2, etc). This enables us to proxy the request onto a specific host without having to duplicate the override multiple times, each functionally equivalent, but simply checking the $host variable for a different value. In the example above this is demonstrated by identifying and capturing the numerical value after the ‘qahost’ part of the hostname, e.g. (?<num>[12456]) and so when defining the upstream, we can use the num capture group as part of the value assigned: webapp_qa$num . Meaning, we can ensure requests with specific hosts reach the intended upstream: qahost1.buzzfeed.com → webapp_qa1 qahost2.buzzfeed.com → webapp_qa2 qahost3.buzzfeed.com → webapp_qa3 When looking at how to override specific routing behaviour using an override key, there were two things I glazed over at the time: Location and override match order. Naming format required for override keys. The problem that we ultimately face is one of ‘precedence’. NGINX location blocks are “first match wins”, which means the order of the location blocks is important because if you have two routes (in this order): /foo /foobar You’ll find that requests for /foobar will be handled by the first route and not the second as its pattern matching is too ‘loose’. This is why when using regular expressions we will tend to err on the side of caution and use very explicit anchors (e.g. ^ and $ ) so that instead of /foo we would likely use something more controlled like ^/foo$ . The location precedence issue is also why we used a list data structure for our location blocks, as this allowed us to guarantee the order of the routes when we transformed the YAML into Python. Where as dictionaries did not have the same guarantees of insertion order (well, that was the case up until about a month ago, before it was announced that future Python versions would start to guarantee dictionary insertion order). Unfortunately our individual overrides couldn’t also be defined as lists as we needed the ability to use YAML’s merge syntax to reuse large chunks of configuration. By using a YAML object, it meant we could define an override outside of a specific location block, and then use the YAML merge key syntax ( <<: ) to insert the override into multiple location blocks, thus reducing lots of duplication. In doing so, we lost the ability to guarantee the override order. To solve that problem we decided that override keys should be numerically based, so that we could use our template engine’s dictsort feature to order the keys for us. That’s it for part two of this three part series. To recap, we looked at: The old and new request flow and how Site Router fits into that. Site Router’s configuration interface and how it builds upon our Rig platform. The routing design and the various options available. Real world examples for some of Site Router’s features. In the final part to this series we’ll look in more detail at the various technical challenges we stumbled across (and fixed), as well as how we debug Site Router issues and also how we use integration tests to verify Site Router behaves the way we expect it to. If any of this sounds interesting to you (the challenges, the problem solving, and the tech), then do please get in touch with us. We’re always on the lookout for talented and diverse people, and we’re pro-actively expanding our teams across all locations around the globe. We’d love to meet you! ❤️ To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 83 DevOps Https Http Request Buzzfeed Posts Scalability 83 claps 83 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-15"},
{"website": "Buzzfeed", "title": "scalable request handling an odyssey part 3", "author": ["Mark McDonnell"], "link": "https://tech.buzzfeed.com/scalable-request-handling-an-odyssey-part-3-c29aac9c39a", "abstract": "Latest Posts Events Apply To BuzzFeed One of the primary functions for any website is the ability to serve content based on a given user request. This is the third and final article in the series. In Part 1 , we looked at BuzzFeed’s architecture and discussed how user requests were being handled, as well as figuring out what a new service (Site Router) would look like and why it needed to exist. Next, Part 2 of the series explored the original request flow and how Site Router fitted into that. We also reviewed Site Router’s configuration design as well as some ‘real world’ examples of the supported features. In this final segment, I’ll recount the various technical challenges we stumbled across and fixed. I will also review how we debug Site Router issues and use integration tests to verify Site Router behaves the way we expect it to. Here is a list of the items we’ll be covering today: Caching old DNS. Breaking caching behaviours. Splitting up large configuration. Lots of redirects are hard to maintain. REGEX OMG. Unable to use if statements effectively. Follow requests through state changes. Toggling HTTP headers with maps. Active-Active static assets across cloud providers. Testing NGINX and mocking upstreams. Building test abstractions. Rollout Strategy. System observability and monitoring. Prepare yourself, it’s going to get wild… 🐗 One of the primary reasons we switched from the open-source NGINX to the commercial version was due to a problem with how NGINX handles the caching of DNS queries. When NGINX starts up, it resolves the DNS for any upstreams we define in our configuration file. This means it would translate the hostname into an IP (or a set of IPs as was the case when using AWS Elastic Load Balancers ). NGINX then proceeds to cache these IPs and this is where our problems started. If you’re unfamiliar with load balancers, they check multiple servers to see if they’re healthy and, if they’re not, the server is shutdown and a new server instance is brought up. The issue we discovered was that each server instance’s IP was changing. As mentioned earlier, NGINX caches IPs when it’s booted up so new requests were coming in and being proxied to a cached IP that was no longer functioning due to the ELB. One solution would be to hot reload the NGINX configuration. That’s a manual process, and we would have no idea when that should be done because there is no way of knowing when a server instance would be rotated out of service. There were a few options documented by NGINX that tried to work around this issue; however, all but one option meant we would have to redesign our entire architecture because they didn’t support the use of NGINX’s upstream directive ( which we are leaning on very heavily as part of our configuration abstraction ). The one solution that would allow us to utilise the interface design we had turned out to only be available via NGINX Plus. The cost of going commercial was justified: we wouldn’t need to rewrite our entire service from scratch and there were additional features that we could utilise moving forward (some of which we had already been considering). Once we switched over, the solution was a one-line addition using the resolver directive that allowed us to define a TTL for the cached DNS resolution. As part of the release process for Site Router, we wanted to roll it out incrementally. This gradual process meant we could privately test and monitor the system to ensure we weren’t introducing a large breaking change into every other service in production. With this approach in mind, we added some logic to our CDN that would bucket specific users so that their requests would be routed via Site Router (but only if the request had the header X-SiteRouter-Enabled set). All other users would be directed to the services directly as per the normal routing behaviour. At this point we were concerned about users getting a cache HIT from requests served by Site Router while we were still ‘kicking the tyres’ on the new service. We decided to utilise the HTTP Vary header to help us serve different cached content to the user depending on the context of their request. But this didn’t work as expected. The reason it didn’t work was because CDN logic was incorrect. We had modified the code so that we were adding X-SiteRouter-Enabled to the Vary header only when it was set on the incoming request. What we should have done was to make sure we always added X-SiteRouter-Enabled to the Vary header. Caching is hard: you need to be very careful when making changes to logic that relates to caching because you can cause all sorts of problems if not handled correctly. Our config.yml contained everything. After about a year we realised we should probably split out the configuration into separate files because it was becoming quite hard to maintain. The situation became even more apparent once we started having to support multiple hosts (e.g. www.buzzfeed.com , www.tasty.co , etc). The solution was to have a top-level /hosts directory containing config responsible for host specific routing logic: One benefit of doing this was that we are now able to introduce new features to the host files whilst also being able to reduce unnecessary duplication. For example, we’ve added a host_settings key which allows us to abstract away settings on a per-host basis, such as the need to define an upstream value within each location block. In the above example, our host file defines a single upstream called ‘foo’, which all our location blocks will implicitly use unless overridden on a per location basis. BuzzFeed has been around a long time, and we’ve seen a lot of services come and go. We’ve accrued a large number of old paths that need redirecting to new service endpoints. In some cases, we can have hundreds of redirects that need to be implemented, and that becomes quite a lot of ‘noise’ inside of our routing configuration. To solve this issue we implemented a solution whereby redirects can be placed inside separate configuration files that are dynamically compiled into our main configuration at build time. These are then imported into our main host files like so: The host key allows us to control which host the new redirects should be sent to. The use of {} means we can replace the subdomain to be environment specific (e.g. stage.example.com or www.example.com depending on if the Site Router is running in staging or production environments). All redirect files are located inside of a top-level /redirects directory (so they’re easy to locate), while the content of these files is demonstrated below: We use the power of regular expressions to match incoming request paths to a specific location block’s routing logic. Every now and then a specific feature of a regex pattern will throw you a curveball. One of those was the use of quantifiers (specifically braces ). To demonstrate, the following example is broken: The reason this is broken is because when we generate the associated location block for this in NGINX code we’ll find it produces the following output: Notice how the opening brace has actually been interpreted as a new starting block in the NGINX code, and so this completely breaks the nginx.conf file! To avoid this issue we discovered we should wrap the regex pattern in quotes. We already use single quotes in the YAML file but those quotes aren’t persisted once we transform the YAML into Python, so we needed additional quotes inside of them: This will now result in the correct rendering of code within the nginx.conf file: Due to the potentially complex set of behaviours a route can have (such as overrides, redirects, path rewriting, failovers, etc.), it becomes increasingly important to be able to follow the logic branches of the code to understand how a request was actually handled. For example, if I make a request for the path /foo what actually happens? Did I hit the expected upstream (end of story) or was there an internal path rewrite that meant I went to the upstream but now using a new path of /foobar ? Maybe I went to a completely different upstream than expected because I’m based in the UK region and there is an A/B test currently running that means I’m being sent to a new upstream? To help us identify these specific scenarios we add debug HTTP headers to our responses, which indicate the path taken internally by NGINX. These generally look like the following: X-SiteRouter-Upstream : indicates the upstream as defined within configuration. X-SiteRouter-Override : indicates new upstream that we overrode to. X-SiteRouter-Fallback : indicates the upstream we failed over to. X-SiteRouter-Error : indicates type of error as well as includes a unique request id. X-SiteRouter-RouteIndex : indicates the configuration route index that matched. X-SiteRouter-RouteDescription : indicates description defined in the configuration. X-SiteRouter-Server : indicates specific server (e.g. static assets redundancy). X-SiteRouter-Path : indicates path taken by server (e.g. static asset bucket info). These headers aren’t available to the public. When we need them, we enable the headers dynamically via query parameters. But this then introduces an interesting problem, which is how do we configure that in NGINX code? This leads us nicely onto our next section… We can’t use standard if statements in our NGINX code because those are considered evil by NGINX (and can even cause NGINX to segfault 🙀). In this case, we’re forced to utilise a different type of directive, known as “ maps ” in NGINX. Just to clarify, NGINX can use if statements, but the variables we inspect as part of the condition are the problem and so by using the map directive we can create variables that aren’t going to cause NGINX to freak out. Here is the general structure for using the map directive: In the above example, we can see that we have to specify a variable we want to look at. I change $some_variable_to_inspect to be something like $query_string . Next, we define what value we’re hoping to find assigned to that variable (e.g. “expected_value”) and also provide a default value if we don’t find what we’re looking for. Finally, the end value is assigned to a new variable (e.g. $a_new_variable_is_created ). The reason 0 and 1 are used in the above example is because these values are coerced into truthy/falsy values, and if we do decide to use an if statement later on in our NGINX code, then we can use it to inspect the new variable and NGINX won’t have an issue, like so: Now we understand why if statements are problematic, we can revisit our earlier question of how do we enable our debug headers by checking a query string. Consider the following example map code: We want to now use the $debug variable to decide whether to show the debug information in the HTTP response headers we send back to the client, but if we try the following example (which uses an if statement) we would find it doesn’t work: Instead we use a little trick in NGINX whereby if you attempt to add a header with an empty value it won’t be shown. For example, if I was to stick the following code into an nginx.conf file, then the header wouldn’t be added to the response as the value provided is effectively empty: Meaning, we need to use the map directive to determine whether the value is empty or not: In the above example we are inspecting the new $debug variable (which we created via the earlier map directive) and we say: if the value assigned is 1 then assign the value “foo” to a new variable called $upstream (otherwise assign an empty string to it). Our code can now utilise the new $upstream variable to display the debug information only if the user actually provided the relevant debug query parameter: We use Site Router for serving static assets such as JavaScript, CSS, and images (e.g. /static/js/foo.js or /static/css/foo.css ). In order to provide the appropriate availability and resilience expected, we utilise multiple cloud providers for serving our static asset content. We also want to implement this behaviour — not using a redundancy design (‘active-passive’), where if one provider fails we then failover to the other — but instead use an ‘active-active’ model which means both providers are working side-by-side and we round-robin requests between them. The benefit to this approach is that we can be sure both providers are working and are also consistent in the content that they serve. We implement an abstraction within the upstream block which allows an engineer to specify nested cloud providers. This configuration would ultimately generate multiple virtual servers, one for each cloud provider specified. This is then generated into the following example NGINX configuration code: All an engineer needs to do now is to define a new route and to associate the upstream with the static_assets upstream. Any requests now passed to that upstream will be distributed between the defined virtual servers using a weighted round-robin balancing method. Meaning: the first request will be proxied to AWS, the second proxied to Google, the third proxied to AWS, and so on back and forth. When testing Site Router, we want to be sure the behaviours we’re verifying are isolated and not resulting in real requests reaching our origin servers. In order to achieve this we need to modify our generated NGINX configuration file to use mocked endpoints (for which we use a Python web server to handle all outgoing requests from NGINX). Running our tests are actually triggered by a simple bash script and this is where most of the magic happens as far as dynamically modifying our NGINX configuration. In essence we compile our nginx.conf file and then use sed to modify the code just before running our test suite. One of the key modifications we make is to switch any server directive to using the same IP and port as our mock web server. This means we’re able to handle all outgoing proxy requests and to review their responses to see if we went to the correct location. We also utilise the debug headers we spoke about earlier to give us extra contextual information about whether the request was routed through the correct pathways as defined in our routing configuration files. Another change we had to make was in how we handled 301 redirects in NGINX when running our test suite. When NGINX triggers a 301 redirect (for example, to advertise.buzzfeed.com), instead of making a request to the real IP that an external DNS would typically resolve to, the request should be sent to our local IP 127.0.0.1 (we do this by adding the relevant hosts to the service’s /etc/hosts file). When we started to implement redundancy for static assets, we realised we needed a way to verify the failover behaviour between cloud providers (e.g. if one provider failed to respond then the other provider should handle the request). In order to achieve that behaviour, we again use sed to modify the nginx.conf so that any proxy_pass reference to AWS is changed to a simple return 404 “Not Found” , and any proxy_pass reference to Google is changed to a simple return 200 “OK” . The way we have these upstreams defined in our configuration means that AWS will always be requested first, and so we can be sure the redundancy is working as expected by checking we got a 404 followed by a 200. We write our integration tests in Python, meaning we can develop abstractions around the test suite code to make them easier to work with. We use py.test’s parametrize (table matrix) feature, which allows us to pass multiple test scenarios through a single set of assertions and each ‘test’ is then placed into a separate host specific file. This means engineers don’t have to worry about anything other than the abstraction. Below is an advanced example covering a few different features an engineer might utilise as part of constructing a request to be tested: The above example demonstrates a test that verifies a request to /foo for the host www.buzzfeed.com. When sent with a specific custom header, indicating the user’s “edition”, the test states that it expects an override to be triggered ( 01_japanese_edition ) and that a 301 redirect to /jp/foo , as well as receiving a specific set of response headers will occur. We mentioned earlier how we initially wanted to do private testing of Site Router in production, and we explained how one of the side effects we encountered was through the misuse of the Vary HTTP header. When rolling out the Site Router into production for wider user testing we decided the best approach would be to adopt a ‘regional’ rollout process. What this meant was that we selected a country that had a smaller footprint than our primary audience, and if we had issues rolling out the Site Router service, we would be able to roll back and purge our caches without causing a stampeding herd effect on our origin servers. If you have a large delta in traffic patterns between regions you can also consider using Fastly’s geoip.city lookup to rollout to specific cities within a chosen region. This would allow you to reduce the cache purge blast radius. To help with the cache purging we would modify the incoming request to include an additional header that our upstream applications would be able to identify. If this new header was found, then the service would include a new key within their Surrogate-Key HTTP response header. The new key added to the Surrogate-Key header would indicate if the response was generated for a request that had come via the Site Router. This subsequently would allow us to purge just those affected requests rather than having to purge our cache by the URL alone (which could have caused a lot of extra traffic flowing through to our origins otherwise). In order to observe the Site Router service, we rely heavily on Datadog for monitoring and logging. Site Router is built upon NGINX which ‘out of the box’ doesn’t support instrumenting your own custom metrics. This meant with the open-source release we would have had to depend upon the basic monitoring provided. It’s possible to extend NGINX using OpenResty and Lua , but we decided against this option initially in favour of switching to NGINX Plus, which provided us access to many more granular metrics and allowed us a greater insight into what was happening with our requests and the various upstreams. We also have PagerDuty configured to be triggered by our Datadog 5xx monitors. To this end, we have many monitors setup for Site Router, but only the 5xx monitors (setup for each of our upstreams) are directed at PagerDuty as these are the most critical ‘user-facing’ errors and demand our immediate attention when on-call. That’s it for this final part of our three part series on request handling. It’s been a long journey, but let’s have a look back over a few key things that we’ve learned about these past few weeks. We started out understanding a bit about what a HTTP request is, how it’s constructed and why inspecting the response headers can be useful to understand how clients are meant to handle future requests for the same resource (such as caching behaviours). We introduced the CDN as a core part of our resilience strategy and how we were too reliant on it to perform routing specific logic. We then discussed the various decisions that go into proposing a new service and how in our case we have agreed to migrate our routing logic from the CDN into a new NGINX based service called “Site Router”. From there we proceeded to build various abstractions around NGINX in order to enable engineers to get what they needed done quickly and efficiently, with as much flexibility in features as possible, without them needing to be experts in how to write NGINX code. We also need to have an understanding of basic data structures to provide a consistent expectation for our users with regards to NGINX’s ‘first match wins’ routing behaviour. Finally, in this segment, we covered many of the hurdles encountered along the way, such as: How NGINX tries to be helpful by internally caching DNS queries. How the Vary HTTP header can result in users not getting a consistent experience if implemented incorrectly. How you should be constantly evaluating your services and refactoring them to ensure they stay efficient solutions for your users (for us, this meant splitting out our large configuration into separate files, which also helped to expose additional functionality). How new requirements, like needing to handle hundreds of redirects in a sane way, can help you to tease out code bloat. How ‘understanding’ a piece of software and knowing it are two different things, e.g. having to utilise map directives in smart ways to avoid issues with if statements, and how the compilation of our abstractions caused unexpected broken results. How important being able to debug and trace routing requests, especially due to the amount of state changes that can be applied throughout a single HTTP requests lifecycle. How using an ‘active-active’ pattern can provide a more robust and trusted system than an ‘active-passive’ design. By using active-active it means we can be sure both A and B are functioning correctly and consistently, rather than discovering at the last moment that when needing to failover to B it doesn’t actually work. How testing a proxy service like NGINX can introduce different types of problems, such as how you manage DNS resolution internally to prevent things like 301 redirection. How you need to plan a solid rollout strategy and how you might need to rollback . Making sure there is appropriate monitoring and instrumentation can help you identify issues and alert on them if necessary. That’s it. We’ve covered it all now. So let’s see who we have to thank for all this… Although the majority of the code has (up until this point) been written by myself , it would be very foolish to think I alone would be capable of such a feat of success without a lot of help from a lot of very smart individuals. There is absolutely no way the Site Router project would have turned out nearly as successful as it has without the following honourable mentions: Clément Huyghebaert (Director of Engineering) John Cleveley (Director of Engineering) Andrew Vaughan (Site Reliability Engineer) Raymond Wong (Staff Site Reliability Engineer) Dang Vang (Senior Software Engineer) Yeny Santoso (Senior QA Analyst) Of course there has been much discussion, great ideas and useful input given by many more people than those mentioned above, and to you I give a very warm hug and say a big “thank you”. Finally, thank you to Ian Feather and Caroline Amaba for their help converting this entire series of jumbled notes and broken thoughts into something altogether much more coherent. If any of this sounds interesting to you (the challenges, the problem solving, and the tech), then do please get in touch with us. We’re always on the lookout for talented and diverse people, and we’re pro-actively expanding our teams across all locations around the globe. We’d love to meet you! ❤️ To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 19 2 Thanks to Swati Vauthrin . Nginx Scalability Https Buzzfeed Posts DevOps 19 claps 19 2 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-03"},
{"website": "Buzzfeed", "title": "docbot a documentation librarian for slack part 1", "author": ["Chloe (Rowan) C."], "link": "https://tech.buzzfeed.com/docbot-a-documentation-librarian-for-slack-part-1-bd91f64b0680", "abstract": "Latest Posts Events Apply To BuzzFeed As a new BuzzFeeder, one of the things I thought about often while onboarding was documentation. When using documentation to set up my environment and learn about our systems, I realized that the state of docs was inconsistent: some services had thorough documentation, other services had stale or incomplete docs. In addition to this, documentation often ended up in multiple places, or worse…deeply buried and impossible to find without the help of a co-worker. A few months after I started, BuzzFeed threw our Summer Hack Week 2017 . Hack Week at BuzzFeed is a time when we’re encouraged to stretch our muscles and our creativity to design hacks that we think will be fun or useful. To me it seemed like an excellent opportunity to attack the problem of documentation. For Hack Week, I proposed a Slack bot project, with the goal of creating a documentation librarian. Creating a Slack integration allowed us to leverage the fact that everyone at BuzzFeed already uses Slack for office communication, and to get the project going quickly. (A Slack bot can be as simple as defining a single “slash command”, or you can build a whole web app and connect it to Slack via an API.) During Hack Week, I worked with a small team (shout outs to summer intern Derek O’Brien and to Miguel Coquet and Arielle Benedick 👋🏼 ) to get a proof of concept up and running. Since BuzzFeed is primarily a Python and Go environment, we made the decision to create an app in Tornado (a Python framework) which would provide the greatest opportunity for others to work with us. We also decided that for proof of concept purposes we would back the app with a single SQL table, which we populated manually based on research done by team members. Due to its status as a “hack,” the original Slack bot didn’t have a thorough design phase. Instead, the group creating the hack got together, brainstormed on features, and chose a handful we thought could be accomplished within the week. We also made decisions that would never fly in a production service, such as hard coding in URLs for a number of documents. These decisions helped us move quickly during the proof of concept phase, but we knew we’d end up having to change them later if this service ever made it to production. By the end of Hack Week, we did have a working service. We could call up the readme files for any of the services listed in our table, as well as retrieve the point of contact for the service. We hooked up features to add a service’s documentation to the table and to mark whether a service was still in development or if it had been deprecated. However, the Slack bot was about to undergo an evolution… Riding high on the wave of enthusiasm after Hack Week, I was ready to push this bot towards production, where it could be the integration we all dreamed of! The only problem was it needed a name and an owner. I landed on the name Docbot as a self-evident amalgamation of Documentation and Bot. While I wish I had been more clever in naming it (I’m competing with names like “ rig ,” “AV Slater,” and “Dashbird” here!) it was time to figure out where Docbot would find a home. While I was searching for the appropriate people to help me work on this, I realized a natural fit would be the recently-created Better Docs Initiative, a working group I’m part of within BuzzFeed Tech focused on improving documentation. The group was formed by engineers who were invested in seeing the state of documentation improve at BuzzFeed. (In our documentation TODOs is an article about the Better Docs initiative!) That meant a shift in the team: from the scrappy hack week team that got it going to folks in the Better Docs Initiative who had time to take it on. Fortunately, that transition was made easier by the fact that Arielle is also a Better Docs member, so I didn’t lose everyone from my original team! With that settled, Docbot was ready to undergo a conversion to being production software. The first step of the process was to write a proposal for implementation. Since there had been only a makeshift design brainstorm, this proposal gave me an opportunity to think through the strengths and weaknesses of the hack, as well as the genuine needs of the software going forward. Writing out a design and implementation plan highlighted some of the trade-offs made during hacking, such as provisioning a SQL table for a data source. A major change to implementation I made during the proposal stage was to change the data storage method. I wanted to find a way to create a “source of truth” for the bot that didn’t rely on human intervention. After surveying the landscape, I realized there was already a perfect source of truth available: our services themselves. Our code at BuzzFeed is organized in a mono repo (a single repository with a directory for each service). Each service in the mono repo must include a service definition with certain required field in order for it to be part of rig, so we added a new field. We then crawled these service directories in order to create a single API call. The new proposed architecture of the bot looked like this: As the biggest impact from this project would be the resources required to provision it, this proposal was delivered to the SRE team. Once the proposal was approved, I was approached by another engineer, Ola (who’s on Better Docs with me), who wanted to know if she could use Docbot to work with a mentee on! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 62 Slack Documentation Buzzfeed Posts Technology Hackathons 62 claps 62 Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"},
{"website": "Buzzfeed", "title": "scalable request handling an odyssey part 1", "author": ["Mark McDonnell"], "link": "https://tech.buzzfeed.com/scalable-request-handling-an-odyssey-part-1-d91a295af4d8", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed’s architecture is fairly flat and boring (which is a good thing), it indicates that we’ve chosen technology, as well as architectural patterns, which are tried and tested. This post is part one of a three part series reviewing a section of BuzzFeed’s architecture, related to handling user requests, and understanding a bit more about what this architecture looked like before compared to now and why it matters. We’ll begin by reviewing how content is requested, understanding the performance implications of our architecture, how a CDN can help with these issues as well as looking at evolving our architecture to support something ultimately more maintainable and accessible to our engineers. So strap yourself in, grab a ☕ and let’s get started… One of the primary functions for any website is the ability to serve content based on a given user request. The complexity behind the process of request handling will change significantly depending on the type of website users are interacting with and the service architecture surrounding that website. Let’s begin by inspecting a HTTP request for a BuzzFeed resource and understanding the various headers that are sent back to the client once the request is made. In order to make a HTTP request, we require a tool that speaks the HTTP protocol. Below is an example using curl to request the BuzzFeed home page: A HTTP request is made up of a ‘resource’ that you wish to reach (this could be a file or a web page, for example) as well as some ‘headers’ that allow you to provide additional context for the request being made. When the server handling a request sends back a response, the content is exposed via a ‘body’ field, while there is a separate set of response ‘headers’ that provide additional information to the client that made the request. If you’re a web developer, then there’s a good chance you’ll find yourself inspecting HTTP headers a lot during your day-to-day work (i.e. HTTP headers are great for debugging). There are browser based tools for making HTTP header modifications (as well as viewing the response headers) but I generally find that working with curl — in a terminal — to be the most efficient approach. For example, if I wanted to view only the response headers that have cache information within them, then I would execute the following command: What this command does is it makes a silent (i.e. no progress bar or errors are shown) HEAD request and then sorts the output and finally filters out every header except the ones that have the word ‘cache’ used as either part of its name, or as part of the value. The output from such a command could look something like the following: Below is a snippet of the HTTP request headers we use when making a request for the BuzzFeed home page via curl (these are generated by the curl tool after it has parsed our given command): First we state that, for this request, we wish to GET a resource located at the path / using a specific version of the HTTP protocol (version 1.1). We also specify that the Host server we wish to connect with should be www.buzzfeed.com . We then state that we’ll Accept any type of response, and that it doesn’t matter what that response is (it could be plain text or HTML, or anything for that matter). Finally, we add some additional information about ourselves (User-Agent) and in this case we are indicating to the server handling this request that we are using the curl tool. A snippet of the HTTP response headers, from a BuzzFeed server, might look something like: There’s a lot more information contained in the real response headers, but I’ve omitted them for brevity. One important thing to pay attention to when looking at this snippet of the response headers is that the first line is informing us that the server we communicated with was also using the HTTP protocol version 1.1 and that the ‘ status ’ for our request was considered to be a successful one (i.e. 200 OK). The purpose of these response headers is to help clients (a client being either curl or a web browser etc) to know how to handle not only this request, but any future requests for the same resource. For example, if the request was made by a web browser it would be able to inspect the Cache-Control header in order to determine if it was able to cache a copy of the response for a set period of time, thus reducing the number of requests needed to be made to the origin server handling the request. For a full list of the possible HTTP response headers, see this reference . Interestingly, you’ll see from the earlier response headers that there were Cache-Control and Pragma HTTP headers being set and both of them indicated that the origin server does not want its content to be cached by the client (i.e. the values assigned to those headers were: no-cache, no-store, must-revalidate and no-cache). Yet we can still see some ‘custom’ response headers (such as X-Cache) which suggest the content is indeed being cached? We’ll revisit why this is later on. But for now, let’s move onto considering performance. BuzzFeed is a global brand, and our users are distributed across the world. When a request is made for our content, we need to ensure that the content is delivered in a timely matter. Our content is currently served out of the US east region, so what happens if a request is made by a user based in the UK? In this scenario a concern would be the possible latency penalty our users in the UK would have to pay in order to request content from our servers. To resolve this particular performance concern BuzzFeed utilises a CDN (Content Delivery Network) to help us geographically distribute our content to servers located at key regions across the globe. When a user makes a request for BuzzFeed content the request is first routed to a POP (Point of Presence) nearest to their location and then within that POP they’ll be directed to a specific server instance (known as an edge node ). The CDN is responsible for the replication of our cached content at these POPs. BuzzFeed has historically relied very heavily upon one particular layer of our infrastructure to handle the majority of our routing requirements. That layer being our CDN. Our particular CDN provider ( Fastly ) provides us with the ability to programmatically define behavioural logic ‘at the edge’ using a C based programming language called VCL . This language allows us to inspect and manipulate incoming requests and to control how they are handled and where those requests should be directed (to learn more about Fastly’s implementation, read this ). Fastly helps to protect us from heavy traffic patterns and problematic scenarios such as DDoS attacks. Fastly also allows us to easily keep our content fresh by providing an API that let’s us purge our stale data across all our edge nodes within milliseconds (this is done either by specifying a ‘range’ of items to purge, or by providing a specific identifier ‘key’ which can purge one or more items from our cache). Clem H., Director of Engineering, has talked previously about moving to Fastly and how it has improved not only BuzzFeed’s ability to better understand and monitor its cache, but also our ability to improve client latency, cache hit ratio and our ability to scale to meet the growing demands on BuzzFeed’s infrastructure. Earlier, when looking at the BuzzFeed home page HTTP request you’ll remember that we noticed the response headers were suggesting the content should not be cached and yet at the same time was being cached. This actually makes sense now that we understand the caching is being handled by our CDN instead of the client. By way of a special header our services provide to Fastly we can indicate exactly how we want our content to be cached by Fastly as well as when/where it expires or is purged. This is one of the extra benefits of utilising a CDN such as Fastly. Otherwise, we’d be at the mercy of clients — i.e. web browsers and downstream proxy servers — caching our content, and if we needed to purge that cache we could potentially have a much harder time doing so and also doing so in a timely fashion (e.g. a breaking news story that needs to be seen immediately). So up until mid-2016, what was BuzzFeed’s service architecture? Well, at a very high level, our original architecture looked something like the following (overly simplified) diagram: Placing a CDN in front of your “origin” servers is fairly standard practice nowadays and can help not only with distributing your content, but also preventing unwanted or dangerous traffic… In BuzzFeed’s case, our origin servers are individual microservices for handling specific parts of our website. For example, our home page is served by one microservice while our article pages are served by a separate microservice, and so on for the various types of content that we provide. At this point in time the situation we found ourselves in was one where we had bottlenecked a core part of our logic (that being the CDN logic responsible for interrogating incoming requests and providing an appropriate response) and in doing so we were failing to separate specific behaviours which would end up restricting our ability to scale, as well as provide better consistency and security around future changes to that logic. In order to improve upon what we had, we decided to extract out this problematic aspect of our system into a new service layer. We’ll cover the details of why that decision was made, but first we need to understand the inherent thought process involved when thinking about creating a new service. Creating a new service isn’t cheap (both in the sense of cost and overhead), so we need to be sure that doing so is the right decision. When deciding upon whether to define a new service, there are a few fundamental questions that need to be answered, such as: What should it do? Why does it need to exist? How will it help us? How will it evolve? Let’s consider each of these questions in turn, with regards to what ended up becoming BuzzFeed’s “Site Router” service… Remember, the architecture was originally designed to proxy requests between multiple possible origins like so: The concept behind the new “Site Router” service was simple, but both effective and essential in supporting our ability to scale: The service should allow engineers (of all skill levels) to be able to easily and simply create new routing logic, which directs incoming traffic to the relevant origin servers responsible for handling those requests. It should be a highly scalable solution, as well as being feature rich and secure. As far as being “highly scalable” we want to be sure that if there are issues with our CDN provider (e.g. we lose our cache), then the Site Router is able to handle many thousands of requests per second. To be “feature rich” means we intend for engineers to have a lot of control over the routing behaviours (these features will be covered in the next installment of this series). Finally, to be “secure” means we’re able to properly test the routing logic engineers are implementing, in order to ensure we’re not breaking user or system expectations (and by moving the routing logic out from VCL means there’s less opportunity for engineers to break our caching behaviours which are configured at the CDN layer). BuzzFeed’s technical architecture is designed and built to support our users at scale. As with most large scale systems, we lean heavily on our ability to cache content that has been sourced from our origin servers, and to serve stale content in those rare times when a part of our system misbehaves or is unhealthy. We utilise many different tools, services and applications in order to support our scalability requirements, and one of the key components of the overall system is the CDN layer (which acts as our front door). So with all this good stuff happening at the CDN layer, what was the problem? The issue was twofold: We wanted greater security (I’ll explain more about what this means in the next section) We wanted to extract the complicated VCL logic from the CDN layer. The primary reason for wanting to extract the routing code from the CDN was that it was difficult to manage, was lacking test coverage around specific logic (and itself was hard to validate) and ultimately this meant we had a hard time preventing engineers from making unsafe changes that could end up negatively affecting our caching strategies and/or causing incoming requests to be routed to the wrong origin server. We have a lot of VCL logic within our CDN and if our CDN provider (Fastly) were using a standard version of the Varnish HTTP accelerator , then we could have solved part of the problem (i.e. testing the service more concretely and its various behaviours) by setting up a local instance of Varnish and running our VCL logic through it. But as that wasn’t possible, by extracting the routing logic from the VCL and providing a clear and clean configuration model in its place, this helps engineers to focus more on making their services available quickly, without having to worry about performance best practices or have concerns around breaking our existing caching strategies. By providing a configuration model we abstract away those issues. We would provide greater confidence in the service by defining abstractions that allowed engineers to write simple tests (both at the unit, integration and smoke test level) that verify the routing logic is valid and functioning as expected. From the point of view of security: we would be able to ensure all our microservices were private and sitting behind internal load balancers within a protected VPC network, instead of having to be public services just so Fastly could direct traffic to them. The Site Router would sit inside its own cluster within the same internal network so it could freely communicate with the other containerised services that we have. We would configure Site Router to accept traffic only from Fastly, thus reducing our attack surface. By extracting routing this way, we could incorporate custom behaviors and firewalls, or even hook in other third-party SaaS providers. Ultimately what we ended up with was a high-level architecture looking a little bit like this: The evolution of the service would (and did) happen organically. Our intentions were to look into the possibility of open-sourcing the Site Router code base so it can be easily dropped into an existing system architecture (although this will likely require some modifications to allow decoupling from the BuzzFeed platform — but it is definitely a possibility). As for our internal consumers (i.e. other BuzzFeed engineers) we’ve seen incredible uptake of the Site Router service. We’ve received and managed the feedback we received and implemented many new features beyond what we had originally envisioned. It has been wonderful reaching out and helping multiple teams across the tech organisation to realise the power and flexibility of the Site Router, and we expect this to continue. That’s it for part one of this three part series. To recap, we looked at: What it means to make a HTTP request. How we handle the performance concerns (i.e. latency) inherent with making HTTP requests from distributed points across the globe (which come from the fact that BuzzFeed is a global brand). The what, why and architectural design of our routing service layer. Probably most importantly: we discussed the thought process and considerations that goes into deciding if a new service is even the right solution. In part two of this series we’ll look in more detail at the design of the Site Router service layer, including its configuration interface, as well as examples of some of the key features. By the way, if any of this sounds interesting to you (the challenges, the problem solving, and the tech), then do please get in touch with us. We’re always on the lookout for talented and diverse people, and we’re pro-actively expanding our teams across all locations around the globe. We’d love to meet you! ❤️ To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 285 Thanks to Swati Vauthrin . Http Request Scalability Cdn Buzzfeed Posts Https 285 claps 285 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-15"},
{"website": "Buzzfeed", "title": "buzzfeed techs 2017 year in review", "author": ["Swati Vauthrin"], "link": "https://tech.buzzfeed.com/buzzfeed-techs-2017-year-in-review-6c2955c7efec", "abstract": "Latest Posts Events Apply To BuzzFeed A lot can happen in a year, and 2017 is definitely no exception. BuzzFeed Tech had a great year, and while there were a countless number of projects, events, and learnings, we asked everyone in the tech organization to tell us what they thought were some of the biggest highlights for them in 2017 (hint: there’s a lot ). We launched tech.buzzfeed.com this year! The BuzzFeed Tech team has written some great articles over the course of a year, ranging from a list of our favorite hacks ( 13 Mind Blowing Hacks that BuzzFeed Tech Swears By ) to how we hastily deploy hundreds of updates and experiments a day ( Deploy with haste: the story of rig ). We also started sharing the @buzzfeedexp Twitter handle amongst the BuzzFeed Tech family. Every week, we have a volunteer take over the handle to give the Twitterverse a window into the life of a BuzzFeeder in Tech! We had a lot of fun tweeting 📣 . One of our viral favorites came from Sam Thurman with his very controversial tweet of eating blob tea: Diversity at BuzzFeed and in BuzzFeed Tech This year we brought in 52 new hires across the Tech organization! 🌈 Jonah published his diversity report . To top it off, we won in the Media category for the 2017 Diversity in Tech Awards Presented by Code/Interactive ! This ! What you’re reading right now. We published 28 posts this year! 💯 Programmatic Ads — read about it here and this post about the engineering behind it: tech.buzzfeed.com Video Pager + Unsolved launch! — best experience possible for superfans of the show. Check it out here ! Tasty app and Tasty.co launch! Read about what The New York Times had to say here ! We also wrote about some of the data science that went into it: tech.buzzfeed.com Mentorship program (pilot from July) — people across Tech can participate to be mentors or mentees and we’re super excited to see the benefits from this incredible effort! COP — “Centralized OAuth Proxy” — Our Infra team launched a service that consolidates authorization and authentication across many BuzzFeed services and provides a single sign-on experience Vidder — a web-based video editing tool that makes creating short-form BuzzFeed videos as easy as creating BuzzFeed list posts International Homebuilder Launch — giving our International editors tools for their homepages Transcoder V2 API — responsible for converting video files from one format to another, resulted in a reduction in the number of video renditions made and bandwidth costs by 40% TileMaker Launch — pretty much revolutionized how quizzes are made/translated and added millions of views! PrimeDay 2017 & Holiday Gift Guide Launches! Babadook Easter Egg — similar to last year’s Halloween spooky mode , we added a little something special for our readers to celebrate Pride Facebook Page Health Dashboard — a data-focused dashboard to track the health and growth of all our BuzzFeed Facebook pages over time Cultural Cartography — a project spearheaded by our data scientists to analyze how our content connect with people’s actual lives, and how people use our content to connect with each other BuzzFeed Tech continued the Hack Week tradition! For one week in July, the BuzzFeed Tech team put down their usual work to take on some projects that were also getting pushed to the backlog, do something completely different (BuzzFeed & Chill), or just learn something new (computer networking)! Here’s an overview of the amazing projects that came out of the week! Hosted three more Primetime Tech Talks (after launching December 2016!) on Digital Content Creation , Mobile App Testing and Data Products ! We also had a special Intern edition event during the summer. Held our first Women in Tech Summer Soiree. Read about it here ! This year BuzzFeed Tech went to the Grace Hopper Celebration , hosted in Orlando, FL. BuzzFeed Tech also hosted a happy hour event, where BuzzFeed producer and celebrity Ashly Perez emceed a friendly Nifty crafting competition! A lot of our individual BuzzFeeders were part of talks and meetups as well: BuzzFeed hosted a “Studio Session” as part of the 99U design conference . Elaine Dunlap spoke about the design process behind working with Nifty and how custom technology supports their workflow. Participants also took part in an hands-on “Nifty” creative problem-solving challenge. Peter Karp gave a talk on Stitcher , our internal video editing software at PyGotham . Sara Gulgotta is one of the organizers for QueensJS , and in October, fellow tech BuzzFeeders Caroline Amaba and Angie Ramirez gave talks: Caroline’s talk was on selecting the right Javascript framework for different projects, and Angie gave an overview on video transcoding ! Sarah Meyer had a bit of a tour this year, giving two talks on regular expressions at JSConf and NodeJS Interactive . She also gave her “Sharing is Caring” about our shared component library, BuzzBlocks at numerous conferences: RuhrJS , NodeConf EU , and Nodefest Tokyo ! It doesn’t stop there, Sarah was also part of a Fireside chat at AMP Conf ! Will McCutchen gave a talk on rig at AWS re:Invent , one of tech’s biggest conferences of the year. Swati Vauthrin was the commencement speaker for The George Washington University School of Engineering 2017 graduation. 🎓 Our amazing publisher and fearless BuzzFeed Tech leader, Dao Nguyen gave a TED Talk on What Makes Something Go Viral! This was a really exciting year for us with a few twists and turns. As BuzzFeed continues to evolve as a media company and destination for people to connect with content, we at BuzzFeed Tech will be right there with it. We are looking forward to what 2018 will bring! ✨ Happy New Year! ✨ ❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️ Special thanks to Caroline Amaba , Jessie Wu, and everyone else in Tech for helping consolidate this list. Team work! 🙌 To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 148 Thanks to Jessie Wu . 2017 Buzzfeed Posts Year In Review Digital Media Publishing 148 claps 148 Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-28"},
{"website": "Buzzfeed", "title": "programmatic ads the engineering perspective", "author": ["Caroline Amaba"], "link": "https://tech.buzzfeed.com/programmatic-ads-the-engineering-perspective-353c7cb51f5d", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed pioneered Native Advertising using various content formats (quizzes, lists, etc.). As part of the Ads Group, I work on implementing and improving ad products for our clients on our website, and BuzzFeed has writers, designers, and video producers who create content for these advertisers as BuzzFeed posts and videos. In an announcement a few months ago, BuzzFeed has begun putting programmatic ads on the website. Programmatic ads are created by the advertisers who buy ad slots on the website in a “marketplace” of sorts, commonly associated with “banner ads” though that is no longer the case in the modern web. Native Ads are premium units that showcase content created by and for the BuzzFeed audience, sponsored by our clients. We love native ads because of their seamless feel on the website, and that vision and way of advertising has not changed or been abandoned. So, why start putting programmatic ads? Won’t it hurt the website? As Jonah Peretti, BuzzFeed’s CEO put it in the Business Insider Article : Tactically, programmatic [ads have] improved in terms of loading times, mobile experience, and ad quality and opens up another way for us to monetize our huge audience. So, the decision came from up top to implement programmatic ads in order to generate additional, incremental revenue for BuzzFeed. After all, we’re still a media company, and we want to continue creating great content for our vast audience. But, I’m not here to talk to you about all of the business about this. As you may have read in past articles on our tech blog, the Tech team has a number of microservices. A good number of them power the website and our other Owned & Operated (O&O) properties, so in order to keep things as consistent as possible on the front-end, there is a shared component library. The Ads Group has been using this library to house the central core of our ads setup. This was a huge timesaver for me as I did the initial setup work for making the new ad calls (I’ll get into that in a bit!) and setting up the spots for the new units. Calling a programmatic ad was not at all different than how we called the native ads: the biggest difference was simply handling the data differently. The Ads Group has a master configuration for all of the spots, and so I added the slots for the programmatic units and made sure they would render. So, how does our system for getting ads work? We currently are using few third-party services to serve and source our ads — such as DoubleClick for Publishers (DFP) and Facebook Audience Network (FAN). In our case, DFP manages advertisers and distributes their digital ads across the Internet. In addition, DFP provides tools for our in-house sales team to input data and creative assets on behalf of our advertising clients. We also have in-house tools that help us rapidly develop and place our native ad units. On our end, we decide where these ads will be placed. We have specific spots on the website and mobile apps where ads can go. Using the DFP’s SDK on the respective platform, we request an ad. If there is an ad: great, we get the data and input the fields appropriately (and this will vary between native and programmatic ads). We also tag our ad slots with certain requirements and parameters; thus, DFP will also have to look for an ad that meets the criteria of the ad slot. If there isn’t an eligible ad (or any ads!) to put there, we gently collapse the loading wireframe or sometimes we have “backfill” which are usually internal BuzzFeed promotions (like for our Tasty app ). We animate the collapsing unit in order for a smooth transition as other sidebar units (other articles, social network links, etc.) fill in the space. As I mentioned earlier, we currently have two main kinds of ads on BuzzFeed.com: native advertising and programmatic ads. BuzzFeed Native Ads are highly customizable, more premium units that boost first-party BuzzFeed content built by and for the BuzzFeed audience, sponsored by our clients. These units have information about BuzzFeed posts, or other engaging media (like GIFs, video, or key imagery). The code handles this data and renders a unit to that unit’s certain specifications. You may have recently seen the Display (“GIF”) Card unit on the right side on desktop, and at the bottom of articles, or you may be familiar with a post unit. From an engineering standpoint, these units are super flexible and easy to coordinate with the look, feel, and overall experience of the website. Here’s a bit of pseudocode as to how we go about loading in a native ad unit onto our website: As you can see, we will end up doing a bit more code-work in renderNativeAd() on the website itself in order to display the unit’s creative. Typically, we’ll have a custom layout for this unit, and once DFP sends through the fulfilled ad unit slot, we lazy load any images, text, etc. into the template we’ve made for this particular unit. Now let’s quickly compare the flow for programmatic ads: The code does a lot less work here because programmatic ads load in as iframes from DFP. The iframe contains the code developed by a third-party to display their ad. We just make sure we have the ad slot fulfilled, and if it is, we show it. Programmatic ads rely on the advertiser to develop their own creative — the images and text you see — for their ads. There are a few things the Ads Group has put in place to have quality and engaging ads get placed on our O&O properties. The most visually obvious thing we do is ensure good placement on the website, and that the context around the ad unit is not confusing to the user. We have also coded this process so that if we don’t get an ad back for any reason, that spot collapses and doesn’t show empty space on the website. In addition, with DFP we’re allowed to pick and choose certain advertisers that are allowed to put units on our website. Advertisers are also categorized, and our Ads Distribution team has the ability to block “bad” ads: very poorly designed units that don’t live up to the quality of our website, questionable content, or even ads that our users send in feedback on (DFP in particular also allows for its own ad reporting and blocking option as well). Advertising is an interesting, and sometimes convoluted, industry. As a publisher, BuzzFeed wants to be sure that we’re as transparent as possible about who is allowed to put ads on our website. We’ve implemented an “ads.txt” : a transparent spec set forth by digital advertisers and publishers that lists who we allow to advertise on our site ( you can read more about it here ). With the improvements in the Internet and web technologies, programmatic ads have become better, higher quality, and have evolved to include tools that better monitor bad actors. In addition, even though these ads are generated based on your browsing history and/or the host website’s content, advertisers are now trying to design these ads so that they are appealing, not (usually) annoying. Despite this, the Ads Group still wanted to make sure we service the needs of our clients — their display units getting view impressions to a specific audience — but not entirely disturb the BuzzFeed reader. As such, we planned to roll out the exposure of the programmatic ads gradually. Even before the consideration of programmatic, the Ads Group ran some inline tests using native ad units. Additionally, when Facebook introduced Instant Articles, programmatic ads were first tried there, using the Instant Article as a sort of sandbox environment. These first tests were to see if users would immediately correlate programmatic with bad ads (something that many of us who used the web in the late ’90s and early ’00s are familiar with). Some time has passed since then, and BuzzFeed decided to try programmatic again, but this time on our website and — following after — mobile apps. We first started with some units on right sidebar on desktop of our Homepage and News verticals, as well as related article pages, placing the programmatic ads between some BuzzFeed thumbnails and existing native ad units or promotions. On mobile, we opted just to put a unit at the bottom of an article. The Ads Group has data on click-through rates (CTR) and impression rates (CPM) from the units that already exist in these spots on the website. We used this data to determine where we wanted these new programmatic units to go. We put these ads behind an A/B Flag with a percentage of exposure, so we could monitor the effects these new spots would have on our users. As we saw very little to no change in user experience metrics, we ramped up the exposure of ads. We still have a holdout percentage that will not (for the foreseeable future) get ads on BuzzFeed.com in order to have a control to compare against. By having this holdout group, we are able to compare various user experience (UX) metrics over time. In addition, we’re able to consider our future planning in new placements, or how we can make our current ad unit placements more effective. The week we decided to launch the program, we actually ran into some hiccups on launch day. We were new to what to expect out of these ads, and we did not account for a few things and how they would interact with the rest of the site. Instead, we reversed our changes, updated, and tried again the next day. The second time around, there was much success, and we were very excited to start seeing the numbers! We also carefully tagged some of the metrics (e.g., load times, errors, no ad fulfillments) in order to keep an eye on the website experience as well as the effectiveness of the units. These first few weeks would inform how we continued on with the program. The BuzzFeed app developers that work with us on Ads also implemented the programmatic units, following the same gradual roll-out, a bit after we launched on the website. The BuzzFeed App experience and the user base are different than the website: our app users are some of our most engaged readers and quiz-takers! Some of our measurements that we’ve been collecting on the programmatic units on the web did not necessarily inform how the experience would work on the mobile apps because of the different user behaviors. We have also gotten great feedback from our users that allowed us to consider our programmatic ads program. BuzzFeed Tech takes user feedback very seriously, so we’ve been coming up with new plans and ideas for programmatic ads on BuzzFeed App in order to enhance the experience. Over the past few months, we’ve changed how ads are visually rendered in context, slightly adjusted their placement, and added more to the site at inconspicuous spots in order to continue to maintain a good user experience. The Ads Group has also begun to experiment and try out more sizes and better placements for engaging advertising. The Ads Group is working on improving and enhancing all of our ad products that exist on O&O properties and mobile apps. By collecting data and observing all of our incremental changes, we can make informed decisions about improving the placements of programmatic units (as well as our long-existing native ones). Also, this data gives us great information on the value of the BuzzFeed audience in terms of what sort of vendors are bidding for ads on our website. This data can then be used, not just for better ad products and sales leads, but inform BuzzFeed content or website features. Since we are constantly reviewing this data, engineering can move fast on incremental updates and tweaks to things like load time and lazy-loading visibility. Even small visual tweaks to the programmatic ad units’ designs are things we consider as users and BuzzFeed staff give us feedback. The Ads Group and Ads Distribution team have also implemented private marketplaces (PMPs) in order to make sure we allow consistent, high-quality vendors advertise on our site. In addition, we’re looking into header bidding: a more advanced way of opening up the ad inventory to multiple vendors simultaneously in order to increase our ad slot fill and variety of advertisements. This is a slightly different flow from our code before that asks for “regular” programmatic ads, and we’re currently exploring how to best implement this method on our O&O properties. All the while, the Ads Group has not left our native ad products behind. In fact, our findings in programmatic help us improve these products as well. We’re also looking into ways of uniting programmatic and native units on the website in order to create cohesive advertising campaigns and programs for the BuzzFeed audience. BuzzFeed’s programmatic ads program is one that is still evolving as we explore and gather data about these placements. In addition, this informs how we engineer these units on the website and mobile apps in order to maintain the best user experience. BuzzFeed Tech is constantly learning and improving, as well as keeping a pulse on advertising trends, technology, and best practices. If there are any questions or curiosities about how BuzzFeed Tech works on ads or anything at all related to BuzzFeed, be sure to follow us on Twitter @BuzzFeedExp and reach out! Different members of the BuzzFeed Tech family take it over each week! Sharing our experiences & discoveries for the betterment of… 73 Thanks to Jessie Wu and Katelyn Stoll . Advertising Programmatic Buzzfeed Posts Publishing Digital Advertising 73 claps 73 Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-07"},
{"website": "Buzzfeed", "title": "fearless deploys", "author": ["Liz Liao"], "link": "https://tech.buzzfeed.com/fearless-deploys-adaa2c43d9fe", "abstract": "Latest Posts Events Apply To BuzzFeed Your pull request has been approved, your tests are green, QA has given the thumbs up and now it’s time to merge and deploy your code into production. So, how are you feeling? If you were anything like me when I started at BuzzFeed, you’re probably feeling preeeetty nervous. At the time, I found doing a production deploy of a user-facing product nerve wracking because often it did not go smoothly. In a typical scenario, we would deploy, notice errors and then immediately start working on a hotfix, knowing that every minute that went by increased the probability that users were encountering broken code. Recently, I realized much has changed and production deploys are no longer the stress inducing events they once were. Frequency of manual and automated testing, improved tooling, quick error diagnosis, and fast rollbacks have all contributed to my confidence when doing a deploy. The Beginning When I first started at BuzzFeed, our team did feature based releases for our internal tool which meant that the schedule was determined by our product manager. When a release was ready, our product manager would brief users on what features were coming in the release. He would then plan the deploy with our users to minimize downtime and also made sure that key users were around to test the new features that were just deployed to production. Since there was a lag between when features were done and when they were deployed, we had separate branches for production, staging and development. Having three separate branches produced a lot of maintenance overhead because often production and staging only had a subset of features and bug fixes in development. A production deploy was a scheduled event that would take often take the good part of a day. Because the new version contained multiple features and/or bug fixes, it often involved the support of most of our team and some of our users: developers, site reliability, a product manager, editors and sometimes even our DBA. Emotional state when deploying*: 😱 😱 😱 😱 😱 😱 *my emotions projected on each team member Introducing Rig Then came Rig , our in-house PaaS . One important concept introduced was that “master is golden” —meaning that changes are tested, reviewed and QAed before they are merged. All versions of master should be deployable and nothing can be deployed without passing tests. This was great! This process meant way less branch maintenance, resulting in less human error and a more dependable master branch. Deploys for both staging and production were much faster and individuals could deploy on their own so testing happened more frequently. However, our team was still doing feature based releases and a single deploy to production included a rather large changeset. From my point of view, that meant going from several nervous people to one: me. Emotional State when deploying: 😱 Using Rig: Team Workflow Our team workflow eventually changed to better leverage some of Rig’s strengths. Because deploys were fast, we started to deploy on every commit to master. The person who made the changes was now the person who owned deploying them to production, and smaller sets of deployed changes made it easier to pinpoint issues. An error no longer involved the entire team. Emotional State when deploying: 😳 Using Rig: Checking all the things In one instance, I did a production deploy, checked that the web application looked good and I went on with my day. Twenty minutes later, our team received a report that the same web application didn’t even load. I was initially confused and realized that I checked too early and the version that I was looking at was in fact the old version. So lesson learned — use your logging and monitoring to check things and find them issues before your users do! Fortunately, Rig’s features include support for PaperTrail and Datadog (distributed logging and monitoring, respectively). During deploys, we now used PaperTrail, our logging service, to see any issues in real time. Multiple services can be viewed individually or interleaved together. Our team also recorded metrics in the code and set up Datadog dashboards to display them. A quick analysis of the state of the deploy has been useful to determine whether a quick rollback was needed. Emotional state when deploying: 😰 Feature flags How many times have you been in the situation where there’s a bug in production and it’s because there was a typo in something that you just deployed but OMG the change still needs to be reviewed, built and deployed and that will take at least 30 min? I’ve totally been there. When writing new features, our team often wraps new code in a feature flag. This is flag that can be turned on and off without a deploy. We have a service that stores the flag state, provides an API and web interface and we make call to this service every time our service handles a request. Because our team develops using feature branches, feature flags allow us to merge unfinished features or shared components to master. QA regression tests with the flag off and we remove the flag and corresponding code when things have been in production for a while. The best thing about them? Anyone on our team can turn that sucker off when things are going awry. No special knowledge needed. No sweating bullets while implementing the rollback plan….or hotfix…. or waiting for the new build that includes the hotfix. Emotional state when deploying: 😨 Datastore tooling Deployments that include migrations are my least favorite because they’ve caused the most issues in the past. The test data we were using was too simple to uncover migration issues or perhaps old code with the migrated database wasn’t tested well enough. A failed migration often meant that I had to get in touch with our DBA since a simple rollback wasn’t possible. This often meant that the time to resolve the issue took longer. Thinking back, I realized that many of the issues that arose with deploys happened because the transition wasn’t tested well enough. So, it was really exciting when tooling was introduced around our databases. Developers and QA could now easily refresh staging dbs with production data, reset a failed migration, or rerun migrations. No more asking your DBA or mucking with SQL — just reset with a push of a button. Emotional state: 😓 Now This post was inspired by an incident where I voiced hesitation to do a deploy on a Friday. This was an old rule we had put in place in case a hotfix or rollback took a long time or people were just not available. A teammate responded that we should trust in our process and that we as a company and team have worked hard to make improvements. After many smooth deploys I’ve built up faith in the workflow that we as a team and company have developed over time. I’m lucky to work with lots of talented folks because that this wouldn’t be possible without our kick ass QA and developers acting as the pillars in this workflow. These days, deploys still make me a bit nervous because I’m a worrywart but I probably need a different kind of help for that. Emotional state: 😅 Recommended reading: tech.buzzfeed.com tech.buzzfeed.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 79 1 Thanks to Matt Reiferson . Continuous Delivery Deployment Imposter Syndrome Buzzfeed Posts Continuous Integration 79 claps 79 1 Written by Senior Software Engineer @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Senior Software Engineer @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-11-10"},
{"website": "Buzzfeed", "title": "meet buzzfeed techs 2017 intern squad", "author": ["Katheryn Zhao"], "link": "https://tech.buzzfeed.com/meet-buzzfeed-techs-2017-intern-squad-7d9d3bcefa05", "abstract": "Latest Posts Events Apply To BuzzFeed This summer, nineteen interns worked on BuzzFeed Tech’s various products and services. We got to see first-hand how BuzzFeed maintains and improves the platforms which allow our vast audiences to enjoy their favorite media content. Under the guidance of mentors from across BuzzFeed’s various tech teams, we designed products, analyzed data, improved internal tools, and built all sorts of apps. It was hard work, but we had fun doing it! Keep reading to get an idea of the insights we gained, the challenges we faced, and the perks we enjoyed in our ten weeks at BuzzFeed. If you’re interested in a tech internship for next summer, you can follow along with the application process here . Massachusetts Institute of Technology, 2019 What did you work on this summer? I worked with the Data Infrastructure team and my main priority was cleaning up our framework that helps launch spark batch jobs on Amazon Elastic MapReduce clusters. I helped generalize the workflow to create a common interface for developers to work with. I also began developing a prototype for integrating our service with Google Cloud. Why BuzzFeed? BuzzFeed has always been my go-to site when I’m just browsing the internet. They create a variety of content, from quizzes about what kind of dog I am to breaking news investigations, which our generation often lacks knowledge about. As a software engineer, my goal has always been to work for a company with a great culture and a big impact on the younger generation, and I believe BuzzFeed does just that. What’s been the highlight of your BuzzFeed experience? That would have to be Hack Week. Seeing everyone in BuzzFeed Tech work across different teams and even office locations was very interesting. I took this opportunity to learn something new and ended up working with someone from the People’s Team to develop my first SlackBot, which helps direct users to an expert if they needed help on something. New York University, 2018 What did you work on this summer? My summer project involved creating a new service that utilizes the Google Video Intelligence API to make videos searchable by content. In addition, I contributed new features to existing video infrastructure services as well as a SlackBot for documentation discovery during Hack Week. Why BuzzFeed? Having worked previously in small businesses, I knew I wanted my first tech internship to be at a larger and more established company. BuzzFeed provided that, but at its heart, it still felt like a scrappy start-up. Despite being an intern, I was able to make substantive contributions on a regular basis like any other engineer. What’s been the highlight of your BuzzFeed experience? It’s hard to choose, but working on my Hack Week project was a lot of fun. I was lucky to land on a great team developing a SlackBot to help users find documentation for existing services. It was a great way to meet other people in the company and tinker with new technologies. Macalester College, 2019 @ProgrammerQueen What did you work on this summer? I worked on an internal tool used by editors to manage the Top Story and Breaking News bar on the homepage. Additionally, during Hack Week, I built the backend of a book app to demonstrate the functionalities of WebSockets when integrated into the BuzzFeed infrastructure. Why BuzzFeed? I never really considered myself a BuzzFeed user until recently, when I realized I actually view tons of content originating from BuzzFeed across various social platforms! BuzzFeed delivers platform-specific content to the apps where its audience already spends their time, and then collects data to create and distribute even more viral content. It’s a continuous cycle of building, learning, and improving. How cool is that?! TL;DR: It’s an exciting company doing exciting things and I wanted to be part of it. What’s been the highlight of your BuzzFeed experience? Contributing code that millions of users interact with is really exciting! I can say, “Yes, I did help with that feature! 😎” Also, diving head-first into unfamiliar technology was challenging, but it was incredibly rewarding to work with intelligent programmers. Not only did I get advice on how to code, but I also got to see how others approach problems through pair programming. Barnard College, 2018 @lazhangna What did you work on this summer? I worked on a tool called the Ad Audience Generator, which creates custom audiences, or groups of users that our ad partners want to target, who have consumed different kinds of BuzzFeed content on a particular platform e.g. users who have watched at least three seconds of any video posted on one of our Tasty Facebook pages. Hopefully, it will save the BuzzFeed employees who currently create and maintain such audiences manually a lot of time. Why BuzzFeed? BuzzFeed has a really well organized internship program — I knew that I’d have a formal mentor and work on a project that would make it into production. The culture was also a plus! They hired me even though I talked about the Remy Ma vs. Nicki Minaj beef in my cover letter. What advice would you give yourself, if you were starting again? The best part of this internship was having a boss who trusted that I knew what I was doing — my advice would be to trust myself as much as others do. University of Minnesota — Twin Cities, 2018 @acucak What did you work on this summer? I worked on the Tasty iOS app. When I got to BuzzFeed, the app still needed a ton of work before launching. I added different UI elements, made design and bug fixes, and implemented some new features. Why BuzzFeed? BuzzFeed’s work environment and broad social reach have made it a rewarding place to intern. Millions of people use BuzzFeed every day, so even contributing to the smallest feature will result in a big impact. Combine that with the fun, well-balanced company culture, and working here was a no-brainer for me. What’s been the highlight of your BuzzFeed experience? The Tasty app launch day was so much fun! Popping bottles of champagne, eating cupcakes, and watching Twitter explode with people raving about how excited they are for the app. Stanford University, 2018 What did you work on this summer? I was a Product Management Intern on the Apps team, working on a project to make the BuzzFeed Android app Chromecast-compatible. I also ran an experimental A/B test on increasing engagement with content in a low-performing section of the BuzzFeed iOS app. Why BuzzFeed? BuzzFeed is revolutionizing how we discover and share information, and continually providing new ways for us to connect and relate to each other. This summer I had the opportunity to see firsthand the data-driven way in which BuzzFeed’s digital products are developed. As an added bonus, I’ve also gotten an inside look at what goes into creating the content that I love. What’s been the highlight of your BuzzFeed experience? Hack Week was an awesome opportunity to collaborate with new people. Our project was a content concierge service designed to customize content flow based on how a user is feeling. Getting to see our product vision come to life in 5 short days, and presenting a demo of it to all of BuzzFeed Tech, was definitely one of the highlights of my summer. Duke University, 2019 What did you work on this summer? An internal custom notifications service! Soon, video producers will get to choose to be notified when their content is published, scheduled for release, or published. They will also be able to customize which Slack channels are alerted and filter by criteria like language, region, and franchise. Why BuzzFeed? BuzzFeed is using technology to make a huge social impact, and I wanted to be a part of that effort. Also, as an avid fan of the site, it’s been really cool for me to see how tech contributes to the content I consume. I wanted an internship experience where I feel integrated into the team I’m working with and can see my contributions being put to use. I’ve definitely gotten that at BuzzFeed. What’s been the highlight of your BuzzFeed experience? The Minneapolis and Los Angeles interns were flown out to spend a week at the New York office. It was a great opportunity to meet the people I usually work remotely with and hang out with all the interns together. I also got to see more of New York City, which is a huge plus! Cornell University at Cornell Tech, 2018 @trishalaneeraj What did you work on this summer? I built a framework which clusters BuzzFeed Quizzes based on their content and structure using unsupervised machine learning. This resulted in a consistent method for generating low-level tags for the quizzes. I then worked on creating a service that updates the current filters on the Quizzes page using the tags I produced. Aside from my main project, I also built a tool for the quiz creators which allows them to easily check the difficulty level of any Trivia Quiz based on historical data of scores obtained by users. Why BuzzFeed? I’ve always seen BuzzFeed as a creative and energetic company. I knew it had a great data science team and a wonderful work culture. It was a really simple choice! What’s been the highlight of your BuzzFeed experience? That is a very difficult question because this summer has been super exciting and I’ve learned something new every day. I found the group discussions very enriching, from Tech Talks to Data Mysteries. I think the best part of this internship was being told that my work would make it into production, and seeing so many people both within and outside of my team excited about my project. Stanford University, 2019 @btrooo What did you work on this summer? I worked on component-izing and refactoring most of the cards, bars, and other features that you see on the Feed and Article pages of the BuzzFeed website, as well as the Weaver service we use internally to organize, monitor, and build our numerous content feeds. The result was more data visibility and a streamlined workflow. Why BuzzFeed? BuzzFeed’s at the intersection of media and tech, which aligns perfectly with my interests. While some people may know BuzzFeed mainly for its funny memes and Tasty videos, I think it’s an interesting company that is in many ways driving forward what new media is and will be. I wanted to see that and explore the technology that powers it. If you could make any BuzzFeed video, what would it be? I already made this one , and it’s all I could ever ask for. Boston University, 2018 @puzzledsean What did you work on this summer? For my summer project, I added a new service for Pinterest in our distributed pipelining service, Ferret. This data pipeline gathers metrics data from the Pinterest API, which can then be served to BuzzFeed content creators so that they can see how well their content is performing on the platform. I also worked on optimizing endpoints in the Laser Tag API so that users can query for newer, more advanced filters and access data that had previously been timing out. Why BuzzFeed? As a side hobby, I enjoy making fun videos to share random things that I’m doing in life. Working at BuzzFeed seemed like the perfect fit given my creative interests and desire to grow as a developer. It’s been a great experience learning about many of the frameworks and architectures used by BuzzFeed to maintain and keep track of its content. If you were a Tasty recipe, which would you be? Emoji french fries :) University of Washington, 2018 @derrickhho What did you work on this summer? For the first half of my internship, I worked on designing an internal tool to help facilitate performance feedback to BuzzFeed employees. And for the second half, I worked on the Site Experience team, exploring how and why people share quizzes and making design improvements to this experience. Why BuzzFeed? BuzzFeed has played a big role in shaping the design community as a whole. Coming from a different background, I learned a lot from BuzzFeed’s contributions to design, and jumped at the opportunity to work alongside this team. The culture here is also insanely diverse, and I think it is important for a designer to work in such an environment. If you were a Tasty recipe, which would you be? Banana Bread Ice Cream Cake . I don’t like bananas, and I don’t like dairy. But I love banana bread, and ice cream. Columbia University, 2017 What did you work on this summer? This summer I was an intern on the Shopping team! I worked on a couple projects. I built a new ad unit that is part of an experiment to bring more options for product advertising to BuzzFeed, while still maintaining the BuzzFeed brand. I also built an internal tool for the Marketing and Data teams to set up notifications for various metrics on BuzzFeed posts. Why BuzzFeed? The tech team culture at BuzzFeed is honestly one of the best you’ll find at any company. There is a really solid representation of women and non-binary people, and the environment was incredibly inclusive. Everyone is so friendly and cool (and they dress really well)! If you were a Tasty recipe, which would you be? Realistically, a 30-second video of someone putting a pan of water on the stove and then forgetting about it. Georgia Tech CS Ph.D., 2020 What did you work on this summer? I worked on developing a human-machine collaboration service which informs BuzzFeed’s content distribution strategy on Facebook. I built machine learning models which will provide post-publishing recommendations for the social distribution team. This includes suggesting BuzzFeed links which would perform best on certain Facebook pages, and identifying which pages to post on for the most content shares. Why BuzzFeed? DATA! BuzzFeed has access to massive amounts of cross-platform data on how people interact online. The opportunity to engage in applied data science research that has the capacity to impact the experiences of millions of users was a major reason I was drawn to Data Science at BuzzFeed. If you were a Tasty recipe, which would you be? Fidget Spinner Cookies . What did you work on this summer? This summer, I built out an application that allows BuzzFeed employees to create nicer looking internal marketing emails, with the goal of increasing the overall readability of these emails. I also contributed to the company’s internal dashboard, used by content creators and their managers to view the performance metrics of their content, by creating a feature that highlights new additions to this dashboard. Why BuzzFeed? Having worked at a smaller agency before this summer, I wanted the opportunity to move into the world of product creation, as well as a larger-company environment. At BuzzFeed, I was able to contribute to a product I personally enjoy using while also exploring technologies that I didn’t have any prior production-level experience in. What’s been the highlight of your BuzzFeed experience? Hack Week!! For this Hack Week, I joined forces with two junior engineers and created BuzzFeed & Chill, our idea of what the BuzzFeed homepage would look like in the worlds of our favorite TV shows. See it here . Prime Digital Academy, 2017 @rukiaasm What did you work on this summer? For my summer project, I worked on a user management feature for PubHub. PubHub is a tool for social curators and editors to publish videos to BuzzFeed’s distributed platforms from one place. I worked on a feature that allows users to be easily added and removed without a production deploy. I did this by creating a Users endpoint in the PubHub API and connected it to PubHub UI. I’m so glad I got to work with amazing and talented team members in the MN, LA, and NY offices, from whom I learned so much! Why BuzzFeed? I have a background in international relations, so I’ve always wanted to do work that impacts people on an international scale. I really love the variety of content BuzzFeed produces, from hilarious quizzes and great Tasty recipes to serious news stories. I was also really attracted to BuzzFeed’s work culture, how nice and supportive everyone is, and how open they are to new ideas! If you were a Tasty recipe, which would you be? ANYTHING AND EVERYTHING AVOCADO! My obsession with avocados is unreal! What did you work on this summer? I designed a Tasty Skill for the Amazon Alexa Show device! It was such a rad project — I learned a ton about voice user interface (VUI) and graphical user interfaces and how they connect with each other in the context of cooking. I worked with a small team of 4 and had a lot of autonomy over what I contributed, which is pretty awesome as an intern. Why BuzzFeed? The product design team at BuzzFeed is very talented and I’m constantly impressed by the work they produce. They are extremely transparent with their process, very agile, and super friendly. I had been following and reading about the team in the BuzzFeed Design blog for a while, so when the intern opportunity came up it was a no-brainer! If you were a Tasty recipe, which would you be? Without hesitation, Jiggly Fluffy Japanese Cheesecake . Rutgers University — New Brunswick, 2017 What did you work on this summer? This summer I got to work with the site Infrastructure team! While working with site infrastructure, my project was a backend service designed to cache the most recently viewed and uploaded content to BuzzFeed.com. In the event that BuzzFeed.com were to go down, incoming requests would be directed to this cache service, which would respond with the most relevant content at any given time. What made this project so cool was that it was essentially a cache that hosted BuzzFeed’s minimal viable product. Why BuzzFeed? When I confirmed my internship with BuzzFeed, I wasn’t completely clear as to why I wanted to work there. While I couldn’t pinpoint the one thing that made me sign on in the beginning, I know now that it was a combination of my expectations, the work culture, and the sense of camaraderie I felt while interviewing that made me say “yes.” BuzzFeed as a company is very transparent about its motives, and the culture is designed around doing great work and having fun. After experiencing all the dimensions of BuzzFeed, my expectations have been more than met. What’s been the highlight of your BuzzFeed experience? Without a doubt, Hack Week. During Hack Week, I got to design and develop a retro easter egg that was approved to go live on the main site. This experience showed me how BuzzFeed stands apart from other internet companies by being open to new ideas! What did you work on this summer? I worked on the recommender system for articles on the website. This system tries to predict the articles that the users are likely to be interested in, and suggests that content that they are more likely to want to read. Why BuzzFeed? I’m interested in algorithms which increase user feedback, and BuzzFeed has a lot of data on that! Coming from a technical background, I also found it refreshing to take in the practical and user-oriented perspectives of people in other departments. What’s been the highlight of your BuzzFeed experience? I enjoyed having technical discussions with my mentor and seeing how our experimental results revealed the structures in the data. I also loved going to talks with journalists and hearing them describe their experiences. Moreover, what has impressed me the most is the spirit of learning and experimentation here. Many people here have had a career change, and are continuously learning as they work, and I find that courageous and inspiring. @Charlyn.Buchanan What did you work on this summer? This summer, I worked on adding the ChromeCast feature to the BuzzFeed app. This feature will enable users with a ChromeCast to stream BuzzFeed video content on their TV! It essentially turns a user’s phone into a powerful yet familiar custom remote, which can be used to bring the BuzzFeed experience to real life in a new and immersive way. I got to dive into the intricate BuzzFeed codebase, work with new tools and libraries, and create a user-facing product that enhances the BuzzFeed experience! Why BuzzFeed? BuzzFeed has been a long-time staple in my media diet. The relatable, current content always kept me coming back. I jumped at the chance to to contribute to a company that I engage with regularly. How often do you get the chance to work on an app you already love and work with people who are genuinely excited about what they’re making? Best. Summer. Ever. If you were a Tasty recipe, which would you be? If I were a Tasty recipe, I’d be Honey Lime Sriracha Chicken Poppers. Because BAM!! Check out some other posts from our interns this past summer! medium.com tech.buzzfeed.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 73 Internships Tech BuzzFeed Buzzfeed Posts Buzzfeed Tech Jobs 73 claps 73 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-25"},
{"website": "Buzzfeed", "title": "5 things ive learned as buzzfeed s first product management intern", "author": ["Chloe Rosen"], "link": "https://tech.buzzfeed.com/5-things-ive-learned-as-buzzfeed-s-first-product-management-intern-e7e2f32f43d", "abstract": "Latest Posts Events Apply To BuzzFeed This summer I joined BuzzFeed’s Product Team as their first Product Management intern. At BuzzFeed, PMs are responsible for working hand-in-hand with engineers and designers to create delightful, needs-based products that are changing how we discover and consume content online. When I found out I would be interning with Apps for the summer, I was so excited for the opportunity to join a team building products that millions of people use every day. Three months later, I finish my internship having learned an incredible amount about what it’s like to be a PM at a high-growth company. Here are my top 5 takeaways from my 10 weeks in Product: Data drives so much of what happens at BuzzFeed, and nowhere is that manifested more than in the development of our digital products. For user-facing products, data is like a window into the minds of users. At BuzzFeed, making changes that directly impact our user-base is an ongoing process of testing, building, testing, building, all the while carefully measuring and analyzing the results. In my internship, I got the chance to work with data closely. I ran two A/B tests, both targeting ways to increase user engagement with the feed of posts at the bottom of articles. I wanted to learn whether the low-performance stats we were seeing meant users didn’t find the content relevant, or if they simply weren’t conditioned to check this feed. With intricate tracking already in place to measure the metrics we were looking to move, answering this question was a matter of designing the right experiments to isolate and test both hypotheses. One of my biggest takeaways from the summer was how valuable it can be to see tangible, measured results of goal-driven experiments. I came into my first week at BuzzFeed as a native iOS user, having never touched an Android device. My manager presented me with an Android phone and a Chromecast, encouraging me to spend time on product discovery before diving into my main summer project of making the BuzzFeed Android app Chromecast-compatible. With both the products in my hands, I spent days studying and documenting every little detail I could about how the two devices interacted with each other. I then put together a competitive analysis, documenting and analyzing other Chromecast-compatible apps. Seeing other product managers’ choices — and recognizing the elements I thought did and didn’t work from a user’s perspective — was an instrumental part of how I scoped the project down, prioritizing the most important elements for the first version. Understanding what would make this feature successful on a system that wasn’t native to me required me to empathize with the potential user base. I learned the value of qualitative data: designing and conducting a set of user research sessions gave me the opportunity to see the feature in users’ hands, hear real-time feedback on how they think about using Chromecast, and ultimately ended up informing my decision on several UX decisions I had been struggling with. One of the highlights of my experience this summer was definitely the people I worked with day-to-day. BuzzFeed is known for having an incredible company culture — it really is the most fun, happiest place to work — and I got to collaborate closely with so many different and talented professionals: engineers, data scientists and product designers (among others). I partnered closely with Charlyn, the Android engineering intern, to execute the Chromecast project together. I learned that a critical part of being a good PM is spending time getting to know cross-functional team members and collaborating in the process of developing the vision for the feature. Interfacing with Charlyn every step of the way to understand our choices’ technical implications turned out to be a powerful asset when it came time to build. Over the course of building out the functionality for our Chromecast project, there were unexpected issues that arose. Working within the timeline of 10 weeks was challenging, and one of the greatest skills I enhanced this summer was learning to adapt and tweak details of the MVP as different issues became apparent throughout the development process, all while keeping our overarching end-goal priorities for the feature in mind. Understanding how to work through unforeseen complications provided an awesome opportunity to hone my problem-solving skills and learn even more about the technical process of developing a mobile product. One of the biggest sources of knowledge this summer was the people around me. I learned that the best way of tapping into the incredible information base contained in the pods of desks to my left and right was simply to reach out and ask questions. Being totally immersed in one feature experience provided the most wonderful opportunity to get my hands dirty, but I also loved learning more about other elements of both apps that I wasn’t working on directly. Our daily stand-ups for iOS and Android allowed me to fully understand the direction in which the whole Apps team was moving. Seeing many different pieces of a project come together and exciting features come to life as the stand-ups progressed was pretty cool. During my internship, there was always some new challenge, some new question to consider, and I was constantly pushed to think more critically about decisions I had already made, investigate options more thoroughly, communicate and collaborate more effectively. I leave my internship with a tangible feature I can say I helped implement, along with so much more perspective on what it’s like to be a PM day-to-day. These past 10 weeks have given me a glimpse into how it feels to live and breathe a product that has real impact, and I’m so excited to keep learning more. Check out some other posts from our interns this past summer! medium.com medium.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 269 Product Management Internships BuzzFeed Buzzfeed Posts 269 claps 269 Written by Chloe Rosen is a senior at Stanford University studying Symbolic Systems. Sharing our experiences & discoveries for the betterment of all! Written by Chloe Rosen is a senior at Stanford University studying Symbolic Systems. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-26"},
{"website": "Buzzfeed", "title": "women in tech kicked off the summer with a soiree", "author": ["Caroline Amaba"], "link": "https://tech.buzzfeed.com/women-in-tech-kicked-off-the-summer-with-a-soiree-86620ff983d4", "abstract": "Latest Posts Events Apply To BuzzFeed June 21, 2017 was the Summer Solstice, and what better time than to get the warm weather started with meeting the women in tech community and having some drinks out on a rooftop? BuzzFeed cares about diversity in tech and connecting women in the tech community with each other, so BuzzFeed Tech hosted a Women in Tech happy hour to network and make friends. About 175 women and non-binary folk attended the soiree on the newly-opened rooftop space in the BuzzFeed HQ office in New York. While there was a small hint of possible rain, the sun came through and the weather was great! Johnny Walker sponsored the event with some great summery cocktails. Sweet BuzzFeed swag and amazing party baskets from Johnny Walker were raffled off throughout the evening. Bingo boards with relevant tech-related icebreakers encouraged everyone to mingle and network. Everyone enjoyed the drinks and food, as well as the chance to soak in some of the summer sun. It also just so happened to be #NationalSelfieDay, so you bet that selfies were taken! Everyone from engineers to project managers to designers came out for the soiree. There were even a few people that were visiting New York City who also made it out to the event. It was a great, casual evening, and it was exciting to have everyone there! We’re planning on having more seasonal Women in Tech events at BuzzFeed soon! Be sure to follow us on Twitter @BuzzFeedExp for updates on tech events like this and more in the future! Sharing our experiences & discoveries for the betterment of… 3 Thanks to Will McCutchen . Women In Tech BuzzFeed Buzzfeed Posts Buzzfeed Events 3 claps 3 Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-02"},
{"website": "Buzzfeed", "title": "when your colleagues are your customers 8 tips for pms building internal tools", "author": ["Alice DuBois"], "link": "https://tech.buzzfeed.com/when-your-colleagues-are-your-customers-8-tips-for-pms-building-internal-tools-7ecd90c6579d", "abstract": "Latest Posts Events Apply To BuzzFeed In four and a half years in product management at BuzzFeed, I’ve spent time working on both consumer-facing products and internal tools. Both are fun and challenging, but it’s building internal tools that really makes my heart sing. Here are some tips that I’ve learned from building software for the people who create, edit and distribute BuzzFeed content. When you build internal tools, you are in the fortunate position of having users who are also colleagues. Most product managers don’t have this luxury. Take advantage of it! When I send an email or wait to hear from users, the feedback comes in a trickle. When I hold “office hours” in their space and cajole people to come talk to me, the quantity of feedback increases. When I sit with someone and watch them work, that’s when the feedback comes pouring out. Set aside time to shadow users. Watching people do their work is the best way to make sure that you really, truly understand the ins and outs of their job. A deep understanding of your stakeholders’ workflows can help you nail the details of an implementation. It’s also a great way to identify easy, small changes you can make that will make people’s day. It can be great for your team to mix in some simple quick wins that users will love, even if they won’t really move the needle on team or company KPIs. You can earn a lot of appreciation and goodwill from users by taking something that used to be tedious and painful and making it easy. And it can be fun for the team to crank out a few fairly easy features that people go bananas for. In one sprint, an engineer in our production tools group built a dropzone that automagically parses the track title and track ID of stock audio files from the Warner Chappell catalog so that video producers didn’t have to type the info in manually — some long videos might have well over a dozen different audio tracks. It’s not as if we thought this feature would blow the doors off of productivity and 3x the number of videos a person could make. But manual data entry is booooooooorrrrring. We’re thrilled when we find fairly simple way that we can automate a tedious task and make people a little happier at work. When we demoed this feature for producers, they screamed very loud and I’m pretty sure one guy shouted “HASHTAG BLESSED!” It was a delight. The key is that you can’t let these quick wins take over the whole roadmap. They should be projects that you know will be simple from a design and engineering perspective and very popular with the people using the tool. Even as early as the planning phase for a feature or product, I imagine myself writing the launch email or running an onboarding session. It’s pretty easy to assess whether I’m going to be excited to tell people about the amazing new thing they are getting or I’m going to be stressed about rolling out some new thing that they are going to see as a hassle or an imposition. In my head, I imagine every person using a tool as a little ball. For the launch, you need to move the all the balls from Point A (their existing tool or workflow) to Point B (the new version you are going to give them). If the new thing is way better and easier, then it’s like letting balls roll downhill. It really doesn’t take much effort to get them to where you want them to be. You build a good thing, you train them how to use the good thing and they clearly recognize the benefit of the good thing. They embrace it and take off. The better the thing, the steeper the downslope. A very good sign is if people you haven’t even onboarded yet are using or asking to use what you’ve built. If the new thing is a pain in the neck, then it’s like moving the balls up a hill. It will take a huge amount of effort from you to push, encourage, cajole or berate everyone until they are at the top. If you get tired and take a break, you will look back and realize everyone has rolled back to the bottom of the hill because they do not like Point B and want to remain in Point A. The worse the new thing, the steeper the upslope. A very steep upslope is not fun. Ask Sisyphus. It’s important to make clear that there are totally valid reasons to launch something that you know is going to be an imposition on the users. Maybe you need additional metadata that will allow your tools to do some amazing powerful new thing. Maybe you need to fix an essential security issue. There are any number of good reasons to work on a feature that won’t be received with great joy by the people who use your tools. A sad-but-true fact is that many many people don’t really care all that much about features / workflows / requirements that are good for “BuzzFeed” or good for some other team or department. They care about features that have an obvious benefit to themselves. So try to figure out some way that the launch makes their life a little better so you aren’t trying to roll 200 balls up a steep hill. You’ll often find that some subset of people who use your products have lots of ideas about how to make them better. These product-minded users aren’t necessarily your key stakeholders but they’re incredibly helpful people to have close relationships with. They are often willing to give you feedback on early designs, to be beta users for a new feature or to share their thoughts on prioritization. Seek out these product-minded humans then ply them with treats and shower them with compliments until they become your friend. If you realize that a large set of your users are using your tool in some odd way that isn’t what they should be doing, pay attention. There is some unfulfilled need that they are inventing a solution for. If you figure out the problem that they are trying to solve with their hack, you’ll wind up with ideas for the product roadmap. If a stakeholder asks for something and you aren’t going to put it on your roadmap, don’t nod your head and say “mmmmm” and then disappear. It’s hard to say no. It can be tempting to be vague, leaving stakeholders with the impression that you’re going to work on their request. This might be less unpleasant in the short term but it’s bad for your relationship in the long term. If you aren’t going to prioritize something they want, you should tell them in a straightforward way why you won’t. It’s useful if your stakeholders already have a sense of what you are working on and why, so you can explain how this new idea isn’t higher priority and your team doesn’t have time for it at the moment. Users are like baby ducks. They imprint on the person who teaches them about a tool and that’s who they go to with questions and feedback. You want those baby ducks to imprint on you or people on your team so that you hear all of their questions and feedback! Your team should own launch announcements, no question. You should also be a part of onboarding or training where possible. Baby duck users also imprint on workflows. It’s worth teaching people how to do something properly, because information about how to use a product is often shared casually peer-to-peer from current employees to new hires. You want to make sure that if a baby duck turns to their neighbor (an older, more experienced duck) and asks a question about how to use the product, the older duck gives them the answer you want them to give and doesn’t give them some bad info that is not, in fact, how anyone should be using the tool. Teach those baby ducks right and they will grow up to pass on their good habits to future generations of ducks. This is an exaggeration. Some small percentage of people are goody-goodies who read their emails and follow instructions and click through to the awesome user guide you created. But a lot of people are never going to open all the careful documentation you have prepared. So make the tool itself simple to use and hard to mess up. It’s also easier to teach the baby ducks if you don’t have to sit them down for a two-hour tutorial where you explain the nuanced differences between Option A and Option B and Option C and Option D and Option E because they probably do not care and won’t remember. Have you built internal tools for your company? What tips and strategies do you have for building first-class products for your colleagues? By the way, BuzzFeed Product is hiring ! Come work with our amazing team. If you live in LA and want to talk in person, send me an email! alice.dubois@buzzfeed.com Sharing our experiences & discoveries for the betterment of… 576 3 Product Management BuzzFeed Media Tech Buzzfeed Posts 576 claps 576 3 Written by Director of Product at BuzzFeed. Sharing our experiences & discoveries for the betterment of all! Written by Director of Product at BuzzFeed. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-07"},
{"website": "Buzzfeed", "title": "buzzfeed tech summer 18", "author": ["Bryan Hughes"], "link": "https://tech.buzzfeed.com/buzzfeed-tech-summer-18-71724240e3d3", "abstract": "Latest Posts Events Apply To BuzzFeed It’s that time of the year again! Many of you reading this are already contemplating which internships you’d like to pursue for the Summer of 2018. Contemplate no more! Make BuzzFeed Tech your home this summer and join us in our quest to deliver news, impactful stories, videos, recipes (Tasty!) and entertainment to an audience of billions. We’re offering internships within Engineering, Product Management, Data Science and Product Design. Our program is ten weeks long with positions available in Los Angeles, NYC, Minneapolis and London. Meet last year’s talented bunch of interns and read about their experience as BuzzFeeders below! tech.buzzfeed.com tech.buzzfeed.com www.buzzfeed.com Apply to our Software Engineering internship in NYC here Apply to our Software Engineering internship in Los Angeles, CA here Apply to our iOS internship here (Minneapolis, MN) Apply to our Android internship here (Minneapolis, MN) Apply to our Data Scientist internship here (NYC or Los Angeles, CA) And if you’re around, come see us at: Grace Hopper Celebration at the Orange County Convention Center in Orlando, October 4th through October 6th HackNY Hackathon at Columbia University, October 14th and 15th HackRU Hackathon at Rutgers University, October 14th and 15th NS Hackathon 2017 at Lehman College, November 10th and 11th Have a question? Ask Bryan or Nicolette: Bryan Hughes bryan.hughes@buzzfeed.com Nicolette Nelson nicolette.nelson@buzzfeed.com Sharing our experiences & discoveries for the betterment of… 52 Internships Summer Internships BuzzFeed Buzzfeed Tech Buzzfeed Tech Jobs 52 claps 52 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-08"},
{"website": "Buzzfeed", "title": "how we tagged 14 000 buzzfeed quizzes using k means clustering", "author": ["Trishala Neeraj"], "link": "https://tech.buzzfeed.com/how-we-tagged-14-000-buzzfeed-quizzes-using-k-means-clustering-95fc46bc6daf", "abstract": "Latest Posts Events Apply To BuzzFeed At BuzzFeed, we have lately been concerned with the quality of the metadata related to our Quizzes. In the interest of our users being able to discover our best content, we decided to redo the tagging system. Currently, we have an author-provided set of tags for each of the Quizzes and while they are good tags, we felt the need to automate this process for the sake of consistency. So, in the past few weeks we worked on automating the process of generating low-level consistent tags. That means we won’t just have tags like ‘Food’ or ‘Trivia’, we will also use ‘Pizza’, ‘Geography Trivia’, ‘Who Said It’, etc. After some exploratory analysis of the available data, we landed on Clustering. This blog post will be a walk-through of the technical steps involved in this task. We will start by giving a refresher on clustering and K-Means Clustering algorithm, which also includes some details of implementation, and then explain how exactly we prepared our data for clustering. Towards the end we’ll briefly touch upon how to interpret the resulting clusters. Why are we clustering? Like we mentioned earlier, we are looking for a whole new set of rather low-level tags. We don’t just want to know that a quiz was about movies, we also want to know if it was about animated movies. Since we don’t have our quizzes tagged this way already and there’s no ‘ground truth’ or labels we want our algorithm to learn from, we’re going to be looking at unsupervised machine learning algorithms. Clustering is the most popular approach among those. It is the task of assigning a set of data points or observations into subsets called clusters such that the observations within the same cluster are similar with respect to a predefined metric and the observations in different clusters are dissimilar. While there are several ways to go about clustering, we choose K-Means Clustering because it scales well to large number of samples. What is K-Means Clustering? K-Means is an iterative clustering algorithm that partitions a dataset to form coherent subsets of all data. The algorithm iterates between 2 steps — the cluster assignment step and the move centroid step. Here is an animation link that will give a better insight about the iteration process. The optimization objective of the algorithm is: This is the average squared distance between each data point and the location of the cluster centroid to which it has been assigned. c is the index to which the data point is currently assigned. This algorithm, through several iterations, tries to find c and to minimize the above cost function. For overall conceptual clarity, this video might be helpful. We used the scikit-learn implementation of K-Means which is widely used and well-documented. It is worth discussing the parameters briefly: n_clusters — the number of clusters we want, which needs to be declared upfront init — the method for initialization. ‘K-means++’ selects initial cluster centers in such a way that they are distant from each other. This allows quick convergence and better results compared to completely random initialization. max_iter — the maximum number of iterations (assign and move steps) each initialization should go through. n_initialization — the number of different initializations given, that is the number of times the algorithm will be run with different centroid seeds. Depending on initial conditions, different clusters are formed. The final results will be the best output of n_init consecutive runs in terms of inertia. random_state — a random seed for reproducibility. The remaining part of this post will focus on how to prepare the ‘data’ that the model is fit on, and how to obtain and interpret the predictions. What is the training data? We fed the algorithm on the textual content of BuzzFeed Quizzes. We decided to extract features specifically from the following title, blurb and author-provided tags. For example, for the following quiz: Title Blurb And lastly, author provided tags in the existing metadata The above is not a content-related tag, but simply an indication of where it was posted on the website. Other examples of author provided tags are ‘celebrity’, ‘movies’, etc. At a later stage we will also incorporate questions, but as of now features from questions did not appear very informative. This is mainly because the format in which questions are present in our metadata is highly varied. In the older quizzes we only had questions in the form of images instead of text, and some times text was embedded within images. Also, a huge chunk of questions did not convey much out of context. For example, ‘Pick a shoe’ or ‘Does this make you laugh?’ couldn’t really be associated with a topic or a type of quiz. They could be a part of any quiz. Now, we have all the text. We’re not ready yet! As expected, there are all sorts of abbreviations, punctuation and special characters in our quizzes, and we want to clean it up. Additionally, we’re going to perform some standard text preprocessing like removing special characters, lower-casing, removing stop words and lemmatizing. This will give us clean sequences of words. Feature Extraction We cannot input text sequences we generated above into our algorithm. Algorithms, in general, expect numerical or vector features, especially of fixed shape. So, we will represent our words as vectors, or in other words, ‘vectorize’ them. The approach for ‘vectorization’ includes tokenizing (giving an integer ID to each ‘token’ separated by space or punctuations), counting (frequency of occurrences of these tokens), and lastly normalizing (giving weights to tokens based on the frequency of occurrences). For our text data, we used term frequency-inverse document frequency (tf-idf) features which allows us to weigh terms appropriately. Words that occur too often and don’t mean much, are given less weight and vice versa. On analyzing our intermediate results, we noticed that ‘new’ from ‘New York’ and ‘Orange Is The New Black’ was not seen any differently by our algorithm due to tokenization. Thus, we decided to vectorize phrases, i.e., groups of words that co-occur often, rather than vectorizing each word independently. This was particularly useful in looking at named-entities as one token. For example, ‘New_York’, ‘Big_Brother’, ‘Harry_Potter’ etc. We have used a combination of NLTK , gensim and sklearn (all open source packages) to tokenize, detect phrases and vectorize. Below is the code snippet: The resulting tfidf_matrix is the data we fit our model on. The max_features parameter is used to set the maximum vocabulary size, i.e., it sets the dimensions of our feature space — we’re now looking at a 6000 dimensional space. So, tfidf_matrix we obtained has the dimensions number-of-quizzes x max_features . Understanding the results Next, we move onto understanding the results from fitting the model and making predictions. One of the attributes of the fit model is to be able to see the exact coordinates of cluster centers (see Figure 1). Remember the max_features parameter we briefly discussed above? We set it to 6,000. That means that we’re looking at a 6,000 dimensional space and the clusters we obtained are found in this feature space. The dimensions of the above array is 40x6,000. We can use Euclidean distance to find the words and phrases that lie closest to the cluster centers. Then we can determine the main topic of the cluster and give it a name. To do that, our first step would be to sort the index cluster centers in descending order. Then we obtain the top terms in each cluster by finding out which terms are closest to the centroids. Below is a snippet for obtaining top 60 terms in each of the cluster: If we print those terms, it would look something like this: In this cluster, the term ‘music_video’ is the closest to the cluster center of Cluster 0. ‘identify’, ‘youtube_comment’, etc. are the next few closest words. Recall that some terms are actually 2 words connected by an underscore because they co-occur very frequently. Printing the top terms is very important since that will allow us to assign a label to all of these clusters. Cluster 0 above appears to be mostly about music videos. Similarly, if we print out terms in all of the clusters we created, we can attach each cluster with a label. How many clusters? We kept this one for the last because the answer is rather unsatisfying. In all fairness, this is one of the most challenging parts about K-Means. K-Means is a computationally difficult problem ( NP-Hard ). It does converge to a local optimum fairly quickly, though there is no guarantee that the global optimum is found. We did experiment with a few methods of fixating on the number of clusters, like silhouette coefficient and elbow method , but found manual inspection to be the best. We have currently identified 40 clusters. Again, since this is an unsupervised learning method, we cannot measure performance using metrics like accuracy (we don’t have ground truth values). We asked the quiz writers to validate a small sample of quizzes from each cluster with a google form. Visualizing clusters The 40 clusters we created existed in a 6000 dimensional space. We needed to perform dimensionality reduction so that we could visualize the clusters. We used Principal Component Analysis for projecting them to 2 dimensional space. Through a zoom of the visualization we created, we would like to draw your attention to 3 clusters — ‘TV Shows’, ‘Nostalgia’ and ‘Disney’. It is easy to see how similar clusters are closer to each other. How are we exactly using these clusters? Apart from backfilling tags for all our quizzes, and periodically re-clustering to find new tags, we are planning on using these to update the filter tabs on the BuzzFeed Quizzes page. We recently included filter tabs on the page so that users can binge on their favorite types of quizzes without spending time going through the whole lot. So if you only like ‘Trivia’ Quizzes, just hit the tab and there you have it. Since that worked out really well, for our users and for us, we plan on taking it one step further, this time with automatically generated tags rather than human annotations and filtering. Our goal was to be able to update these filters periodically based on what’s trending and what’s evergreen. We particularly wanted them be very specific in nature and not too broad. While ‘Food’ is an acceptable filter, we wanted to be able to give you ‘Pizza’ as a filter. We also wanted to prioritize on filters we know a huge number of our users would be interested in. It would be useful to have a filter dedicated to ‘Game Of Thrones’, for example, but not for ‘Pretty Little Liars’. We could simply put the latter in the ‘TVShows’ filter since it’s not very popular these days, not evergreen like ‘Harry Potter’. With the new set of tags which are more fine (low-level), we can provide you with even more interesting filters like ‘Weddings’ or ‘Style And Fashion’. We can then potentially update them periodically as we discover new tags via clustering! We surely were entertained creating and analyzing these clusters and hope you all have fun using them to discover our content! Check out some other posts from our interns this past summer! medium.com medium.com To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 185 Machine Learning Data Science Buzzfeed Posts BuzzFeed 185 claps 185 Written by Machine Learning @cybcubecom, previously — graduate student @cornell_tech @cornellcis | Personal Blog: http://trishalaneeraj.github.io/ Sharing our experiences & discoveries for the betterment of all! Written by Machine Learning @cybcubecom, previously — graduate student @cornell_tech @cornellcis | Personal Blog: http://trishalaneeraj.github.io/ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-20"},
{"website": "Buzzfeed", "title": "hackweek buzzfeed and chill", "author": ["Jessie Wu"], "link": "https://tech.buzzfeed.com/hackweek-buzzfeed-and-chill-b3ec2e0ca2f2", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed Hack Week is a biannual tradition that started in summer 2015 where fried chicken delivery Slack bots, data analysis chrome extensions, and projects like Humans of BuzzFeed were created in a span of a work week. Teams for Hack Week form naturally, whether it is through word of mouth, a pitch during the weekly internal Tech Talks, or reading about the project on a spreadsheet or on Slack. Some projects are teams of one and some aren’t even projects at all — a few spent the week learning about computer networking , advanced algorithms, and machine learning. So, what did Jess Kustra, Brandon Choi, and I work on for our first ever Hack Week project? Introducing BuzzFeed & Chill — the website where your favorite TV characters go to for their daily dose of lists, quizzes, Tasty recipes, and investigative reporting. It is here where: Members of Game of Thrones ’ Night’s Watch bookmark 7 DIY Hacks To Protect Yourself From White Walkers. Pawnee residents get their political fix by reading Ron Swanson from Parks and Rec ’s 1 Reason Why The Government Matters on BuzzFeed News. (P.S. The answer is not Trump) There’s omurice. Then there’s omurice from Terrace House with a scathing message. Let Tasty walk you through this recipe in Here’s How You Should Actually Be Preparing Omurice. Ingredients: a squeeze of ketchup and a whole lotta shade. In the world of Broad City , you can find Abbi Abrams scrolling through 20 Reasons Why Bed Bath and Beyond Is A Godsend , sponsored by Bed Bath and Beyond, sitting next to a stack of expired coupons (wait, BBB coupons never expire). The most shared and talked-about content in Adventure Time was the controversial exposé on Princess Bubblegum being shady AF. The last day of Hack Week, Demo Day was a blur where we merged and deployed our last bit of changes, and saw so many creative, innovative, brilliant projects from the people of BuzzFeed Tech. Some of our favorites included projects featuring augmented reality, VR, and a Slack game to learn other BF people’s names and faces. My team and I toasted to a job well-done after a week of pair programming, taking ice cream breaks, and struggling together through what seemed like endless development error logs. Here at BuzzFeed, we have over 200 services built with different technologies and plenty of legacy code. All three of us work on internal tools, several of which are powered by the front end library React. Hack Week was the perfect opportunity for us to check out the owned-and-operated codebase of our public-facing services. What we wanted to accomplish was to React-ify the web service for BuzzFeed’s landing page. What we accomplished: We created React components for each element of the homepage and added BuzzBlocks, another web service with the shared elements and stylesheets, for the out-of-the-box landing page. After we had everything running, we each customized themes for our favorite TV series. When a TV show, let’s say Parks and Rec, is selected in the drop-down menu (a state child component) in the header, its value, parks-and-rec , bubbles up to the parent component (also a state component). That adds the associated state class attribute parks-and-rec-theme to the container div in the parent component. This classname dictates what styles and data structures of content to be applied to the components. For example, the parks-and-rec-theme selector changes the font to Times New Roman and Comic Sans, and it applies the JSON data structure in badges[parks-and-rec] to the row of badge icons. That is how you see the Pawnee government seals and fake “broken” icons in the right nav bar. Government websites: the true MVP of all websites. Our nice-to-have’s: BuzzFeed & Chill was built by us for us, so we went wild with it with an anything-goes mentality. However since we only had four and half days to complete it, naturally there were some stretch goals that weren’t implemented: Fake content redirect page — Since we didn’t have actual content or links, clicking the links only redirected to the same page. It would have been fun to have the fake titles redirect to a HTTP status page that said “error 204 No Content: Fake News! This content does not actually exist.” Loading indicator — While React is pretty fast when updating the page based on state change, there is still an occasional lag, for example Parks and Rec badges would slowly transition out of the Game of Thrones theme. A loading bar would prevent the users from seeing this overlap of the two themes. Pop-up Easter egg: BuzzFeed production celebrated Pride Week this year by adding Babadook and rainbow confetti as an Easter egg on all BuzzFeed LGBT posts. We wanted to add a similar feature where a character from the selected TV series would randomly pop up. What we wanted to learn: The first day of Hack Week, we shared things we wanted to learn. Collectively, we wanted to understand how BuzzFeed.com worked but we each had different things we wanted to hone in on. For Brandon, he wanted to learn React. For Jess, it was to be more hands-on with product and architecture design. For me personally, it was to learn how to lead a team. What we actually learned: Brandon was able to pick up React really quickly. After creating the first component and walking through how the state worked together as a group, he was able to add a season premiere countdown Timer component based on the Game of Thrones state theme. Pretty impressive! The best part was right after Hack Week ended, he went straight to applying what he learned from BuzzFeed & Chill to his team’s current projects in React. Another thing we learned a lot more — a LOT more — than we had anticipated was the architecture and flow of how our services talked to each other. Jess’s previous experience working for an agency where she had to start from scratch and spin up services for new projects was really helpful. She had a solid, intuitive grasp on the role of webpack in web apps and knew which areas to target. As for myself, I learned being a leader doesn’t necessarily mean being an expert in every running part of software. A good leader asks the right questions to see the big picture, strategizes the most efficient approaches, and keeps their team engaged and motivated. However, my biggest takeaway was accepting the fact that mistakes will be made along the way, and that it’s not a bad thing. Making mistakes means recognizing the problem and making decisions on how to fix it, and learning from mistakes ultimately translates to gaining more experience. Together, we learned the hard way: The pros and cons between using the create-react-app library versus cloning an existing service as a front end template. We did both, each which had its own set of problems. Cloning proved to be less painful in the end. Adding an auth proxy in order to make it go live with our PaaS deployment system rig . We tried, gave up, tried again. Getting webpack to run nicely with our private node modules. We went to hell and back — let’s just leave it at that. It was also during this time that Brandon learned about my cravings and bad eating habits. Hey, nothing is wrong with eating Chipotle’s new queso and Panda Express at 7am. What we struggled with: We struggled in the first half of the week before enjoying the journey (which is quite the opposite of eating Panda Express). The first two days were brutal when we were trying to get everything set up. We’d start the day motivated, with eyes bright and high hopes. By 3pm, our energy level would plunge, and we would constantly look at each other with mopey eyes as our console rolled through a laundry list of red ERR! errors. We wondered if we’d have anything to show on Hack Week. Alas, everything fell into place after we fixed our OAuth upstream issues and corrected the paths for our webpack. One unexpected non-technical struggle was realizing how hard it was to come up with titles for fake content. BuzzFeed has a certain unspoken algorithm for click-worthy titles that Jess started catching on to after looking at past BuzzFeed content. It essentially became a MadLib formula where she could just replace the nouns and verbs. Boom! Catchy Title With Cool Slang Reference With Maybe A Subtle Joke is generated. I reached out to our #terrace-house Slack channel for help in coming up with titles. A data scientist suggested “9 Things To Do When You Find Out Your Roommates Are Lowkey Hooking Up.” One person from Editorial was spitting out titles left and right like a generator. He turned my “The Meat Crime” into “Sometimes I Guess It’s Okay To Cry Over Meat.” It is moments like these that remind you you’re working alongside brilliant individuals, and colleagues who are willing to spend a few minutes of their schedule to help you relive the best moments of your favorite Japanese reality TV show. Quite important use of company time TBH. What we enjoyed: Even when we were stuck, we were having fun. Getting lunch together each day made me realize how often I ate in front of the computer and worked through lunch. Another take-away I had was watching how others tackle obstacles. Jess and Brandon are the type of people who are determined in cracking the problem by hammering possible solutions left and right until it works. On the other hand, I am the type that needs to step away and take a break from the problem by working on something more trivial that can quickly produce results, and then returning to the original problem with a renewed sense of determination. Each approach is effective in its own ways. What I really enjoyed was finding relevant GIFs, googling most iconic scenes for fake content creation, and having a pretty valid excuse to rewatch old episodes of our favorite TV series. It’s for the sake of research after all. Our day-to-day work life: Our New York BuzzFeed office is designed with plenty of open space for people to float around and collaborate, and is divided by departments on each floor. Normally engineering teams have sprint planning meetings and weekly stand-ups to share updates, pick up tickets to work on, and collaborate on any correlated tasks. For Learning Tools team’s sprint planning, we share wins and fails before going through tasks. This is great because it allows us to celebrate small wins together and provides an opportunity to share any additional context or clarify misunderstanding in failures. After that, we dive into our Jira sprint board to give updates on in-progress tasks while creating and assigning new tickets. Hack Week work life: For Hack Week, we opted for an IRL sprint board using some cards and good ol’ scotch tape. There’s nothing like the satisfaction of physically moving cards from in-progress to done, old school style. Each day we took turns leading the meetings, giving everyone an opportunity to come up with daily check-in questions and planning the day. Having a retrospective session each morning helped us reflect on what could have gone better and think about how we’d structure our day. Having daily standups and pair programming allowed us to grow together as a team. Pair programming was something Jess wanted to try since that is something that rarely happens at BuzzFeed. Typically we work independently and collaboration only happens through Slack, meetings, or when someone is stuck on something. For BuzzFeed & Chill, pair programming (triplet programming in our case?) half of the day allowed us to learn from each other and see how others worked. It also created great camaraderie among the team because we struggled and found the light at the end of the tunnel together. Many things came out of Hack Week — creative experimenting, new feature ideas, learning new things, and consequently, a sandbox for a couple of junior-level engineers to practice leadership, break things without consequences, and intersect things they love: BuzzFeed, coding, and Game of Thrones. Special thanks to: @ddunlop @taylorbaldwin @raymond @clineamb #rig #buzzblocks #nyc-leftovers Google for all our images To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 8 JavaScript BuzzFeed React Hackathon Buzzfeed Posts 8 claps 8 Written by Software Engineer at the New York Times, previously at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer at the New York Times, previously at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-03"},
{"website": "Buzzfeed", "title": "computer networking made simple", "author": ["Angie Ramirez"], "link": "https://tech.buzzfeed.com/computer-networking-made-simple-81b592d7bd5e", "abstract": "Latest Posts Events Apply To BuzzFeed For BuzzFeed Hack Week this year, a week in which all of BuzzFeed Tech puts down their day-to-day responsibilities to work on a project of their choosing, I took a computer networking class through Udacity. BuzzFeed Hack Week is not limited to “hacking” or building a project using code, design software, etc., but can also be an exploration into a topic you might want to learn about. Below is what I learned. H ow does the internet work? How does it actually transmit information from one side of the world to another? How does it know where information is coming from, and where it should be sending the information? How does the internet recover from an overloaded of requests? If you’re like most people, you type in a page address into your browser, wait for the page to load, and if it doesn’t, you connect and disconnect to the internet, and try again. Have you ever asked yourself how the address “facebook.com” gets sent from your computer to the internet, how facebook.com receives the request, and how it sends the facebook.com homepage to your computer? If so, read on. For starters, you should know, the internet is controlled by the Transmission Control Protocol/Internet Protocol (TCP/IP), a set of rules that tell computers or other devices how to packet, address, send, and receive information. After you type in “facebook.com” to your browser, a couple of requirements have to be fulfilled for the request to go through: Just like when sending an envelope through the United States postal service (USPS), when your computer tells the internet where it would like to request information from and send information to, it has to do so using TCP/IP rules. Just like USPS would not send your envelope if the send address was missing, placed in an unexpected side of the envelope, or written without a destination city, zip code, etc., TCP/IP rules state that in order to deliver “envelopes” or packets of information to and from different locations, it needs to know their IP addresses (you might have seen an IP address before, a number that looks something like this 208.80.154.224). An IP address is a set of numbers separated by four dots, that act like your “driver’s license” number on the internet, an official identification number that is unique to your device. We’ll talk about how this number came to be later. In addition to the IP address, TCP/IP requires that a request or a packet have packet headers to tell it what to do. These packet headers do include the source/destination IP address, but also how to deliver the packet of information. For example, USPS handles a packet containing fine china labelled ‘fragile’ very differently than a packet containing sturdy reams of white paper; the same is so for different kinds of data transmitted over a network, like ‘image’ or ‘text’, to name a few. The World Wide Web, email, and file transfers rely on TCP/IP. Though there is another protocol used for connections that are not streaming data, the User Datagram Protocol (UDP) , we will focus on TCP/IP here, the most widely used protocol in the transport layer of the Internet Protocol Suite (other layers include the link layer, the internet layer, and the application layer). In order for USPS to deliver a package, there must be a mailbox, a porch, or even a package delivery door in a large building, some sort of entrance, for it to delivery the package through. Interestingly, your computer has 65,536 available “doors” or ports that it can open at any given time. Why 65,536 you ask? Seems like a random number, but it’s one that’s linked to the “size” of the field on the “packet header” that’s carrying that “door” or port information. “Size” or “length” when you’re writing a paper, for example, is measured in letters or characters, but in programming, the length of information is measured in bytes (a byte is equivalent to eight bits, or an octet as computer scientists call it, and each bit is a binary digit , a 1 or a 0…yes like you’ve seen in the matrix). To put it in perspective, TCP/IP packet headers can be anywhere from 20–60 bytes, which means, 160–480 individual bits, 0’s or 1’s. The space for a port number is only 16 bits (and remember, each of these either a 0 or a 1). Try this short mental exercise: if you only had 0’s or 1’s to tell someone that there were 20 people in the room with you at an event, how would you do it? What sort of system would you invent with these 0’s and 1’s, to make sure the other person understood that there were 20 ppl in the room? You’re not allowed to use sticks, draw the number of people, or speak in any way. The answer is, you’d probably come up with a secret code that only your friend understood. You might choose to say that the number 1 will be represented by “1”, the number 2 by “10”, the number 3 by “11”, the number 4 by “100”, so on and so forth (by the way, this is actually what these numbers in binary code represent!). To fully answer the question above, then, there are 65,535 available “doors” or ports on your computer because that is the highest number that can be represented by a 16-bit or 2 byte binary number (if you’re into math, since every bit can either be a 0 or a 1, one of two numbers, and if there are 16 bits allowed, 2¹⁶ would give you the total number of ports available, and, since in computer science we start counting at zero, the highest number port would be 65,535 not 65,536). IP addresses, the numbers that tell a network where to find your computer (described in number 1 above), or where to send packets of information, have similar length constraints. They can be a maximum of 32 bits long, which means about 4.3 billion possibilities (this system is called IPv4 ). That’s 2³² possibilities! S o, we need an IP address, packet headers, and a port to actually send and receive information. But how the heck does the internet handle so many requests at once? Imagine that a billion packets were sent to be shipped at the post office on a given day, but there were only enough trucks, planes, and cars to deliver 10 million packets a day. How long would it take the post office to deliver all of the packets? 100 days. That’s a long time. Maybe the post office would enlist twice as many trucks, drivers, and charter more planes to handle the increase in packages being sent. That, however, would take a long time to do — USPS would have to hire a lot of people. The internet works in a similar fashion, though when it’s busy, it does something USPS would probably never do (at least not on purpose) — it “drops” packages and does not deliver them. This is called TCP congestion control. Imagine if for example, you have a home internet connection that can handle sending or receiving 1 million packets per hour, but the connection outside of your home can only handle 500,000 per hour, and a bottleneck of requests forms. It turns out that TCP has protections against this — TCP doesn’t actually start sending all the data requests you made at once (like “calling” facebook.com), but does so slowly at first, increasing speed only when it receives word from the place you’re looking to reach that the packets are going through at a normal speed. If there are too many packets to send, the router , a device that connects one IP network to another, will drop the packets to relieve pressure on the connection. Since the request is dropped, and no information comes back from the place or “ server ” you are trying to reach, the request will “ time out” , and give you an error. The connection between your computer and the internet will speed up again as there are less packages to take care of. Were it not for the router, the whole connection would time out, which means, no packets of information would be sent or received anymore. Did you realize above, when talking about IPv4, that there can only be 3.2 billion IPv4 addresses, yet if the world is over 7 billion people, and all were to be on the internet with their own IP addresses, not to mention on multiple devices each with an IP address, 4.3 billion is not enough combinations? If you did, pat yourself on the back — the issue agitated programmers for a while. At first, to delay the problem, programmers built something called the Network Address Translation (NAT) , a workaround to not having enough addresses left. NAT is like an office telephone system. An office, for example, has many employees, all of whom may have a phone on their desk that rings if their extension is called. Though the person on the other end of the line dials a central number, they are re-routed to a specific employee’s telephone number after entering their extension number. Just like there is a single telephone number where anyone can reach all company employees, NAT uses one public IP address to send and receive packets (every computer has it’s own “extension”).The “extension number” in a NAT is the port (explained in number 1 above) number on an individual computer that a package is either coming from or going to. The NAT (or a proxy, a similar system that works on specific HTTP requests) knows which port numbers “belong” to which computers. The NAT is in charge of re-writing the addresses on these packets, the “to” or the “from”, so that packets get to where they need to go. Though NAT is still used today in offices, home networks, and other places, it did not solve the scarcity of IP addresses. Knowing a system based on IPv4 addresses would not be feasible forever, programmers came up with a new system in 1999, the IPv6 . Instead of IP addresses being constrained to 32 bits, IPv6 uses 128 bit addresses, which allows for 2¹²⁸ addresses, more than can ever be used in the foreseeable future. A system that has 3.2 billion users (estimated by the International Telecommunication Union in 2015), however, is not easily transitioned to a new IP address system. Given that the old system and the new system are not compatible, most networks and devices made today must accept both IPv4 and IPv6 addresses. The transition, however, has proven to be a slow one. As of July 13, 2017, only about 20% of Google users access it over IPv6. As is apparent in the map below, that percentage is higher in more developed countries that can afford to purchase new devices and software. Y ou now can imagine how packets of data are sent back and forth through the internet, making sure to follow the rules of TCP/IP, and understand how the internet keeps itself from shutting down when there are too many packets being sent back and forth around the world. You understand how basic binary numbers work, and have been introduced to the ever-present IPv6 IP address transition. Hopefully you’ve been intrigued enough to take this introduction and keep exploring the wonderful world of computer networking principles. Check out more projects that we worked on during Hack Week here . To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 1 Thanks to Jessie Wu . Internet Computer Networking Tcp Computer Science Buzzfeed Posts 1 clap 1 Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-03"},
{"website": "Buzzfeed", "title": "lettuce evaluate some recipe word embeddings", "author": ["Meghan Heintz"], "link": "https://tech.buzzfeed.com/lettuce-evaluate-some-recipe-word-embeddings-64f76e61ac0c", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed Tech is bringing some presents to the Tasty 2nd Year anniversary. Instead of bringing the second cheapest bottle of champagne available, we’ll be releasing the Tasty App , so our fans can enjoy a seamless Tasty cooking experience. We want to bring the best possible Tasty experience to our loyal users, and that means helping them find recipes they’ll like more easily. Over the past two years, Tasty has produced +1700 cooking videos and sifting through them could be a chore when you’re looking for that perfect one-pan chicken dish. To improve that process for our fans, we will employ a variety of machine learning techniques to recommend recipes our users will love. But first, we needed an effective way of describing recipes so that even a computer can understand how delicious Avocado Carbonara is. Enter Word2Vec by Mikolov et al. (There’s are already from fantastic resources to learn about Word2Vec conveniently collected here . Plus, check out more papers by the creator Mikolov to learn more about the technique and I will restrain myself from trotting out the olde king + woman = queen example.) Word2Vec, developed at Google, is a model used to learn vector representations of words, otherwise known as word embeddings. Why would we care about word embeddings when dealing with recipes? Well, we need some way to convert text and categorical data into numeric machine readable variables if we want to compare one recipe with another. Traditionally, this has been accomplished a few ways. A popular technique is turning each potential category into a boolean variable, otherwise known as “ dummy coding ”. If we wanted to describe that a recipe has chicken in it, we would include a column for chicken with a 1 to indicate chicken was in the recipe. A fish recipe would have a 0 instead of 1 in the chicken column. This technique works up to a point, but each new ingredient or attribute grows your matrix by another column adding more and more sparsity. Additionally, creating these boolean variables can be cumbersome as we have to make sure to map things like 2% Greek yogurt, Greek yogurt, and 2% yoghurt all back to the same representative variable. Another method is to encode your categorical variables as a number. Unfortunately, this is arbitrary, e.g. who is to say “chicken” is 1, but “steak” is 7? You could end up with “tuna” being much closer numerically to “marshmallow” than to “salmon” since the encoding doesn’t attempt to understand the relationships of the variables to be encoded. Word embeddings solve these issues by limiting the number of required features to the number of dimensions in the vector and by retaining information about the relationships of words. Training a Word2Vec model requires phrases or sentences. In our case, instead of passing in ingredient lists, we’ll use our recipe preparation steps. This means we just aren’t crafting features based on the recipe ingredients but it’s entire preparation process. Which means we’ll end up with more information on the techniques and methods employed to make the magic happen. While most of the code is cleaning and formatting the data, the actual training of the model is quite simple. While this model is implemented in many languages, I decided to use gensim’s Python implementation . Visualizing more than three dimensions is hard, and we’ve just created embeddings of 70 dimensions with word2vec. Luckily, when we want to visualize our high dimensional word embeddings, we can employ a dimensionality reduction technique. Below, we can see some of the vector embeddings for common ingredients projected onto two dimensions by t-SNE. t-SNE or t-Distributed Stochastic Neighbor Embedding is a dimensionality reduction method. The positions of the ingredients below represent probability distributions rather than actual positions in space. t-SNE plots can be difficult to interpret as the hyper parameter, perplexity, can drastically change the size and distance between clusters. However, we aren’t trying to interpret clusters, but rather hoping to evaluate whether or not our model learned something useful about our recipes. (See this post by Wattenberg explaining more about how to use T-SNE effectively.) Espresso and Coffee are right next to each other in red. Fettuccini, macaroni, pasta, and linguine are also closely clustered in teal. Likewise, tequila, vodka, gin, and champagne are tightly knit together in purple. Rosemary, thyme, and sage in green, narrowly escaping the perfect Simon and Garfunkel reference . There may be some strange relationships in this plot, and every run of t-SNE will return a slightly different interpretation. That said, it does appear our model is learning quite a bit about the food space. Another way to evaluate word embeddings is to evaluate the similarities of words for which we have a good understanding of how they are related. Below we can see the vectors for torte and cake are very similar (similarity scores from 0- 1) but not so similar to salad (sadly). Likewise, chocolate is very similar to ganache, but not so much to guacamole. The beautiful thing about word embeddings is they are composable. Just like when you string words together to form a sentence, combining word embeddings produces a meaningful new embedding. We’ve created these ingredient and preparation embeddings, but we really want recipe embeddings. We can sum, average, or concatenate our word embeddings to create our recipe embeddings. Summing or averaging vectors like that may sound counterintuitive like we’re losing a lot of detail about our subjects, but in practice, it’s been found to be very effective. Below we’ve plotted all our recipe embeddings after summing them up. Now that we have recipe embeddings, we’ll use these as features in content based recommendation systems, related content modules, and user preference clustering. A content based recommendation system will let us make personalized recommendations for users based on their past views and likes. Here’s a sneak peek at what personalized recipe recommendations might look like Tasty App soon! Happy Cooking! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 88 1 Machine Learning Data Science BuzzFeed Buzzfeed Posts Tasty 88 claps 88 1 Written by Sr. Data Scientist at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Sr. Data Scientist at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-07-28"},
{"website": "Buzzfeed", "title": "23 amazing projects from buzzfeeds 2017 hack week", "author": ["Amy Filmore"], "link": "https://tech.buzzfeed.com/23-amazing-projects-from-buzzfeeds-2017-hack-week-d0f610dd0e94", "abstract": "Latest Posts Events Apply To BuzzFeed The BuzzFeed Tech Team puts on a Hack Week once a year to help stretch our imaginations, work with people outside or normal teams, and have fun as a Tech organization. This Hack Week, we created 38 different projects from Tech teams all across the globe. BuzzFeed Tech is made up of ~230 folks, including engineers, product designers, product managers, QA analysts, data scientists, data analysts, product support specialists, and project managers. We maintain and build internal tools as well as consumer facing products like our website and apps! Ever wonder what it’s like to experience a fifteen foot tall BuzzFeed reaction badge in “real” space? That’s exactly what we set out to answer using iOS 11’s new ARKit. Consisting of a badge composer, a library of custom 3D objects we created a phone-sized window into augmented reality to get a feel for the basics and test the limits of the technology. The app we built, while simple, is surprisingly engaging! We had a bunch of fun holding up our phones and walking around giant virtual (AKA “nonexistent”) objects as passers by gave us sideways glances. Team: Laticia Chance, Rituja Donadkar, Jay Henry, Joseph Bergen BuzzFeed’s popular quiz format, but as webVR experience where users navigate a three dimensional space answering questions. The quiz is called “Which Simulated Reality Are Actually Trapped In?” where the choices made will determine your virtual destiny. Team: Manuel Palou BuzzFeed and Chill lets the user pick a different theme for the BuzzFeed landing page in order to view BuzzFeed.com from the TV series world’s perspective. We implemented separate themes for Broad City, Adventure Time, Parks & Rec, Game of Thrones, and Terrace House. Once the user selects one of these TV shows, the landing page completely changes to fit the respective style and content associated with the show. This project allowed us to play around with our favorite shows and try on the shoes of our awesome content creators downstairs. To read more about the team’s experience through Hack Week with this project check out their post here . Team: Jessie Wu, Jess Kustra, Brandon Choi Award Winner*: Mostly like to be monetized and have Sophie Turner come visit the office because of it *self-proclaimed In-Frame creates a video overlay that lets users explore information related to the current scene. Team: Kiran Booth-Patel, Ivan Butyliuk, Ian Feather BF, RN provides BuzzFeed content that’s right for you right now. Bored in class? Having a bad day? We’ve reimagined content discovery to be tailored to specific timely moments, instead of by topic area/vertical. We’ve also removed the feed, so once you’ve told us how you’re feeling right now, we drop you into a continuous content consumption experience. Team: Allison Krausman, Alp Ozcelik, Lauren Zhang, Brandon Truong, Chloe Rosen Normally, users would have to enter the app to take one of BuzzFeed’s many quizzes. Now, simply by adding the BuzzFeed Quiz Widget, users can complete any quiz from their home screen. Team: Kevin Skrei This Android App allows a user to browse Tasty recipes and export to 3rd party shopping apps. This functionality can be shared as an open spec or for select business partners. This project, was written from scratch in less than 3 days, using the Kotlin Programming Language. This app doesn’t simply export simple text but a rich informational format. Furthermore this Tasty app knows nothing about the shopping apps, but relies on the Android Intent mechanism to register an interest. Team: Steve Peterson RetroFeed is an easter egg on the BuzzFeed homepage that retro-ify’s the website. Team: Noga Raviv, Annie Zhang, Douglas Rudolph (we are all interns!) We wanted to make the home feed more dynamic by including standalone imagery and quiz questions. We added a static image component, a react carousel of images, and an interactive quiz unit. We also created a better comics-browsing experience with template changes to /comics, pulling from all recent content posted to our BF Comics Facebook page. Team: Sean Gilbertson, Lindsey Maratta, Swara Kantaria, Matt Semanyshyn The goal of the project was to enable bidirectional real-time communication between our visitors and platform, and to see what awesome applications we could build on top of it. We produced 3 projects: Favicons : A multiplayer game to guess the words composed of favicons Le Book Game : A turn-based multiplayer game where players collaborate on a book Real time feed updating , complete with a novelty buzzing-feed-on-update! Team: Amanda Nguyen, Artyom Neustroev, Dang Vang, Mark Gannaway Video Roulette is a game that can generate over 100K unique new videos from existing BuzzFeed content and encourages users to discover content in a playful way. By dynamically creating a mashup of videos selected by the game it demonstrates the possibilities and future direction of automated video editing. Team: Alex Perevalov, Claudia Mei, Peter Karp A slackbot allowing for slash commands to find documentation for services, as well as making interesting documents (like the BF Guides to Computing) more easily discoverable. Future goals include expanding to other documents, and creating a bot interactive to help with onboarding. Team: Rowan Cota, Derek O’Brien, Miguel Coquet, Arielle Benedek (with additional contributions from David Dunlop, Dao Nguyen, and others… A glossary of terms both industry standard and unique to BuzzFeed that is accessible to ALL employees. Eliminating duplicate spreadsheet efforts by having a centralized place where users can consume, share and easily contribute. Available via a webpage or slack via a simple command [/WTFis]. Useful to onboard new team members regardless of departments and a quick refresher on new or forgotten terms. Team: Kayne Bordes, Caylee Betts BubbleFeed is a Interactive Data Visualization of the BuzzFeed.com site. Written for in JavaScript and D3. Pulling in the urls for each post, hit counts per post and summary text, BubbleFeed creates interactive bubbles for each post. Users can preview an article by pressing on it. Open the link in a bubble opens the post in a browser window just like a normal browser. Team: Chris Applegate, and Dan Meruelo We used Apple’s new Core ML technology to run a convolutional neural net that gauges the user’s emotion (https://gist.github.com/GilLevi/54aee1b8b0397721aa4b) via the iPhone’s front-facing camera. We can then leverage this for mood based quizzes and challenges. Our demo app shows the user a funny video while challenging them not to smile for 30 seconds. We use the emotion prediction probability to gauge when the user is starting to smile and give them a warning, and if we ever can’t find a face we’ll pause the video and timer. Team: David Mauro, Wolasi Konu, Pete Walters Find On Image is a brand new quiz format which we’re already seeing authors do amazing things with. All you need to do is: upload an image, define areas people can tap on, and the results to give them based on where they tapped! Our engine supports both Trivia and Personality formats, which gives authors great flexibility and lets their imagination run wild. We’ve had crosswords, “tag urself”s, map games, celebrity tie-ins and many more proposed. As there’s a fully fledged self service builder, people can experiment with it without being delayed or needing to commit to presenting us with a finished article. We also built easy editing support in right from the start, so International teams are adapting successful quizzes into all sorts of languages as we speak — all without us helping! The team heroically built the entire project and put it into production within Hack Week and continues to support and develop upon it. Visitors are finding it fun, intuitive and engaging, saying we’ve been “upping our quiz game” ;) Try one out today, with This Dessert Quiz Will Reveal A Deep Truth About You or Prove You Have Perfect Colour Vision By Spotting The Odd Colours In These Pictures being popular examples of Personality and Trivia types. Team: Celine Chang, Paul Curry, Anjali Patel, Ola Sendecka SkillsBot promotes connectivity, skill-building and problem-solving at BuzzFeed by connecting any user with an expert peer that can lend a helping hand with a topic, software or skill. Team: Max Brawer & Terrance Liang OpsBot is a Golang based Slackbot, primary function is dynamic incident channel generation and user invitation (+ other features) Team: Raymond Wong, Mark McDonnell A Chrome extension that turns the BuzzFeed Homepage into a simple synthesizer using the Web Audio API Team: Patrick Carey sli_api is a Slack integration and API allowing users to create and execute Slack commands that make API calls and format the results Team: Steven Gemmen, Agustina Varela, Benjamin Stockwell Using in-video retention data from YouTube, our system identifies the most watched parts of a video and retrieves them as clips. These clips can then be stitched together to make a highlight reel, or shown to creators to inform them about the most appealing parts of their video. Team: Ali Baghshomali, Jonathan Ma, Jennifer Greenwood A new chrome extension allows the sales team to generate previews of mock ads for potential advertisers using existing assets. Advertisers can save and share links to their ad previews while they consider investing in an advertising campaign with us! Team: Caroline Amaba, Rebecca Close, Hana Carpenter, Andrew Paulus QuizPro is a chrome extension that helps quiz-creators quickly get data on their quizzes — such as what quiz results are most common and which ones are shared the most. Team: Dilip Rajan, Will Herrmann, Emma Byrne, Lyle Smith Check out the rest of our Tech Blog for tons of interesting posts about how we work here at BuzzFeed Tech. If you think you could be a good fit for our team, check out our open positions ! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 52 Hackathons BuzzFeed Buzzfeed Posts Technology Tech 52 claps 52 Written by Director of Project Management @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Director of Project Management @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-03"},
{"website": "Buzzfeed", "title": "come join us for buzzfeed tech talks", "author": ["Swati Vauthrin"], "link": "https://tech.buzzfeed.com/come-join-us-for-buzzfeed-tech-talks-7d12165d05a", "abstract": "Latest Posts Events Apply To BuzzFeed On Wednesday December 14, 2016, we held our first Tech Talks @ BuzzFeed Primetime event . Tech Talks have always been a regular thing within BuzzFeed Tech however they were mainly for our internal team. We decided to take them Primetime because we wanted to give the NYC tech community a place to share, connect, and get to know each other. Why did we do this? Bring the NYC tech community together — we’re growing and we need to continue to engage with one another and most importantly learn from one another. Talk about the great work that is being done at BuzzFeed and other tech companies in NYC. We want this to be an opportunity for others to learn about the technology being created at companies across NYC whether it be Engineering, Design, Data Science, or Product. We have a great space! Our new office has a fabulous canteen plus a rooftop that can accommodate many people. We want to take advantage of that to bring the NYC tech community together. The theme of this inaugural event was Experimentation because we saw a lot of teams experimenting within BuzzFeed and elsewhere. We wanted to share our discoveries and hear about what others learned as well! Here was the lineup for our first event: Deploy With Haste 🏃 An introduction to rig, our end-to-end platform for software delivery. Presenter: Matt Reiferson — VP of Engineering @ BuzzFeed Experiment With The Future 🚀 Introducing BuzzFeed’s Emerging Consumer Technologies initiative. Presenters: Chris Johanesen — VP of Product @ BuzzFeed Jon Morehouse — Staff Software Engineer @ BuzzFeed Iterate With Confidence 🐱 🐶 Learnings from building ABeagle, BuzzFeed’s internal A/B testing framework. Presenter: Ailin Fang — Data Scientist @ BuzzFeed Comments at Scale 🔊 Our experiment to grow The New York Times community. Presenter: Erica Greene — Engineering Manager @ The New York Times Look out for more events in the future! Each week, a different BuzzFeed tech team member takes over our Twitter account. Follow BuzzFeed Tech on Twitter @buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… Thanks to Matt Reiferson and Jessie Wu . Startup Tech Talk Experimenting Buzzfeed Events BuzzFeed Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-26"},
{"website": "Buzzfeed", "title": "essential elements of giving good feedback", "author": ["Chloe (Rowan) C."], "link": "https://tech.buzzfeed.com/essential-elements-of-giving-good-feedback-3e2722565887", "abstract": "Latest Posts Events Apply To BuzzFeed My best friend is a hell of a woman: known for being as direct as stepping on Lego bricks barefooted, but also someone people routinely go to for advice and feedback. I spent years wondering why it was that so many people sought her out to ask her to polish their work or tell them what they were doing wrong. Finally, I couldn’t take it anymore. Late one night, while sitting on a porch in the cold, she told me we should talk about communication. She gave me her three principles for giving feedback: Is what you’re about to say honest ? Is what you’re about to say necessary ? Is what you’re about to say kind ? Currently, it’s mid-year review time, and I was having a conversation about gathering feedback with a fellow BuzzFeeder. I kept bringing up how important kindness is in the whole process, and they suggested I should write about it. When I sat down to write, I realized I couldn’t talk about kindness without talking about the other two aspects, so let’s dive in… Even though we’re thinking about feedback right now because it’s mid-year review time, there are many times feedback happens at BuzzFeed. From interpersonal feedback (like reviews) to code reviews, from design meetings to team meetings, giving good feedback is a crucial part of the day to day here. As a reviewer, honesty includes reviewing the feedback you’re providing. Taking the time to sit down and look through it to make sure you’re not projecting assumptions on the work in front of you is a valuable exercise. It can often reveal comments that use words like “obvious,” “easy,” “simple,” or even “basic.” These words should be seen as flags for your understanding of the problem space and the assumptions you’ve made in regards to the other party’s knowledge. Frequently, examining them reveals that the assumptions are flawed, and thus potentially intellectually dishonest. Honesty can also mean telling someone you’re not the right person to offer feedback. Whether it’s because the thing being reviewed isn’t an area of expertise you can speak on, or because you’re not able to give it full attention, honesty requires that if you can’t engage in the review with care and intent you should disclose. It’s better for everyone involved if the feedback comes from the right people. Answering the question, Is it necessary, helps keep scope creep in check. It also reduces the cognitive and emotional stresses of seeking feedback by reducing the number of things someone has to actually be concerned with. It is much easier to go into a feedback session knowing that you’re only dealing with feedback on the thing you brought to the table. Think of it as minimizing the scope creep of the feedback itself. This can be hard because human nature is to believe that if you see something that can be fixed you should say something. Unfortunately, the tendency to want to mention everything you see can actually dilute the effect of your feedback! How do you figure out what’s necessary then? The person asking for feedback will tell you. If they bring you a draft blog post and say, “Hey, can you look this over and help me out with it,” they’re probably not looking for advice on how to expand their thesis to cover a tangential project you’re also working on. If someone asks for a code review, it wouldn’t be the time to look at the code surrounding what they wrote and point out that someone else poorly named variables and they should go in and change it just because they’re in there. Kindness often seems like the most difficult quality to exhibit in giving feedback. When asked to give feedback, we all try to be helpful which is a major aspect of kindness. Whether it’s pointing out a mistake or weak area, or offering praise on something you’d like to see more of in the future, helpfulness in feedback is natural. I’ve definitely fallen into traps by being helpful to the exclusion of another major part of kindness: empathy. Empathy in feedback starts with making sure that your feedback is balanced. That doesn’t mean that it has to be “all good” or even a majority good; rather, it means that your feedback calls forward the strengths you see as well as the weaknesses. Even if there are a lot of weaknesses and only a few strengths, highlighting both helps the person on the other end see where they can improve and where their skills are providing a good foundation for them to build on. Empathy also means taking time to review your feedback before you send it. I first brought reviewing up in honesty, but it’s equally important for empathy. Make sure when you reread the feedback that you’re giving that you haven’t left comments that could be easily misinterpreted, or that seem confusing. If you review your feedback the day after you wrote it, and anything leaves you feeling like you have a question, that’s probably a place to revise. At BuzzFeed we also exhibit another really strong part of empathy: the idea that feedback should strive to be blameless. If, on review, your feedback reads as though you are angry or disappointed, take that as a chance to drill in further and get to the real root cause. For example, feedback that someone “wrote bad tests” would go over poorly and wouldn’t lead to real change. On the other hand, revising that feedback and saying something like, “the tests for this code don’t cover the edge cases, which is an important part of testing,” gives the receiver something actionable and demonstrates that you care about their improvement, not just venting bad feelings. So my best friend tells me all this about honesty, necessity, and kindness, and then she lays the secret of it on me — those are the same rules she uses for all communications. She just treats giving feedback as a communication exercise on hard mode. It completely reframed how I think about feedback and contextualized these guidelines as a framework for conversation. Now, I approach feedback with these points in mind, and I ask myself if the things I’m about to say are honest, necessary, and kind, regardless of whether I’m the giver or the receiver. That seemingly small change in framing has taken feedback and reviews from a dreaded activity to something I look forward to, knowing I will come out of it smarter and better than I went into it. To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 30 Feedback Buzzfeed Posts BuzzFeed Performance Reviews Advice 30 claps 30 Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Written by pop-culture. ethical technologist. part-time superhero. | sre @ BuzzFeed | femmeops.club [all opinions on my page my own, I do not speak for my employer] Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-07-17"},
{"website": "Buzzfeed", "title": "what its like to interview with buzzfeed engineering", "author": ["Luke Vnenchak"], "link": "https://tech.buzzfeed.com/what-its-like-to-interview-with-buzzfeed-engineering-cbe9c1704e67", "abstract": "Latest Posts Events Apply To BuzzFeed This post focuses on hiring for individual contributor roles at BuzzFeed Engineering. Interviewing is stressful! Not knowing what to expect in an interview makes it worse. In order to alleviate some of that stress for potential engineering candidates, we thought we’d answer some common questions, share our views on tech interviewing, and walk you through BuzzFeed Engineering’s hiring process. At BuzzFeed, we are passionate about understanding our users and building great experiences for them. Similarly, we want our interview process to be a great experience for you. We want to give you the best chance to show what you are capable of while also moving through the process as quickly as your schedule allows. We aim for transparency throughout the process and know you are interviewing us as much as we are interviewing you. Because we want to get a sense of what it is like to work with you, our technical interviews are not built to trick you. We view them as discussions rather than tests. We focus on topics and exercises that are applicable to the work we do and the challenges we face. In order to understand how you have dealt with real-world challenges, we talk through some of your past projects. Because it is a common way engineers at BuzzFeed visualize system architecture, we do use whiteboards in our discussions. Our pair programming exercises help us understand how you turn ideas into code and communicate that thought process. Ultimately, we are more interested in finding great problem solvers with capacity to learn than we are in finding candidates who have experience in a specific technology. BuzzFeed tech has more than 20 teams or squads that work on a wide range of initiatives covering consumer products (e.g. buzzfeed.com, native apps), internal tools (e.g. content creation, video production, content distribution, content performance dashboards) and infrastructure (e.g. ops, data infrastructure, platform engineering). We often don’t know which squad a candidate will land on until we’ve completed the interview process. You will interview with members of multiple squads as we figure out which one is right for you. Squad assignments are made based on your interests, our needs, and most importantly, where you will be best setup to succeed. We want to put new hires in a place with the right opportunities and mentorship because your initial team assignment is just a starting point for long term success at BuzzFeed. Don’t worry, we will loop you in on this discussion and you’ll know which team you’d be joining before you need to accept an offer! A recruiter will explain the steps in the BuzzFeed hiring process, which you already know because you’re reading this post! They will ask you about your skills and experience and what you are looking for in your next role. They can answer any questions you have about the team structure, benefits, and culture. The recruiter will be an advocate for you and will guide you through the rest of this process. At this point you will talk to an experienced manager, director, or VP from our engineering team. This call will not be a technical deep dive. The manager will be talking with you more about your expectations and experiences as we start to figure out which role and which team might be the best fit for you. This is a great opportunity to ask questions of a senior member of our team so be ready for that! The next step is a technical interview, typically held face to face. If you don’t live near one of our offices, we will do this via a video conference. In this one-hour session, you will go through a whiteboarding discussion or a pair programming exercise with one or two of our engineers. We have moved away from take-home assignments for most of our engineering roles because we know your time is valuable and we think a discussion is more efficient. However, if you think an assignment would be a better way for you to showcase your abilities, we are happy to accommodate. The last step is the interview day, which consists of four or five one-hour interviews in one of our offices. Two of the interviews will be technical in nature, but we are also interested in your views and experiences on how teams come together to build great products. Most of our interview loops include members of our product design or product management teams because we work closely with those teams every day. Since we have a distributed team with engineers in Los Angeles, Minneapolis, London, and New York, some of these interviews may be conducted via video conference. We hope you will leave these interviews with more of an understanding of how we are organized, what our teams are up to, and what working at BuzzFeed is like. If you are interested in going through this process with us, check out buzzfeed.com/jobs . We’re hiring engineers and managers in Los Angeles, Minneapolis, London, and New York! Each week, a different BuzzFeed tech team member takes over our Twitter account. Follow BuzzFeed Tech on Twitter @buzzfeedexp ! Sharing our experiences & discoveries for the betterment of… 31 Recruiting BuzzFeed Diversity In Tech Buzzfeed Posts Buzzfeed Tech Jobs 31 claps 31 Written by VP Engineering @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by VP Engineering @ BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-01"},
{"website": "Buzzfeed", "title": "this is the end or is it how to manage the end of a products life cycle", "author": ["Arielle Benedek"], "link": "https://tech.buzzfeed.com/this-is-the-end-or-is-it-how-to-manage-the-end-of-a-products-life-cycle-273b7ebafdf", "abstract": "Latest Posts Events Apply To BuzzFeed When is a product born? From an idea? From the first product spec? The beta version? Launch day? While we can discuss the merits of each — all are valid starting points. But what about the end of a product’s life? When does that happen? Does it die a slow death by neglect? Is it forever relegated to a life of “maintenance”? Maybe there is a plan to sunset the product or repurpose it for new projects. The point is, the end of a product’s life cycle is a bit more ambiguous. In fact, it’s a conundrum faced by many — how you handle it largely depends on company priorities, processes, and most importantly, culture. I would argue that this last point — having a culture of user empathy spanning engineering, product, design, QA and product support (that’s me!) — is key to maintaining the integrity of your tech organization . As members of the BuzzFeed Tech Team, we are very aware of the shifting media landscape moving like quicksand beneath our feet. What makes us successful is our agility and ability to respond to this ever-changing environment in step with the rest of our company. Practically, this means we are constantly re-organizing our teams to reflect these changes in priorities.This is especially problematic for legacy products that may be left behind amidst restructuring, in times of re-architecture, or when moving to a new stack. At BuzzFeed, we’ve taken several approaches to managing the end of life for our legacy products. Some methods work better than others and I’m here to share a few of these experiences and what we’ve learned. In one case, we put development on hold for our suite of translation tools. This was a concerted decision to shift focus onto different product areas. Since these tools were in a stable condition, our engineers were able to move onto new projects without much distraction. However, nothing is built in a vacuum and when bugs arise, we have one engineer that is able to debug these issues. The risk with this approach is known as the “ bus factor ”. If you keep product knowledge restricted to one set of individuals, this means they are inherently responsible for the product’s outcomes even when they are no longer tied to the product in an organizational sense. Additionally, they may eventually leave the company and this knowledge shouldn’t be lost to the ether. So, how do we counteract this? By sharing knowledge. “The more you know” — a truly inspiring and relevant adage, thanks NBC! How best to share knowledge? Documentation is one way. Build it as you go. Have fun with it, make fancy user guides with loads of gifs, maybe some flow charts, go crazy! Have a documentation day team outing, shuffleboard anyone? Another way? Spread the knowledge across tech. Here at BuzzFeed we have a weekly “ tech talks ” series where anyone in the organization can present a talk about a product they’ve been working on to encourage collaboration and sharing of insights. And sometimes there’s beer, that helps. Finally, make sure everyone on the team, no matter their capacity, is well-versed in the infrastructure, user requirements, and expected behavior of a product so everyone can tag-team if an issue arises. We’ve made great strides in improving the product development and engineering experience across the tech organization. We use a consistent set of technologies and approaches that reduce the on-boarding effort for any single service. This makes it easier for engineers to observe and debug any service, even if they haven’t yet worked on it. Having the ability for anyone to triage bugs on products in “maintenance mode” is key to maintaining trust and reliability in your products and also maintaining a positive relationship with your stakeholders. Sometimes you build an experimental product, which over time, accumulates a diverse set of users who incorporate it as part of their daily workflows. Great! You built a product and it’s super useful. However now, the team that built it is dispersed. But wait! Those same users are requesting features and enhancements but no one is left to work on the product. So what do you do? In this case of this tool, an asset library, it was up to product support to man the help channel and communicate these requests to…whom? That was the challenge. The product lacked engineering resources and decision makers as this tool was orphaned during a recent re-org. Eventually the volume of requests forced our team to reassess the current usage of the product. Finally, we decided to move some of the utility from the old product to a new product that would be actively maintained. In this scenario, it would have helped if there were established engineering and product owners that persisted amidst the various re-orgs. We’ve made efforts as an organization to keep track of who the resident experts are on certain products. It’s a good idea to have these discussions on accountability when a re-org is anticipated so that when a future decision needs to be made- you know who is responsible. Communicating to users that you are not going to work on a product is JUST AS important as actually fulfilling their requests, this promotes transparency and trust. Products break, there’s no avoiding it. Usually you fix what’s broken. However, when you are rebuilding your backend architecture, it makes little sense to fix what’s broken in the old stack. A better idea is to move the product over to the new stack. However, when the light at the end of a tunnel is far away and you have other high priority tools that need re-architecting, it means some products are neglected. This was the case when our A/B testing headline tool broke. Every editor in the organization was using this tool. It broke frequently. We would fix it frequently, but there came a point where we knew we had to kill it or rebuild it in the new stack. Eventually we sat down to investigate if this was a worthwhile endeavor. Turns out total post views increased on average by 31% so we decided to rebuild it. With a sting team and a deadline, we had the MVP out and ready in a matter of weeks and it works fantastically (increased total post views by 35% and resolves tests in 20 minutes compared to 48 hours). While the end result was extremely positive, it took us a while to make a definitive decision - kill or rebuild. Making these difficult decisions at the onset of a re-architecture period will improve the confidence of the users in your product, so that they can set realistic expectations when making requests or reporting bugs. Even if you put the decision on hold, communicate this to your users. Explain the situation, lay out a timeline- they will understand. The end of a product’s life is not always so clear cut. Some approaches to managing the end of a product’s life cycle work better than others, but what we’ve learned is that making a decision is the hardest part and communication is essential. Creating a culture of open communication and transparency is the cornerstone of user empathy . Be honest and upfront with your users, no matter what decision you make at the end of a product’s life cycle. If your tool is going into maintenance mode, TELL YOUR USERS. If your product is going to be sunset, TELL YOUR USERS (and hopefully provide a workaround or make a really good argument for why it’s no longer useful). If the product is going to be repurposed, awesome! But still, TELL YOUR USERS. Most importantly — use what you build. There is no easier way to empathize with your users than by becoming one yourself. My point is, remember why you built this product and who you built it for. Those users don’t go away even when priorities shift. You’ve likely created something totally amazing that they’ve adopted as part of their daily lives, so feel great about that. By following these steps, hopefully you can maintain trust and faith in your tech org and most importantly, create a culture of user empathy. I promise it will pay off! Sharing our experiences & discoveries for the betterment of… 6 Technology Product User Empathy How To Buzzfeed Posts 6 claps 6 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-31"},
{"website": "Buzzfeed", "title": "crafting the product design hiring experience part one", "author": ["Sabrina Majeed"], "link": "https://tech.buzzfeed.com/crafting-the-product-design-hiring-experience-part-one-65c924bc7a41", "abstract": "Latest Posts Events Apply To BuzzFeed For the past two years, I’ve been knee-deep in trying to solve one of the most difficult design challenges I’ve faced in my career: hiring. How do you find and hire the best designers in this increasingly competitive industry? At BuzzFeed, we’re looking for product designers who are talented executioners, strategic thinkers and humble to boot. It’s a tall order that requires a thoughtful and intentional process. What started with a few sweeping changes to introduce a sense of order and consistency to our hiring approach in early 2015 has since been iterated upon and improved with each successive interview. Shedding light on what’s worked (and hasn’t) for us internally has had a positive impact on other teams at BuzzFeed, so we’ve decided to make our hiring process and approach public. This is the first in a three part series, focusing on our philosophy and why we as designers are actively concerning ourselves with hiring. The second and third parts will more tactically describe how we recruit and how we interview people, respectively. Within the first half of 2015, BuzzFeed’s design team grew 50% from twelve to eighteen people. At our scale filling our entire headcount within the first half of that year was both imperative to accomplishing our goals and impressive compared to our pace of hiring in the past. The change to our hiring processes between 2014 and 2015 was mostly philosophical. As designers, we began to view the full extent of the hiring process as our responsibility, rather than off-loading half of the process to the recruiting team. Our previous involvement in hiring only consisted of showing up to an interview that magically appeared on our calendars. Recruiters took care of sourcing, initial screens, planning in-person interviews, and all communication with candidates. This can be a tempting ownership structure in a large company, especially when the actual products you design demand so much of your time and focus. But the thing is, more than that button placement, or debating the merits of a hamburger menu, hiring is the most important thing we will do as a company . We can’t build great products without great people, and we can’t hire great people without a great process. In the past we spent so much time obsessing over the users of our products and their experience, that we neglected the fact that recruiting is yet another touch point in which people come into contact with BuzzFeed both as a business and a brand. Like anything else we put out into the world, we wanted to make sure that “Interviewing at BuzzFeed” is a good user experience. We wanted to be the kind of a company that even a candidate we turned down would still strongly recommend their friends apply to. We strove to design a hiring process that is respectful and appreciative of people’s time and energy. This meant making ourselves, as designers and design managers, personally accessible to prospective candidates. Not making them jump through unnecessary hoops for our time. It meant being prepared, organized, never late for interviews and making sure that candidates know what to expect. It meant being better than the typical Tinder date and not ghosting on the people we didn’t feel were a good match. Here’s what valuing people’s time and effort didn’t mean: It didn’t mean we lowered our standards for hiring in any way. In fact, quite the opposite. It meant we’re holding ourselves to the same high standards that we expect of our future colleagues. In the past a candidate might not have even talked to an actual designer on the team until they were brought in for an on-site interview. Our lack of involvement in the earlier parts of the process was problematic because… On-site interviews require taking a large group of people (about 5 to 6 for us, which is the size of a small project team at BuzzFeed) away from their day to day responsibilities. The time taken for interviewing extends beyond the time allotted to speak with the candidate, but also includes time taken to prepare for the interview and time taken afterwards to synthesize feedback and make a hiring decision. With this in mind, the fewer interview loops that result in a “no hire”, the better. The success of our hiring process can be measured by a high interview-to-hire ratio . On-site interviews that result in a hire are good investments of everyone’s time. In 2015 we brought in eleven people on-site and only three of them resulted in a “no-hire” decision. Only one of the candidates who received an offer turned us down, which means we ended up hiring 7 out of 11 people. This benchmark holds us accountable for our team’s time and to ensure that our hiring process actually works. Most of the work to ensure that on-site interviews are successful happens before the candidate even walks through the door. In the next installment of this series, I’ll share more tactically how we screen and vet potential candidates. Sharing our experiences & discoveries for the betterment of… 193 1 Thanks to Tom Harman . Hiring Product Design Recruiting Tech Buzzfeed Posts 193 claps 193 1 Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-24"},
{"website": "Buzzfeed", "title": "5 things im learning going from agency work to product work", "author": ["Caroline Amaba"], "link": "https://tech.buzzfeed.com/5-things-im-learning-going-from-agency-work-to-product-work-b28f57df344d", "abstract": "Latest Posts Events Apply To BuzzFeed How I went from doing advertising agency campaigns to product work at BuzzFeed. Before joining BuzzFeed’s Tech team about three months ago as a software engineer, I was a full-stack web developer at an advertising agency. I was not an account manager that would interface with the client, so this story isn’t going to be about agency life in general. I’m here to tell you about how I grew as a developer, what made me want to switch from agency work to product work, and what challenges I’m facing — and conquering! — as I transition from what I used to know to what I’m learning now. *Kinda. Simply put: a campaign is just a centralized effort by a brand or client to push a specific product or event. When there was a microsite or activation that required a technical connection to a social platform API, the accounts teams would call upon the Friendly Neighborhood Tech Team ® to build it, if my old agency didn’t already consult the brand’s agency of record, another dev shop, or the client’s in-house tech. Work like this was frequent and varied. At any given time, I would probably be working on at least two projects, and later on, overseeing a third, maybe even planning a fourth. Although the variety was nice, often this variety was typically in the form of the creative design. When the dev team would build Something New And Cool™ for one account, other account teams would pitch a version of it to their client, and we would simply rebuild it with some small functionality tweaks and design updates. There is a lot of variety in agency work because campaigns are relatively short-lived. I have had a campaign literally last one week: we had spent at least 80 hours on it, and that was just development time, not all of the days and weeks prior to tech team hand-off that included brainstorming and creative design. Development was quick and scrappy to get it done on time (I’ll talk more about speed later on). At BuzzFeed, I’m on the Ad Experience team. We have several products we implement on our various, larger services in order to serve BuzzFeed users good advertisements. The team has reusable components and services that we’re putting in place to be able to work seamlessly with the other teams within the BuzzFeed Tech. The product(s) usually live “forever” for us to iterate on it (not literally forever, but my colleague Arielle has an informative article on the product lifecycle ). As the team iterates, I find myself going back and thinking about optimizing my code or how to make my logic “future-proof” should some feature need to be added to this section in the future. I never really got to do this in practice in the agency world. I could always outline or discuss how I would have done things better in a post mortem for a campaign — sometimes doing so helped for a re-implementation of that campaign under a different guise, but it never was truly improved . This brings me to #2: As someone who has recently joined BuzzFeed Tech, there’s a lot of existing code: I repeat, a lot … a monolith, if you will . It took me my first couple of weeks just to get setup and familiar with the ins and outs of the code and the deployment process. At the agency, because I was usually the singular developer or principal architect on a project, I knew all of the project’s ins and outs, its quirks and shortcuts. Comments were mostly just pseudocode and some occasionally-cryptic reminders (or TODOs frozen in time; more on that later). Sure, I had naming conventions, and the team had some coding style choices in order to keep things consistent and readable, but there was never truly an outlined coding style guide at the agency. I would occasionally have another developer supporting my project, but the second developer was never for anything so functionally changing about the project, simply bug fixes or tweaks that conformed to whatever code style and architecture I had already put in place. Despite my team at BuzzFeed being approximately 12 people, there is also the aspect of my team that touches other services within the tech organization. It will never be just me looking at the code anymore. Good documentation, comments, and a consistent coding style guide are part of my transition over to working with a large team (even if I don’t agree with some of the conventions… I think leading commas are great ). For example, in BuzzFeed Tech, there is a library of setup docs, write-ups on why our systems are built in certain ways, lists of helpful links, etc. We have an ongoing guide titled The BuzzFeed Guide to Computer & Software Systems that is actively being written. It explains our underlying software and infrastructure so everyone in the organization is on the same page. In addition, I actually have regular code reviews now. Our pull requests are required to have at least one peer review whenever before we move onto testing and merging. These reviews are helping me learn, even with the smallest of updates and commits. There was sometimes light code review at the agency, but it was mostly done post mortem in order to “do it better next time.” Speaking of getting it done… ( Look at that segue! ) Speed is everything at an agency. Getting things done in the shortest amount of time to save the client money and to get things out the door quickly is necessary to move on to the next project. It’s not to say that quality isn’t valued at the agency or speed isn’t valued here at BuzzFeed Tech, but there is a subtle difference between speed and agility. There were many times I would have an engineering problem at the agency where I would continue to say to myself, “There has to be a better way.” I could figure out a creative solution, but when you work for a client, your project is then put into a box of technical constraints. The way you believe is the best way to execute the project may not allowed by those constraints. I had to be scrappy and get creative in some of my problem solving to get around some of the requirements of various campaigns; other times, I would have to just force it and try not to be grossed out by whatever blasphemous jargon of digital text I considered code. If it worked and got the job done, it was usually fine. I always did my best to optimize and do things The Right Way, but doing so also may not have been in the best interest of time. I could have a proper solution for an aspect of the project, but executing my solution could be just out of reach in terms of how fast I could implement it, especially while there were other aspects of the project to work on. That was the hardest part: sacrificing one part of the project for another or even the ability to finish the project on time and within the scope of the campaign. In Product World, you have the ability to come back to something later, so coding it fast and loose the first time is not so jarring, especially if we do want to quickly deliver new features and test them. On top of that, while working at the agency, some subtle performance things on the user-end could be sacrificed for a campaign. A product is continuously visited (harkening back to the lifespan of a campaign versus product) so engineering things The Right Way is always in our best interest, and — over time — better solutions will be found as data is gathered, users share their experiences, software engineers iterate, and technology progresses. The ability to ship quickly but continue to improve is what “agile” software development is; perhaps that’s why it became such a tech buzzword. ** Well, not just one, but I wear significantly fewer hats now. As one of the first developers of the agency I joined, it was a department that needed to grow and was very rough around the edges for some time. As such, I had to be flexible and wear many hats: I could be a UI designer, UX researcher, front-end or back-end developer, infrastructure and operations, QA Engineer, database admin, manager, etc. I had a perspective on many aspects of web development, but usually I learned just enough to get by and get the job done. As the agency grew, others were hired to fill these roles, but everyone still wore each other’s hats for coverage in case someone got sick or was out of the office. An analogous thing happened with web technologies and web platforms the campaigns would interface with. The agency had many clients with many different requirements. Maybe the client had a proprietary CMS that the agency devs had to learn and interface with; another might’ve allowed us only jQuery; perhaps there was a Twitter-feed aggregator or another third-party vendor that had an API we needed to consume and integrate with the campaign. In terms of a “web stack,” the agency dev team did not really define one; we tried to stick with some of our tried and true boilerplates. Other times if a project would benefit from the features of a particular framework, like Angular or React, we would utilize it. For myself, this required me to learn just enough to get the framework to do what I wanted, and sometimes I never would have a full understanding of what I was doing and (most likely) wrote code contrary to framework’s opinion, breaking it to bend to my will. For Javascript in particular, it was nice to have experience across many different frameworks and learn what I liked and didn’t like, but I never really mastered any of them or had the chance to do so, because I was always on to the next one . Working at BuzzFeed, I’ve got one JS framework I need to learn (it was also surprisingly one I haven’t tried until now, so refreshing!). This is not to say my knowledge of all the other frameworks and aspects of web development have been thrown out the window; contrarily, I use that knowledge for context when writing code here or discussing methodologies in how we’re building products here at BuzzFeed. By honing in on one set of opinions and conventions, I can focus on shipping good code with sound logic. Essentially, I have more brain space and time to working on all previous list items: doing it right and good collaboration among other engineers. I’m about to bring it down and get a bit introspective. I believed for a time while looking for a new job and applying to other companies that I was not a good enough web developer because I wasn’t singularly proficient at a small smattering of technologies. I’ve met a few other agency web developers and freelancers that feel similarly. This feeling was more exaggerated for myself because the agency gig was my first job out of college, and I was in the agency world for five years (which according to some of my colleagues is a long time). Was I too closed up in this agency-dev bubble? Did I learn enough? Was what I was doing relevant to current web landscape? Am I doing enough side projects to learn new web tech? Am I good enough? I had based this bar of being “good enough” on my first interview during my college search was for A Big Tech Company, and I did not make it past the first code interview. It was reiterated to me many times by my friends and colleagues that the interview was five years ago: I must’ve learned something doing all the work I did at the agency, right? I absolutely did! I learned a lot of things in terms of soft and hard skills. I gained wide breadth of technical knowledge. No depth isn’t necessarily a bad thing, and the wide breadth allowed me to figure out what I actually like and am passionate about in web development and software engineering. I apply a lot of what I learned over time at the agency to what I’m doing now, and I realize I have a good perspective working on the Ads Experience team here at BuzzFeed (going from the side of wanting to put ads on All The Things to making the ads native and subtle). Also, you can learn all of the Javascript frameworks you want, but as long as you’re coding, you’re getting better at coding . I’m still relatively new at BuzzFeed, and I’m still constantly learning! It’s a refreshing change, and I’m very excited for all of the projects and products Tech is going to be rolling out! To keep in touch with us here and find out what’s going on at BuzzFeed Tech, be sure to follow us on Twitter @BuzzFeedExp where a member of our Tech team takes over the handle for a week! Sharing our experiences & discoveries for the betterment of… 4 Thanks to Swati Vauthrin and Jessie Wu . Software Development Agencylife Buzzfeed Posts BuzzFeed 4 claps 4 Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @BuzzFeed — playing games, climbing things, coding stuff Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-29"},
{"website": "Buzzfeed", "title": "from westinghouse to android instant apps", "author": ["Steve Peterson"], "link": "https://tech.buzzfeed.com/from-westinghouse-to-android-instant-apps-60fbfaca4ebe", "abstract": "Latest Posts Events Apply To BuzzFeed About a year and a half ago, BuzzFeed began working with Google on an initiative code named Westinghouse. This would allow an Android app to run natively without installing it. Today we know project Westinghouse as Instant apps. Westinghouse is in reference to George Westinghouse, who believed in Nicola Tesla. Together they won the Battle of Currents for providing electricity to homes and businesses. Thomas Edison’s DC (Direct Current) solution required that a power station was located within a mile of every house. Westinghouse and Tesla’s AC (Alternating Current) allowed a power station to be located hundreds of miles away from the homes and businesses that it serves. Clearly AC power is for everyone whereas DC power is only for those in densely packed urban areas. Instant apps, like George Westinghouse’s AC power provides your app’s native experience to everyone on Android, not just those who have installed the app. Working with Google, we decided to expand our BuzzFeed Video app to include Instant apps. Initially, the Android Studio goodness that made our lives so much easier was not there: No debugger integration, no auto-complete. We resorted to log statements and actually remembering a class’ package name and entering import statements by hand. You don’t appreciate something until it’s gone. True both for relationships and IDEs. The SDK was installed by hand, and the wh (Westinghouse) scripts were written in Python. One thing that did make life a lot easier is that the build scripts used Gradle. Except for the –P flag, this is standard Gradle command line syntax. Using our trusty Python scripts we could run on a device. Not just any device, but we had a couple Nexus 5X phones with special powers (rooted phones with a custom kernel actually). Google introduced the concept of an atom . Think of this as a Gradle module only different. Two new gradle plugins were introduced: the atom and the instantapp. Also a couple new attributes floated around inside of the AndroidManifest.xml file (don’t pay any attention to all this — it’s all been simplified). We branched from our production code, and began the process. After many Diet Cokes/Mountain Dews and Espressos, we got our first running version (thanks Paul Marino and Dan Tann from the original BuzzFeed Video App team)! From our excitement, one would have thought we conquered nuclear fusion. Even this first version was surprisingly fast and stable. We did have the occasional random crash, but we worked through those quickly. Google was pleased with our Instant app. They mentioned that they might showcase it at one of the booths at Google I/O. Go Instant apps! It was a Friday, and I left work early so my wife Sherry and I could visit our son Kai attending college in Madison Wisconsin. I received a message from Ryan Johnson, the VP of Mobile at BuzzFeed (Ryan has since been bumped upstairs into one of our business units). Google was getting a crash with our Instant app running on an older version of Android that we had not tested. Oh yeah, also they wanted to demo it to Sundar Pichai , the CEO of Google, in two hours (no pressure!). I was driving and my wife was messaging Ryan. We looped in Aaron Goldberg from BuzzFeed’s Android team. We stopped at a coffee shop somewhere in the middle of rural Wisconsin (but it had killer Wi-Fi!) and Aaron and I conferred. Aaron fixed the offending call, and we were all set, with one hour to the demo. The demo went well and now we were told that our Instant app might be demoed during the keynote. We were also told, that the producers for the I/O keynote change and cut sections at the last minute, so we shouldn’t get our hopes up. I received an email from Google the evening before the keynote, stating the importance of not discussing Instant apps until the conclusion of the keynote. Sounded very promising. Finally the big moment arrived. Here is the YouTube video of Google’s introduction of Android Instant apps to the world: This was definitely the highlight of my technical career. “I wrote that”, I thought silently to myself, following the non-disclosure directive. BuzzFeed made the decision to embrace Instant apps and go for it. Our starting point was the temporary code branch that had more TODOs and commented out lines of code than we could count. At the same time, BuzzFeed Mobile had two other initiatives underway: Absorb the Video app functionality into our core app, Convert to a mono-repo allowing numerous apps to be built from a single code base. At first, doing all of these simultaneously scared the you know what out of me. As it turns out they fit rather synergistically. The vast majority of the effort to create an industrial strength production Instant app, wasn’t Instant apps specific, but modularizing our now unified code base. Historically Android developers used a simple app architecture of an app module, and perhaps a library module. What we quickly found was it made sense to have smaller specialized modules. Features of the app were separated into their own modules. In addition, it made sense to abstract a feature even further into definitions and implementation modules. For example, we can have a base module that has abstract classes or interface definitions. Then a module that implements the definition for the APK, and a different module implements the definition for the Instant app. We did this only for functionality that varied between the APK and the Instant App (which is now few and far between). This eliminated ugly code such as By using polymorphism (or composition), instead of if/else logic, our code was much cleaner (and compact too)! We only pulled in the code that we truly needed. Another tedious task was using ProGuard to reduced unused code. Again, this was not directly related to Instant apps, but utilized for all released APKs. Because of the need to aggressively reduce the binary image for faster loading, we pursued more explicit matching than may typically be used for a standard APK. Of course, let’s not forget that Google had a few items they needed to enhance for Instant apps to reach full featured status: Google Search, Google Play, Android Studio, The Android kernel. As I’m sure you have heard by now, Android Instant apps is all grown up now and released to the public . Android Studio 3.0 directly supports developing Instant apps. and all the Android Studio goodness is back and then some! Its tools now address the major pain points of ProGuard usage and refactoring the code base into separate modules. After over a year of excitement (and silence) about BuzzFeed’s Instant app, I can tell the world about it. So here it is: Our BuzzFeed Instant app is now available for your viewing pleasure on most newer Android devices! Simply search for your favorite BuzzFeed show, such as BuzzFeed Tasty , Try Guys BuzzFeed or Nifty BuzzFeed. You get the same rich native experience as if the app is installed. One of my favorite moments is witnessing people who see Instant apps in action for the first time. You can talk about Instant apps, but to fully comprehend it you must experience it. BuzzFeed has only begun our Instant apps journey. We are currently adding even more BuzzFeed functionality to Instant apps, so stay tuned. Who knows? It might even be written in Kotlin. Sharing our experiences & discoveries for the betterment of… 167 2 Android Instant Apps Android Studio GoogleIO Buzzfeed Posts 167 claps 167 2 Written by Android, Instant apps, Kotlin Sharing our experiences & discoveries for the betterment of all! Written by Android, Instant apps, Kotlin Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-12"},
{"website": "Buzzfeed", "title": "crafting the product design hiring experience part two", "author": ["Sabrina Majeed"], "link": "https://tech.buzzfeed.com/crafting-the-product-design-hiring-experience-part-two-add787570f8f", "abstract": "Latest Posts Events Apply To BuzzFeed We’ve been iterating on our design hiring process for the past two years at BuzzFeed. Shedding light on what’s worked (and hasn’t) for us internally has had a positive impact on other teams at BuzzFeed, so we’ve decided to make our hiring process and approach public. This is the second in a three part series, and will focus specifically on sourcing and recruiting. To learn more about why our design team is so involved in this process, read part one . In my previous post I stressed how design has taken ownership of the entire hiring process, from end to end. To get a better understanding of what I’m talking about when I describe “the process”, here’s a diagram of what a candidate would experience when they interview at BuzzFeed: In this post, we’ll cover the first half of this process: how we find and screen potential candidates. One of the biggest changes we’ve made in our process is we have stopped publicly posting Product Design job listings on BuzzFeed.com. This may seem counterproductive— after all, BuzzFeed does get a lot of traffic. The problem is that “Product Design” is still a relatively new field, with a title that can be easily misinterpreted for adjacent roles. When we did publicly post listings for Product Design roles on BuzzFeed.com, the majority of the applicants were product managers, marketers, graphic or editorial designers, illustrators, physical product designers, and even mechanical engineers. Digital product designers were a scarce minority, and coming across them in the pool of applicants was much like finding a needle in a haystack. Instead, we recently set-up a casual google form for applicants which can be accessed through Medium , a platform which tends to attract a higher percentage of product designers than our own site. Aside from the time we save from not having to manage a pool of ineligible applicants, not relying exclusively on external job listings has forced us as designers and design managers to proactively reach out and build up diverse professional networks. We can’t blame it on the pipeline if underrepresented candidates don’t apply to work at BuzzFeed. The onus is on us. We find many of our prospects through Twitter, Medium, and yes, even Dribbble. For better or worse these sites have a reputation as cesspools of self-promotion, but that’s why they work for sourcing candidates. In fact, all of these platforms have been a more valuable recruiting resource for us than the de facto professional networking site, LinkedIn. While Dribbble offers quick transparency into a designer’s visual chops, Twitter and Medium are where we find written content about a designer’s work. Ideally through these sources we are also able to find a designer’s portfolio in order to get a more in-depth understanding of their process. These artifacts give us more insight into a designer’s thought process, execution, and approach to design than the standard résumé. We also discover prospects when they’ve engaged thoughtfully with us on any of these platforms. For example, last year Cap tweeted about the underrated challenge of designing tools: At BuzzFeed, our internal tools are some of the most impactful products we work on. Designers who expressed either their experience or their interest in tackling these complex workflows got our attention and were added to our list of prospective candidates to reach out to. Let me illustrate how this proactive approach to recruiting actually increases your reach to potential candidates, compared to a more reactive approach that relies exclusively on incoming applicants: When you rely entirely on incoming applicants, you’re limiting your candidate pool to the small cross-section of people that are qualified, interested, and available. Even if you have a decent number of applicants that meet these criteria, your pool is constrained to the local maxima. Proactively reaching out to qualified candidates and doing your part to convince them of the reasons why they should be interested is an effective way to grow your base of potential candidates. This isn’t to say that prospective candidates shouldn’t apply for the jobs they want and instead wait to be reached out to. Personally, I wouldn’t even be at BuzzFeed if not for applying online. It’s also not our intention for it to feel like this job is exclusive to those already within our network— quite the opposite. What we want is to provide prospective candidates direct access and communication to us, the hiring managers for these roles. So if you’re reading this and interested in a design role at BuzzFeed , feel free to directly reach out to me , Cap , Caylee , or Tom . We want to talk to you! Honestly, there’s kind of an art to writing emails to prospective candidates that actually earn a response. Especially when reaching out to candidates who aren’t actively looking for a job. Interviewing is often a risky, anxiety-inducing, and vulnerable process. You need to convince someone who is currently in a secure position to take a risk and open themselves up for potential rejection. The best way to do this is by being genuine, detailed, and doing the due diligence of actually learning about them before reaching out. Let’s look at this recruiting email I recently received as an example: This is by no means a bad email and if I were already interested in the company it could very well work. Assuming I’m not, I’ve highlighted a few areas for improvement. First, the recruiter mentions being impressed by my background and experience at BuzzFeed. My public LinkedIn profile makes it clear that I’ve had recent experience as both an individual contributor designing apps and as a design manager. The lack of specificity here makes me think that the recruiter hasn’t spent much time actually assessing my background. Second, the recruiter mentions a redesign but doesn’t identify what platform or audience the redesign is for. There’s an implied assumption that I’m also familiar with the company’s work. This makes it hard for me to identify if I’m a good fit for the role, or if it’s an area I’d be interested in growing in. Third, instead of offering me a chance to learn more about the role and get the information I need, I am asked to put together work samples to send over for evaluation. For designers who aren’t actively looking, putting together an up to date and well-documented portfolio is a time-consuming process and the recruiter here hasn’t yet convinced me it’s worth the effort. Here’s how I would have re-written the email: Ideally, what I want to get out of an email like this is a high-level connection between my experience and the company’s current needs and a low-risk opportunity to learn more before deciding to formally pursue the role. As designers we know that by considering our user’s goals and tailoring experiences around them we can better meet our company or client’s business needs. The same can be applied to recruiting. Be mindful of the candidate’s goals and thoughtful in your communication from the very beginning of the recruiting process to the very end. I’m of the philosophy that we should always be ‘recruiting’, even if we’re not actively hiring. We don’t need to constantly reach out to individuals or send cold emails, but we should be periodically engaging with the design community. Some examples of how we do this are through hosting meet-ups like Design Driven , open-sourcing our work , and writing about our process . These are all opportunities for us to share both our work and design philosophy with the industry at large, in hopes that it will connect us with like-minded individuals who we could reach out to whenever we need to grow the design team. If you wait until you have a role that needs filling to take on these types of endeavors, it’s going to take considerably longer before they start working to your advantage. Think of it like an investment. You may not see the return from the first hire, or even the second, but at some point you may notice that more and more people are responding to your cold emails. They’ve heard good things about you, they’re fans of what you’re doing over there, and they want to learn more. After we’ve gotten confirmation that a prospect we’ve been talking with is interested in interviewing for the role, the next step involves two screens in order to vet the candidate before bringing them in for an in-person interview loop. Prior to November, we had been doing a discussion-based “first screen” that focused on teamwork, collaboration, and process. If we felt like the candidate’s approach to design and work was aligned with ours, we would then do a “second screen” in which we ask the candidate to walk us through examples of past work so that we can evaluate their design decisions as well as how they came to those conclusions. We recently flipped the order of these two screens, after seeing a pattern where many candidates were easily making it from the first screen (“how you work”) to the second (“your work”), but very few were passing the second screen and going onto the on-site interview loop. While it seemed logical to arrange the screens in order of increasing difficulty, therefore progressing from a more casual conversation to a review that one has to prepare for, our hiring funnel began to look like this: When ideally, it should be closer to this: One possible explanation for this pattern is that our “first screens” weren't thorough enough, and that’s something we’re currently iterating on. The other is that one of these skill sets is more of a pre-requisite than the other, and for a designer, frankly, that’s the ability to execute. If a candidate’s design work doesn’t meet our expectations then ending the conversation there seems like a fairer and more efficient use of everyone’s time. After we made this change, the initial contrast of seeing so few candidates make it to the second round in the process seemed alarming. Then I realized that before what we were actually seeing was a lot of false positives, and in fact our new process was doing its job in helping us identify the best fits for the roles we were hiring for. Sometimes, if we feel like we need to gather more information about a candidate, we may ask them to do a take-home design exercise before bringing them in for an interview loop. We compensate people for this exercise and try to pick an abstracted prompt that doesn’t require a lot of internal knowledge. We also try to pick a problem that we aren’t currently thinking about today, as we might then fall into the trap of comparing their solutions to our own. A good example of when we might ask someone to do a design exercise is if we feel they have potential, but their existing work may be impacted by forces outside of their control, such as limited opportunities for design in the company’s process and organization. The design exercise is an opportunity to see how a candidate would approach a problem independently of their current company’s structure. We’ve now covered the sourcing and screening process that leads up to the in-person interview. In the next installment, I’ll describe in detail how those interviews are structured and how we make the big “hire” or “no hire” decision at BuzzFeed. Sharing our experiences & discoveries for the betterment of… 277 2 Thanks to Tom Harman and Cap Watkins . Hiring Product Design Recruiting BuzzFeed Tech 277 claps 277 2 Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-26"},
{"website": "Buzzfeed", "title": "heres what a women in tech happy hour is like at buzzfeed", "author": ["Angie Ramirez"], "link": "https://tech.buzzfeed.com/heres-what-a-women-in-tech-happy-hour-is-like-at-buzzfeed-f79f8a0c8b01", "abstract": "Latest Posts Events Apply To BuzzFeed On Thursday March 30th, the Women in Tech group at BuzzFeed hosted a happy hour for women software engineers, product managers, project managers, designers, product support and IT team members, together responsible for maintaining the smooth operation of our internal technology tools, BuzzFeed’s website and apps, and social media sites. The event featured bounteous wine, non-alcoholic beverages, and a melange of vegetable, cheese, and hummus plates, but more importantly, the opportunity for the women of BuzzFeed Tech to have a safe, comfortable space to get to know each other, talk about their current work, and develop relationships that are often crucial to the long-term success of women in technology, an underrepresented group in the industry (30% of BuzzFeed Tech identify as women or gender neutral). After half an hour of casual conversation, Adrienne Fishman, a software engineer who has been at BuzzFeed for two years, led the group in a game that helped us get to know one another a bit better. The game involved learning who in the group you shared certain similarities and differences with, and sparked connections amongst the women that might not have been made in our typical day-to-day. As the game went on, laughter reverberated around the room as attendees shared statements ranging from, “I’ve never dyed my hair,” to more personal statements about family, past work experiences (“I’ve never had a female boss”), and emotional well-being. Concurrently, BuzzFeed Women in Tech in our Minneapolis office held their own event at the cocktail room of a local craft distillery. Attendees swapped stories about work, life and all that lies in-between; there were plenty of opportunities for both bonding and mentorship over mojitos and mocktails. It was the perfect opportunity to get to know one of Minneapolis’s latest women developer hires, Laura Wright, who started in Minnesota last week. The event in New York City concluded at 6pm, but many women stayed to enjoy more wine, snacks, and conversation. I personally met several others I’d never spoken to, but had seen often around the office, whom I now feel comfortable coming to for advice, coding help, and friendship. Past and future women-focused tech events have and will include a small-group conversation with Todd Levy, BuzzFeed Chief Technology Officer, a conversation with Gilad Lotan, Vice President of Data Science, various volunteering events, and of course, more happy hours. For profiles of some of our women in technology, check out “ 25 Ways To Dress Like A Tech Employee ” Sharing our experiences & discoveries for the betterment of… 4 1 Thanks to Jessie Wu . Women In Tech Buzzfeed Posts Diversity In Tech Workplace Diversity Buzzfeed Events 4 claps 4 1 Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"},
{"website": "Buzzfeed", "title": "crafting the product design hiring experience part three", "author": ["Sabrina Majeed"], "link": "https://tech.buzzfeed.com/crafting-the-product-design-hiring-experience-part-three-f20322c821b9", "abstract": "Latest Posts Events Apply To BuzzFeed We’ve been iterating on our design hiring process for the past two years at BuzzFeed. Shedding light on what’s worked (and hasn’t) for us internally has had a positive impact on other teams at BuzzFeed, so we’ve decided to make our hiring process and approach public. This is the last post in a three part series, which will focus specifically on how we structure our in-person interview loops and how we make the big ‘hire’ decision. To learn more about why our design team is so involved in this process, and more about our sourcing and screening process, read part one and part two , respectively. In my previous post I described the sequential steps that help us identify which candidates would be a good fit for BuzzFeed. If the results of our preliminary screens give us confidence, we’ll invite the candidate to come on-site for a series of interviews with the extended team. This is the final round in our interview process. When I first transitioned into management I had less confidence in my own ability to make hard decisions. I’d find myself torn after the screening process. Perhaps the candidate did very well during the portfolio review, but there were several red flags in their screen on collaboration. I was often tempted to invite them to come on-site anyway. I figured that in these situations a second opinion from the hiring panel would help me make the ultimate decision. Unfortunately, more often than not this resulted in more false positives and fewer offers. It was an inefficient use of everyone’s time including the candidate. An on-site interview is not a tie-breaker; it’s purpose is not to help an undecided hiring manager make up their mind. Today, in order to bring a candidate on-site, the team members involved in the screening process need to feel confident that the candidate would receive an offer after the on-site round. If after two screens and the optional design exercise we still don’t have the evidence we need to inspire confidence, it’s as clear a signal as any that it’s time to move on. So what is the on-site interview for, if our minds as hiring managers are mostly made up during the screening process? The on-site loop allows us to get the perspective of adjacent disciplines that designers need to work closely with, like product and engineering, as well as supplemental feedback from additional members of the design team. It’s also an opportunity for the candidate to meet and assess the people they would be collaborating closely with if they were to join the team. Our interview panels are composed of at least five people in addition to the hiring manager: two designers, one product manager, one engineer, and one front-end engineer. We try to keep our panel size small, not only to be conscious of everyone’s time, but because it facilitates trust within our team. A candidate shouldn’t need to meet every individual designer on the team, nor should every designer need to approve of them. This is the benefit of having consistent, documented hiring criteria. The benchmarks remain the same regardless of which designer is or isn’t on the panel, enabling the designers to trust each other’s best judgement. Interview loops consist of a portfolio review with the entire hiring panel, followed by 1:1 interviews with individual members. Two of these interviews are design exercises that make use of a whiteboard. For these, as well as our front-end exercise, it’s less about solving the problem or coming up with The Best™ solution in a constrained amount of time, and more about giving us insight into the candidate’s thought process and approach to problem solving. We also realize how taxing these more performative interviews may be if done in succession. So when scheduling on-site interviews we try to alternate the order of ‘exercise’ interviews with ‘conversational’ interviews to avoid burn out during the already nerve-wrecking interview process. Here’s what a typical on-site interview loop might look like: We feel strongly about setting up candidates for success, and we actually want the people we bring in for interviews to do well. Many companies optimize for current employee performance and setting their people up for success. Yet when it comes to hiring, many companies place the burden on the candidate to ‘prove themselves’ against sometimes unrealistic odds. We prefer to start with the assumption that the candidate already meets our expectations, if they’ve made it this far. Our interview loop is less like a series of tests with increasing difficulty and more like a typical day at work: one in which you may work on some design projects, talk to your product and engineering peers, and present your work to stakeholders. I also recently updated the evaluation rubric we use in design interviews to better align with our role expectations , which we use for performance and leveling conversations with existing designers on the team. The previous scorecard reflected eight different categories, some of them redundant and not always applicable to the role, the candidate, or the interviewer. We’ve since narrowed our evaluation criteria to five categories: Another goal of mine when redesigning our rubric was to make sure the criteria we used to review candidates also reflected the debrief discussion that happens in person. Looking back at historical data, there were some candidates who scored very well against our old rubric but were still not extended an offer. On the other hand, there were candidates with mixed feedback that did receive one. I realized the discrepancy lay in the level of the role (senior vs mid-level vs entry level), which was not well reflected in the previous evaluation criteria. In Greenhouse (the recruiting software the entire company uses), the default scorecard template forces interviewees to rate candidates on this perplexing scale of iconography: This translates to strong no, no, neutral/not applicable, yes, and strong yes. I felt strongly that we shouldn’t ever need to select “neutral” or undecided. The argument for it being there is that there are times when an interviewer may not have enough information to form a conclusion. I believe this is symptomatic of how deliberate an interviewer is during their time with a candidate. To address this I always direct people to Hubspot’s excellent write-up about their hiring process , which I think succinctly sums up the goals of an interview: Decide if the candidate should work at your company. Make the candidate want to work at your company. You should try to make every question and statement work to accomplish one of those goals. “Strong No” also seemed unnecessary for us, because our screening process was designed to identify any “strong no’s” before they even make it on-site. That leaves us with a rather binary “yes/no” scale on the scorecard, but what was happening in our debrief discussions was much more nuanced. Instead of a confident yes or no, most of our conclusions were conditional. For example, “if s/he came in as a mid-level designer I’d say yes, but if we’re expecting a senior designer, I’d say no.” As a result, we changed the scale on which candidates are evaluated to reflect our design roles : associate, mid-level, senior and staff. Each level is assigned a value between 1 and 4. If we have a role that we believe requires mid-level experience, a candidate should need to score above an average of 2 to receive an offer. While I think this is a more objective way to evaluate candidates than a binary scale, it also provides room for nuance. If a candidate averaged out at a 1.8 but shows the potential to learn and ramp up quickly, it would still be reasonable to extend them an mid-level offer. So let’s say we have a candidate that did well during the in-person interview, and we’d like to extend an offer. Our leveling assessment from the interview also informs where the candidate would fall in our established salary bands, which determines their cash compensation offer. We do not negotiate compensation offers on the Product Design team, nor do we offer signing bonuses. Instead we give candidates our best offer upfront. This is part of an effort to ensure compensation is consistent across the entire design team, and is fair to folks who aren’t comfortable negotiating. The result is fewer discrepancies between people doing the same work at the same skill level. Onboarding is the connective tissue between the interview process and the candidate’s experience as a full-time member of the team. We recently laid out a roadmap detailing what a product designer’s first week should look like. From personal experience a few of us on the team noted that when we first started at BuzzFeed, we had a lot of downtime that we weren’t quite sure what to do with. We wanted to alleviate this by giving new designers tasks they could complete at their leisure during their first week, such as simple exercises to introduce them to Solid (our CSS style guide) or the task of writing a BuzzFeed post in order to become familiar with our CMS. We also created an interactive checklist for managers detailing tasks that need to be completed leading up to the new designer’s first day. Another important part of the onboarding experience is social opportunities for the new designer to meet the team, particularly team members who weren’t involved in the interview. Aside from welcome events, a small thing we’ve recently started doing is putting together a collaborative Spotify playlist for the new designer to listen to during their first week. Obviously, “Gather the Crowd” is a required opening title to the soundtrack. We’ve done a lot over the past two years but there are still many opportunities for improvement. Some areas we’re working on next include how to best train and educate our interview panelists. Interviewing well is an exercise in good judgement and decision making — and those are skills that extend their value beyond recruiting. It goes without saying that what works for us may not work for every company. Larger design teams hiring at scale may understandably need to offload more of this process to their recruiting and HR partners. Maybe one day we’ll be in that boat too. But for now, taking responsibility for the end-to -end hiring process has allowed us to quickly grow our team, stay connected to our industry at large, and keep ourselves accountable for our process being 💯. By being so involved in hiring I’ve gotten to meet so many talented designers. Whether or not they end up at BuzzFeed, they’ve helped us shape and improve our process and I hope we’ve been able to make the typically intimidating process of interviewing a rewarding experience for them as well. Interested in joining the BuzzFeed product design team? Read more about the candidate experience & apply here . Want to nerd out more about design recruiting? Hit me up on Twitter . Sharing our experiences & discoveries for the betterment of… 176 2 Thanks to Cap Watkins and Caylee Betts . Hiring Product Design BuzzFeed Recruiting Tech 176 claps 176 2 Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Written by Design Manager at GitHub Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-25"},
{"website": "Buzzfeed", "title": "deploy with haste the story of rig", "author": ["Matt Reiferson"], "link": "https://tech.buzzfeed.com/deploy-with-haste-the-story-of-rig-ca9a58b5719a", "abstract": "Latest Posts Events Apply To BuzzFeed BuzzFeed was launched over ten years ago with a small engineering team supporting a single monolithic application that powered buzzfeed.com. This worked great and was absolutely the right choice given the requirements at the time! Fast forward to 2015, a year where the engineering team nearly doubled. The tools and interactions that previously worked weren’t scaling with our expanding team, infrastructure, and products. Our release-driven approach to deploying the monolith was beginning to really show its age. Releases had grown in size and scope until they required a designated “manager” who was responsible for shepherding it into production. We soon discovered that large, infrequent deploys , coupled with a general lack of observability, greatly increased the risk of every deploy by making production issues difficult to debug and resolve. At its worst, it took us days to deploy and validate a release. Simultaneously, our company focus and suite of products were evolving. The growing popularity of our video content and a general shift toward a distributed-first content strategy meant that we were building systems (and engineering teams to support them) outside of the buzzfeed.com monolith with vastly different requirements. We felt that we needed a service-oriented architecture that better reflected our maturing distributed organizational structure and desired workflow. At this point, we lived in a world where building a new system involved a series of high-friction coordination points in the form of JIRA ticket queues, manual resource provisioning, and a little dose of “doing it live”. The process was woefully inconsistent, lacked transparency, and was really hard to predict. It was a classic game of hot potato with a huge wall in the middle. A development and deployment pipeline affects everything you do — when it’s cumbersome and inefficient it makes it hard to experiment and iterate, and can overwhelm you with technical debt. Teams are forced to just get it done , eliminating opportunities for consistency, conventions, and standards. We also needed better abstractions. Most importantly, we wanted to define a “standard interface” for an application and, in return, provide a guarantee that if it met those requirements it would Just Work™ . We didn’t want to require teams to touch byzantine config management code or say the magic words in order to deploy . Finally, we wanted teams to be more engaged after deploying, when systems are in production. We wanted to foster an engineering culture where we collaborate on debugging and resolving production issues, and that responsibility doesn’t fall squarely on Site Reliability. In order to do that well, we needed all services to be instrumented and monitored — and needed tools to be widely accessible and easy to use. Observability should be a natural side effect of building an application in a standard way. So we set out on a journey to improve the “engineering experience” — but what on earth is that? The availability, ergonomics, reliability, and effectiveness of the tools and approaches used to develop, validate, deploy, and operate software systems. Basically, we’re answering this question: How often, in the process of trying to build a product, do you want to throw your laptop? We want that number to be zero . It all started during a hack week in January 2016. A few of us in the Infrastructure group decided to build a proof-of-concept PaaS. (Yes, everyone on earth is currently building their own PaaS. See Why Didn’t You Use X? for why we built one, too! ) Having only five days to get something working we needed to take some shortcuts. Obviously, we spent most of those five days agonizing over the name. In the end, we came up with rig : v. Set up (equipment or a device or structure), typically hastily or in makeshift fashion Seems appropriate, right? We felt pretty strongly about a few key properties and requirements: Autonomy : Provide standards and tooling to accelerate product engineering teams. Freedom : Creating and deploying a new service should require no coordination. Make it easy to do the right thing and trust users by default. Efficiency : Deploy applications to clusters of homogenous compute resources. Pragmatism : Build on top of AWS’s battle tested platform of EC2, ELB, and their container scheduling service, ECS. Completeness : Standardize and control the pipeline from development, CI, and production, including management of secrets. Observability : Out-of-the-box support for system and application distributed logging, instrumentation, and monitoring. Taking inspiration from the high-level application abstraction found in PaaSTA , we began by building out a CLI in Python to serve as the entrypoint for users. We quickly adopted a basic set of conventions, e.g. a service is a top-level directory in the mono repo that contains: service.yml — the intrinsic properties of a service, e.g. CPU/MEM reservation, instance counts, network interfaces, and organizational metadata. Dockerfile — container definition to be built by CI. config.yml — plain text application configuration. secrets.<cluster>.gpg — per-environment GPG encrypted application secrets (inspired by blackbox ). Additionally, we established some runtime requirements: It must execute as a stateless process . It must read config from the environment . It must output logs to stdout/stderr . It cannot expect any local state to be persisted. It must be robust to failure and startup and shutdown quickly , expecting that to happen at arbitrary and unexpected times. HTTP services must implement a health endpoint. We created a VM-based development workflow to provide a consistent, repeatable, and ephemeral environment to run our tooling and infrastructure. Inside the VM, the CLI serves as the primary interface. Running a service is as simple as rig run foo , including managing dependencies like mysql, redis, and other services. This is possible because of the conventions laid out above and the convenient packaging and tooling provided by Docker. Additionally, rig supports live code reloading when running a service to preserve a fast feedback loop during development. This works because we mount the repo into the VM and into the container. Running tests is similarly trivial ( rig test foo ). As you push commits up to your branch, builder (a bundled rig service based on Jenkins ) integrates your code, performs global sanity checks, builds the container image, runs tests, and pushes the image to a container registry. Finally, deploys are initiated from a web UI, where you select the service, the versioned container image, and a target cluster. The service is then “scheduled” by ECS onto our EC2 instances registered in that cluster, based on the service’s configuration (e.g. its CPU and MEM reservation and desired number of instances). For new HTTP services, this process also provisions the load balancer (with TLS) and DNS entry. This process typically takes no more than a few minutes end-to-end, depending on the size of the container image and test suite, even for brand new services! You can imagine the impact this might have on an organization struggling with weeks long provisioning and deployment! Unfortunately, a proof of concept does not necessarily make for a production worthy system. There were significant reasons to transition to rig, however it wasn’t without risk. Primarily, we were concerned about operational risk. For example, we hadn’t previously used Docker, a relatively immature technology, or ECS, a fairly new AWS managed service, in a production environment. Despite its immaturity, one of the most attractive qualities of ECS is its support for “bringing your own infrastructure”. This meant that we could leverage existing rock solid AWS services like EC2, ELBs, RDS, ElastiCache, etc. and allowed us to retain tight control over the hosts, including well-understood topology and lifecycle primitives (VPCs, SGs, and ASGs) and the base OS and software they ran. This helped us to gain confidence more quickly, as long as we trusted the “black box” that is the ECS managed scheduler. Still, we had questions around stability, operability, observability, and security — things like: How does an ECS cluster handle host failure? What happens when the ECS agent or Docker daemon fails? At what granularity should we monitor containerized services? What is the network performance impact of Docker bridge networking? We had previously experimented with Terraform to manage our cloud resources. We felt that its declarative nature would enable transparency and support for peer review in infrastructure changes. We doubled down on Terraform for rig, using it to provision all of its underlying infrastructure. This provided an automated and repeatable method of provisioning clusters, which we used to quickly stand up and test hypotheses at the infrastructure level. We addressed observability in three ways: Distributed logging : We shipped all logs to Papertrail , tagged and searchable by service, version, and cluster. Instrumentation : We integrated DataDog , which has deep support for Docker, but also acts as a standard statsd receiver, aggregating metrics from instrumented services. Monitoring : We built monitor (another bundled rig service, based on Nagios), which automatically discovers hosts, services, and other resources running in the cluster, configures alerts, and routes notifications based on service metadata to Slack, PagerDuty, etc. At this point, we felt like we had good monitoring coverage and were collecting the insights we needed to debug failures. After running all of our operational exercises, we selected low-risk, minimal-workload systems to migrate first. This allowed us to gain experience in production , without compromising critical workloads. Let’s start with some numbers. Rig became “generally available” in April of 2016. Since then, we’ve observed double-digit percentage growth in the number of production services each month. By February 2017, we had 227 services in production! We’ve also executed 18,228 deploys since June 2016 (when we started tracking them). That averages out to ~150/day! The sheer number of services and deploys is evidence of the effect it’s had on our rate of delivery. Culturally, it’s been nothing short of a game changer. As operators, we have enormous leverage because of the consistency with which services are configured, deployed, and run. Investments we make in rig have a huge multiplier, e.g. we added service auto-scaling, and instantaneously all services on rig could benefit from that feature. Product engineering teams feel ownership and empowerment to do their jobs efficiently and effectively, with more time to focus on the “business domain” of the task at hand. If you lower the cost and overhead of building and deploying a new service, you encourage low-risk experimentation and iteration — the ingredients that have made BuzzFeed successful. Lastly, the efficiency and utilization of our infrastructure has increased dramatically as we schedule services in Tetris-like fashion , vastly reducing costs and allowing us to invest strategically in homogenous reserved capacity. In short, because ECS, Kubernetes, Docker Swarm, and the like are just building blocks, and we wanted to expose set of opinionated, higher level abstractions to our users. The tools in this space rarely provide a robust development environment or have answers for CI, secret management, or monitoring, which means, no matter what, you’re left gluing the puzzle pieces together. Rig is that glue, and provides solutions for all of these problems! We didn’t build a container scheduler (ECS), nor a CI system (Jenkins), or even observability tools (Nagios, DataDog, Papertrail) — we just developed a cohesive UX around a set of robust AWS services and established open source projects and gave it a damn good name . Our work isn’t done! There’s really no better way to learn about a system’s pain points than to methodically operate and use it in production over an extended period of time. To get feedback from our internal users, we’ve set up support channels in Slack, run regular office hours, given tech talks, and performed user research through interviews and surveys. Treating infrastructure like an internal product has helped us better understand everyone’s needs and establish a roadmap that blends our vision with changing requirements. Operationally, we’ve learned that working with Terraform at scale is hard . We’ll talk about this in future posts, but suffice to say that although it has numerous benefits we have yet to resolve some of its painful workflow issues. For example, even though it’s automated and repeatable, we want to make it vastly simpler to provision a rig cluster from scratch (we’re very much inspired by kops — perhaps we should just use Kubernetes?). From a user’s perspective, working with GPG-encrypted secrets isn’t a very friendly or intuitive process, and given that it’s one of the first onboarding steps, we want to simplify or eliminate this friction. Also, although our approach to secrets has been effective, we want to more easily be able to audit and rotate them, which is still far too difficult and time consuming. The shift to rig greatly accelerated our transition to a service-oriented architecture. In future posts we’ll talk about other tools and systems we’ve built that help us observe and control the complicated interactions of this growing distributed system, namely our API Gateway . Finally, we intend to open source all of this in the near future! The hypocrisy isn’t lost on us (see Why Didn’t You Use X? ). We’ve got lots of questions about the purpose and value of open sourcing rig — it seems like a huge ask to adopt all of our silly opinions. Still, you just might happen to agree with our approach, in which case we’d love for you to benefit from rig too! At a minimum, sharing our implementation might inspire you to build your own (thanks again, PaasTA). Obligatory “if this sounds like the kind of thing you want to work on”, we’re hiring ! P.S. no blog post would be complete without first thanking Wilbur McClutchen. Truth be told, rig has been a significant group effort. Big shoutouts to the Site Reliability and Platform Infra teams for delivering, scaling, and supporting all of this! Sharing our experiences & discoveries for the betterment of… 426 1 Docker DevOps Buzzfeed Posts Paas Infrastructure 426 claps 426 1 Written by MS-DOS C Engineer Sharing our experiences & discoveries for the betterment of all! Written by MS-DOS C Engineer Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-03"},
{"website": "Buzzfeed", "title": "architecting trust in client side analytics part two", "author": ["David Mauro"], "link": "https://tech.buzzfeed.com/architecting-trust-in-client-side-analytics-part-two-f54909a75b47", "abstract": "Latest Posts Events Apply To BuzzFeed In the first part of this two-parter, we explored the pieces that made up our analytics system for our iOS app. Now we’re following up with an exploration of how we unit test and QA that work, and we’ll also look at what this all looks like in practice with some code samples from our app. Getting back to data trust , let’s dig into how to best unit test everything as well as make things simpler for QA. Our implementation is certainly designed to reduce errors, but because they’re inevitable, testing is crucial. Two types of unit tests When testing our analytics it’s important to break up the testing responsibilities. There are two main areas that need to be tested: That tracking event calls are made at the appropriate times (e.g. did we track the screenView event when the screen appeared?). That the correct data is sent for a given event (e.g. do screenView events sent to the GoogleAnalyticsObserver result in the correct screenView string value being sent to GA?). It’s important to keep these two testing concepts distinct. It’s easy to fall into the trap of validating the data at the same time that you check that an event happened, but that will make your tests more fragile, with more duplicate code, and will be harder to update when your analytics needs change. Test your tracking calls For these tests, we’ll use a mock observer that keeps track of every event it was told to track, and then ask it if it received the event we expected. It’s about as simple as you’d expect: And now a test asserting that a screenView event happens on viewDidAppear . Testing provider data integrity The next step is to validate that if we track a screenView event, GA will receive the expected data. To do this, we’re going to rely on the receipts discussed in the previous post. This is a simple matter of directly calling the observers tracking method and asserting against the receipt it returns: Prepping for QA That last step for ensuring the trust of your analytics is making it very easy for your QA team to verify the events are firing with the correct data. For this we’re going to add a way to query for particular receipts (which we’ll also be using for QA). It’s not super helpful to dig into the details of what the ReceiptFilterProtocol looks like other than to say it allows us to get events that match particular providers, event types, timestamp windows, etc. The getEventReceipts(filter: ReceiptFilterProtocol?) method we added to our AnalyticsController gives us an easy to way to get all of the receipts in reverse chronological order, so we just need a system for displaying them. The details here also aren’t important, but for example: we display a table where each receipt gets its own entry. You can tap on a cell for details about that event (the metadata contents), and you can filter the list down to particular providers and/or event types. We hide this feature in our settings in internal builds: So how does this stuff look in practice. Here’s an actual code-sample from our app: You’ll notice we’ve broken out the associated data into specific types. This is helpful because those types actually get re-used. For instance, feedPaginationLoad and screenView events use the same associated data structures. It also gives us a way to name those properties without using named tuples, which can get out of hand pretty quickly. We use a lot of enums here because they provide us a way to a) strongly type everything , b) associate data with those types (which is often another enum type embedded within) and c) switch over those types so we get compiler errors when we add new cases (your analytics needs will grow over time, so design for it). We’ll typically use extensions on our AnalyticsController to handle creating these data structures and handing them off for tracking. Here’s another example from our app: There’s a lot of simple logic that just converts the parameters into the data our analytics system needs, so keeping that logic in extensions like this helps keep our view controllers and view models a little leaner and naive about the analytics logic. This will make it easier to find what needs to change as your data requirements are updated. There are other crucial factors we’ve addressed, as well as plan to continue improving on in the future, that are equally important to the overall health of our analytics systems. We’ll briefly mention some of these other aspects to consider, but leave deeper discussion for follow up posts. Data Process Before beginning this overhaul, we worked very closely with our data team for establishing strict data processes that explain how we add new event types and providers, and we audited our documentation for existing events. When new features are introduced, we ensure the data team is roped into discussions about what kind of data we need to collect, and ensure that our cross-platform analytics documentation guide is updated before we begin development. Backend Monitoring and Alerting For our in-house analytics tools, we set up monitoring so that we’re notified of unexpected trends in our critical data streams. We integrate these monitors with Slack so that particular channels can receive ad-hoc, daily and/or weekly updates on the health of a particular event or provider. This step is particularly important for helping us sleep at night. End-to-End Acceptance Tests This is something aspirational we’re working towards, but we’d like to run UI tests and then check that they result in the expected data sent out for a given provider. Right now our QA team is essentially doing this manually, but we hope to alleviate this burden with as much automation as we can reasonably manage. We’ll follow up in future posts! Ultimately, it’s important to circle back to our users. We’re collecting this information so we can create a platform that they’ll use and enjoy, so we shouldn’t lose sight of the fact that we’re serving them. Collect only what you need to better serve them, and do not collect any data that could put your users at risk. Use client identifiers that are associated with the app installation and not the device itself, and do not collect personally identifiable information. Sharing our experiences & discoveries for the betterment of… 11 Swift iOS Buzzfeed Posts 11 claps 11 Written by Engineering Manager for BuzzFeed’s Apps Team Sharing our experiences & discoveries for the betterment of all! Written by Engineering Manager for BuzzFeed’s Apps Team Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-10"},
{"website": "Buzzfeed", "title": "checklists an operational gift", "author": ["Sri Ray"], "link": "https://tech.buzzfeed.com/checklists-an-operational-gift-aaf42cf0be12", "abstract": "Latest Posts Events Apply To BuzzFeed At BuzzFeed we love lists. I mean, we helped pioneer the list format! Lists are easily digestible and are clear and concise in their meaning. They are no replacement for a long form read, but hey, nobody’s got time to read a leather bound novel by a fireplace during a production operation. When a tech team is faced with an operation that involves production systems, it involves strategic thinking to execute steps with no downtime. There is little room for error. No production operation is too big or too small for a checklist. Similarly, no situation is too strenuous for one. At BuzzFeed, we have resorted to writing a checklist for the most trivial of operations as well as in crisis situations putting out fires. Taking a few moments to jot down steps and getting peer feedback goes a long way in ensuring a smooth process. Checklists are the best thing since ice cream was invented. The scientific usefulness of ice cream in a production operation is arguable but trust me on the checklists. However, I will say, #everythingisbetteraftericecream Use numbers ! Alphabets are cool, but can also be misheard — “z” is zedd to me, but probably zee to many of you. Avoid sub-lists . It’s hard to reference something like “2.a”. Every step is important. If it’s not, it probably doesn’t belong in your production checklist. Write out full commands in the list. In a crunch, you want to be able to blindly copy and paste commands instead of fumbling around figuring out what flags and options you may need. If code needs to be merged in link to the appropriate pull request. [sricola] Prominently mention who is responsible for each step. Try out the checklist in a non-critical environment and iterate as needed before getting to production. Build in logical breaks-points in the checklist where the system is stable to give people a breather. Always keep a back out plan in mind while writing out steps. Mention clearly if the back out plan ceases to be an option after a certain step aka the point of no return . Get peer feedback and address points of concern prior to execution . Lets face it, life never goes as planned. Having a handy back out plan is always a good idea. A back out checklist should adhere to the same standards as the rollout checklist above. Again, I cannot stress this enough, make it as verbose and complete as possible. Write out full commands on each step. Under pressure, the most trivial of steps take on a high degree of complexity. Although we always strive to preserve a rollback strategy, sometimes (more often than one would like) going backwards might be impractical or impossible. Be extremely aware of this and mention it in the rollout checklist. We call this the point of no return . Driver : The person who calls out the steps and keeps everything in sync. The driver controls the operational flow and makes final decisions on when to proceed to the next step. Ideally, the driver does nothing but drive the list. Executor(s) : People assigned to each step to execute the line item. Watchers : You should have more than one of these. These are people who have knowledge of the system and are watching on the sidelines to call out any anomalies in logs or monitoring. OMFG Officer : This is a largely decorative position (typically our very own Todd ). This person has the power and confidence to make an executive decision if something goes south and drastic measures have to be taken. This person usually has a good sense of the business impact of taking particular actions and has the ability to reach out to external stakeholders in a crisis. Make a dedicated chat channel and invite appropriate parties. Clearly communicate the checklist. Hop on a voice only call . Video calls are strenuous on your laptop’s CPU, memory, and your internet connection. The Driver should call out each step verbally (and in chat) and make sure there is no objection from anyone. The Executor should confirm that they are running the step and paste the command into the chat room. This allows the Watchers to know which step is in progress. The Watchers should paste any and all confirming or opposing output and logs into the chat room . Cross out or check off completed steps. Do NOT modify the checklist mid-operation unless it is a do-or-die situation. If something goes wrong take a moment to step back and re-evaluate . To reiterate: No production operation is too small or too big for a checklist. Similarly, no situation is too strenuous for one. Always write one — it ensures smooth process! Sharing our experiences & discoveries for the betterment of… 42 Checklists Ops DevOps BuzzFeed Buzzfeed Posts 42 claps 42 Written by Travel. Food. Ice Cream. Tech. | Former @fastly @buzzfeed @bitly @knewton | www.sricola.com Sharing our experiences & discoveries for the betterment of all! Written by Travel. Food. Ice Cream. Tech. | Former @fastly @buzzfeed @bitly @knewton | www.sricola.com Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-18"},
{"website": "Buzzfeed", "title": "architecting trust in client side analytics part one", "author": ["David Mauro"], "link": "https://tech.buzzfeed.com/architecting-trust-in-client-side-analytics-part-one-2625567cc57c", "abstract": "Latest Posts Events Apply To BuzzFeed Last year many of us here at BuzzFeed spent a lot of time and effort overhauling just about every aspect of the analytics tracking in our flagship iOS app. Getting this stuff right is deceptively difficult and it’s something we were able to do only by first getting it wrong. What follows is my attempt to summarize the central challenges of implementing analytics tracking on a mobile client and how we overcame them. I’ll focus on some general themes that are relevant to any platform, while diving into more technical details about our iOS/Swift-based solution. Part 2 will get into how we unit test this system and show some examples of it in action. Getting to know the problem Analytics data about how our apps are being used is extremely important to us at BuzzFeed. We use this data to monitor the health of our owned and operated platforms, to measure the success of new features and content, and to power a rapid learning cycle. On the client-side, we are responsible for the integrity of this data from the time a user generates an event, until we successfully send that data to our data providers , such as Google Analytics (GA) and Quantcast. To make the problem more complex, we expect these data providers to change over time, as well as the events we’re monitoring. Because this data is so important and the demands on the client are so dynamic over time, we’ve been thinking a lot about the trust we have in our data. BuzzFeed’s mobile app users are typically our most engaged users, so any flaws in the analytics tracking logic will have an outsized impact. If one of our apps ships a bug that affects the analytics data we are sending, we will lose trust in all of the data. It can be very difficult, sometimes even impossible, to recover lost data on the backend. So while it is important from a product perspective to determine what to track, it’s up to us as developers to get the how right. If we are to trust it, our analytics system needs to be extremely safe and robust. The hidden nature of analytics can make spotting these bugs much more difficult than spotting the kinds of user-facing issues your engineers and QA team might be used to looking for. That is why it is important that in addition to unit testing everything , you also provide the tools your team needs to help find the inevitable bugs. In building a trustworthy client-side analytics system, we’ll have three central concepts, or models: the observer , the receipt , and the event . Separating providers The observer is essential for decoupling the generic events that the app dispatches and the provider-specific details derived from those events. Keeping provider-specific logic in dedicated observers helps us write SOLID code. In the next post I’ll discuss how we take advantage of this to write less fragile and DRY er tests. Each provider we want to send analytics data to will have its own observer responsible for: listening for events, sending data off to the appropriate provider, and sending back receipts. Essentially, it’s transforming a platform agnostic event data structure into the data structure that a particular provider needs. The receipt an observer returns is optional because some providers won’t care to observe a particular event, in which case they will not return a receipt. Keep your receipts This concept of the receipt is central to our system. Anytime we report an analytics event to a provider, we will generate a receipt for that event, e.g. if we have a sessionStart event that is reported to both GA and Quantcast, we will generate two receipts. We can then use those receipts to validate the event in two ways: 1) to unit test that we’re sending the appropriate data to GA and Quantcast for that sessionStart event, without needing to mock those providers’ APIs, and 2) we can display the receipts in-app for our internal builds so we can manually validate them without having to sniff network traffic. Your receipt will want to strongly type some things for easier identification and inspection (time the event occurred, provider name, etc.), but most of the provider-specific details (the “screenName” for a GA screen view for instance) will need to get packed into an arbitrary dictionary since the structure of data will vary from provider to provider and event to event. Here’s what our receipts look like: Generic Event Data Some providers require certain fields to be sent along with the event data. The important thing to keep in mind here is that the data requirements vary wildly based on both the event type and the provider. Because these requirements vary, we use an enum with associated values (rather than a single rawValue type). This gives us the type safety we need, while also getting flagged by the compiler when we add a new Event case that is not handled by any of our observers (assuming you never use a default clause). Here’s what that enum might look like: The ScreenType mentioned above isn’t terribly important, but here’s an example of what it might look like: The details aren’t important here, but you’ll notice there’s no “screen name” specified yet. Those are the sort of provider-specific details that the Event doesn’t need to know about and we’ll leave to each observer to deal with. Now that we’ve gone over the central concepts of observer , receipt , and event , we can combine them for a simple, testable, and QA-able example. We have two events, sessionStart and screenView . For now, we'll make a single GA observer that only cares about reporting the screenViews Provider-specific data GA will typically want a specific string value that represents the name of the screen we’re on, so we need a way to get that from a ScreenType (the associated data type of the screenView event). We want to do that while keeping this data isolated from other provider-specific data we might want for other provider integrations, so we’ll use an extension: It’s important that we did this separately as opposed to say making those strings the rawValue of the ScreenType . If we were to do that, we’d be coupling the reporting GA does to our generic event data, making it more difficult to integrate with other providers. Essentially, “home” is the appropriate screen name for GA, but it might be, for instance, that another provider requires a specific Int to identify the screen being viewed. Creating our observer Now that we have the data we need to report on screen views, we can build our first observer. Planning for more providers Because we fully expect to introduce more observers later (for instance a QuantcastObserver for sessionStart events), we’re going to make a single controller that will own all these observers. Users will tell it to track an event and it will be in charge of passing it down to its own observers. You can see that this is pretty straightforward, other than the usage of a thunk to deal with some generics details I won’t go into — just create an AnalyticsController , add your observers, then send events you want tracked. One important detail to note is that we kept track of the eventReceipts in a private variable. This will be relevant in the following post when we’ll come back to those receipts and how we use them to help us QA and unit test our system. And then we’ll follow up with some code samples of how this was actually implemented in our app. Sharing our experiences & discoveries for the betterment of… 8 Swift iOS Mobile App Development Analytics Buzzfeed Posts 8 claps 8 Written by Engineering Manager for BuzzFeed’s Apps Team Sharing our experiences & discoveries for the betterment of all! Written by Engineering Manager for BuzzFeed’s Apps Team Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-10"},
{"website": "Buzzfeed", "title": "2017 buzzfeed tech internship", "author": ["Bryan Hughes"], "link": "https://tech.buzzfeed.com/2017-buzzfeed-tech-internship-33615dbf11dd", "abstract": "Latest Posts Events Apply To BuzzFeed We’re leading the charge to a new digital future. We’re developing new creative processes and practices, new formats and mediums, and the very best user experiences for the biggest audience ever assembled. Have you ever wondered how we do it? Well, come join our engineers as an intern this summer! Experience what it’s like to be part of an innovation obsessed culture building a global, cross-platform network to deliver news and entertainment to an audience of billions. You’ll work on high-impact projects across modern tech-stacks surrounded by those who want to teach and help you learn. Our program is ten weeks long with positions available in Los Angeles, New York City, Minneapolis and London. Meet last year’s talented bunch of interns and read about their experience as BuzzFeeders below! Apply here ! www.buzzfeed.com www.buzzfeed.com If you have questions, please email Bryan or Tara: Bryan Hughes bryan.hughes@buzzfeed.com Tara Motazed tara.motazed@buzzfeed.com Sharing our experiences & discoveries for the betterment of… 10 Tech Internships BuzzFeed Buzzfeed Tech Jobs 10 claps 10 Written by Sharing our experiences & discoveries for the betterment of all! Written by Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-06"},
{"website": "Buzzfeed", "title": "5 amazing reasons to choose monolithic version control", "author": ["Brian Seitz"], "link": "https://tech.buzzfeed.com/5-amazing-reasons-to-choose-monolithic-version-control-3215a29daecf", "abstract": "Latest Posts Events Apply To BuzzFeed What is your favorite thing about working at BuzzFeed? Since I started a few months ago, I’ve heard this question a lot. If I’m talking to someone who never spent a Friday night recompiling their Linux kernel, I might say something like all the animated gifs in official email correspondence or the coffee machine that automatically grinds beans on-demand. But if you catch me in a sincere moment, I will say without hesitation: monolithic version control. Choosing a repository structure is like choosing a god. You can debate the technical pros and cons ad nauseam, but it is partially a cultural decision and partially a leap of faith. Inertia is usually a factor, too. It’s like any other holy war : Vim vs. Emacs Tabs vs. Spaces Dogs vs. Cats Reasonable people can disagree on Vim vs. Emacs but also collaborate on the same repository. You can even disagree on tabs vs. spaces if you never edit the same file (or even if you do ). But you can’t disagree on repository structure. A choice is necessary. The default choice is to use separate repositories. A small repository is free to create and easy to maintain in 2017. If, like me, you are old enough to remember CVS , you know this wasn’t always the case. On the other hand, maintaining a monorepo is not easy. It requires thought and coordination. You cannot make up your own standards, choose your own build system, or pick your own workflow. You need to agree on a lot upfront. It may seem oppressive, especially if you’re used to working on a small team with its own repository. Everyone also needs to be aware of how their actions in the monorepo affect others. Even at the moderate scale of hundreds of services, technical issues begin to crop up more frequently. And when they do, they affect the whole company, not just one team. For example: If you store some binary files in an individual Git repo, it usually works fine. But if everyone is doing that in a Git monorepo, the repo size might soon become unwieldy. You will face scaling challenges no matter what repository structure and version control system you choose. But if you use individual repositories, usually you will face fewer problems initially. So why should you defy inertia and choose a monorepo? Isn’t it easier, safer, and faster to just keep everything separate? Here’s some amazing things that will happen when you work on a monorepo. The first thing I realized about monolithic version control is that it’s the ultimate onboarding hack. When you have no idea what you are doing, where anything is, or what the right way to do something is, reading other people’s code can answer many of your newbie questions. The more examples you have, the more likely you’ll find what you’re looking for. When every team’s code is already checked out and a simple grep away, looking for answers this way becomes a new reflex. By contrast, in organizations with many separate repositories, another team’s code is frequently treated as a foreign entity. It’s a black box at the other end of an arrow in an architecture diagram. If you get curious, you can start asking questions: Can you tell me where your repo is? Can I have read access to your repo? Can I submit a pull request to your repo? But the more curious you get, the more likely you are to hear the answer no . If you work for a spy agency, maybe the default answer should be no. But usually source code doesn’t need to be so classified. A monorepo sets a cultural norm that the default answer to Can I have access? is yes , at least when it comes to code. If you want curiosity to be rewarded rather than stifled, yes is the better default. My favorite rule of the BuzzFeed monorepo: Master is golden. Code can’t be merged to master until it’s been code reviewed and verified on staging. Soon after that, it’s typically released to production. This means as an outside observer of another team’s service, I don’t need to know anything special about their workflow, versioning, or release schedule. It also means I don’t have to do my least favorite thing and the thing I’m most likely to forget: manually bumping version numbers. The only version that matters is HEAD of master . Software systems are complex. It is a full-time job to understand how a system interacts at just one commit hash. When you introduce the additional complexity of dependency resolution, it becomes many times more difficult to debug problems. For shared libraries, you may still want to use semantic versioning. But at the service level, it’s much more sane to have a unified view of the live system. My second favorite rule: Write good commit messages. When I’m developing code, the last thing I’m thinking about is commit messages. Sometimes I write bad ones. And I know I’m not the only one who does this. If you have your own repository, it can be tempting to leave those bad commit messages in as you merge to master . This is especially true when you’re pressed for time and working on a live system. I’ve done this on countless occasions. But when your commits will be shared with the entire company, it becomes important to clean up your history. Each commit should be a self-contained feature or bug fix. And each commit message to master should explain what the change does in detail. At BuzzFeed, we use this guide as a starting point to writing better commit messages. “A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable.” — Leslie Lamport Here’s a counterintuitive claim: The more micro your services are, the more compelling monolithic version control is. If you have a few large applications, go ahead and put them in a few separate repositories. But if you have hundreds of microservices, you should put them in a monorepo. Just because your services are micro doesn’t mean the total amount of running code needed to develop a new feature end-to-end is micro. The surface area of what you need to understand is the same or greater. The more monolithic a system is, the more likely that surface area is limited to a single application that you already know how to build and run. But in a microservices architecture, you cross into different services frequently. You become affected by “a computer you didn’t even know existed.” If you want to build a multi-service feature or debug a systemic problem, eventually you’ll face an existential crisis: Hell is other people’s build processes. If you only have a few large applications, maybe it’s worth spending an afternoon creating the right build environment for all of them. But if you have hundreds of microservices all with different build processes, it’s not feasible to run them all locally. You might give up and say I’m blocked and file a ticket with another team. Or maybe you’ll launch code to staging and see what happens when your code interacts with its upstream and downstream dependencies for the first time. This makes for a long and error-prone development cycle. Here’s a different approach: Use a monorepo that everyone has access to. Leverage VMs and containers to standardize how to build and run services in a way that works both locally and in production. Enforce that standard via continuous integration in the monorepo. Now the “computer you didn’t even know existed” is just another service you can see, run, and modify right on your laptop. And if you want to modify it permanently, you can submit a pull request. It’s possible to be this strict about standards without a monorepo, but I’ve seen it promised a million times and delivered exactly never. Once you have one rogue repository, you will soon have many more. Your standard will eventually be lost and forgotten. Synergy is an easily-mocked word, but it has a genuine meaning: an interaction that creates an effect greater than the sum of separate efforts. If someone tells you a light bulb manufacturer and a fishing company can create synergy in a joint-venture, you might want to short their stock. But delivering production software systems is fundamentally about creating synergy through the interactions of code that was built by many different people. Because good software development and system design is frequently about modularization, it’s easy to forget the final product is not about how great the individual pieces are. Sooner or later, your code will need to work together. Different levels of testing can help predict how services will interact, but the only true test is actually running them. To me, that’s the fun part. I can’t build all of BuzzFeed by myself. But I can build a small piece of it. Seeing how that piece fits into the greater whole is my greatest motivator, and it’s most easily accomplished with a monorepo. Sharing our experiences & discoveries for the betterment of… 47 2 Git Github Buzzfeed Posts 47 claps 47 2 Written by Software engineer in NYC Sharing our experiences & discoveries for the betterment of all! Written by Software engineer in NYC Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-21"},
{"website": "Buzzfeed", "title": "analyzing 10 million inauguration emoji", "author": ["Gilad Lotan"], "link": "https://tech.buzzfeed.com/analyzing-10-million-inauguration-emoji-d23ce9ae2876", "abstract": "Latest Posts Events Apply To BuzzFeed During Donald Trump’s inauguration ceremony, we asked our readers to answer a series of questions on BuzzFeed using only emoji. Over the period of a few hours, we received hundreds of thousands of responses, tens of thousands of distinct emoji sequences, and a whopping 10 million emojis. The data spells it out loud and clear: The pouting face emoji has been chosen by you all as the official Donald J. Trump emoji. One way for us to gauge the diversity of sentiment among all responses was to model the emoji sequences as a network graph. Graphs give us a great way to understand relationships between items and identify clusters — regions of dense connectivity. The graph above represents the relationships between all emojis submitted in response to the question “What do you think about Trump’s speech?” Each circle (node) represents an emoji, the lines between them (edges) represent the number of times two emojis appeared together in a sequence, and the colors represent cluster: clear groups of emojis that appeared many more times with each other, compared to the rest. The green region includes reactions that are clearly supportive of the speech (smiling face, thumbs up, grinning, etc.), while the blue, orange, and purple reactions represent clearly negative sentiment. What’s especially interesting here is the central position that the US flag holds, effectively acting as a bridge between the two sides of the network. It is the emoji that’s most used with those supporting and opposing Trump’s speech. Using a measure called pointwise mutual information (PMI), we can start to gauge the relationship between items that appear in sequences. PMI is a measure of association used in information theory and statistics. It is a great way to find collocations and associations between words in sentences — and also emojis in sequences. A high PMI score between two items means that the probability of co-occurrence is slightly lower than the probabilities of occurrence of each of the items separately. For example, word pairs such as “puerto” + “rico,” “pay” + “attention,” and “nobel” + “prize” have high PMI values. These are combinations of words that are closely affiliated with each other. By computing the PMI scores for both the thumbs-up and thumbs-down characters in relation to all other emojis, we can effectively organize the range of relationships (similarities and differences) between each sentiment (pro/con) and the rest of the emojis. The Russian flag, interestingly enough, sees low PMI values, likely due to the fact that it doesn’t consistently appear with one of the sides (content vs. disappointed). With that, you can see how this plot helps us organize the range of emotions and emojis, from the happy, smiling, and joyous to the saddened, disappointed, and confused faces. Here’s the full post: www.buzzfeed.com Come play (with our data)! We’re hiring for full-time positions in NYC/LA as well as summer internships . Sharing our experiences & discoveries for the betterment of… 30 Emoji Social Media Data Science Donald Trump Buzzfeed Posts 30 claps 30 Written by Head of Data Science @buzzfeed| previously @betaworks, @microsoft | Adjunct Professor @NYU | @globalvoices Sharing our experiences & discoveries for the betterment of all! Written by Head of Data Science @buzzfeed| previously @betaworks, @microsoft | Adjunct Professor @NYU | @globalvoices Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-08"},
{"website": "Buzzfeed", "title": "hello world", "author": ["Angie Ramirez"], "link": "https://tech.buzzfeed.com/hello-world-4be10d4496d4", "abstract": "Latest Posts Events Apply To BuzzFeed Welcome to tech.buzzfeed.com ! We are very excited to be using this platform to share experiences and discoveries with you as we continue building BuzzFeed’s media platform. Our engineers employ a lean, iterative development process where we integrate and deploy code often. We build for scale and support millions of users. We work closely with other teams within Tech including Product, Design, and Infrastructure to deliver our products — this type of collaboration is key to our success. Our designers ensure our website, mobile apps and internal tools are as simple to use as they are beautiful to look at. They collaborate closely with engineers, data scientists and product managers to ensure the features we ship aren’t just great to use, but solve real human needs. Our data scientists find the right solution to fit the problem and know that the right solution might be the one that doesn’t use machine learning. They are intensely skeptical of data and know what it can and cannot tell you. They are able to take a complex problem and distill it down to core questions, and are always looking for ways to step back and look for process improvements. Our product managers work with teams of designers, engineers, project managers, curators and content creators to build features for our mobile apps and website. They lead teams in launching and iterating on products with great user experiences, making content easy to consume and fun to share. Finally, our teams are diverse — we’re made up of people with varying backgrounds, experiences, and skill sets, who have the opportunity to lean into their strengths as well as develop new ones. We have the innovation obsessed culture and structure of a venture-backed tech company with a tech team focused on building the media platform for today’s world, and the future. We deeply value open source and give back to the community by speaking, hosting meetups and contributing to projects we benefit from. BuzzFeed Tech Offices: Our tech teams are located in London, Los Angeles, Minneapolis, and New York. Sharing our experiences & discoveries for the betterment of… 3 Media Design 3 claps 3 Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer @Livepeer. Previously @BuzzFeed @Yale. Traveler. Writer. Thinker. Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-12"},
{"website": "Buzzfeed", "title": "getting better or how i learned to stop worrying and close the gap", "author": ["Hana Carpenter"], "link": "https://tech.buzzfeed.com/getting-better-or-how-i-learned-to-stop-worrying-and-close-the-gap-e351bcf0e69c", "abstract": "Latest Posts Events Apply To BuzzFeed I’m going to start this off with a quote from Ira Glass that I hope many people in my position (i.e. those trying to enter a creative field) are familiar with. It’s long. You can always skim, I guess. All of us who do creative work, we get into it because we have good taste. But there is this gap. For the first couple years you make stuff, it’s just not that good. It’s trying to be good, it has potential, but it’s not. But your taste, the thing that got you into the game, is still killer. And your taste is why your work disappoints you. A lot of people never get past this phase, they quit. Most people I know who do interesting, creative work went through years of this. We know our work doesn’t have this special thing that we want it to have. We all go through this. And if you are just starting out or you are still in this phase, you gotta know its normal and the most important thing you can do is do a lot of work. Put yourself on a deadline so that every week you will finish one story. It is only by going through a volume of work that you will close that gap, and your work will be as good as your ambitions. […] You’ve just gotta fight your way through. That’s a solid sentiment, right? I forget where I first read it, and don’t normally go in for quotes, but I feel like Ira is describing something true. Anyone who makes anything can feel the agony of influence. That’s something to which I’ve found myself especially susceptible, and I’ve let it cage me in. Normally, I work on BuzzFeed’s IT team. It’s a busy job that involves a lot of communication and troubleshooting, but it doesn’t require any creative output. I’m interested in design, and I’ve done freelance web design/development in the past, but because of the cage I could never bring myself to apply for jobs in those fields. And then I drunkenly let this all slip to Cap at a party that IT and Product Design had, and he was like “Why the hell are you not working for us? Come do a residency!” So, out of the blue, I’ve had the unique and terrifying opportunity to be a part of the product design team. Unique because I have no professional experience as a designer, and have done very little to prove that I can be one. Terrifying because I am embarrassed of everything I have ever made, and whenever I have to show my work to anyone I want to apologize and leave. Every step of the way these people shown me the value of doing work , and how to focus on that instead of constantly comparing my work to everyone else’s. I’ve had to fight my way through my own insecurity. It’s been a sudden and exciting period of growth for me, so I thought I’d write down some of the best things I’ve learned so far. Standards are probably more important than taste or instinct. At BuzzFeed we have a CSS framework called Solid that we uses as a starting point for site designs. We templatize things in Sketch so they’re ready for quick mockups. Solid’s visual treatment guidelines and ratios have been agreed upon by the product design team, so they keep critiques out of the weeds. It’s easier to give and get feedback when people don’t have to ask questions about symmetry, hierarchy, borders, and button proportions. They prevent you from having to think too hard about hierarchy and spacing while you’re working, too. Standards will set. You. Free. Anything that helps you break down the big, subjective problem you want to solve is meaningful, actually. The BuzzFeed design process is really a useful way to think about all kinds of problems, because it breaks the act of solving them down into shades of directness. First you define your problem at large, then you collect information about it and come up with possible solutions in a noncommittal way, then you pick through that information and those solutions, keeping what’s most relevant/useful. And your teammates help you along the way. ( The whole process is laid out here if you’re interested .) No matter how big the problem is, the pressure isn’t on to solve it right away. You just have to make a space that everyone can get their heads into whenever they want to think about it. Google Docs are good for creating and sustaining that! Especially if you don’t bring it up. If you reject an idea, make sure you have a defensible reason for doing so. Don’t be afraid to come back to something you initially threw out, either! As a wise LA-based product designer once said: Time spent is not time wasted . Frustration and self doubt are not defensible reasons to throw something out, but you’re definitely going to feel those. You just have to understand them as part of the problem-solving process. You’re feeling frustration so that the people who eventually use your thing won’t have to. When you are building a tool, good user flow means more than a nicely-gridded layout in the end. Good user flow represents time spatially, respects user habits and design patterns, and understands the power of color/emphasis to draw focus. It won’t be what you first arrive at, and you can’t always apply a formula to achieve it. User experience is one of the things that critiques can be most useful for! And also not, because other designers tend to see things a certain way. So make sure to talk to your actual users a lot! Getting 23 criticisms and 1 compliment in a critique is a good thing. More critical feedback = more opportunities to identify and patch up the holes that are causing your boat to take on water. When you’re learning, especially if (as Ira Glass said above) you have taste, it can hurt to see everything needing immediate and sustained improvement so badly. When your teammates point out a misstep, even when they do it nicely and softly, while stroking your hair (no, nobody ever did that to me here), it suddenly seems so stupid and obvious. This is actually an important feeling that you should cherish, because it’s the feeling of active learning. You will not always get to feel this; to mix it with shame is a big waste of time and energy. So take notes, or, better yet, have someone else take notes for you! I shared these takeaways in our year-end crit, and the team agreed that I’ll be learning many of these lessons (especially the one about taking criticism) forever. Even if I make bad stuff, I trust my methods and the people around me to correct for my inexperience. They don’t want me to be good , they want me to get better . They look at themselves and their products as works in progress, forever improving but never complete. And that, also, feels like the truth! Sharing our experiences & discoveries for the betterment of… 19 1 Design Creativity Self Improvement Tech Buzzfeed Posts 19 claps 19 1 Written by 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 Sharing our experiences & discoveries for the betterment of all! Written by 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-29"},
{"website": "Buzzfeed", "title": "my first two years as a mobile dev", "author": ["Paola Mata"], "link": "https://tech.buzzfeed.com/my-first-two-years-as-a-mobile-dev-183b8910f6b8", "abstract": "Latest Posts Events Apply To BuzzFeed Today I’m celebrating a “Buzziversary.” I’ve been an iOS developer at BuzzFeed for two years. Some days I still can’t believe my life, and others I take it for granted like everyone else. What hasn’t changed at all is my love for this work. It only becomes more exciting and challenging the more I learn. Through my involvement with NYC Tech Latinas , as well as a number of other organizations, I’ve had the opportunity to meet countless people who are new to programming, trying to get a foot in the door, or just starting out in their first dev roles, many of them from groups underrepresented in tech, and share my experience with them. In previous posts, I shared about how I transitioned into tech and landed a fantastic job. But now I’d like to take the time to share some of the more difficult lessons I’ve learned since. Early on, I was afraid to ask too many questions because I thought it would reveal how inexperienced I was and how little I knew. I spent way too long trying to figure things out on my own, when I could have instead checked in with my team. This caused me so much unnecessary stress. It’s been my experience that teammates are always happy to help and share information. I have never once been punished in any way for asking a question. In fact, as I’ve gained experience and tackled problems of growing complexity, communication has become vital and often necessary, so I think it’s a good idea to get into that practice early on and strive to be better at it. For the first few months, I would bring my laptop home and work in the evenings, sometimes weekends. This was not expected of me. I did it because of all the pressure I put on myself to be at the same level as the rest of my team, which was completely unrealistic, because they had been doing this work years longer than I had. At some point, I decided to purposely leave my laptop at the office and never look back. It was the right choice. These days I don’t often write code outside of the office. Some may disagree and think that one should “always be coding” and have a bunch of side projects in order to get stronger at it, but I think I’m a better and happier programmer when I have a good work-life balance. Furthermore, this isn’t a race. I plan on being in this field for a long time. I’ll be the first to admit that getting feedback on your code can be daunting and soul-crushing. I’ve had some comments in code reviews that read like essays. Some days, when my emotional state isn’t so great, it’s easy to take it personally and get down on myself, but that would defeat the purpose. The truth is that I have benefited immensely from getting feedback. It has, without a doubt, made me a stronger programmer. The fact that my teammates take the time to write thorough reviews, and that I’m given the opportunity to improve my code — that my team believes in my ability— is something I’m thankful for, even if I don’t always feel that way in the moment. At some point in my second year, I was unhappy with certain aspects of my work life. I did not talk about it with anyone for fear that I might be seen as problematic. In meetings with my manager, I would say everything was fine, but secretly I was starting to consider if I might be happier elsewhere. This went on for a few months until I realized that I needed to open up about what was bothering me or else nothing would ever change. So I did, and it was very uncomfortable, but so freeing. It was a productive conversation that changed the course of my career. I feel fortunate to work with great people and in a supportive environment that allows me to have difficult conversations. If you follow me on Twitter, you know I am very active in the iOS/Swift community. It adds fun to my days to joke around with all my best internet friends while at work. But also, it’s a constant reminder that we are all in the same boat trying to stay afloat while Apple keeps throwing stuff at us. I’ve made some great friends via Twitter. They have helped me with everything from Xcode issues to managing anxiety to getting through a divorce. I can’t say enough about how much these connections have meant to me the last couple of years. A special shout out to Fernando Paredes , for all his help and for introducing me to Chingo Bling. These are just a few of the lessons I’ve learned so far. If I think of any others, I will add them to this post. Feel free to share any lessons you’ve learned. Would love to hear them! Sharing our experiences & discoveries for the betterment of… 44 2 Work Life Balance Life Lessons Tech iOS Buzzfeed Posts 44 claps 44 2 Written by iOS Developer at BuzzFeed, Tech Community Leader 👩🏻‍💻🏋🏻‍♀️ Sharing our experiences & discoveries for the betterment of all! Written by iOS Developer at BuzzFeed, Tech Community Leader 👩🏻‍💻🏋🏻‍♀️ Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-05"},
{"website": "Buzzfeed", "title": "13 mindblowing hacks that buzzfeed tech swears by", "author": ["Jessie Wu"], "link": "https://tech.buzzfeed.com/13-mindblowing-hacks-that-buzzfeed-tech-swears-by-8890049f2ec3", "abstract": "Latest Posts Events Apply To BuzzFeed Save time at work so you can read more BuzzFeed posts. Here at BuzzFeed, we love to build and we build fast. With multiple squads in twelve groups, our projects range from building Rig, our own deployment system, to implementing new experiments like the Inauguration Emoji project. Whether it is pushing to production often or using data science to analyze 10 million emojis , these useful shortcuts, commands, and tools help us be more productive on a day-to-day basis. 1. bash | Grab the top commit hash from the current branch Mark McDonnell, Senior Software Engineer What is your hack? alias getcommit=\"git rev-parse HEAD | tr -d '\\n' | pbcopy\" What does it help you with? This helps me to grab the top commit hash from the current branch without taking my hands off the keyboard (otherwise I’d have to git log and manually select the commit hash, or I’d have to reconstruct the above command from memory which just results in 5–10 minutes of faffing about). So this is useful for any number of reasons, but I’ll either be sharing the hash or I’ll need to use it for a git cherry-pick within another branch (that sort of thing). If you had a hacker name, what would it be? Milky Joe (you’ll recognise it if you’re a fan of “The Mighty Boosh”) Anything else? I use pbcopy (a macOS command), but sometimes you’ll get a line break as part of the contents being copied. This could be an issue depending on where you’re pasting the copied commit. I ended up fixing this by using the tr command to delete the newline. 2. bash | Reverse bash history search Ashley Miller, Engineering Manager What is your hack? Control + R + \"search term\" What does it help you with? Reverse bash history search is one of my favorite command line hacks. It does full text search of all of the commands in your history. When you find the one you want, press enter and it runs! Saves me so much time. If you had a hacker name, what would it be? Crash Override 3. Dropbox | Instant screenshot sharing from Dropbox to Slack Sam Kirkland, Associate Product Manager What is your hack? When you set up Dropbox to automatically store Mac desktop screenshots in a Dropbox folder, it instantly generates a link to the image and copies it to your clipboard. That means the URL is immediately available to paste into Slack, where the screenshot unfurls and it can be viewed by an entire team a second or two after you take it. What does it help you with? I report a lot of bugs and highlight a lot of features for my team (which works on our publishing software), so this is an essential way of making sure others can quickly see what I’m seeing. 4. Terminal (macOS)| Investigating a pesky process or app that is slowing down your computer Andrew Jocson, IT Systems Engineer What is your hack? sudo fs_usage and sudo syscallbypid.d What does it help you with? These commands are great if you want to see exactly what’s freezing or slowing down your computer. (Only use them if you are comfortable with the Terminal! sudo commands can be dangerous to use.) sudo fs_usage — Run this command, then wait a few seconds and hit Control + C to stop it. This command reports system calls and page faults related to filesystem activity in real-time. Scroll up and look for apps that are making a lot of calls and close them to see if it solves your issue. sudo syscallbypid.d — Run this command then wait a few seconds and hit Control + C to stop it. This command will count the number of system calls for each process. Consider closing the app that has a high count of calls, or research it! If you had a hacker name, what would it be? Sushi 5. Atom text-editor package: solid-completions | Atom autocomplete for BuzzFeed’s Solid CSS framework! Jessica Anastasio, Associate Software Engineer What is your hack? This tool autocompletes Solid class names in HTML and template files and gives you access to Solid global variables in SCSS. What does it help you with? This is super helpful and time-saving for building new user experiences or features on the fly — I don’t have to constantly refer to the Solid documentation! If you had a hacker name, what would it be? cyb3r chick Anything else? This Atom package can be found here! https://atom.io/packages/solid-completions This solid-completions package was created by our very own alum, Mark Shuster. Solid is BuzzFeed’s open-source CSS and component library. Read more about it in BuzzFeed Design’s blog post ! 6. Harmantom | Critique Randomizer Tom Harman, Design Manager What is your hack? http://harmantom.com/order/ What does it help you with? It randomizes the order designers present their work in group critiques, as well as the all important task of randomly selecting a jokester. This person gets to kick things off in an appropriately humorous manner. If you had a hacker name, what would it be? Crazy Clawz 7. bash_it | A collection of community Bash commands and scripts Raymond Wong, Senior Site Reliability Engineer What is your hack? A bash framework to manage your bash shell. What does it help you with? You can enable a lot of shell command completion. Don’t remember parameters for ssh? Just type ssh , and then tab, it’ll list out the options. Don’t remember the git branch you wanna clone? Just tab it after git clone , and it’ll list the branches. Also it allows custom themes for your terminal, so no more generic bash prompts. If you had a hacker name, what would it be? rayzorinc Anything else? Go to https://github.com/Bash-it/bash-it to install it. Example themes screenshots: https://github.com/Bash-it/bash-it/wiki/Themes 8. bash | Repeating the previous command Laura Wright, Software Engineer What is your hack? sudo !! What does it help you with? !! repeats the previous command, so sudo !! is sudo + the last thing you tried to do. It is great when you run a command as your user that requires root — simply sudo !! rather than retyping the command. If you had a hacker name, what would it be? kung fu fingers 9. Keyboard | Literally only use the keyboard Manuel Palou, Software Engineer What is your hack? Literally only use the keyboard. Unplug your mouse until you get used to only moving with the keyboard. What does it help you with? Moving around with a keyboard is infinitely faster and less taxing than fussing with the mouse. It’s not as “easy” because it requires memorization, but once you get used to it you’ll be glad you gave up your mouse. If you had a hacker name, what would it be? I do have a hacker name, it’s manny. Anything else? In practice, I still use my mouse because there are some places not optimized for keyboard navigation and some tasks still need a mouse (photoshop, illustrator, etc). However I always strive to avoid it and make a note of when I use to do something I could have done faster with a shortcut. 10. Jetbrains Products (e.g. IDEA, PyCharm) | A keyboard-centric approach Sean Gilbertson, Senior Software Engineer What is your hack? Command + Shift + A What does it help you with? Lets you do everything the editor can do, via the keyboard. You will discover untold worlds of shortcuts and it will be very hard to not use a Jetbrains editor in the future. If you had a hacker name, what would it be? JeffFromTodaysSpecial 11. bash | ps auwwx | grep [p]ython Dan Meruelo, Site Reliability Engineer What is your hack? ps auwwx | grep [p]ython What does it help you with? Ignores the grep process in the results returned from the ps command. You only see the ps information from the command you are interested in. The above command finds all python tasks, but the search can be changed for any process. If you had a hacker name, what would it be? mellowyellow 12. vim + git | Load all unstaged changes into vim Nick Hedberg, Software Engineer What is your hack? started =! bash -c ‘vi $(git status — porcelain | cut -c4-)’ What does it help you with? I made that command a git alias started , which I use to load all my unstaged changes into vim. So at the end of the day I can close vim and when I want to start working again I git started . It’s also great for reviewing all your changes before you commit. 13. Emacs | Emacs all the way Sri Ray, OPS monkey What is your hack? alias vi=emacs What does it help you with? Emacs is a much superior editor! If you had a hacker name, what would it be? nofiltersri Anything else? I mean seriously, why would one not use emacs? It’s so much better than vim. Want to learn more about the things we work on and the culture at BuzzFeed? Check out our tech blog posts on what our teams are made of , lessons learned in the product design process , and more! tech.buzzfeed.com tech.buzzfeed.com Sharing our experiences & discoveries for the betterment of… 13 Git Bash Hacks Web Development Buzzfeed Posts 13 claps 13 Written by Software Engineer at the New York Times, previously at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Written by Software Engineer at the New York Times, previously at BuzzFeed Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-28"},
{"website": "Buzzfeed", "title": "checkout the meetups we host", "author": ["Swati Vauthrin"], "link": "https://tech.buzzfeed.com/checkout-the-meetups-we-host-1ea10a63672c", "abstract": "Latest Posts Events Apply To BuzzFeed We’re excited to share the following Tech meetups that we host across our offices. Check them out! Know Thy Interpreter: Deep Dive into Python What : We review various aspects of the Python interpreter When : every other Tuesday Where : NYC office Math and Algorithm Reading Group What : We read books, papers, and articles about algorithms and math methods that are applicable to data science and machine learning. We generally go very deep into topics, and work at an advanced undergraduate to graduate level. When : most Wednesdays Where : NYC office NYC Tech Latinas What : Hack nites and workshops around dev, design, professional development. Open to Women only When : Typically 3rd Tuesday of the month Where : NYC office Design Driven What : Design Driven NYC is a community that lives at the intersection of design, user experience, and technology. We are a group who believes that great design is storytelling at its finest, and that it is a critical element to any successful product. Check out designdrivennyc.com for photos, videos, and open UX/design positions across the #DesignDrivenNYC community. When : Once a month — check website for schedule Where : NYC office Write To Speak What : Writer’s workshop for tech talks When : Monthly (irregular. Check back for dates) Where : NYC office DjangoNYC What : Django-nyc’s goal is to create a community for those interested in Django to meet, share ideas, create, learn, and have a good time. The focus will be Django and web application design and development. When : Wednesdays about once per month Where : NYC office Twin Cities iPhone Developers What : iOS and aspiring iOS developers get together to learn and network. When : 3rd Wednesday of every month Where : Minneapolis office PyMNtos What : Python meetup When : Every other month Where : Minneapolis office Sharing our experiences & discoveries for the betterment of… 4 Design Meetup Technology BuzzFeed Buzzfeed Events 4 claps 4 Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Written by VP of Engineering @ BuzzFeed. Loves fashion, sports, tech crunching, and being a mommy! Sharing our experiences & discoveries for the betterment of all! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-31"}
]