[
{"website": "Localytics", "title": "When Things Go Wrong", "author": ["Tony Wieczorek"], "link": "https://eng.localytics.com/when-things-go-wrong/", "abstract": "We believe the highest performing engineering teams have a process to identify, triage, fix and learn from service degradations and downtime. At Localytics, we build highly available and scalable systems, and part of the key to our success is learning from failures. One way we foster a learning culture is through how we respond to pages we get about our services. We wanted to share our process for constantly improving. At Localytics, we're big believers in the power of chatops to scale our team. We use integrations with Slack to deploy code, run database migrations, and monitor dozens of services. Pingdom, New Relic, AWS Cloudwatch, and coded circuit breaker patterns post to a #prod-issue channel in Slack. All of those systems separately notify the team through PagerDuty integration. An engineer's first response to receiving a PagerDuty is to open Slack and check #prod-issue for more details, rapidly speeding up triaging the issue. While you might get a page about a particular service being down, checking #prod-issue lets you see if there are, for example, larger service problems before digging any further. Since it's a standard Slack chat room, engineers can coordinate the response and debug the issue in realtime, no matter where they are physically located. The chat history is a convenient record of what happened during the event and serves as the official record of the incident. After the initial crisis is over, our engineers move into the #postmortem Slack channel. This is a place to hypothesize about what happened, challenge assumptions, and volunteer an Incident Leader to follow through. If the issue was a known risk that has now come to fruition, there's a good chance this conversation turns into a laundry list of ideas to fix the underlying issue. We often use the ideas generated here when we have the formal, blameless postmortem in Step 4. A self-volunteered Incident Leader, usually the first person notified or \"on the case\", sends out an email to let the rest of the company know what happened. That distribution list goes to our Executive Team, Product leaders and Marketing department. It lets them decide whether to notify our customers proactively, or generate content to send to customers. Here's the email template we use: Subject: Issue Name, Date and Time Hi everyone, We experienced a production issue impacting [Service Name] . The issue began at [Time] on [Date] and ended on [Date] . Issue team: What is the issue and what caused it? State what happened and why, in terms a general, non-technical audience can understand. Example: Database went down due to a bad query. What is the impact to customers? State the specific services that were impacted. Example: Push campaigns failed, attributions were not attributed, etc. Which customers were affected? Include a link to a Google Sheet containing the following fields: org name, org_id , app_id , and campaign_id where applicable What is the current status of the issue? If the issue is resolved, state the fix and when it was deployed. If the issue is unresolved, give an approximate timeline for the fix. Next steps Product Marketing will follow up on this email thread with customer-facing messaging. Within 24 business hours, the Incident Leader schedules and leads a Blameless Postmortem. This meeting can be anywhere from 30 minutes to several hours long. The point is to identify the processes, technologies and monitoring gaps that might have contributed to the issue. The blameless concept is an important one. Postmortems should not be an exercise in identifying and disciplining specific engineers. Issues with services are rarely the fault of a single person. More likely, our internal processes and tools lead to the downtime, and the postmortem should focus on resolving those. This requires a belief that in a well-functioning team, people make rational decisions with the information they have, the resources available to them and the team support they receive. The only time spent identifying individual engineers is to bring them to the postmortem discussion. Here are some examples of the different kinds of questions you ask in a blaming postmortem and a blameless one: Example Incident 1: A database query brought down the database. Example Incident 2: Geographic coordinates are not processing correctly. The Incident Leader's task is to fill out a Postmortem page on our internal Confluence wiki. Here's the template we use: Date Participants : @engineer1, @engineer2 Start every Postmortem stating the following Incident Leader : @engineer Description [Short description of what the problem was] Timeline [2016-03-15] - @user does a thing Contributing Factors [What caused the problem? Use the 5 Whys if you'd like.] How Did We Fix It? [What did we do to stabilize our systems, either temporarily or permanently.] Customer Impact [How did this affect customers?] Corrective Actions [A list of TODOs with engineer names and expected dates] We hope you find this process useful. We think it's been key to our success as we scale Localytics. Photo Credit: Storm by javier ruiz77", "date": "2016-09-13"},
{"website": "Localytics", "title": "Exploring CLI Best Practices", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/exploring-cli-best-practices/", "abstract": "Like at many software companies, we at Localytics build command-line interfaces (CLIs) that manage our internal infrastructure and processes. These tools cover a broad range of applications, including integrating with our background jobs server, creating and maintaining AWS resources, as well as handling deployment and continuous delivery. We’ve written all of our CLIs with ruby, using thor to handle parsing and dispatching the commands. For the last couple of weeks we’ve been fine-tuning many of these CLIs, and we’ve learned some things along the way about the user experience. Even though most of these CLIs are internal, we’ve found they need the same level of fidelity as external ones. Users expect a certain degree of quality, even from internal tooling. Below is our list of best practices we’ve learned throughout this process. While this is certainly not an exhaustive list, if you follow these best practices you will be well on your way to creating an intuitive CLI that users will be happy to use. 1. Every option that can have a default option should have a default option. Writing out tons of options when you’re invoking a CLI is exhausting. It’s prone to spelling errors, and usually results in users having to record their last usage or rely on their bash history in order to find the right incantation to make your CLI work properly. Most users coming to your CLI are trying to accomplish a task and don’t need advanced configuration. Move your consumers faster through your CLI by making the most common path the default, while still allowing the fine tuning options for those that need it. 2. Provide long, readable option names with short aliases. Longer option names are great for scripting the invocation of a CLI because it’s clear what’s happening (e.g., --profile ). Shorter option names are great for consumers on their laptops that can remember them (e.g., -p ). Provide both to support both use cases. 3. Use common command line options . If your CLI is consistent with common patterns across the industry, your users are more likely to find it intuitive. Don’t use --recur when you can use -r and --recursive . You want your script to contain the fewest number of surprises; being unique doesn’t provide anything other than confusion. 4. Provide options for explicitly identifying the files to process. A lot of CLIs perform some action over files or directories, be it reading them, parsing them, or even just counting them. Instead of requiring your users to execute your CLI in a specific working directory, provide the ability to point to those files directly. This saves the end user the effort of having to remember the current directory, and leads to many fewer extraneous cd statements in the middle of scripts. 5. Don’t have positional options. Options that depend on the position in which they were given are difficult to work with. If you’re going to parse command line options yourself, make sure they can be specified in any order. 6. Provide an extensive, comprehensive help command that can be accessed by help , --help or -h . You want your users to forget how to invoke your CLI, it means it was intuitive and didn’t require extra brain power to grok. For those moments when they can’t figure out what they need to know, provide an intuitive help command that lists every option, and how to use it. If it’s a more extensive CLI, make sure there’s a help command for each individual command as well. This advice is particularly salient when you come back to working on it after a while and can’t remember how to invoke your own CLI. 7. Provide a version command that can be accessed by version , --version or -v . If your CLI is going to be distributed, make sure you provide an intuitive way to access the version information. It will save you and the end user time if it’s easy to access, as bug reports can come with a version attached. Follow semantic versioning so your users can expect breaking changes only on major releases. 8. Don't go for a long period without output to the user. Sometimes your script will take longer to execute than people expect. Outputting something like ’Processing…’ can go a long way toward reassuring the user that their command went through. Engineers especially have a natural tendency to distrust something they didn’t write themselves, which can lead to people exiting out of a program that they think is hung. 9. If a command has a side effect provide a dry-run/whatif/no_post option. Especially for CLIs that impact production systems, it’s handy to have the CLI run through the motions without actually executing anything. This gives extra reassurance that what the user typed in corresponds to what they think it’s going to do. 10. For long running operations, allow the user to recover at a failure point if possible. It’s a terrible experience to get halfway done processing a large number of files/items/etc. only to have the script crash with no way to restart where it left off. It may not even be the fault of the script itself - it could be something as simple as network connectivity. If your script fails halfway through, process the errors appropriately and allow the script to restart where it left off. 11. Exit with nonzero status codes if and only if the program terminated with errors. Consistent exit statuses mean your CLI can be embedded within larger shell scripts, making it much more useful. Allow your users to switch on whether or not it was a clean exit, and handle the errors as they see fit. Conversely, don’t exit with a nonzero status code if your CLI didn’t encounter an error. Your cleverness will end up confusing and frustrating your users, especially if -e is set. 12. Write to stdout for useful information, stderr for warnings and errors. Depending on the context your CLI is run in, stdout and stderr can point to very different locations. Don’t make it unnecessarily difficult for your users to find the correct logs when there’s an error, or to parse the logs between what they need to know and what’s just a warning. 13. Keep the CLI script itself as small as possible. This point is less specific to CLI design, and more general good software design. Move as much business logic out of the actual CLI script as possible. Your script will be much more easily extended with a more modular code design. If you want a web or application view of the functional logic that your CLI performs, it’s much easier to reuse if your code is already properly factored out of the main CLI controller. As an added benefit, this makes the code easier to test. 14. Reserve outputting stack traces for truly exceptional cases. For users that aren’t familiar with CLIs, stack traces can be intimidating. Oftentimes, even with good error messaging, the additional output can lead users to think something went wrong with the actual program as opposed to their configuration or option combination. If you can tell when exceptional behavior is going to happen in your program, process your own errors properly, and output only the information that the user needs to know. As part of the last point, we are open-sourcing two libraries we have built to make it easier to invoke thor CLIs within a safe execution context, as well as handling other types of callbacks. They are hollaback and thor-hollaback . thor-hollaback adds callbacks to thor à la rails controllers. Using thor-hollaback , you can accomplish this point by: As an example, see the below: As our continued use of CLIs to manage infrastructure and processes increases, we will continue to rely on this list for helping us build usable, intuitive interfaces for our users. We hope this list will help you build better CLIs as well. Both gems are available on rubygems.org and are freely available for use. When you use them, please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2016-10-11"},
{"website": "Localytics", "title": "Tweaking DynamoDB Tables for Fun and Profit", "author": ["Joanna Solmon"], "link": "https://eng.localytics.com/tweaking-dynamodb-tables/", "abstract": "Amazon’s DynamoDB is a great technology: it’s easy to set up and provides all the benefits of an infinitely scalable distributed database without the operational overhead. Unfortunately, DynamoDB can also be very expensive for high volume, high throughput storage. This post will show how we were able to tweak our deduplication schema to drastically decrease our storage costs. At Localytics we take in several billion datapoints each day. Each datapoint represents a user action taken within an application such as purchasing an item or levelling up in a game. Most of of our datapoints come from mobile devices which may lose connection before a request is acknowledged, leading to repeated sends and duplicate data. In practice, poor wireless connections are common and duplicates may account for up to 10% of our incoming traffic. This means we see hundreds of millions of duplicate datapoints each day. In principle, deduping is simple. Each time we receive a datapoint, check its UUID against a store of previously seen UUIDs. If the UUID is present, ignore the datapoint; if the UUID hasn’t been seen, add the UUID to the store and process the datapoint. The question is: how can we make deduping in DynamoDB cost effective while processing billions of datapoints each day? If we take in 5 billion datapoints each day we’ll need to do at least that many daily reads and/or writes to the table. Additionally, we’ll need to store at least 150 billion UUIDs in the table to give us 30 days of dedupe history. Why do we need such a large dedupe window? If an app sends a failed upload while a user is closing it, the upload won’t be resent until the user reopens the app which could be days or weeks in the future. The read/write costs for the table will be fixed relative to the volume of incoming data because we need to do a read and/or write for each incoming datapoint. However, we will show how storage costs can be controlled with some clever tweaks to table setup. If we take the most naive approach and simply store each UUID in Dynamo, we’ll have a table that looks something like this: Each row contains a single, indexed, 32-character (128-bit) UUID. When deduping an incoming datapoint, we can take advantage of Dynamo’s Conditional Writes to run the check and write in a single operation. This approach is obviously impractical because it provides no way to age out data over time. Before we address the aging issue, let’s look at the storage costs assuming we take in 5 billion datapoints each day. Dynamo stores all non-binary data (including numeric types) as UTF-8 strings .  UUIDs consist of characters in the standard ASCII range so each of the 32 characters will take up one byte of storage. In addition to the 32 bytes per UUID, there is a 100 byte storage cost per index in the table, making each row in the table account for 132 bytes. Without removing any old data, after one year of collecting 5 billion datapoints each day we’d expect to have 1.8 trillion entries in the table, or 238 TB . At a cost of $0.25/GB/month , the naive table implementation will cost us $60,100/month after one year. Obviously we don’t want our table to keep growing indefinitely. We need some kind of age-out scheme. DynamoDB does not yet offer any kind of TTL on their tables, but we could add a unix timestamp field to each entry and run a daily job to delete all entries older than 30 days. In this case our table might look something like this: Now the number of entries in our table will be capped at 150 billion. The size of each entry will increase by 10 bytes to account for the 10-character timestamp making them 142 bytes. Now our table size will be roughly 21.3 TB costing $5450/month . This gets gets us close to an order of magnitude in storage savings, though we’re ignoring the increased cost in reads/writes for the daily cleanup. We can still do significantly better without the additional work of manual cleanup. In the previous schema, 100 out of every 142 bytes came from the index, making the index responsible for 70% of the storage cost. Storage costs for our data will be fixed based on the number of UUIDs, but if we can limit the size of the index we can create significant savings. We can limit the index size by take advantage of Dynamo’s Set Types . Instead of giving each UUID its own row, we take the first n bits (more about choosing n below) of each UUID and use that as an index that maps into a set of the suffix bytes. Now our table looks like this: Dynamo’s conditional writes again allow us to do a single test-and-set operation , conditionally inserting a new suffix into the suffix set. The indexed prefix table structure will limit the number of entries in the table to 2^n . Say n is 33, or the first 9 characters in the UUID, now our table will only ever have 8.6 billion rows. Thus, even without age-out, after one year we are still only paying the 100 byte index cost 8.6 billion times instead of 1.8 trillion. Now the index size for the table is fixed at 860 billion bytes (800GB). With a year’s worth of data, the storage size will then be 9 bytes for each of the 8.6 billion prefixes (72 GB) plus 1.8 trillion suffixes of 23 bytes (38.6TB). Now only 2% of the storage volume is in the index, down from 70%. After a year, with 1.8 trillion UUIDs stored, the cost for this table would be $9860/month . This gives us significant savings over the naive implementation but is still double the storage cost of the naive schema with aging out. This approach saves a good amount of space and money but we’re still growing the table indefinitely and still don’t have a way to age out the data. The suffix sets will grow indefinitely as we continue to dedupe data. To begin aging out the data, we can create a new suffix column on the entry for each month. To keep 30 days of data and allow aging out, we create a column in the database for each month, deleting two months previous when it is present. The table looks something like this: For example, on January 1, 2017, we’ll insert a UUID into the suffix set of the Jan_2017 column only if it doesn’t already exist in the Dec_2016 suffix set. As part of the same operation, we’ll also delete the Nov_2016 column if it’s present for the row. All of these operations occur on the same row so they can be done as a single conditional write operation in Dynamo. Now we’re both aging data out and limiting our indexes by splitting the UUIDs into prefixes and suffixes. The last piece is the question: how long should the prefixes be? Dynamo charges for writes in write units of 1KB . Any object larger than 1KB will cost at least two write units, so it’s beneficial to try to keep each object in the table under 1KB in size. The randomness of UUIDs ensures we’ll get an even distribution of suffixes across our prefix indexes so we can calculate the average number of suffixes in each column as 2 * (# of UUIDs per month) / 2^n . As an example, to ensure the vast majority of entries are under 1KB, we need to ensure that the average size remains under 800 bytes. If our suffix strings are 23 characters long, we can safely store 35 suffixes over two months and would want to choose n such that (300,000,000,000 / 2^n ) < 35 , making n = 33 and putting a hard limit on the number of rows in the table at 8.6 billion. As before, our index is 800GB, our 8.6 billion 9 byte prefixes take up 72 GB, but now we only ever store two months worth of 23-byte suffixes at 6.27 TB. This makes the total cost of this implementation $1820/month . Finally, we can compare the cost for all three approaches and see that we’ve reduced storage costs by almost two orders of magnitude over the naive approach. In summary, if you want to save on storage costs with Dynamo, you should ask yourself: Photo by Stuart Boreham", "date": "2017-01-04"},
{"website": "Localytics", "title": "Best Practices and Common Mistakes with Travis CI", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/best-practices-and-common-mistakes-with-travis-ci/", "abstract": "At Localytics, we’ve worked hard to ensure that all of our services are built and deployed using our own internal best practices. One of those core best practices is that the service in question is using continuous integration (CI). CI greatly reduces the amount of time an engineer needs to spend finding the origin of a bug by running tests early and often. By integrating each change into a central repository where the tests can be run, we can precipitate faster cycle times and a more productive work environment. Our CI tool of choice is Travis CI . Travis’s tight integration with GitHub (our version control provider) allows us to quickly create an integrated testing environment for each of our services. Travis is widely used by the most popular GitHub repositories, with about one third of repositories with 50 or more stars using it. Travis builds code based on a .travis.yml file checked in to the root of the repository. By default, the build environment is Ubuntu 12.04 LTS with four gigabytes of memory running on two cores. Most of the build process is configurable through the .travis.yml file, including the machine it’s running on, the entire build lifecycle , notifications , and deployment of artifacts . Over time, we’ve developed the five best practices listed below. There are plenty of others out there, but we found these particularly helpful for efficiently using Travis within our team. One of the most important metrics in increasing development speed is decreasing test time. Every extra second tests are running is time that could be spent doing further development. If tests run for more than 10 minutes, developers will often start a new task rather than wait for the tests to finish. If the tests then fail, the developer now has to switch back to the original task. Constantly switching between tasks dramatically reduces productivity. Minimizing build time is both difficult and time-consuming, but in the end the investment is worth it because it allows developers to focus on one task at a time. Travis has a section on “Speeding up the build” in their documentation, and lists multiple ways to achieve this. The simplest is to take advantage of their build matrix and achieve “parallelism” by segregating your test suite. For some projects, we’ve also implemented our own test segregation by only running the tests that need to be run. This works particularly well if you have a repository with distinct top-level folders that have no cross-dependencies. First, query the git history to find the files that changed. Then, only test those directories. For example, the below code from one of our repositories only tests the rails code if there are changes to files within the “rails” subdirectory: The .travis.yml script is great for small scripts like but much beyond two or three lines it can become pretty unwieldy. We’ve found that moving those kinds of scripts into their own bin/test or bin/deploy can make testing simpler and allows you to run them outside of the Travis environment. When developing an application, you only need to test against the version(s) of the language that you’re running in production. At the most you might also test against the next version to see what’s going to break when you upgrade. For a library, however, you’ll want to test against every possible version with which your library could be run. For example, our Humidifier library is tested against four ruby versions to ensure engineers using it in various applications will not run into unaccounted-for bugs because they are using a different ruby version. Especially when you’re working with a large team with multiple Travis-enabled repositories, you’ll want to avoid running any unnecessary builds. The most common reason a build might be unnecessary is that it’s just a documentation or comment change. For simple changes like these, add “[ci skip]” to your commit message, and Travis will automatically skip that build. The other common reason is if you’re on a pull request and push code that was incorrect and then immediately push a fix. That first build is still going to run, but should be cancelled as quickly as possible so as to not overpopulate your team’s queue. When things break in Travis, usually the tests are failing. In that case, usually they are also failing on your local machine. In this case, the build can be fixed easily by pushing the same changes that fix the problem in your development environment. The more difficult case is when tests pass in your development environment but not in Travis. When this happens, Travis’ Docker containers are your friend. Simply mount your code as a volume and run the tests (e.g., docker run -it -v $(pwd):/code quay.io/travisci/travis-ruby /bin/bash ). In addition to best practices, we’ve also run into trouble often with the five common mistakes listed below. Either a new engineer is using Travis for the first time, or an experienced engineer forgets; all of the mistakes listed below are easy to make. Travis offers the ability to cache files between builds to decrease the setup time needed before the tests can be run. This is a particularly attractive option, and works well for things like cache: bundler (which will cache all ruby gem dependencies) and cache: yarn (which will cache node package dependencies). You can even cache arbitrary directories where you might write your own dependencies. Avoid over-caching though, as this can end up doing more harm than good. At one point we ended up with a cache for one of our projects that was almost a full gigabyte; removing the cache sped up the build. Don’t use the $? bash special parameter to determine the exit status of a previous command. The lines in .travis.yml are processed internally by Travis, and will not have the exit status you expect. Instead, follow the best practice listed above and extract anything that complicated into its own script. Travis provides a variety of environment variables that you can use in your build scripts. The most easily abused is $TRAVIS_BRANCH , particularly if you’re determining whether or not to deploy. Be cautious though - when Travis is building a pull request, $TRAVIS_BRANCH will be set to the target as opposed to the origin. To determine if the running environment is merged into master, you’ll need to [[ $TRAVIS_PULL_REQUEST == “false” ]] && [[ $TRAVIS_BRANCH == “master” ]] and to determine the exact branch you’re on you’ll need to shell out to git itself. Don’t rely on after_* -style callbacks to break the build . If the script that you’re calling in an after callback is somehow broken, the build will succeed and you will not be notified. Therefore, don’t put anything in that section of the pipeline that’s mission-critical. Instead, put that as another line inside the script section. We’ve generally reserved the after_* callbacks for things like reporting code coverage, as it won’t break anything if that fails. By default, Travis will build twice for each pull request . One of the builds will be the build for the branch itself, and one of the builds will be for the potential future merge commit against the target of the pull request. For us, because we merge to master quickly, it’s rare that the merge commit build will find something that the branch build doesn’t. Because of that, we’ve disabled the second build for a lot of our less fundamental services. The Travis interface doesn’t provide a lot of insight into the metrics around your builds, but they provide the tools to find these yourself relatively easily. Using the travis ruby client we were able to inspect our entire Travis configuration: finding the percentage of breaking builds per project, or finding the average amount of time the builds took. We decided to write some of these scripts in elixir . There wasn’t an equivalent Travis client written in elixir, so we wrote our own, which we are open-sourcing today: https://github.com/localytics/travis.ex . It’s up on hex and freely available for use. When you do, please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2017-01-10"},
{"website": "Localytics", "title": "Reacting to Change", "author": ["Nate Tenczar"], "link": "https://eng.localytics.com/reacting-to-change/", "abstract": "If you sit quietly enough while reading this article you’ll hear a sound. It’s the faint tapping of fingers on a keyboard, off in the distance in parts unknown. Those self-assured fingers are putting together yet another think piece on JavaScript in this—the year two thousand and seventeen—and how it makes the engineer writing it feel. You’ve read those articles. You’ve commented on them on Hacker News. You might have even ranted about them on Twitter. This is not one of those articles. Instead, this is the story of how we revitalized our frontend with a selection of new tooling and frameworks presented without comment on the pros and cons of other tooling and frameworks. I’ll actually be discussing how React is probably the greatest piece of software ever written and Dan Abramov should be proclaimed supreme overlord of JavaScr… oh there you go typing furiously in the comments section. Ah, right, I promised there would be no opinions on frameworks or tooling. Well, that’s a bit of a polite fiction. It’s impossible to avoid sharing opinions these days when new and interesting JavaScript libraries are popping up left and right. But the standard caveat applies: the opinions expressed here are the ones that have worked for us in our specific use case. If you find yourself in a similar situation, maybe you’ll give them a try also, but I’m not here to prescribe them for you—I’m not actually legally allowed to prescribe anything. With that in mind let’s get started. In the summer of 2015, our frontend engineers sat down and tried to envision what our ideal frontend development environment would look like. Did we want to keep using Angular, as we had been the last couple years? Was it time to evaluate our build tools? How much did we want hot reloading, and why did we want it so very, very much? There were a lot of questions, and you don’t need another JavaScript fatigue article to tell you there are also a lot of answers to those questions. These are some of the decisions we made. First, we decided to transition from Angular to React as our framework of choice. We considered Angular 2, but we didn’t think it was mature enough at the time for us to want to plot an upgrade path. React, on the other hand, had a growing user base of noteworthy software shops heavily invested in its success along with interesting solutions for handling application state in projects very similar to ours. As an added bonus, a handful of our developers were already using it in personal projects and were excited to start using it at work. Next, we concluded that the time had come to decouple as much of our frontend from Rails as possible, with our ideal world being that Rails would only serve as a REST API for our JavaScript to interact with. Microservices are all the rage these days, but the main reason for doing this was to get our frontend assets off the Rails pipeline. Part of the reasoning for this was that our engineers really wanted to use webpack as our build tool. Webpack, in addition to being popular in the community, offers a variety of things such as hot module reloading, a development server, and build performance tools. Separating our build pipeline from Rails also offered a way for UX focused engineers to work completely independent of Rails, developing visual components that would be used in our application before implementing them as features. The first thing we would tackle would be getting off the Rails asset pipeline in favor of webpack building all frontend assets. The world when this effort began: About a year and a half ago all of our dashboard assets (JavaScript, CoffeeScript, and SASS) were built by Rails, using Sprockets. Rails managed all asset compilation, minification, and cache busting, as well as placing the assets in a CDN. The plan was to slowly offload that work to webpack incrementally, as we had a dozen or so engineers working on our application and we wanted to interrupt them as minimally as possible. We allowed engineers to adopt new changes when it was convenient for them but also letting them know that their current workflow would eventually be deprecated. As a result, each step of the plan took place with opt-in functionality. The first things to move off the pipeline were our SASS builds, with the responsibility of building our SASS falling to Gulp and node-sass, with browsersync to serve the compiled assets in development. Nothing changed in production, but this shift unlocked hot SASS reloading for our developers. As our monolithic rails app grew and load times for some analytics heavy pages increased, the hot reloading was a welcome addition for our developers to increase velocity on the frontend. While both Gulp and browsersync would not be a part of our build pipeline for long, they are noteworthy because they provided one of the first instances of hot reloading in our application and enabled UX focused engineers to work faster. After successfully using CSS builds as a guinea pig for the project, it was time to start moving away from CoffeeScript as the language of choice. The first step was to enable developers to start using ES6, and we did so by using the Babel gem to continue using the rails pipeline temporarily. Enabling our developers to write ES6 made the transition away from CoffeeScript easier as they slowly acclimated themselves to new JavaScript language features (as well as exposing some of the deficiencies of CoffeeScript). Developers were still allowed to continue working with CoffeeScript, but all new features were encouraged to be written in ES6 so that the deprecation of CoffeeScript could begin. Next, it was time to move CoffeeScript and JavaScript compilation off Sprockets for good. This was accomplished by slowly refactoring all of our existing code to CommonJS modules, moving the files away from the Rails app into its own separate “client” directory, and using Webpack to compile everything in that directory. Rails had previously handled javascript modules ( as you can see in an old post of ours about a year on angular ), but now CommonJS gave us a javascript only solution for modules and dependencies so that we could start using a javascript build system. Finally, we started using webpack to do our SASS builds also, and with the final CoffeeScript files moved to CommonJS our refactor to webpack was complete. All frontend assets are now built by webpack in both development and production. Our end state: With our build tooling now set up for ES6 compilation, the time had come to begin using React. The path forward here, though, required reconciling our large Angular codebase with a brand new framework while Angular directives, controllers, and services were slowly phased out. Angular would need to continue to live (and still does) at the top level of our application, loaded on the page by Rails and setting up all the existing features and functionality. In order to use React inside of Angular, we would end up writing a shockingly simple wrapper that turns a React component into an Angular directive. This way, we could have Angular render the directive, which in turn would start the process of rendering React. This very simple angular directive serves one purpose: taking an AppContainer and rendering it on the page at the div in the template. When the directive is destroyed, similarly, the React component should also be unmounted. You can also see that this is a Common JS module, so a separate file handles turning this into an Angular module so that Angular can find and then use the directive. Since we could now put any arbitrary container, or smart component, on the page, we had unlocked the ability to start putting as much React as we wanted on Angular pages with the root being the Angular directive. This allowed us to rewrite entire pages from the ground up, first starting with replacing features individually before combining them under one container at the top level of the page. There is still quite a bit of Angular in our codebase, but we’re able to work around this by writing all new features in React and slowly replacing Angular as we go without being forced to do a complete rewrite. With React in the codebase, it was time to assess how we would be handling application state as we moved away from Angular. We landed on Redux as the framework of choice, and set up the store for our application state. All smart components would share the same store, and these smart components could be placed anywhere on the page either with directives or nested below fellow react components (judiciously). As part of our effort to divorce Rails and JavaScript, we began exposing our Rails models with an API for our asynchronous Redux actions to fetch data from. We decided that Rails models would form a 1:1 relationship with their counterpart in the Redux store, and wrote reducers accordingly. Previously with Angular, we were using the RailsResource library to handle all Rails model interactions. Now, all rails models are fetched, updated, deleted, created, etc. with simple AJAX requests using the fetch polyfill. Many of the problems with our existing Angular code was the use of controllers and scope led to application state that was difficult to reason about. With Redux becoming our state container of choice, these difficulties evaporated and caused us to actively think about application state in ways we had not before. In particular, when working on new features redux and stateless components caused us to proactively consider different loading, error, and blank states for features in our application. React with Redux opened the door for us to start using developer tools in Chrome to both develop components and reason about application state. The react developer tools allow us to easily inspect the DOM created by React to find components and their respective properties for both debugging problems and implementing new features. Similarly, the redux developer tools have greatly helped squashing bugs, as following the state changes in successive actions makes for a great workflow. Shortly after introducing React, work began on what we dubbed “Widgetpedia,” or a common React components library for internal use on our application. Widgetpedia not only functions for us as a library of reusable components, but also provides an environment for rapid prototyping of new ones. With no dependencies on our Rails APIs to load data, Widgetpedia is a place to do pure frontend development and iterate with a UX focus. The addition of hot reloading for our React components allowed us to create new ones quickly and easily before using them in features in our application. It took quite a while to come up with a conclusion to this blog post, and perhaps there really isn't one. The frontend world is rapidly changing and there is no be all-end all solution. You need to look no further than this very engineering blog for posts from a couple years ago detailing how Angular JS solved the problems we had then. There's still work to be done in order to finish separating our frontend from Rails, and there will always be new and popular Javascript frameworks and libraries. For now though, the outlook of our frontend roadmap is one that will make our developers, and most importantly our users, quite happy.", "date": "2017-02-02"},
{"website": "Localytics", "title": "Faster Snowflake Queries through Clustering", "author": ["Michal Klos"], "link": "https://eng.localytics.com/faster-snowflake-queries-through-clustering/", "abstract": "At Localytics we have petabytes of data that needs to be served at low latencies and we use Snowflake in our mix of data processing technologies. Snowflake, like many other MPP databases, has a way of partitioning data to optimize read-time performance by allowing the query engine to prune unneeded data quickly. In Snowflake, the partitioning of the data is called clustering, which is defined by cluster keys you set on a table. The method by which you maintain well-clustered data in a table is called re-clustering. The analogous concepts in well-known MPP databases is: In Snowflake, all of the data is stored on S3 so the concept of a distribution key in Redshift or a segmentation key in Vertica does not transfer. If you have familiarity with transactional databases like MySQL, cluster keys are similar to clustered indexes, though MPP databases do not have a traditional concept of an index. Snowflake has fantastic documentation about the technical details behind clustering so this blog post will concentrate on the strategy for reaching and maintaining a well-clustered data state for optimal query performance. As an early adopter of Snowflake, we were guinea pigs for the clustering feature before it became generally available so we've picked up some learnings along the way that we’d like to share. One caveat is that even though clustering is very handy for our use case, it is not always necessary -- for example in the case of time-series data arriving mostly ordered, leaving the data \"naturally sorted\" may be better. As an early adopter of Snowflake, we were guinea pigs for the clustering feature before it became generally available so we've picked up some learnings along the way that we’d like to share. The primary methodology for picking cluster keys on your table is to choose fields that are accessed frequently in WHERE clauses. Beyond this obvious case, there are a couple of scenarios where adding a cluster key can help speed up queries as a consequence of the fact clustering on a set of fields also sorts the data along those fields: Snowflake is pretty smart about how it organizes the data, so you do not need be afraid to choose a high cardinality key such a UUID or a timestamp. As you bring a new table into production on Snowflake, your first task should be to load a large enough amount of data to accurately represent the composition of the table.  If you did not apply the cluster keys to the table within the creation DDL, you can still apply cluster keys to the table after the fact by using the alter table command, for example alter table tbl cluster by (key_a, key_b) . Snowflake provides two system functions that let you know how well your table is clustered, system$clustering_ratio and system$clustering_information . Your table will more likely than not be very poorly clustered to start. system$clustering_ratio provides a number from 0 to 100, where 0 is bad and 100 is great. We found that the ratio is not always a useful number. For example, we had a table that was clustered on some coarse grain keys and then we ended up adding a UUID to the cluster keys. Our clustering ratio went from the high 90s to the 20, yet performance was still great. The clustering ratio did not handle the high cardinality cluster key well in it's formula. For this reason, we stopped relying on the ratio and instead switched to the clustering_information function. Side-note: A nice trick with the clustering_ratio function is that you can feed it arbitrary fields and it will tell you how well the table is clustered on those keys, as well as where predicates like so: The clustering_information function returns a JSON document that contains a histogram and is a very rich source of data about the state of your table. Here is an example from one of our tables: The buckets 00000 through 00064 describe in how many micro-partitions (similar in concept to files) your cluster keys are split into. The 00001 bucket meaning that only one micro-partition contains a certain cluster key and 00064 meaning that 32 and 64 micro-partitions contain a cluster key. In our concrete example above, the 13 million in the 00001 bucket tells us that 13 million of our micro-partitions fully contain the cluster keys they are responsible for, this is great when a query utilizes one of those cluster keys, Snowflake will be able to quickly find the data resulting in faster query latency. 43 micro-partitions contain cluster keys that are also in up to 64 other micro-partitions, which is bad because we'd need to scan all of these micro-partitions completely to find one of those cluster keys. The goal is have the histogram buckets be skewed towards the lower numbers. As the higher end of the histogram grows, Snowflake needs to do more I/O to fetch a cluster key, resulting in less efficient queries. The goal is have the histogram buckets be skewed towards the lower numbers. What a good histogram state looks like for a table depends on a multitude of factors including the cardinality and coarseness of the cluster keys. For finding the optimal clustering state, load a good amount of data into the table and then manually re-cluster it over and over, checking the histogram with each run. At some point the re-clusters will lose effectiveness and you will reach an equilibrium point where re-clustering more does not pull data towards the smaller buckets anymore -- this is the clustering state that you should aim to maintain. For example, when you start the process your biggest bucket for a table may be 00256 and then after 10 reclusters, you're at 00032 and you can't budge it down anymore with further reclusters -- this is equilibrium. At some point the re-clusters will lose effectiveness and you will reach an equilibrium point where re-clustering more does not pull data towards the smaller buckets anymore -- this is the clustering state that you should aim to maintain. We’ve found that you can chart the clustering histogram and it produces some useful and aesthetically pleasing charts. What we look for in these charts is that we are maintaining the stratification of the buckets at our equilibrium point over time. Here are a couple of examples of charts from our monitoring software: When loading new data make sure you are re-clustering the table after each load. If you are trickle loading, you can separate the load and re-cluster process by re-clustering every several minutes rather than with each load. Since Snowflake supports concurrent DML, this is a good approach for maintaining low latency ingest times. The re-cluster command takes in a re-cluster \"budget\" argument named max_size which is the number of bytes that the compute warehouse will shuffle into optimal micro-partitions -- too little and your table degrades in its clustering status with each load. If you choose a number that is too high, Snowflake is smart enough to not waste compute cycles re-clustering beyond what is necessary. However, Snowflake may still re-cluster the table “too well”, meaning that it would be clustered beyond the point of diminishing returns for query performance per the Pareto principle . As you load data and re-cluster, monitor the histogram and your re-cluster max_size amount. Keep loading data and tweaking the re-cluster amount until you have found an equilibrium point where each load plus re-cluster keeps the histogram steady in the desired state. Track the histogram over time, things may change and suddenly you may find that you were in the 00032 bucket on the high end and now you're in the 01024 bucket which would be bad. If this happens, tweak your re-cluster amount up to course correct. Keep loading data and tweaking the re-cluster amount until you have found an equilibrium point where each load plus re-cluster keeps the histogram steady in the desired state. Snowflake is a powerful database, but as a user you are still responsible for making sure that the data is laid out optimally to maximize query performance. If you follow the steps outlined in this post, you will remove a bunch of factors that could lead to less than optimal query performance. Here is a summary of the steps: Ashton Hepburn for his tireless work on our data loader and re-clustering infrastructure at Localytics. Thierry Cruanes and Benoit Dageville and the rest of the Snowflake team for guiding us through optimizing our environment. Photo Credit: James Padolsey via Unsplash", "date": "2017-03-13"},
{"website": "Localytics", "title": "ODBC and writing your own ActiveRecord adapter", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/odbc-and-writing-your-own-activerecord-adapter/", "abstract": "Today we are open-sourcing our ODBC adapter for ActiveRecord, which allows Ruby on Rails applications to communicate with ODBC-compliant databases. The impetus for this work was an effort to update one of our APIs to run with the latest Rails and ruby. Along the way we released Rails 3.2.x, 4.2.x, and 5.0.x versions of the adapter, along with deploying incremental upgrades to our API as we went. Below is the story of how we made it happen. ODBC (or Open Database Connectivity) is the specification of an API that acts as a common gateway through which a client program can access disparate databases without having to account for individual interfaces. In the Rails world, this is largely analogous to ActiveRecord, which acts as an ORM wrapper around databases and allows applications to communicate with them with the same API. ODBC itself has been around since the early ‘90s. In 2001, by virtue of ruby’s ability to be extended by C libraries, Christian Werner wrote a ruby wrapper for the ODBC C library. Then in 2006 Carl Blakely wrote an ActiveRecord ODBC adapter for Rails 2.1. Both of these libraries work with most of the commonly used DMBSs that you would normally connect to using ODBC, including MySQL, Oracle, DB2, Progress, etc. Our API has used it to connect to both a Vertica database (in production) and a PostgreSQL database (in test). When we started this work we were running Rails 3.2.22.5 (the latest released version of the 3.x branch) and ruby 2.1.5. Our database connection was running through a minimally-touched fork of Christian Werner’s ActiveRecord adapter (it had been updated just enough to get it working in Rails 3). The fork also contained our own hacks to get it to function appropriately when connecting to our data warehouse (our in-house Vertica cluster). Side-note: a large reason why it continued to function is that while the semantics of the functions implemented in the ActiveRecord adapters changed, the call signatures didn’t. In most cases they continued to take the same number of arguments - the values simply changed class. The values continued to respond to the same API, so the functions continued to work. This can likely be counted both for and against ruby depending on your proclivity for dynamically-typed languages. It became clear that the biggest blocker preventing us from upgrading our API’s Rails version was the adapter. Through ActiveRecord’s evolution, it became progressively more difficult to minimally update our fork. We decided to take a ground-up approach and write our own ActiveRecord ODBC adapter that could be swapped in for our existing one. Using a combination of our existing adapter and Rails’ own MySQL and PostgreSQL adapters on the 3.2 branch, we ended up with our initial version. First, a few notes about the way ActiveRecord organizes its code. When a Rails application boots up, it establishes all of the necessary connections to various databases (in the default use case, just the one) through the ActiveRecord::Base::establish_connection method. This method calls out to ActiveRecord::Base::*_connection , where the * is whatever value you specify for the adapter key inside your database.yml . This function is responsible for creating a new adapter object (a subclass of ActiveRecord::ConnectionAdapters::AbstractAdapter ), which is then returned and used as the active connection. The subclasses implement the behavior necessary for the individual DBMS to fulfill the correct interface. While some functions needed to be implemented differently for each DBMS (mostly schema-related logic), some could be shared because of ODBC’s abstraction. The functions that needed to be overridden in order for us to get feature parity with our existing adapter were: In test mode we were using PostgreSQL as a suitable proxy because of the ability to quickly create and seed a new database in both CI and a developer’s laptop, so our first priority was getting a passing test suite for that DBMS. Fortunately, at this point we were able to lean heavily on our existing codebase to function as a test suite proxy. Running our API’s tests allowed us to iterate quickly and remove bugs as we found them. In order to support multiple backend DBMSs, we defined a subclass of ODBCAdapter for each one, overriding the necessary behavior. When a connection is first requested, the ::odbc_connection function queries the connected DBMS for the name and then instantiates the associated ODBCAdapter subclass. If none is found, it creates a null connection. Below is a diagram describing this hierarchy: The null connection actually works in most cases for non schema-related queries for databases that mostly reflect the SQL standard. ARel does a pretty good job of assuming the correct quoting and everything tends to work out. This means that for our own purposes, we didn’t need to create a full-blown Vertica adapter, we only needed to override the methods that we were using. We built out the ODBCAdapter::register method to allow the end user to create their own adapters specifically for this purpose. A minimal Vertica adapter is effectively then: Once we had the adapter built, we swapped it in for our existing adapter. We then began the painstaking process of upgrading both Rails and ruby versions. Along the way we encountered the various improvements that had happened to ActiveRecord over the years, including the type map in Rails 4.2 and the introduction of SqlTypeMetadata in Rails 5. Upgrading to ruby 2.4 proved somewhat difficult because of rb_scan_args explicitly checking the number of arguments provided (which became the difference between ruby-odbc versions 0.99997 and 0.99998 ). Eventually we ended up with our API running Rails 5.0.2 and ruby 2.4.0 in production, using the latest version of our adapter (just in time for the 5.1.0 beta to be released the following day). Polymorphism is a common pattern in programming. You define a common API that multiple objects implement, allowing them to be treated as the same type in various contexts. The name may vary by language: it’s referred to as interfaces (Java, PHP, Go), traits (Scala, Rust), and even roles (Perl). In ruby, it doesn’t have a name; the enforcement of the API’s contract is left to the programmer. Advocates of statically-typed languages mark this as fault for ruby: you can’t rely on the compiler to indicate that a method that needs to implemented hasn’t been. On the other hand, in ruby there is no need to explicitly indicate that multiple objects respond to the same methods. This opens the door for some of the biggest flexibility in ruby, e.g. using method_missing to build mocks in tests , adding a try method to both Object and NilClass , or implementing to_json in various classes so that they can be serialized properly. Both sides of the argument were displayed while building this adapter. Finding the correct methods to implement was a matter of relying on documentation and source code, not relying on a compiler. However, we were able to quickly switch in the adapter and test whenever we made incremental improvements. The lesson this highlights more than anything else is that modern programming languages make tradeoffs in design - as programmers it’s our job to take advantage of the strengths and cope with the tradeoffs. This as opposed to bemoaning the weaknesses and citing them as a reason the language is dead or dying. Either way, we are now successfully connecting to our data warehouse using ODBC, running Rails 5.0.2 and ruby 2.4.0. The adapter is up for public use on rubygems.org , feel free to use it yourself to develop your own Rails applications. When you do please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2017-03-07"},
{"website": "Localytics", "title": "Cogito - Adding “I think” to “IAM”", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/cogito/", "abstract": "Today we are open-sourcing Cogito , an abstraction of AWS IAM syntax. IAM (Identity and Access Management) is AWS’s module that dictates the ability of various users and resources to mutate other resources within the AWS ecosystem. Permissions are described through a JSON-structured policy document that lives within the AWS console. In AWS accounts with many microservices like ours, IAM policies  quickly become difficult to maintain. Ensuring a consistent system while balancing security checks with ease-of-use can lead to such a headache that users avoid dealing with it by tending toward allowing blanket open permissions. In addition, the structure of the JSON policies was difficult to remember and work with in larger tooling. We wanted a tool that could abstract away some of this pain, as well as provide us with a starting point from which to move forward with better practice around our IAM policies. We wanted an intuitive way to describe policies without having to remember a complicated JSON structure, as well as the ability to check our policies into source control. The solution we built for all of these problems was to design a new, intuitive syntax that we could maintain in our own repositories with a minimal learning curve. This was the basis of libcogito . libcogito is a small C library that allows translation between the AWS-specified JSON policy document syntax and Cogito’s own syntax. For example, with the built-in AmazonS3ReadOnlyAccess policy, you get: In Cogito’s syntax, this becomes: For this small policy you can already see a large reduction in size - for more expansive policies the benefits are even greater. Through Cogito you end up writing human-readable statements that are easier to maintain and understand. You can even validate the syntax with the library on your own system, as opposed to having to hit an AWS endpoint to validate. Once we built libcogito , we turned our attention to integrating it into our workflow. We did this in a couple of ways. The first was to build support into our editors for easier management. As it turns out, building .tmbundle s is relatively straightforward, and so we built cogito.tmbundle for syntax highlighting. We also integrated Cogito into humidifier , our open-source tool for managing CloudFormation resources. We’ve been successfully managing our CloudFormation resources for over a year now with the help of humidifier . However, in order to write managed policies with humidifier , you still ended up having to write the JSON and then dumping that into one of the properties. With Cogito, we were able to remedy this by building cogito-rb , a ruby gem that wraps libcogito and allows us to do things like: This code could then be checked in alongside a TestPolicy.iam file, allowing easy maintenance and readability. For more examples of how to use Cogito with Ruby, check out the cogito-rb repository and its associated documentation. This gem is released and available on rubygems . Finally, we integrated through CloudFormation syntax itself, as a custom resource . Custom resources enable stacks to hit external APIs when CloudFormation stacks are created or updated. In our case, this allowed us to build a lambda that uses cogito-py and an AWS linux compiled version of libcogito to perform the translation. All of that is to say, we can write IAM syntax in our CloudFormation stacks. An example stack would look like: When this stack gets deployed, a new managed policy will be created with the Cogito-translated policy document containing: An example of how to deploy this AWS Lambda and more extensive examples can be found in our cogito-resource repository. For more examples of how to use Cogito with python, check out the cogito-py repository and its associated documentation. This package is released and available on pypi . Using Cogito, we were able to build even more tools into our workflow. For example, the following snippets allow us to dump all of our existing IAM policies to a zip file, and then restore from that zip file whenever we want: Overall, we’ve had great success using Cogito within our workflow. The various integration points make it easy to understand and maintain, and it generally works well as means of solving our various pain points with AWS IAM syntax. If you also have headaches working with a large amount of policies in your system, feel free to install cogito in any of its various forms. When you do please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2017-06-07"},
{"website": "Localytics", "title": "Connecting to Snowflake with Ruby on Rails", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/connecting-to-snowflake-with-ruby-on-rails/", "abstract": "At Localytics , one of the tools we use for data processing is the Snowflake data warehouse. We connect to Snowflake in a couple different ways, but our main data retrieval application is a Ruby on Rails API. To accomplish this we use a combination of unixODBC (an open-source implementation of the ODBC standard), Snowflake’s ODBC driver , and our own ODBC ActiveRecord adapter for Ruby on Rails. This sequence of tools allows us to take full advantage of ActiveRecord’s query generation and general ease-of-use while still enjoying all the benefits of a fully cloud-enabled data warehouse such as Snowflake. First, a bit of background on the ODBC standard. ODBC is a common interface through which you can connect to multiple backend databases in the same manner. In this way it enables users to write code now and maintain the ability to migrate later, while also mitigating the pain of learning each DBMS’s idiosyncrasies. You connect to a data store through an ODBC adapter, which implements the ODBC interface for that specific DBMS. For example, the following code will execute the query SELECT id, name FROM users on any database without you needing to make changes to the code, just by passing in a data store name (DSN) as the first command-line argument to this script. DSNs are a string of key-value pairs representing the connection configuration. They correspond to an entry in an odbc.ini file that you configure. You can then reference the configured DSN using an implementation of ODBC (e.g., unixODBC) to connect to an ODBC DBMS like Snowflake. For example, in your odbc.ini file you might have: The configuration above operates under the assumption that you’ve previously installed the adapter for each type of DBMS to which you’re attempting to connect. Installing unixODBC is relatively straightforward on *NIX-based machines (on Windows ODBC actually ships with the OS by default). Run whichever package manager your machine uses (e.g., brew , apt-get , yum , etc.) to install unixodbc and unixodbc-dev (to get the headers needing for linking). Fortunately Snowflake provides great documentation on how to handle the Snowflake-specific steps of getting ODBC set up, so follow those instructions as well. Once you do, make sure to take full advantage of the isql utility that comes with unixODBC , as it can be invaluable for debugging. isql will drop you into an SQL terminal connected to any given DSN; for example: Once you’re comfortably set up with unixODBC and Snowflake’s adapter, you can configure your Ruby on Rails app to connect to Snowflake like you would any other data store. First, add the odbc_adapter gem to your Gemfile like so: Then run bundle install to download the gem to your system. (Note that the major and minor version of the gem are linked to the dependent Rails version, so if your app is not yet running Rails 5.0.x , you’ll need to specify 4.2.3 or 3.2.0 ). Then, edit your config/database.yml to specify the Snowflake connection for a given environment, like so: This tells Rails to use those connection settings when running in production mode. The final step is to register Snowflake as a valid connection option within the odbc_adapter gem. By default, odbc_adapter ships with support for MySQL and PostgreSQL . Fortunately, it also ships with the ability to register you own adapters as well. To accomplish this, add the following code to an initializer, e.g. config/initializers/odbc.rb : This code does a couple of things. It tells the odbc_adapter gem that if when ODBC reports back the connected DBMS’s type it matches the /snowflake/ regex, to use the subsequent block to create a class to act as the adapter. We’re then using the PostgreSQL adapter as the superclass, because the syntax is close enough so as it work. Finally, it handles the Snowflake-specific setup of turning off prepared statements, quoting column names correctly, and forcing strings to come back in UTF-8 encoding. Once you’ve configured the odbc_adapter gem, you can take advantage of it by connecting your models to that connection. First, create a model that corresponds to a table in your Snowflake schema. For instance, in our production schema we have a table called fact_events . Second, call establish_connection to tell ActiveRecord to connect to the correct database configuration from database.yml . For example: Note that if all of your models are going to be reading and writing from Snowflake for a given environment (development, production, etc.) then you can name the connection after the environment and the establish_connection call becomes unnecessary. With these models in place, you can perform any of the normal ActiveRecord queries . This configuration works for us, and we’ve been happily running this code in production since January of 2017. That being said, there are still a couple of things that we’d like to build into our adapter to make it even better. Currently, every project that uses Snowflake needs the initializer mentioned above because the odbc_adapter gem doesn’t come with Snowflake support baked in. At the moment subclassing the PostgreSQL adapter works for us, but we’d like to fully support Snowflake’s driver so that we can take advantage of some of the more advanced UDF capabilities that Snowflake has to offer. The latest version of Rails was recently released, so in order to upgrade our applications we need to go through and ensure that our adapter works with all of the new capabilities of the latest version of ActiveRecord. Our adapter supports prepared statements for the PostgreSQL adapter, but it’s explicitly turned off for MySQL and Snowflake. We’d like to take advantage of caching prepared statements to cut down on memory allocations and generally improve performance by enabling it for these two adapters. Snowflake is a great option for a cloud-based data warehouse, and solved a lot of problems that we’ve had with previous solutions to the problem of storing massive amounts of data. By being ODBC compliant, it enables us to connect using all of our favorite tools with minimal setup. If you also would like to use Snowflake with Ruby on Rails, feel free to install our odbc_adapter gem and give it a shot. When you do please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2017-06-09"},
{"website": "Localytics", "title": "Six Recipes for Software Managers", "author": ["Adam Buggia"], "link": "https://eng.localytics.com/six-recipes-for-software-managers/", "abstract": "The engineering managers at Localytics have been working together for over a year, and through ups and downs, we’ve grown and learned a lot as a team.  The goal of this post is to share some of the lessons we’ve learned with the community.  Some of the content is aspirational but all of it is what we believe are best practices for leading software teams, presented to you in cookbook form.  We hope these recipes provide value to fellow software leaders as well as a window into our engineering culture. Weekly one-on-one meetings between managers and reports is a fundamental component to a healthy engineering organization.  The purpose of these meetings is for managers to support engineers in their day-to-day work as well as help them define and achieve long-term career goals. Collecting periodic surveys is another way to keep a channel open between managers and employees.  Even if you are holding effective one-on-ones, it is important to augment them with weekly surveys: Surveys should be conducted weekly and be lightweight. 15Five CEO David Hassel believes employees should spend 15 minutes every week to fill out progress reports and managers spend no more than five minutes to read each of them. One-on-ones and surveys will have little impact if employees don’t feel comfortable sharing their thoughts.  “Psychological safety” is described by HBS professor Amy Edmondson as ‘‘a sense of confidence that the team will not embarrass, reject or punish someone for speaking up’’.  Google has found this to be the most important quality of high performing teams and if any member of your team does not feel that your workplace is a safe environment  then it should be management’s top priority to address. It can be difficult to find the right format and frequency with which to disseminate status information across a multi-team department.  Transparency is important but oversharing can dilute your message and cause information overload. It’s important to develop teams that are resilient to changing priorities and attrition.  This can be accomplished with a thoughtful approach to team size and composition.  Teams should be large enough to withstand more than one member leaving and diverse enough to handle a wide variety of work. The best developers keep up to date of the latest technologies and techniques and so should their managers.   Reading blog posts, articles and books are basic ways to sharpen management skills.  Many managers don't spend as much time connecting with their peers within the company or industry but learning from other’s experiences one-on-one can be an effective way to step up your game. Photo by Todd Quackenbush", "date": "2016-09-23"},
{"website": "Localytics", "title": "Introducing Exploranda, a set of tools for exploring complex systems.", "author": ["Raphael Luckom"], "link": "https://eng.localytics.com/introducing-exploranda-a-set-of-tools-for-exploring-complex-systems/", "abstract": "At Localytics, data is central to our mission. One of our core principles is to \"lead with data.\" When I envision that, I often think of an analyst using graphs to make a case for a business decision, or a principal engineer using benchmarks to advocate for a particular technology, or an ops engineer's alarm going off, showing a warning about a production system. But when I think more carefully about it, these things are the results , or at least the intermediate products, of data-driven decision-making. In this post, I will focus on the earlier steps of the process, and introduce exploranda , a tool for enabling those first steps. The first step in making decisions based on data is letting something catch your attention. Obvious things will catch your attention sometimes. S3 will go down. Third-party APIs will go down. Sometimes (rarely, of course!) code that you yourself wrote will break. For most of the obvious things there will be a known response pattern: investigate the outage, do whatever is possible to mitigate or undo its effects, conduct a postmortem to address any structural or longer-term issues uncovered by the event, and find a place for longer-term actions in the backlog. This pattern is so effective and well-established that we can use tools to preempt it sometimes: automated monitoring systems tell us about worrying trends in the steady-state of our deployed infrastructure, CI keeps forseeable bugs out of production, CD automates quick rollbacks or forward-fixes. In other cases, the thing that catches your attention will not be obvious, or the cause of an obvious thing will be difficult to discover. Maybe a service \"feels slow,\" even though all the metrics say it should be fine. Maybe transient errors keep occurring in a particular cluster but don't leave evidence in the logs. Maybe you're switching from a familiar technology with well-understood properties to a newer one that isn't as well-known but might offer advantages. These are examples of data exploration tasks that call for curiosity, openmindedness, and flexibility. They are characterized by \"unknown unknowns\" and open-ended problem statements. Unfortunately, the tools that work so well for preempting forseeable problems are not as well-suited to investigating unforseen problems or answering arbitrary questions. Adding a suddenly-important metric to a monitoring system might require a redeployment at a time when a system is already in a bad state. When evaluating a new service, standing up a whole monitoring solution or integrating the new service with an existing monitoring solution might not be an efficient use of time. When investigating a transient problem, the first few things you choose to measure may end up being totally irrelevant. The security requirements of a critical system might put it beyond the reach of third-party tools, but it might be accessible from within a particular network. Exploranda is a tool I built to help with these use-cases in my daily work. It is a Node.js package that enables data exploration by breaking it down into a three-step pipeline: getting raw data from arbitrary sources, transforming the data to highlight features of interest, and displaying the data in a useful way. Exploranda allows me to build complicated models of Localytics infrastructure from simple, mostly-declarative objects. For instance, we have an Elasticsearch cluster running in AWS ECS. The individual nodes are containers running on container instances. The containers write to an EBS volume mounted on the container instance that isn't the root volume. If I want to see the IO stats for the container with the highest disk usage, this is the process I would have to use: By the time I get to the information I was after, I've lost most of the context. I don't just mean that I've lost my train of thought, though I probably have--I also mean that the page with the IO metrics on it has no other contextual information relevant to my question. It doesn't have the IP of the Elasticsearch node. It doesn't show the disk usage. And if I want to compare the IO of this node to the IO of the others, that process is a multiple of the complexity of this one. Here is how that process is represented using exploranda : And when I run that, this is what I see, right in the console: All of the IO stats for all of the instances, identified by their IP addresses, updated in real time, without making a single infrastructure change. Conversely, adding this metric to an existing system would be a much larger effort: because the exact volumes we care about are only identifiable by tracing back through the chain of relationships to the Elasticsearch nodes (or at least their container instances), we would probably need to deploy a special process, like a lambda or an ECS task, to get the data and post it to the monitoring system. Or maybe we could tag the volumes as belonging to Elasticsearch...and set up a Cloudwatch dashboard for them...but we'd have to tag them with the correct IPs as well...Is there a convenient way to fit that into the deployment system? Should we have a design discussion? This kind of friction is deadly to the practice of data exploration, because it raises the cost of asking questions and discourages you from pursuing data unless you already know that it will be relevant. In fact, the measure of success of data exploration should not only be \"Did I learn about the thing that caught my attention in the first place?\" Instead, it should also include \"Did I find something surprising that I wasn't looking for, but which turned out to be important?\" With incredible regularity, I find that using Exploranda to observe the systems I work on uncovers unexpected and interesting things without requiring a lot of effort. Of course, this is not a replacement for other monitoring systems. It polls for data and updates in real time without storing historical data. It complements structural, always-on monitoring systems in two ways: first, it enables easy, flexible data gathering for the times when you just want to answer one question or examine something without a lot of investment. Second, it lets you test out metrics that would be expensive to integrate into an existing system, giving you a cheap way to decide if they would be worth the effort. Finally, Exploranda's transformation and display steps are optional, making it a convenient adapter for any tasks that require AWS or other data. Part of my motivation was the simple fact that traversing the graph of AWS services, from task definition to container instance to ec2 instance to AMI name, commonly results in deeply-nested code that is difficult to reuse, maintain, and extend. Exploranda's use of consistent, documented patterns is intended to alleviate some of that pain, and make it easier to integrate data from multiple sources. Right now it is possible to get information from AWS APIs and HTTP(S) requests; shell and SSH commands are areas of interest I hope to address in the near future. Exploranda is still in beta: the major version is 0, and there may be breaking changes before v1 is released. It's available on github and via npm install exploranda . Feedback is welcome. Exploranda depends on the excellent blessed and blessed-contrib libraries for console display, the awe-inspiring async and lodash libraries for functional primitives, and request along with the aws-sdk to get dependencies. Its development dependencies are minimist for argument parsing in the examples, and jasmine and rewire for tests.", "date": "2018-01-25"},
{"website": "Localytics", "title": "Infrastructure As Code With humidifier-reservoir", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/infrastructure-as-code-with-humidifier-reservoir/", "abstract": "A little more than a year ago we made the first commit to the gem that eventually became humidifier . It’s evolved quite a bit in the last year, including integrating AWS’ resource specification which had the side-effect of greatly stabilizing the API. Here at Localytics, we’ve been using humidifier in myriad ways, including managing our AWS infrastructure, launching and configuring new servers, and aiding in refactoring our network ACLs for better security (to name a few). Today we are open-sourcing humidifier-reservoir , a tool for building AWS infrastructure using humidifier and simple configuration files. This tool has evolved out of our continued effort to streamline our infrastructure while maintaining the configurability that we’ve worked hard to build into humidifier . Below are descriptions and examples on why we built humidifier-reservoir and how you can integrate it into your workflow. First and foremost, we originally built humidifier with flexibility in mind . Previously we had evaluated other excellent tools like Terraform , Ansible , and SparkleFormation . All of these tools work well and are well-supported by the community; however, we wanted something that was both more tightly integrated with AWS CloudFormation and that also allowed us more configuration options that were specific to AWS. When working with humidifier, we found that it was great for dynamic infrastructure - or, infrastructure that changed regularly (e.g., auto-scaling groups for new application versions and their associated target groups). The weakness, however, came from static infrastructure. When we used humidifier to create AWS components that weren’t changing regularly, we ended up duplicating a lot of the functionality in infrastructure configuration tools, with even more boilerplate. In order words, the flexibility of humidifier was outweighed by the burden of having to write everything in code. We ended up determining that the best way forward was to take the best of both worlds - the flexibility of humidifier with the simplicity of non-code configuration files. That resulted in the birth of humidifier-reservoir . humidifier-reservoir allows you to map 1-to-1 resource attributes to resources that can be immediately deployed to CloudFormation. It also allows you to define custom attributes that you can then use humidifier to further configure to your needs. For example, you can specify a couple of AWS IAM users in a users.yml file: In the above example, path , user_name , and groups are all part of the CloudFormation resource specification, so they go straight through into the resultant JSON. Using humidifier-reservoir , you can simplify this further by defining a custom mapping like so: With this mapping in place, your configuration can be simplified down to: This can greatly increase the speed with which you can develop CloudFormation templates, and ultimately makes it easier to deploy them. Finally, using tools already built into humidifier , you can deploy each change incrementally using change sets to view each change as it happens. Further improvements to process can be gained by combining these two open-source AWS infrastructure tools with our third tool: cogito . With some simple code, you can take cogito -syntax IAM statements in deploy them into CloudFormation templates, as in: (In the above example, replace the underscores with / , as it should be in nested subdirectories.) Then by running the ./reservoir CLI, you will have a valid CloudFormation document that you can deploy immediately. At Localytics, we are always working on making our tools better and examining our processes for potential gains. For us, this represented a large speed gain in managing and maintaining CloudFormation templates. By keeping configuration files in a single repository and building tooling around it using humidifier-reservoir , all of our static infrastructure can now be deployed into CloudFormation templates using a simple CLI. Furthermore, changing our existing infrastructure is just a matter of opening a pull request. humidifier-reservoir is up on GitHub here and free for use. When you use it, please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2017-08-23"},
{"website": "Localytics", "title": "Saving Money and Protecting Privacy With Bloom Filters", "author": ["Tristan Garwood"], "link": "https://eng.localytics.com/saving-money-protecting-privacy-with-bloom-filters/", "abstract": "The EU General Data Protection Regulation (GDPR) went into effect on May 25th, 2018 and stipulates that companies processing personally identifiable information (PII) must carry out requests from end users to be ‘forgotten’ from their systems. Implementing this right to be forgotten (RTBF) for our products means that not only do we need to take care of deleting historical data, but also filter out new data uploaded by the Localytics SDK to our backend services. It is impossible to guarantee that data from opted-out users won’t end up in our ingestion pipeline due to the inherently unpredictable nature of distributed systems, so we have to do a GDPR status check against every data-point. To put this in perspective, we ingest on the order of 50k data-points per second on a typical day. As an AWS-native shop, DynamoDB is our NoSQL database of choice for ultra low-latency high-throughput point lookups in our ingestion pipeline. We already use DynamoDB to track and update user profiles for every data-point. At current prices (2018) the required additional read capacity would cost an extra $4500 per month at our present volume. In order to help reduce the extra costs associated with GDPR compliance, we took a look at bloom filters. Bloom filters are probabilistic data structures that allow for quick lookups and efficient memory usage. The tradeoff is that bloom filters have a non-zero false positive rate. However, it will always give you a positive answer for something that is actually in the bloom filter. The size and lookup speed of the bloom filter is directly related to this false positive rate. Our implementation of the GDPR data-point filter is as follows: We fill a bloom filter with opted-out users, serialize it, and upload it to S3. The bloom filters need to be recreated each time because you can only add, not remove keys. Our ingestion pipeline periodically downloads the newest bloom filter version and loads it into memory. We first check an incoming data-point against the bloom filter, and for every positive bloom filter check we make a call to a DynamoDB table that stores the user’s true opt-in/out status. When we verify that the data is from an opted-out user, we simply drop the data-point. This implementation allows us to adjust the amount of DynamoDB calls and thus our costs based on the false positive rate. Once we perform the check for that user against our DynamoDB database, we additionally cache the result to save on subsequent DynamoDB requests. Let’s look at a typical second long window of our data ingestion pipeline and crunch some numbers. First, let me define 1 data-point/second as 1 dpps for convenience, also let’s assume that the dpps from opted-out users is small relative to everyone else. With 50k dpps being pretty standard for daily peak usage and a bloom filter false positive rate of 1 in 10,000, you’re looking at about 50 dpps making it through the bloom filter. Our caching layer (to handle duplicates) typically cuts the actual number of requests to the database in half, so in the end we actually only need to make about 25 dpps dynamo calls. This is only about 0.05% of all the data-points that we actually need to hit the database for and only amounts to about $2 of extra dynamo charges per month. The size of the serialized bloom filter files amount to about 750 KB each, so there is room to decrease the false positive rate even further as long as available memory is not an issue. All in all, the pattern described above for doing simple filtering checks against a database turned out to be very successful. If we had only used a caching layer instead, we would be looking at about 1 thousand times more database calls. We are considering other places in our system where a dynamically updated bloom filter could be helpful in reducing database read costs. As long as you have sufficient memory available and are able to periodically and asynchronously recreate the bloom filter, it is definitely worth considering these handy data structures.", "date": "2018-08-27"},
{"website": "Localytics", "title": "Meet Our Engineers: DeRon Brown", "author": ["Casey Park"], "link": "https://eng.localytics.com/meet-our-engineers-deron-brown/", "abstract": "How did you get started in software engineering and why? I went to a rural high school in Virginia. When I was young I was always interested in math and science and later loved classes like calculus and physics. I was interested in computers, but I didn’t know anything about it, and my high school didn’t offer any programming classes. Either way I was interested in engineering and decided to pursue computer engineering in college. I got into MIT and enrolled in the computer science and engineering program. It was the hardest thing ever. There were students that have been programming for years, and I was brand-new at it. I had to rely on my peers in the beginning, struggled with it, got a lot of help, but eventually fell in love with the subject. It was definitely challenging, but I loved the idea of being able to create something from nothing. What was your path to Localytics? Right after college, I got a job at ESPN and stayed there for four and a half years. I worked in the Innovation group doing R&D and eventually moved to the mobile development group. I went to college in Boston, so I wanted to come back and join a startup. One of the recruiters I worked with put me in touch with Localytics, a company I was fairly familiar with due to ESPN’s partnership with the company. Then, I went through the phone screening process and on-site interviews. After talking to key people here and hearing about some of the problems they are solving, I knew it was where I wanted to work. What are you working on right now? I’m currently working on a project called Places. It’s essentially geofencing a particular area and associating that location with marketing campaigns and other analytics. Do you have any side projects? I have one big side project I’ve been working on for two years. It’s a social networking app called Flockery . The idea is that you create groups of your friends that correspond to your real-life social circles, which I call flocks, and then you use this app as a platform to discuss and plan your social encounters. Anyone can edit and add things to an event, such as polls, time, a location, and other details. Side projects are encouraged here, and that was one thing that really struck me during my interviews. One of the interviewers told me that if we don’t have engineers or employees leaving the company in the next few years to start their own companies, we’re not hiring the right people. We want to hire motivated, high caliber people who may be founders one day. I love this culture at Localytics. What's a fun fact about yourself? I played football at MIT. My junior year I lead the nation in rushing across all divisions and nearly did the same my senior year. I was a 2-time All-American, and my story was featured on the cover of the USA Today. I wanted to go pro, so I went to a bunch of pro days and tried out but didn’t make any team, but someone at ESPN saw my story in USA Today, found out that I was a computer engineer, and thought ESPN would be a great match. They reached out, and I interviewed and got the job. What does Localytics Engineering value? Localytics Engineering values quality. Testing and code coverage are very important to us. We also do a great job of collaborating across disciplines and bringing in outside perspectives, which can be then used to solve problems. I believe that in addition to building a great product, maintaining accuracy and quality is equally important for providing the best service to our customers and their users. What tools do you use everyday in your job? Google, Stack Overflow, Slack, Xcode, Android Studio, Charles Proxy, Sublime, Genymotion. Why do you take pride in working here? I take pride in working here because our product helps to make better mobile apps. The experience of apps trumps most other experiences on the web and desktop. As a developer, I love the idea of building great experiences for people whether it be an ESPN app or Flockery . I like solving interesting problems so that the users can have a better overall app experience. Wanna work with people like DeRon? We're hiring .", "date": "2015-09-03"},
{"website": "Localytics", "title": "On the Road to Continuous Delivery", "author": ["Adam Buggia"], "link": "https://eng.localytics.com/road-to-continuous-delivery/", "abstract": "There are plenty of articles and presentations describing mature Continuous Delivery systems but few that discuss the early evolution of a deployment process. It’s a long road from startup infancy to enterprise adulthood and there are many forms a deployment process will take along the way. Over the past year and a half our engineering team has tripled and the data we process for our customers has increased exponentially. We’ve spent much of that time thinking about how to continue to ship quality code quickly. The Front End team is one of three engineering teams at Localytics. Here is how we looked in the Fall of 2012: Shipping code began by developing locally, writing unit tests (usually), and then opening a Pull Request on GitHub. Once the PR was reviewed and merged to master, running cap deploy (a capistrano command)on the console would deploy master to production. Someone might check Jenkins at some point but Continuous Integration was not formally part of the deploy process. This setup may seem familiar and can work well for small teams but issues crop up as the team grows and the speed of development increases. Here, I highlight three key enhancements to our deployment process that have helped us grow from startup infancy to, say, engineering adolescence: blue green deployment, comprehensive testing, and broader automation. Deploying to a production server using Capistrano is risky. Sometimes deploys fail and the differences between Production and Development environments are difficult to anticipate and test. We eliminated this risk by instituting a Blue Green Deployment system. We configured a load balancer with two clusters of two Rails servers each. At any given time, one of these clusters would be the \"hot\" cluster, used by our customers via dashboard.localytics.com. The other cluster would be the \"cold\" cluster, accessed by an internally available URL. Both environments were connected to the same production databases so the only difference was the URL to accessed them and the version of the code they were running. We updated our deploy process to include two Capistrano tasks instead of one: Let’s dive into the implementation. For technical reasons I won’t get into here we decided not to use Amazons’ ELB service. Instead we rolled our own load balancer which includes nginx for SSL termination chained with HAProxy for load balancing and URL-based routing. We used EC2 Tags to determine which Rails servers were hot and which were cold. Given these details lets take another look at our two Capistrano tasks. cap cold deploy looks up which servers are cold by querying AWS for EC2 instances where tag group = “cold”, then deploys master to those EC2 instances. cap switch_to_cold gets a list of both hot and cold servers from AWS. We also want to update HAProxy which means pointing dashboard.localytics.com traffic to cold servers, and pointing cold server traffic to hot servers. Last, we update EC2 tags. Cold servers are now hot and hot servers are now cold. Implementing our hot and cold (or Blue Green Deployment) system has been an important change that allows us to make a final sanity check before exposing our customers to new code. Also if, shortly after a deploy, Airbrake or New Relic alerts us to a problem with production, we can quickly rollback by switching traffic back to the previously hot servers. Not only was Continuous Integration missing from our deployment process, but our code coverage was a dismal 23%. As a first step we wired up Poltergeist and built out integration tests using Capybara. Next, we configured Jenkins to execute tests on every push to GitHub rather than when we merged to master. We were now notified of test failures as we were developing rather than just before we were ready to deploy. As a team we committed to writing more tests and our code coverage began to climb. A new challenge created by our increased code coverage was the execution time of our test suites. We updated code to speed up tests by tweaking garbage collection and optimizing our factories. We also explored hardware upgrades such as faster CPU, more memory and SSDs. We realized it was a good time to pay someone else to worry about many of these details, so we migrated our test suite to CodeShip . CodeShip makes it easy to execute tests in parallel, it has a slick UI and integrations with many third parties such as Coveralls which we now to use for tracking our code coverage. CodeShip also integrates well with Github so if a test run fails, the Pull Request gets flagged by CodeShip. This prevents the author of the PR from merging to master which prevents broken code from making it to production. Continuous Integration was now officially baked into our deployment pipeline! We had made dramatic improvements to our process but we knew it would be a while until our code coverage approached our goal of 80%. We considered quick wins that could provide us more confidence when making changes to production and came up with Live Tests. There are a several aspects of our production environment that are difficult to replicate in isolated tests. Signing up for a new Localytics account begins on our marketing site and flows onto our Rails app. Data used by our sales team to demo our product is found only on our production analytics database. Eventually we will build mocks, fixtures and staging environments to fully test what is now unique to production but in the meantime we found a high ROI by creating Live Tests that run directly on our production environment. We wired these into the Capistrano task that deploys to our cold servers. Revisiting our deploy process from above, deploying to cold now includes running Live Tests: cap cold deploy Live Tests now serve as a final gate to switching production traffic to cold servers. We don’t want to make these tests too heavy. If they are slow or if they reveal bugs, the deployment pipeline can become blocked. It’s better to test as much as possible before code gets on cold so while we enjoyed these quick wins brought to us by Live Tests, we are continuing to focus on increasing our code coverage within our Continuous Integration test suites. Eighteen months ago, our systems were simpler and our team was smaller. Making changes to production was not a big deal because it was easy for us to keep everything in our heads and avoid mistakes. But as the complexity of our system grew, the knowledge required to update production grew and as we added engineers, the number of people required to know that information increased. To mitigate this exponentially increasing overhead, we reduced the required knowledge by automating common processes. Having solid automated systems abstracts away the underlying details and reduces the opportunity for human error. It allows us to reduce the mindshare dedicated to repeated tedious tasks which allows us to spend more time focused on the product. The first major automation project was Stager. Before Stager, we had a limited number of provisioned staging environments available for Front End engineers to demo and test features. If someone wanted to stage a feature, they would first track down an unused server, then configure the server to point to the correct database, then find the correct Capistrano task for that environment, and finally deploy the changes. But it soon became difficult to keep track of which environment had what features. Product Managers wanted to preview new features so engineers would deploy them to a staging environment, and by the time the PM went to see the feature, the environment had been taken by another engineer. Enter Stager. Stager listens for Pull Requests and automatically creates staging environments for each feature branch. Create a PR and, two minutes later, your changes are live on a fully operational environment. It even adds a comment on the PR with a link to the environment: Close the Pull Request and Stager spins down the environment and removes the comment from GitHub. Front End engineers don’t need to worry about staging environments, Product Managers don’t need to ask where features are staged, and our deploy.rb is much simpler. It just works and was only a couple days before we couldn’t imagine living without it. Stager is based on nginx, Sinatra and Docker. It will soon be open-sourced and is the topic of an upcoming blog post. At this point in our growth, deploying code to production involved running two Capistrano tasks and optionally running migrations on our production database via Rake. The more engineers who could deploy code, the more difficult it became to keep track of who was deploying what. We built Deployer to get this process out of our consoles and into a centralized web application in order to make it easier to: No doubt, many companies have built similar systems for their own deployment needs. There were several requirements specific to our needs which made using open source options, like Deployinator , not a good fit for us. The first step to building Deployer was to provide a wrapper around the Rake tasks we were already using and we were able to save time by leveraging ideas and code from the simple but well-thought out Strano project. We then built out more features around our workflow, added authentication to Github and launched Deployer. We said goodbye to the days of deploying from our console and eliminated contention caused by multiple engineers deploying at the same time. We’re now able to deploy code with the click of a button using the same Capistrano tasks under the hood. We have visibility to who is deploying what and we’re able to rollback to any previous version with a single click. Taking inspiration from GitHub's implementation , we integrated both Deployer and Stager into our chat windows. We’ve been using Slack for the past few months and have enjoyed the slick UI and powerful features. From within a Slack channel, Front End engineers can easily: Logging the conversation between our engineers and our production systems in one Slack channel allows everyone to keep up to date with the current state of production and enables new team members to quickly learn how to deploy code on their own. When the Front End team was only three engineers we were each expected to bring features from design to production which meant we were solely responsible for the quality of each feature. As our team grew we invested in our infrastructure so high quality could remain a reasonable expectation for each of us, even as the system became more complex and engineers became more specialized. This has allowed us to maintain agility and limit the inevitable decrease in incremental productivity for each new hire. Our engineers are the most productive and motivated when it is easy to ship and hard to make mistakes. None of the ideas here are new concepts. We have plenty of work ahead before we have a robust Continuous Delivery system but we’ve done our best to balance our time between building our product and investing in our infrastructure to keep the maturity of our delivery process commensurate with the size of our team. Today we deploy to our web application several times a day. The Localytics Dashboard is one of many internal systems where we dogfood our analytics. The chart below shows a couple weeks in April where we were deploying to the Dashboard three to seven times a day. The App Version dimension below is represented by the Git commit hash of the code that was live at the time. Thank you to Jon Bass, Sandeep Ghael and Tristan Harward for contributing to this article. Thank you to @atmos for advice on our first steps into Chatops.", "date": "2014-05-14"},
{"website": "Localytics", "title": "Akka without the Actors", "author": ["Brian Zeligson"], "link": "https://eng.localytics.com/akka-streams-akka-without-the-actors/", "abstract": "Welcome to the Localytics Engineering Blog! I hope this post is super interesting. Just wanted to drop a note to say that we're hiring Scala Engineers right now, so if you're interested, check out the job description here . Enough business! Enjoy the article! Akka Actors aim to provide \"simple and high-level abstractions for concurrency and parallelism.\" We've made good use of them at Localytics, but I found them in practice to be cumbersome and low-level. I looked  for an alternative approach, and just as I was about to give up, an attractive option fell in my lap: Akka Streams . The following explores why Streams is a simpler, higher-level solution. My initial aversion came from just how much boilerplate you find in actor based code. Every line of code is another liability waiting to come back and bite me, so the less I have to write, the better. Let's see what a Hello World looks like with an Actor: Besides verbosity, there are subtle, yet serious, problems lurking in this actor code. A few examples of underlying problems are: Actors lose type safety . Note the return type of the receive() method on our actor above. You are locked into this as you are defining an abstract member required by the akka.actor.Actor class. Types like Any and Unit show points in your code that are not leveraging compile-time checking, which is arguably the biggest benefit of a statically typed language. Actors do not compose well . Quoting Noel Welsh of Underscore.io: \"By default actors hard-code the receiver of any messages they send. If I create an actor A that sends a message to actor B, and you want to change the receiver to actor C you are basically out of luck.\" Actors often create opacity around the flow of data through a program. This is for the same reason they do not compose. There are far better ways to spend effort than tracing the path of a message by clicking from one actor to another due to a lack of transparency. Actors need tuning . While it's better than managing thread pools, it is still a dark art , and we don't have to settle. Just as I was giving up hope and getting ready to begrudgingly subject another microservice to the gripes listed above, our Chief Software Architect Andrew Rollins gave an impassioned talk about reactive streams at one of our regular engineering lunch-and-learn sessions. Angels sang and I saw a light come down from the sky. This was the answer I was looking for. After a bit of googling and experimenting I had decided to bet the success of my microservice on the Akka Streams library. I found it solved each of the major problems I had with actor based code in the following ways. This is a type-safe implementation. I have materialized an actor for the map stage that punctuates each element from the Source list, but if I had an Int in that list, the program would fail to compile as the map stage explicitly calls for a String. Streams are designed with composability in mind. Let's say you wanted to scale up your \"Hello, World\" app by saying \"hi\" to everyone in the solar system. Here's how you make a larger system from many smaller parts: This makes it trivial to change your data flow, even at run-time, if your program calls for it. Cumulative ordered operations are as simple as chaining calls to via . With minimal use of the FlowGraph DSL , we can achieve more interesting things such as the sayAndShout flow where each individual element is processed in multiple ways. The flow of data is transparent . Look at the implementation of the run method in the example above. If you can make a good argument that this is not clear as day, I'll eat my hat. Streams are auto-tuning . Akka Streams implement the Reactive Streams pattern under the hood. The furthest-downstream stage dictates the pace of data moving through the stream. This ensures that backpressure propagates with no intervention required. This principle is well-illustrated in this slide from Typesafe's Reactive Streams presentation: This also seems like an appropriate time to come clean and admit that the title of this post is a lie . Akka Streams uses actors under the hood to implement the reactive streams pattern. The beauty of this is that I get the all the benefits of actors without having to worry about them in my code. When I first told Jacob Schlather, our resident Akka-tuning expert (and actor proponent), that I did not want to use actors in my code, he challenged me to keep CPU usage above 70 percent. His reasoning was that many of our services do their slowest work in IO. One of the biggest wins you can get is to free up CPU bound work from waiting on IO to complete. I was thrilled to find that my first crack at a streams-based implementation averaged 90 percent CPU consumption, with absolutely no tuning. Here's the current CPU profile for the stream-based service in production: This came for free! It's all thanks to the Reactive Streams implementation that Akka Streams gives you. This service is doing a significant amount of network IO in stream. It's remarkable that the underlying implementation is smart enough to optimize around the slower parts and maximize use of available resources. The next question in mind was \"how will it scale?\" We've tripled the amount of work we are doing with this service. Without having to scale up hardware, our throughput has increased along with it: Today, the service keeps up with our incoming data pipeline using a quarter of the resources required by a non-stream-based service doing comparable work. We are thrilled with the outcome. There were definitely a few challenges along the way. Here are a few takeaways that will make our next streams-based project easier to build: While you might never stream Units like this in real life code, comparing the output of these programs makes the behavior of conflate -> mapConcat apparent: Note that the second run, using conflate -> mapConcat does all the fast work first. As soon as the slow stage can't keep up with the fast stage, conflate kicks in. In this example, we build a list of results from the fast stage, which are then flattened back into individual stream elements for hand-off as the slow stage consumes them. This is a really nice way to optimize resource usage around a slow step in your program. The streams library provides a SynchronousFileSource that will do this for you given a file object, but if you find yourself working with an InputStream other than a file on local disk you will need to do it manually. Here's an example of how you would do that for a specific section of a stream (note the call to withAttributes): Async operations are great for IO, but for CPU bound work the overhead of creating a future or allocating a thread is a needless cost and a net loss This was almost a show-stopper for me. The service in question is a queue consumer, and has to process a large amount of work per queue message. Pulling all of that work in at once defeats the point of the stream. If I don't know where the work ends, how will I know where to tack on the queue message to be acknowledged? A bit of Googling showed that this is not exactly a solved problem, but it is a problem that a few people have clearly encountered. Tim Harper went as far as extending the core components provided by the Akka Streams library. His work provides a version that supports acknowledgement, discussed here. Just at the point where I was beginning to consider a different route altogether, I came up with this simple \"hack\": You could argue this is dirty, but it so far does exactly what I need, and until I'm shown an equally simple and better alternative, I'm sticking with it. This small example is actually representative of the work we are now doing with streams: By creating a separate stream for the file, then concatenating the queue element to the end of the stream, I am guaranteed that the queue message will arrive at the Sink exactly after the last line of the input stream is produced. The Element type family is there to provide type consistency through the stream to the Sink. Running the code yields as follows: All said and done, Akka Streams has been a huge success for us. We've cut down our boilerplate, increased readability and maintainability, reduced time spent tuning performance, and picked up a few tricks to make our next stream based service faster to build. Have your own experiences with Akka Streams? Share in the comments.", "date": "2015-10-11"},
{"website": "Localytics", "title": "Stager: Staging Environments on Autopilot", "author": ["Brian Zeligson"], "link": "https://eng.localytics.com/stager-staging-environments-on-autopilot/", "abstract": "At Localytics we take our code live early and often. It helps us to test features that are in development, and communicate progress throughout the company. We do this to work out in the open, catch issues and deviations as quickly as possible, and just to feel more connected to the end results of our work. Easy deployment is a good thing . The way we used to test our changes on community staging environments was pretty complicated. It required a manual process of deployment using a Capistrano task, on a limited number of reserved boxes. You'd drop into chat to ask if anyone was using the server you planned to deploy to, count to 20 (or something) and pull the trigger. If you were to object to this, and you happened to be getting a cup of coffee during that countdown, your features were clobbered by the next deployment. As the team grew, we knew that our process around staging had to improve to keep our team efficient and open. Enter Stager . Stager takes a Github pull request, and turns it into an isolated running instance of that codebase, entirely automatically. There is no more process around staging your work, it just happens as a byproduct of your normal workflow of shipping code. One week after integrating, Engineering could not imagine working without it. Fast forward 6 months to present, and we've since integrated it further into our systems and our product, services, support, and even finance teams are just as accustomed to having links to Stager instances available for working with forks of our platform. Stager is solving a problem for us that is not unique to Localytics, so we decided early on that we would open source Stager, and paid close attention to flexibility and configurability. Customizations and third party integrations are super to easy to add and the included plugins provide solid templates to start with, so it is easy to tailor to your specific environment and workflow. The remainder of this post will walk you through setting up a Stager instance that responds to Github pull requests, exactly as we're using it at Localytics. If you're interested in setting up a Stager server to integrate with your own environment, read on. And remember, if you need further customization beyond what is covered here, it's covered in detail in the documentation . Stager is based on Sinatra, Docker, and Nginx, so you need those in place first. For the walkthrough, I've launched a new Amazon EC2 running Ubuntu 14.04. After running the commands under the Prerequisites and Quick Setup section of the README, I can now visit http://demo.stager.io and see that things are running correctly. Note that Stager will use subdomains for routing to staged instances, so you also need a wildcard dns entry for the domain name you are using to point to your Stager server. This is a subject that is covered well elsewhere, and there are several methods available. To keep things simple here, we're going to use the manual \" change and commit \" process outlined on the Docker website. Our application is going to be a simple static website, helping us move quickly and focus on getting our Stager instance integrated. We'll run the website using a ruby one-liner, so let's start with an image that already has ruby installed. The Trusted Build Ruby image seems like a good way to do this, so let's run the following: Once that's finished, start a container based on the image as follows: We can now clone our project into the image. For this walkthrough, let's use the stager demo project here, courtesy of html5up.net. Fork the repo then run the following Next we need a boot script. This is what will kick off the web server when Stager launches a container. Run the following inside the container: Last we need to save our changes to a new image. In a new terminal, with the container still running, run the following: Once that's done, you can exit the container session in your original terminal. We now have a docker image for running our app, and we can move on to configuring Stager. Stager needs a minimum configuration before it knows what to do with your docker image. Edit the config.yml in the stager directory, and change it to look like the following: What we are doing here is telling Stager how to use the Docker image we have created, how to route requests from the host machine to a running container, and how we will authenticate. We are also defining a user. Note the BasicAuthentication strategy uses plain-text password stored in the config, and is really only intended for demonstration, or closed network implementation. When we integrate with Github in a future step, this will get better. We've done enough now to use Stager for managing our instances. Let's test with curl from localhost. If you have been running Stager this whole time, kill and restart it. On Stager server: On localhost: Stager will respond with a url. Specifically it adds a url-friendly \"slugged\" version of your container_name parameter as a subdomain on top of your Stager host. Our staged application is now available there: http://test.demo.stager.io Stager uses request handlers to allow customization and extension of the normal workflow via configuration file changes. To turn on Github integration we need to add fields to our config.yml file: Briefly let's look at what each new section does. event_listeners / post_launch_handlers : These are different classes of request handlers , which are configurable and pluggable classes that allow modification of the normal behavior of Stager. Here we have added the behavior to launch and kill staged instances when a PR is opened and closed, comment on a PR when a staged instance is launched for it, and provide an endpoint to assist in taking us through the Github OAuth flow, particularly for use with the Stager CLI gem. github/incoming_auth: We will need to add a webhook to Github in order for Stager to stay in sync with our PRs, and this section tells Stager what credentials to expect with those postbacks to ensure the request is coming from Github. To create the postback, click Settings on your repository, then Webhooks & Services. On the Webhooks page, click Add Webhook, and fill out the form on the following screen. In Payload URL, put the following, replacing the creds with your own and the domain with the domain where your Stager server resides: http://user:password@demo.stager.io/event_receiver Note the user:password you use before the actual URL, then update user and password under github: Under \"Which events would you like to trigger this webhook?\" Example: With this, the webhook is configured, and Stager is able to authenticate any requests from Github, ensuring these calls cannot be faked by anyone else. github/outgoing_auth: In order for Stager to leave a comment on a Pull Request after it has launched an environment, it needs to be able to authenticate to Github. In the case of a private repository, this only requires read access to the repository, so it is recommended that you create a separate Github account with minimum required privileges to your repository for this purpose. For public repositories, it is still best practice to have a separate Github account solely for use by Stager, since you will be storing the credentials plain text in the Stager config file. Once you have created and/or chosen the account under which Stager will post to Github, add the username and password to the user: and password: fields under the github: outgoing_auth: sections of your Stager config.yml We now have completed configuration for Github integration. Restart your stager instance for the config changes to take effect. Now Stager will launch a new instance whenever we open a Pull Request, our docker image needs to know what to do with the information it receives. Stager passes the post body parameters from the webhook to a new container as environment variables, so we can infer the repository and branch from the $image_name and $container_name environment variables respectively. Let's open another shell session with our stager-demo image and update the /run.sh script from Step #2 as follows: Update the startup script to look as follows: Note if you are working with a private repository, your image needs to have an ssh-identity added for a user with access to the repo, along with necessary keys. This can be the same user you created for the PR comments above, with read-only access. Again leave this session running, and in a separate shell, run the following You can now exit the shell session from the first console with those changes saved to the docker image. At this point our Github integration is done, and we can test it out by opening a pull request. You should also notice that when you close or merge the Pull Request, the instance disappears, and when changes are pushed to the branch that has an open PR, the instance refreshes itself. Right now we are controlling access for direct requests to Stager with a plain-text user/password list in config.yml, which is way less than ideal. Since we are already integrated with Github, it makes sense to use Github to authenticate any direct requests made to Stager. We can do this by changing our authentication strategy to GithubAuthentication, which infers a repo name from the image_name parameter on any request, and uses a Github OAuth token to allow a request if it comes from someone with push access to that repo. Let's update our config.yml to use this authentication strategy. Change your config.yml to look like the following: Briefly let's look at the config we have added: repo_owner: This is how the authentication strategy will know which repository to check for access. If you forked the demo application, this will be your Github username. GithubAuthorization: This creates an endpoint that performs the Github web OAuth flow, so that you can use Stager to retrieve an OAuth token. This is used by the Stager CLI to make things more convenient, detailed below. client_id:/client_secret: This is used by the GithubAuthorization endpoint, to perform the web OAuth flow. To get this information, register a new Github Application for Stager to use. Be sure to put the following in the \"Authorization Callback URL\" field. http://your-stager-domain.com/event_receiver The client_id and client_secret are displayed on the following page. Stager is now configured to use Github for authenticating direct requests. Unfortunately, this means that all of your calls to Stager need to be signed with a Github token, so in order to make this easier, we've put together the Stager CLI gem. Rather than curling around manually retrieved Github tokens, the CLI works with Stager to take you through the web OAuth flow and save your token in a config, providing you a higher level syntax for working with Stager that will add your authentication information to all requests under the hood (In case you are thinking of alternate integrations, it is equally extensible as the main Stager project). To get set up with the Stager CLI, follow the installation instructions in the README and then run the following your localhost: Now we can launch and kill stager instances with the following syntax: Remembering that our demo app is now reading branch name from the CONTAINER_NAME parameter, let's launch the master branch. Notice that the first time you run this it will take you through the web OAuth flow to retrieve a Github token. This will be saved in a local config file so you do not have to re-auth on future requests. After some prompts, you should eventually see a browser opened to master.your-stager-host.com, running the master branch of your application. With that, you now have a running example of how we've been staging our work at Localytics for the past few months, and should have a clear idea of how you can make this work for your project (hint: it's mostly about the docker image.) A word of caution for public repositories: Be aware that you are essentially granting anyone with a Github account access to execute any code they want on your Stager server by simply opening a Pull Request. At Localytics we are only using this for private repositories, but if you want to use it for a public repo your security concerns become a much bigger challenge. Remember that the code is running inside a Docker container, and you control access to what application runs (inside the run.sh boot script.) Beyond that, it is open season. Be careful and mindful, and if you do decide to take on the challenge of securing a public repo integrated with Stager, please share your experience and approach, in a gist, on your blog, or in the comments.", "date": "2014-10-28"},
{"website": "Localytics", "title": "A Year on Angular.js on Rails: A Retrospective", "author": ["Joel Rosen"], "link": "https://eng.localytics.com/a-year-on-angular-on-rails/", "abstract": "It's been almost a year since we started using AngularJS in production here at Localytics. We began by experimenting with a small piece of our site and were so happy with it that we rewrote our entire analytics dashboard product — overhauling nearly four years' worth of legacy production code — in Angular. With a surprising lack of documentation or blog posts on real-world use of Angular on production Rails applications, we've had to be fairly creative with how we organized our code and integrated the two technologies. Most documentation on Angular code organization and workflow assumes a JavaScript/Grunt-centric worldview, and it was hard to see how we would introduce Grunt into our Rails kingdom, where Sprockets and Capistrano reign supreme. Meanwhile, we've seen a few cute examples on how to structure a fresh Angular/Rails hello-world seed app, but odds are, if you're in production, you've already got a big steaming Rails app chugging away and don't enjoy the luxury of starting from scratch with rails new my-todo-list-cookbook-twitter-seed-demo-world-app . So we thought we'd share some of the practices and tricks we've come up with at Localytics to get Angular running on Rails. The hardest part about getting started was figuring out a sane way to organize and serve our Angular code through the asset pipeline, while keeping page loads snappy in development. Even before deciding to switch to Angular, page loads had already become maddeningly slow in our local Rails dev environment, with Sprockets taking nearly a full second to serve each JavaScript file. Desperate for a solution, we’d even begun resorting to manually concatenating our JavaScripts into single monolithic files to take some of the load off of Sprockets, abandoning any semblance of modular code organization. Now with a new JavaScript framework on our plate, we were ready to make the leap to Grunt, of which we'd heard much praise. But how? To compile and fingerprint our JavaScript assets through Grunt, we'd also need to stop using Rails' path helpers to link to them from our ERB. And we were quite comfortable deploying with Capistrano. Would we have to rewrite our deploy scripts to accommodate Grunt? The prospect of abandoning the tried and true Rails Way™ for some half-conceived hybrid workflow that would certainly complicate and might or might not ease our development process sounded more ill-advised the more we pondered it. In the end, we came up with some less drastic solutions with which we've been able to dramatically cut asset load time while staying on Sprockets (namely, the rails-dev-tweaks gem, and moving our development environment out of an Ubuntu VM into native OSX). With the asset pipeline humming smoothly again, we've been free to experiment with patterns to integrate Angular while taking full advantage of Rails' directory structure and asset pipeline. Here’s what we’ve been doing: Before moving to Angular, we'd been following the Rails default of loading application-wide JavaScripts in application.js, which in turn loaded shared library-like scripts used throughout the site in /lib . Additionally, we loaded page-specific scripts via a naming convention, where each controller required its own manifest file in /app/javascripts named [CONTROLLER NAME]_controller.js : To keep all our page-specific scripts precompiled in production, we had the following line in our application.rb : With Angular, we chose to stick with this practice of keeping shared code in /lib and page-specific scripts in /app . This way, everybody on our team knows to be careful and write unit tests when changing shared code in /lib , and we minimize site-wide regression risk by keeping page-specific code quarantined. Since we needed to keep our existing code running while we built out our new Angular screens, we kept things organized by simply adding an /angular subfolder: With this new folder we needed to add another entry to our precompile list in application.rb : Now the trick was to find a way to both load up the required scripts for a screen, and to tell Angular the name of the module to bootstrap. Our simple but effective solution was to use an instance variable named @ng_app , which we use in three places: Since Angular module names are just strings, which allow slashes, we can even use a nested directory structure for our scripts if we want. For example, we keep all admin-related controllers and scripts namespaced, so the @ng_app for the admin home screen is just \"admin/home\". As you can see, we embraced Sprocket's naming convention of index.js for our JavaScript manifest files. Through trial and error, we've found it's best to keep these manifest files as lean as possible. The only things that go in them should be Sprockets directives ( require , require_tree , etc.), and a top-level Angular module definition, with configuration or run blocks as needed. Otherwise, all your JavaScript ends up living in files named index.js, which will make you go insane. We also experimented with two patterns for Angular module definition. Our first experiment — let's call it the Top-Down Approach — looked like this: With this approach, the manifest file defines the module, calls require_self first, so that the module definition is run, then calls require_tree . Then each file in the tree re-opens the module and tacks things onto it like controllers and services. While easy, we eventually found that as our Angular codebase grew, this method wasn't great for maintainability since our modules tended to grow monolithic, and unwittingly led to creation of circular dependencies that didn't surface until we tried to refactor or write tests. Now we try to stick to a more granular Bottom-Up Approach that looks like this: Note that with this pattern, require_tree is called first, then require_self is called and the top level module is created, explicitly requiring each submodule in the directory. This way, it's easier to stay on top of dependency management since each module is explicit about its requirements, which also makes it easier to isolate individual modules for unit tests. One of our earlier and more spectacular failed attempts to integrate Angular and Rails was to serve Angular templates on the fly through the asset pipeline. Many Angular examples demonstrate lazily loading and caching partials from the server on demand with templateUrl . We tried this method by creating an /app/assets/templates directory from which we served HTML partials, which we linked to by using asset_path in js.coffee.erb files. So for example, we would set up our Angular routes like so: This kind of worked, except that we eventually realized that we had to deploy twice every time we touched one of these partial templates to get the changes to show up. We suspect this was due to a bug in Sprockets, where the js.coffee.erb and asset_url links would be compiled and evaluated before the HTML templates were recompiled, so the links would always be pointing to versioned templates from the previous deploy. We considered pulling these files out of the asset pipeline and serving them from /public , but we really didn't want our users' browsers to cache these templates between deploys, and fingerprinting was the only sure way we knew of to ensure 100% cache busting. In the end we made a little Rails helper function to render all our partial templates into <script type=\"text/ng-template\"> tags: Sure it would be nice to only load Angular templates on the fly like all the cool kids do, but we think the overhead of a few extra lines of HTML on page load is minimal, and with this problem solved, we've been able to devote our energies to more important things like... Testing was another major reason we had wanted to switch to Grunt and break free of the asset pipeline. We wanted to use Karma to unit-test our JavaScript, but didn't see how this would be possible if our assets were served through Rails. And end-to-end tests with Angular's test runner or this new-fangled Protractor thingy seemed entirely impractical, considering the amount of backend data setup our Rails tests require just to render a page, for which we rely heavily on tools like FactoryGirl. We just couldn't imagine how to bridge the JavaScript and Ruby testing worlds using the tools at hand, and all the documentation assured us that any end-to-end test attempted on an Angular page from our Ruby testing environment would fail, because none of our Ruby test drivers would know how to wait for Angular to compile the page. As it turns out, they were wrong. Capybara and Poltergeist play just fine with Angular, and we've begun building out some wonderful integration tests with MiniTest that navigate to our Angular screens, click around using normal CSS selectors (none of this super-Angular-specific element( by.binding('yourName')) silliness for us, thank you), and even assert that records in our test database have been changed through our simulated user interaction. In addition to normal Rails integration tests that run against a test database, we've also put in place a set of livetests that run against our deploy targets through an after:deploy hook in Capistrano. These livetests provide full stack coverage against real production data, even ensuring that asynchronous queries to our API are returning successfully and that our charts load without errors. To accomplish this, we've taken advantage of one of Poltergeist's nicest features, which is enabled by default: the ability to re-raise JavaScript errors in Ruby. This means we can get some great practical test coverage just by hitting every page and waiting for Ajax requests to finish. Unfortunately, this doesn't work right out of the box with Angular, because its $exceptionHandler service catches any errors thrown and safely logs them to the console, preventing Poltergeist from raising them in our tests. To circumvent this, we used Poltergeist's extensions option to inject a script into each page that effectively monkeypatches Angular's error logger: As for unit testing, we are using Karma to run Jasmine tests, which we mostly write for our Angular services. It's a little hairy, but we achieved this through a rake task that boots up a Rails server, adds our /spec/javascripts directory to Sprockets's load path (the same as the jasminerice gem), spits out a karma.coffee configuration file that includes a link to the test files on the server, and runs Karma against this file using PhantomJS. We are unable enjoy the full awesomeness of Karma this way, since it isn't polling our served assets for changes and automatically running tests in the background, but at least our continuous integration server is able to run our Jasmine tests, and we haven't been forced to pull our scripts out of the asset pipeline to do so. Now that we have these two powerhouses, Rails and Angular, working side by side, we are faced with a new dilemma: which tool to use? When we first began using Angular we were overwhelmed by its power and became tempted to use it for everything, even for tasks that would have been better done server-side. Now with our honeymoon behind us, we can take a tempered look at Angular and consider these principles before setting finger to keyboard: To illustrate these principles, allow me to share a story in which we wantonly violated all three. We began with a new Subscription model in Rails to track customer access to our new products. We needed information from this model in the UI, so the first thing we did was create a Subscription service in Angular, which we populated with the current user's subscription records by dumping them all into gon on every page load. Some of these subscriptions might have been expired or duplicates, so we filled the Angular service with methods to retrieve the most current and relevant subscription, and wrote Jasmine tests for these — all mostly duplicates of methods and tests that already existed in Rails. After we released our new UI and the had dust settled, we took another look at this Angular service and found that basically the only thing it was ultimately used for was printing a static string in the footer of the page that said \"Thanks for using Localytics Enterprise Analytics.\" We cut the entire service out — about 30 lines of CoffeeScript plus 60 lines of Jasmine tests — and replaced it with a one-line method on the Ruby model, which we called from our ERB template. Refactoring never felt so good. We're still reinventing our practices as we evolve, but we're mostly pretty happy with how we have Angular and Rails playing together, and no longer plan to abandon the asset pipeline. That is, until we throw away our Rails project and rewrite it in Clojure, as my boss keeps half-joking. Do you have an Angular/Rails app in production? We’re interested to hear how others have tackled these problems. P.S.: We're hiring .", "date": "2014-01-27"},
{"website": "Localytics", "title": "Spark, Whistling Past the Data Platform Graveyard", "author": ["Michal Klos"], "link": "https://eng.localytics.com/spark-passing-data-platform-gravestones/", "abstract": "We are in the Golden Age of Data. For those of us on the front-lines, it doesn’t feel that way. Every step forward this technology takes, the need for deeper analytics takes two. We're constantly catching up. Necessity is the mother of invention, and mum’s been very busy. In the past decade we have seen ground-breaking after ground-breaking technological advancements in data processing technology and in 2015 we have arrived in the age in the Spark. Let's take a look at the needs (and innovative responses) that brought us to where we are today. These were the dark days of inflexible batch processes that produced inflexible canned reports. It felt like you were in a full body cast. This was probably for the best, because otherwise you might just hit your head against the wall. The most common approach to data platforms was to use batch Extract-Transform-Load (ETL) processes to transform incoming data into ready-made chunks that would be bulk-loaded into a data warehouse. For low-latency queries, data warehouses were complemented with OLAP Cubes. Inflexibility was immense and most data platforms were on a daily schedule with simple changes in business logic resulting in weeks if not months of cascading technical work. An OLAP cube is a multidimensional database that is optimized for data warehouse and online analytical processing (OLAP) applications. This was golden age for commercial vendors yet difficult times for enterprises managing even small amounts of data. Businesses needed to move quicker, and these bulky processes were slowing businesses down. Businesses had ad-hoc questions that were beyond the reaches of canned reports and this was the main catalyst of change. From 2006 to 2009, Multiple Parallel-Processors (MPP) databases brought scalability and ridiculous speed to the Data Warehouse and made OLAP cubes obsolete, resulting in a consolidation of the stack. MPP vendors such as Greenplum , Netezza , and Vertica rose to dominance and former industry leaders responded with their own solutions such as Oracle's Exadata; Teradata had been playing in this space already. Even in the nascent days of MPPs change was occurring: business needs were evolving and the ugly beast of semi-structured data was rearing it’s head. Semi-structured data started flooding data platforms with the rise of NoSQL databases like MongoDB and the increased need to analyze RESTful and SOAP API log and response data. The liberation of developers from strict schemas conflicted directly with the foundation of relational databases. Companies wanted to analyze these new data sources and the pressure to massage semi-structured and unstructured data into a strict schema put a massive strain on ETL processes. Beyond this, there was another fundamental problem: companies were accumulating and collecting data that they could not fit into a relational data model because they did not yet know how they were going to use it. The restrictiveness of needing a data model a priori meant that truly exploratory analytics to unlock hidden value in data was still nascent. Hadoop came onto the scene and gave businesses a place to dump ANY type of data and allow proto-data-scientists to poke sticks at it, relieving the pressure on MPPs to be everything to everyone. In 2009, Cloudera released the first version of their Hadoop distribution, and by 2010 Hadoop was starting to become a household name in mainstream enterprises. The losers in this shift quickly became ETL tools, which were displaced in droves by Hadoop which could do all of that heavy lifting as well. The best practice architecture quickly became Hadoop + MPP with Hadoop becoming de-facto ETL platform transforming data to be loaded to MPP databases. Whatever could not be shoved into the MPP database was analyzed in Hadoop — albeit much more slowly through tools like Hive and Pig. This was a good point of stability, but business needs were changing again: increased data volumes put massive pressure on the MPPs to load data quickly, and the data with the most value to extract shifted from being the structured data to the semi-structured data that was sitting in Hadoop. This meant that executing analytics directly on Hadoop became critical. MPP vendors put out \"Hadoop connectors\" that could pull data from Hadoop into the MPP for processing -- but this had a very negative impact on performance as compute needed to be close to storage. There was another simultaneous shift -- the need to analyze streams of data in near real-time. The solution was starting to become clear: the world needed a system that could take in vast amounts of data and perform batch and streaming operations without flinching. Nathan Marz created the concept of the Lambda architecture based on his work at Twitter. In Lambda, there is a \"batch layer\" in Hadoop which does safe, reliable processing of data, and a \"speed layer\" which does real-time stream processing and does needs not be super-reliable. The Lambda stack is traditionally created with Kafka + Storm as the speed layer and Hadoop as the batch layer. The stack would process the same data in both layers, the speed layer reacting as soon as the data was created and the batch layer following up with more reliable, hardened processing. The Lambda architecture’s main issues come from its complexity. Jay Kreps did a great job of exploring this in his blog post . Data engineers needed to implement business logic in two places, with two frameworks, managing two infrastructures, and composing a single view of the data from two sources. The market and community reacted to these shortcomings—there was Summingbird which provided a common API for the speed and batch layer; and then there was Hortonworks included Storm in their Hadoop distribution, unifying infrastructure and management to some degree. Lambda is very relevant today and has a lot of great properties, but many data engineers kept their eyes fixed on the horizons, looking for changes in technology that would make building a stack less like rocket science. Today, Spark is dominating mindshare in the data engineering community. Even as an emerging technology Spark addresses a lot of the issues discussed in the previous sections: Apache Tez deserves a mention because it is a framework that overlaps with Spark in terms of being able to construct an Direct Acyclical Graph (DAG) that distributes and executes processing across tiered storage. Tez was developed to plug into existing frameworks that have data engineer-friendly APIs such as Pig, Hive and Cascading . It was not meant to be used directly by data engineers because it’s APIs are so low-level. As a result, it has not received the same amount of attention in the community, but Hortonworks is responding with the Spark-on-Tez project which should be exciting to watch. Last but not least, even though Spark can live outside of Hadoop, the two are intertwined since most of the data Spark will be processing lives in HDFS. The gravity of HDFS is immense as it builds a “data fabric” unto which analytic applications are built, and cannot be ignored. Spark will need to continue to build upon and improve its Hadoop ecosystem support. At Localytics we are continuously innovating how we approach our data platform. We are ingesting 2B data points a day into our stack built with SQS, Scala+Akka, DynamoDB, EMR and MPP databases. As our business has evolved, we needed to introduce another cornerstone piece, and we chose Spark. Last week, we deployed into production our first Spark-based product, Auto-Profiles , that is feeding off of our firehose of data. We’ve been really pleased with Spark thus far, and we are going to build more. If you are looking for a fun, exciting challenge, come join our team !", "date": "2015-01-28"},
{"website": "Localytics", "title": "Onboarding Engineers", "author": ["Adam Buggia"], "link": "https://eng.localytics.com/onboarding-engineers/", "abstract": "An effective onboarding process is important for productivity and morale. For engineers, this process begins when the candidate accepts the job offer and ends when their code is first used by your customers. Onboarding can be broken down to two phases: the time before the first day and the time to the first production deploy. Hiring managers should make sure both phases are executed as efficiently as possible. Starting a new job can be stressful. The time between accepting an offer and showing up the first day can be filled with anticipation as well as anxiety. Once the the Employment Agreement is signed I make sure to immediately set up all internal accounts for the new hire. At Localytics, this means email account and distributions, Slack account and channels and Github access. We usually have employees contributing to our internal technical discussions before their start date which provides us value and makes them feel like a part of the team. This is also the time to invite new hires to any company social events occurring before their start date. If the start date is not for a few weeks, invite the employee out to lunch with the team. It is important to keep the connection warm. Even before the first day in the office, employees should have the opportunity to experience the culture and feel as though they are a part of the team. It makes the first day smoother and reduces the risk of counter offers. On the first day, the newly minted engineer should be empowered to make an impact as soon as possible. Ideally, engineers should deploy to production on their first day . At Localytics, our most recent hire and new graduate, Chris Jelly , deployed an important and non-trivial enhancement on his first day. In part, this was facilitated by our prior investment in internal tools which should be a priority in any technology company. Deploying code to production should be as easy as possible. At Localytics, deploying to production is as easy as entering two commands into your chat window . Empowering your engineers to deploy code on their first day isn’t just important for the short term value for the company, it is also motivating for your engineers. Harvard research shows that nothing is more motivating than progress . Imagine how frustrated Chris would be if his first day was spent toiling with Ruby versions, MySQL drivers and complicated staging environments. Instead, his first day ended with him leaving the building receiving high fives from his new teammates. P.S.: We are hiring .", "date": "2014-06-17"},
{"website": "Localytics", "title": "Optimizing MongoDB: Lessons Learned at Localytics", "author": ["Andrew Rollins"], "link": "https://eng.localytics.com/optimizing-mongodb-lessons-learned-at-localytics/", "abstract": "I recently gave a presentation on MongoDB at MongoNYC 2011. The presentation was titled “Optimizing MongoDB: Lessons Learned at Localytics.” The purpose of the presentation was to give a run-down of the tips and tricks learned as we’ve grown on MongoDB. These include tactics we’ve used to keep our data and index sizes down, as well as gotchas and how we met challenges. Here I will elaborate on the finer details, as well as address some points that were raised by others. Before I get into the details, here’s a high level of what’s covered: Here are the slides from the talk, if you'd like to peruse them: This section is all about basic tips to decrease the size of your documents. When storing many documents, these small wins can really add up. One thing I’d like to elaborate on was the idea of prefix indexes. This concept already exists in other databases, but in MongoDB we have to emulate it ourselves. The idea is that if you have a large field with a low cardinality (i.e., there aren’t many possible values), then you’ll save tons of space by only indexing the first few bytes. This is done by splitting the large field into two. The first field is called the prefix, and it’s what you’ll index. The other field contains the remainder of the field (suffix). When you do a find, be sure to specify the prefix and suffix in the query. How many bytes you index is subject to how many possible values you expect. BSON (MongoDB’s internal format) can represent 32-bit and 64-bit integers much more efficiently than arbitrary length binary strings, so I usually pick a 4 or 8 byte prefix packed into an integer. As another note, for UUIDs, MD5, SHA, etc., I said to use Binary data types instead of hex strings. This saves a ton of space. One finer detail was that I specified using Binary subtype zero. Some language drivers still default to type 2, but type 2 is both deprecated and uses a handful more bytes in BSON form. Something I didn’t say – which Brendan of Scala driver fame points out - is that you should use type 3 and 5 for UUID and MD5 respectively. In this section, I detail how fragmentation happens, and how it also arises during migrations. As @ikai said , “pay attention in your OS class.” Fragmentation was what nailed Foursquare late last year. Thankfully, a great post-mortem was provided. Fortunately, you can easily deal with fragmentation. The simplest way is to regularly compact your database. Up until now you’d use “mongod –repair”, but in 1.9+ you’ll be able to do a faster in-place compaction. Both methods hold a lock for the duration of the compaction, so you need to compact slaves, swap them in as primaries, repeat. This might sound like a lot of work, but it’s really pretty easy. 10gen is aware of the need for online compaction (i.e., doesn’t lock for the entire duration of compaction). You can be sure they are working on solutions. Online compaction will be a huge plus, and address one of the larger criticisms of MongoDB. You can also avoid fragmentation with careful padding of records (which MongoDB will attempt to do automatically), but it means you need a good idea of how big your records tend to get. At the end of the day, every database needs regular maintenance, whether they do it automatically or not, and you have to plan ahead for how this affects system load. As noted in the presentation, fragmentation can also happen as a result of chunk migrations in a sharded system. Fortunately, this is easily avoided by either pre-splitting and moving or by choosing better shard keys . Pre-splitting is pretty obvious, but why a better shard key helps might not be. Avoiding fragmentation with a better shard key usually means choosing a shard key that has insert time as its first component (FYI, ObjectId does innately). In absence of deletes, MongoDB will keep appending data to files. If your shard key is prefixed on time, a chunk will contain data inserted around the same time and thus near each other on disk. When items get moved, larger contiguous ranges of disk will get moved, meaning less fine grained page fragmentation. The other implication of insert time based shard keys is that you don’t add to old chunks, only the latest chunk. This means the newest chunks are hot, but it also means you only split (but not necessarily move) what’s hot in RAM. You can avoid constantly having one hot chunk by increasing the coarseness of your time prefix. For example, if you use a day granularity, then you’ll have one hot chunk at the start of a new day, but it will quickly split and move such that the rest of the day sees even spread across the shards. There is an advanced tactic for avoiding hot spots with time based shard keys – pre-create chunks and move them while their empty. Do this periodically before the start of each new time interval. In other words, if your shard key is {day: X, id: Y}, before the start of each day pre-split enough chunks for the upcoming day and balance them across shards with moveChunk . Here I share some tricks and lessons learned about hardware and running in EC2. Some tricks such as pre-fetching and “shard per core” are really only necessary today because of MongoDB’s simple concurrency model. In the future they will become pointless, because 10gen is working on large improvements to the both the granularity of their locks and when they yield. This is, in my opinion, an area which will have a huge impact on MongoDB’s performance. 10gen is very aware of this, and in the MongoNYC conference it was stated that they expect to provide major improvements to concurrency in every release for the next year. It’s high on their todo. Other lessons, such as those about EC2 and EBS, are really just observations/data to help you frame your approach. Be sure to check out these references: I’d like to address the reaction the slides received on Twitter, particularly the “Working Set in RAM” slide. This slide involved a graph from a benchmark we did to understand the degradation in performance when your working size grows beyond RAM. We spun up an instance on EC2, attached 16 EBS volumes, installed MongoDB, and hammered it with clients. The workload was an infinite loop: write 500 bytes in a new doc, query a random old doc with equal probability, repeat. The graph shows ops/sec over time. It dips when data plus indexes grows beyond memory, taking about two hours to bottom out. …the real point of the “Working Set in RAM” slide is that, as with any database, when your working set is in RAM you will achieve orders of magnitude more throughput than when it is not It’s a rather pathological case, because the working set is the whole DB, and the read spread has even probability. In the real world, this is rarely the case. With most workloads, you tend to read the more recently written data, meaning that even if you have tons of data, you probably only read the most recently written portions the vast majority of the time. For example, you might have 100GB of data, but 90% of the time you read things written in the last 10GB. In that case, you’d still be able to service most requests from RAM with a modest setup. However, what we tested was a much less forgiving scenario. Consequently, the graph taken by itself could misrepresent the actual outcome for “normal” workloads. Extreme scenarios aside, the real point of the “Working Set in RAM” slide is that, as with any database, when your working set is in RAM you will achieve orders of magnitude more throughput than when it is not. That’s all I wanted to say with that slide. Some critics, however, picked out the exact numbers (something which in hindsight I should have left out without more explanation) and used them as evidence against MongoDB. “MongoDB only gets 1k ops/sec when hitting disk? That’s terrible, I can do 10K ops/sec in [insert conventional RDBMS] on my hardware.” MongoDB can also do that. The test I showed was on EC2 with EBS, a rather slow IO system. Given commodity hardware with a RAID array of high RPM drives, I’ll show you 10K+ ops/sec on the same disk-bound test. Most comments attacking the 1k ops/sec shown in those graphs are really complaining about EC2/EBS, not MongoDB, even though they might want to blame MongoDB. Case in point, for workloads identical to the test shown, MySQL will do roughly the same ops/sec directly on EC2 or using Amazon RDS. I’ve also tried several other databases with similar workloads. You’ll have a hard time doing random disk-bound work on EC2 regardless of the DB. The bottom line is that the controversy around that graph and its disk-bound throughput number really isn’t MongoDB vs [insert your favorite DB], it’s all about the scenario, the hardware, and the workloads for which a particular database is designed. Many of the counter-points involving other databases were actually referencing different tests and/or different hardware. It’s important that we recognize the distinction and know what we’re actually faulting. Declaring that your DB crushes MongoDB – when you’re talking about different workloads or hardware – is especially irresponsible without context. It sets unrealistic expectations for those new to these systems and databases. I highly recommend checking out the comments in this Hacker News post, Scaling with MongoDB (or how Urban Airship abandoned it for PostgreSQL) . There are some very insightful comments about working set, disk-bound workloads, and how MongoDB compares to other databases. I’ve commented with some of my own extended thoughts in the post , but the threads involving FooBarWidget and rbranson are particularly enlightening. Overall, MongoDB has been great. I had no intention to come across as negative, and was in fact rather shocked when people cited my presentation as proof to abandon MongoDB. While my presentation might have included some cons, every database will present similar challenges. They all require tweaking (and so does your data model). In the end, we achieved the performance we needed at the cost we needed. I’m still open to new ideas, though, so I encourage discussion on other approaches. MongoDB isn’t perfect and could especially use improvements to its concurrency and compaction. It’s important that we approach these databases scientifically, putting them through the wringer with relevant tests. Claims of throughput and performance should be taken with a grain of salt. What were the test parameters, what was the hardware? Read up on these databases, understand their internals, and design accordingly. Don’t take numbers in marketing material entirely on faith. That said, there are reasons beyond performance for choosing a database. Apart from MongoDB itself, 10gen has provided us rock solid support. Even though it’s an open source database, it’s nice knowing that there is a company dedicated to its advancement. Many new solutions – especially in NoSQL land – don’t have that kind of force backing it. Some do, but 10gen is definitely up there. On top of that, the community has been stellar. MongoNYC 2011 was good proof of that. There’s also documentation, language support, 3rd party services, feature set, ease of use… the list keeps going. Another factor has been stability. I’ve done rigorous stability tests on many databases, and among the new wave of databases, MongoDB (1.6+) was one of the best (also huge props to Riak, equally stable if not better). While it’s hard to beat the stability of tried and true DBs like MySQL and PostgreSQL, it’s been worth the risk to get the benefit of integrated sharding and auto-failover that’s easy to use. At MongoNYC 2011, Harry Heymann of Foursquare cited this as his number one reason for choosing MongoDB over sticking with their existing RDBMS solution. Check out the video of Harry’s “MongoDB at Foursquare” presentation. We are very happy with MongoDB and 10gen. I’m just trying to help other people by sharing what I’ve learned.", "date": "2011-06-27"},
{"website": "Localytics", "title": "AWS Lambda and Event-Driven Computing: The Next Big Thing for Data Pipelines", "author": ["Andrew Rollins"], "link": "https://eng.localytics.com/aws-lambda-and-event-driven-computing-the-next-big-thing-for-data-pipelines/", "abstract": "Two weeks ago, several of us from Localytics attended AWS re:Invent , Amazon’s yearly event. Jon Bass , Mohit Dilawari , and I spent the entire week representing Localytics at the conference. We learned a ton. We heard about new features, met with product teams, and even participated in the Internet of Things HackDay. We originally wanted to write a blog post recapping everything, but found we had so much to say about AWS Lambda and Event-Driven Computing that it deserved its own post. Lambda is an important development for AWS. Let’s dive a bit into Lambda and see what it means for event-driven computing and data pipelines. Consider a common problem in AWS: what do you do if you want to listen to change events from any part of your infrastructure and react to those events in a timely manner? Until recently, your best bet was CloudTrail. CloudTrail dumps AWS API request logs to S3 and posts notifications to SNS. This is pretty good, but CloudTrail isn’t necessarily timely (up to 15 minutes of lag) because you are essentially waiting for logs to rollover and post. Ultimately, if you were doing something such as processing images uploaded to S3, you’d be better off enqueuing messages to SQS yourself in your application code upon receipt of the image instead of waiting for CloudTrail. Wouldn’t it be better if services posted directly to SQS, SNS, or some type of real time stream like Kinesis? Amazon realized this and announced event notifications for two services: S3 Notifications and DynamoDB Streams . Both of these were announced during re:Invent. S3 Notifications post to SQS and SNS upon certain S3 operations, and DynamoDB Streams are effectively operation logs on your DynamoDB tables that are exposed in a similar fashion to Kinesis Streams. To be clear, Dynamo Streams are not Kinesis - for example, they provide exactly once semantics instead of the at least once semantics found in Kinesis - but the Kinesis Client Library works against both Kinesis and DynamoDB streams. Neat. We now have real time event streams coming from two services and more services are sure to follow. However, with the proliferation of events across various services, new problems arise. Fragmentation is a problem, because events are exposed in different mediums (SQS/SNS vs Kineses compatible streams). Orchestration is another problem, because we still need to build out infrastructure to listen and process the events. Or do we? Enter the big reveal: AWS Lambda . AWS Lambda In a nutshell, AWS is building the primitives to deploy simple workers that respond to events and trigger other events without having to manage servers or event queues. Here’s Amazon’s description: “With Lambda, you simply create a Lambda function, give it permission to access specific AWS resources, and then connect the function to your AWS resources. Lambda will automatically run code in response to modifications to objects uploaded to Amazon Simple Storage Service (S3) buckets, messages arriving in Amazon Kinesis streams, or table updates in Amazon DynamoDB.” ( Lambda announcement ) A Lambda function is a Node.js application. An event happens, AWS launches your Node.js application and passes in the event information. This happens on a farm of servers managed by AWS. You get charged for slices of time. Fragmentation and orchestration are none of your concern - Amazon handles all of the plumbing. Official support for more frameworks is on the horizon, but people have already hacked in languages such as Go . Lambda isn’t a particularly new concept - databases have had triggers for a long time - but doing this at the cloud level is game changing. Building multi-service, event-based triggers on your own would have been a massive undertaking - you’d have to assemble SQS, Kinesis, DynamoDB, and EC2 clusters just to do it properly - but now you get it as part of AWS. As an aside, it’s interesting to see Lambda announced right alongside the EC2 Container Service, because you could possibly see Lambda as an assault on containerization. Containers are often sold as a lightweight way to distribute disposable tasks, but Lambda is even lighter. With Lambda, the possibilities for data pipelines are endless, and the beauty of it is that you get to focus on your business logic instead of passing around data. You define a directed acyclic graph of small, composable functions and the rest happens on it own. This is the promise of Event-Driven Computing and Lambda. For example, imagine the beginnings of a mobile app analytics pipeline like Localytics. First you upload datapoints into S3. That triggers two lambdas, one that writes aggregates to DynamoDB and another that sends a push notification back to the app upon certain conditions (suppose you want to send promotions to the user on certain actions). Writing the aggregate to DynamoDB can trigger another lambda, too, perhaps one that is monitoring for thresholds and sending email alerts. All of this works with S3 Notifications and DynamoDB Streams behind the scenes, but you don’t need to concern yourself with that or with managing clusters of workers. Another example would be managing infrastructure. Suppose you need to manage membership in a load balancer. Today you need to wire up your own discovery service and notification mechanism for when instances come up or down. This might mean polling EC2 APIs. However, EC2 will have notifications at some point, too. At that point, this scenario becomes trivial with Lambda. We’re at the beginning of something very interesting happening in AWS. If you want to learn more, there’s a Getting Started with AWS Lambda presentation , as well as a presentation titled Event-Driven Computing on Change Logs that describes Amazon’s vision. Localytics is exploring advanced use cases. We’ll provide follow-up blog posts as we make progress. Want to help? We're hiring .", "date": "2014-11-25"},
{"website": "Localytics", "title": "Better Asynchronous JavaScript", "author": ["Francois Ward"], "link": "https://eng.localytics.com/better-asynchronous-javascript/", "abstract": "JavaScript is single threaded. To avoid blocking applications when performing an ajax request or reading a file, these operations are usually executed asynchronously. Some mechanism is then required to keep track of what is happening. Most asynchronous JavaScript APIs like XMLHttpRequest expose a way to assign a function, or callback , that will be invoked when the task is completed. When multiple requests need to be executed sequentially or in parallel, our code can quickly become hard to maintain and follow. Fortunately, there are several ways to make things more manageable. Let's take a look at the various patterns available to developers today and what we can do to make async code easier to deal with. The most basic pattern for handling asynchronous operations is the callback and it is at the heart of all the methods this article will cover. This pattern far predates JavaScript and is commonly known as continuation passing style , or CPS. An asynchronous function will take one additional argument, the continuation, and will be responsible for invoking it when the work is done. The caller will continue its own work in that callback. To illustrate how we use callbacks, let's examine a restaurant where the staff takes orders: The takeOrder function happens to be asynchronous, however, we can't cook the food until the order is finished. We need to notify the kitchen when they can start cooking, which will require a callback: This callback uses the standard popularized by Node.js. It takes 2 arguments. The first is an error, which is either null when execution is successful or contains an error object on failure. The second is the function's result. Callbacks don't require this to function, but a standard is necessary to keep APIs consistent. Read The Node Way . Using this standard will also help us convert our code with more advanced techniques later in this article. The cookFood function is also asynchronous, but customers can't start eating their food until it's finished cooking. We'll need another callback for the kitchen to notify when cookFood is done. Since the order contained an array of food items, we'll return the meals as an array too: Here's where things become more complicated. The eat function is also asynchronous. Customers all start eating at the same time, there is no way to know when they will finish. We want to bring the check, but we have to wait until all three customers are done. We need some to keep track of this. To solve this simply we'll use a variable to count down as customer's finish eating ( note that we have to handle the results individually each time the callback is invoked ). To further complicate things, our restaurant's kitchen isn't very reliable. While any part of our code could result in an error, we want to specifically report problems in getting food to the customers. We'll check in cookFood 's callback if err contains a value, and if it does, we'll stop there: There are a few problems with the callback method which leads us to look for better alternatives: The caller is responsible for passing in a callback. If it doesn't, the operation will end without giving a chance to the caller to handle the result or any execution error. There is no official standard for designing an API that accepts callbacks. Poor chaining options. Every time we add a new operation to our restaurant, we have to add a new callback. Doing so will either increase the level of nesting in our code or make it jump between functions that could be defined anywhere. This issue has led the community to dub the situation where an application has too many callbacks as the Callback Hell . Even with all these problems, it is important to understand how callback works as we will keep building on top of them to make our lives easier. There are libraries to help ease the pain of working with callbacks. A notable example is the fantastic async.js , which would help eliminate nesting and can handle the parallel scenario. It also has features to handle more advanced use cases such as throttling (what if only two customers could eat at a time?) If you target only modern browsers and newer versions of node/io.js, or are willing to use third party libraries, a better option becomes available: promises . Promises attempt to be the solution to callbacks shortcoming. They serve as a wrapper around asynchronous operations make them chainable, composable and allow consistent error handling. You may have heard of promises referred to as tasks or futures , though depending on the language these may have different semantics than JavaScript promises. Promises are also related to the Continuation monad . See this discussion for some background related to monads and promises The core idea behind this pattern is that a function responsible for asynchronous work returns a promise . The promise object has a standard interface, the most important part being then .then method. .then takes a callback as its first argument. If the asynchronous work has already been completed, then callback will be invoked right away, even if it is assigned long after. If the work has not been completed, the callback will be invoked when it will be. Objects with a .then method are known as .thenable . Not all .thenable are promises but are often still compatible. See the Promises/A+ for more details. There are many benefits that come with promises: The promise API can be seen as a standard, generic and more scalable way to work with callbacks. The callbacks are still there, but promises force us to keep them clean. Promises are part of the EcmaScript 6 . They're available out of the box on newer browsers and version of nodes. Libraries such as Q , BlueBird and Angular allow us to use promises in older environment, or with additional features. Let's revisit our restaurant , now with promises. First, we take the order. Notice that this time, takeOrder has a return value (the promise), so we'll return it to the caller: To pass the order to the kitchen, we chain .then with a callback that takes the order as argument. We return the result of cookFood (also a promise) to allow further chaining. If we stopped here, the application calling restaurant() would also be to keep the chain going .then , giving it access to the result of cookFood . For example, it would be possible to do We'd rather have our customers enjoy their meal though, so we'll add them to the chain instead. If eat also returns a promise, we run into a problem: to keep the chain going, we need to return the result of eat , but we can only return one value. The promise api has an .all method which allows us to combine multiple promises into one by passing it an array. The next step of the chain will receive a result array, allowing us to handle all the values at the same time. To take care of our kitchen's high failure rate, we can pass a second argument to .then , which will be only invoked if there is an error. Returning a rejected promise via Promise.reject will break the chain. One thing to note is that unhandled errors (ie: if takeOrder was to throw an exception) will be lost. Promise libraries each have their ways to handle unhandled exceptions, such as logging them to the console or allowing developers to specific a catch-all function. Let's reiterate how our restaurant was improved: Libraries like Q and Bluebird let us convert node-style asynchronous functions into functions that return promises. Given a function which takes a callback in the form function(err, result) {...} as its last arguments, these utilities will create a new function that returns a promise. This is one of the big benefits of sticking to the node standard when using the callback style. Promises make things a lot easier, but the code is still significantly harder to read than synchronous code, and error handling still involves callbacks and workarounds as opposed to leveraging native language constructs like try/catch To do better, we need enhancements in the language itself. We'll need generators. A generator function is a special kind of function that enables control flow with yield expressions. Whenever the function is invoked, it starts in a paused state until told to execute until it encounters a yield , at which point its execution will stop, and control will return to the caller. A generator function returns an iterator object that can be used to to communicate with it. Generators requires EcmaScript 6 (sometimes called EcmaScript 2015) support and are only recently supported environments such as io.js To use ES6 across a variety of browsers or in node.js without feature flags you can use Babeljs , a fantastic ES6-to- ES5 compiler which includes Regenerator . Bonus points: it supports JSX/React too! Another alternative is Traceur from Google. Take the following generator function. The star * is used to define a function as a generator. You can only use the yield keyword in such a \"star\" function. Let's break down what happens when using the function foo : The generator function could also call yield in a loop, while reading some database output or something similar as convenient sugar to create an interator that over the data. It can of course contain regular code (such as the console.log statements in the example above) that will be executed normally until the next yield or the end of the function. The concept of a function that stops its execution and yields to its caller, until it is told to resume is also referred to as a coroutine . The ability to stop execution and come back as necessary is exactly the language construct we need to abstract away our callbacks altogether when combined with promises: yield a promise whenever we want our code to stop as it would if we were writing synchronous code, and in the promises' .then callback, call .next on the iterator to resume. The code above isn't very generic: we need to keep track of iterators and add a . then to the promise to use resume execution. This boilerplate adds up. A generic wrapper can be made with the following properties: Q and Bluebird have already done this for us with Q.async and Promise.coroutine , respectively. Generator functions also helps handling errors: We can use fooIterator.throw('boom!') to propagate the error up to the caller like a synchronous exception. This saves us from needing error callback and gives us the ability to use JavaScript's try/catch blocks to handle exceptions. Let's see how we could leverage Bluebird's .coroutine method to simplify our restaurant. For ease of use, we'll start by assigning the coroutine function to an alias, async . We'll then make our kitchen function a generator with the * and start taking orders. To retrieve the value of the asynchronous task, all we need to do is yield the promise. The async helper will take care of correctly resuming executing and returning the result of the task. Chaining promises (remember: under the hood, these are still the same methods as in the promise example) looks like synchronous code, except for needing to yield whenever we need to use the result of asynchronous tasks. Again, it's all just promises. To wait on multiple parallel tasks, use Promise.all . The Bluebird wrapper used in this example only handles promises by default, so even primitive values such as the total need to be wrapped in a promise. We can use Promise.resolve to do so. Bluebird's .coroutine only works with promises by defaults, so we have to remember to wrap primitive values in promises. If you want things to be even more seamless you can use yield handlers , to automate the process. The code is still quite readable without them, though. Error handling is a lot more intuitive: the async wrapper will bubble up exceptions using the iterator's .throw method, giving us the ability to handle them synchronously with try/catch . We gained a lot by adding generators to the mix: By introducing generators, we cut down our code by roughly 30% and made it far more readable by eliminating all the callbacks and replacing them with native JavaScript constructs. The only thing left is getting rid of the wrapper making the role of the yield keyword more intuitive Using generators to yield promises has so many benefits, many people think it should be part of the language. Similar concepts have already been introduced in other languages such as C# For JavaScript, there is the async functions proposal for ES7 (yes, ES7, not ES6, even though the later is still only a stage 1 proposal as of this writing). Async functions take the patterns we used with generators/promises and make them part of the language as keywords. If you use Babeljs and enable experimental features with the --stage 1 switch, you can try them today. Let's rewrite our restaurant one more time with async functions: This isn't much different from using yield or generator methods. This syntax is just an early proposal and is subject to change. The proposal is fairly popular, and if you sit in chatrooms of bleeding edge projects like Babel or Aurelia , you'll hear from people who swear by it. We still don't know what form it will take when it gets officially accepted into the language or even if it will be at all. There's also the issue of tooling. JShint , one of the more popular JavaScript linter, handles a large portion of ES6, but doesn't support async/await . ESlint has it on their list , but it's low priority. Other tools and IDEs that support ES6 may also fail to parse it. There are various forks, plugins, and hacks to get it to work. It's up to you to decide if it's worth the trouble to have a bit of extra syntax sugar over the Q/Bluebird wrappers. A note on CoffeeScript As of 1.9, CoffeeScript supports the yield keyword and generator functions. In CoffeeScript, the * isn't necessary and any functions containing a yield is automatically compiled to an ES6 generator function. Unlike Babel, however, CoffeeScript doesn't have an option to convert generator functions to ES5 with Regenerator , so you'll have to do it yourself, by running it after the CoffeeScript compile step, unless you're targeting a platform that supports generators natively such as io.js . With generator functions you would then be free to use Q.async and Bluebird's .coroutine to simplify your asynchronous code. If you want something closer to the real async/await syntax you can look at more exotic flavors of the language, such as Iced CoffeeScript , or push for support in CoffeeScript itself Everything shown here is available today. Which one you can use depends on your target environment and what compromises you are willing to make. With all these options available now and in the future, there's no reason to fall in callback hell. Keep it clean, and enjoy all the benefits of asynchronous JavaScript!", "date": "2015-04-30"},
{"website": "Localytics", "title": "Your Code Coverage is Bad and You Should Feel Bad!", "author": ["Yasyf Mohamedali"], "link": "https://eng.localytics.com/your-code-coverage-is-bad-and-you-should-feel-bad/", "abstract": "Localytics is open-sourcing Shamer , a service which provides gamified code coverage and automatic Github integration while allowing you to question your sense of self-worth as a developer. Check out the project on Github , and read on for the story of its inception: how Shamer transitioned from a simple what if to a valued part of our engineering culture. It all started fairly innocuously. “What if we could click to view the coverage reports right from the pull request?” The conversation of the moment was about the code coverage reports that get generated automatically every time our test suite passes on Codeship, one of the solutions the frontend team uses at Localytics for continuous integration. These reports let us know exactly what parts of our codebase are covered by tests, and how much of it remains untested. As our venerable Engineering Director Adam Buggia is often heard saying, “we at Localytics have a culture of testing”; the ability to get instant feedback on the status of our tests is crucial to our development pipeline. In order to make it as easy as possible for developers to review new code, we wanted to inline a summary of these coverage reports, as well as provide a link to a detailed break-down. I decided to take inspiration from Stager , a Docker-based internal service which comments on new pull requests with a link to an automatically generated staging environment. My new microservice would generate coverage reports, host them in a central location, and comment with a link to the hosted reports. \"Yasyf built a simple service to use Github auth to get access to temporary S3 tokens.\" It soon became apparent that a good solution was to upload the HTML files generated by SimpleCov and Istanbul to an Amazon S3 bucket directly from Codeship. A small service with a single endpoint, running on Heroku , would take care of commenting on the PR. All viewing authentication would be handled by Github, based on organization membership and push permission for a specific repo; the people who had access to the code would also be able to view the coverage reports. Along the way, we somehow developed the notion that a service which ties together Github and arbitrary stuff stored on S3 would be a useful thing to make public. A few hours later, github-s3-auth was born. The first iteration of github-s3-auth was fairly simple: Combined with some PyGithub hackery, we had exactly what we set out to build. Developers had easy access to code coverage for their latest branches, omissions in tests were easy to spot, and everyone was happy. \"Well what if we could keep track of changes, so we know who isn’t writing tests?\" Well, almost everyone. Adam takes company culture very seriously, and suggested that it would be a good idea to add coverage deltas in the comment on Github, so everyone knew exactly what the code in front of them was doing to our overall coverage percentage. In hindsight, those who half-jokingly agreed with him may not have truly wanted to turn coverage into a competition, but it was too late. \"This should keep things interesting!\" Our Github/S3 authentication app was about to take on a whole new purpose in life. As Inception taught us, “once an idea has taken hold of the brain it's almost impossible to eradicate.” A few lines of Python, a quick MongoDB instance, and presto! The Localytics Code Coverage Leaderboard had come to life. PRs now had detailed statistics (otherwise known as simple deltas) about their coverage, and a note, forever ingrained in the sands of Github Comments, on the author’s leaderboard rank. Everyone was ranked based on their cummulative coverage contribution to date, sortable by coverage for a certain language. There was no way to validate the totals; all one could do is scrutinize the ranking table and wonder why they were in the red, or what exactly caused their coverage to drop by an astounding 1.2%. It was around this time that the questions began flooding in. “So jokes aside, how is this calculated?” “How do I get my code from like 4 months ago counted in here? I must have an easy 20% ruby increase on that one.” “If multiple people contribute to a branch and write a whole bunch of tests we all win, BUT who gets the code coverage points?” “Can we see the breakdown of which PRs are impacting our coverage?” “I think our coverage bot needs some coverage.” These were all very good points, and it became apparent that when you make something a competition, people get very serious very quick. This was great for Adam (and company culture), but was less great for the person who had hacked everything together in a night, especially when things started to go wrong. I’ll admit, the ranking algorithm was not the most sophisticated thing in the world, nor was the leaderboard very secure. In fact, it was something along the lines of the following. Which, believe it or not, was rather prone to bugs. Many coverage reports left people confused and bewildered at their results. There were times when changing a line of HTML resulted in a drop of over 5%, and conversely instances of adding ten untested methods which boosted coverage significantly. While this was tolerable initially, things quickly got out of hand. “There's a PR gold rush right now. Every PR is adding 16% ruby coverage.” After waking up to the total decimation of any meaning in the leaderboard, we started talking about ways we could improve our gamified coverage setup , which was still just an addon of sorts to github-s3-auth. Around this time, it was suggested that this leaderboard be broken off into a separate project, as it no longer really had anything to do with Github, S3, or authentication. Many feature suggestions were also made, including a drill-down view, branch-tracking (so as to be able to compare your code to the coverage of master wherever you branched off, instead of what it currently happened to be), and memes to shame those who dropped coverage too much. This last idea was particularly interesting, as it took what we were doing from a competition to outright (semi-)public shaming.  Apparently, we at Localytics also have a culture of shaming, and out of the ashes of github-s3-auth, Shamer was born. Today, Shamer is much more accurate and feature-filled. It’s used by the entire frontend team to track coverage rankings, view reports, and of course, shame people (like me) who don’t write enough tests. We feel that a tool like Shamer could have uses outside of Localytics, so we’ve open-sourced it. Feel free to check it out on Github . Pull requests and other contributions are welcomed, and don’t worry - there isn’t any shaming set up for Shamer (yet).", "date": "2014-08-20"},
{"website": "Localytics", "title": "Using Angular.js at Localytics", "author": ["Joel Rosen"], "link": "https://eng.localytics.com/angularjs-at-localytics/", "abstract": "This is the story of how we rewrote our Backbone-powered web application in AngularJS, using nearly half the number of lines of code. (It is a love story.) In the beginning, there was spaghetti code. Then out of the chaos came Backbone, and it was good. Our data lived in Models which lived in Collections which were observed by Views, and our application grew. But with growth came complexity, and as we began to nest our Views in deeper layers, evil Zombie Views plagued us, and we despaired. Backbone promised a simplicity that eluded us in our real-world application: \"In a finished Backbone app, you don't have to write the glue code that looks into the DOM to find an element with a specific id, and update the HTML manually — when the model changes, the views simply update themselves.\" – Backbone.js docs We found this statement misleading. In Backbone, View.render is a no-op, so the above would perhaps be better written: \"...when the model changes, you are responsible for binding your view to the model’s change event and writing a render method that can be called each time this event fires, and make sure to unbind it when you’re done.\" For us, this was easier said than done. First , to keep our views manageable, we followed thoughtbot's example and wrote composite views made up of smaller views responsible for discrete parts of the page. Parent views had to keep track of child views and make sure to clean up after them. The render method's purpose became less clear with nested views. Should it render itself, then recreate and rerender all child views? Should it reuse existing child views and just tell them all to rerender? We ended up investing many lines of code and development time on composite view library classes to help manage these cases, not to mention the time spent hunting down zombie views that weren’t being cleaned up correctly. Second , our app was heavy on user input. We never found a good way to keep our form inputs, models, and views in sync. We began by writing glue code that looked into the DOM to find an element with a specific id and grab the data from that element on keypress. That caused the model to change, so sure enough, our observant view rerendered, causing the input to lose focus. Instead of simply rerendering the entire template, we wrote more glue code that looked into the DOM to find an element with a specific id and change the value of that element to reflect our model. Now we had double the amount of awful DOM manipulation code in our views, so we turned to a little two-way data binding library called Stickit . The plot thickens. The tale of our journey down the path of Stickit is an epic in itself. In a nutshell, things got worse. Stickit's philosophy is to “clean up your templates” (make them less declarative) by moving your data-binding logic to your view. So now we had a whole library whose purpose in life was to litter our views with the names of more DOM elements. Our zombie views grew fatter. We were ready for a framework that offered more out of the box, and any moral objection we might have had to soiling our templates with non-standard HTML had been completely beaten out of us at this point… The straw that broke the camel's back was Chosen , the jQuery plugin that turns ordinary select elements into pretty searchable dropdowns (an essential part of our UI). It doesn’t work with Stickit and it doesn’t work with Backbone views that haven’t yet been inserted into the DOM. But we persisted, writing tomes of code to coerce these three strangers into playing together. When it came time for a UI revamp of our app marketing platform , we realized we were going to have to rewrite enough of our existing code that we jumped at the chance to try something new. We were ready for a framework that offered more out of the box, and any moral objection we might have had to soiling our templates with non-standard HTML had been completely beaten out of us at this point, so AngularJS's declarative two-way data-binding was starting to sound real good. When we found AngularUI , complete with datepicker and a Chosen lookalike ready to go, we were sold. It was, to be sure, mind bending at first. Without our familiar friends, Model and Collection, we weren’t even sure how we were supposed to model our data, a question on which AngularJS is conspicuously unopinionated. We also weren’t sure how to organize our code, and the Angular documentation wasn’t very helpful here, as it's riddled with sample code which it then advises you not to follow in a real application. But a few days in, it was evident that we were writing so little code in Angular anyway that it didn’t really matter if our organization wasn’t perfect. We finished the project in two weeks and in about half the number of lines of code, including HTML markup, as our old Backbone version: Data-binding worked like magic. We wrote zero lines of DOM manipulation code, and spent zero minutes tracking down memory leaks or unpredictable event binding behavior. As an unexpected bonus, Angular's module API and dependency injection system also turned out to be way cooler than we imagined. In Backbone we had struggled with a hand-rolled module loader to organize our code, and it took no small amount of discipline to keep passing all the variables needed into our views (which were nested several layers deep) instead of throwing our hands in the air and just using the module loader as a place to stick global variables. Now Angular takes care of all of this for us, managing our services and providing instant access to them wherever we require. As it turned out, the select2 dropdown widget that comes with AngularUI didn’t quite meet our needs. The author acknowledges that the plugin is slow with large datasets , and we wanted something that would work with ngOptions. So we wrote our own directive for Chosen, open-sourced on Github . The directive works on any <select> element, and plays well with ngModel and ngOptions : To close, here are a few things we learned from the transition, and a few questions we’re still working on answering: Directives are hard . Working with isolate scope and transclusion is tough, and Angular's documentation on the subject doesn’t make it easier. We found that before jumping into writing an ambitious directive, it's best to start by not writing a directive — just using normal templates and controllers — and then roll that code into a directive once you really figure out what your requirements are, or once you start repeating yourself. It's also best to write smaller, less obtrusive directives that play well with others. For example, we went through a few iterations on the Chosen directive before realizing that it was best to keep it simple and define it as an attribute on a standard <select> element, instead of creating a element that would render its own template. By keeping it lightweight and retaining access to the <select> element outside of the directive, we can still add other directives on top of it, like ngModel , ngOptions , ngChange , or any of the directives that work with validation. Angular documentation needs some love . But we were able to figure it out anyway, and it turns out there isn’t as much to learn as we’d initially feared to get started. The developer's guides to directives and forms are now our two best friends. Form validation is great but requires some thought . Our application helps customers create targeted in-app messaging campaigns through what's essentially multi-page form wizard, so dealing with form and model validation was tricky. Angular's built-in validation directives like ngRequired helped us eliminate a huge chunk of tedious validation code that had previously lived on our Backbone models, but we still had some model-related business logic that couldn’t be defined declaratively, and which needed to be persisted if the user left our app and came back later without stepping through each page of the form again. Integrating model and form validation was tricky, and our implementation probably needs some rethinking. We were also surprised to find that Angular doesn’t seem to support required validation on groups of radio buttons, and had to resort to adding a hidden input element with a required attribute to validate that a user had selected an option. Declarative is good . It's funny, but we’ve seen interactive web apps with HTML and Javascript come full circle. We used to write things like <a href=\"#\" onclick=\"doSomething()\"> . Then we realized it was bad to couple presentation and behavior, so we made our Javascript unobtrusive, keeping our templates clean. But now we’re back at it again, writing <a href=\"\" ng-click=\"doSomething()\"> . Have we learned nothing? My experience with Backbone has led me to conclude that the ethos of \"keeping one's templates clean\" is an antipattern that creates brittle connections between Javascript code and DOM structure. Web development is messy business, and the ugly code that brings HTML to life has to live somewhere. By abstracting it out of our templates, it just clogs up our views with hardcoded references to HTML elements whose names, classes, and structure might change—and which should be allowed to change without breaking behavior and needing a corresponding change in a separate view class. Philosophy aside, Angular helped us get our job done with a lot less code, leaving us more time to do the things we love, like drink beer frolic in the sun write more code. Plus the t-shirts are pretty cool.", "date": "2013-04-09"},
{"website": "Localytics", "title": "Intro to Node on AWS Lambda for S3 and Kinesis", "author": ["Nick Sergeant"], "link": "https://eng.localytics.com/taming-aws-lambda-for-s3-and-kinesis-at-localytics/", "abstract": "AWS Lambda is an on-demand computation service that allows you to write code that responds to events, and can be executed at scale within the AWS ecosystem. It has some unique benefits that make working with it particularly desirable. It's cost-effective, scalable, and presents an alternative to spinning up heavy servers to do straightforward event-based work. At Localytics , we process billions of data points in real-time.  At the end of our processing pipeline we output our data to Kinesis streams and S3 buckets.  This allows teams to process either live data via the stream or historical data via S3. The format of the data is identical. Lambda was an ideal fit for handling both data sources, as we could write the event handling logic as a single Lambda, and make our data-processing code source-agnostic. Lambda responds to events from a variety of sources. For our purposes we were focused on handling Kinesis stream events and S3 PUT events. See here if you'd like to learn more about the types of events that Lambda supports. We were tasked with creating a new service that could process historical and live data. As we've made the format identical between S3 and Kinesis data sources, we were able to write a single lambda to handle both event sources. This reduced the surface area of our code that needed to be maintained and clarified the deploy process. Our Lambda will receive an event when invoked from an S3 PUT notification. It looks like this: It's important to note that we're only given metadata about the object (not the data itself). It's on us to get that object from S3. Also, we store our data gzipped, so we need to ungzip the data before we can do something with it. Here's the functional code that handles this in our lambda (we'll show a complete example later on): Our Kinesis stream is always on and channeling data, so our lambda simply listens to the stream and acts upon it. When Lambda responds to a Kinesis stream event, our event source looks like this: Records[0].kinesis.data is what we want. The beauty of this event source is that it contains base64 encoded data. Very simple to decode and use in our lambda: Let's walk through creating and deploying a single lambda that can handle both S3 PUT notifications as well as Kinesis stream events. The full codebase for this example can be found on GitHub . First off, there are two specific permissions that you'll need: Your lambda execution role will also need permissions for whatever services you want to use within your function. If you intend to be working with S3, for example, you need to specifically grant your execution role permissions for whatever you intend to do with S3. Let's create a file named MyLambda.js , and require some things: We'll be using async as mentioned previously to pull objects from S3 and unzip them with zlib.gunzip . aws-sdk is required for working with S3. Let's initialize the SDK: Since our code is running as a role within the Lambda system, we don't need to provide credentials. The SDK will happily make any requests you ask of it, and the role's permissions will dictate what we can and cannot do. Let's write some code that will handle Kinesis events: When we get a Kinesis stream event, we could have any number of records to process. Our code expects that, maps the base64-encoded value and joins them to provide a single base64-decoded string that we can work with. Then we call doWork(data) . In the real world you might be doing asynchronous work on the data (and you may be interested in reading Better Asynchronous JavaScript ). context.done() is how we let Lambda know that we're finished doing work. That's all we need to do to handle Kinesis event streams, so let's move on to S3 PUT events. This should look familiar from earlier in this post. When we get a S3 PUT event, we know that we'll only ever have a single record to work with. So we pass that record to our s3Handler , download the object, unzip the object, and finally doSomething with the data. Now that we have our two specific handlers for each event type we intend to support, we need to handle the direct event source from Lambda: Our actual handler is very simple. If the event looks like an S3 event, let the s3Handler do the work. Otherwise, if it looks like a Kinesis event, let kinesisHandler do the work. This is all of the code that's necessary to write your first lambda that supports both S3 and Kinesis. Now that we have our code that we want to deploy to Lambda, it's time to actually upload it. A few basic first steps: Once those are set, we need to package our module up: Now we can upload the module: If your upload was successful, you should receive a response like this: You can see your uploaded lambda on your dashboard . From there you can also edit/invoke with sample data. Now that your lambda has been created and uploaded, you can add event sources to it via the dashboard . As mentioned, both S3 PUT events and Kinesis streams will work properly with this lambda we've created. To make working with Lambda a bit easier, we wrote a starter Lambda module . We defined a handful of Make targets which can make managing a single lambda a bit easier: We hope you find it useful! Be sure to drop an issue on GitHub for any questions / bugs. Lambda presents a new way of programming application logic around events instead of infrastructure. We think this has the potential to bring entirely new types of applications and workflows to market, and it fills a gap in AWS's cloud computing lineup that makes it easier and faster to do real-time work on data within the ecosystem. Even aside from the affordability and durability of Lambda, being able to direct chunks of logic to process individual events from systems represents an opportunity for data-heavy organizations to drastically streamline their technical infrastructure.", "date": "2015-05-11"},
{"website": "Localytics", "title": "Presto for the Enterprise - Video from the Boston Hadoop User Group", "author": ["Andrew Rollins"], "link": "https://eng.localytics.com/presto-for-the-enterprise/", "abstract": "Last month, Localytics hosted the Boston Hadoop User Group for an evening presentation titled Presto for the Enterprise . Presto is an open source, distributed SQL query engine for running queries on top of multiple data sources. Matt Fuller and Kamil Bajda-Pawlikowski from Teradata gave the audience a tour of Presto and Teradata's contributions to its development. We captured a video of the presentation and Matt posted the slide deck ; you'll also find them embedded at the bottom of this post. This presentation is great for anyone interested in Presto that has yet to try it. The talk covered the following: Thanks to Matt, Kamil, and Teradata for presenting and sponsoring pizza! For additional perspective from Teradata, check out their blog post titled Bringing Open-Source Presto to the Enterprise . From the perspective of Localytics, we're interested in technologies that have the potential to unify different data sources, providing the necessary query planning and execution environment to do higher order computations. Presto is one such technology, and so is Spark (which we wrote about in Spark, Whistling Past the Data Platform Graveyard ). If technologies like Presto excite you, you should consider joining Localytics on its mission to build the best app marketing and analytics. Embedded video: Slides: Thanks again to everyone that made this presentation happen.", "date": "2015-08-13"},
{"website": "Localytics", "title": "Tips & Tricks for debugging unfamiliar AngularJS code", "author": ["Francois Ward"], "link": "https://eng.localytics.com/tips-and-tricks-for-debugging-unfamiliar-angularjs-code/", "abstract": "You've been given an Angular code base. How do you figure out where everything is? Where scope variables are coming from? What controller does what? Angular doesn't make it easy, but there are few tips and tricks that can make your life easier. Tools like Angular Batarang can help, but they're not always installed when you need them. Take some time to understand the underlying concepts, and your life will be easier in the long run. Angular Batarang is not a great way to do this kind of work. I would not recommend trying it out until you've attempted these techniques... Here are a few tips that will save you a couple of hours next time you are thrown in the deep end of an Angular code base. Most of these tips require that debugging info be available. If they don't work for you, open up the console and type angular.reloadWithDebugInfo(); Almost everything in Angular revolves around the scope . Angular creates a scope hierarchy with controllers and directives . Directives can sometimes have their own isolate scope. Being able to know the value of the scope at a given place in our page is valuable to debugging, and easy to do. Angular lets you access the scope available to an element via the .scope method. An isolate scope is one that is unique to the directive and is not affected by anything outside of it (aside from bindings) It can be a bit confusing, but try it out! It can be a lot faster than setting breakpoints and inspecting variables. If you're not sure which of the above commands to run, just try them all. They're perfectly safe, and you'll eventually find the data you're looking for. For more information on the angular scope, refer to the angular documentation Once your Angular application grows in size it can become challenging to figure out where variables used in our templates come from. For a single directive with isolate scope, it's easy. In a hierarchy of controllers and directives, try some of these methods for tracking them down: There is one small details that make things more complicated however: scope inheritance. A controller up in the hierarchy could be providing values available on the child scopes. There's a few ways you can go about figuring it out: You may have noticed elements with a class \"ng-scope\" when looking through the inspector. They represent the elements where new scopes got created Using the above techniques, you'll be able to find at what point in the document the property is being set. It's definitely a lot of work, but is usually faster than trying to guess, unless you wrote the code yourself! Inheritance in JavaScript is different from inheritance in languages like Java or C#. JavaScript uses prototypal inheritance. Accessing a value that doesn't exist on a given object gets delegated to one higher up in the chain, if available. Prototypal inheritance is amazingly useful when debugging Angular applications (see link below). The techniques above use it to analyze scopes to find where values come from. If you've ever had issues getting your two-way data bindings to behave the way you expect, the culprit may be prototypal inheritance. There are many tutorials on how inheritance work in JavaScript on the net. You may want to start here . A good understanding of these concepts will make your Angular experience a lot smoother. It's a good foundation for any JavaScript developer. This may seem obvious, but we often forget about it. If you need to figure out which controller is responsible for a section of the page, just look for the ng-controller attribute in the DOM. If you don't see one, go up one level and look again. If you're like me, you may have trouble finding an attribute in a ton of HTML. To find find it, click on an element, run $($0).attr('ng-controller') , and if nothing comes up, click on its parent (using the bread crumbs).epeat until you get a controller name back. With a big of jQuery magic, we can also find the controller that is nearest to our selection in one step: $($0).closest('[ng-controller]').attr('ng-controller') If you use ng-if , ng-switch , or ng-views , you may have wondered why your elements didn't show up when searching for them in the inspector. Unlike ng-show which shows and hide elements by changing their visibility, these directives add and remove their content from the DOM completely based on conditions or routes. Angular leaves a comment, such as <!-- ngIf: isFoo --> where the element would have been if isFoo had been true. If the condition is more complicated, you can use the techniques we described earlier to understand why an element isn't being shown. In For this example, you would run $($0).scope().isFoo , and you would get a falsy value. That's why the element didn't get added to the DOM, and was replaced by a comment instead. The comment will be in the DOM when the element has been added, not just when it is absent. Look right below it for an element with the same condition. If there isn't one, then you can assume the condition was not met. What if we want to force the element from the previous section to show without going through the scenario that would set isFoo to true ? We can do it from the console! The first step, selecting the element, is very important. If you don't select the right element, you may or may not get the results you expect because of how prototypal inheritance delegation work. Your value may not always get propagated up or down the chain. In the previous section, we used the $digest method to trigger changes. You may wonder why we had to do that. Why didn't the DOM change the moment we set isFoo to true? Objects can't notify Angular that their properties have changed without proxies or custom setters. One Angular's goals is to let us use plain old JavaScript objects. Angular solves this using the digest cycle to inspect expressions and watchers for changes. Digest is run by Angular when an event it is aware of happens. This could be a an $http request or an ng-click event. Proxies are a new feature of ES2015 http://babeljs.io/docs/learn-es2015/#proxies When something happens outside of Angular's known world about (maybe a jquery plugin, or a console command), it doesn't automatically run $digest and nothing happens. In this case we'll need to execute $digest ourselves. As soon as the digest executes, things will get updated. ES2015 is the next version of JavaScript, also known as ES6. It contains features that can be used by Angular in the future to improve performance by simplifying how object changes are detected, such as proxies. While it is possible to use most of ES2015's features using Babel , a javascript compiler, fast proxies must be implemented natively , so the current version of Angular uses dirty checking instead. Angular is built on top of the concept of dependency injection . It can feel a little bit magical. While a full explanation of Angular's DI is out of the scope of this blog post, here's a few things to keep in mind as you wrap your head around it. Note that the following is a big oversimplification! I strongly recommend learning the different types of angular services. They are a significant source of confusion when trying to wrap your head around an AngularJS code base. You don't have to debug through your JavaScript code by adding console.log or debugger statements everywhere, though it can be useful. Consider familiarizing yourself with breakpoints , available in the inspector of all major browsers. Chrome even supports conditional breakpoints, where the breakpoint will only stop execution if a certain JavaScript expression returns true. This is useful when debugging repetitive code where only a specific iteration is interesting (such as nested loops). You don't have to go through the entire loop to get where you want. At the end of the day, the most important thing to understand about AngularJS, is that It's just JavaScript . There's no magic, and it is bound by the same rules as any other JavaScript applications. Dependency injection is just a big list of key value pairs from which objects are pulled from and passed to your components. The way scopes work is based on prototypal inheritance. The $digest loop is just a task that iterates over expressions and watchers when Angular-related events occur. You can force it to happen. By understanding the good old JavaScript under Angular's hood, you can reduce the time it takes to debug your code base. It is worth the time to take a look at how Angular implements its own methods, including the directives for basic form elements from the Angular repository. It takes a bit of time, but gives a lot of background on how things end up working. CC background photo by flickr user Jon Candy , \"Longleat Maze\"", "date": "2015-08-10"},
{"website": "Localytics", "title": "Performance in Big Data Land: Every CPU cycle matters", "author": ["Konstantine Krutiy"], "link": "https://eng.localytics.com/performance-in-big-data-land-every-cpu-cycle-matters-part-1/", "abstract": "At Localytics we need to perform data aggregations at mind-blowing speeds. To do this, we run HP Vertica as a core component of our data platform. Vertica is a cluster-based, column-oriented analytics platform that provides an ANSI SQL interface with many analytics capabilities. My name is Konstantine Krutiy, and I lead the Vertica team at Localytics. The clusters I am responsible for contain all of our analytics data (which amount to more than 1 trillion data points). In August I presented at the HP Big Data Conference , and my session covered real-world methods for boosting query performance in Vertica. The talk was called \" Extra performance out of thin air \". At the core of this talk is the concept that every CPU cycle matters . This post is the first in a series exploring this idea as it pertains to data platform performance. Let’s start with a simple math exercise. In our equation we have CPU power on one end and a big dataset on the other. In terms of CPU power, the fastest enterprise-grade hardware runs at 3.7 GHz . Currently there is no numerical definition for “Big Data”, but my peers and I concede that you have “Big Data” once you cross 100 billion records . Now we’re ready to calculate how much time we can save if we eliminate 1 CPU cycle from each iteration. In these calculations I assume that we run on one core and eliminate one CPU cycle from the operation, which will touch each record in the dataset. We calculate one CPU cycle operation at 1 / 3,700,000,000 of a sec and multiply it by 100 Billion records. 1 / 3,700,000,000 sec X 100,000,000,000 = 27 sec That is an oustanding result! You can clearly see that one CPU cycle can represent tens of seconds of processing time. Think about the potential here! One tiny tweak, one CPU cycle saved, and a great reduction of processing times. Let’s see how this tweak effects the real world. Different data types will force Vertica to use a different number of CPU cycles to process a data point. To illustrate the concept above, we’ll choose a data type for numeric values (as numeric values generally have a very high cardinality so the database engine will need to touch many different values to produce the final result). In this exercise we have the task of creating a column to store dollar amounts from $0 to $99,999,999,999,999.9999. We will start by looking at the different ways that $395.17 can be stored in the database. Here we will have at least 3 different options: We created a billion-record data set in 3 different tables using these different types. The column data type was the only difference. We ran the same queries on each of those tables 5 times in a row. Here are our raw findings: In simpler terms (averaged, time in seconds): Yes !!! We just saved 10 sec of processing time by choosing the best possible data type. I am sure the front-end can easily add a dollar sign and divide by 100 when Integer used as data type in the database. Doing this should be faster than the 10 sec we just saved at the database level. Follow these rules of thumb when making data type choices in Vertica: Now I hope you’re on board with this idea that every CPU cycle matters. To build on this idea of savings out of nowhere (and give you a sense of what future blog posts will hold), here’s an example that saves: If you are running Vertica version 6 and executing your SQL statements via JDBC driver with the default settings then you acquire a lock for every SQL statement. If all you do is SELECT then you do not need those extra locks. When you avoid locks you will see the benefits mentioned above. In Vertica, locking behavior is controlled by the AUTOCOMMIT mode on session level. If AUTOCOMMIT = ON (jdbc driver default) , each statement is treated as a complete transaction. When a statement completes changes are automatically committed to the database. When AUTOCOMMIT = OFF , the transaction continues until manually run COMMIT or ROLLBACK . The locks are kept on objects for transaction duration. This Java snippet shows how to switch AUTOCOMMIT to OFF : It did not take a lot of time to implement the needed changes. Here is a screenshot from one of our monitoring solutions, which shows the impact on lock counts by setting AUTOCOMMIT to OFF . This graph shows the number of locks acquired from Vertica. A lower number means we’re not spending CPU cycles acquiring them, and we’re eliminating the time spent waiting for them, handling them, and releasing them. The result of such a simple code change was quite dramatic and caused significant increase in query throughput. Can I call you a TRUE BELIEVER now? Every CPU cycle matters . Now, no blog post would be fun without some code! The full slide deck from presentation on HP Big Data Conference can be found here: http://www.slideshare.net/kkrutiy/extra-performance-out-of-thin-air Hero image above comes from “ CPU Flame Graphs ” by Brendan Gregg, where he discusses this type of visualization for sampled stack traces", "date": "2015-10-28"},
{"website": "Localytics", "title": "Flirting with Elixir", "author": ["Ian M. Asaff"], "link": "https://eng.localytics.com/flirting-with-elixir/", "abstract": "Fellow Localytics engineer, Michael Macasek , and I just got back from ElixirConf and we are pumped. The things people are doing with Elixir are incredible. We heard talks on distributed computing, self-healing architectures, embedded systems, and web development. The last time I saw this much enthusiasm for new tech was back when the Ruby & Rails rocket was preparing to launch. \"I loved what I saw, but hated what I didn't see.\" – Jose Valim We'd both been dabbling in Elixir before the conference, and after meeting the community, we knew Elixir was about to take off. It's a programming language that runs on the Erlang virtual machine . Elixir's creator, Jose Valim, famously said that when he looked at Erlang, \"I loved what I saw, but hated what I didn't see.\" So he built a language that has all Erlang's strengths, plus more—macros, a nicer syntax, and a mature toolchain. Elixir is Erlang with more strengths and fewer weaknesses. Erlang (pictured below) is language that's been around since 1986 and open-sourced in 1998. It's known for powering telecommunications systems that have achieved nine nines of uptime —that’s not a typo. Nope! Erlang was built to create systems that are fast, responsive, and reliable. It’s well-suited for messaging systems, high velocity/volume big-data systems, web applications, etc. For example, Phoenix is a web framework for Elixir that is gaining serious traction. It's being touted as the next-generation replacement for Rails, complete with an elegant ORM called ecto . It supports WebSockets out of the box and was measured to be 15x faster than Rails . In the same way that JavaScript has evolved beyond its web-browser roots, Elixir has built on top of Erlang to evolve in a similar way. Elixir isn’t magic. There are a few reasons it may not be the right tech for your use case: Elixir is a pleasure to use. It's functional, which makes testing, debugging, and concurrency easier . It has expressive, Ruby-like syntax, complete with the magical pipeline operator |> that reinforces the \"pipeline of data\" paradigm. It's also got a polished suite of language tools: Docs are a first-class citizen of the Elixir ecosystem, brought to you by ExDoc . If you've clicked any of the links above, you've noticed the beautiful and thorough documentation. Diving into the Elixir ecosystem is like unboxing something from Apple. We process about three billion data points a day, so our backend systems must be high-throughput, fault-tolerant, and always responsive. That's Erlang's bread-and-butter. The secret sauce to highly-available, distributed systems is the Open Telecom Platform. OTP is a library of abstractions that makes building such systems straightforward. Elixir wraps OTP and interops with the rest of the Erlang ecosystem with zero overhead. Elixir gives us all of Erlang's battle-tested libraries, abstractions for writing concurrent code, and experience running in production. We run several high-traffic Rails apps that require all the bells-and-whistles of any respectable production code. This includes background jobs, caches, multiple application servers, monitoring systems, etc. Elixir allows us to replace each piece of infrastructure with a single technology in a single repo using the Elixir concept of an \"umbrella project\". Since we're dealing with a single technology, it’s easier to debug and less code to manage. OTP makes it trivial to write code that not only alerts us when things go wrong, but restarts things from the last known good state. Elixir is also compatible with Erlang's mind-blowing observer tool, which allows detailed remote debugging wrapped in a GUI. If you're a dev who has ever had to jump through hoops to see what your app is doing, :observer.start will change your world.  There are also considerable performance gains. Phoenix is a lot faster than Rails, which means we can use cheaper hardware to support equivalent traffic. It’s still early in our exploration of Elixir at Localytics, but the language has gone viral among our developers. We’re considering breaking up our monolithic backend into services and using channels in our web dashboard for real-time data feeds. As we ramp up, we're looking forward to watching and contributing to the budding Elixir community. If you're inclined, below are a few Elixir resources that should help you get started:", "date": "2015-10-15"},
{"website": "Localytics", "title": "Faster SASS builds with Webpack", "author": ["Francois Ward"], "link": "https://eng.localytics.com/faster-sass-builds-with-webpack/", "abstract": "If hot reload is part of your CSS development workflow, compilation performance is critical. That 5-second compile time in your large application is now a liability. You're already enjoying a more efficient workflow if you're using hot reload for CSS. You no longer have to wait for full page reloads after every change. Optimizing like this is addicting. The couple of seconds your SASS takes to compile is no longer acceptable. Every millisecond counts! Enter Webpack . Webpack is amazing. It allows you to organize CSS as you would JavaScript, but that power comes at a performance cost. Other build tools such as Gulp can be much faster. Fortunately there are ways to squeeze some speed out of it. If you're not already using hot reload and style injection, you should! The first step is to confirm what is slowing down compilation. CSS is a common culprit, but not the only one. Webpack provides a fantastic profiler. To use it, first run webpack in profiler mode and output the statistics to a file: webpack --progress -c --profile --json > stats.json , then upload it . You'll get to see a cool dependency tree, and various benchmarks on how long each modules took to process. If you see a lot of time spent in your CSS or SASS, keep on reading. Webpack is very good at building JavaScript. It will rarely be the bottleneck, but you should still confirm it with the profiler before going down the rabbit hole. If you're using node-sass, this is the most important step. Since version 3.X, node-sass has been one of the most robust CSS pre-processors available. Unfortunately, libsass 3.2, used in node-sass 3.3 introduced a significant performance regression . sassc is the command-line driver the libsass team uses to test libsass directly (since libsass is just a library). The above benchmarks were used to try and isolate if the performance regression was node-sass specific or if it was in libsass. Not only was it fixed in 3.3 (part of node-sass 3.4), libsass is now faster than it has ever been, and is one of the fastest of the major pre-processors, neck and neck with PostCSS . In our dashboard's codebase, the time spent in node-sass went from 2.7s~ with node-sass 3.3 to as little as 700ms in 3.4, a 74% boost! Webpack's css-loader introduced modules in version 0.15, and with it introduced a massive performance regression . In the bug report, a css-loader user reported that 0.18 took 60 seconds to compile their code, while 0.14.5 took only 16. Further investigation showed that the regression was introduced in 0.15. If you don't use CSS modules, 0.14.5 still works fine with the latest verson of Webpack as of this writing (0.12.2). Webpack's css loader is responsible for handling sourcemaps as well as resolving urls and paths in @import statement. Sourcemaps are a great tool to handle compiled languages, though they have a significant cost during compilation. Running webpack against our SASS (no javascript) with inline sourcemaps took 4.6 seconds. Turning them off reduced compilation time to  2.35 seconds: almost half the time! This is unfortunate: Webpack's sourcemap support is amazing (when using inline sourcemaps, it just works, with no effort on the side of the developer. They just show up in the Chrome inspector). We ran a quick survey in our frontend Slack channel asking if developers felt sourcemaps were worth a 100% compilation time increase. In the end, we decided to side with performance. You don't have to give up all your debugging options in the name of speed, however. node-sass provides another option: source comments . When source comments are enabled, node-sass will include a comment above each selector in the output referring to the original source. For example: This way, developers can look at the generated css, but still find where a style originated from. Source comments are also much faster than source maps, since they're simpler. We are able to compile our SASS in 2.5s with source comments: a great compromise between performance and ease of debugging. You may be able to squeeze even more performance by getting rid of css-loader altogether. The css loader's primary purpose is to use webpack's path resolving and rewriting abilities for @import statements and urls. In production, it's very important (inlining images as base64 urls, cache busting by putting hashes in file paths and rewriting urls to point to your CDN). In development though, if your folder structure works for regular CSS urls, you don't need any of this. That means if you're also using source comments instead of sourcemaps and don't use CSS modules, you don't need css-loader at all and can use raw-loader instead. The raw-loader is just a passthrough for text, with no processing. By switching to raw-loader, we gained another 5-10%. If you're using hot reload for your css, you're already using a file watcher of some kind to pick up the change, but you should make sure to use webpack's own --watch option , or its development server . Webpack will be able to cache some information and speed up your compilation even more. By using --watch, our compile time hovers between 1.7 and 2 seconds. Let webpack handle combining your stylesheets. A great feature of Webpack is JavaScript incremental compilation. Even if you have a massive codebase that takes several minutes to compile, you'll only have to wait a few hundred milliseconds when you modify a file. Webpack keeps track of the state of your code and only recompiles what changed. Unfortunately with CSS things aren't as sophisticated. Every time you make changes to a file, Webpack will recompile all files that depend on it. If you're relying on a single entry file with a ton of @imports Webpack will have to recompile everything every time anything changes. Instead of having a monolithic stylesheet, like you can leverage Webpack's ability to just require your css where it's needed. Simply use: require('../stylesheets/foo.scss'); from JavaScript in the component that needs it. Using the ExtractTextPlugin , Webpack will combine them into 1 output file (or more, if you have multiple chunks). Webpack will no longer have to recompile everything for every change, drastically speeding up compilation after startup. There are a few caveats with this solution: Webpack has a little known feature allowing the use of an array of configuration in the webpack.config.js file. When using an array of configuration, Webpack will run multiple compilers, while still sharing internal resources such as file watchers for efficiency. If your codebase wasn't built with Webpack from the beginning, you may still rely on the ExtractTextPlugin and a top level entry point for your sass code. Webpack will put the styles in the same bundle as the javascript, then parse the modules to extract it in an external file. The more javascript in the bundle, the longer this step takes. We can speed up this operation by running two Webpack configurations: one for JavaScript, and one for CSS. This isn't the \"webpack way\" of handling CSS. You should require your stylesheets in javascript modules that need them. However, if you have a legacy code base, this is a great way to save an additional 100-200ms. Ha! No, seriously. Kind of. Building our SASS in a VM running Ubuntu on top of MacOSX turned out to be 2-3 times faster than building on the host. While css hot reload is a powerful way to increase velocity of designers and developers alike, it introduces new challenges when it comes to css processors. Every second counts when you're trying to debug a styling issue with trials and errors. Webpack, while powerful, has a large overhead over simpler tools such as gulp (in our testing, Webpack can add as much as 1.5 seconds in a large codebase). By ensuring your code is lean, your dependencies are up to date, and you don't use loader options you don't need, it is possible to keep things snappy without losing flexibility. The \"sunset tower crane\" image above is Creative Commons CC0 licensed https://creativecommons.org/publicdomain/zero/1.0/deed.en", "date": "2015-11-23"},
{"website": "Localytics", "title": "Implementing a Centralized Directory Service for AWS Infrastructure with Amazon Simple AD and SSSD", "author": ["Matt Juszczak"], "link": "https://eng.localytics.com/implementing-a-centralized-directory-service-for-aws-infrastructure-with-amazon-simple-ad-and-sssd/", "abstract": "Localytics Engineering has doubled in size in the past year. The number of SSH keys floating around on running instances was absolutely unwieldy. We responded with a centralized directory service for our AWS infrastructure. We used open tools in a straightforward way, and our team loves it. The following is a step-by-step walkthrough of our solution based on Amazon services (plus new features in Ubuntu and RHEL/CentOS that made our lives a little bit easier). We'll cover the reasoning behind our decisions along with instructions on how you can set this up yourself. Bonus: below are links to our open source Chef cookbooks (which will perform most of the work for you) Let's get started! First, we needed an LDAP-compatible backend directory. We were hesitant to use OpenLDAP because of the effort required to customize the community Chef cookbook for our needs. Also, we prefer to utilize \"as-a-service\" tools whenever possible to simplify administration. Enter Amazon's Simple AD. Although geared to replace Microsoft Active Directory (which means it’s targeted for Windows instances), a Simple AD instance is still LDAP (actually, Samba) under the hood. It was our best choice, albeit with a few limitations. We were aware of the directory integration features of PAM and NSS with pamldap and nssldap, respectively, along with nslcd. Historically these modules have been troublesome (and not well documented). To make matters worse, their caching support is nonexistent, so an LDAP connectivity outage would leave servers inaccessible. Enter SSSD, the centralized access point for all authentication and authorization requests for pam, nss, sudo, and more. SSSD is powerful, as it: You must be using at least Ubuntu 14.04 LTS and/or CentOS 6.x, as they ship with a recent SSSD version that includes these key features. While SSSD provides a mechanism for fetching SSH keys from LDAP, OpenSSH still needs to read and trust those keys as if they were in the usual location ( .ssh/authorized_keys ). As of CentOS 6.x and Ubuntu 14.04 we no longer have to redeploy configuration management or run complicated scripts just to replace SSH keys! This is achieved through an AuthorizedKeysCommand configuration parameter, which specifies a command string to return keys for a given user. The SSSD service allows us to centralize password authentication, public/private key authentication, host access, and sudo roles and access. Configuring Simple AD is probably the most challenging part of this setup. In brief, we’ll cover: To begin, spin up a Simple AD instance. You'll need to load some custom schema files. Create sudo.ldif with these contents , ssh.ldif with these contents , and sudoers.ldif with contents similar to this , which will create a root sudo role and gives all users access to it. Make sure to replace the Base DN references in each entry with the Base DN of your directory server (i.e., dc=example, dc=com). To load these LDIFS, execute these commands in your terminal, being sure to replace placeholders with proper values: If you get an error that --user is an invalid option, make sure you have the samba package installed alongside ldb-tools. Now create a user so you can test your setup. In this example , the user is tuser. Once the user is created, it must be imported by issuing the associated ldbadd command: If you have a host with samba-tool installed, you have the option to set a password for the new account: You must also use samba-tool to grant authenticated users read access to the CN=Sudoers container that was created via previous LDIFs. These commands must be run against each AD Server (as this will not replicate): Now add an SSH key to the test user you've created with something like this and apply the LDIF: Your Simple AD instance should now be properly configured. The next step is to configure SSSD and OpenSSH using a test instance and tie it into the directory service. First, spin up a vanilla Ubuntu 14.04 LTS instance, switch to a root shell, and edit /etc/resolv.conf to include your search domain and nameservers, making sure to use the correct IP addresses and domain name: Once that is in place, execute realm list in the CLI to test connectivity between your instance and Simple AD. You should see information about your Simple AD domain. Assuming that everything is working so far, the next step is to install the sssd and samba packages and join the Simple AD domain: There is a known bug that can cause a package error during realm join which we have worked around by temporarily using adcli in our Chef cookbook. Your server should now be successfully joined to the domain. Unfortunately, realm doesn't give you a completely usable SSSD configuration file, so it's necessary to overwrite /etc/sssd/sssd.conf with something like this , being sure to replace placeholders with proper values. Finally, add the following lines to /etc/ssh/sshd_config , which will tell OpenSSH to look for SSH keys via SSSD: Be sure to execute service sssd restart and service ssh restart . To test your setup, run id tuser to verify that SSSD can talk to Simple AD and retrieve user and group information about your test user. Next, verify that it can access and return SSH keys with /usr/bin/sss_ssh_authorizedkeys tuser . Finally, verify that your test user has root sudo privileges with /usr/bin/sudo -U tuser -l . Testing the configuration of Simple AD, SSSD, and OpenSSH is simple with our Chef cookbooks which are available on GitHub: While you probably want to use an organization-specific wrapper cookbook, we have tried to make it as easy as possible to clone the SSSD cookbook and simply \"get it working\". As noted above, our Chef cookbook temporarily uses adcli instead of realm to join the domain due to this bug . Future versions will revert back to using realm. To get started, make sure you have the chef-dk installed and a sane Ruby setup, then simply clone and set up the sssd cookbook and run the default test suite: In order to verify that it is possible to join and interact with your newly configured domain, copy .kitchen.local.yml.EXAMPLE to .kitchen.local.yml and update it to match your environment details, such as EC2 instance information, IP addresses of your primary and secondary AD servers, AD domain, and a user that has at least one public key and one sudo role configured. The user that you specify is used by integration tests to verify that all services are functioning correctly. Our default test user, “Guest,” likely does not have the configuration necessary to pass the integration tests so you'll want to make this is “tuser” if you used our examples above. Next, create an encrypted data bag key used for locally created data bags and configure an encrypted data bag with a valid administrative username and password, used for joining the domain with realm: The format of the data bag is: Once you're all set, simply chef exec kitchen test with-registration and voilà! If all tests pass, you've successfully configured your Simple AD server. Comments and feedback on our setup are welcome! Hero image above is of power lines and supporting structure in lane west of Main Street at Pender Street. March 10, 1914. Photo: British Columbia Electric Railway Company, CoV Archives, AM54-S4-: LGN 1241.", "date": "2015-12-03"},
{"website": "Localytics", "title": "Vertica at Scale in AWS", "author": ["Michal Klos"], "link": "https://eng.localytics.com/vertica-at-scale-in-aws/", "abstract": "At Localytics, we run a massive Vertica cluster in Amazon as a core component of our Data Platform. Our business is scaling at a fast pace, and our platform infrastructure (including Vertica) must scale with it. On the Platform team, we are constantly looking for ways to keep up with growth. This post explores the journey to arrive at our current configuration of r3.4xlarge memory-optimized instances backed by Amazon Elastic Block Store (EBS). Until earlier this year, we ran our cluster on hs1.8xlarge storage-optimized instances which feature up to 48 terabytes of direct-attach storage. Learn more about Amazon EC2 instance types (like hs1.8xlarge and r3.4xlarge) We decided to switch to EBS for the following reasons: We made our decision just as AWS released their shiny new D2 instances fleet. We stuck with our plan to run EBS-backed for the operational benefits. Here are the challenges we faced during the migration , and how we iterated on configurations. When we stood up a r3.8xlarge cluster backed by EBS it measured up very well against the hs1.8xlarge cluster, it had on a per-node basis double the compute power : 16 vs 8 cores at 2.5 ghz vs 2.0 ghz, more than double the memory : 244 GB vs 117 GB and it was running on a newer OS/kernel : Centos 6 w/ 2.6.32 vs Centos 5 w/ 2.6.18. Compared to the HS1s, though, we would have less i/o bandwidth available to us (800 MB/s vs 2400 MB/s) but we have historically not been bound on i/o. Additionally, in single-user query benchmarks it ran 2x as fast in comparison to the hs1.8xlarge cluster. When we re-directed a portion of our traffic to this cluster, Vertica promptly suffered performance degradation. However , according to all of our system monitoring tools -- the cluster was not stressed. We created a new benchmark tool that could simulate production load traffic on a different cluster. We did this simply by replaying Vertica query history the same exact way it happened in Production. This new benchmark allowed us to identify the root cause of the problem. Here are the results of our new benchmark tool: We now clearly saw that the new cluster had major issues. Vertica could not clear the queries fast enough and as more came in, it created a traffic jam which caused Vertica to start queueing and eventually timing out queries due to our timeout settings-- leading to a high aggregate runtime and error rate. We iterated on different changes we can make in our cluster and found that the majority of the performance problems were due to using the 2.6 kernel running into CPU spinlock, which is a known problem in virtualized environments. What this means is that too much time is spent by the CPUs in coordinating between themselves. Also remember that the hs1.8xlarge cluster had half the number of cores. We upgraded our kernel to 3.10 and found these new results of our benchmark: Despite the 3.10 kernel being better about spinlock, we still observed some spinlock. We decided to try running on r3.4xlarge boxes since there would be less CPUs to coordinate and found success: We did not for our use case see an issue with having 2.0Gb/s dedicated for EBS traffic vs. a general purpose 10Gb/s on the 8x boxes. We cashed in on our 2x performance gains with the above configuration and were happy for a while. But, our business is growing so fast and we are scaling at a rate where some of our wins were erased and we needed to look for the next edge. At Localytics, we cannot be satisfied with our wins for too long because each day brings several terabytes of data as well as new clients using our product. Up until this point we had been running Vertica 6.1, so we looked for more performance gains in Vertica 7.1.2. After we initially upgraded, the good news was that our performance had doubled again, the bad news was that nodes were randomly dropping due to a memory page fault error. After this, we tested many versions of the kernel with Vertica 7, and found that kernel 4.1.8 worked well. Below is a response time graph of when we turned Vertica 7: The majority of the difference we found in Vertica 7 that improved performance was that it helped lessen the magnitude of our performance spikes thus helped us avoid query traffic jams and thus produce faster results. We have been running on this configuration for a couple of months, and we are planning our next step up. Stay tuned for updates on how we iterate our Vertica infrastructure in future blog posts. If you are looking to come help us find the next edge in our infrastructure. Please apply on our job page or message me directly ( mklos@localytics.com ) Hero image of Mountain Goats courtesy of Flickr user annyuzza, CC-BY-NCND-2.0", "date": "2015-12-04"},
{"website": "Localytics", "title": "Performance in Big Data Land: Efficiency in CPU cycle usage", "author": ["Konstantine Krutiy"], "link": "https://eng.localytics.com/performance-in-big-data-land-every-cpu-cycle-matters-part-2/", "abstract": "Let’s continue to explore the idea that “every CPU cycle matters” when dealing with billions of records. In the last post, I introduced the impact of extra CPU cycles when you’re querying billions of records. This post will continue to explore areas of CPU cycle waste across layers and slices of technology. Waste can occur when a computing system is spending cycles on tasks, which is not directly related to useful work (like running SQL query). Finding and eliminating this waste will free up CPU cycles and improve system performance. The CPU itself is a very complicated component, but all complexity is hidden under the hood. Generally only hardware experts spend time on CPU configuration and tuning. Majority of people tend to think that tuning the CPU will not produce any significant performance gains. We decided to challenge that assumption and looked deep into CPU tuning. Image below shows the BIOS screen, which will allow you to make changes to CPU behavior. Some of those settings change internal CPU processing tremendously. To experiment with CPU tuning we built a 3 node HP Vertica cluster and loaded it with several terabytes of data. Our goal was to research the impact of BIOS settings on the SQL queries execution time. We ran the same queries on the same dataset and on the same hardware, but with different sets of BIOS settings 3 times in a row. Lower time means more efficient CPU usage and better query performance. Here are our findings (averaged, time in seconds): Fastest execution time we saw during benchmarking is 552 sec. Slowest time is 877 sec. The difference between the slowest and the fastest run is 40%. Those 40% represent a possible improvement as an outcome of CPU tuning effort. Consider the CPU’s tuning effort.  In our testing we clearly saw that disabling HyperThreading is greatly beneficial toward SQL performance for Vertica. HyperThreading is the feature which makes one physical core appear as two cores to the operating system. This feature is beneficial for virtualized environments, but in Vertica workloads we saw a negative impact. Any database computing system will consist of 4 major layers, which sit on top of each other. Those major layers are: Each layer depends on the layers below it. So when companies work on a new version of a product in any of the layers they use previously released products from the layers below. This dependency builds a pattern of releases in which technology in a higher layer is released later in time. Following this diagram can give you a graphical illustration of this idea. Take a look at the green check marks on the diagram below. Those check marks illustrate a good “technology slice”. Technologies included in a good “technology slice” are designed and tested to work together and provide you with the best possible performance. Newer servers are most common solution in the situation when you need to get more performance out of the system. In most cases one will buy the latest and greatest servers, but will keep the database version from the existing setup. Let’s say that our database was released 1 year ago and it was certified on OS, which was released 2 years ago. Once you take recently released servers and use a version of OS that is 2 years old (or older) you no longer have a good “technology slice”. You are trying to build your stack using technologies from different slices. One of our research projects was about the replacement of older hardware with recently released hardware. In our experimental setup with  recently released hardware we observed 2x performance degradation on SQL execution times. This was very unexpected. Further troubleshooting uncovered the fact that the system spent more CPU on system/kernel processing than on processing user/nice processing. In that case the older kernel was not able to work efficiently with the newer CPU architecture. The old kernel burned CPU cycles on managing higher core count instead of doing useful work. This issue was fixed when the OS vendor released kernel patches so that our environment was artificially pushed into the same “technology slice”. Keep your system at its best possible performance levels by building your stack using technologies from one slice. When you are forced to use technologies from different slices make sure that the CPU cycles are spent on doing useful work. Every CPU cycle matters !!! Can I call you a TRUE BELIEVER now? My name is Konstantine Krutiy, and I lead the Vertica team at Localytics. The clusters I am responsible for contain all of our analytics data (which amount to more than 1 trillion data points). In August of 2015 I presented at the HP Big Data Conference , and my session covered real-world methods for boosting query performance in Vertica. The talk was called \"Extra performance out of thin air (full slide deck is here) \".", "date": "2016-03-03"},
{"website": "Localytics", "title": "Meet Our Engineers: Brian Zeligson", "author": ["Casey Park"], "link": "https://eng.localytics.com/meet-our-engineers-brian-zeligson/", "abstract": "How did you get started in software engineering and why? I didn't finish high school, but when I was 17, I enrolled in a vocational program which led me to a tech support role at JPMorgan Chase. Around 21, I moved from New York to Boston to study music at Berklee. I had basically left the tech world behind when a friend of mine emailed me and asked me if I was interested in joining their SAAS startup. I took a chance on it and it totally redefined my idea of what a job could be. It became obvious to me that there was a huge opportunity in software for anyone with access to Google and a strong drive, both of which I had, so I aggressively started building out a portfolio and kept at it. Eventually I started putting myself out there as an engineer and after a few years I landed my first engineering job. Things took off from there. What was your path to Localytics? Prior to Localytics, I built out the engineering team at Streetwise Media, the company behind bostinno.com. I had been there as the first engineering hire at 10 people and grew with them 'til almost 40 when the company sold to American City Business Journals. Around that time I started to really crave something super technical, something that I felt was far above my current skillset. I came across Localytics in the monthly hiring post on Hacker News, reached out, and the rest is history. What's a fun fact about yourself? I competed in the 2009 Massachusetts Golden Gloves. I've boxed on and off for 14 years now and it's a compass for me in and out of the ring. What are you working on at Localytics right now? I'm working on a closed loop remarketing solution which helps engage an app's user base by specifying a subset of users and then targeting them for marketing elsewhere. Simply put, if a Localytics customer wants to show an ad that says \"buy one, get one half off on sneakers,\" they can go into Localytics and say \"show me everyone that bought a pair of sneakers in the last 6 months, and I want you to show them this ad on Facebook.\" What's neat about what we're working on right now is that after sending that ad, a few days later, you can log into Localytics and find out all the people that came back through this ad, and the number of users who bought a pair of sneakers. This is really powerful because it lets you keep your current user base as well as winning back some of the users that started to slip away. What’s exciting about this job? I have a really strong sense of how much value my work actually brings to the table. Everything is really transparent and the impact my work has to the company is exciting. Things are moving fast enough that you get immediate feedback as to how successful your feature was and how much revenue it generated, or even how much faster your coworkers can go by using the software you've built. For example, Stager was one of the first things I released at Localytics almost a year and a half ago and is now a staple in our deploy pipeline, used by half of engineering every day. These kind of results make me feel good about what I have done and get me hungrier for more; the feeling of success is really addictive. What is your biggest accomplishment? My biggest accomplishment is my team. Although I can’t take full responsibility for it, I am proud of the quality of people we have on my team. They’re all talented and good people, who are enthusiastic about their job. Why do you take pride in working here? I’m here working next to MIT and Stanford grads, prospective Facebook and Google hires. Being completely self-taught and having risen to the level where I get to hang with people as smart as the folks we have at Localytics is a huge milestone in my life. What tools do you use everyday in your job? JIRA, Scala, Ruby, Javascript, and Github would be the primary tools I use. Then there’s Vim for my IDE, Chrome (I usually have about 200 tabs open), the keg, and Knob Creek Bourbon whiskey. How do you spend time outside work? Any hobbies? I have been boxing lately. I started going to the gym and started to prepare to get back in the ring. Other than that, I like getting drinks with my coworkers. Wanna work with people like Brian? We're hiring .", "date": "2015-09-03"},
{"website": "Localytics", "title": "Serverless Slackbots Powered by AWS", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/serverless-slackbots-powered-by-aws/", "abstract": "Today Localytics is open sourcing two tools that help you quickly scaffold custom Slack commands using AWS Lambda, AWS API Gateway, and the Serverless framework. These tools grew out of our continued investment in making ChatOps the primary way Localytics engineers manage our infrastructure. With Serverless and a small bit of scaffolding, we’re now able to create custom Lambda functions that handle slash commands from Slack in about 10 minutes. ChatOps helps break down silos by making infrastructure management a collaborative process. Our engineering team is able to quickly see activity from our systems and tools within the same stream of information as the general discussion amongst the team about their daily work. Our chat service of choice - Slack - provides convenient integration points with their slash commands utility . AWS Lambda is a natural fit for implementing slash commands because of its event-based architecture. Since we are deployed on AWS we also get to leverage the integrations that Lambdas have with other pieces of the AWS puzzle, like S3 event notifications and DynamoDB streams. The Serverless framework helps fill in some of the gaps that Lambda has with its development and deployment workflow. Serverless lets you set up lifecycle stages that bind together your Lambda functions along with their dependent AWS resources, so you can easily promote code from dev to test to prod. Serverless also has an active and responsive community that is great to work with. For scaffolding we’ve built our serverless-slackbot-scaffold . This is a khaos template, which is a handy utility package that uses handlebar templates to scaffold projects. The README of the repository describes the process for installing and running the templating engine. Once the new directory has been created, you’ll see a sparse serverless app, featuring a nodejs component (a Serverless structure with a specified runtime) that contains a slackbot function (representing a lambda function). Over the course of developing our own projects, we realized common patterns were emerging around routing subcommands to the correct functions. As a result, you’ll notice that the package.json file in the nodejs directory requests an additional package besides the default Serverless dependencies, which is the lambda-slack-router package . This package’s easy DSL allows us to quickly manipulate the JSON payload coming from Slack, verify the integrity against a predefined token, and call the correct subcommand. This means we’re not restricted to commands like “/bot” but have easy access to “/bot ping”, “/bot echo”, and so on. With this package in place, the only part of this bot that needs editing is the actual business logic of the Lambda. Example routers are provided in the scaffold repository to get you started. Once the business logic is written, we’re free to deploy to AWS. Following the deployment steps from the templated project’s README we can deploy the function to a specific stage (our template contains the test, dev, and prod stages pre-configured), acquire the postback URL, and put that into the Slack configuration. Once everything has resolved, you can use the “/bot help” prebuilt command to see how to use your new bot. The Serverless framework has aided us immeasurably in handling development and deployment of our functions, as well as brought a sense of pre-defined structure to our projects. Over the course of using Serverless, we’ve explored (and contributed to) a number of the available plugins which can be quickly implemented to augment Serverless’ abilities. As Serverless and its ecosystem continues to grow, developing Lambda functions is going to become even easier than it is now. Feel free to use our scaffold and router for yourself to develop your own Slack integrations, and when you do please share your experience, approach, and any feedback in a gist, on a blog, or in the comments. Photo credit: \" Robots \" by sneakerzoom is licensed under CC BY 2.0", "date": "2016-03-08"},
{"website": "Localytics", "title": "Amazon EBS SC1 Initial Impressions", "author": ["Brett Warminski"], "link": "https://eng.localytics.com/amazon-ebs-sc1-initial-impressions/", "abstract": "As you may have read in our previous posts ( here , here and here ), one of the largest pieces of our data infrastructure at Localytics is the petabyte scale Vertica analytics database that we host on Amazon Web Services. We've been relying on Amazon's Elastic Block Store (EBS) as the storage solution for this database for more than a year now. EBS allows you to provision virtual block devices and attach them to your compute instances as regular drives, giving you the ability to effectively decouple the size of your computing resources from their storage. EBS has been a good choice for us because of the flexibility it allows in the operation of our warehouse. One feature that we really take advantage of is the ability to quickly snapshot and restore the contents of these virtual drives to and from S3. In addition to being an excellent backup/recovery solution, it also allows us to replicate the entire contents of our data warehouse in order to scale or benchmark new features. Recently, we had the opportunity to spin up a warehouse with Amazon's shiny new SC1 magnetic backed EBS volume type. Amazon already has three different storage types, the \"standard\" magnetic backed storage, a general purpose  SSD backed storage solution called GP2 and a provisioned high-throughput SSD solution called io1. The SC1 type is a member of their new SC/ST family of magnetic backed volume types that are designed specifically for data warehouse use cases such as ours. They offer a lower price point, stronger performance consistency guarantees and \"burst\" optimization for long sequential reads that column-oriented databases such as Vertica really love. SC1, specifically, is optimized for colder storage. EBS standard magnetic has worked pretty well for us in the past. Over the summer, we flirted with GP2s and found that we really liked the consistency the newer generation of SSD backed volumes offered, but couldn't justify the price as our data volume has grown exponentially. For a quick background, here's our current cluster setup and requirements: Based off a month of performance testing, SC1/ST1 hits the sweet spot for us because it's designed with our use case in mind and offers an excellent price per performance tradeoff. Here's a side-by-side comparison of the peformance distribution of the two different volume types serving a 24 hour sample of read/write queries that take Vertica a median time right at 1 second to serve on standard EBS: \"pct\" refers to the percentiles of query times, ie 50pct means 50% of our queries ran in this amount of time or less. The small / large designation here is a combination of client size (active data points) and date range of time-series data traversed. Our data is optimized to take advantage of client and date order. As you can see, there is about a 500ms penalty added to this set of queries, but the top end of query time ends up in the same bands as standard volume types, which is well within our performance expectations. Other benchmarks we've run show the standard deviation between runtimes of similar queries to be significantly smaller on SC1 than on standard volumes. We were so impressed with the performance, that we were comfortable powering our analytics dashboard for a day. Here's our New Relic view of three days of performance. There was such a small difference in peak performance, it's hard to distinguish which of the three days above was backed by SC1 volumes. Performance characteristics aside, running this configuration is expected to shave 10% off our platform operating costs, which is a huge win. There are a few things we plan on trying now that these drives are generally available: As we grow our marketing platform capabilities, EBS products like this will allow us to maintain the high level of data granularity that we pride ourselves on. For us, shifting workloads to this new volume type is a no brainer. I hope this is as interesting and exciting for you as it is for me. If so, you might want to do this kind of stuff as your day job and we'd love to hear from you. Hit me up ( bwarminski@localytics.com ) or visit our jobs page directly.", "date": "2016-04-19"},
{"website": "Localytics", "title": "Testing AWS Scala Microservices", "author": ["Ben Darfler"], "link": "https://eng.localytics.com/testing-aws-scala-microservices/", "abstract": "In January and February of this year we open sourced three SBT plugins that have had a huge impact on our ability to test our AWS based Scala microservices architecture: This is our story . Stop me if you've heard this one before. After one too many production bugs in the early days of the company, some engineer (me) goes off and builds up a minimal set of end to end tests. Feeling smug, this engineer moves on with his life and gets back to shipping features. Time moves on, the team grows (hey everyone), revenue flows in (sweet), microservices flourish (awesome) and the end to end tests grow like a Kudzu (dear god). What once was a fast and stable set of tests that covered the integration of two microservices is now a massive, slow and brittle test suite that attempts to test the complex interactions between nearly a dozen services. Yes, ours is not a unique story but we'd like to think that we have brought our unique brand of pragmatism to bear on the problem. Our progression towards a sane testing setup began at our fall hackathon . I had a suspicion that our end to end tests were slowing us down but I wanted to lead with data. Thanks to the awesome work of Nate Tenczar we use a slack bot named Dibs to moderate access to a number of shared resources including our end to end test environment. Dibs is a pleasure to work with and on the side it dumps metrics into our internal \"dogfood\" system. (Yes, we use Localytics to monitor parts of Localytics.) After some quick digging, I found that it often took developers 3 hours to run the end to end tests. Since our build server clocked the tests at around 15 minutes per run this meant that folks were frequently rerunning the test suite multiple times due to flaky tests and broken infrastructure. With this data in hand, it didn't take long to drum up support for addressing the issue but first we wanted to know what, specifically, was causing the pain. We knew the tests felt \"icky\" but could we quantify that better? After a few rounds of lively discussion we came up with three core issues: With the problem more clearly understood we turned to the literature and found Martin Fowler's excellent slide deck on Testing Strategies in a Microservice Architecture . His diagnosis was spot on: Since end-to-end tests involve many more moving parts than the other strategies discussed so far, they in turn have more reasons to fail. End-to-end tests may also have to account for asynchrony in the system, whether in the GUI or due to asynchronous backend processes between the services. These factors can result in flakiness, excessive test runtime and additional cost of maintenance of the test suite. It was now very clear to us that we were missing some of these \"other strategies\" that Fowler spoke of. More specifically we needed a way to test an entire service in isolation. This would allow each developer to test locally in parallel without fear of broken external dependencies. Moreover, it would give us a place to migrate most of the end to end tests. We still wanted a limited set of end to end tests as a sanity check but the vast majority of tests were really service level tests with no other place to live. The question then turned to one of execution. The end to end tests were initially set up to address the difficulty of testing locally with all the necessary AWS service dependencies. While some might propose refactoring our systems to allow for injecting fake test dependencies we tend to take a more pragmatic approach to testing that doesn't require abstracting away our underlying services just so we can test. This does blur the line between a unit and an integration test but we find it gives us higher code coverage with less time spent writing tests. 5 years ago we had no other options than to test directly against AWS services. Thankfully things have progressed since then. At our first hackathon, a small team of engineers built a Docker setup that allowed developers to run the end to end tests on a single laptop. While it served its purpose it was a bit of a pain to setup and even more of a pain to maintain. After struggling with orchestration, networking, and the manic pace of Docker development we came to a simple conclusion. Docker, with all of its idiosyncrasies, was adding more overhead than value to our development process. However, when you scratched beneath the Docker surface, there was a heart of gold. The crew that built this project had found a whole suite of open source, stand-alone mocks for a variety of AWS services and, after some hunting around, we discovered Graham Rhodes from Gilt had written an SBT wrapper for one of them . We quickly jumped on this pattern and ran with it. Graham hadn't had the time to keep up with the project so we gave it a new home and a few siblings in the form of an SBT wrapper for SQS and for S3 . These SBT plugin projects have a number of significant wins over the previous Docker setup. Once the SBT plugins are integrated into a project they are responsible for downloading their respective mock services. This zero setup configuration is not only super convenient for developers but it also makes it trivial to run in our continuous build system. Additionally, the plugins are designed to hook into the SBT lifecycle. This allows them to be transparently started before the tests begin and spun down after the tests complete. Again, this removes a significant barrier for both developers and the continuous build system. Finally, by removing the pain of the Docker setup, they make testing easier and more enjoyable and that leads to more tests, better code coverage, and improved code quality. With the AWS dependencies settled we turned towards dealing with SQL. Thankfully this is a bit more of a solved problem. We had been slowly migrating over to ScalikeJDBC as our SQL library of choice and their testing support is fantastic. We whipped up a small utility for loading schema and seed data and, after some small tweaks to our SQL queries, settled on H2 for our testing database. With all of our dependencies taken care of we enabled the integration test support in SBT and were able to move our first end to end test into a locally runnable integration test. We still have a ways to go to dismantle the end to end tests but we now have a vision for how to get there and a set of tools to help us along the way. Photo Credit: NatalieMaynor", "date": "2016-04-27"},
{"website": "Localytics", "title": "Humidifier - CloudFormation made easier", "author": ["Kevin Deisz"], "link": "https://eng.localytics.com/humidifier-cloudformation-made-easier/", "abstract": "Today we are open-sourcing Humidifier - one of the tools that we use internally to manage infrastructure on Amazon Web Services (AWS) at Localytics. This Ruby gem allows you to programmatically generate domain objects that represent the AWS CloudFormation (CFN) resources you want, handling the details of JSON manipulation under the covers. We’ve found that Humidifier not only increases development speed, but also results in easy-to-understand, maintainable code that allows you to focus on building resources instead of programming in JSON. As our infrastructure at Localytics continues to scale on AWS, we’ve become more and more reliant on CFN. Being able to create multiple interrelated resources in a fast and reproducible way is a must for a fast-moving technology team, especially when living in a microservice environment. CFN’s key strength lies in its ability to manage large amounts of infrastructure. Its JSON structure, however, can be inflexible, difficult to manage at times, and challenging for newcomers to CFN. Things as simple as referencing another resource in the same stack or concatenating strings requires complex objects that invariably decrease development speed. Many tools currently exist in the industry that solve the problem of provisioning and maintaining AWS resources, such as Terraform and SparkleFormation. While we admire these tools, we found ourselves wanting something a little different. We found that SparkleFormation’s DSL was too complex for our use cases, and we missed having our resources provisioned as part of CFN stacks (which you give up with Terraform). We wanted to write the code ourselves but retain the ability to leverage CFN’s strengths. Fortunately, the CFN docs are in pretty good shape. This page in particular became the inspiration for Humidifier, as it lists every possible resource that CFN supports. Each link from that list contains the specifications for all of the attributes on each CFN resource. With those specs in hand we built Humidifier to scrape the docs, build Ruby classes for each resource, and provide accessors for every attribute available. Through these same utilities, Humidifier is able to stay current by checking and updating the specifications on a regular basis. The result is a Ruby gem that provides complete programmatic access to CFN resources, including the ability to quickly and easily deploy CFN stacks and change sets. Using Humidifier you can drastically reduce the amount of lines that you need to write to describe your infrastructure. The following is an example of a simple stack with a load balancer and auto-scaling group built with CFN’s JSON structure. Using Humidifier, we can build the same structure in Ruby. This code is shorter, easier to understand, and can be tested and reused like any other code you write. Humidifier is up on GitHub here and free for use. The docs are available on GitHub pages here . When you use it, please share your experience, approach, and any feedback in a gist, on a blog, or in the comments.", "date": "2016-06-20"}
]