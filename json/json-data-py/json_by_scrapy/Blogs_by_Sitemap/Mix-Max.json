[
{"website": "Mix-Max", "title": "Improving Mongo performance by managing indexes", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/improving-mongo-performance-by-managing-indexes/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on December 9th was\nabout Introducing search-string: an advanced search string parser Querying large collections efficiently Imagine you want to learn more about database performance, and you have in your hands a very large\nbook about databases in general. How can you search for your topic of interest? More often than not, the answer is: go to the index (usually located at the back of the book),\nlook up the topic (usually in an alphabetical list), and the index will tell you the page where that\ntopic is discussed. If the book doesn't have an index, you probably need to go through each page and try to find\nrelevant information about performance - that would be a tedious and long endeavour. Similarly, when you ask for some document in a database, the database tries to use an index to quickly find\nthe results for you. If there's no index to use as reference, it has to check each document, the same way\nyou would have to if your book didn't have an index. The database should probably be able to\nhandle it if there aren't a lot of documents to search, but when your database has close to 1 billion\ndocuments and is being queried thousands of times in a second... then it becomes a problem. Indexes are data structures that allow databases to quickly find documents in a\ncollection. In Mongo you define an index with a command like this: db.events.ensureIndex({\n  action: 1\n}, {\n  background: true,\n  name: 'events_index_by_action'\n}); The above tells the database to construct an index on values from the action property of the events collection (additionally, it tells the database to build the index in the background ).\nSince building an index is a blocking operation, for very large collections it can take many hours\nfor indexes to finish building, causing the database to stop answering any other queries. With the above index the database will be able to efficiently retrieve results when querying by action . For example, this query: db.events.find({\n  action: 'send email'\n}); will be very fast, because the database can quickly get all documents matching the action send email and return it to you. Now, if you want to view send email events since one month ago: db.events.find({\n  action: 'send email',\n  date: { $gt: moment().subtract(1, 'month').toDate() }\n}); The database will be able to retrieve all send email events very quickly using the index. However,\nif send email is a very common event, the query will be very slow! This is because you can't further\nfilter by date using that index, so the database will have to check each of the millions of send email events. Defining efficient indexes To fix the case when you want to filter our events by date range, you can continue building on top of\nthe previously defined index, and define it like this: db.events.ensureIndex({\n  action: 1,\n  date: 1\n}, {\n  background: true,\n  name: 'events_index_by_action_and_date'\n}); The above compound index will improve the performance of the query that also\nfilters by date . However, while it looks correct at a quick glance, if you visualize how the\ndatabase would look for documents using the index, you can probably guess why it's not actually\nthe best index for that query. Imagine that you have a book of events. In the book's index, you see maybe 20 different types of\nevents, and for each event you see a very long list of pages where a certain event can be found for a\ngiven date. Assuming that your events are more or less evenly distributed by action, and your collection\nsize is 1 billion events, if you search by event first, you reduce the search area by roughly 1/20.\nThis means that, out of an original 1 billion of documents, you now have to scan 50 million documents. What if the index instead showed you the date first instead of the action first? You would be able to find\nevents from a given day quickly, and then get the events grouped by action. Assuming you have 4 years worth\nof historic event data and are querying for the events of a single day, indexing by date first reduces your\nsearch area by 1/1460 in your first step! Now you have to scan only ~1.35 million documents - around 37 times\nless than if you scan by event type first. The index would look like this: db.events.ensureIndex({\n  date: 1,\n  action: 1\n}, {\n  background: true,\n  name: 'events_index_by_date_and_action'\n}); When creating a compound index like the one in this example, ask yourself this question: \"Which\nproperty of my find query is the most 'unique' one?\" In the above example, date is more unique than action , because each day has a new unique value. This 'uniqueness' property of a document\nattribute is called 'cardinality'. The higher cardinality you have for your first properties in the\ncompound index, the better it will perform, because higher cardinality fields do a better job at\nreducing the search area of the query. Ensuring that indexes are efficiently used Now your collection has nicely defined indexes with high cardinality fields on top, ensuring that your\nsearch space is reduced significantly from the beginning. Great! Now how can you ensure that your\ndatabase uses the index as efficiently as possible? For indexes to be efficiently used, you want them to fit in the RAM available in the\ndatabase server. RAM in Mongo is mostly used for keeping the most frequently requested data and indexes -\nthis is known as the working set . On a WiredTiger storage engine, the default amount\nof RAM used for the working set is 50% of RAM - 1gb or 256mb , whichever is highest. Assuming you\nhave a server with 32gb of RAM, this means there is 15gb for cache. Mongo uses this space to juggle around\nthe most commonly retrieved data and indexes (it can load a subset of an index).\nMongo also uses memory for other tasks, like managing connections and handling aggregations, not to\nmention other processes running in the machine beside Mongo. It's not uncommon that, in order to support the variety of ways to query a collection, you'll need to\ndefine many indexes. It is very tempting to improve lookup queries by adding indexes\nwithout much consideration, but this is also an easy way to bloat the database with many indexes. You\ncan inspect your database's overall index size like so: db.stats().indexSize\n65171598336 This is the size of all indexes in the database in bytes. In this example it's 65gb. This is not an\nideal size for a server with 32gb as per the example above! Because these indexes can't fit in memory,\nyou'll be performing reads from disk and will be severely limited by disk I/O throughput. It is not easy to know how much memory your database needs. Some questions you might want to consider: How large is your data? How frequently is data requested (to determine approximate working set sizes)? How large are your indexes? What is your projected data growth in the short/mid term? Strategies to keep indexes size small Here are a few ways to keep index sizes small roughly ordered in ascending difficulty: Remove unused indexes You can examine the indexes of a given collection and their usage like so: db.events.aggregate([ { $indexStats: {} } ])\n[\n  {\n    \"name\" : \"events_index_by_date_and_action\",\n    \"key\" : {\n      \"date\" : 1,\n      \"action\": 1\n    },\n    \"accesses\" : {\n      \"ops\" : NumberLong(5941923910128),\n      \"since\" : ISODate(\"2017-11-02T05:35:39.726Z\")\n    }\n  },\n  {\n    \"name\" : \"events_index_by_modified_date\",\n    \"key\" : {\n      \"modifiedDate\" : 1\n    },\n    \"accesses\" : {\n      \"ops\" : NumberLong(0),\n      \"since\" : ISODate(\"2017-11-02T05:35:39.726Z\")\n    }\n  }\n] In the above example, there are two indexes. Under accesses , you can see that the first index\nhas been used many times. Meanwhile, the second index has not been used at all: it is a candidate\nfor removal. Suppose there were a third access with very few accesses, say around 100. That index might\nbe a candidate for removal. However, it is important to understand which query used the index to\nunderstand the repercussions of the removal of said index at the application level. Note that the ops number might be deceptive, because the number of uses are counted since the time\nMongo server process started, in this case, since November 2nd. Remove redundant indexes Similar to above, you can inspect the definition of your indexes. For example in this output: db.events.aggregate([ { $indexStats: {} } ])\n[\n  {\n    \"name\" : \"events_index_by_action\",\n    \"key\" : {\n      \"action\": 1\n    },\n    \"accesses\" : {\n      \"ops\" : NumberLong(55819201221949),\n      \"since\" : ISODate(\"2017-11-02T05:35:39.726Z\")\n    }\n  },\n  {\n    \"name\" : \"events_index_by_action_and_date\",\n    \"key\" : {\n      \"action\": 1,\n      \"date\" : 1\n    },\n    \"accesses\" : {\n      \"ops\" : NumberLong(3748596),\n      \"since\" : ISODate(\"2017-11-02T05:35:39.726Z\")\n    }\n  }\n] You can see that both indexes are used, so at first glance they are both needed. However, the second\nindex makes the first one redundant, since queries on action alone will be able to use the second\nindex without problems. In general, for compound indexes, a query will be able to use it as long as\nthe fields in the query appear in order. For instance a query with date only will not be able to\nuse the second index above, because date is the second indexed property in the compound index. Use sparse indexes Index sizes can be significantly reduced by making indexes sparse. When defining an index, you can apply\na constraint that tells the index which documents it should index. This constraint is named $partialFilterExpression . For example, in the scenario of querying\nevents by type and date, the product requirement is to support searching events that\nwere triggered by manual user interaction. Since you don't care about events triggered through automation\nor API usage, you can define an index like so: db.events.ensureIndex({\n  date: 1,\n  action: 1\n}, {\n  background: true,\n  name: 'events_index_by_date_and_action',\n  $partialFilterExpression: {\n    source: 'interaction'\n  }\n}); Assuming that 60% of events are triggered by direct user interaction, this means that the\nindex only indexes 60% of documents in the collection, saving memory. Reduce collection size The less data in a database, the smaller the indexes will be, and the less memory will be\nrequired to keep it in RAM and ensure snappy responses. You can reduce collection size by moving old\ndata away to \"cold\" storage. In the case of the events collection, there are four years worth of data,\nand it is likely that there isn't a lot of demand to retrieve old data. You can remove that data from the\ndatabase to another place for archiving, as long as you provide means to retrieve said data (with the\nunderstanding that it is a slower process. Keep indexes simple. Compound indexes are highly powerful, since they will support creating more granular filters. However,\ncompound indexes are more complex to maintain and larger by nature. Try to minimize the number of\nfields in a compound index in order to keep them small. This is probably easier said than done, but this requires a good deal of time spent in the design of\nthe database schema. Just because Mongo is considered \"schemaless\" doesn't necessarily mean that you\nshould just start writing code, disregard design of the schema, and figure it out along the way.\nTo define a good schema, it is important to have good understanding of the product, considering\ncurrent requirements as well as foreseeing future requirements. Designing a good schema requires\nsome abilities in reading the future. Sharding Sharding the database is another option. What this basically means is that data is partitioned by\nsome criteria (a shard key) and kept on multiple clusters. This option may be even more complicated\nthan the previous option - it requires a good deal of understanding of the schema and, moreover, that you\nhave a plan for supporting sharding from the moment the schema is designed. For a shard setup to be effective, the shard key must guarantee an even distribution of\ndata across the shards. For example, imagine you have 5 shards storing data for 5 different action types.\nIf 70% of events are send message , one shard will receive 70% of your data, while the rest will be\nreceiving 30% across the 4 of them . Conclusions It can be surprising how often you can find low hanging fruit when it comes to improving performance in\ndatabases. Simply removing unused and redundant indexes can be a big boost in performance.\nFurthermore, spending time designing a solid, future proof schema can be rewarding in the long term,\nallowing you to run your database in smaller servers and helping to implement other performance\nimprovements such as sharding. Do you like working on interesting performance problems? We’re hiring !", "date": "2017-12-10"},
{"website": "Mix-Max", "title": "How Non-Engineers Test New Features at Mixmax", "author": ["Simon Xiong"], "link": "https://www.mixmax.com/engineering/how-non-engineers-test-new-features-at-mixmax/", "abstract": "Mixmax is essential to our customers’ workflows. Because of this, when we deploy new features to production, we want to avoid introducing any regressions that might disrupt our customers’ work. Before deploying new code, we want to make sure that: Our code works correctly. The new code doesn’t introduce any usability problems. Engineers ought to be able to verify #1, since that’s simply a matter of implementing the spec they developed. It's not always best for the person who wrote the code to verify #2, since they'll often have gotten used to the UX while building it. To reduce the risk of introducing features with usability issues, we ask non-engineers test out new features before releasing them to the public. At Mixmax, we have 3 methods for letting non-engineers test new features: Test on our staging environment, which mirrors our production environment. Give testers access to the developer’s local environment using a technique we call \"shared testing\". Ship the new feature to production under a feature flag, and test on production. This post describes what these methods are, their advantages and disadvantages, and how we decide which methods to use when shipping a new feature. 1. Staging Our earliest solution was to set up an environment that mirrored our production environment, except without using production data. We called this the staging environment . First, we deploy our new feature to staging, which every Mixmax employee can access through a secure VPN connection. We then schedule a meeting with our QA team to test the feature. We call this meeting a test party . When the test party is complete and all issues have been resolved, we ship the code to production. Advantage : Deploying to staging is almost the same as deploying to production, except no real users will be affected. This makes staging a high-quality, low-risk testing environment. Disadvantage : We only have one staging environment, and it’s part of our deploy pipeline. This means testing on staging can block other engineers from deploying changes to production until the test party is complete. 2. Shared Testing As our team grew and we were able to develop more features simultaneously, we noticed our staging environment and deploy pipeline were constantly being blocked by test parties. In response, we began using a technique we call shared testing . In shared testing, the developer runs a command that: Makes a copy of all their code. Runs the code on different ports than the ports normally used during development. Uses ngrok to give testers access to the shared testing ports. Advantage : Shared testing is a low cost testing method because we don’t need to deploy any code. We also avoid blocking other engineers from shipping code to production while testing. Disadvantage : Shared testing doesn’t mirror our production environment, and not every part of our product can be tested using shared testing (for example our Mixmax Chrome extension). 3. Feature Flags in Production The third technique we use is to ship the new feature to production, but put it behind a feature flag. A feature flag is a few lines of code that decides whether to load the new feature or the old feature based on the user. Here’s an example: if (User.isMixmaxStaff()) {\n  // Load new code.\n} else {\n  // Load old code.\n} Advantage : Testing using a feature flag is the most accurate testing method because we’re actually testing on production servers. Testers can test by using the new feature in their everyday workflow, which means the tests more accurately reflect how users actually use the new feature. Disadvantage : Some features cannot be placed under a flag because the old code has been removed or almost totally refactored. In addition, adding feature flags introduces tech debt into our code because we have to deploy a second time to remove all the feature flags. Guidelines For Choosing a Testing Method The goal of testing is to make sure our features are polished while minimizing risk for users and cost for developers. With this in mind, here are some guidelines we follow when deciding which testing methods to use for a given feature: We use a feature flag when possible because it provides the highest quality of testing. However, even if we use a feature flag, a subset of our users will be affected if there are usability issues with the code changes. If the cost of disrupting these users is high, we do a test party on shared testing (preferred) or staging before deploying to production. If the cost is low, we can ship to production under a feature flag and do a test party on production if needed. If we can’t use a feature flag, we do a test party on shared testing before deploying to production. This has the lowest cost for developers because we don't have to deploy our code to staging, which will block our deploy pipeline. If we can’t test on shared testing, we block our staging environment and do a test party on staging. Choosing the right testing method is about maximizing the benefits of testing while minimizing the costs. If you follow this basic principle, you can build a great testing process regardless of which testing tools your company uses. Interested in working on the communication platform of the future? Email us at careers@mixmax.com and follow us @Mixmax", "date": "2018-01-25"},
{"website": "Mix-Max", "title": "Simply scalable Pritunl VPN deployments", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/simply-scalable-pritunl-vpn-deployments/", "abstract": "Being secure Every security minded organization knows the need for a secure manner to access their private networks, but even in this modern “Infrastructure as a Service” world, VPNs often have to be built manually. When they work well, no one knows that they’re there. When there is even the slightest issue though, everyone notices - accessing internal portals takes an appreciable amount of time due to large latency spikes, teams have difficulty interacting on private resources due to flakey connections... it’s not a pretty world. To ensure these issues never arise, VPNs either need to be oversized or they need to be able to autoscale - they must be highly available (HA). Today we’re going to talk about autoscaling Pritunl - our preferred VPN solution at Mixmax. Pritunl We love Pritunl at Mixmax - it’s relatively simple to setup and it’s built to be highly available. It also has single sign on, which makes getting users set up with their credentials much easier than with OpenVPN. It’s also more secure than OpenVPN’s alternative, because Pritunl will create temporary, authorized download links for users to retrieve their personal credentials, whereas in normal OpenVPN deployments credentials have to be shared in some manner (via USB, email, etc). Pritunl also has built in auditing of user activity as well as visualization of the load on your deployment. All of this sounds great, so what’s the problem? The problem While deploying an HA Pritunl configuration is much easier than other systems, it’s still a manual process. In addition to that, due to the manual nature of adding new nodes to the cluster, Pritunl can’t easily autoscale out of the box. While this is fine for most users, we wanted a VPN solution that was as hands off as possible. In order to be able to do this, there were a few problems to solve: We need to know the correct Mongo URI for the Pritunl node to start up with, otherwise it won’t be able to identify other nodes to coordinate with. We need to register the new host as part of our server set that defines the Pritunl nodes, otherwise any new nodes won’t be able to register themselves to accept user traffic. We need to disable the Source/Destination check on the EC2 instances, otherwise they would refuse to proxy network traffic. Good thing we like solving problems! Getting creative Let’s walk through how we solved the previous problems in the user data template file that every new Pritunl node starts with. Bootstrapping the necessary data Bootstrapping data is a difficult problem, or rather, it’s a difficult problem if you don’t use a secret management system. Here at Mixmax, we use Vault for storing secrets and auditing access to them. As such, we were able to use Vault in order to retrieve three sensitive credentials that each node needs during its initial boot sequence (which we run as the instance’s user-data). # Retrieve the Vault binary for our platform.\nwget https://releases.hashicorp.com/vault/0.9.3/vault_0.9.3_linux_amd64.zip\n\n# Unzip the downloaded zip file to access the `vault` binary.\nunzip vault_0.9.3_linux_amd64.zip\n\n# Move the binary into location known to our $PATH.\nsudo cp vault /usr/local/bin/\n\n# Get the instance's PKCS7 signed document.\npkcs7=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/pkcs7 | tr -d '\\n')\n\n# Make sure we know where the correct Vault is.\n# Note that these variables are passed in via our Terraform template file provider.\nexport VAULT_ADDR=${vault_addr}\n\n# Authenticate to Vault.\nresult=$(./vault write -field=token auth/aws/login role=${pritunl_node_role} pkcs7=$pkcs7)\n\n# Now we can login with the token.\n./vault login $result\n\n# Let's next grab the Mongo connection URI to join the cluster.\nmongo_uri=$(./vault read -field=value secret/${mongo_uri_location})\n\n# Once connected to the cluster, we'll need to register this server so\n# get the API token and secret.\napiToken=$(./vault read -field=value secret/${api_token_location})\napiSecret=$(./vault read -field=value secret/${api_secret_location)\n\nOnce we have the necessary credentials, we need to tell our local Pritunl service about the Mongo URI.\n# We need to stop the service before we modify the Mongo connection URI.\nsudo stop pritunl\n\n# Time to join the cluster!\nsudo pritunl set-mongodb $mongo_uri\n\n# Now that we've modified the Mongo connection URI, let's restart the server.\nsudo start pritunl Awesome! Now our node knows how to communicate and learn about all other nodes in our deployment. Registering the host for work Now that our node is connected to the rest of our deployment, we need to register it as able to accept network traffic. Thankfully Pritunl has an API that will allow us to do this. # We also need to figure out the ID of the host that we're on so we can register it.\n# Use a script to install Mongo 3.4.\n# We use it instead of inlining it here as we don't need to make inline changes\n# to it.\nbash <(curl -s https://gist.githubusercontent.com/ttacon/98da5515e1662441c7093d83386cd610/raw/dc100a899dd0bbbaa7af6c47a3c3cad96d2afd8c/install-mongo-tools.sh)\ncat <<EOF > host.js\ndb = db.getSiblingDB('pritunl');\nhostname = '$(hostname)';\nhost = db.hosts.findOne({ hostname }, { _id: 1 })\nprint(host._id)\nEOF\n\n# Run the script to get the `hostId`\nhostId=$(mongo --quiet $mongo_uri host.js | tail -1)\n\n# Create a python script that we'll use to add the host to the known server\n# block.\ncat <<EOF > setup.py\nimport sys, argparse, requests, time, uuid, hmac, hashlib, base64\nBASE_URL = '${vpn_base_url}'\nAPI_TOKEN = '$apiToken'\nAPI_SECRET = '$apiSecret'\n\n# Setup known arguments.\nparser = argparse.ArgumentParser(prog='pritunl-host-modification (phm)')\nparser.add_argument('--host', help='The host to either remove or add to the server block')\nparser.add_argument('--action', help='Either to add or remove the host from the server block')\n\n# Parse the arguments from the command line.\nargs = parser.parse_args()\n\n\ndef auth_request(method, path, headers=None, data=None):\n    \"\"\"\n    Makes an authorized HTTP API request to our Pritunl server for the given\n    path and method.\n    \"\"\"\n \n    # Create the auth params that we'll need in order to sign the request.\n    auth_timestamp = str(int(time.time()))\n    auth_nonce = uuid.uuid4().hex\n    auth_string = '&'.join([API_TOKEN, auth_timestamp, auth_nonce,\n        method.upper(), path])\n    auth_signature = base64.b64encode(hmac.new(\n        API_SECRET, auth_string, hashlib.sha256).digest())\n    auth_headers = {\n        'Auth-Token': API_TOKEN,\n        'Auth-Timestamp': auth_timestamp,\n        'Auth-Nonce': auth_nonce,\n        'Auth-Signature': auth_signature,\n    }\n\n    # Any any extra headers that were passed in.\n    if headers:\n        auth_headers.update(headers)\n\n    # Make the request.\n    return getattr(requests, method.lower())(\n        BASE_URL + path,\n        verify=True,\n        headers=auth_headers,\n        data=data,\n    )\n\n# Seatbelts for script usage.\nif not args.host:\n    print 'Must provide a host identifier'\n    sys.exit(1)\n\n# Allow both the ability to add and the ability to remove hosts to and from\n# server blocks.\nif args.action == 'add':\n    print 'Adding host \"{}\" to the server block'.format(args.host)\n    response = auth_request(\n        'PUT',\n        '/server/${server_id}/host/{}'.format(args.host),\n        data={\n            'id': args.host,\n            'server': '${server_id}'\n        }\n    )\n    print response.status_code\nelif args.action == 'remove':\n    print 'Removing host \"{}\" from the server block'.format(args.host)\n    response = auth_request(\n        'DELETE',\n        '/server/${server_id}/host/{}'.format(args.host)\n    )\n    print response.status_code\nelse:\n    print 'Must provide an action of either add or remove'\n    sys.exit(1)\n\nEOF\n\n\n# HACK: occasionally the servers take a few seconds to propagate the changes\n# via Mongo :(\nsleep 10\n\n# Add the node to the server block.\npython setup.py --host $hostId --action add Perfect, now our host can accept traffic as part of our VPN. Disabling the source/destination check Lastly, we need to disable the source/destination check all EC2 instances in AWS start up with by default. # Lastly, we need to disable the source/dest check for this instance.\n# We need to do this for any nodes that need proxy network traffic that isn't\n# specifically for that node (i.e. VPN nodes and NAT nodes).\ninstance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\naws ec2 modify-instance-attribute --no-source-dest-check --instance-id=$instance_id --region=us-east-1 Et Voila! With those three steps you’ve got all you need to be able to setup an autoscaling group in AWS that can register new nodes as you need to scale up and scale down! In the near future, we’re also hoping to open source the Terraform module that we use for this at Mixmax so others can use it as well! Enjoy working on problems that you can't copy-paste a solution for? Drop us a line .", "date": "2018-03-27"},
{"website": "Mix-Max", "title": "Elasticsearch vs Redshift for Real-Time Ad-Hoc Analytics Queries", "author": ["Cameron Price-Austin"], "link": "https://www.mixmax.com/engineering/elasticsearch-vs-redshift/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on\nDecember 10th was about Understanding query performance in Mongo . The Mixmax Insights dashboard is like Google Analytics for your mailbox. How many messages did I\nsend? How many were opened? How many received replies? How do I compare to the rest of my team? Just like Google Analytics, it provides a number of predefined views, but also lets users create\ncustom reports based on their own filters and queries. Under the hood, we use Elasticsearch to aggregate the data for these reports from a fairly large\n(~2 billion) dataset of messages. We decided upon Elasticsearch after benchmarking a few candidate solutions, including AWS Redshift.\nAs part of the Mixmax Engineering Blog Advent Calendar , I’ll explain the\nbenchmarking process and results. Requirements Our solution needed to respond in real-time to ad-hoc, user-defined queries, returning aggregate\nstatistics on matching messages, grouped by one or more fields (sender, time etc). Ideally, it also needed to be capable of serving existing analytics elsewhere in our app, such as\nthe sequence reports and live feed, which at the time were served using an ugly system of\npre-cached fields in Mongo. It needed to operate at our current scale, house at least twelve months of historic data, and be\ncapable of growing with our customer base, continuing to serve our needs for at least the next\nyear. Candidates Mongo We use Mongo as our primary data store, so\nthe idea of using a familiar technology was attractive. We figured we might be able to use Mongo’s\naggregate feature across a single, large, heavily-denormalized collection of messages. Elasticsearch Like Mongo, Elasticsearch was already part of\nour stack. We’d been using it to serve our live feed and had already found it quite capable of\nproducing aggregated analytics in real-time (despite some early teething issues ). Redshift Redshift was, in our mind, the\ngold-standard of data warehousing and retrieval. Our team didn’t have any experience with it, but\nwe knew of other startups who employed it for similar use cases. Implementation Message data has some one-to-many relationships. For example, each message has one or more\nrecipients. In our system, it can also contain zero or more templates, and each user (and therefore\ntheir messages) can belong to zero or more teams. Mongo and Elasticsearch allow us to express these relationships on a single document (using Array\ntypes), but Redshift is relational, and required additional tables. Our final schema for Redshift\nwas: CREATE TYPE message_type AS ENUM (‘normal’, ‘sequence’, ‘template’);\nCREATE TABLE messages (\n  _id varchar(24) not null distkey primary key,\n  userId varchar(24) not null,\n  sent integer not null sortkey,\n  numRecipients smallint not null,\n  numOpened smallint not null,\n  numClicked smallint not null,\n  numDownloaded smallint not null,\n  numReplied smallint not null,\n  numBounced smallint not null,\n  firstOpen integer,\n  lastOpen integer,\n  firstClick integer,\n  lastClick integer,\n  firstDownload integer,\n  lastDownload integer,\n  firstReply integer,\n  lastReply integer\n);\nCREATE TABLE messages_teams (\n  messageId varchar(24) not null distkey encode lzo,\n  teamId varchar(24) sortkey not null encode lzo,\n  primary key (messageId, teamId),\n  foreign key (messageId) references messages(_id)\n);\nCREATE TABLE recipients (\n  messageId varchar(24) not null distkey,\n  email varchar(254) not null,\n  domain varchar(250) not null sortkey,\n  primary key (messageId, email),\n  foreign key (messageId) references messages(_id)\n);\nCREATE TABLE messages_templates (\n  messageId varchar(24) not null distkey encode lzo,\n  templateId varchar(24) sortkey not null encode lzo,\n  primary key (messageId, templateId),\n  foreign key (messageId) references messages(_id)\n); We distributed all tables on the message id. For messages, this essentially distributed rows\nacross nodes in a round-robin. For the FK tables, this ensured they were co-located on the same\nnode as their parent message. We also set sort keys (essentially cluster indexes) on the dimensions used for querying. Benchmark Process We set up each candidate system and populated it with 100m sample messages (based on real,\nanonymized user data) from 40k users We continued to stream in new messages and edits to existing messages (e.g. incrementing numClicks etc.) at a rate of 10 writes per second, simulating our approximate update load We executed a sustained series of pre-defined queries against each solution at a rate of 100 per\nsecond (roughly what we expected in production usage) We then averaged the response times The queries were: Query A — User’s own activity for specific month, grouped by day Query B — User’s own activity over entire year, grouped by week Query C — Team activity for specific month, grouped by user Query D — Team activity for specific month, grouped by recipient domain Query E — Team activity for specific month and domain, grouped by user Query F — Team activity for specific month, grouped by template Results Mongo needed to be excluded early on. It completely choked at this load profile, taking ~10 minutes\n(!) to return results. Elasticsearch and Redshift performed better: Average response time (ms) for each query. For Elasticsearch we used an elastic.co hosted 16GB, 2DC profile. Taking the speed average and plotting it as a function of estimated monthly cost: Conclusion Elasticsearch was the clear winner. It was the fastest (by far) and comparatively cheap. This was surprising. We don’t have much experience with Redshift, but it seems like each query\nsuffers from a startup penalty of ~1s (possibly Redshift analysing the query and splitting it\nbetween nodes?). It might be more suited as a solution for data scientists rather than as part of\nan application stack. We’ve now been using our Elasticsearch reporting cluster in production for about six months and\nperformance has been great (especially on Elasticsearch v5). We did need to upgrade to a 32GB\ninstance as we added additional indexes, but overall the performance-to-price ratio is still\nexcellent, and we’ve been happy with our choice. Notes In Redshift, we tried setting the message id as both the distkey and sortkey , so the query\noptimiser could perform merge joins, but this hurt performance instead of improving it We set primary and foreign keys, but these aren’t enforced in Redshift — it just uses them to\nimprove its query planner. Adding them didn’t noticeably improve performance. We performed encoding optimisation on all tables. Apart from the few explicit codings set in the\nFK tables, these were already optimal. We added numeric boolean columns like wasOpened (which is either 0 or 1 ). These were\nredundant given we have numOpens , but allowed us to do aggregate operations (such as SUM(wasOpened) ) without needing to use a case statement (e.g. SUM(CASE WHEN numOpened = 0 THEN 0 ELSE 1 END) ). This improved performance quite a bit. Have a knack for engineering solutions to software problems that prioritize user experience? We’re hiring !", "date": "2017-12-11"},
{"website": "Mix-Max", "title": "How to correctly specify default options in ES6", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/default-options-in-es6/", "abstract": "When writing modern JavaScript, it's common to collect multiple optional parameters\ninto a trailing \"options\" object. This allows clients to pass a subset of options\nand gives context to arguments at the call site. It also permits the API to\nuse ES6 default parameters. But it can be tricky to get default parameters right\nwith objects. Here's how. Options objects Once upon a time, if you wanted to dispatch a mouse event, you had to call an\nAPI that looked like this: function initMouseEvent(type, canBubble, cancelable, view,\n  detail, screenX, screenY, clientX, clientY,\n  ctrlKey, altKey, shiftKey, metaKey,\n  button, relatedTarget) {} There were sensible defaults for every parameter except type , but the API was\nnot able to express this. Specifying a value for relatedTarget meant specifying\nvalues for every other parameter, leading to insanely long, inscrutable function\ncalls like: let evt1 = document.createEvent('MouseEvents');\nevt.initMouseEvent('mouseup', true, true, window, 1, mouseX, mouseY, 0, 0, false, false, false, false, 0, el); This was fixed by collecting the optional parameters into a trailing object: function MouseEvent(type, mouseEventInit) {} With this sort of API, the client can specify a subset of options. And as a bonus,\nthe options are named at the call site, kind of like Python's named parameters: let evt2 = new MouseEvent('mouseup', { relatedTarget: el }); Default parameters Another downside of APIs like initMouseEvent is that they can't effectively use\nES6 default parameters . This is because parameters are set left-to-right, so\nmultiple default parameters are rarely practical: function f(x = 1, y = 2) {\n  return [x, y];\n}\n\n// Specifying neither x nor y:\nf();  // [1, 2]\n\n// Specifying x but not y\nf(2); // [2, 2];\n\n// But now there's no way to specify y and not x. With an options object, you can write function f(options = { x: 1, y: 2 }) {\n  return [options.x, options.y];\n} However, this API does not express that x and y are independent options.\nInstead, it forces x and y to be passed together: f(); // [1, 2]\nf({ x: 1 }) // [1, undefined] The problem is that options is defined wholesale. The way to fix this is to\nuse another ES6 feature, destructuring assignment , to describe the structure\nof options . You'll no longer be able to reference options per se, but it's often nicer to get\n\"direct\" access to each key, and then you can specify the defaults on a per-key basis: function f({ x = 1, y = 2 }) {\n  return [x, y];\n}\n\nf({ x: 1 }); // [1, 2] But there's one final fix to be made. The previous version of the function has\nforced the client to pass an object: f(); // Uncaught TypeError: Cannot destructure property `x` of 'undefined' or 'null'. The way to fix this is to default the options object to an empty object. function f({ x = 1, y = 2 } = {}) {\n  return [x, y];\n}\n\nf(); // [1, 2] Conclusion Default parameters are an awesome ES6 feature. However, the ability to specify objects as\ndefault values is a clear footgun . You should always\nspecify default options by destructuring, while making sure that the options object is defined by\ndefaulting to an empty object. Working at the cutting edge of JavaScript development means getting used to\nsome new syntax. If you'd like to join us, drop us a line at careers@mixmax.com .", "date": "2018-01-24"},
{"website": "Mix-Max", "title": "Troubleshooting `npm link`", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/troubleshooting-npm-link/", "abstract": "As of this writing, Mixmax runs 15 Node microservices. We keep this manageable by sharing a ton of\ncode between services, in the form of npm packages both public and private. To keep those packages manageable, we develop them in their own repositories. But this poses a\nchallenge for local development — how do we quickly test a new version of a package inside another? Luckily, npm provides a command called link , which symlinks one\npackage into the node_modules directory of another. But developing modern JavaScript modules is at\nleast two steps and often three: Transpile/bundle your JS (optional) Get that JS into the node_modules directory of your application Get your application to load the new JS and npm link only helps with step 2. Here’s how to get steps 1 and 3 working. This process can be frustrating and seem magical, so two\nprinciples before we start: If you don’t see code in your running application, it’s because it’s not there. :) This will show\nyou how to determine exactly what code require and import are loading and why it might not be\nwhat you’d expect. Avoid FUD! \"Always rebuild/restart after making a change\" is not necessary and if you blindly\nrely on doing that you will not only waste time, you may be disappointed. This article will help\nyou take the minimum number of steps to get your changes live. Troubleshooting server packages This problem takes the form “my changes aren't visible when I require('my-package') . Follow this\nprocess: Ask, what’s node_modules/my-package ? If it’s a regular folder vs. a symlink, npm link your\nlocal module. Ask, what does require('my-package') actually do? That is, check which file it will load, as\nspecified as pkg.main ; this could be a build\nartifact rather than source code (even for a server package, since some of our packages use Flow ). At Mixmax, artifacts are usually written to\na directory called ‘dist’. If the main file is such an artifact, build the package. At Mixmax,\nthis is usually done via a package script called “build”, so run npm run build . Many packages\nalso offer another script called “watch” that will automatically rebuild as you make changes. Restart the server to reload the code. See the “Restart less” section below for\nthe most efficient way of doing this. Sometimes packages ship server and browser modules, where the browser module is bundled and\ntranspiled and the server module is not. You only need to rebuild if the server uses\nbundled/transpiled JS, as specified by pkg.main : our convention is that bundled/transpiled files\nare written to a directory called dist . Troubleshooting client packages We use ES6 modules and Rollup client-side, so this problem takes the form “my changes aren't visible\nwhen I import foo from 'my-package'; Step 1 is the same as in \"Troubleshooting server packages”\nabove—make sure you’ve run npm link . Then: Ask, what does import foo from ‘my-package‘ actually do? Once again, check which file it will\nload. This will usually be pkg.main , but may\nbe pkg.browser instead if the package ships a different module for the client than the server. Either way, import almost certainly loads a build artifact, since all of our client-side packages are\nself-hosting (transpile themselves to ES5 as well as bundle themselves); so build the package. At\nMixmax, this is usually done via a package script called “build”, so run npm run build . Many\npackages also offer another script called “watch” that will automatically rebuild as you make\nchanges. Rebundle your client application to reload the code. See the “Restart less” section below for the\nmost efficient way of doing this. In some rare cases, import foo from ‘my-package’ may not actually import foo from node_modules , either because we haven’t yet configured Rollup to do so, or because we’ve told\nRollup to do something different for my-package for a special reason. But if my-package is in node_modules to begin with, you can generally trust that the project’s been configured to import\nit. A diagram for troubleshooting Restart less At Mixmax, we use Gulp to transpile/bundle each microservice’s JS as well as\nlaunch the web server. You only ever need to restart this Gulp process (at Mixmax, supervisorctl\nrestart … ) if you changed something in the Gulpfile. Don’t do this otherwise! Rebuilding the entire\napplication and starting the server can take many seconds. The build process uses file watchers to restart the server and rebuild the client if their source\ncode changes. But those watchers don’t monitor node_modules , for performance reasons and since we\ndon’t usually expect those files to change. This means that you have to manually reload the server or client when their dependencies change. But\nthis has an easy fix. To pick up your change to a Node module, you just need to “poke” the file\nwhere you’re importing that module, by doing one of the following: inserting a new line and saving running touch /path/to/file at the command line some editors like Sublime Text will touch the file if you save it (e.g. Cmd-S), even if there\nare no pending changes Updated 2018-07-26 : See the last section of this\npost for a way to do this automatically! Bonus: npm unlinking npm unlink foo , unfortunately, does not reinstall the “real” foo (though this is going to change! ). To fix this, unlink by\ndoing npm unlink ../path/to/package rather than npm unlink <package name> . You’ll once again need to poke the file where you import the module to get it to reload. How could this be made simpler? One answer might be to use a monorepo . That might require\nunifying all our packages’ build processes, whereas some of them use different sets of Babel and\nRollup plugins at the moment. We’d also have to do research into the most efficient use of file\nwatchers across the entire tree. And if we pulled our services into this monorepo as well, we’d have\nto refactor our CI/CD process to deploy individual directories rather than entire repos. Using one repo per package makes it pretty easy to reason about, test, document, and publish those\npackages—especially if we’re going to share them with the public, which is important to our open-source culture . So we have an incentive to\nfigure out how to optimize this npm link process. My feeling is that having to link and build individual packages is not that bad. You do each of\nthose steps once per change you have to make to a package, and many of our packages even have\n“watch” scripts to rebuild as you make changes. The thing to fix is having to manually reload the\nserver or client after rebuilding your package. We might do this by conditionalizing our application\nfile watchers to monitor node_modules/@mixmaxhq , since we make changes to our private packages\nmore often than we do our public ones. Updated 2018-07-26 : See the next section! [Updated 2018-07-26] Automatically reloading after rebuilding As discussed in the \"Restart less\" section, our application file watchers don't\nmonitor the contents of node_modules , and so won't automatically reload the client and/or server\nwhen you rebuild your linked package. Our previous solution for this was to touch an application\nfile after rebuilding the linked package, but we always had to do this manually. I recently saw a coworker rebuilding his linked package by running npm run build && touch\n/path/to/application/file . This let him rebuild and reload in a single command — but he still\nhad to run this manually after every change, whereas we can continuously rebuild (only) by running npm run watch (which delegates to rollup -cw under the hood). But seeing the commands in\nconjunction like that made me wonder — could we run the latter as part of the linked package's\nbuild process? A little digging revealed rollup-plugin-execute ,\na Rollup plugin that can execute shell command(s) sequentially after Rollup finishes bundling. By\nregistering that plugin with our build process like so // rollup.config.js\nimport _ from 'underscore';\nimport execute from 'rollup-plugin-execute';\n\nexport default {\n  input: 'src/index.jsx',\n  plugins: _.compact([\n    /* other plugins */\n\n    // Touch the specified path (if any) after the bundle is generated, to trigger file watchers.\n    // In practice: specify the path of the module that's importing this package. That way your\n    // application will automatically reload to pick up the new package!\n    process.env.TOUCH_PATH && execute(`touch ${process.env.TOUCH_PATH}`)\n  ]),\n  output: [{\n    format: 'es',\n    file: pkg['main']\n  }]\n}; We can now continuously rebuild and reload by doing env TOUCH_PATH=\"/path/to/application/file\" npm run watch . If you're interested in advancing the state of the art in open-source JS, drop us a line at careers@mixmax.com .", "date": "2018-07-23"},
{"website": "Mix-Max", "title": "To Yarn and Back (to npm) Again", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/to-yarn-and-back-again-npm/", "abstract": "Last year, we decided to move all of our JavaScript projects from npm to Yarn . We did so for two primary reasons: yarn install was 20x faster than npm install . npm install was taking upward of 20 minutes in many of our larger projects. Yarn's dependency locking was singificantly more reliable than npm's. Check out last year's blog post (linked above) for more details. 13 months with Yarn Yarn solved the annoying problems we faced using npm, but it came with issues of its own: Yarn has shipped very bad regressions , which made us afraid of upgrading. Yarn often produces yarn.lock files that are invalid when you run add , remove , or update . This results in engineers needing to perform tedious work to remove and add offending packages until Yarn figures out how to untangle itself so that yarn check passes. Frequently when engineers would run yarn after pulling down a project's latest changes, their yarn.lock files would become dirty due to Yarn making optimizations . Resolving this required engineers to make and push commits unrelated to their work. Yarn should perform these optimizations when commands update yarn.lock , not the next time yarn is run. yarn publish is unreliable (broken?) ( tracked issues #1 , tracked issue #2 ), which meant that we had to use npm publish to publish packages. It was easy to forget that we needed to use npm in this special case and accidentally publishing with Yarn resulted in published packages being un-installable. Unfortunately, none of these workflow-breaking issues were fixed during the 13 months we used Yarn. After a couple of especially painful weeks full of 15-minute Yarn untangling sessions, we decided to take a look at moving back to npm. npm 6 npm made significant improvements during the time we used Yarn in an attempt to catch up to Yarn's speed and locking - the issues that originally led us to Yarn. As annoying as our Yarn issues were, we couldn't afford to lose these benefits, so we first had to validate that npm had addressed our original issues. We decided to trial the latest version of npm available at the time, npm@​6.0.0 , since we wanted to take advantage of as many speed improvements and bug fixes as possible. npm​@6.0.0 was reportedly a relatively minor major update , so we figured that using it wouldn't be dangerously risky. Strangely, npm​@5.8.1 the version of npm we had tested prior to 6.0.0's release, failed to install dependencies on several of our engineers' machines (OS X Sierra/High Sierra, node​@v8.9.3 ) with various errors (eg cb() never called! ). Speed We were happy to find that in a trial of five samples per package manager, npm performed about the same as Yarn on average: Yarn: $ rm -rf node_modules && time yarn : 126s npm: $ rm -rf node_modules && time npm i : 132s A step in the right direction. Our investigation continued :). Locking npm introduced package-lock.json in npm@​5.0.0 - the npm-equivalent of Yarn's yarn.lock . npm shrinkwrap can still be used to create npm-shrinkwrap.json files, but the use case for these files is a bit different per npm's docs : The recommended use-case for npm-shrinkwrap.json is applications deployed through the publishing process on the registry: for example, daemons and command-line tools intended as global installs or devDependencies. It's strongly discouraged for library authors to publish this file, since that would prevent end users from having control over transitive dependency updates. package-lock.json files, on the other hand, are not published with packages . This is equivalent to how Yarn does not respect dependencies' yarn.lock files - the parent project manages its own dependencies and subdependencies (with the caveat that if libraries do publish npm-shrinkwrap.json files when they shouldn't, you'll be stuck using their dependencies). Locking validation npm doesn't have an equivalent to Yarn's yarn check , but it looks like some folks ( like Airbnb ) use npm ls >/dev/null to check for installation errors such as missing packages. Unfortunately that check counts peer dependency warnings as errors, which has prevented us from using it, since we often fulfill peer dependencies via CDN . npm recently introduced npm ci , which fortunately provides some validation. npm ci ensures that package-lock.json and package.json are in sync as one form of validation. It also provides some other benefits - check out the documentation for details. We never observed install inconsistencies when using npm previously (only Yarn seems to have these issues :)), so we figured that we would be safe using only npm ci . Yarn annoyances In addition to catching up to Yarn's speed and locking guarantees, it seemed that npm did not have any of the issues that had been bothering us with Yarn! Check, check, check npm​@6.0.0 checked all of the boxes for us, so we decided to move forward with it! After a successful 3-week trial in one of our services, we migrated the rest of our services and projects! Recommendations deyarn We've published an open-source module called deyarn to help you convert your projects from Yarn to npm! Using engines to enforce npm use We recommend using the engines option to help yourself avoid accidentally using Yarn when you want to use npm. We've added a configuration like: {\n  \"engines\": {\n    \"yarn\": \"NO LONGER USED - Please use npm\"\n  }\n} to all of the package.json s in our internal projects. deyarn (linked above) takes care of this for you :). Try it out! We tested that this flow would work for our needs and we suggest you do too. If you need the absolute\nfastest package manager, then you may still find Yarn to be best .\nBut if you're looking to simplify your setup, we've found that npm 6 recaptures a critical balance\nbetween speed and reliability. Want to help us build the future of communication using npm?", "date": "2018-05-29"},
{"website": "Mix-Max", "title": "Choosing (and using) Javascript static typing", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/flow-vs-typescript/", "abstract": "Overview We love Javascript at Mixmax, and some specific attributes we love are its flexibility and speed of development. This helps us ship and iterate on new features quickly. However, one language feature that quickly shifts from improving to impeding developer speed is Javascript’s lack of a type system. What starts as not having to worry about types and function signatures quickly turns into confusion and bugs. That’s where a static type system comes to the rescue, improving developer confidence and mitigating type errors. When does static typing help? For a while, descriptive variable names, unit tests, and solid documentation can provide enough type information to passably avoid adopting a static type checking system, but eventually, more complex functions and a larger codebase strain. At Mixmax, we used common code styles (both linting and best practices) and documentation formats (we like JSDoc best) to make code clear and readable. As an example, a function definition might look something like this: /**\n * Retrieve a contact related to the given email\n *\n * @param      {String}  email\n * @return     {Promise<Object>}\n */\nasync findByEmail(email) {\n  /* finding one... */\n} For a function with one argument and a simple return value, this works great. It’s clear what the function requires and returns. However, a few versions later, with more requirements, the same function has added a bit of complexity and now looks like this: /**\n * @param      {String}        email\n * @param      {Object}        [query]\n * @param      {Array<String>} [fields]\n * @param      {Object}        [options]\n *   @prop      {Number}       [options.limit]\n *   @prop      {String}       [options.paginatedField]\n *   @prop      {String}       [options.next]\n *   @prop      {String}       [options.previous]\n *\n * @return     {Promise<Object>}\n *   @property   {Array<Object>}  results\n *   @property   {String}         next\n *   @property   {String}         previous\n */\nfindByEmail(email, query, fields, options = {}) {\n  /* find by email… */\n} The documentation has quickly gotten out of hand with three optional values, multiple nested attributes and unclear return values (what does the return object have if there are no results?). It would be easy to call this function and forget the order of arguments and required properties which changes the behavior of the rest of the code. This is where static type checking comes in handy. Static type checkers at a high level go through your code and warn of invalid function calls, object descriptions, and other mistakes that wouldn’t be caught with just linting. Choosing a type system Of course, we’re not the first to run into these problems with an untyped language, so there are already a few well-supported options for adding a types to existing Javascript projects. Two options that are widely used and well supported are Flow and TypeScript . Though the end goals of Flow and TypeScript are similar (making sustainable, large Javascript projects), they each accomplish their goals very differently. To evaluate the differences between Flow and TypeScript, we had 3 questions on which to evaluate them: How can we incrementally adopt them (and revert if necessary)? How do they perform on hundreds of files in one project? How will they change how we write Javascript? Incremental Adoption As a small ( but growing ) engineering team, it’s important that whichever system we choose provides immediate benefits to productivity and reliability rather than requiring a month-long rewrite or a steep learning curve. Luckily, both Flow and TypeScript support file-by-file transitions, so neither would’ve forced us to rewrite entire projects at once (whew). Additionally, we wanted to be able to easily switch back from either TypeScript or Flow to vanilla Javascript. This way, if we started using one type system and found something different or better we wouldn’t be locked in by the choice. Here there’s a little difference between the two. Switching away from Flow would be easy, since it’s just transpiled using a babel preset . The results of this transpilation maintain comments, spacing, and other nice-to-haves, just without Flow’s type annotations. As a result, switching from Flow would just be a matter of renaming directories and removing Flow-enabled files. On the other hand, switching from TypeScript wouldn’t be such a breeze, as TypeScript is a different language that’s compiled (rather than transpiled) to Javascript. As a result, the .js files created by TypeScript’s compiler lacked much resemblance to the original code (removing spacing, comments, etc.). This meant a high barrier to switching back, and switching required some other tool or manually migrating. Performance Prior to using either Flow or TypeScript, it was important for us as a team to understand the impact our choice would have on our build times and in our editors. Primarily, we wanted to ensure that adding a type system to our build process wouldn’t bloat build times and that it wouldn’t slow down local development constantly waiting for slow compilation (or transpilation). While there aren’t a ton of benchmarks available, there’s some evidence that Flow increases compilation time for every file added and TypeScript does not. Though, in practice, there is no performance difference between the two. Additionally, given that both frameworks are used successfully in large projects, we’re willing to call this one a draw. One thing we noted is that both TypeScript and Flow offer options to only check changed files which is great for keeping development snappy. Project Philosophy Up to this point, Flow slightly edged out TypeScript for our purposes, offering less risk of adoption it. But our final question, “how will this change how we write Javascript,” proved the most influential in our decision. To contrast them directly, TypeScript on one hand is a fundamentally different way of writing Javascript, adding lots of syntactic sugar and typing features, to the point where it no longer has the same feel as the Javascript we usually write. The additional features can be nice, sure, but are orthogonal to the problem that led us to typing in the first place. On the other hand, Flow fit nicely into our existing methods, conventions, and best practices. We didn’t have to learn completely new syntax for the benefits of a type system to be immediately apparent. It also let us be as type safe as we wanted. In testing, we found that Flow provided more guarantees without requiring extremely detailed definitions, while TypeScript generally lagged behind in that regard. Finally, and most importantly, Flow is built to optimize type-checking, while TypeScript is a different flavor of Javascript that happens to have types. Exemplifying this is Flow’s type inference that immediately provides more feedback on unsafe portions of code. While TypeScript also provided immediate type checking, it checked and found fewer unsafe uses in general. In the end, Flow’s drop-in benefits, focus on type-checking, and low transition cost convinced us to start using it throughout our code base. After 4 months, it’s serving us well and continuing to see widespread use throughout our npm packages! If you want to write some type-safe Javascript, come join us at Mixmax ! We’re hiring!", "date": "2018-05-15"},
{"website": "Mix-Max", "title": "Mixmax hosts August SFNode Meetup", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/sf-nodejs-meetup-aug-18/", "abstract": "Mixmax was proud to host SFNode again at our office this past Thursday. This is the second time we've hosted SFNode. We truly enjoy bringing together the community of Javascript developers in San Francisco to learn together. Plus, our office is pretty great, so we hear ;). Joel Lord from Auth0 presented on using Password Managers, and engineers can build better forms inputs to enable them. Erick Wendel presented on managing asynchronous operations in Javascript using async/await and Promises. Interested in bringing together the developer community? Join us !", "date": "2018-08-06"},
{"website": "Mix-Max", "title": "The Effective Engineering Demo", "author": ["Logan Davis"], "link": "https://www.mixmax.com/engineering/how-to-give-an-effective-demo/", "abstract": "At Mixmax, our engineering team strives for a bottoms-up culture of product ownership. This means that anyone on the team, not just the founders or our product leads, can contribute their great ideas! We believe that all of us is smarter than some of us. ;) One of our strongest mechanisms for bottoms-up product ownership is our weekly Demo Hour. Every Thursday at 2:00 PM Pacific (when all of our remote employees are online at the same time :P ), we all hop on a Google Hangout to demo the cool things we’ve been working on. Things an engineer might show off at Demo Hour include a recent feature release, an impactful UX tweak, or a “cool idea” prototype. Many of our bestselling features, from our rules engine to our recommended send times , began life in this way - as prototypes and product pitches. But of course, anyone who’s pitched anything will tell you that having something great is only half the battle: you also have to communicate what it is that you’ve got. To that end, this blog post will give you a few of my learnings from my first year at Mixmax on how to give an effective engineering demo! Choose What To Demo Early in my Mixmax career, I rewrote a big legacy view into React. At demo hour, I hopped on the Google Hangout, shared my screen, and clicked around in the newly rewritten view. “Look!” I said. “You can click here, and it’ll make a folder, and here will add recipients to the sequence…” After thirty seconds of this, I trailed off. “And, uh, that’s all exactly the same as what you could do before. But it’s all completely rewritten in React now!” Everyone laughed, said “good job”, and we moved on. But what I’d realized thirty seconds into my demo was that this work was not very demo-able. Not all work is! And that’s OK - some of the most important work an engineer can do isn’t a flashy new feature. But when showing something off to the whole team, it’s important to consider what makes a good demo . Some rules of thumb: new features make great demos, while bugfixes generally don’t. Frontend work tends to be more impactful to non-technical employees than infrastructure improvements. Lastly, don’t underestimate the value of tiny UX improvements. The best reaction I’ve ever seen at demo hour was for a three-liner that an engineer coded up to make it easier to copy an email address - but it resolved a huge pain point for everyone who was using our product. Invest in Impact Here’s a secret about internal prototypes: your demo doesn’t actually have to work. “Wait, what?” you may be asking. “How can you demo something that doesn’t work?” Let’s return for a moment to the purpose of a prototype demo. The purpose of a demo is to convince people of the potential for your cool idea. But building really impactful features is hard! If we insisted that our demo prototypes be ready to ship before we demo them, we’d never spend any time working on actual features. Sure, we want to show off our cool ideas, but we also don’t want to invest too much time into building something that the team may decide we don’t want to ship. How do we balance our time while showing off big, blue-sky ideas? The solution is to invest wisely when building prototypes. Instead of focusing on internals, invest your effort in the parts of the demo that you think will be impactful to your audience . Practically speaking, this typically means that you should focus on visual ideas. Many demos at Mixmax consist of mostly frontend work with mocked out backends and internals. Don’t be afraid to hardcode data or stub out API endpoints, either! Tell the Story of What You Made... The goal of a demo is to engage your audience, and convince them that what you built is awesome and useful. Humans naturally engage with other humans - not with buttons. So, turn your demo into a story . Let’s say you built a new feature that lets users send messages instantly when they complete a Salesforce task. Which presentation would you find more engaging? I made this new view in Mixmax where you can type in a subject and a body. Later on, when you complete a task in Salesforce, Mixmax will send that message to someone. OR Let’s say I’m Kendall. Kendall is a salesperson, who uses Salesforce tasks to log calls that she completes with clients. She wants to send follow-up messages after her calls. Today, this means switching over to her inbox to manually type out a message to the person she just called. But with this new feature I built, the instant she logs the call, her message is sent for her! ...Not of How You Made It A really common pitfall when presenting your work to other people is overexplaining how you did it. Try to avoid this! Remember, your audience is not you . For you, the most memorable part of the project may be the really hard technical problem that you solved. But for your audience - especially non-technical folks - “how hard the engineering was” isn’t very interesting. People on your team want to know things like what value your work delivers to users and how it can be effectively pitched, not how many lines of code it was. Of course, you can always leave time for folks to ask you how things work afterwards! Prepare for the Worst There are three inevitable things in life: death, taxes, and your live tech demo not working. Of course, you should absolutely practice your demo exhaustively! Every time you show something off, you’re investing your teammates’ time in the presentation, and presenting without practicing isn’t often a good investment. But all the practice in the world won’t make your demo bulletproof. Any number of things can go wrong, from outages to unexpected corner cases to the internet cutting out. This is why you need to prepare for your demo to fail . How can you do this? When practicing your presentation, you might leave some test data around to show what things would look like if they worked. Or you might keep design mocks open in another tab. Just don’t try to fix your code live - if something goes wrong, instead of investing your team’s time in watching you try to live-debug, fall back to your pre-planned backup. And keep in mind: if things really aren’t working, there’s always another demo! Conclusion Want to have product impact on your team and keep everyone in the loop? You too can give a great demo! Just remember: choose your demo wisely, built it efficiently, tell a story about what you made (not how you made it), and be ready for when it inevitably breaks. Or, if you really want to learn to give great demos, come join us at Mixmax! We’re hiring!", "date": "2018-06-18"},
{"website": "Mix-Max", "title": "Writing regression tests is (not) hard", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/writing-regression-tests/", "abstract": "Setting the stage Imagine for a moment you work on an email analytics, automation, and enhancement platform, and a customer writes in about a bug they’re experiencing. We know the bug is in a long, complex function: async function complexFunction(value) {\n  if (!value) return;\n  if (!value.name) throw new Error('Name is required for `complexFunction`.');\n  const count = await db.values.count({ name: value.name });\n  let newValue = value;\n  if (count <= value.length) newValue = foo();\n  return newValue;\n} Fixing this bug involved a small change to fix an off-by-one error: - if (count <= value.length) newValue = foo();\n+ if (count < value.length) newValue = foo(); Great! We fixed the bug, and you can move on to the next task after doing your due diligence and updating the unit tests for this function to accommodate the change in logic. However, it turns out that the function doesn’t have any unit tests — and worse, it’s a long function that relies on external libraries and a database connection. You know you should write a test… but it looks like a lot of work, and what are the chances this will happen again? (Spoiler alert: it will happen again) This situation above comes up a lot, and writing tests can seem time-consuming and intimidating, especially in large or complex codebases. At Mixmax, we care about testing! It contributes to reliable, maintainable code and makes our lives as developers easier in the long run. Writing those first tests in a particularly difficult piece of code makes you a hero next time something breaks and the test catches it in CI rather than hearing it from customer reports. So, in the interest of making writing tests easier, here are some strategies we use at Mixmax! Create a test context Existing code often relies on lots of external dependencies, data, functions, or even entire services. Whichever of those your code relies on, it’s helpful to create helper functions to setup and teardown external state so you and fellow developers aren’t stuck writing the same or similar boilerplate for each test. This external setup is often called the testing context since it’s the state all tests expect. This is an example of what a test context helper function that sets up a database connection looks like: const mongoist = require('mongoist');\nconst { MongoMemoryServer } = require('mongodb-memory-server');\nasync function start() {\n  const mongod = new MongoMemoryServer({\n    binary: { version: '3.4.6' }\n  });\n  const uri = await mongod.getConnectionString();\n  // The mongo client.\n  const db = MongoClient(uri);\n  return { db, mongod };\n}\nfunction stop(mongod) {\n  return mongod.stop();\n}\nmodule.exports = { start, stop }; Here, start sets up a mongo server unique to the test context so other tests don’t modify the database during testing. Usually there’s one context per test file. Now, you can use the helper when setting up your new unit test: const { start, stop } = require('../../helpers/context');\ndescribe('testFunction', () => {\n  let t;\n  beforeAll(async () => {\n    t = await start();\n  });\n  afterAll(() => stop(t.mongod));\n  // it('TODO: write a test', () => {});\n}); Now it’s time to write an actual test. But where to begin? Test early returns first It’s helpful to write functions that return early rather than having nested\nconditionals for clarity and readability. However, the first test that usually\ncomes to mind is one that runs the entire function, ending with the last line,\nbut these are also likely the most difficult tests to write since they exercise\nthe most code. They also require understanding all the prerequisites and side\neffects of the function – no small task for large, existing code bases. Instead,\nit’s often easier to test places that return early or throw exceptions first,\nleaving a complete run of the function for the end. In our complexFunction it’s far easier to test the first two conditionals than the final one: async function complexFunction(value) {\n  if (!value) return;\n  if (!value.name) throw new Error('Name is required for `complexFunction`.');\n  const count = await db.values.count({ name: value.name });\n  let newValue = value;\n  if (count < value.length) newValue = foo();\n  return newValue;\n} Adding two tests for an undefined value and catching an exception requires no extra setup and already covers one third of the statements in complexFunction : it('should return null when passed an empty value', async () => {\n  expect(await complexFunction()).toBe(undefined);\n});\nit('should throw when name is undefined', async () => {\n  await expect(complexFunction({})).rejects.toThrow('Name is required for `complexFunction');\n}); Now that the easy tests are taken care of, it’s time to use a few other techniques for the rest of the function. Taking advantage of mocking In testing, mocking usually refers to fake implementation or data that’s set\nup specifically for one or more test cases. While there are many different\nlibraries for, and types of mocks, we’ll focus primarily on mocking data and functions . To finish testing complexFunction we’ll want to mock the\ndata in db.values as well as what foo() returns. To mock data, add expected values before running your test and ensure that\nthe values are unique to your test itself to prevent conflicts with existing\nor future tests. it('should return the original value when count is >= length', async () => {\n  const name = Math.random();\n  await t.db.values.insert([{ name }, { name }]);\n  const value = { name, length: 3 };\n  const newValue = await complexFunction(value);\n  expect(value.name).toEqual(newValue.name);\n  expect(value.length).toEqual(newValue.length);\n}); Taking this incremental approach, tests cover all but one statement in the\nfunction and the remaining bit of code exercises the updated logic. Testing\nthe final line, ideally involves understanding foo and ensuring it runs\ncorrectly, returning expected values. However, to ease writing these tests,\nmocking the function (using jest’s builtin mocks or another mocking library ) should suffice. We can\nadd mocking to a hypothetical function foo by making a few modifications to the\ncontext start function: // Import the required module\nconst module = require('./module');\n// Replace `foo`'s implementation with a mock function\nmodule.foo = jest.fn();\n// Add foo to the returned values\nreturn { db, mongod, foo: module.foo }; Interested in writing testable code? Join us at Mixmax , we’re hiring!", "date": "2018-11-05"},
{"website": "Mix-Max", "title": "Who doesn't love videos?", "author": ["Gaston Sanchez"], "link": "https://www.mixmax.com/engineering/your-vidyard-video/", "abstract": "Video in email works. It can 5x your email conversion rate. That’s why we’re excited to announce our new Vidyard integration! Below’s an example of Katie from our sales team embedding a video. As you can see above Vidyard and Mixmax work well together because we help you humanize your message and create a connection with your customers, prospects or candidates. To send a video, just type /video or select ‘video’ in the Enhance menu! But wait, there’s more! We’ve recently added Vidyard to our workflow automation. Now you can create ‘if-then’ workflow triggers based on when a video is viewed. For example, when a video is started, then create a call task in your CRM. The sky’s the limit - you can create whatever workflows you need to make your day more effective. Don’t miss this great deal for Mixmax customers - purchase 5 seats of GoVideo Professional at a 50% discount! The list price for 5 seats is $6000 USD and is offered to Mixmax customers at $3000 USD. GoVideo Pro offer unlimited video creation, storage and sending capability. All purchases include set-up, onboarding and training. Offer expires October 31, 2018 To redeem the offer please email GoVideoPro@Vidyard.com and include ‘Mixmax + GoVideoPro + Your Company Name’ in the subject line.", "date": "2018-09-04"},
{"website": "Mix-Max", "title": "Progressive UX design for engineers", "author": ["Logan Davis"], "link": "https://www.mixmax.com/engineering/progressive-ux-design-for-engineers/", "abstract": "At Mixmax, our job is to give our users half of their day back. We want to empower people to focus on the real interpersonal work of communication by designing, automating, and integrating away “toil”, i.e. mindless drudgery. We don’t want our users to spend their lives: Scheduling meetings via sixty-two back-and-forth emails Updating the status of every contact and lead in Salesforce by hand Copy-pasting the same content into every recruiting message So we built a product that makes all of that stuff “just work”, and lets users focus on the parts of their jobs that they actually have to think about. But for engineers on the Mixmax team, the responsibility to respect our users’ time extends beyond adding cool features and into our implementation decisions. Even more than most companies, we have an obligation not to make users wait for our product to load, process, or respond. With that in mind, here are five best practices for building snappy, respectful web applications. Optimistic rendering Optimistic rendering is the design pattern of behaving as though an action were successful before that action has actually been completed. In web apps, this most commonly means updating a view based on the assumption that an issued network request will return 200 OK. For example, when a user creates a sequence, template, or rule in Mixmax, we instantiate a client-side model representing the asset and render the editing UI before making a single request to our servers, based on the assumption that the POST request to create the model will succeed. Why render optimistically? Optimistic rendering is the only way to divorce your web application’s performance from network conditions. In a local development environment, when your server-side code is running on your own machine, it’s easy to forget how waiting for a network request can induce sluggishness. Users in Australia are not likely to be so forgiving. At Mixmax, we’re a glass half full kind of engineering team -- we’ve solved many a latency issue by simply being optimistic about requests, instead of blocking rendering or navigation. That said, as the old Russian proverb goes: “trust, but verify”. It’s important when rendering optimistically to handle the failure case, updating your UI to match reality when something goes wrong. Luckily, promises provide a natural affordance for this: the catch() handler. To put it in a meme: A lot of folks are wary about optimistic rendering because failure cases can be difficult to handle (e.g. if the user has made edits since asset creation failed, or closed their browser assuming success). We’d argue that it’s unwise to generalize practices based on such edge cases. What is the failure rate of a given action? How much better is the UX in the success case if you render optimistically? When you consider these tradeoffs carefully, you’ll often discover that a slightly surprising experience for .01% of requests is worth instantaneous UI response in the 99.99% case. Judicious use of loading spinners Of course, you can’t always render optimistically. Consider what happens when you have to load data for a dashboard, or search for a record in Elasticsearch . In such cases, the client can’t render content before it actually gets the underlying data! (More generally, a loading state is almost always necessary for GET requests, but avoidable for POSTs, PUTs, or DELETEs.) When you need data before you can render, showing a loading spinner is better than not showing anything at all. The overriding principle here is that your UX should always immediately respond to user input in some way. When a user clicks a Mixmax meeting template, they don’t expect to have to wait a second before anything happens -- so we show a loading spinner until their data is ready to render. If you want to get really cute, you can do better than a literal loading spinner. Sometimes it makes sense to bootstrap data for detail views in the first GET request for a page, even if that data isn’t visible in the page’s default state. We do this in our Tasks dashboard to load task details faster, trading off a bit of loading time on initial page render for instant opening of every task’s edit view. You can also go in the other direction and load no data on the initial render. Companies with lots of resources, like Slack and Facebook, render “ fake UX ” while loading data, simulating the shapes and colors that their content would fill. This can be costly in terms of designers’ time, but rendering these templates server-side and sending them to users before doing anything slow (like fetching from a database) can really increase users’ perception of your app’s speed. Of course, even with the slickest loading state in the universe, your users will notice if that first render takes ten seconds to pop in. In fact, 53% of mobile users will abandon your page if it takes more than three seconds to load! This is where backend performance comes in. At Mixmax, we measure latency on all of our API endpoints. If the p90 latency on an endpoint breaches a set threshold, our alerting system automatically files a JIRA ticket to improve the performance of that endpoint. This is also a great way to motivate yourself to clear out tech debt! Simulating bad network conditions One of the most important skills of a product manager or designer is to have empathy for the user. Many companies, understanding this, encourage employees to “dogfood” their product by using it in their own day to day operations. Dogfooding done well helps to identify high-value, low-cost bugfixes and features. But much dogfooding is not done well, and instead serves to build a false sense of security. The crux of the problem: just using your product isn’t enough to accurately simulate the user experience! To identify and prioritize issues, you must simulate your user’s actual operating conditions. Do you have a brand new laptop and high-speed office internet? Your users don’t. Maybe you use your own product every day, but have you tried using it… On an airplane? On a phone? On a five-year-old laptop? If not, you’re probably deluding yourself, as far as your app’s actual performance goes. At Mixmax, before we release any feature, we test it in Slow 3G with throttled CPU. In this, as in many other things, the Chrome dev tools are your friend: Predictable interfaces OK, so your app is fast. You optimistically render where you can, show loading states quickly where you can’t, and do this all in bad network conditions on old machines. Awesome! But be careful -- all of the goodwill that this speed has earned you can be lost if you’ve sacrificed UI predictability in the name of speed. In UI design, though in no other respects, we at Mixmax strive to emulate the Weeping Angels, a recurring villain from recent seasons of Doctor Who. The Weeping Angels are a species of alien assassins that (as an evolved defense mechanism) turn to stone when they’re observed. Like a Weeping Angel, when your UI is being interacted with, it should not suddenly attack the user! We’ve all used UIs that get this wrong: The Java updater that interrupts whatever you’re doing to ask you to approve a download That one pop-up ad that says you’ve won a new iPhone Those social icons on your phone that pop a new row of contacts in under your thumb right before that thumb hits the screen to share a photo with your significant other. every single time If you have an alert to display, display it in direct response to a user action. If you’re optimistically rendering, and the user has moved on to some other action, try not to pop up a “save failed” notification while the user is editing some unrelated text field! Challenge yourself to find a way to show failure without focusing an element in the DOM, e.g. by adding some notification text to the page instead of popping up an alert. Your goal is to avoid disrupting the user’s flow state, and to behave in ways that they expect. Semantically correct behavior No list of best practices would be complete without a mention of HTML and HTTP semantics. For instance, what does your API return in response to PATCH requests? Technically it’s not in the RFC , but some frontend model layers (ahem, Backbone ) are quite insistent that APIs should only return modified fields, and do bad things when you break this expectation (like resetting any changes that the user made while the request was in-flight!) In order to get autosaving Backbone models right at Mixmax, we had to be careful to abort any in-flight HTTP requests before making new ones. And of course, there’s accessibility to think about! This topic deserves its own whole book, but: have you tried using your app with a screen reader recently? Do your images have alt tags? This is one area of our product at Mixmax that we can definitely improve upon -- certain user interfaces, like flyouts that open when hovered, can be very difficult to use for those of us who don’t interact with the web visually. As a general principle, remember that best practices are usually best practices for a reason -- and even if they aren’t, libraries and tools will often assume that they are, and cause you all kinds of trouble if you don’t follow them. So save yourself the headache and stay compliant! Conclusion We all only have so much time in this world. Who wants to spend theirs waiting for an email to send? Or reading the padded-out conclusion of a blog post? Interested in building lightning-fast interfaces? Join us at Mixmax ; we’re hiring!", "date": "2019-02-13"},
{"website": "Mix-Max", "title": "Watch Brad and Olof demo Mixmax at Meteor Devshop", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/watch-brad-and-olof-demo-mixmax-at-meteor-devshop/", "abstract": "Check out Olof and I presenting Mixmax at Meteor Devshop in San Francisco on December 4, 2014.", "date": "2014-12-11"},
{"website": "Mix-Max", "title": "Mixmax hosts NodeSchool", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/mixmax-hosts-nodeschool/", "abstract": "Mixmax was proud to host NodeSchool again at our office -- for the 6th time! NodeSchool SF is a monthly meetup dedicated to helping people learn JavaScript. We help people of all skill levels (from beginners to experts!) discover what's possible with JavaScript. Our March event is happening now! Thanks to @Mixmax for hosting us again! ♥️✌️ pic.twitter.com/XMfWFaHNf8 — NodeSchool SF (@nodeschoolsf) March 30, 2019 We'll be hosting again soon, so follow @NodeSchool on Twitter. We hope to see you here! Interested in bringing together the developer community? Join us !", "date": "2019-04-02"},
{"website": "Mix-Max", "title": "Create a Mixmax slash command using Webtask.io", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/create-a-mixmax-slash-command-using-webtask-io/", "abstract": "Out of the box, Mixmax is a powerful tool for making your communication more powerful, compelling, and efficient. Did you know that it’s also an extensible platform that a developer can quickly and easily customize to fit their organization’s specific needs? One of the ways Mixmax can be extended is through the creation of “slash commands”. These are commands that start with a slash character ( / ) and allow you to insert rich content into your message without ever taking your hands off the keyboard. ♥ Webtask.io , like its name implies, is a great platform for hosting web-accessible code that performs simple online tasks on demand. It’s designed to provide a developer–even a novice!–with an environment and tools to get their code up and running on the web with minimal effort. In this tutorial–which is designed to be accessible to anyone regardless of developer skill level or familiarity with Mixmax–I’m going to show you a quick and simple way to use Webtask.io to provide the interactive backend that powers a Mixmax slash command. In order to complete this tutorial, you need to have a Mixmax account connected to your Gmail. If you haven’t signed up for a Mixmax account yet, go do so ! What makes a slash command A Mixmax slash command has three key parts: The command itself that you’ll type to start creating your rich content, A way to provide suggestions for the inserted content, and A way to resolve the command and insert the finalized content into the email. In case you’re unfamiliar with Mixmax slash commands, let’s try one out. In your Gmail account with Mixmax connected, click the “Compose” button to start writing a new email, and in the email body, type a slash: / . You’ll immediately see a popup with a list of available slash commands, each of which can insert HTML content into your message to make it more powerful, interesting, funny, interactive, or relevant. Try typing /image toast and you’ll see the suggested images. Select one, either by scrolling with the up and down arrow keys and pressing return or by clicking with your mouse, and watch it be inserted automatically. In this example, the slash command is /image , the suggestions are the popup list of images matching the search term “toast”, and the resolution of the command is what happens when you select an image and it gets inserted into your email. To create our own slash command, we’ll be handling all three of these. Getting started The first step is to tell Mixmax that you’re creating a new slash command. To do this, click the Mixmax button in the Gmail header to launch the Mixmax Dashboard . Then, in the sidebar of the Mixmax Dashboard, click “Settings”, and finally within the Settings page, click “Developer”. (Don’t worry if your menus don’t look exactly like this; different Mixmax plans have access to different features.) You’ll end up at the Developer Settings page. At the bottom of the list of built-in slash commands, click the “Add Slash Command” button to display the form where you’ll set up your slash command. The slash command we’re going to build will allow you to dynamically generate a QR code and insert it into your email, so that people who are viewing the email on their laptop or desktop can quickly access the QR code’s content (which could be a URL, for example) on their mobile device. In the “Name” box, type “Generate QR Code”, in the “Command” box, type “qr”, and in the “Parameter placeholder” box, type “[URL or other text to encode]”. With these values filled out, we’re ready to start making our command work by building the two API endpoints necessary. Suggestions API vs. Resolver API What’s the difference between the suggestions API and the resolver API? Remember, the idea behind a slash command is to help the user get something done quickly that makes their communication better. By providing a suggestions API as part of a slash command, we can immediately start giving the user options to insert content as soon as they provide us with some input, and Mixmax will show those options in a popup for easy access. However, those suggestions don’t necessarily contain every bit of rich content that the end result will, which is why a slash command uses a resolver API to flesh things out. When the user selects one of the suggestions, that data is passed to the resolver which can add even more data before creating the resulting HTML that will be added to the email. Getting set up with Webtask.io To build our endpoints, we’re going to use Webtask.io, which lets us write code right within our browser (or on the command line, if you prefer), and then run that code by loading the URL provided to us. In a new tab, head over to Webtask.io, click “ Try it now! ”, and sign in with one of your existing accounts. Within moments, you’ll be looking at an empty code editor, ready for you to start writing. Setting up your suggestions endpoint A slash command’s suggestions endpoint needs to do three things: Accept the text that the user has typed after the slash command, Generate a list of possible values that correspond to that text, and Deliver that list with some metadata back to the slash command. When you type some text after a slash command in a Mixmax message compose window, it sends that text in the query string of a `GET` request to the suggestions API URL for that slash command. It also includes some other information about the user making the request: their email address and what Mixmax thinks their timezone is, in case that’s helpful for your command. That means the data will be sent as something like example.com/suggestions?user=you%40gmail.com&timezone=America%2FNew_York&text=your+text+here . The only bit of that we need is the text . In Webtask.io parlance, we’ll receive the text from the context object that is automatically passed as an argument to our function, and use the callback function also provided to deliver it as our response. The rest is up to us. Let’s get started creating the endpoint for our slash command’s suggestions API. Click “Create a new one” in the middle of the screen, then hover over “Webtask Function” and click “Create empty”. Name your new webtask function qr-suggestions and click “Save”. You’ll be presented with an empty code stub. Delete all of the existing code and replace it with the following: module.exports = function (context, callback) {\n    callback(null, [\n      {\n        title: \"Type a URL or other text and press return.\",\n        text: context.query.text\n      }\n    ]);\n  } The slash command’s suggestions API is supposed to return a list of suggested possibilities for the output of the command. However, a QR code is a QR code, so a list of suggestions doesn’t really make much sense in this context. Instead, our suggestions API just tells the user to go ahead and type their content and press return. When they do so, Mixmax will send their text to the resolver API endpoint, which is where the real work is done. Click the save icon in the top right (or press ⌘-S or Ctrl-S). Then, at the bottom of the screen, you’ll notice a URL that ends with qr-suggestions . This is your webtask’s API URL. Copy it, head back to the Mixmax Developer Settings tab that you left open, and paste that URL into the “Command Parameter Suggestions API URL” box. Only one piece left! Setting up your resolver endpoint Back in the Webtask.io editor, we need to create another webtask function to be our command resolver. Click the “Create new” button in the sidebar at the far left, then again hover over “Webtask function” and click “Create empty”. Name this function qr-resolver . In order to generate the QR code image that we’re going to return when we resolve our command, we’ll need to use an existing service. GoQR.me generously offers a free, open API for doing just this. Even better, their API requires no setup or authentication–all we have to do is generate the right URLs and they’ll give us a QR code PNG image in return. Once again, delete the default code in the editor and replace it with the following: module.exports = function (context, callback) {\n    callback(null, {\n      body: `<img src=\"https://api.qrserver.com/v1/create-qr-code/?size=100x100&qzone=1&data=${encodeURIComponent(context.query.text)}\">`\n    });\n  } In a nutshell, we’re taking the text that the user sends, and resolving it to an HTML img tag with the URL of a generated QR code with that text encoded. Using encodeURIComponent ensures that no matter what text the user types, we’ll create a valid URL, so the img tag won’t fail to display. Save this code, copy the URL from the bottom of the page, and in your Mixmax Developer Settings, paste it into the “Command Parameter Resolver API URL” box. At this point, you can click “Add Slash Command” and see that “Generate QR Code” has appeared in your list of slash commands. Trying it out Head back to Gmail, and in your Mixmax message compose window, type /qr . You’ll see that the command is now available from within your email message. Type some text, like “ https://mixmax.com ”, and you’ll see the output from your suggestions API. When you press return, your resolver API will take that text and send back the HTML necessary to render a QR code right in your email. Using a barcode scanner app on your phone, give it a try! The sky’s the limit Now that you’ve created your first Mixmax slash command using Webtask.io, the possibilities are endless. We’ve barely scratched the surface of what’s possible in this tutorial; both tools give you immense power to create and automate workflows within your email communications that can make your life easier and your work more efficient and effective. If you’re interested in learning more about how you can use the Mixmax API and other developer tools to make Mixmax work for you, head over to Mixmax Developer to start digging deeper. And if you want to use Mixmax to empower your team and your organization and make your communications more efficient, effective, and impactful, get in touch with us today ! Finally, if you want to work on products that help people communicate better, improve the way the world does business, and are exciting and just plain fun, we’re hiring !", "date": "2018-09-25"},
{"website": "Mix-Max", "title": "How to work remotely", "author": ["William Wettersten"], "link": "https://www.mixmax.com/engineering/how-to-work-remotely/", "abstract": "This is the final part of the Mixmax 2017 Advent Calendar . The previous post on\nDecember 11th was about Elasticsearch vs Redshift for Real-Time Ad-Hoc Analytics Queries . If you work at a technology company, you probably know at least a few people who work outside of the office environment. The mechanics of the software industry make it seamless to log on from almost anywhere in the world and be productive. As an engineer, I have had very little trouble working remotely, and I accomplish more work here than I did while I was in the office. Getting to this point took some work and produced some nice insights, which I will go into in this post. My path to remote work Like many technology workers, I kicked off my career in the Bay Area working for some amazing tech companies. The area is great if you are starting your career in tech, but after a few years life took me elsewhere when I got married and we decided to join my wife’s family in Japan. Unfortunately, the company I was at didn’t offer an option to work remotely. I didn’t want to compromise on my career by taking a job at any company other than a top-notch fast growing tech startup. That’s when I found out about Mixmax. The company is well set up for remote work (we are a communications platform, after all), and we have learned how to make everything come together seamlessly. Telecommuting effectively If you are working from a different time zone, as I am, the most important thing to do is separate your work into synchronous and asynchronous work . When I come online at 7am Tokyo time, I know that it is 2pm in San Francisco, and I have 4 or so hours to have face time with my colleagues. Isolating things like meetings from more heads-down solo work is incredibly useful when you are in the office, and when you are out of the office even more so. Plan and design your day to maximize productivity. Just as you plan your work days for in office work, you should plan your remote days as well. This means setting an alarm, showering, eating real food, and yes, putting on pants. Spoil yourself with a truly great desk . You have full power to design your workspace, so there is no reason to compromise. If possible, dedicate a room to being your office. I went to Ikea and bought a dinner table and turned it into a huge desk. I also got my favorite monitor (Apple Thunderbolt) and a nice keyboard, mouse and chair. You might be surprised by how good this makes you feel. Another priority for any remote employee should be working to maintain relationships with your coworkers . This often times means simulating “coffee break” talk over Slack or Hangouts. In my case, many of these employees are themselves remote, so we created a Slack channel for remote employees to socialize. It is one of my favorite places to hang out virtually. Finally, do your best to stay in tune with the home office . One easy way to do this is to set your work computer's time zone to that of the home office. You might also get a standalone clock that does this. It is also quite useful for knowing when to call family and friends back home. More and more people are doing remote work, hopefully these five tips are helpful to you in your effort to work remotely or build a good remote culture. Or, if you’d like to default to building new tech, open-sourcing it, and talking about it, you can\ncome work at Mixmax. :) Join us!", "date": "2017-12-12"},
{"website": "Mix-Max", "title": "Don’t wait for your PaaS’ status page to tell you why you’re down", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/make-your-own-monitoring-2/", "abstract": "When you’re using a platform-as-a-service, it’s tempting to draw a boundary between\nyour application and that platform—to look at your hosting provider’s status page and\nassume that they’ve got their end all under control. But what do you do when your app goes\ndown and their status page still shows green? Yesterday morning, we learned the limits of third-party monitoring the hard way. When we started\nto see errors from our email-sending service, the first thing we did was to check the status page of Compose , our database-as-a-service provider. Our application\nuses a Redis-based job queue to communicate with the send service, so if our Compose Redis\ndeployment was down, that could have accounted for the problems we were seeing. But Compose showed all systems green, prompting us to dig through our code in increasing\ndesperation. First, some emails failed to send; then, they all did. We immediately rerouted users to\nold Gmail so they wouldn’t experience a disruption of their workflow and requeued the emails\nso they’d send later. Then, we dug into the problem: If the problem wasn’t our hosting\nprovider, it must have been our app… but how? Only when we tried connecting to Redis one more time did we find that it had gone down\ncompletely—despite the status page. The problem wasn’t our app at all! This was encouraging, but we were still down, and above all: Why hadn’t we realized\nsooner that our app had difficulty connecting to Redis? As we traced the timeline of the outage, we saw that Redis hadn’t gone down all at once: the\nconnection failure was at first intermittent. But surely our monitoring system, Pingdom , could have detected even an intermittent outage\ngiven that it polls the application once a minute. But of course… we had never told Pingdom\nto check Redis . All we had instructed Pingdom to do was to check for a HTTP 200 when it polled our default route.\nBut the app shouldn’t have been signaling that it was healthy if it couldn’t connect to\nRedis; we needed the monitored route to check whether Redis was responsive. And once we made that\nchange, we figured we might add some other checks, like whether Redis stayed within the expected\nmemory threshold. We even decided to bundle these checks up and publish them to NPM as redis-status . We decided it would be clearer to add a new route to our service, exclusively for running these\nsorts of self-checks. Here’s how that “health” route looks now: var express = require('express');\nvar router = express.Router();\n\n// Construct a `RedisStatus` object configured to check the status of \n// the Redis server named 'send'. \nvar sendStatus = require('redis-status')({\n  name: 'send',\n  port: process.env.REDIS_PORT,\n  host: process.env.REDIS_HOST\n});\n\n// If 'send' is healthy, this route will print 'great'; otherwise it will print \n// the reason that 'send' is not healthy. A monitoring service like Webmon or \n// Pingdom can raise non-'great' responses as alerts. \nrouter.get('/health', function(req, res) {\n  sendStatus.checkStatus(function(err) {\n    res.send(err || 'great');\n  });\n}); We urge you to follow the same sort of practices: set up Pingdom or Webmon . And\ndon’t settle for their default settings, which just test whether your health route gives an\nHTTP 200— make a health route that checks subsystems and returns a meaningful status .\nUnless your project is a static site, there’s a lot more to it “working” than\nwhether or not the server renders something to the page. This post isn’t mean to condemn the use of database-as-a-service providers—we think\nthe benefits vastly outweigh the risks—but to illustrate the power of double-checking your\nassumptions and collecting as much information as possible. Status pages are great, but you need to define what makes your application available. A third-party can’t ultimately say why your application is down. Try our redis-status module and let\nus know what you think. And if you’re looking to tackle similar breakneck engineering\nchallenges and scaling problems, send us a note to careers@mixmax.com .\nWe’re hiring like crazy and would love to grab coffee.", "date": "2015-03-23"},
{"website": "Mix-Max", "title": "Scaling Mixmax: Front-end performance", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/scaling-mixmax-front-end/", "abstract": "This is the next post in a series on how we scaled Mixmax from a successful prototype to a platform that scales to many thousands of users. In this post, I’ll describe some of the front-end performance bottlenecks we experienced while using the Meteor framework and why we had to move one part of our app off it. We value performance deeply at Mixmax. It’s also very important to our users. Since Mixmax is deeply integrated within Gmail, our users expect it to behave exactly like the built-in Gmail UI. Our users should never see a loading spinner using Mixmax inside of Gmail, just as you’d never see a loading spinner inside of Gmail itself. First, a quick primer on Mixmax architecture: Mixmax is a Chrome Extension that replaces your Gmail new email compose window with its own editor, loaded using an iframe. The reason we use an iframe is to keep our code and CSS entirely separate from Gmail and to minimize the failure-prone integration points with Gmail. However, as an iframe, we incur a network round trip cost every time you click the Compose button. That’s why load time and initial rendering performance are absolutely critical to our user experience. Load time performance was one of our biggest performance complaints. Our time to first render was over 8 seconds (90th percentile). Just having finished migrating our backend to a new microservices architecture , it was now time to rethink the front-end. Performance Analysis The core metric that we wanted to optimize for was “time to first render”. Specifically, this is the time it takes between when the server first receives the request to when the user sees the Compose window UI. This time can be broken down into several loading segments: the time spent processing the request server-side, the time loading external Javascript client-side, and the time from the DOMContentLoaded event (once all initial Javascript is run) until the view is rendered. Step 1. Time to first byte Our 90th percentile time to first byte (TTFB) time is 800ms, which is quite fast given that it includes network round trip time of the request and the start of the response. The actual time spent processing server-side is always less than 50ms; the rest is pure network, likely due to the fact that we're only hosted on the US east coast but have a worldwide audience. We measured this using the following code that sent our TTFB time to Keen.io for processing: var timing = window.performance.timing;\nkeen.addEvent('page load time to first byte', {\n  ttfb: timing.responseStart - timing.connectEnd\n}); Here is the data from our internal analytics dashboard (built using Keen’s API) of the 98th, 90th, 50th, and 20th percentile TTFB load times: This is understandably fast since all the server is doing is creating a new Mixmax message object in the database, inlining it in the page (using the Meteor fastrender package ), and then returning the boilerplate Meteor HTML page. Step 2. Loading external Javascript The 90th percentile load time between TTFB and DOMContentLoaded was 5 seconds (90th percentile). This is atrociously slow compared to most other web apps. We had our work cut out here; we needed to examine this segment closely. This segment can be further broken down into two areas: a) Network time loading Javascript We currently load our app’s Javascript from a CDN. It is permacached based on its GIT SHA (version number). However, since we push a new version of the app every day, a new app Javascript file gets built and users need to download it again every morning. It’s also a quite large file - currently 391kb after automatic gzipping - because Meteor includes all templates and libraries in one bundle even if they’re not used. This is actually OK for now since the library is often preloaded and then permacached until our next deploy. However, it might be an issue if a user wakes up in the morning (after our deploy) on a slow connection, launches Gmail, and then has to wait for the 391kb of Javascript to load. They’ll be waiting for a while. b) Executing the Javascript After the app Javascript is downloaded, the browser executes it. This is where way too much time was being spent. Here is the Chrome Web Inspector flame graph of our app bootup: We were spending almost 400ms in pure Javascript, tested on a brand new fast MacBook. So it’s probably several seconds on the average computer. The top bottlenecks are: Initializing moment-timezone.js - 171ms Initializing jquery - 30ms Initializing chrono.js - 20ms Initializing Iron Router - 20ms This is pure CPU time executing the Javascript and bootstrapping these libraries. We needed to put in a lot of work here! Step 3. From DOMContentLoaded to when the compose view gets rendered After the initial libraries are loaded and the initial Javascript is executed, the next thing that runs is the DOMContentLoaded event. This part adds about 200ms to render the view. Here’s a breakdown: As you can see here, the bottlenecks are: Setting up Iron Router (and Meteor Blaze templates) - 75ms. This is where Meteor is setting up data structures for its Blaze templates. setTimeout(0) delay - 50ms. Meteor’s Blaze engine has its own internal event loop that uses a setTimeout to wait for the next cycle of the Javascript event loop to start rendering templates. This setTimeout on start gives the browser the opportunity to relayout the DOM needlessly, costing about 50ms of load time. select2 initialization - 40ms. This is used for our autocomplete ‘to’ field. messageview initialization - 25ms. Needed for our own UI (the message view) to render. How did we fix this? We came up with three primary goals that would help us address these performance problems: 1. Only load libraries when they’re needed We needed to reduce our overall Javascript size (Step 2, Part 1) as well as eliminate the expensive execution time (Step 2, Part 2). For example, moment-timezone.js and chrono.js are collectively almost 200ms just to run on page load. They’re also not used in the initial rendering of the page. We should lazily load these only when they’re needed. 2. Move off Meteor’s Blaze & Iron Router Meteor Blaze takes about 75ms just to set up its data structures for templates. Additionally, it uses a setTimeout call before rendering its first view. While it’s a setTimeout for '0' ms, it still waits for the Javascript task queue to be exhausted, which depending on other resources loaded might be anywhere from 10ms to 500ms. This can all be avoided by rendering templates directly into the DOM on page load and not using Meteor. 3. Use server-side rendering Most of the compose window is static anyways, so there’s no reason we need to render it client-side. We should be able to render the basic view (to, cc, bcc, and message fields) on page load and not even have to wait for the Javascript to be loaded. New architecture Given the above findings of too much Javascript slowing us down (in both network and execution time), we decided to move our compose window off of Meteor entirely. While Meteor one day might support server-side rendering and other page load optimizations , it’s currently not the right framework to use when loading performance is critical to your app's experience. We are however keeping the rest of our app (such as the Mixmax Dashboard) on Meteor. We chose to architect this new service as close to the metal and bare bones as possible. So we chose a basic Express app with a Backbone front-end. Our new architecture has the following characteristics: Server-side rendering We render the entire HTML for the compose window server-side in Express. We even fill it out with as much data as possible (such as the user’s email address) so the user sees a complete UI even before Javascript loads. Then, when the Javascript is finally downloaded and executed, it simply attaches to the server-side-rendered DOM. Backbone makes this easy by offering several usage patterns to attach a view to an element. Load libraries separate from application source In the Meteor world, most packages bring a library with them. Meteor even packages the jQuery source with its own core source. With our new custom architecture, we were able to load all our libraries from popular CDNs where it’s highly likely that the user already has them cached. Additionally, when we push a new version of our app, the users don't need to re-download these libraries. The <head> code looks like this: <script src=\"//code.jquery.com/jquery-2.1.3.min.js\"></script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/underscore.js/1.7.0/underscore-min.js\"></script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/backbone.js/1.1.2/backbone-min.js\"></script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/handlebars.js/3.0.2/handlebars.runtime.min.js\"></script>\n<!-- Our application Javascript -->\n<script src=\"https://d14s452uta6ylv.cloudfront.net/63f05514e61391320f7915439f20f128a86c2ac5/build.js\"></script> Lazily load anything and everything we can As we found out in our analysis, some libraries such as moment.js and chrono.js are very expensive to load. Fortunately, they aren’t required for first render to show anything meaningful to the user. So we are loading them using async script tags at the very end of the page, loaded from a CDN of course: <script async src=\"//cdnjs.cloudflare.com/ajax/libs/moment.js/2.10.2/moment.min.js\"></script>\n<script async src=\"https://d14s452uta6ylv.cloudfront.net/63f05514e61391320f7915439f20f128a86c2ac5/lib/chrono.min.js\"></script> Chrome also recently made loading async scripts even faster . Results Moving to the new architecture was a huge improvement. Our time to first render dropped considerably, from almost 8sec (90th percentile) down to about one second. We also already received great feedback from our users. Some even said it loaded quicker than Gmail’s own compose window. We certainly miss Meteor in our compose window: reactivity on the front-end, its useful local development toolchain, and the plethora of great packages. But we’ve been able to find or build equivalents to those in our new world of Express and Backbone. However, we're still using Meteor for our application dashboard where page load time isn't as important. We’ll continue to publish followup posts about our journey scaling Mixmax from a successful prototype to a product that scales to many thousands of users. Try it today by adding Mixmax to Gmail . Want to work on interesting problems like these? Email careers@mixmax.com and let’s grab coffee!", "date": "2015-05-05"},
{"website": "Mix-Max", "title": "Building the Giphy app using the Mixmax SDK", "author": ["Chanpory Rith"], "link": "https://www.mixmax.com/engineering/how-we-built-giphy/", "abstract": "Update 10/22/15: The technical information in this post is outdated. See newer post about building this. We recently announced Giphy for Mixmax . Now we’d like to tell you how we built it using the Mixmax SDK. We’re even open sourcing it! Check it out at https://github.com/mixmaxhq/giphy-mixmax-app . The Mixmax SDK allows you to extend the Mixmax platform so users can embed your content inside their email. The Mixmax SDK current offers two types of integrations: An “app” that is accessible using the app picker menu. When user selects your app, Mixmax shows your custom URL. Your editor can then inject content into the Mixmax message. A \"text hook\" that allows your app to inject content when the user types specific content. It will call a service URL you provide and replace the user's text in the editor with your content. In this post, we’ll focus on the former - Mixmax SDK “apps” - and will discuss text hooks in a later blog post. If you have Mixmax for Gmail, you’ve already used Mixmax apps. That’s because Mixmax already comes with many great apps: Availability, Q&A Survey, PDF Slideshow, etc. There are many more on the way - we love making apps! Let’s dive in to the anatomy of a Mixmax app. The entry point for a Mixmax app is the mixmax.json file. It looks like this {\n  \"name\": \"Giphy\",\n  \"author\": \"Mixmax Inc\",\n  \"url\": \"https://mixmax.com\",\n  \"appTray\": {\n    \"name\": \"Giphy\",\n    \"icon\": \"public/img/logo_giphy.png\",\n    \"editor\": \"http://yourdomain.com/editor\",\n    \"resolver\": \"http://yourdomain.com/resolver\"\n  }\n} When a user clicks the app’s icon, Mixmax will load the app’s “editor” URL in a new browser window. When the user is finished selecting content for your app (e.g. by clicking a “Done” button that you provide), your editor interface use the HTML5 postMessage API to send an object back to Mixmax using the following boilerplate code: window.opener.postMessage({\n  method: 'done',\n  payload: {\n    /* Arbitrary serializable configuration data */\n  }\n}, 'https://app.mixmax.com'); Mixmax will then immediately POST the same payload back to your resolver URL, along with the Mixmax user making the request. To simulate this call using the open source Giphy app running locally, run: curl -H \"Content-Type: application/json\" -X POST -d '{\"src\":\"https://media2.giphy.com/media/iLM4h2qLVS7qo/giphy.gif\",\"width\":\"300\"}' http://localhost:3000/resolve That URL should simply return HTML as a string that will get added to the email. So in the Giphy app’s case, it simply returns: <img style=\"max-width:100%; box-sizing:border-box;\" src=\"https://media2.giphy.com/media/iLM4h2qLVS7qo/giphy.gif\" width=\"300\"/> The HTML is then inserted into the Mixmax email so your users can enjoy. Want to build an app? Reach out to us at careers@mixmax.com and let us know what you'd like to build.", "date": "2015-04-13"},
{"website": "Mix-Max", "title": "Gmail just broke every Chrome extension. Here's how we fixed ours", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/gmail-just-broke-every-chrome-extension/", "abstract": "The bug: “Refused to load the script...  because it violates the following Content Security Policy” Yesterday Gmail introduced a Content Security Policy that broke Mixmax and other Chrome extensions. We jumped on it quickly and pushed a fix within an hour. Here’s how we diagnosed and solved the problem. First, here’s a short introduction to how Mixmax works. Mixmax is a Chrome extension that upgrades Gmail’s compose window. The extension is very lightweight: it simply injects a <script> tag into the DOM when the browser navigates to https://mail.google.com . This is so we can push new code without needing to update the extension in the Chrome store (which has been an opaque and inconsistent process, taking anywhere from hours to days). The downside of injecting our script directly into the DOM is that we’re treated as a normal script tag, executed in the global namespace and also subject to a Content Security Policy . So when Gmail enforced their CSP policy yesterday, our script was blocked with this error: Yikes! The Chrome extension API anticipates you might want to load third party scripts, of course, so extensions can specify a content_security_policy field in their manifest.json file. Perfect! So we added this to our manifest: \"content_security_policy\": \"script-src 'self' https://d1j5o6e2vipffp.cloudfront.net; object-src 'self'; frame-src 'self' https://app.mixmax.com\" Reload the extension, refresh Gmail… and… still broken. Rats! Problem #1: The content_security_policy field doesn’t work on cross-origin scripts. Our Chrome extension injects our script using the following (simplified) code: var script = document.createElement('script');\nscript.src = 'https://d1j5o6e2vipffp.cloudfront.net/src/build.js';\nscript.crossOrigin = 'anonymous';\ndocument.head.appendChild(script); The reason we add the crossOrigin attribute is because we attach a window.onerror handler to the page to report our uncaught exceptions. Without crossOrigin , the browser will only report ‘Script Error’ instead of giving us a stack trace. After searching around, we found Chrome bug #392338 . It turns out the presence of crossOrigin on a script tag caused our extension’s content_security_policy field to be ignored. So we commented out that attribute to see if it works: //script.crossOrigin = 'anonymous'; Reload the extension, refresh Gmail… and… still broken. What is it now? This is strange because we included frame-src in our content_security_policy field in manifest.json . That’s not supposed to happen. Problem #2: The content_security_policy field’s frame-src directive doesn’t work on remote iframes Due to another Chrome bug involving cross-origin resources, the frame-src CSP directive is ignored if the frame is loaded from a remote url (as opposed to an HTML file within the extension). At this point it was looking like the content_security_policy manifest field was too unreliable. We searched around a bit and came across a great post which describes how to intercept the HTTP headers on the wire to rewrite the content security policy. Here is our adapted code: var hosts = 'https://d1j5o6e2vipffp.cloudfront.net';\nvar iframeHosts = 'https://app.mixmax.com';\n\nchrome.webRequest.onHeadersReceived.addListener(function(details) {\n  for (var i = 0; i &lt; details.responseHeaders.length; i++) {\n    var isCSPHeader = /content-security-policy/i.test(details.responseHeaders[i].name);\n    if (isCSPHeader) {\n      var csp = details.responseHeaders[i].value;\n      csp = csp.replace('script-src', 'script-src ' + hosts);\n      csp = csp.replace('style-src', 'style-src ' + hosts);\n      csp = csp.replace('frame-src', 'frame-src ' + iframeHosts);\n      details.responseHeaders[i].value = csp;\n    }\n  }\n\n  return {\n    responseHeaders: details.responseHeaders\n  };\n}, {\n  urls: ['https://mail.google.com/*'],\n  types: ['main_frame']\n}, ['blocking', 'responseHeaders']); This code requires two new permissions in manifest.json : webRequest and webRequestBlocking . By default, adding required permissions to the manifest file and pushing an upgrade to the store will automatically disable the extension and prompt users to allow the new permissions. But we’re in luck: webRequest and webRequestBlocking aren’t on the list that require user approval . We tested the new permissions a few times with a copy of the extension in the store and it worked beautifully. We hope the Chrome team fixes those two bugs to make the extension manifest content_security_policy field reliable. We’re eager to remove our code that rewrites HTTP headers–it's not an ideal solution because it could conflict with other extensions trying to do the same thing. But at the end of the day, we're back, and that's what matters to our customers. Like working on seemingly impossible problems? Join us to upgrade email to the 21st century!", "date": "2014-12-17"},
{"website": "Mix-Max", "title": "Node fibers: Patterns and anti-patterns using synchronize.js", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/node-fibers-using-synchronize-js/", "abstract": "For all the discussion of “callback hell” in the JavaScript community—it\neven has its own website —there’s no standard solution to the problem,\nand isn’t likely to be until ES6 promises and generators become more widely\nsupported. That’s a particular problem in Node, given that even v0.12 implements a paltry 17% of ES6. But Node also supports the best workaround, one which\nhas received fairly little attention compared to promises: fibers . This is unsurprising given that there is very little information on what fibers\nare and what they can do. The core implementation is intentionally low-level and understanding how fibers work requires an intimate understanding\nof Node’s event loop. But, you don’t need to understand how fibers work in order to reap their benefits.\nAll you need to know is that fibers let you write asynchronous code as if it\nwere synchronous . They eliminate callbacks, offer a minimum of API overhead\ncompared to promises, and don’t require syntactic support like generators. Then, you need some examples to see how fibers can work: a little library on top\nlike the core project recommends . At Mixmax, our fibers library of choice is synchronize.js . Here’s\nhow to use it to write beautifully-straightforward async code. Basics synchronize.js lets you execute asynchronous functions synchronously—\nwithin a fiber. That way, the fiber can yield while the event loop—other\nfibers, timers, callbacks, etc.—keeps running. Fibers “suspend”, but the\nevent loop never blocks. You create and run a fiber using sync.fiber : console.log('one');\nsync.fiber(function() {\n  console.log('two');\n});\nconsole.log('three');\n\n// Console:\none\ntwo\nthree Within the fiber, you can block on the result of an asynchronous function\nusing sync.defer and sync.await . Calling sync.defer creates a Node-style (err, result) callback. Pass that to an asynchronous function and then call sync.await to suspend execution until the callback is called. Using sync.defer and sync.await won’t block the code executing\noutside the fiber: console.log('one');\nsync.fiber(function() {\n  console.log(sync.await(fs.readFile('a.txt', sync.defer())));\n});\nconsole.log('three');\n\n// Console:\none\nthree\n<contents of a.txt> But the code inside the fiber will run synchronously: console.log('one');\nsync.fiber(function() {\n  console.log(sync.await(fs.readFile('a.txt', sync.defer())));\n  console.log(sync.await(fs.readFile('b.txt', sync.defer())));\n});\nconsole.log('three');\n\n// Console:\none\nthree\n<contents of a.txt>\n<contents of b.txt> sync.await lets you handle asynchronous results and errors synchronously: sync.fiber(function() {\n  try {\n    var aData = sync.await(fs.readFile('a.txt', sync.defer()));\n    console.log(aData);\n  } catch(aErr) {\n    console.error(aErr);\n    // Abort.\n    return;\n  }\n\n  try {\n    var bData = sync.await(fs.readFile('b.txt', sync.defer()));\n    console.log(bData);\n  } catch(bErr) {\n    console.error(bErr);\n  }\n}); What sync.await does is convert the arguments to sync.defer() , err and result , into exceptions and return values: If sync.defer() ’s first argument is truthy, sync.await will throw that\nvalue as an error. Otherwise, sync.await will return sync.defer() ’s second argument, whatever\nthat may be. Compare how you’d handle the errors and results using callbacks: fs.readFile('a.txt', function(aErr, aData) {\n  if (aErr) {\n    console.error(aErr);\n  } else {\n    console.log(aData);\n    fs.readFile('b.txt', function(bErr, bData) {\n      if (bErr) {\n        console.error(bErr);\n      } else {\n        console.log(bData);\n      }\n    });\n  }\n}); As you can see, the synchronize.js form is much more straightforward. For more examples, including running functions in parallel, see synchronize.js ’ documentation . How to structure an application using fibers Using synchronize.js in production requires that you consider how your fibers\nwill interoperate with regular old synchronous and asynchronous functions. This\nisn’t much more difficult than the examples I’ve shown above. But your first\nimpulse might trip you up. There are two simple rules to happy development with fibers. The first and most\nimportant is: Functions should never assume that they’re running in a fiber. Here’s what I mean. In contrast to promises or generators, fibers make it\ndangerously easy to refactor code. If I start with: sync.fiber(function() {\n  var data = sync.await(fs.readFile('a.txt', sync.defer()));\n  var processedData = /* do something with the data */;\n}); and then decide that I want to move the reading-and-processing operation into\nits own function, it’s super easy to do so: sync.fiber(function() {\n  var processedData = processFile('a.txt');\n});\n\nfunction processFile(filename) {\n  var data = sync.await(fs.readFile(fileName, sync.defer()));\n  return /* do something with the data */;\n} processFile doesn’t have to be declared using special syntax, like a generator\nfunction. It doesn’t have to return a result or an error using special APIs like\npromises. …but… processFile cannot be called outside of a fiber! processFile('a.txt');\n\n// Console:\nError: no current Fiber, defer can't be used without Fiber! To preserve modularity and avoid unpleasant surprises when using unfamiliar APIs, asynchronous functions should accept callbacks . DON’T PANIC. The difference between callback hell and fibers is that (second rule): You don’t call the callbacks yourself. To fix processFile you just have to wrap its contents in a fiber and pass\nthe callback to the fiber : function processFile(fileName, done) {\n  sync.fiber(function() {\n    var data = sync.await(fs.readFile(fileName, sync.defer()));\n    return /* do something with the data */;\n  }, done);\n} And synchronize.js will call done for you! If the fiber throws an error, done will be called with that error as its\nfirst argument. If the fiber returns a value, done will be called with that value as its\nsecond argument. In this way, fibers interoperate seamlessly with Node-style callbacks. To drive home this lesson, consider calling done yourself in a more complicated processFile : function processFile(fileName, done) {\n  sync.fiber(function() {\n    var data;\n    try {\n      data = /* first `sync` call */;\n    } catch(e) {\n      done(e);\n      return;\n    }\n\n    var processedData;\n    try {\n      processedData = /* second `sync` call */;\n      done(null, processedData);\n    } catch(e) {\n      done(e);\n    }\n  });\n} Look at all the places that done could be called! And the superfluous return necessary to guard against calling it multiple times! It’s barely better than\nif you were only using callbacks. But by passing done to sync.fiber ,\nyou can strip away almost all the cruft: function processFile(fileName, done) {\n  sync.fiber(function() {\n    // No need for try-catches even, assuming that you're ok with `done` handling errors!\n    var data = /* first `sync` call */;\n    return /* second `sync` call */;\n  }, done);\n} Marvelous. Use magic sparingly Now that you’ve learned the rules above, I can share with you the most powerful\nability of synchronize.js : to do away with the API calls altogether. If you pass an object to sync , you make its asynchronous methods synchronize.js -aware.\nAfterward, calling such a method without a callback implicitly calls sync.await and sync.defer : sync(fs, 'readFile');\n\nsync.fiber(function() {\n  var data = fs.readFile('a.txt');\n}); But: this can make it very difficult to tell what code needs a fiber! As before,\nif you’re worried you might be running outside of a fiber, make one. There’s\nnegligible overhead to doing so. At Mixmax, we sync only the most commonly-used asynchronous functions, like\ndatabase accessors. Smooth sailing Fibers bring the simplicity of synchronous programming to Node without compromising\non I/O. Using fibers, you get to code as if Node had threads without\never blocking the event loop. As long as you follow the rules above, fibers are a powerful yet simple alternative\nto callbacks, promises, and generators. To wit: Give every function its own fiber and callback Let synchronize.js call that callback And use magic sparingly. Questions about fibers? Want to tell us that promises are the one true way to\nwrite asynchronous JavaScript? Email careers@mixmax.com or send us a tweet @Mixmax ! Want to explore cutting-edge JavaScript technologies at your day job?\nEmail careers@mixmax.com and let’s grab coffee!", "date": "2015-08-22"},
{"website": "Mix-Max", "title": "Unicode woes in Javascript", "author": ["Andrew Tamura"], "link": "https://www.mixmax.com/engineering/unicode-woes-in-javascript/", "abstract": "At Mixmax, we send a lot of email. So we run into just about every unicode issue. This post describes a key learning about iterating strings in Javascript and why it's so often done wrong. Javascript strings are all stored internally as UTF-16 encoded unicode. This gives first-class support for fun characters ike 😜, 🍯, 🐼. The Basic Multilingual Plane includes characters and symbols used by most modern languages. Whereas emoji characters are contained in the Supplementary Multilingual Plane . Characters in the BMP are represented by a single 16-bit code. Non-BMP characters are represented by an ordered pair (called a Surrogate Pair in unicode vocabulary) of two 16-bit codes. Even though non-BMP characters are human readable as a single character, Javascript's internal storage still treats them as two characters. This may lead to unexpected problems when iterating the characters in a string. Lets look at the simple example of a string containing just one character, the HEAR-NO EVIL MONKEY chracter: 🙉. var foo = '🙉';\n\nfoo.length // what is this string's length? Normally String.length property returns the number of characters in the string. However, reading the MDN docs on String.length , the property actually returns the \"number of code units in the string.\" Non-BMP characters are stored as two code units and so the length property will return 2 instead of 1. Try it out for yourself. When iterating a string, you are actually iterating code points. This will cause your code to have unexpected results if you aren't careful. Take for example this code that reverses a string. var input = 'This is my string 🙉';\n\nfunction reverse(input) {\n  var output = ''\n  for (var i=0; i<input.length; i++) {\n    output = input[i] + output;\n  }\n  return output;\n}\n\nreverse(input) // => '?????? gnirts ym si sihT' For strings that contain only single code point characters (BMP characters) the function works fine. But when we have a character that is represented as two code points, reversing them puts the surrogate pair in the wrong order and the character becomes unreadable. The key to reversing strings correctly is to detect non-BPM chracters, reverse their order first, then reverse the entire string. Check out @mathias's great esrever module inspired by rapper/computer scientist Missy Elliot. Too understand better understand Javascript's handling of non-BMP charaters, read Mathias Bynens excellent post Javascript has a Unicode problem . Want to work on interesting problems like these? Email careers@mixmax.com and let's grab coffee!", "date": "2015-06-09"},
{"website": "Mix-Max", "title": "Scaling Mixmax: From Monolithic to Microservices", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/scaling-mixmax-monolithic-to-microservices/", "abstract": "In the next series of blog posts, we’ll open you up to the technical inner workings of Mixmax and discuss how we evolved it from a simple prototype into a successful product that scales to many thousands of users. In this first post, we’ll tell you about what happened when Mixmax went out-of-control viral and how we quickly diagnosed and fixed a key bottleneck. Launching a prototype Mixmax started just 9 months ago. We kept our product in private beta with friends and family throughout the early months. But we knew we were on to something great. All of our early users loved it and even asked if they could pay for it. So we decided to polish it up a bit more and then “soft launch” Mixmax publicly for the first time on Product Hunt in mid-January. We didn’t expect much—perhaps a few hundred downloads—well within the capacity that we planned for our prototype to support. At Mixmax we follow the principle: make it valuable, then easy, then fast. Our product was still a prototype, built to prove a product-market hypothesis with a small user base. We had focused much of our attention on the delightful UX details that make Mixmax so wonderful to use. We deferred work on performance and scaling until we knew were on the right product path. Or architecture was primitive and fragile: all reads and writes were hitting our Mongo database directly with no caching layer in-between. We over-relied on Meteor framework primitives (live subscriptions using oplog tailing) to implement features of our app. It was exactly what it needed to be (and not a bit more!) for a prototype. Then came ProductHunt launch day on Jan 13th. We quickly rose to #3 on ProductHunt . We were being downloaded thousands of times and there were lots of tweets about how we were the best Gmail add-on. Our traffic continued to rise throughout the day: Then, that’s when we started hitting performance issues. Our TTFB server time slowly crept up to almost 5 seconds—time that the user spent staring at our loading spinner. By Thursday afternoon, our traffic tapered off and users were uninstalling because they couldn’t log in. This was devastating. We needed to fix these performance issues ASAP to keep our new users happy. Determining the bottleneck First, a bit about Mixmax architecture: we started off as a prototype Meteor app. For ease of development, Meteor encourages developers to use Mongo for backend storage. Meteor internally uses the Mongo oplog to monitor updates from across the app. Some of these updates are pushed down to the client over persistent WebSocket connections. This all works great when you only have a few users, but it has significant overhead when scaling to thousands of users and many more database writes. With some quick analysis we correlated our slow performance with new users signing up. This meant the bottleneck was our contact-syncing feature. When a new user signs up for Mixmax, we sync their Google Contacts into our secure database so they have all their contacts available when composing a new Mixmax message. The architecture was simple: we simply queried contacts using the Google API and wrote them directly into our Mongo database. We then set up a new Meteor subscription that watched the database and returned results real time. This naive architecture was inefficient in many ways. First, contact syncing happened on the same server as the app. Syncing took 92 seconds (90th percentile) and consumed 100% for most of the time while it downloaded, parsed, and inserted the contacts into Mongo. The result was the CPU being pinned at 100%: This caused users to lose their connection to the app if they had the misfortunte of being connected to a server that was syncing a new user. It was clear that we needed to rethink our architecture in a big way to begin scaling it to thousands. Our solution was to move to a microservices architecture, starting with the contacts system. Microservices make it much easier to scale each component of our app while keeping the other parts stable. Moving contact syncing off into a separate microservice We set up a new microservice that was the “syncing box”, solely responsible for syncing users' contacts when they signed up for Mixmax. We set up Kue as a job queue for the sync jobs so only one ran at a time on each box. With our PaaS Modulus , we could easily change the number of workers by changing a slider in their UI. We used Compose.io for hosted Redis that Kue would sit on top of. Additionally, Kue has a nice built-in UI for managing jobs: The result was an immediate improvement. No longer did the Meteor boxes get consumed when a new user signed up. Things were starting to look much better. This solution worked well for us for a few days. However, when a lot users signed up within the same minute (we had about one signup every 9 seconds), we noticed the app server’s CPU was still getting consumed. But why was this happening if the database writing was offloaded to the new server? Well, our naive client-side implementation set up a new Meteor subscription when the user started typing in the ‘to’ field in Mixmax. This required that Meteor tail the oplog for the database collection. The net result was that even after syncing and writing just 10,000 contacts to database, the app CPU was consumed for many seconds afterwards: Moving contacts into their own database The CPU spike was due to Meteor’s LiveQuery which was trying to keep up with thousands of contacts being inserted into Mongo. If there’s a subscription registered with the collection, as we have in this case, Meteor will watch the collection and process every update. When we’re writing 10,000 contacts per user within a minute, this causes a CPU spike of about 30 seconds, starving CPU resources from the application itself. Users were complaining about slow load times and not being able to log in (because other Meteor subscriptions were getting dropped). However, when syncing a large amount of contacts, we still noticed the CPU still spiked a bit. It turns out that removing the Meteor subscription from the recent contacts database wasn’t enough: even storing contacts in the same database that Meteor is watching causes the oplog to overwhelm Meteor. This was a known issue with Meteor and it has since been fixed. So the solution was to move the contacts off into their own database, completely cut off from Meteor. The result was instant: the servers running Meteor immediately returned to a normal CPU % and all was well. Contact syncing and querying was just the first of several Mixmax subsystems that we moved out to microservices. Look for future blog posts about moving other key parts of Mixmax infrastructure out of the monolithic Meteor codebase into microservices. This is one of many ways we’re making the Mixmax infrastructure scalable, secure, and fast. Want to work on interesting problems like these? Email careers@mixmax.com and let’s grab coffee!", "date": "2015-03-30"},
{"website": "Mix-Max", "title": "HTTP cache-control header and the Chrome back button", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/chrome-back-button-cache-no-store/", "abstract": "A common pitfall of sites that serve up dynamic information is to not include the proper cache-control headers. For example, a commonly used variant of the cache-control header is this: cache-control: private, max-age=0, no-cache However, it’s not quite right. Take, for example, the Chrome back button. When a user visits your website, navigates away, and returns via the back button (or reopens a tab from history), Chrome will actually still serve up the cached page (despite the no-cache cache directive!). This was admittedly frustrating to us and the source of several Mixmax bugs involving stale content being served up. The solution? Add the no-store cache directive. It tells the browser to really not store anything at all. So this works, and clicking the back button in Chrome will never serve up cached content: cache-control: private, max-age=0, no-cache, no-store See the following repro case: /**\n* Run this by downloading this script to your computer, then:  \n* 1. $ npm install express  \n* 2. $ node thisscript.js\n* 3. Open localhost:8030/nocache and then localhost:8030/nostore\n*/\n\nvar app = require('express')();\n\napp.get('/nocache', function(req, res) {\n  res.setHeader('Cache-Control', 'no-cache');\n  res.send(new Date().toString() + '<br><a href=\"https://mixmax.com\">Click to navigate away and then press ' +\n      'Back. It will show the same timestamp.</a>');\n});\n\napp.get('/nostore', function(req, res) {\n  res.setHeader('Cache-Control', 'no-cache, no-store'); // Added no-store\n  res.send(new Date().toString() + '<br><a href=\"https://mixmax.com\">Click to navigate away and then press ' +\n      'Back. It will update the timestamp.</a>');\n});\n\napp.listen('8030'); If you open http://localhost:8030/nocache in Chrome, click the link and then click back, you'll see the same timestamp. However, if you open http://localhost:8030/nostore , click the link and click back, you'll see an updated timestamp. This has been filed as #516846 . But since there isn't a formal standard for back/forward browser button behavior, it's hard to argue what the right behavior should be. Like working on seemingly impossible problems? Join us to upgrade email to the 21st century!", "date": "2015-10-04"},
{"website": "Mix-Max", "title": "Migrating production services to AWS Elastic Beanstalk without downtime", "author": ["Andrew Tamura"], "link": "https://www.mixmax.com/engineering/migrating-production-services-to-aws-elastic-beanstalk-without-downtime/", "abstract": "Mixmax started out as a monolithic Meteor application hosted on a PaaS provider ( Modulus.io ) that specialized in deploying NodeJS and Meteor applications. As our traffic and userbase grew, we quickly ran into scaling problems with the Meteor framework, poorly optimized prototype code, and the limits of our cloud hosting provider. We’ve blogged about some of our scaling projects previously: Scaling Mixmax: Front-end performance , Unicode woes in Javascript , and more . In addition to scaling difficulties, there were features that we were looking for that Modulus.io didn't offer at the time. We investigated other PaaS providers like Heroku, but eventually decided that moving to AWS would give us the best feature set immediately and put us in the right direction moving forward as well. Working with AWS isn’t easy. Even reading the developer documentation can be headache inducing. Even if you are experienced with AWS, you still have dozens of options to consider for server configuration and deployment. Any googling of the problem space quickly devolves into madness. Read CircleCI’s it’s the future for a taste of how deep the rabbit hole goes. In the end we decided to use AWS Elastic Beanstalk , a PaaS offering built by AWS. Elastic Beanstalk handles configuring the servers, deploying code, and provides a nice dashboard UI to manage your projects. The dashboard isn’t as easy to use as other PaaS offerings, but the other features of Elastic Beanstalk are very compelling. Here is a short list of the features offered by Elastic Beanstalk that we found to be superior to our current PaaS provider. integrated CloudWatch monitoring zero-downtime deploys (batched deploy groups) more flexible CPU/memory offerings reliability of load balancer (Elastic Load Balancer) Mixmax is a real-time communications product with users all over the world. We knew from the start that this migration should be done in stages in minimize downtime of crucial systems. The plan for migration was simple: Step 1: Bring up an identical AWS environment Step 2: Change the DNS records from the Modulus.io hosts to the AWS Elastic Load Balancer. Step 3: After DNS propagation completes, turn off the old environment. Step 1: Bring up an identical AWS environment The first step was to bring up an AWS environment side-by-side with our existing production environment. The basic outline was as follows: configure Elastic Beanstalk environment configure build and deployment tools Elastic Beanstalk’s setup wizard is probably one of AWS’ best offerings to date. Under the hood the setup wizard configures and launches EC2 instances, configures an ELB, and sets up basic CloudWatch alarms. Elastic Beanstalk also offers a CLI tool similar to Heroku’s for even faster environment setup. Our services have staging and production environments that get deployed automatically based on commits to the corresponding Github repo. Fortunately our CI provider Codeship supported multiple deployment destinations, though we found Codeship’s Elastic Beanstalk default integration wasn’t great. It required a new S3 bucket to store the to to be deployed, and along with it a custom IAM policy that bridges S3 and Elastic Beanstalk. We opted to use the Elastic Beanstalk’s CLI tool directly in our deploy script instead. After ensuring that Elastic Beanstalk and Codeship configurations were correct and worked as expected we were able to proceed to the next step. Step 2: Switch the DNS records Most of our services expose an HTTP REST API. Flipping the switch between the old environment and new environment was as simple as changing the DNS records. One lesson we learned the hard way, when setting up the DNS record for your Elastic Beanstalk project, use the Elastic Beanstalk URL and not the Elastic Load Balancer URL. Each Elastic Beanstalk project exposes a rebuild environment action that will tear down every EC2 instance along with the ELB. If you use the Elastic Load Balancer CNAME URL and someone accidentally clicks the rebuild environment button, then your project will be unreachable until your DNS records propagate the CNAME for the new ELB. Step 3: Turn off the old environments after DNS propagation People use Mixmax from all over the world and DNS propagation to remote location in the world may not happen quickly. Our DNS records were had a TTL of 3600 seconds, but we were still recording traffic on our old environments for well over an hour after the DNS change. We opted to leave the old environments running for 24 hours after the DNS change. Running two environments for 24 hours wasn’t an engineering burden because our deployment process automatically pushed to both environments. After the dust settled As much as two PaaS offering can appear similar on paper, in reality the differences are amplified far more than we could have expected. Some of the advantages offered by Elastic Beanstalk are game changing. Accurate minute-by-minute monitoring offered by CloudWatch has provided us with so much more transparency into how our application code runs in production. CloudWatch Alarms are insanely customizable and some quick configuration resulted in less false positive Pager Duty incidents. Zero-downtime deploys have become crucial as our traffic increases. After this migration, the majority of our microservices are hosted on Elastic Beanstalk. We still use Modulus.io to host our Meteor application because Elastic Beanstalk doesn't offer a turnkey solution for SSL, websockets, and session affinity. We learned a lot from this migration and going forward we’re confident we can migrate again if needed. Tweet at us @Mixmax with your opinions on deploying with AWS vs other PaaS/IaaS offerings. If you enjoy working on a real time communication product with a global userbase, drop us a line at careers@mixmax.com . We’re based in San Francisco, CA and are looking for engineers who care about distributed, real-time systems and building a product for the global community.", "date": "2015-10-23"},
{"website": "Mix-Max", "title": "Leveraging AWS API Gateway and Lambda to build a Slack bot", "author": ["Andrew Tamura"], "link": "https://www.mixmax.com/engineering/how-to-get-modulusio-webhooks-into-your-slack-channel/", "abstract": "The engineering team at Mixmax loves Slack integrations. Slack’s webhook API makes it super easy to integrate external data right into the heart of our internal communications. With just a few clicks we can configure integrations like GitHub pull requests or PagerDuty alerts. One of our most wished for integrations was for our PaaS provider ( Modulus.io ) to send project state changes into Slack. Modulus has webhooks for each project upon state change, but the webhook needs some simple manipulation to get it into a working Slack format. It’s not a lot of code to do this manipulation, but setting up yet another project seemed like over kill. We could have added a single HTTP endpoint to an existing service to handle this simple webhook redirection, but that’s against our philosophy of microservices. Once Amazon released API Gateway we knew instantly how we were going to build our Modulus-to-Slack bot. Last Friday Amazon introduced API Gateway ; an AWS service that allows you to build REST HTTP APIs that dispatch requests to other AWS services like EC2 or Lambda. This service is the key to building a quick Modulus-to-Slack webhook API. Using AWS Lambda we can run some server-side JavaScript code in a hosted environment, without having to worry about server configurations or frameworks—just upload your JavaScript function and AWS Lambda will run it for you. By creating an HTTP endpoint in API Gateway that integrates with Lambda, we can create a simple microservice that accepts our Modulus webhook, transforms it into Slack format, then sends it to Slack’s webhook API. First let’s look at the Lambda function. We need to transform the Modulus webhook into Slack’s incoming webhook format . Here is a simple transformation function: var request = require('request');\nvar util = require('util');\n\nvar slackWebhookUrl = '<YOUR_SLACK_WEBHOOK_API_URL>';\n\nexports.handler = function(event, context) {\n  var data = convertModulusWebhook(event);\n  var options = {\n    method: 'post',\n    body: data,\n    json: true,\n    url: slackWebhookUrl\n  };\n  request(options, context.done);\n};\n\n/*  convert a Modulus webhook into a slack webhook */\nvar convertModulusWebhook = function(webhook) {\n  var data = webhook;\n  var text = util.format('%s - %s', data.type, data.project.name);\n  var resp = {};\n  resp.attachments = [];\n  var info = {\n      'fallback': text,\n      'pretext': text,\n      'color': getStatusColor(data.type),\n      'fields': []\n    };\n  info.fields.push(buildCrashAttachment(data));\n  resp.attachments.push(info);\n  return resp;\n};\n\nvar buildCrashAttachment = function(data) {\n  if (data.type === 'crash' && data.servo) {\n    return {\n      'title': 'Modulus Alert',\n      'text': data.servo.log,\n      'short': false\n    };\n  } else {\n    return {\n      'title': 'Modulus Alert',\n      'text': data.project.name,\n      'short': false\n    };\n  }\n};\n\n/*  returns a Slack color depending on the modulus event type. Crash should be red, deploy should be\n*  green, and everything else will be orange.\n*/\nvar getStatusColor = function(eventType) {\n  if (eventType === 'crash') {\n    return 'danger';\n  } else if (eventType === 'deploy') {\n    return 'good';\n  } else {\n    return 'warning';\n  }\n}; I’m not going to do a step-by-step of how to set up AWS Lambda (because ain’t no body got time for that), but it’s fairly straightforward to setup, relative to other AWS services. Setting up an HTTP endpoint with API Gateway is a little confusing, but here is what your final HTTP endpoint should look like. As you can see from the screenshot, I’ve created a resource under the root URL with the path /k12aorxwgz . This is the URL that Modulus’ webhook will POST to. This resource has a POST method configured to run the Lambda function that I created above. Once you have your API configured, you need to create a deployment stage in order to expose your API to the public. Once you have configured a stage for your API Gateway, you can get the endpoint URL and paste it into your Modulus project’s webhook notifications URL. Once you have everything wired up you should get Slack messages like this: API Gateway isn’t perfect. Harim Software Blog has a more in-depth post about some of the configuration annoyances and overall product shortcomings. It’s a great read and I recommend it if you are interested in learning more about using AWS API Gateway in a more full-fledged manner. For building simple one-off APIs, API Gateway and Lambda are a great alternative to spinning up another Modulus (or Heroku) project. If you enjoyed reading this post consider Adding Mixmax to your Gmail and writing us with your opinions and comments. We’d love to hear from you. Reach out to us at careers@mixmax.com and follow us @Mixmax .", "date": "2015-07-14"},
{"website": "Mix-Max", "title": "Bringing Meteor&rsquo;s reactive collections to Backbone", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/meteor-and-backbone/", "abstract": "Last month, we migrated part of our app from Meteor to Backbone in order to improve our load time. As we wrote then , our users benefit from the performance gains provided by Backbone’s minimal footprint and support for attaching views to server-side-rendered DOM, but we found ourselves missing Meteor’s reactive front-end. Today, we are pleased to announce progress toward re-implementing such reactivity in Backbone. In Meteor, reactivity means that: When data changes server-side, the server pushes relevant changes to clients. When client-side data changes, templates using that data automatically update. We chose to focus on the first of these goals. Manually re-rendering templates when data changes isn’t as big a pain as not having that data at all. And with parts of our application in Meteor, it was critical to synchronize their data with the Backbone app to present the realtime UI our users have come to expect. It would have been possible to build out a polling architecture or to use a raw websocket, but we wondered if there was a way to connect our frontend to our existing Meteor backend. Luckily, Meteor’s data protocol is open-source, and there are several JavaScript client libraries. At first, we considered Mondora’s Asteroid project, but dismissed this as unnecessarily complex given our constraints: We only need to support the browser. We only need our Meteor collections to be read-only—we use AJAX to write back to the server, to be able to cancel and parallelize requests. And, Asteroid does not support fine-grained change notifications, which we wanted for ease of integration into Backbone. Luckily, Mondora has separately open-sourced just the part of Asteroid that speaks DDP as ddp.js . Using that project, we were able to re-implement Meteor subscriptions and reactive database queries in less than 300 LoC . Here’s what it looks like in use: // Tells the server to start sending preferences documents to the client.\nMeteor.subscribe('userpreferences');\n\n// Reactively query the preferences of user ‘foo’.\nvar query = Meteor.getCollection('userpreferences').find({\n  userId: 'foo'\n});\n\n// Print changes to the documents in the query’s result set.\nquery.on('changed', function(attrs) {\n  console.log('The preferences that changed are:', attrs);\n}); Using these reactive queries, it’s really easy to synchronize our Backbone collections with the Meteor collections using a Backbone.Collection subclass, Backbone.MeteorCollection . As the query changes, the contents of the Backbone collection are kept in sync, with the Meteor records automatically being converted into instances of the collection’s model and the Backbone collection emitting the usual change events . One way in which we’re using Backbone.MeteorCollection is to immediately unlock full access to email tracking after the user upgrades to a paid plan. We render the tracking menu or an upsell depending on whether the user has an email tracking “feature”. By making the “features” collection an instance of Backbone.MeteorCollection , we can detect the feature being granted to the user as soon as the purchase completes on our server: var FeaturesCollection = new Backbone.MeteorCollection({}, {\n  model: FeatureModel,\n  reactiveQuery: Meteor.getCollection('features').find({ userId: 'foo' })\n});\n\n// Load the initial data, bootstrapped into the page when it is rendered on the server:\n// http://backbonejs.org/#FAQ-bootstrap\nFeaturesCollection.reset({{{initialDataFromServer}}});\n\n// Render the initial state of the UI;\nif (FeaturesCollection.findWhere({ name: 'emailTracking' })) {\n  renderTrackingMenu();\n} else {\n  renderTrackingUpsell();\n}\n\n// Begin tracking changes to the features data.\nMeteor.subscribe('features');\n\n// Update the UI when the data changes.\nFeaturesCollection.on('add', function(feature) {\n  if (feature.get('name') === 'emailTracking') {\n    renderTrackingMenu();\n  }\n}); As you can see, reactive data sources integrate very gracefully into Backbone.  With a little bit of polish, we’re looking forward to releasing our Meteor client and Backbone.MeteorCollection as full-fledged open-source projects. Future work might include implementing reactive templates or support for writing changes back to the server using DDP as an alternative to AJAX. Have any particular requests for Meteor functionality in Backbone? Email careers@mixmax.com or send us a tweet @Mixmax !", "date": "2015-05-27"},
{"website": "Mix-Max", "title": "Introducing the Mixmax SDK", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/introducing-mixmax-sdk/", "abstract": "Mixmax brings many popular web services to your email authoring experience, including Giphy , Google Calendar , Twitter , GitHub , and many more . We receive requests every day to add more. That's why we're opening up Mixmax so any developer can integrate with Mixmax. You can build a Slash Command, Link Resolver, or an Enhancement for: your own use, to release to everyone at your company, or to make public for all Mixmax users to enjoy! Slash Commands - Integrations that are accessed by typing \"/\" in a Mixmax email. Here's an example of /wiki search that shows results from Wikipedia directly in your email. Link Resolvers - When a user pastes a URL on a new line in the editor, Mixmax looks for a Link Resolver to generate a rich preview of the link. For example, pasting a Gist url in an email will replace the URL with the actual code snippet: Enhancements - Integrations that are represented by icons in the Mixmax Enhancement menu. When clicked, they bring up an \"editor\" interface to enable the user to complete a complex workflow such as picking meeting times or building a survey. Examples include Availability, SMS, and Poll. As a developer, you provide a RESTful API that Mixmax will call with parameters specific to the type of integration and the user input. Your URL returns a JSON response that Mixmax uses to insert content into the email. To get started, visit our SDK documentation . We provided code examples for each type of integration to help you get started. In the next series of blog posts, we'll introduce you to building on top of Mixmax. Interested in working on an email platform of the future? Email us at careers@mixmax.com and follow us @Mixmax .", "date": "2015-11-02"},
{"website": "Mix-Max", "title": "How we built 'instant' autocomplete for Mixmax", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/autocomplete-search-performance/", "abstract": "Performance is a big deal at Mixmax. Our users use the product all day, so we strive to make sure it’s a smooth and enjoyable experience. One area that we’ve recently put a lot of work into was improving the performance of the contacts autocomplete field in the compose window: Our original implementation of autocomplete was very slow. When a user typed into the autocomplete field we’d fire off an ajax request to the server for results. This took 824ms (90th percentile) and felt very sluggish. So where was the slowness? Our performance logging told us that the server-side lookup was very fast (9ms for 90th percentile), so the slowness was purely in the network. We needed to come up with a better solution that didn’t rely on the network; we needed to implement a client-side cache. Designing a client-side cache In an ideal world every result would be instant. In order to achieve that, we’d need to cache the user’s entire library of contacts client-side and perform a lookup immediately on every keystroke. However, it wasn’t feasible to store their entire contacts library client-side given that the average user has around 8,000 contacts (think: every unique person you’ve ever emailed). But we can get close. What data structure to use? Our autocomplete UI widget passes us a query string and we pass it back an array of contacts with ‘email’ and an optional ‘name’ property. So it’d be ideal if our cache data structure could easily map a query to some results in near-constant time. So this seemed ideal: { \n  <query>: [<result 1>, <result 2>, ...]\n} Here’s example data: {\n  \"b\": [{\"name\": \"Brad Vogel\", \"email\": \"bradv@mixmax.com\"}, {\"name\": \"Bob Jones\", \"email\": \"bob@mixmax.com\"}],\n  \"br\": [{\"name\": \"Brad Vogel\", \"email\": \"bradv@mixmax.com\"}]\n} Which API to use? HTML5 has many client-side persistence APIs to choose from. There’s the usual ones: Cookies, Local Storage, Session Storage. Then there’s the newer APIs: IndexedDB and HTML5 File API. Cookies were unrealistic because of their small size (4kb in Chrome) and that they’re sent to the server with every request, slowing down the entire site. Newer APIs like IndexedDB and HTML5 File API had limited browser support, more complex APIs, and were more challenging to debug in the browser. IndexedDB was by far the most attractive option given its “unlimited” storage, but its async APIs would have required more complex code. We just needed a simple API that we could store 1000s of contacts in. So that left us with Local Storage. Its API is about as basic as it gets (global object window.localStorage), all reads/writes are synchronous, and it can hold a good amount of data (10MB in Chrome). Dealing with storage size limitations Local Storage is limited to just 10MB. Additionally, it's UTF-16 encoded , so only you really only have half available even if you’re storing basic ASCII characters (side note: there’s discussion of removing this limit). Our data format is highly redundant with many contacts showing up for many query results, so we needed some sort of compression otherwise we’d be really limiting our cache. As any CS textbook will tell you, compression comes down to a tradeoff of time vs storage. Our solutions would traverse this tradeoff. First solution: use an out-of-the-box compression library After some googling around, we found LZ-string - a purpose-built implementation of LZW compression for Local Storage. Perfect for our use case. It seemed to run in a reasonable amount of time when tested locally. So we wrote some code to cache autocomplete results in our format, serialize it as JSON, compress it through LZ-string, and then save it to Local Storage. We let this solution run for a few days and watched our analytics closely as users’ caches began to fill up from their daily use of autocomplete. The results: The lines represent the 99th, 90th, and 50th percentile time to save the cache in Local Storage Length of string (JSON-encoded data structure) that gets saved in Local Storage. The data was alarming: for the slowest 1% of users, it took 550ms to compress the cache and save it to Local Storage. The upside, however, was that LZ-string did a great job keeping the cache size small (<40k chars, so only 80kb (since UTF-16) of our 10mb limit). But a half second of stalling the browser was too slow. Second solution: implement our own compression Since LZ-string was doing a bit too good of a job compressing the results (at the expense of time), we needed a solution that offered a better balance. We decided to roll our own compression solution that simply stored unique email results separate from the queries. So this data structure: {\n  \"b\": [{\"name\": \"Brad Vogel\", \"email\": \"bradv@mixmax.com\"}, {\"name\": \"Bob Jones\", \"email\": \"bob@mixmax.com\"}],\n  \"br\": [{\"name\": \"Brad Vogel\", \"email\": \"bradv@mixmax.com\"}]\n} could be “compressed” to this: {\n  \"queries\": {\n    \"b\": {\n      \"timestamp\": 1445794686069,\n      \"results\": [0, 1],\n    },\n    \"br\": {\n      \"timestamp\": 1445794732044,\n      \"results\": [0]\n    }\n  },\n  \"emails\": [{\"name\": \"Brad Vogel\", \"email\": \"bradv@mixmax.com\"}, {\"name\": \"Bob Jones\", \"email\": \"bob@mixmax.com\"}]\n} In local testing, it didn’t do quite as well in the “space” category, requiring roughly 4x the space of the LZ-string compression. However, it ran an order of magnitude faster. So we rolled it out to production and waited a few days to collect real-world usage: The results were as expected: the cache was significantly faster to compress (23ms for the 99th percentile!) but required 4x more storage. However, the larger storage (of 64k characters for 99th percentile) was well under the size limit (10mb) of Local Storage. So this was a much better time vs storage tradeoff. The best part is watching our overall “user happiness” metric steadily improve each day - the autocomplete cache hit rate (the % of queries that return immediate results) - is now up to 55%. All for just 23ms of time spent saving the autocomplete cache locally. Conclusion This project turned out to be the right balance between time spent engineering (just a few hours total) and improved user experience. We could farther improve it by: Using IndexedDB instead of Local Storage since it has “unlimited” storage. The only reason we didn’t do this in v1 was just that the asynchronous APIs made it a bit more complex to integrate it with our autocomplete widget. But this would be ideal so we’d never have to deal with a user hitting the Local Storage cache limit. Pre-warming the cache so a user's first query is instant right after they install Mixmax. The server already knows who the user emails the most when first syncing, so it could just send those contacts down to the client on first load. We’d have to make our cache matching logic much smarter though to map search queries to those results. The cache is currently growing unbounded so we'll eventually need to cap it to a certain size (in LRU order). It'll be interesting to watch the growth behavior and see where it plateaus out at the max number of distinct queries a user is likely to ever search for. A key lesson in this project was to always measure everything. Don't trust that off-the-shelf libraries will just work for your needs based on what they advertise. Most importantly, balance the needs of your users and time spent on the project. Even though solutions can be basic (such as using Local Storage in this case) you're ultimately responsible for delivering the quickest solution that satisfies your users. Like making products on the web fast ? Come join us! Email careers@mixmax.com and let’s grab coffee!", "date": "2015-11-05"},
{"website": "Mix-Max", "title": "Building Your First Enhancement using the SDK", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/giphy-enhancement-sdk/", "abstract": "This post is part 4 in an engineering blog series about building on top of the Mixmax SDK . See part 1 , part 2 , and part 3 . Mixmax is a powerful email platform that brings the power of the web to your email authoring experience. Our Enhancements bring complex workflows - scheduling a meeting, creating a survey, formatting a code sample - directly to the email without ever leaving your compose window. Today we're going to solve another excruciating painpoint around email workflow. One that we've all felt every day, and wasted countless hours working around... Embedding a hilarious GIF in your email. That's right, we're going to make a Mixmax Enhancement that makes it really easy to search for and embed an animated GIF in your email. Like the previous parts of the blog series building a Slash Command and Link Resolver , we're continuing with the Giphy theme. This time we're building the full-on Mixmax Enhancement. Fortunately, we've open sourced some code to help you get started. Here are the steps: Ensure that you have git and node (>=0.12) installed on your system Check out the open source Giphy Mixmax command by running git clone https://github.com/mixmaxhq/giphy-mixmax-app Inside the directory, run npm install and then npm start . You shouldn't see any errors and it should say [nodemon] starting `node server.js` Restart Chrome in a special temporary mode so the self-signed HTTPS urls. See here . Verify it works by visiting https://localhost:8910/editor in your browser. Go to Mixmax Integration Settings Click Add Enhancement and enter the following values. (note that Giphy is already built-in, so we're calling it My Giphy ) Name Value Name My Giphy Icon Tooltip My Giphy Picker Editor URL https://localhost:8910/editor Resolver API URL https://localhost:8910/api/resolver Activate API URL (leave blank) Now refresh Gmail, open a new Mixmax compose window, hover over the Enhancement picker and find \"My Giphy\" at the top. Search for and insert your favorite GIF! Use cases: anyone at your company can click Internal Knowledgebase in the Enhancement menu to bring up your company's knowledge base support system, and then insert a link for a customer anyone at your company can click File Ticket in the Enhancement menu to bring up a new support ticket form, and then inserts a public link to it in your email any Mixmax user can find your company's brand in the Enhancement menu and access your product catalog, and then insert a link in their email to share with friends Interested in working on an email platform of the future? Email us at careers@mixmax.com and follow us @Mixmax .", "date": "2015-11-30"},
{"website": "Mix-Max", "title": "Building Your First Link Resolver", "author": ["Andrew Tamura"], "link": "https://www.mixmax.com/engineering/giphy-link-resolver/", "abstract": "This post is part 3 in an engineering blog series about building on top of the Mixmax SDK . See part 1 and part 2 . If you're ever on Giphy.com and find an awesome GIF to put it in your email, you have to download the image to your computer and then upload it in to your email. But not with Mixmax - we believe in bringing the web to your email authoring experience. So in Mixmax when you paste the Giphy URL such as http://giphy.com/gifs/excited-the-office-yes-t3Mzdx0SA3Eis into your email, it'll be automatically replaced with the actual GIF. Magic . At least that's how it should work. We first need to build it using the Mixmax SDK Link Resolver framework. Fortunately we've written some open source code to get you started. Here are the steps: Ensure that you have git and node (>=0.12) installed on your system Check out the open source Giphy Mixmax command by running git clone https://github.com/mixmaxhq/giphy-example-link-resolver Inside the directory, run npm install and then npm start . You shouldn't see any errors and it should say [nodemon] starting `node server.js` Restart Chrome in a special temporary mode so the self-signed HTTPS urls. See here . Verify it works by visiting https://localhost:9146/resolver?url=http%3A%2F%2Fgiphy.com%2Fgifs%2Fexcited-the-office-yes-t3Mzdx0SA3Eis in your browser. It should show JSON results. Go to Mixmax Integration Settings Click Add Link Resolver and enter the following values: Input Name Value Description Giphy (giphy.com/gifs/*) Regular Expression giphy.com/gifs/[^\\/]+-[^\\/]+$ Resolver URL https://localhost:9146/resolver Now refresh Gmail and you should be able to paste in the Giphy URL http://giphy.com/gifs/excited-the-office-yes-t3Mzdx0SA3Eis and have it be replaced with the real GIF. Explore the code and make changes, and build your own! Use cases: anyone at your company can paste a URL to an internal knowledgebase and a snippet from the article will be put in the email any Mixmax user pasting a link to your company's public product page will get a rich preview of the company offerings Interested in working on an email platform of the future? Email us at careers@mixmax.com and follow us @Mixmax .", "date": "2015-11-23"},
{"website": "Mix-Max", "title": "Building Your First Slash Command", "author": ["Chanpory Rith"], "link": "https://www.mixmax.com/engineering/giphy-slash-command/", "abstract": "This post is part 2 in an engineering blog series about building on top of the Mixmax SDK. See part 1 here . Let's walk through step-by-step how to build a new /giphycats command using the Mixmax SDK. This command is a spin on the built-in /giphy command that appends the word \"cat\" to Giphy searches. After all, your email needs more cats. Ensure that you have git and node (>=0.12) installed on your system Check out the open source Giphy Mixmax command by running git clone https://github.com/mixmaxhq/giphy-example-slash-command Inside the directory, run npm install and then npm start . You shouldn't see any errors and it should say [nodemon] starting `node server.js` Restart Chrome in a special temporary mode so the self-signed HTTPS urls. See here . Verify it's working by visiting https://localhost:9145/typeahead?text=happy and https://localhost:9145/resolver?text=happy in your browser. It should show JSON results. Go to Mixmax Integration Settings Click Add Slash Command and enter the following values: Input Name Value Name Giphy Cats Command giphycats Parameter placeholder [search] Typeahead API URL https://localhost:9145/typeahead Resolver API URL https://localhost:9145/resolver Now refresh Gmail and you should be able to type /giphycats in a new Mixmax window: However, this command isn't living up to its name - it needs cats! Let's fix that. Open up the file api/typeahead.js and replace the line q: term, with q: term + ' cats', . Now all search terms will be automatically appended with the word cats . Now try: Much better! Now you have more cats in your email. Feel free to explore around and implement a Slash Command of your own. Use cases: customer support at your company can type /kb <search term> to bring up a list of knowledgebase articles to insert a link to in the email any Mixmax user can search your product catalog by typing /yourbrand <search term> to share a link with their friends Interested in working on an email platform of the future? Email us at hello@mixmax.com and follow us @Mixmax .", "date": "2015-11-17"},
{"website": "Mix-Max", "title": "Turn-key Electron apps with Meteor", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/turnkey-electron-apps-with-meteor/", "abstract": "Last month, we released our first desktop application to rave reviews . After several\nimprovements and the release of the Windows version, we’re pulling back the curtain to reveal how we\nbuilt the app. When we decided to make a desktop application, Electron was\nan easy choice. Using a cross-platform framework would allow us to reach the most users, critical for\na business at our early stage. Using a framework based on web technologies would allow us to reuse\nparts of our web application—and best leverage the talent behind it, in particular our designer ’s marvelous HTML and CSS skills. And using a\nframework based on Chromium would give us access to the same bleeding-edge web APIs we use in our web\napplication in our desktop app. But with that choice made, there were still a number of questions remaining: how would we… load our web application in Electron? package the app? communicate between the web and native parts of the app? distribute the app? To answer these questions, we reviewed a ton of Electron documentation, GitHub issues, and prior\nart. The result is a Meteor package, meteor-electron ,\nthat makes it as easy as meteor add quark:electron to get your Meteor application running\nin a native shell. If you’d like to try it out, head on over to the GitHub repo; but if you’re\ninterested in its development, read on! Preface We developed our Electron infrastructure as a Meteor package because the web application we were\nwrapping was a Meteor application, but the techniques and lessons described below would in large part\napply to packaging any web application (using Rails, Django, Express, etc.) as an Electron app. With that said, Meteor facilitates a particularly smooth workflow in that its packages can hook\ninto the build process, server, and client. As you will see, Electron development requires\ncoordinating changes across all three of those areas. Meteor in the cloud, Electron on the desktop We first came across a prototype of meteor-electron when deciding how to load our Meteor app in\nElectron: remotely or locally? There were several packages that promised to do the latter—build and\nrun Meteor inside the Electron app. But that decision came with limited support for building the app for multiple\nplatforms. The primary advantage of the local approach seemed to be offline support—but we didn’t need\nthat for v1. More important to us was getting a cross-platform app out as soon as possible and being\nable to quickly iterate upon it. That made the case for loading Meteor remotely even more strongly,\nsince we could use Meteor’s hot code push to update the bulk of the app. Automatic packaging We were also attracted to meteor-electron by its focus on ease of use. Where other projects\nforegrounded customization and build processes, meteor-electron got us started right away. When we\nadded its package to our app, it automatically downloaded the Electron binary for our system, packaged\nour application, and opened it in a native shell. The packaging step bears some discussion: the officially-supported way to package an Electron app\nis to manually edit\nthe contents of the Electron binary ! That’s crazy. Under the hood, meteor-electron uses the\nexcellent community project electron-packager to bundle your code and resources. What’s more, meteor-electron uses reasonable defaults for packaging, easily customizable through\nyour Meteor.settings file. This turn-key packaging was a great user experience, and one that we’ve enhanced working with\nmeteor-electron’s creator, Mike Risse . For instance, we’ve\nsince improved the project to automatically rebuild and relaunch the app when the native code or the\napplication settings change. Safe native bridge Once we had our application loading in meteor-electron, we started to consider how we could\ncustomize it for the native context. Some simple feature detection APIs were trivial to add thanks to meteor-electron’s ability, as a Meteor package, to load code on the server\n(like the packaging logic) and the client. Providing the client with access to native capabilities was trickier. Or should I say, it was too easy: by default, all Node APIs and modules are accessible from web content. Electron assumes that you will be loading your HTML etc. from disk, and thus views it as a trusted\nextension of the native code. But this assumption fails disastrously for hybrid native-web applications\nlike Meteor desktop apps. <script>\n  require('child_process').exec('rm -rf /*'); // Worst XSS attack ever.\n</script> Thankfully, it is possible to disable this Node integration when opening browser windows. (Bonus:\ndoing so can fix web dependencies failing to load by incorrectly assuming that they’re running in a CommonJS environment .) We do still wish to expose certain native APIs to the web content. Luckily, there exists\na mechanism for doing so called a “preload script”. This script is loaded before other\nscripts run in the page; always has access to Node APIs, no matter whether Node integration is turned\non or off; and can add symbols to the page’s global scope. With Node integration turned off: Native scripts Preload script Browser scripts Access to Node global context < > Access to browser global context < > This means that the preload script can provide the webpage with indirect access to select Node APIs: // Local variables will not escape the preload script.\nvar shell = require('electron').shell;\n\n// Global variables _will_ escape the preload script, except for globals injected by Node,\n// like `require`—-those will be deleted after the script is done executing.\nElectron = {\n  openExternal: function(url) {\n    shell.openExternal(url);\n  }\n  // Notice we don’t save `shell` as a property of `Electron`! Then it’d be available\n  // to the webpage.\n}; <script>\n  // Successfully uses the `shell` module as prescribed by the preload script.\n  Electron.openExternal('https://mixmax.com/download');\n\n  // Logs \"Uncaught ReferenceError: shell is not defined(…)\"\n  shell.moveItemToTrash('~/Documents');\n  \n  // (Ignoring previous line) Logs \"Uncaught ReferenceError: require is not defined(…)\"\n  require('electron').shell.moveItemToTrash('~/Documents');\n</script> meteor-electron’s use of a preload script thus makes it possible for Meteor apps to safely\nmake use of native functionality. Distribution: to be continued… You’ve seen thus far how meteor-electron offers Meteor developers a simple and safe path to creating\ndesktop applications. In part 2 of this blog post, we’ll talk about how meteor-electron adds support\nfor distribution, even automatic updates, without developers having to add any server-side code\nthemselves. Tips for converting webapps to desktop apps? Discuss on Hacker News , tweet us @Mixmax or drop us a line at careers@mixmax.com . You can also come to Meteor Night 1/21 to see\nMike and Jeff demo meteor-electron live. Interested in solving hard problems like this? Come join us at Mixmax.", "date": "2016-01-11"},
{"website": "Mix-Max", "title": "Taming the Imports", "author": ["Eli Skeggs"], "link": "https://www.mixmax.com/engineering/taming-the-imports/", "abstract": "Performance is critical to every web application, and something we care deeply about at Mixmax.\nThat's why we invest in performance best-practices, such as limiting the size of the Javascript that\nwe serve for our application. We use a tool called rollup.js , which performs\ncode\n\" tree-shaking \"\nat build-time. Tree-shaking, in contrast to dead code removal, uses explicit imports to only include\nwhat you use in the resulting code bundle. When we adopted Rollup, we were happily forced to adopt ES6\nimports . However,\nreferencing specific modules threatened to become a relative-pathing nightmare. Our codebase lives\nin a directory structure which includes subdirectories for things like views and models, and has\ngrown pretty deep, so we have to use paths like: import UserModel from '../../../../../../../models/user'; However, some tools like Babel have plugins that support an absolute-style\nimport, where the absolute path refers to the root of the project. This would reduce our relative\nimport, above, into simply: import UserModel from '/models/user'; We couldn’t just use Babel's plugin, because rollup.js needs to resolve the modules and shake the\ntree before Babel runs, so we wrote our own\nplugin for rollup that supports importing\nfrom the root of the project. Cheers! README Add the ability to import modules by the root path, like babel-root-slash-import . // import a module relative to a specified root directory\nimport UserModel from '/models/user';\n\n// illustrative - if we were in /views/user, we can reference a model's module\n// using an absolute path from the root of the project, rather than relative to\n// the current module\nclass UserView {\n  // ...\n} Install $ npm install --save-dev rollup-plugin-root-import Usage import {rollup} from 'rollup';\nimport rootImport from 'rollup-plugin-root-import';\n\nrollup({\n  entry: 'client/src/main.js',\n  plugins: [\n    rootImport({\n      // Will first look in client/src/* and then common/src/*\n      root: 'common/src',\n      useEntry: 'prepend',\n\n      // If we don't find the file verbatim, try adding these extensions\n      extensions: '.js'\n    })\n  ]\n}); API rootImport( options ) Creates a rollup plugin to resolve absolute-pathed imports relative to the project's entry or\nspecified root directory/ies. Options: root an optional string or ordered array of strings, which represent the roots from which to try\nand resolve imports. useEntry if provided, should be either 'prepend' or 'append' , where each signify that the\ndirectory containing the entry should be prepended or appended to the array of roots,\nrespectively. By default, if root is provided and useEntry is not provided, the directory of\nthe entry module will not be used to resolve absolute-pathed imports. extensions , if provided, specifies a string or ordered array of strings, each of which\nrepresents an extension to append to the resolved file if the import isn't found verbatim.", "date": "2016-07-28"},
{"website": "Mix-Max", "title": "Requiring Node Built-ins with Webpack", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/requiring-node-builtins-with-webpack/", "abstract": "Webpack is infamous for being complicated, but it actually does quite a lot for you out of the box. Bundling a\nNode/CommonJS module for the browser can be as easy as webpack index.js bundle.js But when you look at what Webpack has produced, you may find that it has gone too far: its automatic\nshimming of Node built-ins can add hundreds of kilobytes of unused code to your bundle, and encourage\ndevelopers to use 3rd-party re-implementations of Node built-ins rather than perfectly good browser APIs. The Problem At a high level, what webpack index.js bundle.js does for you is convert index.js ’ require calls\ninto a format that’ll work in the browser. If you have modules as such: // salutation.js\nmodule.exports = 'Hello';\n\n// index.js\nvar Salutation = require('./salutation');\nmodule.exports = Salutation + ' world!'; webpack index.js bundle.js will produce a file containing (in part): function(module, exports, __webpack_require__) {\n  var Salutation = __webpack_require__(1);\n  module.exports = Salutation + \" world!\";\n} You can see how it has defined module for index.js , and replaced require('./salutation') with a\ncall to a __webpack_require__ function. This is very simple and entirely unproblematic so long as your require s are for your own,\npure- TC39 modules. But what if you require a Node module like os ? Well, Webpack will automatically handle that too. I can’t find this documented anywhere ,\nbut if you require a Node built-in or use a Node global, Webpack will download a browser shim for it and bundle that with your code. This lets\nyou do something like var os = require('os');\n\n// Webpack's shim for `os.platform` returns 'browser'.\nif (os.platform() === 'browser') {\n  console.log('in the browser');\n} else {\n  console.log('in Node');\n} However, Webpack’s test for whether you need the shim is painfully simple: if you use the Node\nbuilt-in anywhere in your file, Webpack will add its shim to your bundle—regardless of whether that\ncode will be evaluated. This bit me when developing a cross-platform library that\nmade use of cryptographic functionality. Since I was only targeting modern browsers, I intended to\nuse Node’s crypto module in Node, but SubtleCrypto in the browser. I\ntried to accomplish this with the following code: var os = require('os');\nvar IS_BROWSER = os.platform() === 'browser';\n\nvar crypto;\nif (IS_BROWSER) {\n  crypto = window.crypto;\n} else {\n  crypto = require('crypto');\n} Only to find that Webpack had still replaced the require('crypto') call … with 100kB of unused code . Ouch. Further compounding the problem was Webpack’s documentation. I eventually found two ways of\nsuppressing this behavior. Solutions First, you could mark crypto as an external :\nin your webpack.config.js file (no more simple\ncommand-line usage, alas), do: module.exports = {\n  externals: {\n    'crypto': 'crypto'\n  }\n}; This means that Webpack will attempt to import crypto from the environment at runtime, rather than\nbundling its definition: require('crypto') will end up executing code that looks like this: function(module, exports) {\n  module.exports = crypto;  // i.e. `window.crypto`\n} If crypto and SubtleCrypto had identical APIs, this could have actually let me use require('crypto') in both Node and the browser. Unfortunately, they don’t (most notably, crypto ’s\nAPIs are synchronous whereas SubtleCrypto uses promises), so I had to use if (IS_BROWSER) conditionals throughout the module anyway. And given that, the external definition above was still a\nbit of unused code. The second way of suppressing the shim is to make Webpack aware that the !IS_BROWSER branch is dead code . In webpack.config.js ,\ndo: module.exports = {\n  plugins: [\n    new webpack.DefinePlugin({\n      IS_BROWSER: true\n    })\n  ]\n}; Now, since IS_BROWSER is a compile-time constant ,\nWebpack will only crawl the true branches: // This condition is `(typeof IS_BROWSER === 'undefined')` in Node, pre-compilation.\nif (false) IS_BROWSER = false;\n\nvar crypto;\nif (true) {\n  crypto = window.crypto;\n} else {\n  crypto = require('crypto');\n} And the false branches can even be removed during minification! Since this approach was truest to my intention, and the most efficient (no need for the os shim,\nno “external” crypto definition, and with Node-specific code stripped during minification), this\nis the approach I ended up taking. Conclusion I think automatic shimming is a reasonable default for Webpack. It should be obvious to developers\nthat something has to happen to make requiring a Node built-in work in the browser, and indeed I knew\nthat Webpack would shim os —I just didn’t think about it shimming crypto as well. However, I think Webpack should be a lot more transparent about what it does. At a minimum, I think\nit should document when it shims, what it shims, and how shimming can be disabled without developers\nhaving to examine 3-year-old comment threads and examples that talk only about 3rd-party browser libraries . I also think that Webpack should suggest that developers use browser APIs where available rather than\nrelying on the shims. SubtleCrypto is almost equivalent to crypto , the former's use of promises\naside—developers can save 100kB of JS, and worry less about correctness, by using the browser's API\nvs. a 3rd-party implementation. If you agree with any/all of the above, please upvote/chime in on https://github.com/webpack/webpack/issues/2871 . And if you’d like to work in open-source software and use cutting edge JS tech, however painful at\ntimes 😉 , email careers@mixmax.com and let’s grab coffee!", "date": "2016-08-16"},
{"website": "Mix-Max", "title": "Redfour: Binary Semaphores in Redis and Node", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/redfour-semaphore-redis-node/", "abstract": "We just released a small module called Redfour that could be very helpful to you if you’re using Node in a distributed system. It implements a binary semaphore using Redis. What's a binary semaphore? It’s a pattern that ensures that only one process (e.g. server in your cloud) has access to a critical section of code at a time. All other processes must wait for access. When the first process is done, another process will be notified and given access. Example Let’s say you have Node code that checks for expired OAuth2 access tokens and refreshes them, like this: function getAccessToken(userId, done) {\n  var token = getTokenForUser(userId);\n  if (token.expires < Date.now()) {\n    done(null, token);\n  } else {\n    refreshToken((err, res) => {\n       if (err) done(err);\n       else done(null, newToken);\n    });\n  }\n} Now let's say your user is loading part of your app that uses several APIs behind-the-scenes, each of which need a fresh OAuth2 access token to a service. Those APIs (which might be implemented in different services) are all going to call getAccessToken at the same time - resulting in refreshToken being called needlessly. A better approach is for the processes to take turns asking for the token, so the first calls refreshToken and the rest get the cached result. This is where Redfour can help. This same code using Redfour ensures only one process has access to the critical codepath at once, so refreshToken will only be called once. var accessTokenLock = new Lock({\n  redis: 'redis://localhost:6846',\n  namespace: 'getAccessToken'\n});\n\nfunction getAccessToken(userId, done) {\n  // Make sure only one user is being refreshed at a time.\n  var lockTTL = 60 * 1000;\n  var waitTTL = 30 * 1000;\n  accessTokenLock.waitAcquireLock(userId, lockTTL, waitTTL, (err, lock) => {\n    if (err || !lock.success) return done(new Error('Could not get lock!'));\n\n    var token = getTokenForUser(userId);\n    if (token.expires < Date.now()) {\n      accessTokenLock.releaseLock(lock, (err) => {\n        if (err) done(err);\n        else done(null, token);\n      });\n\n    } else {\n      refreshToken((err, newToken) => {\n        if (err) return done(err);\n\n        accessTokenLock.releaseLock(lock, (err) => {\n          if (err) done(err);\n          else done(null, newToken);\n        });\n      });\n    }\n  });\n} How does this differ from other locking solutions? Other locking solutions such as the popular warlock use polling to wait for a lock to be released. This is inefficient and slow, as there might be a delay when being notified that the lock is available. Credit Credit for this design goes to Andris Reinman who worked with us on our email backend fixing several tricky race conditions.", "date": "2016-09-14"},
{"website": "Mix-Max", "title": "Watch Jeff Present the Electron app at Meteor Night", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/meteor-night-jan-2016/", "abstract": "Check out Mixmax engineer Jeff Wear and Meteor open-source contributor Mike Risse presenting our new Mixmax app at Meteor Devshop in San Francisco on January 21, 2016.", "date": "2016-01-25"},
{"website": "Mix-Max", "title": "Mixmax Advent 2016", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/mixmax-eng-advent-2016/", "abstract": "Over the next 12 days we’ll be sharing one engineering post per day, carrying forward a long internet tradition of blog advent calendars. Follow @mixmax on Twitter for the latest posts or check back here daily for an updated list. Our posts will be about our stack and interesting solutions to engineering problems we've encountered. We hope to give back to community what the community has given to us. We also look forward to hearing feedback! Tweet the author of the post (linked to on the right of each post) with your feedback. Open Source Culture Upgrading to Node 6 on Elastic Beanstalk A quick look on some of the new features of Node 6.x CORS: How I Learned to Stop Worrying and Love HTTP Access Control Securely signing requests with Rewt 30x Faster Elasticsearch Queries Rewriting 30,000 lines of code in a friendly way Adventures in the Gmail PubSub API CPU profiling node.js processes live in production Fun with the Mixmax REST API Integration testing for humans How to write an engineering blog post", "date": "2016-12-01"},
{"website": "Mix-Max", "title": "An Open Source Culture", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/open-source-culture/", "abstract": "Open Source Culture Mixmax’s engineering team was founded on the principle of openness. From our very beginning just two years ago, we’ve embraced an “open source culture”: a set of core values that promote transparency and collaboration within our company and with the larger engineering community. We’ve built on entire platform on open source software, from MongoDB, ElasticSearch, and Redis in our backend, to Node and numerous npm modules in the application layer, to Backbone and Electron in the frontend. We’ve contributed to these projects and launched many of our own as well. We’ve blogged extensively, given talks, and engage daily with the community on Twitter and in various Github issues. Some facts: 34 open source projects and 382 Github stars (and growing, fast!). Mixmax has more open source projects than private projects. More than half of the APIs used by our product are open . 19 engineering blog posts . That’s one per 6 weeks on average since the company’s beginning. Significant contributions to popular open source projects: Meteor, Electron, Nodemailer, Bull Queue, AWS SDK Multiple talks at Meteor Devshop and the Bay Area Electron meetup Over 100k unique views for our engineering blog posts Our approach to open source isn’t an afterthought; it’s how we approach building new things. We think of our business as first and foremost to serve our customers, and secondly to push technology forward to serve the greater engineering community. When we begin to build a feature we scope it in that order: 1) How will this meet our customer’s needs, and 2) How can we build this in an “open source” way such that others can use and extend it. This approach in building new features has led to the creation of many of our open source npm modules . Many of those used to be either embedded in a single service or copied file-by-file between services. When we created an npm organization we became able to share these as private modules, but we decided to invest in making the majority of them public. The biggest criticism of an open source culture is the additional overhead, or “overengineering”, slows down product development. It’s something we’re mindful of. We’re careful not to over-generalize our open source modules, or to build solutions to problems we don’t have. But ultimately, we believe writing good, transparent, open-sourceable code, and keeping our dev efforts focused on delivered value for our customers are complementary, not contradictory goals. Open-source development holds our own engineers to a higher standard for modularity, testing, and documentation. Over the long term, community support speeds development and mitigates risk. The initial cost of open-source development is offset very quickly. Open source will remain central to our culture at Mixmax as we scale up and expand to other languages and technologies. If you’re interested in working in an open source culture, come join us !", "date": "2016-12-01"},
{"website": "Mix-Max", "title": "Node 6 LTS is finally here", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/node-6-lts-is-finally-here/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 2nd was about Upgrading to Node 6 on Elastic Beanstalk . The long-awaited stable LTS version of Node 6.x was released last October, and with it, many\nperformance and security improvements, and new ES6 syntax features are now natively supported as\nwell. While some of the new ES6 features can be considered syntax sugar, they also open the door\nfor much more concise and understandable code, as well as opening for metaprogramming capabilities.\nWe will now explore some of these features and how it compares to “old” code. Default arguments, spread operator and destructuring. One common use case is to have a function that has multiple arguments where one or more of them are\noptional. Such a function would look like something similar to: function makeSandwich(customer, bread, filling) {\n  bread = bread || 'wheat';\n  filling = filling || 'ham';\n\n  // Make the sandwich\n} We can make that code more concise and simpler to understand by using default arguments in our\nfunction: function makeSandwich(customer, bread = 'wheat', filling = 'ham') {\n  // Make the sandwich.\n} How about if we pass our sandwich ingredients as an array? function makeSandwich(customer, ingredients) {\n  const bread = ingredients[0];\n  const filling = ingredients[1];\n\n  // Make the sandwich\n} We can make the syntax more concise with the spread operator now! function makeSandwich(customer, [bread, filling]) {\n  // Make the sandwich\n} If we want to retain the ability to keep default ingredients, then we can combine the two features\nlike so: function makeSandwich(customer, [bread, filling] = ['wheat', 'ham']) {\n  // Make the sandwich\n} Unfortunately, the above will only work as long as the second parameter is undefined, if we pass a\nvalue for bread but not for filling, then filling won't be defaulted to 'ham' as expected. What if we want to keep our list of ingredients support open and allow for new ingredients later?\nThen we can use destructuring and rest ! function makeSandwich(customer, ...ingredients) {\n  const [bread, filling] = ingredients;\n\n  // Make the sandwich\n} Later when we can add toppings and sides to our sandwich, we can add that to our destructuring\nsentence: function makeSandwich(customer, ...ingredients) {\n  const [bread, filling, toppings, sides] = ingredients;\n\n  // Make the sandwich\n} Maybe an array is not the best representation of our ingredients, we can use an object and still use\ndestructuring to assign properties to local variables: function makeSandwich(customer, ingredients) {\n  const { bread, filling, toppings, sides } = ingredients;\n\n  // Make the sandwich\n} An unoptimized quicksort-like algorithm that uses the spread operator to perform array\nconcatenations looks like this: function quicksort(list) {\n  const size = _.size(list);\n\n  if (size === 0) return [];\n  if (size === 1) return list;\n\n  const [pivot, ...rest] = list;\n  const [left, right] = _.partition(rest, item => item < pivot);\n\n  return [...quicksort(left), pivot, ...quicksort(right)];\n} Although not a real quicksort because it does not sorts in-place, it demonstrates how concise\nJavaScript code can now be thanks to destructuring and the spread operator in particular. Map and Sets The Map and Set objects are actually supported since Node 4.x, however some details were fleshed out until the Node 6.x releases. Map the Map is a key/value data structure (similar to plain objects). A Map , like a plain object,\ncan store values identified with a key, but unlike an object, we can have some guarantees about\nthese Map and our keys: When iterating over our map, values are retrieved in insertion order. Unlike an object, a Map does not have extraneous keys from the prototype inheritance They are iterable with for .. of Provides useful functions to interact with the map, such as Map#entries , Map#keys , Map#has Map#forEach , among others // A Map can be instantiated with some initial values, pass an array with arrays whose\n// first value is the key and the second is the value to store in the mappings\n\nconst map = new Map([\n  ['one', 1]\n  ['two', 2]\n]);\n\nmap.set('three', 3);\n\nmap.has('two'); // true\nmap.has('three'); // true\nmap.has('four'); // false\n\nmap.forEach((key, value) => {\n  console.log(\"%s: %s\", key, value);\n});\n\nfor(const [key, value] of map) {\n  console.log(\"%s: %s\", key, value);\n}\n\nconsole.log(map.get('one')); // 1\nconsole.log(map.size); // 3 Set The Set is a particularly useful data structure, it has the property that the Set is a list of\nvalues that can't be repeated.\nSimilar to Map , the Set object can be interacted with similar methods const set = new Set();\n\nset.add(1);\nset.add(2);\nset.add(3);\n\nconsole.log(set); // Set { 1, 2, 3 }\n\nset.add(3);\n\nconsole.log(set); // Set { 1, 2, 3 }\n\nconsole.log(set.size); // 3\n\nfor (const item of set) {\n  console.log(item);\n}\n\n// You can create an array from a set with:\nlet arr = Array.from(set)\n\n// ... or use the spread operator\narr = [...set]; Metaprogramming with ES6 One of the not-so-talked features of ES6 is the meta programming capability brought to the table\nthanks to Symbol and Proxy classes. Proxy The Proxy object adds the ability to intercept attribute access to the proxied object. For example, an\ninteresting use case is given by the mongojs library, the Mongo database connection is exposed as a\nProxy object where you can access your MongoDB collections as properties of said object. For example: db.users.find({ /* ...  */ }) Code to enable the above snippet could look something like: const myCollections = require('path/to/my/collections');\nconst myConnection = require('path/to/my/db/connection');\n\nconst proxy = new Proxy(myConnection, {\n  get(conn, prop) {\n    if (Reflect.has(myCollections, prop)) {\n      return myCollections[prop];\n    }\n\n    return conn[prop];\n  }\n});\n\nmodule.exports = proxy; Our proxy object has the target as its first argument, and as a second argument, a handler object\nthat implements the intercepting functions which will be called when attempting to access an\nattribute in the proxy. In this implementation, we define a get function, which will intercept every single attribute\naccess to the proxied object. In our implementation here, we use the new Reflect class which provides several utilities functions to safely inspect other objects, here we check if\nthe accessed property is defined in our collections object, if it is, then we return that, otherwise\nwe delegate the call to the original object. Symbol The Symbol is a new data type, which has some interesting characteristics: It is not instantiable, you can create new symbols with: const foo = Symbol(‘foo’); Two defined symbols are never equal: const a = Symbol(‘foo’);\nconst b = Symbol(‘foo’);\n\na === b // false Properties defined with symbols are not enumerable: const foo = {\n  one: 1,\n  two: 2\n};\n\nconst three = Symbol('three');\n\nfoo[three] = 3;\n\nObject.keys(foo); // ['one', 'two'] Symbols, then, can be used as special object properties that “hide” some data inside the objects.\nNote that, a list of Symbols in an object can still be accessed with Object.getOwnPropertySymbols() so these properties are not completely hidden, but merely separated\nand handled differently than regular properties. You want to use a Symbol when you want to store object metadata that you don’t want to expose if you\nintend your object to be iterated with for … of loops, or properties that you want to hide when\nserializing with JSON.stringify . Another very interesting use case is for implementing interfaces in classes. Javascript exposes\nwell-defined Symbols, one of the most approachable ones is the Symbol.iterator symbol, if your\nclass implements a generator function defined as the Symbol.iterator property, then your class\ncan be looped with for..of and expanded with the spread operator “...” Let’s make a very silly example, we’ll implement a random iterator where, given an initial list of\nvalues, it iterates them at “random”. function random(a, b) {\n  return Math.floor(Math.random() * (b - a + 1)) + a;\n}\n\nclass RandomIterator {\n  constructor(values) {\n    this.values = values;\n  }\n\n  *[Symbol.iterator]() {\n    const values = this.values.slice(0);\n\n    while (values.length > 0) {\n      const next = random(0, values.length - 1);\n      const [value] = values.splice(next, 1);\n\n      yield value;\n    }\n  }\n}\n\nconst iterator = new RandomIterator([1,2,3,4,5]);\n\n// This will print the values on our RandomIterator in random order.\nfor(const i of iterator) {\n  console.log(i);\n}\n\n// You can also use the spread operator!\n\n[...iterator] // Will print the list in the iterator in random order\n[...iterator] // Calling multiple times will return the items in different order. Symbols can open up several ways on how you can interact with your own data structures using plain\njavascript code, also, using Proxies and Symbols you can create a whole new level of meta\nprogramming that was not available until now! Conclusion The new syntax introduced with ES6 allows to express ideas and abstractions in a more concise way.\nSymbols for example open up the idea of mixins implemented in a different way. Default args make it\nclearer the signature of a function without checking the implementation. Spread and Destructuring\nmakes it easier to work with arrays and objects. The Map and Set classes give a better idea of an\nintention of some variable (A map is a key/value pair, a set is a list of items without duplicates)\nand so on. Many of these ideas could be implemented with ES5 code, but the new syntax makes it so\nthat these ideas and intentions are explicit and clear by just looking at the code. Do you want to take advantage of the new syntax offered under Node 6.x? Come join us .", "date": "2016-12-03"},
{"website": "Mix-Max", "title": "CORS", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/cors/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 3rd was about Node 6 features . The Error XMLHttpRequest cannot load https://contacts-local.mixmax.com/api/contactgroups?user=engtestuser4%40mixmax.com&expand=userId. Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'https://app-local.mixmax.com' is therefore not allowed access. The response had HTTP status code 403. This error is probably why you are here. You are looking for a fix. Or maybe you're just curious to learn more about CORS. Whatever the reason, you've come to the right place. Read on to learn more about CORS or skip to the end to figure out how to fix your dilemma. What is CORS? CORS or Cross-Origin Resource Sharing is a method for allowing a web page to access resources outside the domain from which the page is being loaded. You might ask why this is needed; why can't I just load arbitrary JavaScript from random places on the internet? One good reason is cookies. When a browser makes a request to a server in a domain, it sends the cookies that it has for that domain. This means that if I am logged into www.myUnsecureBank.com and I load www.myMaliciousWebsite.com in another tab, the malicious website can fetch data from my bank on my behalf. This works because the malicious site can contain JavaScript that makes AJAX requests outside the malicious website's domain and the browser will happily send cookies with that request. So the malicious website can call the www.myUnsecureBank.com/api/myBankDetails and the server will respond because the browser will send the cookies that identify you as being logged in. Fortunately, because of SOP or Same Origin Policy , the browser will block this outbound request. This is where CORS comes in. CORS is a method that will allow us to make these types of requests, under the right circumstances. How does it work? Essentially, there are two types of cross origin requests: Simple and Preflighted . Simple requests are typically GET , HEAD , and POST requests with specific headers and allowed values for Content-Type . You can learn more about them here . Preflighted requests are, well, any request that is not a Simple Request (we will learn more about why they are called preflighted in a bit). For a Simple request, the browser will send the request with an Origin header and the server will reply with an Access-Control-Allow-Origin header. If the two don't match, the browser will produce a CORS error and not allow the request to complete. For a Preflighted request, the browser will send an OPTIONS request, before making the main request (which is why it is called a preflighted request). The server will reply with Access-Control-Allow-Origin but also with a Access-Control-Allow-Methods header. This header specifies the type of requests the client is able to make ( POST , PATCH , ...). If the main request does not match both headers, then the browser will produce a CORS error and not make main the request. How do I play by the rules? In order to make CORS work, you need server side changes. For simple requests, you simply have to make your server respond with the appropriate Access-Control-Allow-Origin header. For preflighted requests, however, you have to also respond with the correct Access-Control-Allow-Methods header for the browser's OPTIONS request. At Mixmax we like to use the cors package for handling these. We have a Contacts service that runs under the domain contacts.mixmax.com . We also have our main web app which runs under the domain app.mixmax.com . Because our App makes all types of requests to our Contacts service, we have to handle CORS on the Contacts side. This is what our router looks like on our Contacts service: var cors = require('cors'); // Import the CORS library\nvar express = require('express');\nvar router = express.Router();\n\n// Configure CORS for our own services\nvar mixmaxCors = cors({\n  origin: function(origin, callback) {\n    // if origin is a subdomain of mixmax.com then allow the request\n    var originIsWhitelisted = /[^.\\s]+\\.mixmax\\.com$/.test(origin);\n    callback(null, originIsWhitelisted);\n  },\n  // credentials = true is required for cookies to be passed along\n  credentials: true\n});\n\n// Define the OPTIONS route that will be preflighted by the browser\nrouter.options('/api/contactgroups/?*', mixmaxCors);\n// Define the actual API routes\nrouter.use('/api/contactgroups', require('./api/contactgroups')); Conclusion CORS is a feature, not a problem. It allows the modern web to exist by allowing web applications to use all sorts of APIs from different domains. Next time you see The Error , think about how it brings safety to your users and how CORS is an elegant way to handle the situation. Enjoy building elegant APIs? Drop us a line .", "date": "2016-12-04"},
{"website": "Mix-Max", "title": "30x Faster Elasticsearch Queries", "author": ["Cameron Price-Austin"], "link": "https://www.mixmax.com/engineering/30x-faster-elasticsearch-queries/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 5th was about securing server-side requests with JWT tokens . tl;dr - we achieved a 30x performance improvement in Elasticsearch queries by switching from millisecond timestamps to seconds. The problem The live feed in Mixmax queries across half a billion documents in order to produce a chronological feed of your activity. We use Elasticsearch to power this, but noticed some of our queries performed quite poorly (300-500ms) , particularly during periods of peak load. Digging into the problem, we noticed it was isolated to queries which involved a timestamp. We mostly use Javascript here at Mixmax, so date/times across all our systems tend to be stored as epoch timestamps in milliseconds. Elasticsearch is no exception. Here’s a sample of how our indexes are defined: {\n  userId: { type: 'string', index: 'not_analyzed' },\n  timestamp: {\n    type: 'date',\n    format: 'epoch_millis'\n  },\n  ...\n} Note we’re using Elasticsearch’s built-in date type with format: epoch_millis . You can read more about the different supported date formats in their docs . The solution Recreating a sample cluster locally, we noticed that performance improved dramatically if we stored and queried upon our timestamps in seconds e.g.: {\n  userId: { type: 'string', index: 'not_analyzed' },\n  timestamp: {\n    type: 'date',\n    format: 'epoch_millis'\n  },\n  timestamp_seconds: {\n    type: 'date',\n    format: 'epoch_second'\n  }\n} We then setup some profiling code and ran the following two queries across a test set of 500 million documents: // Run a simple query, sorting by the millisecond timestamp.\nconsole.log(client.search({\n  index: ...,\n  body: {\n    query: {\n      bool: {\n        filter: {\n          { term: { userId: ... } }\n        }\n      }\n    }\n  },\n  sort: 'timestamp:desc'\n}));\n// Output: { took: 268 }\n\n// Run the same query, sorting by the second timestamp.\nconsole.log(client.search({\n  index: ...,\n  body: {\n    query: {\n      bool: {\n        filter: {\n          { term: { userId: ... } }\n        }\n      }\n    }\n  },\n  sort: 'timestamp_seconds:desc'\n}));\n// Output: { took: 7 } That’s over 30x faster . We also tested the improvement under various loads: We were curious if we could improve performance even further by using timestamps rounded to the nearest minute and hour, but sadly we only saw negligible performance gains. All of this was very exciting, but also kinda weird. Why would it make a difference whether we use milliseconds or seconds? We’re Elasticsearch newbs, so we checked in with the experts over at the elastic.co forums . I’ll repeat Adrian Grand’s (Software Engineer at Elastic) answer here: To have acceptable performance for range queries, numeric fields also index some prefix terms. For instance long values (which dates are based on) index 4 values by default: one that identifies all bits, one that identifies the first 48 bits, one that identifies the first 32 bits and one that identifies the first 16 bits. These prefix terms help querying fewer terms at search time, which makes search faster: queries typically try to use these terms that match multiple terms and just need to match exact values on the edge of the range. But since we use a precision step of 16 bits, there can still be op to 2^16=65536 values on the edges. However, if your dates are all multiples of 1000, suddenly, there are only ~66 unique values at most on the edges, which helps querying be faster. He also mentioned that in the next major release (we’re currently on 2.3.3), querying will be tree-based, so the performance profile for these kinds of queries will change. So there you go: moral of the story - don’t use millisecond timestamps in Elasticsearch if seconds will do! Enjoy solving engineering puzzles like this one? Drop us a line .", "date": "2016-12-06"},
{"website": "Mix-Max", "title": "Upgrading to Node 6 on Elastic Beanstalk", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/fixing-npm-on-elastic-beanstalk/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 1st was about Mixmax’s open-source culture . In case you haven’t heard, Node 6 went LTS mid-October ,\nwith AWS Elastic Beanstalk adding support at the end of the month . Since Node 6\npromised support for 99% of ES6 features as well as a host of\nperformance and security improvements, we moved quickly to adopt it. We found it to be very\neasy to upgrade locally — we only had to upgrade a few native dependencies to their latest\nversion to pick up new bindings, and did not have to change any code. Kudos to the Node\nFoundation for a stable release and the community for embracing Node 6 well in advance of LTS. It was not as easy to upgrade Elastic Beanstalk,\nhowever — upgrading the platform version persistently resulted in stuck deploys and rollbacks. Debugging this required exploring\nElastic Beanstalk’s inner workings, but we ultimately made fixing it as simple as installing\na Node package . And not only did that package\nenable us to upgrade Elastic Beanstalk to Node 6, but it also sped up npm install by 95%.\nHere’s how we did it. What went wrong We initially tried to upgrade the platform version in place using the upgrade button on our application’s dashboard. But the configuration deploy never finished — boxes would\njust time out. We then cloned the environment with the latest platform. This initially succeeded, only for further deploys to fail. Watching these deploys fail was agonizing. Elastic Beanstalk’s dashboard gives very little\ninsight into what’s going on during a deploy. But you can easily SSH into the EC2 instances\nand tail the deployment logs. Using the EB CLI tool : eb ssh -i <instance id>\ntail -f /var/log/eb-activity.log This revealed that the boxes were getting stuck running npm install : [2016-12-02T01:17:44.287Z] INFO  [27173] - [Application update app-91d6-161202_011650-stage-161202_011650@28/AppDeployStage0/AppDeployPreHook/50npm.sh] : Starting activity... We were perplexed. EB’s docs said that platform 3.1.0 was using npm 2.15.5, same as the\nprevious platform. What was the difference? We quickly suspected that EB’s docs were wrong, since Node 6.9.1 usually ships with npm 3.10.8. We confirmed this on an EC2 instance: [ec2-user@ip-10-20-4-104 ~]$ export PATH=/opt/elasticbeanstalk/node-install/node-v6.9.1-linux-x64/bin:$PATH\n[ec2-user@ip-10-20-4-104 ~]$ /opt/elasticbeanstalk/node-install/node-v6.9.1-linux-x64/bin/npm -v\n3.10.8 (Note: we reported this to AWS on 11/11/2016 ,\nbut as of 12/01/2016 the docs are still wrong 😞.) We upgraded to npm 3 locally and timed npm install in several of our projects. We found that npm 3.10.8 consistently takes about 2x longer to run npm install than npm 2.15.5 . And on\nthe resource-constrained EC2 instances, it was taking even longer — much longer than the command timeout .\nCloning the environment appeared to fix the problem only because EB uses a longer timeout when\ncreating an environment than when deploying configuration changes to an existing environment. So the fix was going to involve downgrading npm 3 to npm 2 … on every EC2 instance, across\nall of our services, whenever Elastic Beanstalk deployed to a new instance. How could we\nautomate this? ebextensions Luckily, Elastic Beanstalk offers a way to hook into its deploy process: by adding\nconfiguration files to a folder named .ebextensions ,\nyou can add scripts for EB to run during deploy and even overwrite its default scripts. This let us make an ebextension file that would install an npm-downgrading script. (Note:\nthese intermediate scripts are for illustration, not use, since the ultimate set of scripts is way better.) # EB runs deploy scripts in alphabetical order http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html,\n# Node is installed using a script called \"40install_node.sh\", and `npm install` is\n# run using a script called \"50npm.sh\", so we downgrade Node in a script called\n# \"45npm_downgrade.sh\".\n\n\nfiles:\n  \"/opt/elasticbeanstalk/hooks/appdeploy/pre/45npm_downgrade.sh\":\n    mode: \"000755\"\n    owner: root\n    group: users\n    content: |\n      #!/usr/bin/env bash\n\n      EB_NODE_VERSION=$(/opt/elasticbeanstalk/bin/get-config optionsettings -n aws:elasticbeanstalk:container:nodejs -o NodeVersion)\n\n      # Make sure Node binaries can be found (required to run npm).\n      # And this lets us invoke npm more simply too.\n      export PATH=/opt/elasticbeanstalk/node-install/node-v$EB_NODE_VERSION-linux-x64/bin:$PATH\n\n      if [ $(npm -v) != \"2.15.9\" ]; then\n        echo \"Downgrading npm to 2.15.9...\"\n        npm install npm@2.15.9 -g\n      else\n        echo \"npm already at 2.15.9\"\n      fi But now we had the challenge of distributing this file across our 14 microservices . Were we\ngoing to copy-and-paste it? No way! install-files Awhile back, we made an npm package precisely to solve the problem of distributing files like\nthis ebextension. The package is called install-files , and what it does is allow\nanother package to install files into its host package’s directory. Let’s say that my-microservice installs the eb-fix-npm package. The eb-fix-npm package\ncan then call install-files source from an install script to copy the contents of source into my-microservice : This tool lets you share files between Node projects the same way you would share code, using\nnpm and declarative package names/versions. And with the ability to quickly distribute\nchanges to the script, we got ambitious. Speeding up npm install Simply by downgrading to npm 2, we were able to upgrade our Elastic Beanstalk environments to\nNode 6. But, in the process of investigating Elastic Beanstalk’s npm script, we noticed\nseveral inefficiencies. First, it installed Node modules afresh on every deploy. So we introduced a cache: files:\n  \"/opt/elasticbeanstalk/hooks/appdeploy/pre/46cache_node_modules.sh\":\n    mode: \"000755\"\n    owner: root\n    group: users\n    content: |\n      #!/usr/bin/env bash\n      # Cache Node modules in /var.\n\n      if [ ! -d \"/var/node_modules\" ]; then\n        mkdir /var/node_modules ;\n      fi\n      ln -s /var/node_modules /tmp/deployment/application/ By comparing timestamps when tailing EB’s activity log, we could see that EB’s npm script\nwent from taking ~4m to ~1m: a 75% speedup . Then we noticed that EB was calling npm rebuild after installing. But modules are\nautomatically built for the appropriate architecture when installing! The only time you need\nto rebuild is when the architecture changes — on configuration deploy. And on\nconfiguration deploy, EB was trying to install new modules — even though package.json doesn’t change on configuration deploy, only on application deploy. So, no npm rebuild on application deploy, and no npm install on configuration deploy: files:\n  \"/opt/elasticbeanstalk/env.vars\":\n    mode: \"000775\"\n    owner: root\n    group: users\n    content: |\n      # Exports variables for use by the other scripts below.\n\n      EB_NODE_VERSION=$(/opt/elasticbeanstalk/bin/get-config optionsettings -n aws:elasticbeanstalk:container:nodejs -o NodeVersion)\n      export PATH=/opt/elasticbeanstalk/node-install/node-v$EB_NODE_VERSION-linux-x64/bin:$PATH\n\n  \"/opt/elasticbeanstalk/hooks/appdeploy/pre/50npm.sh\":\n    mode: \"000755\"\n    owner: root\n    group: users\n    content: |\n      #!/usr/bin/env bash\n      #\n      # Note that this *overwrites* Elastic Beanstalk's default 50npm.sh script.\n\n      . /opt/elasticbeanstalk/env.vars\n\n      cd /tmp/deployment/application && npm install --production\n\n  \"/opt/elasticbeanstalk/hooks/configdeploy/pre/50npm.sh\":\n    mode: \"000755\"\n    owner: root\n    group: users\n    content: |\n      #!/usr/bin/env bash\n      #\n      # Note that this *overwrites* Elastic Beanstalk's default 50npm.sh script.\n\n      . /opt/elasticbeanstalk/env.vars\n\n      cd /tmp/deployment/application && npm rebuild --production During application deploy, our replacement npm script now took ~10 seconds: a 95% speedup compared to the initial duration. Bonus: EB’s configuration deploy npm script doesn't actually do anything — it uses the\nwrong working directory. Our script actually rebuilds your modules if, for instance, you change your Node version. One package to fix everything So there you have it: you can unblock upgrading your Elastic Beanstalk environments to Node 6 and virtually eliminate npm install time by installing a single package, eb-fix-npm . After installation, you’ll\neffectively only have to npm install when Elastic Beanstalk spins up new EC2 instances,\nwithout the cache. But we think we have a way to get rid of this hiccup too. Stay tuned… Like working on the cutting edge of JavaScript devops? Join us!", "date": "2016-12-02"},
{"website": "Mix-Max", "title": "Generating Certificates for Local Development", "author": ["Eli Skeggs"], "link": "https://www.mixmax.com/engineering/generating-certificates-for-local-development/", "abstract": "Mixmax is a rich communications client integrated directly into Gmail using iframes. As such, we\nneed to load all our resources over https - both for the security of our users, and to comply with\nGmail's strict security policies. This creates issues during our normal development workflow, where\nwe configure the app to refer to local resources. Overview Architecture Our platform is hosted across a set of microservices, each having a unique role. We have a\nmicroservice for serving contact autocomplete,  rendering out the compose modal, displaying the\ndashboard, and more. Every service sits on its own fully-qualified domain\nname (FQDN) - contacts.mixmax.com , compose.mixmax.com , and app.mixmax.com , respective to the\naforementioned services. For development, however, we need to host our microservices locally. To\nmaintain consistency and functional parity with our production environment, we put a proxy server in\nfront of our microservices, which handles requests to specific domains, such as app.mixmax.com ,\nand forwards them to the appropriate server. This parity means we don't need to hard-code a\nmicroservice's port to make a request to it. Up until this point, unlike our production environment,\nour proxy only handled http connections. Problem A content security policy determines how the browser handles requests to origins different from the\npage's origin. A strict content security policy like Gmail's is intended to keep the user safe, in\nparticular disallowing insecure requests on a secure page. When those resource requests are\nrejected, our app fails to load, which makes it impossible to test during development. Read more\nabout content security policy on HTML5\nROCKS . Gmail's content\nsecurity policy makes it so that if you inject content that uses insecure resources, the requests\nfor those resources are rejected. From the Mozilla Developer\nNetwork : When a user visits a page served over HTTPS, their connection with the web server is encrypted\nwith TLS and is therefore safeguarded from sniffers and man-in-the-middle attacks. If the HTTPS\npage includes content retrieved through regular, cleartext HTTP, then the connection is only\npartially encrypted; the unencrypted content is accessible to sniffers and can be modified by\nman-in-the-middle attackers, so the connection is not safeguarded. When a web page exhibits this\nbehavior, it is called a mixed content page. As a result, we've had to run Chrome in an insecure mode when testing our changes. This introduces\ndeveloper friction, and makes results from testing locally inconsistent with results post-deploy, so\nwe'd like to mitigate this problem. We'd like to share our findings while reducing friction in our development workflow, specifically\nregarding changes to our proxy server. We'll also discuss the changes we made to support our\nexisting livereload mechanism. The straightforward solution\nentails enabling our local proxy server to serve content over https so that the content security\npolicy allows our app to function. Generating the Certificates Any https connection requires a set of certificates, which serve to prove the identity of the\norganization or entity on the other end of the connection, and to encrypt the data between the two\nparties. Actual certificates should not be used during this process - they are not necessary, and\ndistributing them opens them up to unnecessary risk. Were any developer's computer compromised, the\nattacker could use the certificates to intercept legitimate traffic to the real servers, and cause\nreal damage to both users and the company. Since we don't want to use real certificates, we'll need\nto generate our own. In our case, we have a number of different domains we'd like to handle,\nincluding those on two separate second-level\ndomains : mixmax.com and mixmaxusercontent.com . To support arbitrary servers and minimize the number of changes we'll need to make if we add a new\nsubdomain, we'll use wildcard certificates for *.mixmax.com and *.mixmaxusercontent.com . We'll\nalso create a self-signed certificate authority with which to sign the certificates. Finally, we'll\nadd the certificate as trusted to the root certificate store, which instructs Chrome to trust the\ncertificate. Here's the script: #!/bin/bash\nKEY_SIZE=2048\nSUBJECT=\"/C=US/ST=California/L=San Francisco/O=Mixmax/OU=Engineering\"\nDOMAINS=\"mixmax.com mixmaxusercontent.com\"\n\n# generate the certificate authority private key\nopenssl genrsa -out ca.key \"$KEY_SIZE\"\n\n# self-sign the certificate authority\nCA_SUBJECT=\"$SUBJECT/CN=Mixmax Engineering\" # add a common name\nopenssl req -x509 -new -nodes -key ca.key -sha256 -days 1024 -out ca.pem -subj \"$CA_SUBJECT\"\n\n# create a .key and a wildcard .pem for each domain\nfor domain in $DOMAINS; do\n  # add the wildcard common name to the subject\n  LOCAL_SUBJECT=\"$SUBJECT/CN=*.$domain\"\n\n  # generate the local certificate\n  openssl genrsa -out \"$domain.key\" \"$KEY_SIZE\"\n\n  # generate the certificate signing request for the local certificate\n  openssl req -new -key \"$domain.key\" -out tmp-local.csr -subj \"$LOCAL_SUBJ\"\n\n  # sign the local certificate with the certificate authority\n  openssl x509 -req -in tmp-local.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out  -days 512 -sha256\ndone\n\n# cleanup\nrm -f tmp-local.csr We run this script once for each developer, rather than create one set of self-signed wildcard\ncertificates and check them in to git . This mitigates the admittedly\nsmall risk of the pre-shared certificates leaking, and keeps our developers marginally safer. To\nmake installing the certificates easier, the above script also prompts the developer to add them as\ntrusted certificates to the keychain: sudo security add-trusted-cert -d -r trustRoot -k \"/Library/Keychains/System.keychain\" ca.pem It seems that Chrome doesn't acknowledge the certificate unless it's in the admin cert store, hence\nthe -d flag. Configuring the Proxy Our existing proxy server uses http-proxy to\nactually proxy the individual requests to the microservices, and includes a short mapping of fully-qualified domain names (FQDNs) to local ports. This mapping mirrors our production setup with\ndifferent FQDNs for each app server, and means we don't need to hardcode the ports for each\nmicroservice into our codebase. The original proxy server: const httpProxy = require('http-proxy'),\n  proxy = httpProxy.createServer({ws: true});\n\nconst domainsToPorts = {\n  'app.mixmax.com': 3000,\n  'compose.mixmax.com': 3001,\n  // ...\n};\n\n// Handle requests to the proxy.\nfunction proxyRequest(req, res) {\n  const destinationPort = domainsToPorts[req.headers.host];\n\n  // Only proxy if we have a destination.\n  if (destinationPort) {\n    req.target = `http://localhost:${destinationPort}`;\n    proxy.web(req, res, {\n      target: req.target\n    });\n  } else {\n    res.writeHead(503); // Bad gateway\n    res.end();\n  }\n}\n\n// Handle request upgrades on the proxy.\nfunction proxyWebsocket(req, res) {\n  proxy.ws(req, res, {\n    target: req.target\n  });\n}\n\nhttp.createServer(proxyRequest)\n  .on('upgrade', proxyWebsocket)\n  .listen(80); It may seem as simple as replacing http with https , but it's not. Node uses and\nmaintains its own set of trusted certificates. As such,\nwe can't simply make all server-to-server requests run over https without also making changes to\nevery server-to-service request location. Moreover, we want to give our developers a chance to\nmigrate their code - the parts that don't directly integrate with Gmail - so we'll keep the http\nproxy code. The https proxy code will, similar to the http proxy code, need to map individual requests to the\ncorresponding FQDN. Given the requirement that we support both mixmax.com and mixmaxusercontent.com , and given that we don't want to specify a port with each request, we'll\nneed to use a TLS extension called Server Name\nIndication (SNI).\nWith SNI, before the server responds with its certificate, the client signals the name of the server\nit wants to communicate with, and the server uses this to select the appropriate certificate. Node\nsupports SNI out of the box in both the tls and https modules,\nwhich require a little effort to configure correctly. const domainsToCertificates = {\n  'mixmax.com': {/* cert and key */},\n  'mixmaxusercontent.com': {/* cert and key */}\n};\n\n// Use the server name from the client to select the appropriate certificate.\nfunction SNICallback(servername, callback) {\n  // Grab the top-level and second-level domain names.\n  const domainMatch = /(?:^|\\.)((?:[^.])+\\.(?:[^.])+)$/.exec(servername),\n    topAndSecondLevelDomains = domainMatch[1];\n\n  // Create the secure context for this request.\n  const secureContext = tls.createSecureContext(domainsToCertificates[topAndSecondLevelDomains]);\n\n  // And hand off the secure context to tls to complete the handshake.\n  callback(null, secureContext);\n}\n\nhttps.createServer({SNICallback}, proxyRequest)\n  .on('upgrade', proxyWebsocket)\n  .listen(443); Fixing Livereload Our app now loads fine, but our livereload doesn't work anymore. Livereload is super useful during\ndevelopment, so let's fix it. Chrome is rejecting the websocket connections due to Gmail's content\nsecurity policy, which is causing mixed content issues. We're using gulp-livereload and connect-livereload , and the gulp plugin provides its own development\ncertificates. We haven't instructed Chrome to trust those certificates, so we'll need to feed the\nlivereload connections through our proxy as well. Each of our servers uses a different port for livereload. In the name of reducing the use of\nhardcoded ports, we'll use <server>-livereload.mixmax.com to connect the proxy server to the\nupstream livereload server. At the Proxy On the proxy side, we already have a mechanism to handle different domains - namely, the domainsToPorts map - so it makes sense to handle the livereload subdomains there: const domainsToPorts = {\n  // ...\n  'app-livereload.mixmax.com': 35729,\n  'compose-livereload.mixmax.com': 35730,\n  // ...\n}; That's all the proxy needs, because the existing code supports both loading the livereload script,\nand upgrading requests into websockets for livereload. On the Client On the client side, we can override the port by specifying the port option to connect-livereload , but we'd like to change the domain as well, which means we'll need to use the src option in connect-livereload . There's a catch - by default, livereload will use the domain and protocol of the page, but uses the port option and falls back to the port 35729 . We'd like the script to be served over https on port 443 , but we'd also like the subsequent connections to be over port 443 . When reading the src attribute of script tags, Chrome omits the port if it corresponds to the protocol.\nTherefore, we can't just specify the port in the URL. The undocumented query parameter port solves\nthe problem by overriding the default port: https://app-livereload.mixmax.com/livereload.js?snipver=1&port=443 . Now the livereload script\nmakes the correct request to the correct protocol/hostname/port tuple. if (Environment.is(Environment.LOCAL)) {\n  app.use(require('connect-livereload')({\n    // This is necessary to prevent the mixed-content issues with the secure proxy. The port query\n    // parameter is parsed by the livereload script to ensure that it connects to 443 instead of the\n    // default livereload port (35729).\n    src: 'https://app-livereload.mixmax.com/livereload.js?snipver=1&port=443'\n  }));\n} Conclusions Now our developers can launch Chrome as normal, without needing to include flags to disable web\nsecurity. We hope you've gained some insight into setting up your own secure proxy for development. Interested in a low-friction development workflow? Come join us at Mixmax.", "date": "2016-07-01"},
{"website": "Mix-Max", "title": "Rewriting 30,000 lines of code in a friendly way", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/rewriting-30000-lines-of-code-in-a-friendly-way/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on\nDecember 6th was about Improving Elasticsearch query times . Sequences is one of our flagship features on Mixmax. Sequences\nenables users to create outbounds campaigns that can be configured to the last detail, with granular\ncustomizations per recipient. Sequences evolved from the Mail Merge feature. Originally, Mail Merges were a subset of the\nfunctionality currently provided by Sequences. It allowed to create a single email campaign with\nuser variables that were filled with a CSV uploaded by the user. Eventually, Mail Merges evolved\nto allow sending multiple emails in a campaign, and more recently, we allowed for adding recipients\nafter the initial set of recipients, as well as customizing stages once they were sent and we plan\nto implement many more features in the coming months. Everything that powered Mail Merge was rebuilt on a period of about 2 months. During this time,\nthere was over 30,000 lines of code removed, reimplementing the whole features under the Sequences\nconcept in a fraction of that code. Also, hundreds of thousands of documents were migrated and\namended to match the new collection design and modules interfaces. It was an incredibly daunting task. Sequences being a stable and well known feature for our valuable\nusers, it had to remain stable during the whole process of the refactor. Mail Merges were actually\nbecoming more unstable as we added more features due the high amount of complexity. We even had\ndedicated engineers to support the mail merge infrastructure while we were rewriting into Sequences.\nIt was really a race against time that we eventually beat. A quick look into Mail Merge We use MongoDB as our database, as such, we designed our collection similar to this: {\n  name: 'My Campaign'\n  subject: 'Introduction',\n  body: 'Hello world!',\n  variables: ['name'],\n  scheduledAt: '2016-11-05T18:00:00.000Z',\n  sentAt: '2016-11-05T18:00:00.000Z',\n  messages: ['messageId_1', 'messageId_2', 'messageId_3'],\n  recipients: [\n    {\n      email: 'foo@example.com',\n      variables: {\n        name: 'Foo'\n      }\n    },\n    {\n      email: 'bar@example.com',\n      variables: {\n        name: 'Bar'\n      }\n    }\n  ]\n} The Mail Merge sending flow is specialized for its use case (hundreds of recipients in a campaign),\na lot of code supported this specialized use case and, with time, organically grew to become a very\ndaunting component in our system. Once we decided to expand Mail Merges so it could send multiple stages at the time, we needed to do\nso in the least disruptive way and taking care not to break existing interfaces and components, as\nsuch, we created a new collection MailMergeStage that looks exactly like the MailMerge with a\nfew more additions: // The same as `MailMerge` but with the following added properties:\n{\n  trigger: 'notRead', // The stage will be sent as long as it hasn't been read\n  offset: 259200000 // the stage will be sent after this amount of time (in millis) passes after the previous stage.\n} In addition, the MailMerge collection received a new field: stages which is an array of the\nstage ids belonging to the mail merge campaign. Additionally, message documents receive an\nadditional metadata field: mailMergeId if the message belongs to a MailMerge or mailMergeStageId if the message was sent from a MailMergeStage document. Since a MailMergeStage is almost exactly the same as a MailMerge document, all interfaces\nrecognized these objects without any changes, since stages are sent conditionally, we did build some\ncode that worked exclusively with MailMergeStages but overall, the impact on the existing code was\nminimal. However it already showed signs of code smell : Message documents would either have mailMergeId or mailMergeStageId to reference the parent\ncampaign document. So we needed to query both fields because functions would receive either a MailMerge or a MailMergeStage document. As the codebase evolved, we needed to distinguish between a MailMergeStage and a MailMerge more and more often. The first stage of a mail merge campaign was always explicitly handled in a special way because\nthe first stage happened to be the MailMerge document itself, while the remaining stages were MailMergeStages documents. From its conception, a mail merge was designed to run as a monolithic campaign that runs once.\nAdding recipients after the initial set users uploaded from the CSV was very difficult Also, more often than not, we needed information specific to a Message , but we only kept the Message id field, so if we wanted to simply check the recipients in a given campaign, we needed\nto run a second query on the Message collection. Processing a stage could become a slow process, mainly due the high amount of logic involved during\na message creation. We had many ideas to improve our Mail Merge product but we were more and more slowed down by the\nexisting design of the Mail Merge collections, and the lack of modularity. While we had a powerful\nInbox Syncing feature and scalable and fast infrastructure for our Live Feed, Mail Merges could not\nmake use of these features due tech debt and the highly specific workflows that were in place for\nMail Merge. Refactoring Mail Merges into Sequences The first step on our refactor effort was to determine what is the central actor in a Sequence.\nThe more and more we analyzed the use cases, UX, mocks and future features, we saw how the\n\"Recipient\" is the central actor in the feature: A sequence is sent to a recipient A recipient has individual analytics, independent of other recipients in the campaign. Every recipient responds differently to the campaign. Campaigns should be totally customizable. While they have general configurations, such as message\ncontent, subject and other settings, it was clear that users want to have the option to tailor the\ncampaign for each recipient if necessary. A campaign should be a living entity that can be modified, extended and that can have new\nrecipients added to it at any point in time. Under this line of thinking, a Sequence then became a sort of \"blueprint\" which is carried over for\neach recipient. While with Mail Merge the central actor was the Stage, with Sequences the central actor is the\nrecipient. The document structure for the recipient is similar to this: // SequenceRecipient\n{\n  email: 'foo@example.com',\n  sequence: 'sequence_id',\n  stages: [\n    {\n      id: 'stage_id',\n      scheduledAt: '2016-11-05T18:00:00.000Z',\n      sentAt: '2016-11-05T18:00:00.000Z',\n      body: 'customized body',\n      message: {\n        id: 'message_id'\n      }\n    }\n  ],\n  variables: {\n    name: 'Foo'\n  }\n}\n\n// SequenceStage\n{\n  body: 'Stage Body',\n  subject: 'My stage subject',\n  trigger: 'notRead'\n}\n\n// Sequence\n{\n  name: 'My campaign',\n  stages: ['stage_1', 'stage_2']\n} What makes this design so flexible is that the stages array in SequenceRecipient is capable of\noverriding any setting in the SequenceStage document, allowing to make per-recipient\ncustomizations in a very trivial way.\nthe Stage object is generated doing something like: const stage = _.extend(referenceStage, recipientStage); Where referenceStage is the SequenceStage document with the default body and subject content,\nand where recipientStage is an object in the stages property of the SequenceRecipient document\nthat may optionally have its own body or subject or any other property of a SequenceStage document, thus making the recipient stage unique if so desired. Also, given that each recipient has its stages scheduled independently of other recipients, it is\nalso possible to send recipients at different times, something that was not possible under mail\nmerges given that a stage was sent for every recipient at a given point in time. Making use of the modular infrastructure By making a recipient a central concept in Sequence, we can easily transform the recipient actor\ninto a message actor, naturally, the Message is the central actor almost everywhere in our systems\nand it is certainly the actor in two very important modules to sequences: Send and Inbox Syncing . Previously, with Mail Merges, we had limited possibility to integrate with other modules — this\nwas one of the reasons why there was so much specialized code for sending messages in a mail merge\ncampaign, evaluating sending triggers, capturing analytics and so on. Now, given that a SequenceRecipient can quickly be \"translated\" into a Message actor, the\nSequences feature can seamlessly integrate other modules with a Message interface. Migration to the Sequences infrastructure Despite the very high amount of code going in, we were making use of stable features (in particular\nInbox Syncing), thus, by making use of these modules we were sure to a degree that the Sequences\nfeature was stable as well. Also, we spent a considerable amount of time working on a comprehensive set of unit tests for\ncritical workflows, we also adapted existing tests for the new Sequences interfaces, which made it\npossible to ensure backwards compatibility. Migrating the data over to the new sequences was also a challenge. The Mail Merge feature evolved\nover time becoming more and more complex, this meant that we had some discrepancies on data that\ndidn't match with current logic, specially for really old data. Also the upgrade to Sequences was\na complete update over to the new infrastructure, it was impossible to make incremental updates,\nmaking this upgrade specially difficult. We approached this by first making minimal changes in the frontend. The UI remained the same, we\nbuilt new views that used the same templates. The Sequences backend code lived alongside Mail Merge,\nso that we deploy our services constantly; this meant that Sequences code was constantly pushed to\nproduction in a daily basis which made pull requests smaller and easier to review. When the Sequences feature was ready to be used under the new infrastructure, we opened it for\ninternal use first, so that, besides our regular QA process, people in the office started using the\nnew Sequences feature live on production before we made it publicly available. This allowed to test\nit in real-life situations and helped to iron out the rough edges before our users hit them. We started our migration process on Friday afternoon, migrating the data on the database took around\n11 hours (we underestimated the amount of time needed for the data migration). During the time we\nstopped the sequences feature entirely: we disabled new sequences creation and sequences processing\nto preserve data integrity after the data was moved to the new collections.\nWe faced a few issues during the process, our database went down for a few stressful minutes when we\ndid the switch over due an unfortunate coincidence and actually not related to the switch-over at\nall, we had some performance issues when the code was finally working at full production scale, but\noverall, minor issues that were solved in a matter of hours. Conclusion We made the code vastly less complex by using modules and services being used by other features,\nsuch as the live feed, and also streamlining the sending process so sequence messages get treated\nas regular email messages. By keeping the SequenceRecipient as the central concept in sequences, we now make it much easier\nto implement new features as well as explore new ideas quickly. Since we keep the recipient data in\na single document, we've made writes safer through extensive use of MongoDB findAndModify . We also\nreduced the amount of queries used in most of the code thanks to the collections redesign which\nopened up the option to make better use of MongoDB aggregations, in particular the $lookup pipeline has become a staple in most of our aggregation queries powering the sequences features. We tackled this problem in the right time, mail merges/sequences was becoming more and more\nimportant, we had many unimplemented ideas at the time and the limiting factor was the existing\ndesign and implementation that was locked in into its own logic making it hard to be hooked up with\nother services and modules. Overall, the greatest success metric of the refactor was that users barely noticed about the change.\nThis was a huge success on our book. Since the refactor went in, we've implemented\npowerful features like a completely revamped sequences dashboard view and creation process,\nanalytics directly in the dashboard, adding stages to existing sequences, customizing campaigns per\nrecipient, integration with SalesForce and that was just in the past few weeks. Would you like to help us scale our features for new ideas and for a fast growing user base? Come join us! .", "date": "2016-12-07"},
{"website": "Mix-Max", "title": "Adventures in the Gmail PubSub API", "author": ["Cameron Price-Austin"], "link": "https://www.mixmax.com/engineering/adventures-in-the-gmail-pubsub-api/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on\nDecember 7th was about rewriting 30,000 lines of code . A few months back, we started using the Gmail PubSub API (part of the broader Google PubSub API ). It pushes notifications to an endpoint in our system whenever a user’s inbox changes. This includes new messages arriving, as well as other events, such as a message being read, or moved to a different folder. Our experience with it has been largely positive, however we did encounter a couple of gotchas that we haven’t seen documented anywhere yet. 1. It’s possible to subscribe to a user’s notifications more than once The watch method (which subscribes to notifications regarding a user’s inbox) is supposed to be idempotent. While that’s broadly the case, we found that if you send multiple watch requests simultaneously, you end up with multiple subscriptions, meaning every event for that user gets pushed to your endpoint multiple times! Our system was swamped by this a few days after deploy, reaching over 1000 requests per second before we shut it off. (We were calling watch simultaneously in some cases due to the way events propagated through our internal queueing system - duplicates sometimes occur, but we didn’t protect against them because the method was documented as idempotent. We've since added a user-level lock to prevent this). 2. The messagesAdded collection in history.list isn’t reliable The push notifications from Gmail contain a historyId - you then need to query the API to get all changes which have occurred between the last historyId you saw for that user, and this new historyId . The history.list method returns an array of changes, broken down by change type. For example, labelsAdded contains details of labels being applied to messages. We were mostly interested in the messagesAdded array, which according to the documentation , represents: Messages added to the mailbox in this history record We built our code assuming that all new messages would be listed within messagesAdded . However, inexplicably, that’s not always the case . We had multiple instances where new messages simply never appeared in the messagesAdded array . We were never able to identify why this occurred. In our tests (moving a message out of the spam folder, messages sent from the user to themselves, messages skipping the inbox etc.) we could not reproduce the issue. We did, however, find that when messages were missing from messagesAdded , they did appear in the messages array, which  holds the ids of all messages modified in any way in the history record. We now check the messages array exclusively, which is inefficient, because it means we’re querying messages which aren’t new, but were simply changed in some way (read, moved to a different folder, archived etc). But at least we can now guarantee we’re picking up all new messages. 3. If you subscribe to push notifications for all your users, then sending out a user email blast causes some serious spikes We noticed a weird spike in push notifications (around 10x higher than normal volume), a few days after launch. It took a few minutes to figure out it was due to a newsletter we had sent out to our entire user base, triggering inbox events for every user simultaneously :) Incidentally, we've also noticed that inbox events spike on the hour and half-hour . Our theory is that this is due to bulk marketing emails, which are often scheduled to go out at these times. Enjoy discovering API quirks like these? Drop us a line .", "date": "2016-12-08"},
{"website": "Mix-Max", "title": "Native Support for Websockets on AWS", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/deploying-meteor-to-elastic-beanstalk-1/", "abstract": "Mixmax started out as a monolithic Meteor application hosted on Modulus.io. As our traffic and\nuser base grew we ran into problems with both Meteor itself and our hosting provider,\nprompting us to factor subsystems into their own microservices and move those services to AWS Elastic Beanstalk .\nThis reduced our costs and gave us better operational control. In particular, it let us lock down our\nservices at the network level, using an AWS VPC . Our Meteor application, however, remained outside Elastic Beanstalk. This left our architecture\nvulnerable, as any resource needed by the Meteor app (databases, other services) had to be exposed\noutside the VPC. But Meteor relies on websockets, which AWS did not natively support — until now,\nwith the advent of the Application Load Balancer (ALB). In this post, we describe: approaches to supporting websockets on AWS pre-ALB, and their limitations how to configure an ALB and websockets for use with Elastic Beanstalk and AWS VPCs how to easily deploy Meteor to Elastic Beanstalk Much of this post is relevant to any websocket-using application on Elastic Beanstalk, not just\nMeteor applications. We highlight where this isn’t the case below. Supporting Websockets on AWS Pre-ALB Prior to the release of the ALB, the one and only AWS-native option for load balancing was the\nvenerable Elastic Load Balancer (now called a Classic Load Balancer). You could install any\nload balancer you liked on your own EC2 instance—and we’ll get into that below—but if you wanted AWS\nto handle high availability, automatic scaling, and security, it was the ELB or bust. The problem is that the ELB doesn’t support websockets. There are plenty of articles about load balancing websockets with an ELB, but in all of those configurations the ELB is not actually balancing websockets per se — it’s balancing raw TCP connections, and letting an application proxy\nserver like nginx handle the HTTP upgrade request at the start of the websocket connection. However, websockets can’t be effectively load-balanced by a “TCP-mode” ELB (an ELB listening to TCP\nconnections on port 80, rather than HTTP connections). This is because\nmany users can’t actually use websockets ,\ndue to using old browsers or being in environments that don’t support the WebSocket protocol, like\ncorporate networks with restrictive proxies. So what many applications including ours use is a\nwebsocket emulation like SockJS . If\nwebsockets are not available, the emulation falls back to polling using XHR or JSONP. Unfortunately, a TCP-mode ELB can’t reliably load-balance these fallback transports. This is because\nthese transports rely on firing several requests during the lifetime of a “socket”, all of which need\nto be served by the same process. There are two common ways to implement such “sticky sessions”: IP-based affinity: direct all requests from the same client IP address to the same server cookie-based affinity: direct all requests with the same cookie to the same server The same corporate networks that force the use of SockJS also break IP-based affinity by\nload-balancing traffic across multiple external IP addresses , so you want to use cookie-based affinity. But\ncookies are sent as HTTP headers… and a TCP-mode ELB can’t read HTTP headers. The ELB makes you choose between network-level and application-level balancing (TCP vs. HTTP modes,\nLayers 4 and 7 in the OSI model ); websockets are only\nsupported in TCP mode, and cookie-based sticky sessions are only supported in HTTP mode. In a world\nwhere websockets aren’t universally supported, this means that websockets aren’t supported at all. At this point, others have coped by pairing a TCP-mode ELB with their own proxy server, where the latter sets and reads\nthe session cookie. But this requires a lot more configuration. It also sacrifices Elastic Beanstalk’s\nguarantees of high availability and autoscaling, by intermediating AWS’ load balancer and the EC2\ninstances. Wouldn’t it be nice if AWS offered a load balancer that supported cookie-based sticky sessions and websockets? Configuring an ALB and websockets for use with Elastic Beanstalk This is where the Application Load Balancer comes in. It load-balances at Layer 7, so supports\ncookie-based sticky sessions; and it supports the WebSocket protocol. So enabling websockets on Elastic Beanstalk is as simple as clicking “Create New Environment” and\nselecting an Application Load Balancer, right? Well, no. As of this writing (10/16/2016), the Elastic\nBeanstalk GUI offers very poor support for creating and managing an ALB. This is true even for the\nnew UI that EB is beta-testing. Thankfully, the AWS EB CLI lets you choose an ALB during EB environment creation. You can then configure the ALB and related\nresources through a combination of various AWS dashboards and ebextensions scripts. Setting up the ALB This section applies to any use of an ALB with Elastic Beanstalk. Assuming you’ve initialized the AWS EB CLI, create a new environment (you can’t modify an existing,\nELB-using environment to use an ALB, sadly): If you want to put the environment in a VPC, you’ll also need to configure that at this time by\npassing the --vpc flag to eb create . EB will prompt you to create security groups for the ALB and the EC2 instances but you can ignore that, as it’ll create its own. You will need to manually create subnets, and will\nneed at least two for an ALB. You will be able to change/add subnets later in the EB\ndashboard; the normal UI will appear for that: If you wish to enable HTTPS, you’ll need to add an HTTPS inbound rule to your load balancer security\ngroup. If you configured a VPC, EB will have created a security group for the load balancer and named\nit after the environment: Otherwise, it will have used a default group: To avoid modifying other resources (per AWS’ warning), you’ll want to create your own security group\nand associate it with the ALB: This brings us to finding the ALB, because you’ll be editing its settings in the EC2\ndashboard — again, EB does not currently (as of 10/16/2016) show load balancer settings in the EB\ndashboard. The first tell (in the load balancer table above) is that the “type” of the balancer is “application”.\nSecondly, you can look at the “Created At” date. And if all else fails, you can switch over to the\n“Listeners” tab of the balancer and click the name of the target group of the HTTP listener: Then, in the target group’s “Targets” tab, you’ll be able to match the name of the EB environment\n(highlighted): Navigate to this target group if you haven’t already, since it’s here that you’ll need to enable\ncookie-based stickiness (at the bottom of the “Description” tab) and configure the health check (in the “Health checks” tab). Note that there’s also a health check URL section in the EB dashboard: but this doesn’t do anything. (In an environment using an ELB, it synchronizes with the ELB’s settings;\npresumably AWS will do this for environments using ALBs too at some point.) Lastly, go back to the load balancer and add an HTTPS listener: This assumes you’ve already registered your SSL certificate with AWS. (Aside: you don’t need a TCP listener because, unlike the ELB, the ALB actually supports websockets,\nwhich begin over HTTP(S). And the ALB doesn’t even support TCP listeners, as it load-balances only at\nLayer 7.) Phew, that’s the ALB all set up. Configuring nginx This section applies to any EB environment that uses nginx as an application proxy server. This is\nthe default for EB environments that use Node.js as their platform, and is required to support static\nfile mappings and gzip compression (at least out of the box on EB). Unfortunately but perhaps unsurprisingly (at this early stage), EB’s default nginx configuration does\nnot support websockets. You’ll need to patch its configuration to handle the HTTP upgrade request using an ebextensions script like this: container_commands:\n  enable_websockets:\n    command: |\n      sed -i '/\\s*proxy_set_header\\s*Connection/c \\\n              proxy_set_header Upgrade $http_upgrade;\\\n              proxy_set_header Connection \"upgrade\";\\\n              ' /tmp/deployment/config/#etc#nginx#conf.d#00_elastic_beanstalk_proxy.conf Careful, a failure to enable the upgrade may go unnoticed if you’re using a websocket emulation like\nSockJS — it’ll just fall back to XHR or JSONP. We have also found that our application requires a greater number of worker connections than the default. You could increase the limit by patching EB’s configuration as above, using sed ;\nwe did it by simply copying EB’s configuration and editing the relevant line . So put both those files into an .ebextensions directory in your application’s root directory: Configuring Cloudwatch alarms This section applies to any use of an ALB with Elastic Beanstalk. Configuring Cloudwatch alarms is mostly out of the scope of this article. However, you should note that ALBs have a different set\nof Cloudwatch metrics than ELBs do. Compare: Perhaps at some point AWS will rename the ALB metrics to remove references to “ELB”. Deploying Meteor to Elastic Beanstalk Up until this point-perhaps because of the AWS configuration required-most tutorials for deploying\nMeteor to Elastic Beanstalk have been rather fragmentary . This post is probably the most complete but involves the author doing a lot of unnecessary manual work and also\ndoesn’t enable websockets. For these reasons, many Meteor apps deploy onto PaaSes such as Modulus.io and\nMeteor’s own Galaxy . (Mixmax used both of those options, Galaxy\nmost recently, before switching to EB.) But this article will hopefully convey that it is now simple\nto use EB to realize cost savings and control beyond what non-EB PaaSes can provide. So, here is a simple, complete, and automated guide to deploying Meteor to Elastic Beanstalk. First, configure an EB environment using an ALB, with websockets enabled, as earlier in this post.\nWhen your environment launches, Beanstalk will complain that it is unhealthy. Don’t worry! This is\nbecause EB is trying to run your Meteor application as if it were a regular Node.js application,\nusing npm start . To deploy Meteor to Elastic Beanstalk, you have to turn it into a regular Node.js application.\nLuckily, Meteor offers a CLI command for this, meteor build . It outputs a “bundle” directory\ncontaining a script main.js which will start the application. So fill in your EB environment’s\n“Configuration -> Software Configuration” dashboard like so: (Note: EB will restart your environment every time you save a configuration change, and your\nenvironment will keep being unhealthy until it is fully configured and deployed, so this process is\nsomewhat slow and may involve EB rolling back. It would be more efficient to enact all changes in an ebextension config file so that they could be deployed alongside your application.) Also in the Software Configuration dashboard, set the standard Meteor environment variables of ROOT_URL and MONGO_URL . Unfortunately, you won’t be able to set METEOR_SETTINGS to the contents\nof your Meteor settings file, as Meteor recommends when using bundles — EB doesn’t let you put\nJSON in an environment variable. You might consider switching to use individual environment variables now that you’re on a PaaS that\nsupports such ( cough Galaxy cough ). Alternatively, if you wish to keep using Meteor settings,\nyou can patch main.js to load your settings (see script below). The last part of converting Meteor to a regular Node.js application is installing your dependencies.\nThe way meteor build works is that it copies your currently-installed Node modules into the bundle\nand flags native modules to be rebuilt when npm install is run on the target server. That is,\nrebuilding is all that npm install does on the target server. (This is kinda cool\nactually — it makes deployment faster and more consistent with your local environment.) But, you can’t run npm install at the root of your application, you have to run it within the\nbundle. So, copy this script into your ebextensions directory to do that during deployment. Altogether now, here is a script that fully deploys your Meteor application to Elastic Beanstalk: it installs Node modules bundles your Meteor application as a Node.js application copies your ebextensions into the bundle copies your Meteor settings into the bundle and patches main.js to load them and finally deploys the bundle to EB. Conclusion This post has shown that it is now possible to deploy websocket-using applications like Meteor to\nAWS Elastic Beanstalk using native AWS resources, thus reaping all the advantages of EB—high\navailability, scalability, and low cost—without committing substantial devops time. Configuring the\nnew Application Load Balancer is laborious at the moment, but will only become easier as Elastic\nBeanstalk builds out automation and GUIs for doing so. And adopting Elastic Beanstalk now puts\napplications like Mixmax in the perfect position to scale while giving us control over our platform. Tweet at us @MixmaxHQ with\nyour opinions on deploying with AWS vs other PaaS/IaaS offerings. And if you’d like to deploy cutting\nedge infrastructure to a global user base, drop us a line at careers@mixmax.com .", "date": "2016-10-17"},
{"website": "Mix-Max", "title": "Determining why that server is on fire", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/determining-why-that-server-is-on-fire/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on\nDecember 8th was about the Gmail PubSub API . tl;dr - We run our node apps so that we can profile their CPU usage on the fly. The problem Have you ever watched the CPU usage of your Node.js process spike on your servers?\nDid it feel like you were helpless to wonder at the reason why? One way to figure out what your process is doing is to use a flamegraph — a\nvisualization of the kinds of work its doing (think stack traces), and how long\neach component of that work is taking (in terms of CPU usage). There is already a bunch of amazing material on how to build flamegraphs. However, each example is given only in terms of\nprofiling a process locally (once the user has SSH'd into a server) — there\nisn't a lot of tooling for profiling servers remotely. Today we're going to give\nyou a pair of scripts that allow you to do just that given that you're running on\nAWS. These scripts also handle some advanced deployment scenarios and mitigate some\nbugs reported by other material on flamegraphs. Too many processes The first issue we faced when building these scripts was having too many processes\nto profile: we use Node clustering to get the most out of our instances, however there are very few examples of profiling multiple\nNode processes at the same time in order to build flamegraphs. Endless perf map growth To compound the issue, the JIT symbol map required to build flame graphs grows endlessly . The solution As it turns out, you can pass multiple process IDs to the -p flag of perf record as long as they are comma separated. To mitigate the symbol map\ngrowth issue, we simply run all of our node processes with the --perf_basic_prof_only_functions flag. It does mean that we do get some unknown entries in our flamegraphs, but\nit also ensures that we won't run out of memory on our servers any time soon #tradeoffs . Profiling Node apps To simplify creating these flamegraphs, we wrote a few bash scripts that build\nthe flamegraphs remotely on server of interest and then extract them back to\nthe engineer's machine. profile.sh The first script builds the second, by embedding how long we'd like to profile\nfor and if we'd like to profile all node processes on the box or only a single\none. That script looks like: #!/bin/bash\n#\n# See: https://mixmax.com/blog/determining-why-that-server-is-on-fire for details.\n#\n# Example usage: ./profile.sh $INSTANCE_ID [$DURATION] [$PID]\n# Where:\n#   - $INSTANCE_ID is the EC2 identifier of the instance to profile the node\n#     processes on.\n#   - $DURATION is an optional duration specified in seconds (defaults to 30).\n#   - $PID is a specific process ID to profile, by default we profile all `node`\n#     process on the remote instance.\n#\n# It is also assumed that you have setup your AWS credentials so that the\n# `aws` cli functions correctly.\n#\n# This script currently assumes that you are profiling EC2 instances inside\n# a private VPC and that you ahve setup a bastion tier with SSH forwarding\n# appropriately. If this is not the case, simply change\n# s/PrivateDnsName/PublicDnsName/.\n#\n##########\n\nset -e\n\nINSTANCE=$1\nDURATION=${2:-30}\nPID_MATCHER=\"echo $3\"\nif [ -z ${3+x} ]; then\n    PID_MATCHER='pgrep node | xargs | sed -e \"s\\/ \\/,\\/g\"'\nfi\necho \"building perf script...\"\nsed -e \"s/DURATION/$DURATION/g\" perf.sh > perf.tmp.sh\nsed -i -e \"s/PID_MATCHER/$PID_MATCHER/g\" perf.tmp.sh\n\nIP=$(aws ec2 describe-instances --instance-ids $INSTANCE --query \"Reservations[*].Instances[*].PrivateDnsName\" --output text)\n\n# Collect a sample from the remote node\necho \"running the debug script on the remote node: $INSTANCE\"\nssh ec2-user@$IP 'bash -s' < perf.tmp.sh\n\n# Cleanup after ourselves\nrm -f perf.tmp.sh perf.tmp.sh-e\n\n# Retrieve the svg from remote node\necho \"copying the flamegraph back from the remote node...\"\nscp ec2-user@$IP:/home/ec2-user/flame.svg .\n\n# Open the image in the browser\necho \"opening the flamegraph in Chrome...\"\nopen flame.svg -a Google\\ Chrome perf.sh The second script is a template that is run on the remote instance. The profile.sh script places any duration and PID specific options in this script\nbefore scp ing it to the remote EC2 instance. perf.sh downloads\nall tools necessary to build the flamegraph, profiles the process(es) of\ninterest and then builds the flamegraph. #!/bin/bash\n#\n# See: https://mixmax.com/blog/determining-why-that-server-is-on-fire for usage.\n#\n###########\n\necho \"installing perf...\"\nsudo yum install -y perf\n\necho \"fixing file perms...\"\nsudo chown root /tmp/perf-*.map\n\nDUR=DURATION # this is replaced by profile.sh\nPIDS=$(PID_MATCHER) # this is replaced by profile.sh\necho \"collecting info for $DUR seconds...\"\necho \"collecting info on $PIDS...\"\nsudo perf record -F 99 -o perf.data -p $PIDS -g -- sleep $DUR\n\necho \"running perf...\"\nsudo perf script -i perf.data > out.nodestacks01\n\necho \"cloning flamegraph...\"\ngit clone --depth 1 http://github.com/brendangregg/FlameGraph\ncd FlameGraph\n\necho \"creating flamegraph...\"\n./stackcollapse-perf.pl < ../out.nodestacks01 | ./flamegraph.pl > ../flame.svg How big is the fire? Coincidentally enough, the night before I sat down to write this blog post we\nhad an incident in one of our services. This service in question suddenly\nbegan to pin all of its CPUs. No code had been deployed to it recently that\ncould have caused the issue... Neither was there any unusual load on our system...\nSo what could it be? The engineers on call used our profiling script to sample\nan instance exhibiting the pinned CPU behavior and a healthy one to compare. Unhealthy server Healthy server Putting out flames Comparing the two graphs above immediately showed us that the unhealthy server\nwas spending an obscene amount of time trying parsing HTML. This information\nallowed them to identify a few culprit data pieces that were causing this behavior.\nThey were then able to build a fix and hot-patch the bad data to resolve the\nissue. Notes You may note in the SVGs above that there are many entries of the form: /tmp/perf-$PID.map . This is because we use --perf_basic_prof_only_functions which only outputs the symbols for JS functions and not all symbols (which the --perf-basic-prof flag outputs). We haven't found this to be a large issue in\npractice however as we still get enough information out of the flamegraphs for\nour purposes. Enjoy building greater introspection into infrastructure? Drop us a line .", "date": "2016-12-09"},
{"website": "Mix-Max", "title": "Securely signing requests with Rewt", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/securely-signing-requests-with-rewt/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 4th was about CORs headers . Microservice-based architectures have a lot of server-to-server communication. Some non-trivial portion of that communication will be with APIs that are considered “internal”. To secure this communication, people will often just lock these services away in private networks with strong subnet access controls. However, it’s often much nicer if you also provide some manner of authenticating requests between servers, or at least a mechanism for verifying that the request is coming from a trusted source. This approach will also let you secure services that aren’t fully internal and so can’t be put inside a private network. Enter JSON web tokens (JWTs). In short, JWT is a standard for generating tokens that assert a claim and whose legitimacy can be easily verified. We wanted a simple way of generating JWTs for our internal communication. Furthermore we wanted to be easily able to rotate the shared secret used to sign and verify the tokens. So we wrote rewt - a simplified wrapper for signing JWTs with a shared secret sourced from Redis, that automatically rotates the secret on a predefined interval. Initializing rewt Initializing an instance of root is as simple as telling it where to source the shared secret from. const redis = require('redis');\nconst Rewt = require('rewt');\n\nlet rewt = new Rewt({\n  redisConn: redis.createClient('redis://localhost:6379')\n}); By default, rewt will namespace the secret key under the rewt: namespace with a default TTL of a day. To have the key rotate on a faster interval, or you use a different namespace, you can provide alternate parameters to the constructor: let rewt = new Rewt({\n  redisConn: redis.createClient('redis://localhost:6379'),\n  redisNamespace: 'foobar',\n  ttl: 60 * 60 // One hour in seconds\n}); Using rewt Signing a payload with rewt is extremely simple, just provide the payload to sign and a callback to receive the signed payload: rewt.sign({username: 'hello@hello.com'}, (err, signed) => {\n  console.log(`signed payload: ${signed}`);\n}); Note that the first parameter to sign doesn’t have to be an object, it can also be a buffer or a string. Verifying a payload is equally as easy: rewt.verify(token, (err, payload) => {\n  console.log(`verified payload: ${JSON.stringify(payload, null, '  ')}`);\n}); Worked Example using rewt It may not be immediately obvious of how to use rewt, so in this quick example\nwe will sign a payload with rewt, add it as an authorization header and then\nverify that payload and extract the relevant information. Signing an HTTP request with rewt Signing the request simply uses  the sign functionality to create the token\nand then embed it as an HTTP Authorization header (as a Bearer token). function retrieveProtectedResource(userId, resource, done) {\n  rewt.sign({ userId }, (err, signed) => {\n    if (err) return done(err);\n\n    request({\n      uri: resource,\n      method: 'GET',\n      auth: {\n        bearer: signed\n      }\n    }).on('error', (err) => {\n      done(err);\n    }).on('response', (response) => {\n      done(null, response);\n    });\n    console.log(`signed payload: ${signed}`);\n  });\n} Verifying a request using rewt We can easily build an express middleware component that extracts the userId from the signed payload above and places it on the incoming request object. const BEARER = 'Bearer';\n\nfunction rewtMiddleware(req, res, next) {\n  if (!req.headers.authorization || req.headers.authorization.indexOf(BEARER) !== 0) {\n    return next();\n  }\n\n  let token = req.headers.authorization.slice(BEARER.length).trim();\n  rewt.verify(token, (err, payload) => {\n    if (err) return next(err);\n\n    if (payload && payload.userId) {\n      req.user = { _id: payload.userId };\n    }\n    next();\n  });\n} Voila! We now have a mechanism for ensuring that an incoming request was sent\nfrom a trusted source. Notes One thing to note is that since the key is in Redis and rewt handles rotating it for you, if you ever need to rotate the key manually you can simply remove the key from Redis yourself and rewt will generate a new one to use on the next query. Careful readers will also have noted that we spoke about the key having a TTL. This does mean that there is the miniscule chance that the key may be rotated while a request is in flight. We do not consider this as a large drawback since one should already be using appropriate retry policies for specific error classes and because this time window is very small. Enjoy infrastructure security? Drop us a line .", "date": "2016-12-05"},
{"website": "Mix-Max", "title": "Fun With the API", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/fun-with-the-api/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar . The previous post on December 9th was about cpu profiling node processes . Here at Mixmax, we have a new yet rapidly growing API . This API is available to anyone who is on our Small Business plan ( Give it a try! ) and lets us do all sorts of fun stuff such as adding contacts, checking on poll results, and checking the Live Feed , among other things. In the spirit of the season, these are the 5 steps for creating a holiday guestbook which will add people to a Mixmax contact list . We will use Typeform to create a web form, Zapier to connect the form to our API, and the Mixmax API to add the contacts to our contact list. Step 1. Create a contact list You can do this at app.mixmax.com/dashboard/contactlists Step 2. Get your Mixmax API token You can do this at app.mixmax.com/dashboard/settings/integrations . Make sure to write it down! Otherwise you will have to generate a new token if you forget it. Step 3. Create the form using Typeform You can do this at admin.typeform.com . Ours only has two fields: a Name and an Email. Step 4. Create the API integration using Zapier You can do this at zapier.com/app/editor . Create a Zap which has a Typeform as a trigger and a Webhook as an action. The Typeform should be connected to the form your created in Step 3 and the Webhook should be configured as it is in the above image. You can get your Contact Group ID by checking the /contactgroups API endpoint . NOTE: When Zapier tries to run a test, it will fail since it will pass an invalid email into the email field. You can safely ignore this. Step 5. Enjoy! Remember to sign our guestbook here ! Want to work on tools to help effectively communicate with the north pole? Drop us a line .", "date": "2016-12-10"},
{"website": "Mix-Max", "title": "Monitoring slow Mongo queries", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/monitoring-slow-mongo-queries/", "abstract": "Here at Mixmax it’s no secret that we use Mongo for a lot of different things. One issue that we’d had for a long time though was our ability to monitor our Mongo clusters for slow queries. This is an issue because if an engineer adds a new query to our codebase, but that query doesn’t use an index, trouble will almost always ensure - IOPS will go through the roof, we’ll start seeing Mongo churn through its internal cache because now it’s having to scan full collections, just a whole collection of really bad things. We knew we could try monitoring the slow query log for this, but we didn’t like the idea, nor the fragility of monitoring a continuous stream (we didn’t have the best experience tailing Mongo’s oplog for reactivity in Meteor). Instead, we figured we’d be able to catch the worst culprits simply by periodically querying which queries had been running for a long time. To do this, we wrote, and open-sourced, mongo-slow-queries . Initializing mongo-slow-queries Initializing mongo-slow-queries is as simple as giving it a Mongo DB reference to use to query. const mongojs = require('mongojs');\nconst MongoSlowQueryChecker = require('mongo-slow-queries');\n\nlet db = mongojs('mongodb://localhost:27017/my-db');\nlet slowQueries = new MongoSlowQueryChecker({ db }); By default, the mongo-slow-query checker will classify a slow query as anything that is currently running and has already been running for more than five seconds. To set this to a different threshold, provide the queryThreshold parameter then initializing a new instance: let slowQueries = new MongoSlowQueryChecker({\n  db,\n  queryThreshold: 2 // The unit is seconds.\n}); Using mongo-slow-queries Once you’ve created an instance to check for slow queries with, usage is simply: slowQueries.get((err, queries) => {\n  if (err) {\n    console.log('Failed to retrieve slow queries: ' + err);\n  } else {\n    console.log('slow queries: ' + JSON.stringify(queries));\n  }\n}); Notes on the returned queries To make aggregating slow queries easier, we fingerprint every query that we tag as slow. This means we remove the unique components of a query, while tagging what fields we’re filtering on. As an example, the following query: {\n  _id: 'foo',\n  $or: [{\n    createdAt: {\n      otherIds: ['bar']\n    }\n  }]\n} Would have a fingerprint of: { _id, $or: [ { createdAt: { otherIds: [ ] } } ] } We also tag queries with other pertinent information such as what query the collection is primarily running against and if the query is using an index or not. How we use mongo-slow-queries @ Mixmax At Mixmax, we run these queries via a distributed cron job. These means we’re able to constantly query for slow queries, without tailing or hammering our DB. We then pipe any and all slow queries that are found to Sentry, our error reporting system, where we roll them up by fingerprint and then post to a Slack channel. This means we get alerts for any new slow queries, allowing us to take action immediately (as seen below). Enjoy building next-gen monitoring to be able to predict infrastructural issues? Drop us a line .", "date": "2017-02-21"},
{"website": "Mix-Max", "title": "Integration Testing for Humans", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/integration-testing-for-humans/", "abstract": "This blog post is part of the Mixmax 2016 Advent Calendar .\nThe previous post on December 10th was about making a holiday guestbook using the Mixmax API . Raise your hand if you’ve had an outage that could have been prevented by integration testing. And\nkeep your hand raised if you’ve put off writing those tests for some very good reasons. What if there\nwere another way? At Mixmax, we’ve had a bunch of experience at prior companies writing integration tests for iOS and web, the latter using Selenium WebDriver . And\nthat experience has taught us that such tests are enormously expensive to write and maintain. Selenium\ntests are coupled to all sorts of implementation details (e.g. CSS selectors) that a user would gloss\nright over; and can all-too-often work locally, yet crash or timeout in CI environments, due to the\nCI servers being starved for resources. That said, integration tests fill a crucial niche in a testing ecosystem. Unit and functional tests\nonly verify that individual subsystems are working — integration tests verify that those APIs\nmesh to create the desired user experience. Something’s got to execute such tests before shipping. Some thing … or perhaps some one ? Integration Testing for Humans What we realized was that, for now, it’d be a lot simpler if we ran the tests. All we really needed\nto automate was the reminder to do so. The result was integration-testing-for-humans (ITFH), an open-source CI service that reminds you, the developer, to test your staging environment\nbefore deploying to production. The package defines Express middleware, as well as a standalone server, that\nresponds to GitHub webhooks . Once you set up its hosting\nand decide what test you’d like to run e.g. “Send an email”: var express = require('express');\nvar humans = require('integration-testing-for-humans');\nvar app = express();\n\nvar middleware = humans({\n  githubAccessToken: process.env.ITFH_GITHUB_ACCESS_TOKEN,\n  githubWebhookSecret: process.env.ITFH_GITHUB_WEBHOOK_SECRET,\n  location: 'https://example.com/humans',\n  defaultTest: 'Send an email',\n});\n\nmiddleware.on('error', (err) => console.error(err));\n\napp.use('/humans', middleware);\n\napp.listen(process.env.PORT); it will use the GitHub commit status API to prompt\nyou to test when you open or update a PR: When you click \"Details\", you are shown exactly what you should test. After doing so, you click the button: And can merge the PR: Still… manual testing? If you have the resources and need to write a fully-automated suite, by all means go for it. ITFH is for fast-moving teams like\nMixmax that need to spend their time building the platform, and can afford to ship small bugs. As\nour lists of must-not-break features grows longer, we'll need to test more things before deploying,\nand then it will become more painful to run the tests ourselves than invest in Selenium — but\nfor now, we just need a reminder to run the simplest and most important tests before shipping. At Mixmax we believe in simple solutions to hard problems and just the right amount of process. If\nyou’d like to move fast in that sort of environment, join us !", "date": "2016-12-11"},
{"website": "Mix-Max", "title": "Yarn-ifying Mixmax", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/yarn-ifying-mixmax/", "abstract": "Last week, we finished transitioning all of our services and shared modules to Yarn , Facebook’s replacement for npm . Here’s how we decided to make the move, how we did it, and what we learned along the way. We've published a module to help you convert your own projects if you want to get started right away! Why'd we move? We were initially drawn to Yarn for two reasons: speed and better dependency pinning . Speed Over the past several months, npm install had become very time-consuming for us, taking upward of 25 minutes in some projects. Since we needed to reinstall our dependencies before removing, adding, or upgrading a dependency to ensure the accuracy of our shrinkwrapping, npm was draining a lot of time that we would have preferred to spend developing new features. When benchmarked, Yarn proved to install dependencies more than 20x faster, even with a cold cache. Results of one sample: time npm install - 18:16.91 time yarn (cold cache) - 48.463 Dependency pinning We had been using Uber’s shrinkwrapping tool since we agreed that npm ’s built-in shrinkwrap wasn’t great . Unfortunately, Uber’s tool came with its own bugs and less-than-ideal workflow. When we began to consider Yarn, we were excited by dependency pinning being a first-class feature. Considerations Before moving forward with Yarn, we needed to make sure that it satisfied some prerequisites: Could we use it to install our private modules from the npm registry? Yes (discovered experimentally). Did it pin transitive dependencies in addition to top-level dependencies? Yes . Could we use it in CI (ie Codeship, Travis)? Yes . How would our workflow change? We looked up the commands we’d be using most often and observed that our workflow would be almost exactly the same, but with yarn instead of npm in our commands. Most importantly, how could we minimize risk? With Yarn being a relatively new project, it was important to us to be able to revert to using npm if we discovered that Yarn was not a good fit for us. Our plan was simple: we’d start by only using Yarn in a few projects, and would go back to using shrinkwrap if we decided that Yarn wasn’t right for us. Here's what our workflow changes look like: Action npm yarn Initial installation npm install yarn Adding a package First, reinstall node_modules . npm install --save|--save-dev --save-exact <package>@<version> npm prune npm run shrinkwrap npm run npm-shrinkwrap-check yarn add [--dev] <package> Upgrading a package First, reinstall node_modules . npm install --save|--save-dev --save-exact <package> npm prune npm run shrinkwrap npm run npm-shrinkwrap-check yarn add [--dev] <package> Installing a package globally npm install -g <package> yarn global add <package> Linking a local module npm link ../<directory name> (in directory to be linked) yarn link (in directory using local module) yarn link <module> Implementation Minimizing risk We began by using Yarn in one of our open-source modules, Erik , a node package for running client-side Jasmine tests headlessly. This allowed us to gain familiarity with Yarn before taking on any risk. Ramping up The next phase involved converting some of our services to use Yarn. We started with an internal service in order to minimize our risk, then moved on to other low-priority services. Along the way, we learned a few things: In some cases, yarn check , a command that “Verifies that versions of the package dependencies in the current project’s package.json matches that of yarn’s lock file”, would fail immediately after running yarn for the first time (which generates the yarn.lock file). We were able to resolve this issue most of the time by rm -rf ing node_modules and running yarn again. Other times, the fix was more involved. Some packages unsafely depend on their dependencies being installed in a node_modules directory contained within the module directory. Unlike npm , Yarn tries to install all dependencies (transitive dependencies included) in your project’s root node_modules folder, so some packages break as a result. Luckily, we were able to resolve this issue by upgrading our dependencies since the authors of the underlying packages had fixed the issue. Not all npm environment variables are implemented by Yarn (eg npm_package_directories ). We wound up circumventing this issue by ending our usage of the offending package (for reasons unrelated to Yarn), but some packages might require updates. The --mutex network option is handy for when you’re running multiple instances of yarn simultaneously. Rarely, we’d see 404s when attempting to install private packages from the npm registry. This was often resolved by re-running the install command. When that didn’t work, we were able to resolve the issue by running npm login to regenerate our npm authentication tokens. Going all-in We didn’t see any major issues running the initially converted services with Yarn after a couple of weeks, so we decided to move forward with updating our remaining services ad modules. To help make this process easier, we wrote a module that stripped out our shrinkwrap configuration, generated a yarn.lock file, ran the project’s tests as a sanity check, ran yarn check to ensure the validity of the generated lockfile, and output a list of manual steps to be taken afterward in order to finish the update. This script helped us ensure that we were updating our projects correctly throughout the 75-project sweep. In this last phase, we ran into a couple of issues: We encountered an issue with installing phantomjs-prebuilt on Codeship (our CI service), consistent with this issue . There are several proposed workarounds in that thread; we've added node node_modules/phantomjs-prebuilt/install.js to our Codeship configuration. We also ran into some issues stemming from Codeship caching our node_modules (in accordance with our configuration). Resetting Codeship’s dependency cache fixed these issues for us. Cast-off Yarn isn’t without its own bugs, but its community momentum and energy as well as its prioritization of speed and reliability excite us and have already proven valuable. Since last week, we’ve saved hours of development time that without Yarn would have been spent on npm install s (and, frustratingly, repeated npm install s after consecutive failures). Yarn has also enabled us to resume our tradition of having new engineers ship code to production on their first day, which had previously stalled due to lengthy and error-prone npm install s. Shoot us a message if you’re interested in joining a culture where engineering speed and autonomy are top priorities.", "date": "2017-04-24"},
{"website": "Mix-Max", "title": "Batching CloudWatch metrics", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/batching-cloudwatch-metrics/", "abstract": "tl;dr - We saw a noticeable decrease in our AWS bill by batching CloudWatch metrics. Insight into all aspects of our company is core to Mixmax, as such we love our metrics. We have metrics all the way from product right through internal development. We put metrics on everything. One place we store and inspect a lot of our engineering metrics is AWS CloudWatch as it allows us to seamlessly integrate metrics into our alerting and monitoring system. CloudWatch Metrics CloudWatch data metrics are awesome because you can add extra dimensions to them. This gives you the ability to segment them visually in the CloudWatch dashboard and to build highly granular alerts. Since the AWS CloudWatch API is also pretty easy to use, we can programmatically build alerts and dashboard when we deploy the gathering of a new data metric to AWS. Why do we need to batch them? For a long time, we sent metrics to AWS as soon as they happened and everything was happy. As we began to scale ever and ever larger however, we found that there was a default 150 put-metric-data calls per second rate limit, so we decided to batch our requests. We didn’t want to have to jump through any hoops in modifying our code to do this when sending requests to CloudWatch, so we open sourced a super easy to use Node module for batching these put-metric-data requests: cloudwatch-metrics . Initializing cloudwatch-metrics By default, the library will log metrics to the us-east-1 region and read AWS credentials from the AWS SDK's default environment variables.\nIf you want to change these values, you can call initialize: var cloudwatchMetrics = require('cloudwatch-metrics');\ncloudwatchMetrics.initialize({\n    region: 'us-east-1'\n}); Creating metrics Creating a metric is pretty basic, we simply need to provide the namespace and the type of metric: var myMetric = new cloudwatchMetrics.Metric('namespace', 'Count'); We can also add our own default dimensions: var myMetric = new cloudwatchMetrics.Metric('namespace', 'Count', [{\n    Name: 'environment',\n    Value: 'PROD'\n}]); The metric constructor also accepts a set of optional arguments to control: whether we actually send the metric (useful for dev environments), a callback in case a request to CloudWatch fails, the default interval to wait before sending metrics and a max capacity of events to buffer before we send to CloudWatch (useful if you’re buffering a lot of events in a bursty fashion). var myMetric = new cloudwatchMetrics.Metric('namespace', 'Count', [{\n    Name: 'environment',\n    Value: 'PROD'\n}], {\n    sendCallback: (err) => {\n        if (!err) return;\n        // Do your error handling here.\n    },\n    enabled: true, // Set to false if you don't want to send data (i.e. a dev environment)\n    maxCapacity: 30, // The default value is 20\n    sendInterval: 3*1000 // The default value is five seconds and is in milliseconds\n}); Sending metrics to CloudWatch Sending data for a metric to CloudWatch is then extremely simple: myMetric.put(value, metric, additionalDimensions); The only to keep in mind is that data is sent asynchronously to the server, so when this function is called it will not immediately send data to CloudWatch. It will wait for the sendInterval to expire or for the maxCapacity to be reached, whichever happens first. Why you should batch your CloudWatch metrics As we said before, there is a rate limit on how many data points you can send for your CloudWatch metrics. You can of course have this changed, but as you’re also charged per put-metric-data request you can save a lot of $$$ by batching your requests. In fact, we saw a very noticeable decrease in our month AWS bill! The one constraint to keep in mind with this, is that POSTing put-metric-data calls to AWS CloudWatch is capped at 40KB per request, so there is a limit to how large a request can be. Enjoy simplifying infrastructure costs without jumping through hoops? Drop us a line .", "date": "2017-01-03"},
{"website": "Mix-Max", "title": "Arena: an open-source UI for Bull/Bee queues", "author": ["Randall Ma"], "link": "https://www.mixmax.com/engineering/introducing-bull-arena/", "abstract": "Update 7/14/17: Mixmax switched over to using Bee-Queue instead of Bull, but Arena continue to work for both. At Mixmax, we use the work queue library Bull to power all our distributed queue needs. It is a critical component of our infrastructure and we rely heavily on it to power core functionalities like sending emails and syncing contacts. Despite our high reliance on Bull as a core piece of our infrastructure, up until recently we did not have an easy way to view the status of our queues. To replace an assortment of hard-to-use scripts to retrieve data from Redis, we wrote and open-sourced Arena . Arena solves many operational problems we encountered running Bull at scale. For example, here at Mixmax, we process thousands of jobs per minute. Inevitably, some jobs fail. In the event of failure, we need to be able to quickly pinpoint and recover from these failures without putting extra stress on the queue, which was a problem we encountered with some other Bull UIs that polled for jobs, causing us high performance overhead. We have big ideas for Arena in the future -- visualizations, live updates triggered by Bull events, and information about how Bull load balances different hosts -- to name just a few. We're excited to see Arena grow and hope the community will come along for the ride. Next up, see how you can use the AWS SDK to auto-discover Bull queues and auto-configure Arena . Tweet at us @MixmaxHQ and let us know what you think. And if you’d like to help scale our queueing architecture, drop us a line at careers@mixmax.com .", "date": "2017-06-16"},
{"website": "Mix-Max", "title": "Using CORS policies to implement CSRF protection", "author": ["Eli Skeggs"], "link": "https://www.mixmax.com/engineering/modern-csrf/", "abstract": "This post is a follow-on to our CORS post back in December. We'll describe how\ntraditional CORS policies aren't sufficient defense against cross-site request forgery (CSRF)\nattacks, and unveil a new Node module that layers CSRF protection on top of such policies, cors-gate . We'll show how an Origin-based approach has fewer moving parts than CSRF,\nand pairs neatly with CORS to protect your users against CSRF attacks. Note that this approach\ndepends on modern browser functionality, and will not work if you're targetting older browsers. Using the Origin and Referer headers to prevent CSRF Cross-Site Request Forgery (CSRF) allows an attacker to make unauthorized requests on behalf of a\nuser. This attack typically leverages persistent authentication tokens to make cross-site requests\nthat appear to the server as user-initiated. Prior to our mitigation, a user visiting a third-party\nwebsite while logged in to Mixmax could allow that website to make unauthenticated requests. Using\nCSRF, that website could execute actions with the user's Mixmax credentials. We previously discussed using CORS to secure user data , while allowing some\ncross-origin access. CORS handles this vulnerability well, and disallows the retrieval and\ninspection of data from another Origin. Without the cooperation of Mixmax servers, CORS will prevent\nthe third-party JavaScript from reading data out of the image, and will fail AJAX requests with a\nsecurity error: XMLHttpRequest cannot load https://app.mixmax.com/api/foo . No 'Access-Control-Allow-Origin' header\nis present on the requested resource. Origin ' http://evil.example.com ' is therefore not allowed\naccess. Moreover, for preflighted requests, the browser will make an OPTIONS request prior to making the\nactual data-laden request, so the server won't have a chance to perform the unauthorized request.\nFor simple requests, however, the browser makes the preflight request and simply disallows reading\ndata from the response if the server does not give explicit permission. If the server isn't careful,\nthough, it can still process the request. Until recently, Mixmax was vulerable to cross-site request forgery in spite of our existing CORS\nimplementation. CORS, after all, does not restrict access to data, but instead instructs the browser\nto specifically allow access to responses from cross-origin requests. As such, while it accidentally\ndisables some cross-origin actions (by nature of the OPTIONS preflight), it does not block all\nrequests. A user visiting a third-party site could have that site send an email on their behalf, log\nthem out, or even log them into an attacker-controlled account. Even safe GET requests, which per\nthe HTTP specification should not cause side-effects, can unnecessarily consume resources and may\nrepresent a denial-of-service (DOS) attack vector. There are a variety of ways to defend against such attacks. The simplest is to\ncheck that the request originates from a trusted site, using the Origin request\nheader. This is also the top solution recommended by the OWASP . The Origin header is the same header examined by the cors Node module when adding CORS\nresponse headers. However, such modules generally stop short of failing requests , as a matter of complying with the CORS\nspecification and separating the concerns of allowing vs restricting access . Per\ncors' maintainer, \"a CORS failure should look exactly as if the server has no idea what CORS is - in\nwhich case the request will still go through ,\" (just without the CORS\nheaders). Another reason CORS modules avoid implementing CSRF protections is that browsers may\nnot send the Origin header, as in the following cases: img tags will not send Origin headers unless the crossorigin attribute is set Chrome and Safari will not send Origin headers with same-origin GET requests Firefox will not send the Origin header with any same-origin requests (bug) Firefox will not send the Origin header with cross-origin form POSTs (bug) However, in all of these cases, the browser does send a Referer header. We can make use of the Referer header, which browser-initiated requests may not spoof, in place of the missing Origin header. We have an additional constraint: because we identify the current user by the user query parameter\nlike user=foo@example.com , we have to make sure the Referer doesn't leak to third-party\nwebsites. We can prevent such leaks with the relatively new Referrer-Policy header. When sent by\nour services, this header governs the conditions under which the browser may expose referrer data to\nthe same or other services within HTTP requests. We would ideally set a policy of strict-origin ,\nas this gives us the requester's origin without any sensitive path information and without exposing\nthe user's browsing history over an insecure connection. Chrome does not support strict-origin as of 06.12.2017 . Its implementation\nonly allows no-referrer (and thus no origin, in cases where Origin is not available), no-referrer-when-downgrade (exposes path information), and origin-when-cross-origin , same-origin and unsafe-url , all of which leak referrer data over HTTP. We chose the best available\noption, no-referrer . Safari doesn't support Referrer-Policy at all, but rather an older\ndraft of the specification with never , always , origin , and default values,\nthe latter being equivalent to no-referrer-when-downgrade . We again choose the best available\noption, never , to avoid leaking referrer data over HTTP. Thankfully, Firefox does support strict-origin . This lets us accomplish the crucial goal of\npreventing CSRF attacks while preserving permissible same- and cross-origin access. When Chrome and\nSafari add support for strict-origin , we can prevent unauthorized cross-origin access even to GET\nrequests. During our implementation, we came across a final quirk of Chrome's implementation: the Referrer-Policy has unintended consequences on form-submitted POST requests. As of Chrome 58, the no-referrer policy makes form POSTs send Origin: null for both same- and cross-origin POSTs.\nSafari sends the correct Origin header regardless of the presence of the meta referrer. With Referrer-Policy set to strict-origin , Firefox sends no Origin header, but does send the Referer. cors-gate To implement this defense, we published cors-gate , a module which halts request\nprocessing when the request does not definitively originate from a trusted domain. To best\ninteroperate with our existing CORS middleware, cors-gate comes after the cors module, and reads the\nresponse headers to determine whether the request should be allowed per CORS. The module also checks\nthe Origin against the server's current origin, as defined at startup. The middleware permits all\nsafe requests ( GET , HEAD ) by default, as we cannot reliably determine their Origin, and they\nshould have no side effects. This snippet sets up an Express app to permit cross-origin requests from https://app.mixmax.com and https://other-app.mixmax.com to https://api.mixmax.com . Requests from other origins will\nfail outright, while requests from https://api.mixmax.com (same-origin requests) will continue to\nfunction. const cors = require('cors');\nconst corsGate = require('cors-gate');\n\n// if the Origin header is missing, infer it from the Referer header\napp.use(corsGate.originFallbackToReferrer());\n\n// the expressjs/cors module\napp.use(cors({\n  origin: ['https://app.mixmax.com', 'https://other-app.mixmax.com'],\n  credentials: true\n}));\n\n// prevent cross-origin requests from domains not permitted by the preceeding cors rules\napp.use(corsGate({\n  // require an Origin header, and reject request if missing\n  strict: true,\n  // permit GET and HEAD requests, even without an Origin header\n  allowSafe: true,\n  // the origin of the server\n  origin: 'https://api.mixmax.com'\n})); As a consequence of our chosen microservices architecture, clients and other microservices may make\nrequests to the same endpoint. Moreover, we have an API gateway that enables third-party developers\nto build atop our platform , and we wish to allow cross-origin requests for third-party\nclients - provided they do not depend on the user being authenticated on our domains. To these ends,\ninstead of immediately halting requests from unauthorized domains, we force require these requests\nto authenticate with an API-token, instead of with a cookie. Server-to-server and permitted\nthird-party client-to-server requests can thus continue to operate, enabling their interaction with\nour services. Alternatives for mitigating CSRF We chose to implement a new module to mitigate CSRF attacks. Alternative solutions to CSRF\nprotection also exist. To run down https://github.com/pillarjs/understanding-csrf as of 06.13.2017: Use only JSON APIs (vs. e.g. application/x-www-form-urlencoded ) The unspoken assumption of this guideline is that a Content-Type header of application/json will\ntrigger CORS preflighting, and if you haven't enabled CORS, the browser won't issue the actual\nrequest. This is all well and good except for that navigator.sendBeacon doesn't trigger preflight requests . This solution is also undesirable for APIs where you do want to enable CORS, since preflighting\nwill make your trusted requests slower. Disable CORS As discussed in the overview, this will just prevent client-side JavaScript from accessing the\nresponse to a malicious request. It doesn't necessarily stop the server from responding, which is\nthe essence of CSRF attacks. Check the Referer header This is almost equivalent to the proposed solution, but is incompatible with servers that set the Referrer-Policy header to protect their referrer data. By contrast, Referrer-Policy: no-referrer does not suppress the Origin header for AJAX requests. GET should not have side effects This is good advice regardless of CSRF, especially because no browser sends the Origin header with\nsame-origin GET requests. \"Avoid using POST\" and \"don't use method-override !\" Like the \"use only JSON\" rule, these guidelines assume that requests will hit preflighting and fail.\nIn this case, fair enough, since navigator.sendBeacon only uses POST. It is onerous to avoid using\nPOST, though. Don't support old browsers Our proposed solution relies on this. CSRF tokens Depending on how CSRF tokens are implemented, they can indeed be extremely robust, and address the\ndisadvantages of cors-gate by locking down same-origin GET requests. We determined that this disadvantage is tolerable, so we prevent CSRF by using the simpler proposed\nsolution. CSRF tokens introduce additional complexity for both clients and servers, and add\nnon-negligible overhead due to their reliance on additional state. Acknowledgements Thanks to Jeff Wear for his diligent background research. Thanks to Douglas Wilson for his work in maintaining Express' cors module and informing the development of cors-gate. Interested in creating simple solutions to complex security problems? Join us!", "date": "2017-06-13"},
{"website": "Mix-Max", "title": "React without Redux", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/backbone-to-react-without-rewriting/", "abstract": "Background Mixmax was originally built using MeteorJS , a full-stack JavaScript framework that enabled us to build features extremely quickly. As our product grew in scope, we began breaking our architecture down into microservices to improve performance . Some of the views served by these microservices (such as the compose window email editor we display in your inbox) were fairly static and could be rendered mostly server-side. Since our team was fairly experienced with Backbone and Handlebars and those technologies satisfied our requirements, we used them. Over time, however, Mixmax's scope has grown significantly. In addition to writing rich emails with our advanced email editor, our users can now see team-wide insights into their communications , create sophisticated email campaigns using Sequences , and share availability using their Mixmax calendar . As we built these features, we noticed some shortcomings in Backbone: Rerendering Backbone views could cause cursor focus and scroll position to be lost because our Backbone render functions fully replaced the innerHTML of the views' root elements. Rerendering Backbone views would often result in unnecessary work being done (relative to something like the virtual DOM), which meant that users saw more spinners and glitchineess than was necessary when using Mixmax. Configuring Backbone views to rerender when data changed involved writing code that was relatively verbose and complex. These issues' significance swelled over time. We recently reached a tipping point when implementing double-click-to-rename became a multi-day project and thus decided set out to solve these issues. Potential solutions We initially identified three classes of solutions. Use Backbone differently Some of our issues were resolvable without changing frameworks. For example, in some cases, to avoid losing scroll position, we could update only a specific part of the UI using view functions rather than rerendering entire, top-level views using Backbone Views' render functions. Unfortunately, this effectively meant writing a custom virtual DOM implementation using jQuery in every Backbone view that needed this sort of updating, which seemed extremely costly and complex. We trialed this approach in one view and proved this costliness to ourselves, then ruled it out. // ...\n\n// An example of one of our \"custom virtual DOM\" functions\n/**\n* Updates a template folder's name among the user's list of template folders.\n*\n* @param {TemplateFolderModel} templateFolderModel\n*/\n_updateTemplateFolderName(templateFolderModel) {\n  const id = templateFolderModel.get('_id');\n  this.$(`.js-tag-item[data-id=${id}]`).find('.js-snippet-tag-name').text(templateFolderModel.get('name'));\n}\n\n// ... Keep Backbone's data management and replace its rendering We envisioned an intermediate solution where we would continue using Backbone and our Meteor-like publications libraries ( 1 , 2 ) that we had integrated with Backbone to manage our client-side data, but replace Backbone's UI rendering. This solution would allow us to continue using all of the Backbone models and collections we had already written, therefore lowering the cost of adopting a new framework. Replace Backbone entirely Last, we considered replacing Backbone altogether. The only advantage of this solution we imagined was that we might somehow benefit from being all-in on a new framework. The downside of this approach was that replacing our usage of Backbone entirely would be very expensive in terms of engineering time. We didn't discover any reasons for this cost being worthwhile in our investigation of frameworks, so we decided on the intermediate solution: replacing Backbone's rendering layer. Selecting a framework Considerations We based our investigation into view framworks on a number of requirements and considerations. Given our choice to replace only Backbone's view layer, we required that our new framework interoperate with Backbone models and collections, not require us to rewrite any existing views to minimize adoption cost, and be mature/well-backed for reliability. Additionally we hoped that the new framework would increase our speed of development, be easy-to-use by designers (ie markup-like), enable us to easily write modular code, and be compatible with our existing client-side routing. Selecting React After eliminating most frameworks we came across based on immaturity (eg Preact and Inferno ), we narrowed our candidate frameworks down to Ractive , React, Angular, and Vue. We eliminated Vue because we felt its directives would be less intuitive to designers than JSX or Handlebars-like syntax and because it lacked any apparent, significant advantages over React while being much less mature than React. We liked Angular for being being widely used in production, but eliminated it based on there not being a straightforward way to integrate it with Backbone and it having a steep learning curve. We liked Ractive because it shared many of the benefits React offered while having Handlebars-like templating syntax, which we liked and knew well. Unfortunately, Ractive didn't seem to be widely used in production ( 1 , 2 ) and seemed to be maintained by a small handful of people. We noticed that the project's GitHub issues were not resolved quickly and that there were TODO s in the project's docs, which concerned us. React proved to be the best option, satisfying all of our requirements and nearly all of our bonus desires. Its disadvantages were that its syntax would be a bit confusing for designers who didn't know JavaScript well and, since we wouldn't be using a centralized data store like Redux , we'd sometimes be forced to pass event handlers down through many layers of components when lifting state . Mixmax + React We've developed a number of React patterns since beginning to use the framework. To integrate React components into our existing Backbone view hierarchies, we've been using ReactDOM.render within Backbone views' render functions. // ...\n\nconst MeetingTypesDashboardView = Backbone.View.extend({\n  render() {\n    ReactDOM.render(\n      <MeetingTypesDashboard />,\n      this.el\n    );\n\n    return this;\n  },\n\n  remove() {\n    ReactDOM.unmountComponentAtNode(this.el);\n    MeetingTypesDashboardView.__super__.remove.apply(this);\n  }\n});\n\n// ... We've been using MongoDB's open-source connect-backbone-to-react library to create components (using a higher-order component from that library) that update when our Backbone models and collections change. Doing so has made integrating Backbone with React straightforward and fun! We've also been creating \"container\" and \"presenter\" components as explained by Dan Abramov . Dividing our React components like this has allowed us to achieve the sort of \"view\" and \"controller\" separation we enjoyed with Handlebars templates and Backbone Views. We've published some gists documenting our React patterns in an example container component and presenter component if you'd like to check out all of them! In our few weeks with React, we've written two Mixmax Dashboard sections in it: Meeting Types and Rules .  We're already loving how easy it is to build reusable UI components and how our struggles with Backbone, like maintaining scroll position across renders, are long-gone. Shoot us a message if you're interested in writing React with us :)", "date": "2017-07-26"},
{"website": "Mix-Max", "title": "How to Write an Engineering Blog Post", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/how-to-write-an-engineering-blog-post/", "abstract": "This blog post is the final post of the Mixmax 2016 Advent Calendar . The previous post on\nDecember 11th was about a lightweight alternative to automated integration tests . When talking with friends about these last twelve days ’\nposts, the most common reaction hasn’t been to any individual post but rather the concept itself:\n“how do you have so much stuff to write about?” The glib answer would have been “well, Mixmax does a\nlot!” But (while that’s true :), some team members actually expressed the same worry when we first\ntalked about the idea — even though they are all brilliant engineers, they didn’t immediately see\nwhat they had to contribute. In response, we came up with some strategies that everyone could use to\nwrite a post. And in the spirit of giving with which we began the series, we’ll finish by sharing these strategies with you. Talk about what you’ve built The first and easiest strategy is to talk about what you’ve built . This is easy for Mixmax engineers\nbecause all of our projects begin with an architecture proposal and/or implementation spec, and we default to open-sourcing new infrastructure. So,\nwhen it comes to writing about this work, we basically already have a draft! We just have to take the\nspec or README, add a little bit of background on the technologies involved at the beginning, and add\na little bit about our use case at the end. My colleague Trey’s post about securing server-side requests with our rewt library is a great example of this strategy. The bulk of\nthe post is almost a direct copy of the “usage” part of rewt’s README. To make it a post, he just had\nto describe how and why we built rewt, by talking about different approaches to securing server-to-server\ncommunication and JSON web tokens at the beginning, and then giving an example of how we use rewt to\nsign and verify HTTP requests at the end. Talk about what you’ve learned The next strategy for writing an engineering blog post is to talk about what you’ve learned . This\nis closely related to talking what you’ve built — what makes these posts different is talking\nabout the process, rather than the results. You can also talk about what you’ve learned integrating\nwith technology you didn’t build, for instance 3rd-party APIs. If you’ve found gaps in their\ndocumentation — or even outright bugs — that’s a blog post waiting to happen. My colleague Cameron’s post about integrating with the Gmail Pub/Sub API is a great example of this strategy. All he had to do to write a post was collect the various bugs he\nhad found and support them with links to the API’s documentation. My post about requiring Node built-ins with Webpack is another example. Here the topic was less that Webpack was buggy than that its documentation wasn’t\nclear. But even when the problem’s on the author’s end ( PICNIC ),\ndocumenting missteps will help other developers avoid similar pitfalls. Talk about what you think A theme connecting the strategies we’ve talked about so far is that when it comes to writing a post,\nyou don’t have to make stuff up — you can just write about what you know. But the posts we’ve\ntalked about haven’t just been “facts” — how something works or doesn’t work. If they were, they’d\nbe rather dry and even unsatisfying. What makes a great post is the author’s view on why something\nought to work a certain way. And this leads us to our final strategy for writing an engineering blog\npost: talk about what you think . Your opinions are valuable (no, really). Engineers are always looking for new ideas and new ways to\norganize their thoughts. And what may seem like old hat to you may be completely new to someone else\nsince there's so much to learn and technology is ever-changing. These two facts explain the endless parade of React tutorials and ES6 explainers. And they meant that\nit was totally fine for us to publish our own contribution to the latter category. What makes my colleague Chuy’s post particularly interesting isn’t his code\nsamples, it’s his perspective on why he chose to discuss these APIs: While some of the new ES6 features can be considered syntax sugar, they also open the door for much\nmore concise and understandable code. We all may have to read something multiple times before we get it. Your perspective on a “familiar”\ntopic may be what finally clicks for someone else. You too can write an engineering blog post After sharing these tips with my friend, the one who’d originally asked about how the advent calendar\nwas possible, he still demurred, saying that his company’s legal department would never let him\npublish what he was working on. And to this I say — start small! Document what you’ve built, if\nonly internally, to make things easier for the next developer. Try your hand at writing a spec, and\nmaybe you’ll prove your ideas worthy of implementation. Write a post about how your company’s\nalready-public APIs can be used to build cool things. Or, if you’d like to default to building new tech, open-sourcing it, and talking about it, you can\ncome work at Mixmax. :) Join us!", "date": "2016-12-12"},
{"website": "Mix-Max", "title": "Using Arena with AWS ElastiCache hosted Redis", "author": ["Randall Ma"], "link": "https://www.mixmax.com/engineering/bull-queue-aws-autodiscovery/", "abstract": "Update 7/14/17: Mixmax switched over to using Bee-Queue instead of Bull, but Arena continue to work for both. Last week, we introduced our open-source UI for Bull queue, Arena . Today, we would like to share a piece of configuration magic that we use to automatically configure Arena without having to touch any configuration files or environment variables. This enables Arena to automatically discover new queues added in new Redis instances. By default, Arena reads its configuration from a JSON file, which individually configures its internal mapping from Redis endpoints to Bull queues. At Mixmax, we use AWS extensively. In particular, we use ElastiCache to host our Redis instances and Elastic Beanstalk to host our Arena Bull monitoring service. Because of AWS's flexibility, all we need to do to look up our Redis endpoint URIs is add a single inline policy -- describeReplicationGroups -- to an Elastic Beanstalk role. {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"...\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticache:DescribeReplicationGroups\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n} Then, with a single AWS SDK call and some clever data mapping, we can pass an array of queues to Arena, running as a module for advanced configuration. async function discoverQueues() {\n  // Get AWS SDK output.\n  const {ReplicationGroups} = await elasticache.describeReplicationGroups().promise();\n\n  // Filter out Queue endpoints not found in the whitelist.\n  // Then, build out Queue 'templates' - Arena config objects missing the 'name' key.\n  const templates = _.chain(ReplicationGroups)\n    .filter((group) => group.ReplicationGroupId.includes('production'))\n    .map((group) => [group.ReplicationGroupId, _.first(group.NodeGroups).PrimaryEndpoint])\n    .object()\n    .renameAttrs('Address', 'host')\n    .renameAttrs('Port', 'port')\n    .mapObject((group, groupId) => _.extend(group, {'hostId': groupId}))\n    .value();\n\n  // Look up Queues from a Redis endpoint.\n  // We store the names of Queues in a key called `bull-queues` on the same Redis instance hosting the Queues.\n  const fetchPromises = _.chain(templates)\n    .mapObject((endpoint) => getQueueNamesFromRedis(endpoint.hostId, endpoint.host, endpoint.port))\n    .toArray()\n    .value();\n  const allHostQueues = await Promise.all(fetchPromises);\n\n  // Use the Queue templates and fetched Queue names to construct an array to pass to Arena.\n  const queues = [];\n  for (let [hostId, name] of _.flatten(allHostQueues, true)) {\n    const queue = _.extend({name}, templates[hostId]);\n    queues.push(queue);\n  }\n  return queues;\n} This allows any engineer at Mixmax to add a new Bull queue for any reason, and have that queue reflected in our Arena dashboard automatically. At Mixmax, we love to optimize for the workflows of our engineers. If you'd like to help build out our open-source tooling, drop us a line at careers@mixmax.com .", "date": "2017-06-22"},
{"website": "Mix-Max", "title": "API Paging Built The Right Way", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/api-paging-built-the-right-way/", "abstract": "Mixmax has a very extensive list of APIs that give our users direct access to all of their data. Some APIs, such as Contacts can return millions of results. We obviously can't return all of them at once, so we need to return a subset - or a page - at a time. This technique is called paging and is common to most APIs. Paging can be implemented in many different ways, some better than others. In this post we discuss various implementation approaches and how we ultimately implemented our own approach to serve data stored in MongoDB . We also introduce a new npm module to easily allow you to do the same. Common paging implementation approach #1: Offset-based paging (“numbered pages”) This is one of the more common pagination approaches. The caller of the API passes a page (aka offset or skip ) and limit (aka count or per_page ) parameters. The limit is the number of results it wants and the offset is how many items to “skip” over before starting to return results. For example, Github offers an offset-based API for fetching Github Repos that looks like this: curl https://api.github.com/user/repos?page=2&per_page=100 This pattern has a big flaw: if the results list has changed between calls to the API, the indices would shift and cause an item to be either returned twice or skipped and never returned. So in the above example, if a Github repo is deleted after you query the first page of results, then the previous 101st entry (that wasn’t in your original first page of 100) will now be the 100th entry, so it won’t be included in your second page of results (that starts at 101). Common paging implementation approach #2: Time-based paging A second approach is time-based paging. So if your API (in this case, /items) returns results sorted by newest-first, you could pass the created date of the last item in the list to get the next page of items created before it: Example API curl https://api.mywebsite.com/items?created_before_timestamp=1505086265160&limit=50 This solves the problem in Common Approach #1 above because results are no longer skipped. If you query the first page, and then a new item is deleted, it won’t shift the results in your second page and all is fine. However, this approach has a major flaw: what if there is more than one item that was created at the same time? So if there were 51 items with the created timestamp of exactly 1505086265160 , then when we query the next page, we’ll miss the 51st entry because we’re querying items created before 1505086265160. You’ll miss results. The right approach: Cursor-based paging The best way to build API pagination in a safe way is for the API to return a “cursor”, or opaque string, with the list of results. This token is usually called next or next_cursor or starting_after This token can be passed with the next API request to request the next page. Here’s an example from the Twitter API: curl https://api.twitter.com/1.1/followers/ids.json?screen_name=theSeanCook&cursor=1374004777531007833 By returning a “cursor”, the API guarantees that it will return the exactly the next entry in the list, regardless of what changes happen to the collection between API calls. Think of the cursor as permanent marker in the list that says “we left off here”. Implementing cursor-based pagination (in MongoDB) So we’ve described cursors as the right approach for API pagination, but how do we actually implement them? If you happen to be using Node and MongoDB, you’re in luck, because we recently published a library that will do it for you. It’s called mongo-cursor-pagination . I’ll briefly describe how it’s implemented and our thinking behind it. Since a cursor is just a marker to say “we left off here”, we can simply make it the value of a field in the collection. However, that field must satisfy the following properties: Unique. This value must be unique for all documents in the collection. Otherwise we’ll run into the problem in Common Approach #2: if multiple documents have the same value, then we can’t “pick up exactly where we left off” because not all documents can be distinguished from one another. This also means that all documents must have this field set to something, otherwise MongoDB considers it null. Orderable. Every value of this field must be able to be compared to every other value. This is because we need to sort by this field in order to get a list of results to then return in pages. If the field is a number int , string , or any other sortable datatype in MongoDB, then it satisfies this. Immutable. The value in this field cannot change, otherwise the pages wouldn’t return the right information. For example, if you select the field “name” as the cursor field, and a name is changed while someone is fetching results, then that entry might be skipped. So let’s say that we’re choosing the standard MongoDB field _id for cursor pagination. This is a reasonable choice as it satisfies the above criteria and always exists on all documents in MongoDB. In this example, we’ll implement an API that returns 2 items. It will return a “cursor” (really just the string value of the last _id ) that the caller can pass to get the next page: curl https://api.mixmax.com/items?limit=2 const items = db.items.find({}).sort({\n   _id: -1\n}).limit(2);\n\nconst next = items[items.length - 1]._id\nres.json({ items, next }) Then, when the user wants to get the second page, they pass the cursor (as next ) on the URL: curl https://api.mixmax.com/items?limit=2&next=590e9abd4abbf1165862d342 const items = db.items.find({\n  _id: { $lt: req.query.next }\n}).sort({\n   _id: -1\n}).limit(2);\n\nconst next = items[items.length - 1]._id\nres.json({ items, next }) This solution works great because we’re returning results sorted by _id , which in MongoDB happens to the creation time rounded to the nearest second plus some other entropy. But let’s say we want to return results in a different order, such as the date the item was introduced into an online store, launchDate . To make it clear in the example, we’ll add sort=launchDate to the querystring: curl https://api.mixmax.com/items?limit=2&sort=launchDate const items = db.items.find({}).sort({\n   launchDate: -1\n}).limit(2);\n\nconst next = items[items.length - 1].launchDate;\nres.json({ items, next }) Then, to fetch the second page, they’d pass the next cursor (which happens to be launch date encoded as a string): curl https://api.mixmax.com/items?limit=2&sort=launchDate&next=2017-09-11T00%3A44%3A54.036Z const items = db.items.find({\n  launchDate: { $lt: req.query.next }\n}).sort({\n   _id: -1\n}).limit(2);\n\nconst next = items[items.length - 1].launchDate;\nres.json({ items, next }); However, what if we launched a bunch of items on the same day and time? Now our launchDate field is no longer unique and doesn’t satisfy the above requirements. We can’t use it as a cursor field. But wait, there is a way: we could use two fields to generate the cursor! Since we know that the _id field in MongoDB always satisfies the above three criteria, we know that if we use it alongside our launchDate field, the combination of the two fields would satisfy the requirements and could be together used as a cursor field. This would also allow the user to continue to get the response sorted by launchDate : curl https://api.mixmax.com/items?limit=2&sort=launchDate const items = db.items.find({}).sort({\n   launchDate: -1,\n  _id: -1 // secondary sort in case there are duplicate launchDate values\n}).limit(2);\n\nconst lastItem = items[items.length - 1];\n// The cursor is a concatenation of the two cursor fields, since both are needed to satisfy the requirements of being a cursor field\nconst next = `${lastItem.launchDate}_${lastItem._id}`;\nres.json({ items, next }); Then to fetch the subsequent page we’d call: curl https://api.mixmax.com/items?limit=2&sort=launchDate&next=2017-09-11T00%3A44%3A54.036Z_590e9abd4abbf1165862d342 const [nextLaunchDate, nextId] = req.query.next.split(‘_’);\nconst items = db.items.find({\n  $or: [{\n    launchDate: { $lt: nextLaunchDate }\n  }, {\n    // If the launchDate is an exact match, we need a tiebreaker, so we use the _id field from the cursor.\n    launchDate: nextLaunchDate,\n  _id: { $lt: nextId }\n  }]\n}).sort({\n   launchDate: -1,\n   _id: -1,\n}).limit(2);\n\nconst lastItem = items[items.length - 1];\n// The cursor is a concatenation of the two cursor fields, since both are needed to satisfy the requirements of being a cursor field\nconst next = `${lastItem.launchDate}_${lastItem._id}`;\nres.json({ items, next }); Now we have exactly what we want: an API that allows the user to get paginated results sorted by launch date. As new items are launched and old items are deleted, the paged results won’t be disrupted and an item will never be duplicated or missed in a response. The above code examples explain our thinking behind building the mongo-cursor-pagination library. If you use the library, you won’t need to write the above code - the module will do it for you. We’re already using mongo-cursor-pagination at Mixmax to serve millions of documents via our developer API . A note about MongoDB indexes When using the module that runs queries similar the example code above, it’s important that you create the proper MongoDB indexes. This is especially important for large collections (>1k records) as you might accidentally slow your database. So for the above query, always be sure to create a index (as explained here ) that covers all the properties used in the query along with the cursor field (called the paginatedField in the module) and the _id field. Conclusion Using our cursor-based paging technique and MongoDB, we're able to serve millions of API requests a day. Our APIs respond in the same amount of time regardless of how many resources they're serving, or which page is be queried. Do you want to create cool open source modules with us? Drop us a line .", "date": "2017-09-13"},
{"website": "Mix-Max", "title": "SF ReactJS Meetup: hosted at Mixmax", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/sf-reactjs-meetup/", "abstract": "I recently had the pleasure of presenting at the SF ReactJS meetup which was hosted here at Mixmax. I presented on a blog post that I previously wrote on moving from Backbone to React . The turnout was great! Here is a clip of me presenting: Here are the slides from my presentation : Interested in working on problems like these? Join us !", "date": "2017-10-10"},
{"website": "Mix-Max", "title": "SFNode Meetup: hosted at Mixmax", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/sf-nodejs-meetup/", "abstract": "Last Thursday I presented at the SFNode meetup which we hosted here at Mixmax. My presentation was on Promise patterns in Node 6 and beyond. Here's my presentation: Promises can be an effective way of avoiding a tangled mess of nested callbacks, especially with the introduction of async/await. However, it's not always easy to integrate existing, callback-based modules with newer Promise-based ones and it's an even larger task to migrate existing code to use Promises. We faced this exact problem at Mixmax where we were torn between existing infrastructure using callbacks and the strong appeal of switching to Promises for future development. As a result we developed a few libraries (promise-callbacks, promise-pool, promise-iterate) and techniques (using transpilation) that allowed us to immediately integrate Promises and async/await into existing code without drastic, bottom-up changes. Take a look at the full slides with helpful examples: Here are useful links from the presentation: Gist with examples promise-iterate npm module promise-callbacks npm module promise-pool npm module Interested in joining the open source community? Join us !", "date": "2017-11-08"},
{"website": "Mix-Max", "title": "Announcing Bee-Queue v1.0", "author": ["Eli Skeggs"], "link": "https://www.mixmax.com/engineering/bee-queue-v1-node-redis-queue/", "abstract": "Today we're announcing the v1.0 release of Bee-Queue : a fast, lightweight, robust Redis-backed job queue for Node.js. With the help of Bee-Queue's original author, Lewis Ellis, we revived the project to make it the fastest and most robust Redis-based distributed queue in the Node.js ecosystem. Mixmax is using Bee-Queue in production to process tens of millions of jobs each day . Bee-Queue is meant to power a distributed worker pool and was built with short, real-time jobs in mind. Scaling is as simple as running more workers, and Bee-Queue even has a useful interactive dashboard that we use to visually monitor job processing. Here is how Bee-Queue v1.0 compares to other Redis-based job queues in the Node ecosystem (including its own prior v0.x release): Why Bee-Queue? So, why another job queue in the Node.js ecosystem? Up until now, Mixmax had been using a similar queue, Bull . Bull (v2) served us well for a year, but there was a race condition that resulted in some jobs being double processed, causing major problems at scale. That race condition was fixed by re-architecting the job processing mechanism, which is currently being released as Bull v3 . Unfortunately, Bull v3 also experienced a significant performance regression over Bull v1 and v2. We were faced with the decision to either fix the performance regression in Bull v3, move entirely to a different queue such as RabbitMQ, or start over and write our own simple high-performance Redis-based queue. We started our investigation by looking into what it'd take to fix the performance regression in Bull v3, but we quickly realized that it would have been a huge effort. Fixing the performance regression while maintaining its rich set of features (many of which we weren’t using) would have taken many weeks of work. Since Redis CPU was our primary concern, we needed to minimize the number of Redis calls to process each job, ideally down to our theoretical minimum of 3. We also felt a lack of goal alignment with the project: Bull is feature-rich and generally designed for long-running jobs. We were processing millions of jobs in a small amount of time and needed to keep performance & stability as the top priority. We wanted to hold off on using RabbitMQ; we didn’t want to take on the devops burden of hosting it ourselves (by comparison, we host Redis using very-stable internal-to-our-VPC AWS Elasticache). We could have also used a third party host, but that meant all our high-volume event processing traffic would be leaving our network. We use Redis extensively at Mixmax and have team core competency in it, so we wanted to stick with a Redis-based queue. Additionally, we wanted to minimize the time we spent on this project, and moving to an entirely different framework and technology would have meant significant engineering work before we would feel comfortable relying on it in production. That left a third option: building our own lightweight queue based on Redis that did only what we needed it to, in order to make performance a priority. Before starting the project from scratch, we rediscovered Bee-Queue , having first seen it a couple years ago. We evaluated its codebase again and quickly realized that it’d be a great platform to build upon. Building on an existing queue saved us weeks of implementation time; we refreshed the codebase, identified the missing features we needed, and added those with the help of the original author, Lewis. Successful launch We released Bee-Queue v1.0 this week and have switched over to using it entirely instead of Bull. All our production traffic is now flowing over Bee-Queue and we haven’t seen any problems. Resource usage has declined dramatically, measured by lower Node.js and Redis CPU. We were also able to repurpose existing tooling from our Bull days to work with Bee-Queue. Building up Bee-Queue to serve our needs ended up taking the least amount of time, since we didn’t need to rewrite any application code or even change our existing monitoring infrastructure, which was already built to sit on top of Redis. Bee-Queue is now hosted in a new Github organization that we co-maintain with Lewis. We encourage you to check it out and contribute, or if you're looking for a more fully-featured queue, check out Bull. Our goal with Bee-Queue is to keep it small with performance and stability being the top priorities. Special thanks to Lewis Ellis for editing this post. Enjoy solving big problems like these and contributing to the open source ecosystem? Come join us .", "date": "2017-08-14"},
{"website": "Mix-Max", "title": "Handling 3rd-party JavaScript with Rollup", "author": ["Jeff Wear"], "link": "https://www.mixmax.com/engineering/rollup-externals/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar .\nThe previous post on December 2nd was about Mixmax’s new Sequence Picker SDK . As of this writing, Mixmax has open-sourced dozens of packages\nin our primary language, JavaScript. The linchpin of our open-source strategy is npm, where we publish\npackages for both Node.js and, increasingly, the browser\n( hello React components! ). Our\nbundler of choice, Rollup , makes it super easy to consume those packages\nclient-side. But if you don't take care when configuring Rollup, you can end up publishing hundreds\nof kilobytes of unnecessary JavaScript to your users. How Rollup Works Rollup is a tool that lets you write your application using ES6 modules , even though you can't\npublish those modules directly to your users, because native support is only just starting to land in browsers.\nRollup compiles your modules into a format that all browsers do understand — a single script\nfile — by, essentially, concatenating files together (while reordering and renaming declarations\nto preserve scope). ES6 modules go in… // main.js\nimport { cube } from './maths.js';\nconsole.log( cube( 5 ) );\n\n// maths.js\nexport function cube ( x ) {\n  return x * x * x;\n} And using a Rollup configuration like export default {\n  input: 'main.js',\n  output: {\n    file: 'bundle.js',\n    format: 'iife'\n  }\n}; A single script (the \"bundle\") comes out: (function () {\n'use strict';\n\nfunction cube ( x ) {\n  return x * x * x;\n}\n\nconsole.log( cube( 5 ) );\n\n}()); The only module names that Rollup understands out of the box are relative or absolute file paths,\nlike ./maths.js in the example. This works just fine for your own code — but what about\n3rd-party dependencies? External Dependencies Let's say you're getting started with React .\nDropping the official \"hello world\" example into your project import React from 'react';\nimport ReactDOM from 'react-dom';\n\nReactDOM.render(\n  <h1>Hello, world!</h1>,\n  document.getElementById('root')\n); (once you've set up Babel ) will\nresult in this warning: (!) Unresolved dependencies\nhttps://github.com/rollup/rollup/wiki/Troubleshooting#treating-module-as-external-dependency\nreact (imported by main.js)\nreact-dom (imported by main.js) This happens because there's no file called \"react\" — that's the name of React's npm package.\nThere are two ways of handling this with Rollup, as described by the troubleshooting link from the\nwarning. Unfortunately, both Rollup and React recommend the wrong one. Resolving modules from npm If you've followed React's guide ,\nyou've installed react from npm. You can teach Rollup how to find this package within your project's node_modules directory using the rollup-plugin-node-resolve plugin. Since React exports a CommonJS module ,\nyou'll also need to convert that into an ES6 module using the rollup-plugin-commonjs plugin. Once you've\ninstalled these plugins, add them to your Rollup config file: // rollup.config.js\nimport resolve from 'rollup-plugin-node-resolve';\nimport commonJS from 'rollup-plugin-commonjs'\n\nexport default {\n  input: 'main.js',\n  output: {\n    file: 'bundle.js',\n    format: 'iife'\n  },\n  plugins: [\n    resolve(),\n    commonJS({\n      include: 'node_modules/**'\n    })\n  ]\n}; There'll be no more warnings. But if you open bundle.js , you'll see something shocking: it contains\nthe entirety of React and React DOM. That's 7000 LoC! This occurs because, with the configuration above, React is no longer an external dependency: you've\ndirected Rollup to bundle it alongside your application's local JavaScript. But there are critical\ndifferences between your application's JS and React's. Why bundling 3rd-party dependencies can be a very bad idea First, there's the size. React + React-DOM is over 100kb even when minified. Second is that React and React-DOM aren't changing — until you upgrade their versions, that JS\nwill remain exactly the same. But if you deploy React as part of your bundle, the browser can't know\nthat. If you deploy several times a day, as Mixmax does, you'll blow your users' caches every time\nyou do — forcing them to unnecessarily redownload hundreds of kilobytes of JS. And you'll be\npaying for that bandwidth. So React, and other large third-party dependencies, should not be bundled alongside your application;\nthey should be kept external. But where will they come from then? Resolving modules from browser globals Rollup provided a hint back when you were getting the \"unresolved dependencies\" warning: below that,\nthere was a second warning (!) Missing global variable names\nUse options.globals to specify browser global variable names corresponding to external modules\nreact (guessing 'React')\nreact-dom (guessing 'ReactDOM') By default, Rollup did the right thing: it assumed that your third-party dependencies were available\nas browser globals! (It's a shame that Rollup doesn't recommend this at its \"unresolved dependencies\" troubleshooting link .) Let's look at the bundle Rollup generated (with the warnings): (function (React,ReactDOM) {\n'use strict';\n\nReact = React && React.hasOwnProperty('default') ? React['default'] : React;\nReactDOM = ReactDOM && ReactDOM.hasOwnProperty('default') ? ReactDOM['default'] : ReactDOM;\n\nReactDOM.render(React.createElement(\n  'h1',\n  null,\n  'Hello, world!'\n), document.getElementById('root'));\n\n}(React,ReactDOM)); You can see that Rollup mapped browser globals called \"React\" and \"ReactDOM\" to variables called\n\"React\" and \"ReactDOM\". The latter are what you imported by writing import React and import ReactDOM .\n(The variable names don't have to be the same as the browser globals, but it's common.) This is great! You can import 3rd-party dependencies as if they were part of your bundle,\nwithout their JS actually being part of the bundle. To get Rollup to do this without the warnings, we just need to add the external and globals options to our configuration: import babel from 'rollup-plugin-babel';\n\nexport default {\n  external: ['react', 'react-dom'],\n  globals: {\n    'react': 'React',\n    'react-dom': 'ReactDOM'\n  },\n  input: 'main.js',\n  output: {\n    file: 'bundle.js',\n    format: 'iife'\n  },\n  plugins: [\n    babel({\n      presets: ['react']\n    })\n  ]\n}; external tells Rollup \"it's ok that you can't resolve these modules, don't try to bundle them but\nrather leave their import statements in place\". globals then tells Rollup \"here's how to resolve\nthe import statements after all — to browser globals with these names\". (I'm not sure why\nRollup makes you specify both globals and external , to be honest. But you'll see in the \"bundling\nyour own npm packages\" section below why you might want to use external and not globals . More info. ) As for loading the browser globals, you have your pick of CDN. CDN?! What about npm? There's a reason that React recommends using npm to manage client packages: it's simple. npm install react --save-dev is easier to\nremember and more consistent with the rest of a modern JS developer's workflow than wrangling a\nscript tag. And the explicitness of import React from 'react'; is worlds better than magic browser\nglobals. But the developer experience can be almost as good with a CDN as with npm. Here's how: First, you've seen how import statements look exactly the same regardless of how Rollup resolves\nthe module. So you won't even know this is a browser global when you're developing; you only have\nto consider that once, when you configure Rollup. Second, many CDNs nowadays ultimately source from npm (before caching the JavaScript on their own\nedge servers). So you'll be guaranteed to have the same library version on the CDN as on npm, with\nthe files even addressed in the way they'd be loaded from your node_modules directory. React, for\ninstance, recommends using https://unpkg.com/ : <script crossorigin src=\"https://unpkg.com/react@16/umd/react.production.min.js\"></script>\n<script crossorigin src=\"https://unpkg.com/react-dom@16/umd/react-dom.production.min.js\"></script> At Mixmax, we like to use https://www.jsdelivr.com because it can load multiple npm packages in a\nsingle request, for fewer script tags and optimal compression, and even minify them for you! <script crossorigin src=\"//cdn.jsdelivr.net/combine/npm/react@16.0.0/umd/react.production.min.js,npm/react-dom@16.0.0/umd/react-dom.production.min.js\"></script> By the way, these CDNs are freely available! So they're paying for your users to download React, not\nyou. When we do bundle from npm All the above said, we're not total sticklers about loading third-party dependencies from a CDN. If\nthe package isn't already available on a CDN, or is only a few hundred LoC, then we'll let it slide.\nThe above advice is primarily for your largest dependencies. We also bundle our own packages from npm! Speaking of which… Bundling your own npm packages This section applies to you if your application depends on your own npm packages, and those packages\nhave a large 3rd-party dependency. For example, say that you share the following React component between services: // main.js\nimport React from 'react';\n\nconst Loading = () => (\n  <div className='soft  flexbox  flexbox--column  loading'></div>\n);\n\nexport default Loading; When you publish this module, you do not want to bundle React, for the reasons described above.\n(It would be even worse to bundle React in a library, because then its copy would duplicate that\nloaded by the application!) But the fix is slightly different for a library than an application. In\nthis library's Rollup configuration, we only want to specify external , not globals : import babel from 'rollup-plugin-babel';\n\nexport default {\n  external: ['react'],\n  input: 'main.js',\n  output: {\n    file: 'bundle.js',\n    // Also note 'es' not 'iife', since a library exports something, unlike an application.\n    format: 'es'\n  },\n  plugins: [\n    babel({\n      presets: ['react']\n    })\n  ]\n}; Now, in the resulting bundle, you'll see that import React from 'react'; remains. This\nlets the application that consumes this library decide how it would like to resolve this dependency.\nThis will generally be to a browser global, using the the external and globals options, as above;\nbut could be to a local file or even to npm for some reason. The point is that the application\ngets to choose. You oftentimes see packages list react as a peer dependency .\nSince this prevents react from being installed into that package's node_modules , this is another\nway of preventing Rollup from bundling the module. This is also nice if you want the application to\ninstall react from npm, because if an application forgets to install a peer dependency, npm will\nissue a warning. But this is only a halfway decent way to clarify that this is an external dependency, because the only way to resolve a peer dependency warning is to install react from npm — there's no way\nto notify npm that you resolve the dependency to a browser global. So peer dependencies should be\navoided in favor of external declarations. Then Rollup will take care of warning about\n\"unresolved dependencies\", even if external declarations can't express a particular version range\nwith which your library is compatible like peer dependencies can. User experience above all After years of copy-pasted, locally-hosted scripts, maybe Bower if you were lucky, npm has finally\nmade it possible to easily distribute client-side packages. Rollup builds atop Browserify and Webpack's\nlineage to make it possible to easily consume those packages, while looking to the future of JS\nmodules. But at Mixmax, we take care to not take these achievements for granted. We carefully\nanalyze each additional package to make sure it doesn't compromise our user experience, and if\nnecessary factor it out. Even if loading those dependencies from a CDN is a little more work, it's\nworth it. If you're interested in balancing developer and user experience at the cutting edge of\nJavaScript development, drop us a line at careers@mixmax.com .", "date": "2017-12-03"},
{"website": "Mix-Max", "title": "Running client tests headlessly with Jasmine and PhantomJS using Erik", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/erik/", "abstract": "We recently released Erik ( name origin ), an open-source Node package for running jasmine tests headlessly with PhantomJS and gulp . Background Prior to developing Erik, we ran our client tests semi-manually by running an npm script that opened our jasmine SpecRunner.html file (configured similarly to what's shown in the jasmine installation instructions ) in Chrome. jasmine showed an easy-to-understand output of the test results and re-running the client spec suite was as straightforward as refreshing the page. There were downsides to this process, though. We didn't like having to remember to run npm run client-tests (and often didn't remember) and we weren't able to run the tests in CI (and were therefore unable to block merges and deploys on failing client tests). Earlier this year, we wrote some code that caused some of our client tests to fail. We missed running the client tests before merging the changes into master and later deployed the changes to production, where we realized that they had broken some functionality. After fixing the issue, we decided that it was time to pull the trigger on automating our client tests so that we could avoid similar regressions in the future. Requirements When beginning to identify a solution for running our client tests headlessly, we laid out our requirements: Remote dependencies . In addition to our locally processed and bundled JS, our SpecRunner.html files included several remote dependencies loaded from CDNs. We wanted to continue using those CDN-served libraries rather than bringing the libraries into our own projects. Stick with jasmine . All of our existing tests were written with jasmine . We didn't want to have to rewrite all of our tests in order to start running our client tests headlessly. CI integraton . The main motivation for running our client tests headlessly was to catch failures in CI. Additionally, we wanted to avoid writing custom scripts for running tests in PhantomJS. We preferred to use an existing driver/wrapper to interface with PhantomJS so that we didn't need to worry about ensuring the functionality of a new driver/wrapper or take time to learn a new library (PhantomJS). Deciding on a solution PhantomJS proposes a few packages for tesing headlessly but unfortunately, none of them were sufficient for us. Chutzpah is specific to Windows, guard-jasmine is for Ruby apps, we didn't want to introduce Grunt just for grunt-contrib-jasmine , and phantom-jasmine uses an outdated version of Jasmine and is unmaintained. Eventually, we discovered Karma , a library that allows you to run tests written in a number of testing frameworks in different environments (including Chrome and PhantomJS). Karma turned out to be perfect for us, but we didn't want to duplicate our configuration across all of our microservices. Additionally, we figured that we weren't the only ones with these needs, so we decided to develop an open-source package that would solve our issues as well as those of other developers: Erik. Erik Erik uses Karma and Karma's karma-phantomjs-launcher to register a gulp task for running tests headleslly. It assumes some default configuration to make getting started easy, while exposing the important stuff that package users might want to configure themselves. Erik enables easy specification of remote dependencies, overwrites some Karma defaults for improved performance and test result communication, and works around a couple of bugs ( 1 , 2 ) we discovered while developing Erik. It also offers a \"watch\" mode, which re-runs your tests when your local dependencies change. Shoot us a message if you're interested in joining an engineering culture that defaults to open-sourcing its tools.", "date": "2017-05-25"},
{"website": "Mix-Max", "title": "Terraforming all the things", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/terraforming-all-the-things/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on\nDecember 3rd Handling 3rd-party JavaScript with Rollup . tl;dr - We use Terraform for almost everything and we're never looking back. The problem Have you ever tried to navigate around the AWS UI to hunt down a configuration\nissue? Perhaps someone accidentally clicked the wrong button, and suddenly one of\nyour high throughput Elasticache redis deployments is downsizing back down to a t2.medium just cause... This isn't the fault of you or your team, you can't blame\nthem for being overwhelmed by the sheer amount of UI there is in the AWS web\nconsole. I'm not even going to talk about the hours upon hours it took me to reorient myself\nwith the AWS services dropdown when it was reorganized earlier this year. However,\nmanaging infrastructure is\nhard, and if you're managing it through UI can you really expect it to get any\neasier? Absolutely not. The sheer amount of knobs and dials you can turn when\nconfiguring any system is not only overwhelming, but it makes it easy to\nmiss a confirmation modal or to not realize that there wasn't one. This means\nthat configuration mistakes aren't only very simple to make, but you're only\none click away from a mistake that you might not ever notice (such as making\nan S3 bucket publicly available). Is it all hopeless? Fear not! Configuration can also be done with, well you know, configuration\nfiles. But why stop there? Why not drink a little more of the Kool-Aid and begin\nto version your infrastructure? While \"Infrastructure as code\" can seem\nterrifying and daunting to implement, we're here to tell you that it's very easy\nto incrementally roll out across your infrastructure. Side note: What is Infrastructure as code? Infrastructure as code, technically, means configuring infrastructure with an\nautomated system instead of configuring it manually. So instead of manually\ngoing to the AWS UI and clicking some buttons, or instead of hopping on a server\nand fiddling with some config files, you make changes to machine readable files\nthat your automated system can then use to apply those changes for you. The\nutility of such a system becomes very apparent when a few additional lines of\nconfiguration code can be used to modify your entire server fleet. Moving from a manually managed system to a fully automated one can seem daunting\nbecause it can be incredibly difficult to identify how to even begin the\nmigration process. Not only that, but it can be difficult to find a low stakes\nenvironment in which to begin to test the waters without committing your entire\ninfrastructure to the new process. Infrastructure as code: start with the little pieces At Mixmax, we use many of AWS's services - from Elastic Beanstalk and Elasticache\nall the way through CloudWatch and DynamoDB. We knew it wouldn't be feasible to\nmove our entire world to a versioned configuration system in one fell swoop, so\nwe wanted to use a tool that would allow us to incrementally bring our\ninfrastructure under version control. For us, this meant that Hashicorp's Terraform was a no brainer as we could easily begin to use Terraform to manage small\ndeployments of non-application level systems before committing to managing\nour application services with Terraform. In order to incrementally migrate\nto using Terraform, we began to move components of our infrastructure that\nwere fairly static to be under Terraform's control. There are many other tools\nin this space, but most are primarily application configuration systems that\nwere retroactively bootstrapped in order to also be used as provisioning tools\nwhereas Terraform has been a flexible provisioning tool from the start. First we moved our CloudWatch alarms and our SNS topics and subscriptions to be\ncontrolled via Terraform. Using Terraform modules for this was so successful that engineers who previously never wanted to touch\nCloudWatch alarms began to create them with glee! We'd turned a painful part of\nour development process into something our team found to now be a joy to work\nwith. After that success, we decided to try something with higher stakes, and so\nwe moved our Elasticache redis deployments to be provisioned and managed via\nTerraform. Again, using Terraform modules made this a breeze. Why do terraform modules make this so simple? Well let's look at an example.\nWe utilize CloudWatch alarms across our entire infrastructure in many different\napplications, but one specific one is tracking the number of delayed, inactive\nand failed jobs in our job queueing system, bee-queue .\nBefore, engineers would have to either manually make alarms in the AWS UI or run\na script that wasn't fully intuitive to use. More than once, we'd ended up with\nonly two of the alarms existing, the third having been forgotten. With Terraform\nmodules though, creating three alarms is super simple: module \"process-cool-event-job-queue-alarms-bee-queue\" {\n  source = \"./modules/job_queue_alarms\"\n\n  alarm_name   = \"process-cool-event\"\n\n  # Note that the `alarm` and `ok` actions are SNS Topic ARNs that we use to hook\n  # these alarms up to PagerDuty.\n  ok_action    = \"${var.high-priority-ok-action}\"\n  alarm_action = \"${var.high-priority-alarm-action}\"\n} It's really that simple though - one segment of code for three alarms! How\ndoes this work though? Well, let's look the structure of the job_queue_alarms folder. job_queue_alarms/\n      main.tf\n      delayed/\n          main.tf\n      failed/\n          main.tf\n      inactive/\n          main.tf The root main.tf in the job_queue_alarms directory then looks like: variable \"alarm_name\" {}\nvariable \"ok_action\" {}\nvariable \"alarm_action\" {}\n\nvariable \"delayed_threshold\" {\n  default = \"100.0\"\n}\nvariable \"failed_threshold\" {\n  default = \"100.0\"\n}\nvariable \"inactive_threshold\" {\n  default = \"100.0\"\n}\n\n\nmodule \"too-many-failed-jobs-bee-queue\" {\n  source = \"./failed\"\n\n  alarm_name = \"${var.alarm_name}\"\n  ok_action    = \"${var.ok_action}\"\n  alarm_action = \"${var.alarm_action}\"\n  threshold = \"${var.failed_threshold}\"\n}\n\nmodule \"too-many-delayed-jobs-bee-queue\" {\n  source = \"./delayed\"\n\n  alarm_name = \"${var.alarm_name}\"\n  ok_action    = \"${var.ok_action}\"\n  alarm_action = \"${var.alarm_action}\"\n  threshold = \"${var.delayed_threshold}\"\n}\n\nmodule \"too-many-inactive-jobs-bee-queue\" {\n  source = \"./inactive\"\n\n  alarm_name = \"${var.alarm_name}\"\n  ok_action    = \"${var.ok_action}\"\n  alarm_action = \"${var.alarm_action}\"\n  threshold = \"${var.inactive_threshold}\"\n} While each main.tf inside one of the children directories, looks like: variable \"alarm_name\" {}\nvariable \"namespace\" {\n  default = \"bee-queue\"\n}\n\nvariable \"threshold\" {\n  default = \"100.0\"\n}\n\nvariable \"ok_action\" {}\nvariable \"alarm_action\" {}\nvariable \"treatMissingData\" {\n  default = \"missing\"\n}\n\n# Note that the `delayed` part here is different between this `main.tf`\n# and the `failed` and `inactive` `main.tf` files. Sure, we could refactor\n# this into a single module (we're actually doing that ;) ), but this was\n# one of our first forays into Terraform and we wanted to show our actual\n# first steps.\nresource \"aws_cloudwatch_metric_alarm\" \"too-many-delayed-jobs-bee-queue\" {\n  alarm_name          = \"${format(\"too-many-delayed-%s-jobs-bee-queue\", var.alarm_name)}\"\n  comparison_operator = \"GreaterThanOrEqualToThreshold\"\n  evaluation_periods  = \"5\"\n  metric_name         = \"delayed\"\n  namespace           = \"${var.namespace}\"\n  period              = \"60\"\n  statistic           = \"Maximum\"\n  threshold           = \"${var.threshold}\"\n  alarm_description   = \"${format(\"Too many delayed %s jobs\", var.alarm_name)}\"\n  ok_actions          = [\"${var.ok_action}\"]\n  alarm_actions       = [\"${var.alarm_action}\"]\n\n  treat_missing_data = \"${var.treatMissingData}\"\n\n  dimensions {\n    # Hardcoded as we don't create the alarms on our staging environments.\n    environment = \"production\"\n  }\n} Phew! There's a lot going on here! The general gist of this though is that\nthrough using variables, we can create reusable components that we can then\ncombine to create multiple resources at a time! In our previous example of using\nthe job_queue_alarm module, we used the default threshold values, what if we\nwanted to use custom threshold values? In that case, we'd do something similar\nto this: module \"process-cool-event-job-queue-alarms-bee-queue\" {\n  source = \"./modules/job_queue_alarms\"\n\n  alarm_name   = \"process-cool-event\"\n  ok_action    = \"${var.high-priority-ok-action}\"\n  alarm_action = \"${var.high-priority-alarm-action}\"\n\n  delayed_threshold  = \"500.0\"\n  failed_threshold   = \"50.0\"\n  inactive_threshold = \"120.0\"\n} Et voila! By using variables with default values, we can provide overriding\nvalues to the module at any time, allowing for a very high degree of control\nover otherwise very similar resources. But wait there's more! As we began to use Terraform for more and more across our\nAWS infrastructure, we realized Terraform can be used to provision and configure\nanything as long as there's a Terraform provider for it. Giddy with excitement,\nwe began to quickly Terraform our PagerDuty schedules and service alarms!\nWhile on the surface this seems excessive, it has huge benefits. By Terraforming\nour PagerDuty alarms, we're able to create brand new alarms for new services in CloudWatch\nat the same time that we make those new alarms in PagerDuty - meaning that we can\nprogrammatically connect them, all at the same time! What should I take away from this? Infrastructure as code is incredible, but you shouldn't feel like you have to\nmigrate the world all at once. We've found that by incrementally moving our\ninfrastructure to a versioned provisioning system, we've had not only widespread\nadoption internally but also an increase in interest in getting involved with\ninfrastructure work. At Mixmax, we're not using Terraform for everything yet,\nbut we're enjoying the process of seeing how it's making everyone's lives easier\nwhile we continue to roll its usage out across our systems. Enjoy building smarter infrastructure in an intelligent way instead of wrangling the AWS UI? Drop us a line .", "date": "2017-12-04"},
{"website": "Mix-Max", "title": "Recruiting Engineers Using Online Challenges", "author": ["Spencer Brown"], "link": "https://www.mixmax.com/engineering/recruiting-engineers-with-online-challenges/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on December 4th was Terraforming All The Things . Scaling recruiting We want to fill our new office with stellar engineers. Let us know if you’re interested ;). We've partaken in hackathons and careers fairs throughout the country where we've met hundreds of students who are interested in working with us. Simultaneously, we've been fortunate enough to receive a record number of applications from folks around the world. We're thrilled that so many engineers are interested in working with us! Like any startup, though, time is our most valuable resource and we must spend it carefully. Our recruiting process must be highly efficient so that we can find the best candidates, ensure our candidates have the best-possible experience when interviewing with us, and maintain high engineering velocity. This year, as our candidate pool swelled, our process became excessively time-consuming and we decided that we needed to make a change. Based on what we had heard from friends and engineers at other companies, we thought that using an online coding challenge would save us a lot of time. We started setting goals and got to work! Designing the challenge Beyond being an effective screen, we established that we wanted the challenge to: Resemble real-world work At Mixmax, we believe that it's important for our technical interviews to reflect the sort of work that engineering candidates would do with us day-to-day. To ensure that the challenge would resemble the work we do, we decided to base the challenge on a problem we had actually solved in the product. In addition to making the challenge a more effective screen, we also hoped that this would make the challenge fun for candidates! Support test cases We wanted to let candidates test their code so that they could verify their work before submitting it. Be easy to use by candidates Some coding challenge platforms have hard-to-understand UIs and idiosyncratic testing mechanisms. We want candidates to feel as comfortable as possible when interviewing with us so that they can focus on the problems at hand; ease-of-use is an important part of achieving that comfort. Not be excessively time-consuming While we want to ensure that we're holding candidates to a high bar, we also want to be mindful of our candidates' time. We wanted the coding challenge to take candidates no more than 45 minutes. Be available in a variety of programming languages At Mixmax, we hire based on trajectory and demonstrated ability to learn rather than experience with the components of our stack. We believe that anyone we hire should be able to pick up new languages, frameworks, and technologies relatively quickly (and we'll need them to as our stack expands!). With that in mind, we wanted to make sure that we wouldn't filter anyone out simply because their language of choice was not available. Picking Qualified With those requirements in mind, we set out to pick a platform for our challenge! We considered several options before settling on Qualified . We picked Qualified for a number of reasons: Intuitive UI We felt that Qualified's UI was intuitive and similar to a number of online editors with which candidates might already be familiar, like JSFiddle. Test cases defined using real frameworks Some of the platforms we considered required that test cases be defined with a single string input and a single string output. This was not compatible with the challenge we wanted to use and we were not interested in writing a string parser. Other platforms were more flexible, but still used custom testing frameworks. Qualified, on the other hand, uses popular testing frameworks for most popular languages, which made writing tests easy for us and resulted in the most real-world experience for candidates. Awesome team As we trialed Qualified, we found their team to be delightfully responsive to our feedback. Working with them has been a pleasure. Thanks, Qualified! :) In practice After selecting Qualified, we built the challenge. We framed its description with a story of how the candidate arrived at this problem in their Mixmax work to share more about what it's like to be an engineer here. We decided to let candidates take responsibility for testing their own code. We share a basic test case to help them get started, but leave them to test the full set of requirements. We ask every candidate who takes the challenge to share any feedback they have. Thanks, candidates who have shared feedback! You've helped us improve our recruiting process :). Results Hundreds of candidates have taken our online coding challenge. It's saved us a ton of time and, according the feedback we've heard, many candidates have enjoyed it! I had a lot of fun on the Mixmax coding challenge :D. It was so refreshing! I broke out my Jupyter notebook and just enjoyed myself for 45 mins :D. Great knowing this is a reflection of daily work at Mixmax, it was just the type of work I was expecting and I loved it! I have taken tons of coding challenges in the past few weeks, and this one really stuck out to me because it was a really interesting question to think about and one that actually seems relevant to the role I’m applying for. Let us know if you'd like to give the challenge a shot! :)", "date": "2017-12-05"},
{"website": "Mix-Max", "title": "Writing an effective bug report", "author": ["Chuy Martinez"], "link": "https://www.mixmax.com/engineering/writing-an-effective-bug-report/", "abstract": "When we write bug reports, most of the time we have a clear vision of what just happened or what steps were taken in order to reach to a situation where something not expected happened. It is obvious to us at the moment, so we communicate it with very few notes because it’s very obvious, paste a screenshot and that should make it obvious to the developer what went wrong, right? The problem is, most of the time, that bug report is the first exposure to a developer. When a developer reads a bug report, theories and possible causes of the problem start to arise, the developer starts to think about how the data flowed through the system to reach that situation, after all, software is just a black box that takes some input and transforms it into some output So we may know the output with a bug report that has a screenshot or has a shallow description of what happened, however we do not have the input that kicked off some process that caused a bad output ! An output can be generated by many, many processes, there are many ways to generate a single output, a simplified analogy is for example someone reporting a problem with a calculator saying: “I got the result 1 when I should have gotten result 3 !” What a developer does first is, “What operation using what inputs should give the number 3 but is given number 1?” There are infinite ways to get the number 3 with different combinations of numbers and operations, and we need to find out which of these is causing the problem, a LOT of time is spent figuring out what operation caused the problem. If in that bug report of above, the reporter said instead: “I punched the buttons: 4 - 1 and got 1 but I should have gotten 3!” we would have received the input information to understand what process kicked off the process that caused the bad output, cutting completely the initial research time to find out what caused the problem and immediately jump into a solution, the initial research time can account for over 70% of the time spent fixing a bug. What is a good bug report? First, take some time to go through the bug report process for the Chromium project (Click the top left \"New Issue\" button) just go through the steps and see how much information they ask. First and foremost, we need to know what steps were taken to lead to that situation? Or also named: Steps to reproduce. This is covers the sequence of actions taken to lead to the bad result. Some of the questions that are answered there is:, what page were you visiting what did you click what was your user what module in the system were you manipulating at the time. Document every step as thoroughly as possible, even if it is very obvious at the time, After 2 days if a developer asks for feedback you will have forgotten what exactly happened adding more time to a resolution of a bug, as the chinese proverb says: The palest of ink is better than the best memory . Then we need to know What did I expect to see? Or otherwise named: Expected results Here, the bug reporter must describe in detail what was expected to happen, should the message been sent? Should my changes have been saved? Should I have access to see some resource? Software moves at a very fast pace, and it is possible that misconceptions happen, by stating what is the expected behavior, we can sometimes clarify how some feature should work. Also, by stating what should have happened, it makes it easier for the developer to find out what logic flow path was taken and where to look first. Finally, we need to know What you actually saw? Or otherwise named: Current results Here you will describe exactly what you are seeing, how it looks, what moved around which shouldn’t have or what stayed which should’ve moved. Most of the time, we skip everything to this step by posting a screenshot, however this is bad because, as stated above, we can’t do much by just knowing what you saw. Alternatively, once we know the scenario and what happened, reaching the point of What you actually saw? is trivial, because we now know how you reached to that state. Here’s where you finally add screenshots or a video. Sometimes a screenshot is not the most helpful thing in the world. A video is, most of the time, useful because we can see the steps being taken, that’s one of the biggest reasons why a video is a good reporting tool, but if we already have those written, then a video is just a nice to have. Another very valuable tool to have is a screenshot of your chrome developer tools. You can find instructions on how to open it here This is how you would use it: Open the chrome dev tools and switch to the Console tab Try to replicate the bug you are reporting When you reach the bad results, take a screenshot of the output in the Console tab and add it to your bug report. The above doesn’t always apply, as a rule of thumb, if you see output with a red background, it means that there is something wrong with some of the UI logic, if you don’t it means it’s possibly a backend problem in which case the problem won’t be displayed in the dev tools. And finally notify the corresponding people about the problem. As per the Chromium report form, you need to specify what area is the problem affecting. Maybe a formal concept of \"area\" doesn't exist for some teams, but most of the time we have an idea of what part of the system is the domain of most developers. Why is this important? It is important because teams have a lot of communication going on in all different channels: issue tracker, IM, email notifications, user reports and we need to make that flow of information as effective as possible. A developer might be receiving notifications of issues/tasks outside of its current domain and it can drown notifications where that person attention is needed, this make triaging issues less efficient by having notifications overflows. Also, by notifying/assigning to the correct person, that person can triage the problem and assign it to the developer that lasted worked on the component giving problems if need be. Conclusion A good bug report can cut a big bunch of time spent in fixing a bug. Although it is an extra effort from the reporter to fill a more complete bug report, the value provided by it is very well worth it. Not only it saves time for developers to find and fix the issue, but it also serves as a documentation tool for everyone to better understand the product, by getting everyone more involved with the processes, knowledge about the whole system grows, a clear report cuts time going back and forth between reporter and developer that may be better spent somewhere.", "date": "2017-04-20"},
{"website": "Mix-Max", "title": "Precisely observing structural page changes", "author": ["Eli Skeggs"], "link": "https://www.mixmax.com/engineering/precisely-observing-structural-page-changes/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on\nDecember 6th was about Database-backed job processing . Mixmax is built on Gmail. Our product, and its convenience and power, depends on tight\nuser-interface integration with Gmail. In order to add features to Gmail for our users, we need to\ntrack the structure of Gmail’s DOM and be able to manipulate it. For years, we achieved this by\ncrafting query selectors to identify important elements within Gmail’s DOM, and continuously\nre-applying these selectors as the page changed. As we did this, we attached our own\ncontent to the page. Sadly, many users previously reported performance problems with Gmail when Mixmax was installed. Our\nperformance analysis showed significant processing time being spent in the code that observed the\npage for changes. This code was thus a candidate for optimization. Problem Our performance analysis revealed that our existing implementation configured MutationObserver instances to watch for changes anywhere within the document\ntree, causing major page slowdown. We were using this approach to observe page changes within Gmail\nwithout undue risk of breakage from changes in Gmail’s page structure. To combat the expected\nperformance issues endemic to responding to every little change in the DOM, we wrapped our handler\nin _.throttle , thereby only responding to a small fraction of changes without losing correctness.\nUnderscore’s throttle function returns a closure function which checks the elapsed time since the\nlast call, and delegates to the provided function only when sufficient time has passed. In our case,\nwe discovered that simply running this check was enough to induce low performance, as our mutation\nobservers fired their handlers with such intensity that it overwhelmed poor V8. We wanted to maintain functionality without the massive performance penalty. Optimally, we would\n(instead of watching all changes) identify nodes we expect to change and observe them directly. We realized early on that a good solution must balance performance and reliability. If we made our\ndeclarations too specific, we would risk significant maintenance work to respond to Gmail changes,\nbut if we make them too indirect, we risk missing crucial page updates that allow us to introduce\nour own controls into Gmail. Solution Instead of registering global MutationObserver instances, or leaning heavily on fragile,\nspecialized code to watch the right elements for changes, we now make heavy use of page-parser-tree , a module written by Chris Cowan at Streak . page-parser-tree observes subsets\nof a dynamic webpage, using declarations provided by the developer that identify important sections\nof the page. The declarations consist of realtime watchers and polling finders, which pick out\nspecific elements and add them to tracked sets of elements dubbed “tags.” The watchers declare parts of the page structure that we know will change — things like the\nthread and message list containers, and the compose and reply buttons. Watchers are generally\nhierarchical, and reference other tags to explore smaller and smaller DOM subtrees. The finders define functions that run periodically (usually every five seconds) to detect important\nelements the watchers might have missed. They do so by running query selectors against the entire\ndocument. The finders will discover the same elements as our old approach, but without the\nperformance penalties associated with the global mutation observers. The finders thus serve two\npurposes: to ensure that our integration doesn’t break entirely if we miss an edge-case, and to\nprovide fallback behavior in case Gmail’s page structure changes in a way that our watchers can’t\naccommodate. Should the watchers and finders identify different sets of elements, we log the reported\ninconsistency and some context. These reports give us a channel to proactively identify and fix\nregressions related to Gmail updates. Under the hood, page-parser-tree uses live-set , essentially a set with a subscribe method that\ntracks changes to some group of objects. These live-sets can be converted to Observable objects which have a slightly different subscribe methods with a\ndifferent use-case. Observable objects are useful — see the example in the next\nsection. In addition to page-parser-tree, we have some specialized code to handle poorly supported\nedge-cases, and to achieve reliability unattainable by using page-parser-tree’s watchers. One\nexample is a preview pane (a\nGmail lab) agnostic thread navigation watcher, which emits events when the user changes the current\nthread. The next section includes a few other examples of this. The new approach avoids the performance issue with global mutation observers by not using them.\nProfiling shows that page-parser-tree’s tricks make watching the page no longer a significant\nperformance problem. Implementation We used to monitor changes to Gmail’s structure using global subtree MutationObserver instances.\nWe had an ElementUtils module that provided onElementAdded and ensureElementExists functions\nto detect elements given a query selector. The onElementAdded utility called the given handler\nfunction when it noticed any element matching the query selector, whereas the ensureElementExists function returned a promise that resolved to the first matching element, including existing\nelements. ensureElementExists built on onElementAdded , unsubscribing after the first element. onElementAdded used _.throttle call to reduce the frequency with which it called the handler.\nThe following code, for example, would detect the compose button: const selector = GmailSelectors.COMPOSE_BUTTON;\n// Delay watching for the compose button for long enough so the loading performance\n// is quick, but also short enough so the button doesn't noticeably flash the default\n// gmail color.\nconst wait = 300;\nElementUtils.ensureElementExists(selector, wait).then((origComposeButton) => {\n  // replace the compose button\n}); Internally, ElementUtils used a slightly more complicated variation of the following code: // Find existing elements.\nonMutation();\n// Throttle element queries and handler calls.\nconst wrappedOnMutation = _.throttle(onMutation, throttleDuration, {leading: false});\nconst observer = new MutationObserver(wrappedOnMutation);\nobserver.observe(document, {\n  childList: true,\n  subtree: true\n});\nfunction onMutation() {\n  const elems = $(selector);\n  if (elems.length) {\n    handler(elems, observer);\n  }\n} The new code that detects the compose button proxies through a new common interface, called UI: UI.get('originalComposeButton').then((origComposeButton) => {\n  // replace the compose button\n}); Under the hood, UI uses a getFirstFromTag utility function to get the first node from the originalComposeButton tag. The getFirstFromTag function asks the page-parser-tree instance for\nan Observable corresponding to that tag, and unsubscribes as soon as the\nobservable produces a compose button. This code is roughly analogous to the following, but handles\nnumerous edge-cases: import toValueObservable from 'live-set/toValueObservable';\nconst Watcher = new PageParserTree(definitions);\nfunction getFirstFromTag(tag) {\n  const deferred = $.Deferred();\n  // Get an observable for the given tag.\n  const observable = toValueObservable(Watcher.tree.getAllByTag(tag));\n  // Subscribe to elements in the tag - include elements already in the tag. The\n  // value parameter is unpacked from an object that also contains the removal\n  // Promise, which we don't need for this use-case.\n  const subscription = observable.subscribe(({value}) => {\n    subscription.unsubscribe();\n    deferred.resolve($(value.getValue()));\n  });\n  return deferred.promise();\n} This new detection requires a deeper understanding of how the page changes. When the user simply\nloads their inbox, the compose button will be reachable once the loading view disappears. However,\nif they navigate to contacts, Gmail removes the compose button from the DOM. The compose button\nwatcher must therefore rediscover the button when the page changes. To avoid an overly specific set\nof child selectors, we “jump” between well-known points in the DOM. The tag is defined as a watcher\nand associated finder: watchers: [\n  // The 'pageContent' source references another tag that finds a defined DOM node\n  // that wraps the entire page content (minus things like top-level scripts and our\n  // compose windows).\n  {sources: ['pageContent'], tag: 'originalComposeButton', selectors: [\n    // Use the $map operator to hop from the \"pageContent\" container to the left\n    // sidebar container, which includes the dropdown that navigates between Mail\n    // and Contacts, and is seven levels down from the \"pageContent\" container. Here,\n    // the mapping function will be called with each element from the pageContent tag\n    // (which should be a single element).\n    {$map: (e) => $(e).find('.Ls77Lb')[0]},\n    // We define this immediate-child selector for the .aj9 Mail sidebar container,\n    // which Gmail replaces with the .aXo Contacts sidebar container.\n    // page-parser-tree watches the immediate children of the sidebar, declared by\n    // the previous selector, for when the .aj9 container is added and removed.\n    // By declaring this element as a direct child of its parent, we discover the new\n    // button when the user returns to the primary Mail view.\n    '.aj9',\n    // Under the .aj9 container, we again use the $map operator to jump to the\n    // compose button itself.\n    {$map: (e) => $(e).find('div[gh=\"cm\"]:not(.mixmax-compose-button)')[0]}\n  ]}\n],\nfinders: {\n  originalComposeButton: {\n    // page-parser-tree calls this (by default) every five seconds to ensure\n    // we haven't missed the compose button due to a change that impacts the above\n    // selectors.\n    fn: (doc) => $(doc).find(GmailSelectors.COMPOSE_BUTTON_ORIGINAL).toArray()\n  }\n} Using this formulation for the watcher, we avoid specifying selectors for each element between the pageContent container and the sidebar container, and between the Mail sidebar container and the\ncompose button itself. As such, Gmail is free to change the exact structure it uses within that DOM\nsubtree. The new approach is a dramatic shift in how we observe page changes. Instead of watching the entire\npage for any change, and then rediscovering all elements that match a given selector, we define the\nstructural relationships between key page elements, and have page-parser-tree watch only those\nelements for relevant changes. The new approach is faster, reasonably reliable, and more responsive\nto page changes than our old method. Limitations Do note that page-parser-tree isn’t a silver bullet. It doesn’t support tagging the same dom node\nwith the same tag from multiple watchers, nor does it support unrestricted subtree fanout/deep\nselectors. It has first-class support for identifying immediate children, and provides operators to\nwatch for attribute changes, arbitrary filters, element-to-element mapping, and more . Another important caveat is that watchers aren’t smart enough to watch for attribute changes based\nsolely on the selector. If you ask it to identify elements that are .nH.id , and Gmail changes that\nelement to not have the id class, the element will remain in the tag. To correctly track these\nchanges, we need the $watch operator. We do this when we watch for messages being opened and\nclosed by the user: watchers: [\n  // Filter message containers by whether they are open, updating the tag when the\n  // user opens/closes one of the messages.\n  {sources: ['message'], tag: 'openMessage', selectors: [\n    // page-parser-tree calls cond to determine whether a given element should be in\n    // the openMessage tag, and re-evaluates a given element when any of the\n    // attributes on that element change. When the attributeFilter array is provided,\n    // page-parser-tree only re-evaluates when any of those attributes change.\n    {$watch: {attributeFilter: ['class'], cond: (e) => $(e).hasClass('h7')}}\n  ]}\n] In another case, we identify zero or more form elements within email message bodies, and disable\nGmail’s form submission warning. The old code looked like this: ElementUtils.onElementAdded('form[action*=\"mixmax.com\"]', (forms) => {\n  forms.removeAttr('onsubmit');\n  forms.on('submit', (e) => e.stopPropagation());\n}); The above approach is super robust to changes within Gmail, but introduces performance issues as in\nevery other case. We thus now use page-parser-tree. It identifies the open message container, but we\ncan’t use it to find the actual form elements. The form elements might be anywhere within the DOM\nsubtree, and page-parser-tree does not support deep selectors as they violate the premise of not\nwatching for global changes. Moreover, because there might be more than one element, we can’t use\nthe $map trick from above to identify all the forms, because $map only maps one element to\nanother element — no “fanout.” As such, we do not expose the form elements from our watchers or\nfinders at all, but instead prefer to discover them on top of the open message container tag, and\nprovide an interface for discovering them in the UI abstraction. UI uses the knowledge that these forms will only be present within open message containers, and\navailable DOM as soon as the message has been opened, to find them directly with jQuery: // The actual implementation abstracts this line as subscribe('openMessage', ...)\ntoValueObservable(Watcher.tree.getAllByTag('openMessage')).subscribe(({value}) => {\n  const message = $(value.getValue());\n  // Find the forms that have an action that submits data to a mixmax domain.\n  message.find('form[action*=\"mixmax.com\"]').each(function() {\n    handler($(this));\n  });\n}); Which is then used to disable the warning: UI.added('mixmaxForm', (form) => {\n  form.removeAttr('onsubmit');\n  form.on('submit', (e) => e.stopPropagation());\n}); A final limitation is that tags cannot receive the same element multiple times. We ran into this\nlimitation when adding functionality to support preview pane. It means we must be\ncareful when sharing watchers between multiple tags. For example, it means we cannot use the\nfollowing to detect when an element is empty: // Do not do this!\nwatchers: [\n  {sources: ['replyContainer'], tag: 'nonEmptyReplyContainer', selectors: [\n    // Grab all immediate children of the replyContainer.\n    '*',\n    // Return to the replyContainer - in theory, page-parser-tree would track which\n    // child elements it received the replyContainer element from, and would\n    // appropriately remove the replyContainer from nonEmptyReplyContainer when all\n    // the child elements are gone. page-parser-tree does not currently support this\n    // use-case.\n    {$map: (e) => e.parentNode}\n  ]}\n] A potential solution to the above would be to replace the '*' with '*:first-child' . We also\nencountered this issue when attempting to reuse watchers between different preview pane states and\nmodes, and had to wholly restructure our selectors to correctly match the message list view in all\nview configurations. Observables We use Observable objects to simplify some of our interactions with page-parser-tree. A\nsubscription to an Observable immediately receives any elements that already reside in the Observable (and in the tag), and receives subsequent elements as they’re discovered by\npage-parser-tree. The Observable is provided by zen-observable and\ninstantiable via live-set ’s toValueObservable (demonstrated in getFirstFromTag in the\nImplementation section). Observable subscriptions also expose handy removal Promise objects, which are resolved when the\nassociated element has been removed from its tag. We use removal Promises in a couple places to\ncorrectly manage the lifecycle of our own code. For example, when Gmail is in preview pane mode (a Gmail lab), it re-renders the recipient cell in\nthe thread row into which we inject our reminder button. To reattach the button when the row\nchanges, our UI abstraction watches thread rows, and subscribes to an observable that observes the thread tag. The observable provides both the added element and a removal Promise . UI registers\nits own MutationObserver when it detects a new row, and disconnects it when the promise resolves: const observable = toValueObservable(Watcher.tree.getAllByTag('thread'));\nobservable.subscribe(({value, removal}) => {\n  const row = $(value.getValue());\n  // Find the parent of the node of interest: the node of interest will exist now, but may be replaced.\n  const recipientContainer = row.find(GmailSelectors.RECIPIENT_WRAPPER).parent()[0];\n  const observer = new MutationObserver(_.throttle(() => handler(row), 10));\n  // Watch the recipient container node for child list changes, so we discover the new recipient\n  // wrapper when it's been replaced.\n  observer.observe(recipientContainer, {\n    childList: true\n  });\n  // When the row is removed (or we switch views), disconnect the observer.\n  removal.then(() => observer.disconnect());\n  // Initially call the handler with the row.\n  handler(row);\n}); Our old approach had deadly performance issues. Our new approach has many nuances and complexities.\nThis reflects the nature of the problem — it’s not easy, it has real user impact, and a solution\nmust balance many factors. Have a knack for engineering solutions to software problems that prioritize user experience? We’re hiring !", "date": "2017-12-07"},
{"website": "Mix-Max", "title": "Introducing search-string: an advanced search string parser", "author": ["Marcus Ericsson"], "link": "https://www.mixmax.com/engineering/search-string-advanced-search-parser/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on\nDecember 8th was about Beating Spam Detection Bots . One of Mixmax’s core goals is to provide transparency into you and your teams' communications. The Live Feed dashboard shows latest activity on sent messages. The Insights dashboard allows you to perform analytics on that activity. And the Outbox is a hub for all of your team's planned communication. On each of these pages, you can drill into activity that you are interested in using a powerful advanced search box at the top of the page. We provide a familiar syntax modeled after the native GMail search box syntax for running these queries. You can also see it in action in our public Snippet Search API . One of the first major projects I had at Mixmax was to rebuild and release the new Team Outbox . This was a great first project as it cut across our technology stack and opened up data previously inaccessible to our end users: their teammates' scheduled messages. As part of this rebuild, we wanted to invest more in our advanced search box’s query parser and add some new features. Thanks to Mixmax's strong open source culture , I was encouraged to build this as an open source module from the start. Announcing search-string As a result, I'm excited to announce and share a new npm module: search-string . This module (now used throughout our application) is easy to get started with and packs a few advanced features. Note: the library has no dependencies and is compiled down to ES5 compatible JS so you can use it in the browser or on the server (we do both!). Search-string parses typical Gmail-style search strings like: to:me -from:jane@mixmax.com cheese pizza -mushrooms and returns an instance of SearchString which can be mutated, return different data structures, or return the Gmail-style search string again. Here is some of the functionality in action: const SearchString = require(\"search-string\")\n\n// All pizzas to me NOT from jane topped with cheese and NOT mushrooms. :)\nconst str = 'to:me -from:jane@mixmax.com pizza topping:cheese -topping:mushrooms'\n\n// Parse string into SearchString object.\nconst searchString = SearchString.parse(str);\n\n// Add another topping choise. In this case: NOT onions.\nsearchString.addEntry('topping', 'onion', true /* negated */);\n\n// Get results into an array format:\n/* [{\"keyword\":\"to\",\"value\":\"me\",\"negated\":false},{\"keyword\":\"from\",\"value\":\"jane@mixmax.com\",\"negated\":true},{\"keyword\":\"topping\",\"value\":\"cheese\",\"negated\":false},{\"keyword\":\"topping\",\"value\":\"-mushrooms\",\"negated\":false},{\"keyword\":\"topping\",\"value\":\"onion\",\"negated\":true}] */\nsearchString.getConditionArray();\n\n// Get results back into a human readable string. Useful for passing to APIs.\n// to:me -from:jane@mixmax.com topping:cheese -topping:mushrooms,onion pizza\nsearchString.toString(); Runnable source at RunKit Transformers Something that especially excites me about search-string is how it supports “transformers”. A search-string transformer is a function you pass in as an argument when parsing the string. It takes free text and converts it into a <key, value> pair. This is useful at Mixmax because we infer what you are searching for by pattern matching. For example, when using the advanced search box on the Mixmax Live Feed, if you search for mixmax.com , we translate that to to:mixmax.com . Before search-string, we we used to need another round of parsing (after our initial string parsing!) to extract email-like strings from queries and assign them to default fields. Now that we have search-string transformers, we can parse out domain specific search operators during the original parsing phase. This makes for more performant and cleaner code. Here is a transformer example that continues our pizza theme. const SearchString = require(\"search-string\")\n\n// Declare a short list of known pizza toppings.\nconst pizzaToppings = ['cheese', 'mushrooms', 'onions', 'green pepper'];\n\n// A simple transformer that detects toppings in the string and adds them to the 'topping' search operator.\nconst toppingTransformer = (text) => (pizzaToppings.indexOf(text) >= 0 && { key: 'topping', value: text });\n\n// Perform the parsing including the transformer.\nconst searchString = SearchString.parse('to:me cheese mushrooms dominos', [toppingTransformer]);\n\n// Get back the human readable parsed search string.\n// 'to:me topping:cheese,mushrooms dominos'\nsearchString.toString(); Runnable source at RunKit At Mixmax, we already have a collection of transformers for parsing email addresses and domains. We hope to open source those soon! Your contributions search-string is a small but important module at Mixmax, so we will continue to develop it and share updates with the community. We look forward to hearing your ideas on how we can improve it. Just file a pull request ! :) Love Open Source like us? We’re hiring !", "date": "2017-12-09"},
{"website": "Mix-Max", "title": "Introducing the Sequence Picker SDK", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/sdk-sequence-picker/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar . The previous post on December 1st was about Writing Effective Engineering Specs . Mixmax has a rich developer platform that consisting of several categories: REST API - This is our “traditional API”. It allows you to read and write data from Mixmax programmatically using an HTTP REST interface. Many companies are building on top of this API to integrate Mixmax data into their product or their internal tools. Message Integration SDKs - Enables you to embed content within Mixmax messages by building a slash command or a Mixmax Enhancement . Examples include Giphy, Vidyard, and Yarn . Sidebar SDK - Enables you to add your product in a new tab in the Mixmax sidebar. This could be used by the Mixmax user to show additional data about a person that you can pull from your own product’s backend. Widget SDK - Enables you to embed Mixmax functionality into your website, with minimal coding required. Today we’re introducing a new Widget SDK addition: the sequence picker . By using this SDK, you can embed an “Add to Mixmax Sequence” button in your website (or web-based product) to allow your users to add email contacts to one of their Mixmax Sequences . Using a few lines of HTML and Javascript, you can transform an ordinary HTML button into an \"add to Mixmax sequence\" button. When your user clicks that button, it will show a list of their sequences in a dropdown menu. When they select a sequence, your Javascript provides a list of contacts (email addresses and names) to be added to that sequence, ready for the user to send. Example of the Mixmax SDK Sequence Picker button embedded in a third party product which shows a list of contacts that the user can select and add to the Mixmax sequence (using this API). Clicking the \"Add to Mixmax Sequence\" button will show the user's Sequences Selecting a sequence will open a new tab and add the recipients to the Sequence Adding a sequence picker button to your website Load the widget SDK into your website as described here . Create a new anchor element with a mixmax-add-sequence-recipients-button class to allow Mixmax's widgets JavaScript to discover the element and enhance it into a sequence picker button. Set a data-recipients-function attribute whose value is the name of a global function. When the user clicks on the button, this function will be called with a single parameter, a callback. The function should invoke this callback with an array of recipients to add to the sequence selected by the user. These recipients should have the structure shown here . Example code <script defer src=\"https://sdk.mixmax.com/v1.1.2/widgets.umd.js\"></script>\n\n<div class=\"mixmax-add-sequence-recipients-button\"\n  data-recipients-function=\"getRecipients\">\n</div>\n\n<script type=\"text/javascript\">\n  // For an example of where this data might come from, see\n  // https://github.com/mixmaxhq/mixmax-sdk-js/blob/master/examples/sequencepicker/index.html.\n  window.getRecipients = function(done) {\n    done([{\n      email: 'john.smith@example.com',\n      variables: {\n        Email: 'john.smith@example.com',\n        'First Name': 'John',\n        'Last Name': 'Smith'\n      },\n      scheduledAt: false\n    }]);\n  };\n</script> This SDK is perfect for: An Applicant Tracking System: when a recruiter is viewing a candidate’s profile, they could use the Add to Mixmax Sequence button to immediately send email outreach to the candidate. A CMS: a user could add all sales leads of a particular sales stage to a Mixmax Sequence A customer support tool: your users can add a customer to any one of several Sequences to offer specific support Video tutorial We look forward to seeing how you integrate the Mixmax Sequence into your product! Interested in working on the communication platform of the future? Email us at hello@mixmax.com and follow us @Mixmax .", "date": "2017-12-02"},
{"website": "Mix-Max", "title": "Mixmax Advent 2017", "author": ["Logan Davis"], "link": "https://www.mixmax.com/engineering/mixmax-eng-advent-2017/", "abstract": "We at Mixmax are excited to introduce our second annual Engineering Blog Advent Calendar! Last year , we ushered in the holidays with twelve\none per day blog posts. That went over so well, we’ve decided to make it an annual tradition! For the next twelve days, we’ll be sharing one engineering post per day: some about our stack,\nsome about tricky problems we’ve dealt with, and some about our philosophy and culture. We hope to\nbe just as useful to future engineering teams as past engineering blogs have been to us. Follow @mixmax on Twitter for the latest posts, or check back here daily\nfor an updated list. You can also Tweet the authors, linked to the right of each post, to offer\nyour valuable feedback! Writing Effective Engineering Specs Introducing the Mixmax Sequence Picker SDK Handling 3rd-party JavaScript with Rollup Terraforming all the things Recruiting Engineers Using Online Challenges Database-backed job processing Precisely observing structural page changes Beating Spam-Detection Bots Introducing search-string: an advanced search string parser Improving Mongo performance by managing indexes Elasticsearch vs Redshift for Real-Time Ad-Hoc Analytics Queries How to work remotely Now for the first post. Drumroll please… Writing Effective Engineering Specs Before Mixmax engineers start a major new project, such as recommended send times or delegated sending , there are several questions we\nmust answer. Which users’ needs will the project serve? How will we go about implementation? How much\ntime will the work require? To answer these questions and more, the engineer in charge of the project\nwrites a document called an Engineering Specification (or Eng Spec for short). A well written Eng Spec can be a phenomenally useful tool for making estimates, identifying unknowns,\nprioritizing, and sharing knowledge. A poorly written Eng Spec can be a confusing mess that wastes\ntime and leads engineers into dead ends. Here are a few of the lessons we at Mixmax have learned\nabout how to write effective Engineering Specs. Focus on Users At Mixmax, we don’t start a project unless we can clearly demonstrate the value it adds for our users.\nThat value doesn’t have to be provided directly — for instance, architecture overhauls can\nspeed up long term development velocity — but we’re mindful to not burn time on, say,\nover optimizing developer experience. When we began the switch from Backbone to React , we didn’t do it\nbecause React is more fun (although it is :). We did it because it offered distinct performance\nand user experience benefits. We believe that Specs should always focus on real world use cases. Frequently reconsidering to these\nuse cases while building lets us create targeted and elegant user experiences. We also try hard to\nsplit projects into incremental releases, prioritized by the value they provide to users. This lets\nus deliver 90% of a feature’s value in 50% of the time. A good Eng Spec enables this UX elegance\nand high velocity by keeping a laser focus on users. Set SMART Goals In keeping with our focus on users, we believe Eng Specs should include written goals that\ndescribe the value the project provides. Of course, written goals are useless if you don’t have\nany way of determining whether you’ve hit them. Accordingly, we make sure that all of our goals\nare SMART : Specific, Measurable, Assigned,\nRealistic, and Timeboxed. This is a SMART goal: “Reduce attachment failure rate to sub-0.25% of DAUs by Thursday, Feb 11.” These are not SMART goals: “Make attachments work 100% of the time” : not realistic “Fix attachment failure rate by Thursday Feb 11” : not measurable “Implement sequence folders” : not specific, measurable, or timeboxed Because they are concrete rather than abstract, SMART goals remain useful during implementation.\nHaving these goals written down and frequently measuring our progress against them lets us quickly\nunderstand if a project is getting off track. Even after a project is complete, SMART goals continue\nto be useful for retrospectives and evaluation. Write Actual Code We’ve found that, when writing about software, it’s often clearer and quicker to communicate in code\nthan in English. Every engineer has a unique written and spoken communication style, but at Mixmax,\nJavascript is a lingua franca. Why spend hundreds of words clarifying the exact structure of a\nrecord in the database when showing some JSON would do? At Mixmax, our Eng Specs are littered with code snippets and JSONs. Walking through common use\ncases step by step with example records and pseudocode can help engineers reason about problems,\nthink about performance, and detect edge cases early. Sometimes, we even end up writing code in\na spec that gets literally cut and pasted into our IDEs! Build Estimates Incrementally Making good estimates is important to prioritizing work and keeping a sizable engineering team\nhumming along. Unfortunately, it’s also really hard. The enemy of a good estimate is the\n“unknown unknown”; the work you don’t know you need to do. The bigger the project, the more unknowns,\nand the harder estimation gets. So should you just not make estimates? Of course not! Instead, you\ncan use an Eng Spec to enable accurate estimates by breaking tasks into incremental chunks. At Mixmax, before we start coding on a big project, we divide that project into many small\nsubtasks and manually write out how we’ll attack each one. This allows us to identify any\nunknowns in the subtasks, then potentially address them. Once we’ve done this, we make an\nestimate for each small task, throw in extra time for testing, reviewing, and cushion, and\nadd it all up to get a project estimate! When estimates are built like this, not only are they more accurate (since engineers are\nalways more experienced with the subtasks than with the big projects), they’re dynamic. If\nan “unknown unknown” crops up in one subtask, estimates can be updated. Furthermore, if a\nsubtask doesn’t provide value to users that justifies the time it takes, it can be easily\ndescoped to a later release. Granular estimates lets us retain flexibility while reaping the\nrewards of transparency. Own Unknowns A specification is not a stone tablet to be presented to your teammates — it is a tool for planning\nand communicating together with them. That means that a good Eng Spec asks questions! Teammates are\nan incredibly valuable source of knowledge. We learn more, and make the right decisions faster, by\nadmitting what we’re not sure of and explicitly asking for advice and feedback. This is the whole\npoint of a having a team. At Mixmax, our finished Eng Specs often contain open questions and admit to “known unknowns”. Some\nof these are big enough questions to block implementation. That’s OK! We don’t just start building\nwhen we finish the Eng Spec’s first draft — we share our specs with the team and ask for advice and\nfeedback first. Often, teammates can leverage their prior experience to resolve these open questions. Other times, “known unknowns” will be non blocking. For example, UX details don’t always need to be\nset in stone right away. Indeed, waiting to finalize those details until they can be seen in the\ncontext of the rest of the product can often lead to much smoother user experiences. By contrast,\nhammering out every last detail before building offers diminishing returns on time investment,\nhampers velocity, and leads to inorganic and clunky UX. In summary: by focusing on users, setting SMART goals, estimating incrementally, thinking in code, and\nowning our unknowns, we at Mixmax have been able to use Engineering Specs as force multipliers.\nThey let us design thoughtfully, plan thoroughly, evaluate progress, conduct retrospectives,\ncommunicate clearly, and save huge amounts of development time. If you want to reap these rewards,\nyou, too, can write great specs! Or, if you like communication, embracing uncertainty, and delivering lots of value fast, you can\ncome work at Mixmax. :) Join us!", "date": "2017-12-01"},
{"website": "Mix-Max", "title": "Practical Metrics with Graphite and Terraform (Part 1)", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/practical-metrics-one/", "abstract": "Over the past few months at Mixmax, we’ve been changing the way we track internal metrics. In this series of blog posts, we’re outlining our journey to metrics bliss. In this post, part 1, we’ll explain why we switched away from CloudWatch. In part 2 we describe the architecture of our Graphite cluster. Finally, part 3 dives into how we deploy and maintain that cluster with Terraform. As we’ve written before , up until this point we primarily used AWS CloudWatch as our primary method for tracking and alerting on metrics for all our production systems. However, in the last six months we began running into limitations of CloudWatch, which led us to explore other solutions that could scale more gracefully with our increased and expanding needs. The two primary limitations we ran into were: CloudWatch does not easily support high cardinality metrics. While CloudWatch supports “dimensions” for searching and graphing individual metrics under a topic, it does not easily support aggregations over multiple dimensions. For example, you can display metrics for five dimensions from a single topic on the same graph, but you cannot show the aggregate of all five dimensions. CloudWatch has request, payload, and dimension limits that materially affected our implementation choices. These limits meant that each engineer on the team had to consider whether their new feature or infrastructure would publish too many metrics or metrics with too many dimensions to be useful. Combined, these limitations increased the barrier to publishing detailed metrics for new features, and forced engineers on our team to think twice before adding metrication to new features. Practically, it meant that we had fewer metrics on important systems than we’d like. Feeling that our tooling choices should empower engineers rather than limit us, we decided to explore options other than CloudWatch for many of our internal metrics. To direct our exploration, we outlined a set of requirements for our new metrics tool: the things we value most in metrics tooling. First, we knew the new solution must: Support a high (and increasing) volume of requests without client-side downsampling. Allow aggregating metrics on multiple dimensions. Allow alerting on graphed metrics. In essence, we needed a scalable solution for aggregating high-cardinality metrics and alerting on the results. We also considered a few other attributes that were important, but not absolutely required. We preferred solutions that: Require minimal maintenance. Are easily scalable with our existing tooling. Have an easily useable node.js client. Are backed by fault-tolerant storage. Could be integrated incrementally. It’s worth keeping in mind that our solution could have some risk of dropping metrics and didn’t need to store or query full, plaintext payloads. We also ruled out most managed solutions since they offered more functionality than we needed from this metrics tool and required higher-impact changes to our existing tooling. As a result, we considered three self-hosted solutions that satisfy our three requirements: InfluxDb Prometheus Graphite Prometheus is a time-series metric collection and aggregation server. It offered some interesting options for monitoring remote processes, but required restructuring our backend services to adhere to its polling architecture. We also ruled out Prometheus because it lacked durable storage mechanisms and would require Graphite or InfluxDb to back its metrics storage. InfluxDb is a time-series database that’s part of a larger stack for ingesting, storing, and alerting on time-series data. We ruled it out because the open source version doesn’t have a solution for scaling or high availability and wasn’t well-supported in the node.js ecosystem. Graphite is a time-series database system for ingesting and storing metrics. We finally decided on Graphite because it best accomplishes our 5 preferred attributes by being flexible, offering the ability to durably store data and scale horizontally, and being well-supported in the node.js ecosystem. Though it required a bit more initial setup, Graphite has well-tested, scalable, and open-source implementations that don’t require constant maintenance. Additionally, since Graphite itself is just an API for storing and retrieving data, we gained the flexibility to swap out implementations or data stores as necessary. Continue with part two , where we go over our clustered Graphite architecture which handles hundreds of millions of data points per day. Interested in working on a data-driven team? Join us!", "date": "2019-04-19"},
{"website": "Mix-Max", "title": "Database-backed job processing", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/database-backed-job/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar .\nThe previous post on December 5th was Recruiting Engineers Using Online Challenges . At Mixmax, we take advantage of job queues for handling our high volume data processing, adding\nfault tolerance between our microservices ,\nand driving our background automation . However, not all\njobs are created equal, and sometimes normal processing doesn’t quite cut it. Why batch jobs? In general, job queues function as one way channels to process discrete units of work. As such, they\nare usually designed for sequentially publishing and processing single  jobs where new work is added\nto the end of the queue and consumers continuously handle those jobs, taking actions based on the\ndata within the job. However, some jobs don’t easily fit into this paradigm. Examples of jobs that\ncan be problematic when processed individually: Jobs that are ingested at high volumes and require long processing times eventually back up the\nqueue as the input is higher than the throughput. Jobs where the downstream provider is limited (i.e. databases or rate limited API calls) simply\ncannot process individual volume at the required rate. If either of these is a hard limit on job processing or if job processing is not time sensitive, we\ncan instead batch operations together to make processing each job more efficient. If we’re going to batch jobs, it’s important to note that the FIFO order of queues is likely not the\nbest way to consolidate operations for efficiency. To get around this, we need to rethink the\nstructure of our queues so we can efficiently batch operations, while not sacrificing throughput for\nincoming operations. At Mixmax, our solution was to decouple our queue’s input operations from the\nbatched processing using a database as the layer of abstraction between the two. Structuring the queues At Mixmax, we use bee-queue ( check out the project ), a Redis-backed queue written in node, to handle all our jobs. For this queue, we have functions that create jobs to add to the queue (let’s call these publishers) and register functions to process those jobs as they arrive (these are consumers). To batch our jobs, we use need a set of publishers and consumers to store queued operations, and a set to process the batches of operations (we call these the ‘store queue’ and the ‘flush queue’). We also need to choose a value to partition our batches into disjoint sets. For example, if each job is performed per-user, we split jobs by the user’s unique Id. Store Operations The store queue’s role is to transform incoming jobs, add any data necessary for processing, and then store whatever is required for the flush queue to perform the operation. This makes the code for our store queue really simple: // storeQueue.js\nconst { Queue } = require('bee-queue');\nconst storeQueue = new Queue('storeQueue');\n\n// Publish a new job to the store queue.\nasync function publish(user, data) {\n  await storeQueue.createJob({ userId: user.id, task: data }).save();\n}\n\n// Process jobs as they arrive.\nasync function consume(job) {\n  const { userId, task } = job.data;\n\n  await db.stored.insert({ userId, task });\n  // Publish a flush job.\n  await flush.publish({ userId });\n}\nstoreQueue.process(consume); Notice that the last line of our store queue consumer actually publishes a job to the flush queue. The simplest trigger for the flush queue’s check is when the store queue finishes storing an operation. To trigger the flush queue, we publish a new flush queue job when the store queue completes its processing, and the only information passed to the flush queue publisher is a key that uniquely identifies the batch that should be flushed (in this case, a unique user id). Flushing Operations The flush queue takes stored operations, checks to see if the batch is large enough, and completes the batch if it’s over the threshold for batch operations. The code to publish to our flush queue is only marginally more complex than the store queue: // flushQueue.js\nconst { Queue } = require('bee-queue');\nconst flushQueue = new Queue('flushQueue');\n\n// Publish a flush job and set the static job id.\nasync function publish({ userId }) {\n  await flushQueue.createJob({ userId })\n    .setId(`flush_${userId}`)\n    .save();\n} The first thing to note is that we manually set the id of published jobs in the flush queue. We set this, unique to each batch of operations, using bee-queue’s setId method. It’s important to set the job id here to prevent the same batch from being processed more than once concurrently. By ensuring each job id relates to exactly one batch and that all batches of operations are disjoint (that no stored operation can be in two batches), we effectively lock each batch until the previous batch completes. This could also be done using a distributed lock for each unique batch of operations. Now, we’ve naturally split our operations into batches, so consuming them is just a matter of retrieving those batches and determining if the batch is large enough for us to perform the operations it contains. When consuming the job, we want to make sure it’s sufficiently large to make the batching worth it. If batches are too small, then we’ve added a layer of complexity without actually improving our throughput or avoiding the limits of a downstream provider. // flushQueue.js\nasync function consume(job) {\n  const { userId } = job.data;\n\n  const jobs = await db.stored.find({ userId });\n  if (jobs.length < MIN_BATCH_SIZE) return;\n\n  // Perform operations in bulk.\n\n  // Remove completed operations.\n}\nflushQueue.process(consume); We consume the flush job, use the partitioning value to retrieve the size of the unique set of operations, and check if it’s larger than our minimum threshold. If it’s larger, we can do whatever work is necessary, removing the operations when complete. Otherwise, there’s nothing to do and the current operations can wait until the next one is added to the batch. With those few bits of code, you can use bee-queue and mongo to process incoming operations in batches, rather than performing the same (expensive, slow or limited) operation on each one. Even better batches While it’s likely that the simple approach above can solve many batching problems, there are a few minor tweaks to the batching algorithm that can make it even better: Error handling One shortcoming of the above implementation is that it doesn’t handle errors during processing very well. The only options are either retrying the whole batch if one operation fails or not retrying at all. Neither of those are particularly appealing options, but by adding an additional error property to stored operations and detecting failed operations after attempting to complete a batch of operations, you can remove successful operations from the stored batch, but keep failed operations to retry, incrementing their error count. Then, only retrieve operations below a certain error threshold. Then it’s possible to retry a few times, without retrying indefinitely. With the addition of error handling, the flush queue code might look like this: // storeQueue.js\nasync function consume(job) {\n  const { userId, task } = job.data;\n\n  await db.stored.insert({ userId, task, errors: 0 });\n  // Publish a flush job.\n  await flush.publish({ userId });\n}\nstoreQueue.process(consume);\n\n// flushQueue.js\nasync function consume(job) {\n  const { userId } = job.data;\n\n  // Only allow 5 errors.\n  const jobs = await db.stored.find({\n    userId,\n    $or: [\n      { errors: { $exists: false } },\n      { errors: { $lte: 5 } }\n    ]\n  });\n  if (jobs.length < MIN_BATCH_SIZE) return;\n\n  const succeeded = [];\n  for (let i = 0; i < jobs.length; i++) {\n    cosnt job = job[i];\n    try {\n      // Perform operation.\n    } catch (err) {\n      await db.stored.update({ _id: job.id }, { $inc: errors: 1 });\n      continue;\n    }\n    succeeded.push(job);\n  }\n\n  // Remove successful operations.\n}\nflushQueue.process(consume); Avoid trapped jobs An additional shortcoming of the simple approach is that it is sub-optimal for low volume batches. While some batches might fill up quickly, others might sit below the MIN_BATCH_SIZE indefinitely. So, we should add a timeout condition to operations as well. That is - if an operation has been in the queue for too long, automatically perform it regardless of the batch size. By adding this timeout, we guarantee that items in our queue are never outside the acceptable range of staleness. You can simply track this using a Date field on operations like this: // storeQueue.js\nasync function consume(job) {\n  const { userId, task } = job.data;\n\n  await db.stored.insert({ userId, task, errors: 0, createdAt: new Date() });\n  // Publish a flush job.\n  await flush.publish({ userId });\n}\nstoreQueue.process(consume);\n\n// flushQueue.js\nasync function consume(job) {\n  const { userId } = job.data;\n\n  let jobs = await db.stored.find({\n    userId,\n    $or: [\n      { errors: { $exists: false } },\n      { errors: { $lte: 5 } }\n    ]\n  });\n  if (jobs.length < MIN_BATCH_SIZE) {\n    // Filter jobs that must be flushed.\n    const flushThreshold = new Date(Date.now() - 10 * 60 * 1000) // 10 minutes ago.\n    jobs = jobs.filter((job) => job.createdAt < flushThreshold);\n  }\n  if (jobs.length === 0) return;\n\n  const succeeded = [];\n  for (let i = 0; i < jobs.length; i++) {\n    cosnt job = job[i];\n    try {\n      // Perform operation.\n    } catch (err) {\n      await db.stored.update({ _id: job.id }, { $inc: errors: 1 });\n      continue;\n    }\n    succeeded.push(job);\n  }\n\n  // Remove completed operations.\n}\nflushQueue.process(consume); Though we implemented this pattern with bee-queue and mongo at Mixmax, it’d apply equally well for other queue and database combinations. So, next time you have queues with lots of input and slow consumers (or limited usage), fear not! You can use existing queues together with your database to help compose single operations into more manageable and efficient batches. Interested in building high-volume queues for all kinds of work? Join us!", "date": "2017-12-06"},
{"website": "Mix-Max", "title": "Beating Spam Detection Bots", "author": ["Simon Xiong"], "link": "https://www.mixmax.com/engineering/beating-spam-detection-bots/", "abstract": "This blog post is part of the Mixmax 2017 Advent Calendar .\nThe previous post on December 7th was Precisely observing structural page changes . One way that Mixmax revolutionizes your email is by enabling you to schedule meetings instantly without back and forth messages. Here's what our meeting picker looks like: When a recipient receives this email, all they have to do is click one of the available timeslots, and the meeting will be scheduled instantly with automatic double-booking protection . What Could Go Wrong? The downside of being able to schedule meetings with a simple click is that anyone (or anything) can do it. We've noticed that some recipients have installed spam detection bots in their email client that will click every link in the email to check that the links are safe. As a result, it seems like the recipient tries to \"book\" every meeting timeslot, causing a lot of confusion. A Simple Solution What's a simple way to detect if a bot tries to click all your meeting links at once? By rate-limiting the number of requests! We set the limit on the number of meetings a recipient could schedule to 4 per 30 seconds. We thought this would be good enough to prevent bots from scheduling every timeslot in the email. const limiter = require('express-limiter');\n\nlimiter({\n  path: '/api/scheduleMeeting',\n  method: 'get',\n\n  // 4 requests per 30 seconds.\n  total: 4,\n  expire: 30 * 1000\n});\n\napp.get('/api/scheduleMeeting', require('./scheduleMeeting')); What Could Go Wrong? Part 2 We encountered several issues with this simple rate-limiting approach: If the recipient tries to schedule a meeting shortly after receiving the email, they will be blocked for up to 30 seconds because their spam bot caused them to exceed the rate-limit. We noticed that bots were still able to get around our rate-limiting, resulting in multiple meeting confirmations before the recipient had actually clicked a timeslot. A Better Solution One way to raise the bar for bots is to require that they evaluate client-side Javascript in order to schedule a meeting. Here’s how it works: When a meeting request comes in following a click, we don't immediately schedule the meeting. Instead we send back a fake response consisting of some simple HTML. Inside the HTML, we include a couple lines of Javascript telling the browser to automatically resend the request with a special query param. Here's what it looks like: function redirectMiddleware(req, res, next) {\n\n  // Client has successfully evaluated Javascript and redirected back to this\n  // url with the special query param. Proceed with scheduling the meeting.\n  if (req.query['specialParam'] === 'true') return next();\n\n  return res.send(`\n    <html><body>\n      <script type=\"text/javascript\">\n        const query = window.location.search;\n        window.location.href += (query ? '&' : '?') + 'specialParam=true';\n      </script>\n    </body></html>\n  `);\n}\n\napp.get('/api/scheduleMeeting', redirectMiddleware, require('./scheduleMeeting')); This solution works because the user's browser will load the DOM and execute the Javascript inside the HTML response while most bots won't . If the request was initiated by the recipient clicking a timeslot, their browser will execute the Javascript and resend the same request, except this time with our special query param. Upon seeing this special query param in the url the second time around, we schedule the meeting for the recipient. The Result Even though we have to make a second request for every meeting request, this only occurs upon clicking a link and the delay is unnoticeable. Since adding this check, we've seen a decrease in customer issues about duplicate meeting confirmations and we were able to relax the rate-limit from 4 per 30 seconds to 1 per second, improving the user experience. Generally, distinguishing between bots and a real browser involves focusing on capabilities that the browser has that most bots do not. In our case, we relied on the browser's ability to load the DOM upon receiving an HTML response from our server to verify that the browser initiated the request. We look forward to seeing how you guard against spam bots in your product! Interested in working on the communication platform of the future? Email us at hello@mixmax.com and follow us @Mixmax", "date": "2017-12-08"},
{"website": "Mix-Max", "title": "Mixmax Creative Code Challenge, May 2019", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/code-challenge-may-2019/", "abstract": "Every day at Mixmax, engineers must consider problems and solutions from many different angles: we assess risk in our deploy plans, balance readability and succinctness, and make sure to delight users with small touches. Working within the constraints of professionalism and quality is important for a high-performing team, and it's equally important to find ways for our creativity and playfulness to shine. In May, we set ourselves a challenge: toss all the usual constraints out the window, optimize for creativity, cleverness, and enjoyment instead, and write some code that outputs the Mixmax logo! This code wouldn't go into production, of course, and it wouldn't need to be maintained by other team members. It was purely a way to exercise the right side of the brain and have a little fun. We called it the Creative Code Challenge, and gave ourselves a month to see what we could do. Participation wasn't mandatory, because creativity can't be forced. On the Mixmax engineering team, we each allocate a portion of our work time to pursue anything that interests us or helps us grow and learn, so taking part in the creative challenge was just one of (nearly) infinite options available for spending that time. At the end of the month, four of us had put together entries. tl;dr check out the challenge and the entries on Github , or read on for a summary. Logo Spinner Trey Tacon , our head of platform engineering, is a big fan and advocate for Go, so of course he took the creative code challenge as a refreshing opportunity to exercise his Go skills in an unfamiliar area: image manipulation. He was motivated to create something a little more engaging than a static logo, so his entry applies a spinning Mixmax logo watermark to any image. Logo Golf Tucker Leavitt had never tried his hand at code golf prior to this challenge, and liked the idea of getting to know and use the weird, fun corner cases of the language he writes in every day. His entry prints an ASCII art version of the Mixmax logo to the console, and clocks in at only 117 characters of javascript. Extremely Concise Logo Eli Skeggs says, \"The Mixmax logo has some neat geometries that I realized I could exploit by using spatial transforms to turn the entire logo into a single inequality. I realized it'd be even more fun to code-golf this solution, as it ends up being pretty amenable to a functional approach.\" He chose to work with the Pyth language because \"Pyth is an old friend, and I generally enjoy class-1 programming exercises (those involving mathematics and the terse representation of logic/control).\" Layers of Logos I ( Ryan Freebern ) have always been a fan of hacks with multiple layers, like those often seen in the IOCCC or in PoC||GTFO . While my entry isn't on the same level as either of those, I enjoyed finding three ways to represent the Mixmax logo via JavaScript: as whitespace in the source, as the quine-esque output, and via careful use of commit messages. Take part! Does this look like fun? Feel free to put together your own entry ( see the very loose guidelines here ) and submit a PR. Want to work with a team that understands the value of balancing rigor and fun? Come join us !", "date": "2019-07-02"},
{"website": "Mix-Max", "title": "Mixmax hosts SFNode and presents: UX Design for Engineers", "author": ["Logan Davis"], "link": "https://www.mixmax.com/engineering/sf-nodejs-meetup-june-19/", "abstract": "Mixmax was proud to host SFNode again at our office on June 6. This is the third time we've hosted SFNode. We truly enjoy bringing together the community of Javascript developers in San Francisco to learn together. Mixmax's own Logan Davis presented Progressive UX Design for Engineers . Beyond delivering valuable features to users, engineers have a responsibility to respect their users' time by not making them wait for applications to process, load, or respond. This talk is an introduction to designing web applications that are not just RESTful but also respectful to their users' time. Here's his talk: Interested in bringing together the developer community? Join us !", "date": "2019-06-10"},
{"website": "Mix-Max", "title": "Mixmax Advent 2020: 12 days, 12 blog posts", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/introducing-mixmax-advent-2020/", "abstract": "Following a proud internet tradition of advent blogging, the Mixmax Engineering team will be sharing 12 blog posts over the next 12 days right here in our engineering blog . Our posts will cover a variety of topics from how we dynamically configure our microservices to our philosophy on engineering interviews. They're meant to share a glimpse of what it's like building a platform and team on a mission to eliminate the world's busywork . Follow @MixmaxEngineer on Twitter for the latest posts. Or bookmark this page to see the full list of posts, updated daily. How I successfully planned & ran hack days for the distributed Engineering team at Mixmax Saving money, time, and space with MongoDB indexes Introducing the Mixmax Engineering Handbook Maintaining an end-to-end integration test suite Share meeting recordings automatically with Mixmax Rules How We Hacked* Our Manager The Future of Services at Mixmax Migration from Elastic Beanstalk to Fargate using Terraform A Human-Centric Approach to Retrospectives You're going to want to follow this star (schema) for your Data Analytics A Brief History of Mixmax, Pt. 1 Engineering levels at Mixmax", "date": "2020-11-30"},
{"website": "Mix-Max", "title": "Automate Your Onboarding Emails with the Mixmax API", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/automate-onboarding-emails-via-mixmax-api/", "abstract": "For a small business, connecting with the people who sign up on your website is a key part of establishing a successful\nworking relationship. You want to make sure they're able to get the most out of what you're offering, and sending them\nemails with how-tos, tips, and suggestions is an effective way to do that. Mixmax can power your customizable and personalized onboarding emails to help you start a dialogue with your new users.\nAll it takes is setting up an integration. In this post, I'll guide you through the process of adding a user to\nyour onboarding sequence via the Mixmax API. Get set up with a Mixmax account When someone signs up on your website, we'll send their contact info to the Mixmax \"add recipients to a sequence\" endpoint and add them to your onboarding sequence. In order to do this, you'll need to have a Mixmax Small Business Plan. If you\nalready have one, great! If not, you can sign up here . Create your onboarding sequence Step one is to put together your onboarding sequence in the Sequences section of the Mixmax\ndashboard . Using a Mixmax Sequence provides you with many\ntools to make your emails more engaging by including polls, yes/no\nquestions, embedded videos or gifs, or tools to allow your users to book time on your sales team's calendar directly. At a basic level, your onboarding sequence should consist of a series of messages spaced out over the course of a week\nor two. It should welcome your users to your service and provide personalized tips to help them understand your value\nproposition and learn how to quickly get up to speed with what you can provide them. One thing to remember is that the users who will be receiving this sequence\nare likely to be brand-new to you and you won't know much about them like their full names, phone numbers, etc. so you won't\nbe able to include many {{ variables }} in your sequence messages. You can learn more about creating sequences from our success\narticles . Once it's done (don't worry, you can always\nupdate it later), let's get it wired up to your signup system. Get set up with the Mixmax API Once you've got your sequence ready, it's time to set up access to the Mixmax API. Luckily, this is a straightforward\nprocess. Get and test your API token In the Mixmax dashboard, open up the Integrations settings\npage. Near the top of the page, you'll see an \"API\" integration. This will allow you to generate your Mixmax API token,\nwhich is required in order to use the API. Click the \"Create Mixmax API Token\" button and copy the token shown to\nsomewhere safe. Now you're ready to call the API. In a terminal, you can test it out with a simple curl command: curl --header \"X-API-Token: <your token>\" https://api.mixmax.com/v1/users/me With your token in place, that command will return your Mixmax user ID, something similar to: {\n  _id: \"5e7140e9ce5935225b4999c2\"\n} Now you're all set to use the API. (Having trouble? Email us and we'll gladly give\nyou a hand!) Test your onboarding sequence In order to tell the Mixmax API which sequence to add people to, you'll need its ID. You can find it in the URL while you're editing the sequence; it looks similar to 52ee8b4b7df7362e9aea4c30 . Copy this ID somewhere safe. Let's try adding a user to your sequence via curl . The API endpoint we'll use is the POST /sequences/:id/recipients/ endpoint which\nadds a new recipient to the sequence identified by the ID in the URL. In this case, we'll use the ID of the onboarding\nsequence you created above. Try this, with the appropriate values filled in: curl -XPOST \\\n    --header 'Content-Type: application/json' \\\n    --header \"X-API-Token: <your token>\" \\\n    https://api.mixmax.com/v1/sequences/<your sequence ID>/recipients \\\n    -d '{\n      \"recipients\": [{\n        \"email\": \"ryan@mixmax.com\",\n      }],\n    }' You should see a response like: [\n  {\n    \"email\": \"ryan@mixmax.com\",\n    \"status\": \"success\"\n  }\n] This means you've successfully added me to your onboarding sequence. Our goal is to make your website signup system do\nthis same thing automatically, so let's get into the code. Adding a user to your onboarding sequence in a Node web app To accomplish this in a website powered by Node, you'll need to do essentially the same thing that the curl command\nabove does: send a request to the API with the email address of your new user. There are many possible approaches;\nwe'll talk through two of them: using a generic Node module for making HTTP requests and using the Mixmax SDK for Node. Making the API request directly I like the got module for making requests, which you can add to a Node project by\nrunning (for example): npm install got Here's a simple example of using got to talk to the Mixmax API: const got = require('got');\n\nasync function addUserToMixmaxOnboarding(email) {\n  const onboardingSequenceId = '<your sequence ID>';\n  await got(`https://api.mixmax.com/v1/sequences/${onboardingSequenceId}/recipients`, {\n    method: 'POST',\n    headers: {\n      'X-API-Token': process.env.MIXMAX_API_TOKEN,\n    },\n    json: true,\n    body: {\n      recipients: [{ email }],\n    },\n  });\n} This code expects that you'll be storing your Mixmax API token in the environment variable MIXMAX_API_TOKEN on the\nserver where your code is running. After a new user signs into your website, you'd call addUserToMixmaxOnboarding with their email address as the\nargument, and voilá, they'd be added to your onboarding sequence. Using the Mixmax Node SDK If you'd like your code to be even simpler, you could use one of our SDKs .\nFor example, you could install the Node SDK by running npm install mixmax-api and then your code would look something like this: const MixmaxAPI = require('mixmax-api');\n\nasync function addUserToMixmaxOnboarding(email) {\n  const api = new MixmaxAPI(process.env.MIXMAX_API_TOKEN);\n  const onboardingSequenceId = '<your sequence ID>';\n  const sequence = api.sequences.sequence(onboardingSequenceId);\n  await sequence.addRecipients([{ email }]);\n} Similar to above, you'd need to call this function at the location in your code where the user has signed up and you\nknow their email address. And we're done! Now that your web app is talking to the Mixmax API and putting your new users into sequences, you've got a great new way\nto automatically connect with those users as they begin to learn about what you can do for them. If you have any questions about this blog post, or want to know more about using the Mixmax API to make your workflows\neasier and your products better, please feel free to reach out to us . We'll be\nglad to help! And if you're interested in joining the Mixmax team and making software that makes peoples' lives easier, check out our careers page .", "date": "2020-03-30"},
{"website": "Mix-Max", "title": "5 Things We Learned in Achieving SOC2 Compliance", "author": ["Seth Sakamoto"], "link": "https://www.mixmax.com/engineering/5-things-we-learned-in-achieving-soc2/", "abstract": "SOC2 Type II security compliance. It’s the gold standard for providing buyers assurance you have your shit together information security-wise.  With few trusted independent authorities in the world for information security, the American Institute of Certified Public Accountants (AICPA) - yes, accountants - have stepped in to provide their audit prowess.  With this certification in hand, your customers and partners can trust an independent party has verified you follow a playbook of security best practices. We achieved SOC2 Type II, aka SOC2, compliance recently…yay!  For those of you considering it, or currently going through it, here are five things we learned along the way. Do a “readiness assessment” .  Most if not all SOC2 auditors offer you the ability to do an early audit, to tell you directionally how close or how far away from the requirements you currently are.  It’s usually an optional service.  In our experience, it was worth it.  It’s better to know where you stand early so you can plan & prepare. SOC2 can be used as a force for good. In that it requires practices you should (mostly) be doing anyway.  In a nutshell, SOC2 involves documenting how decisions are made and changes are implemented, ensuring changes are auditable if something goes wrong, and putting some thought into your top risks.  In the right proportion, every company worth anything needs these. Conversely, you should focus on the intent vs following every requirement to the letter. It's easy to go overboard, focusing on the SOC2 requirements as a checklist.  For example, one requirement is to ensure your vendors and partners critical to delivering your service have as good or better security controls than you.  Makes sense right?  However it is also easy to interpret this as “you need an exhaustive list of every vendor you’ve ever paid money to, along with 3 years of documented contact, security reviews, and or certifications for each and every one of them.”  Huge difference in scope, with zero difference in value. Focus on your actual security risks. Related to the previous point, you can easily spend a disproportionate amount of time “peanut butter spreading” your effort in all areas.  If most of your risks are from rogue or accidental insiders, like it is with most companies , then spend your time training your team on the dangers of phishing and putting in place controls and auditing for superuser-type access.  If your service depends on 2 or 3 key vendors or partners, dig in particularly deep with them.  You can assess risk with a simple threat model, developing a comprehensive list of risks then prioritizing the ones to act on based on probability and severity. It's worth it! The badge of legitimacy is worth way more than any whitepapers or other actual proof you provide.  We’re fairly sophisticated security-wise and have a lot of evidence to show, and yet are amazed how disproportionately customers and our team get excited by our SOC2 in hand.  It’ll get you into and keep you from getting kicked out of deals. Ping us at security@mixmax.com if you’d like to connect and hear more!  Also check out our careers page !", "date": "2020-07-07"},
{"website": "Mix-Max", "title": "Introducing the Mixmax Engineering Handbook", "author": ["Brad Vogel"], "link": "https://www.mixmax.com/engineering/the-mixmax-engineering-handbook/", "abstract": "For our third blog post of Mixmax Advent 2020 , we’ll be sharing a chapter from our internal Mixmax Engineering Handbook - our central hub for all engineering documentation. It provides an overview of our engineering practices, system architecture, tooling, infrastructure, and security practices. Every engineer at Mixmax worked hard to create the handbook and everyone takes pride in maintaining it. By sharing this now, you could say that this blog post is many years in the making 😀. The Handbook is organized into 5 chapters: Chapter 1: Day-To-Day Development Chapter 2: Architecture Chapter 3: Delivery Chapter 4: Infrastructure Chapter 5: Security Today, we’ll be sharing the first chapter with you. Chapter 1: Day-To-Day Development This section of the handbook describes how Mixmax engineers write code on a day-to-day basis, using Git and related services to continuously deliver work to users. Every new engineer should read this sometime in their first week, and return frequently thereafter. It aims to answer the following questions, among others: How should I approach the practice of writing code at Mixmax? How do I submit changes to Mixmax’s codebase? Who will review my work, and how? When should my code go out to users? It focuses on the philosophy and mechanics of development, not the planning and communication that occur before and during coding. 1.1 Coding Principles & Philosophy This section does not attempt to enumerate every standard good coding practice, but rather the ones specific to our software and our overarching goals as a team and a company. In general: be a good engineering citizen and steward of the code. Functioning as an engineering team means having trust and respect for each other, and taking responsibility for producing the best software that we’re able to. Mixmax has to feel rock solid. We’re taking responsibility for our customers’ outbound communications, which is how they present themselves to the professional world. If our software seems flaky, they won’t trust it. Mixmax has to feel snappy. We sell Mixmax as a solution to improve productivity and efficiency. Interfaces that lag or force a user to wait can make them feel like Mixmax is slowing them down. Mixmax has to feel intuitive. Users already feel comfortable with Gmail and their CRM, so when Mixmax is integrated with those systems, it can’t be a jarring change. It should feel like a natural extension that augments the user’s existing expectations and workflows. Mixmax code has to feel cohesive. An engineer who starts to work in a part of the code they haven’t worked with before shouldn’t need a long period of overhead work to understand what’s going on. Variables and functions should be self-evident, and have names that clearly reflect their purpose and documentation and comments as needed to clarify behavior and establish context. Tracing a chain of function calls or how props are passed should be obvious. There should be very little cleverness or magic. Mixmax must be secure. Our users’ data is their data, and it’s our job to protect it. When developing, make sure to follow best practices (see the OWASP Secure Coding Checklist here ) and to review code with an eye to ensuring application security. More (optional) reading: Our code-styles repository A fun blog post, Progressive UX design for Engineers 1.2: Continuous Delivery At Mixmax, we practice continuous delivery . This means that we build and deploy our code in small, frequently shipped batches. We try to get our work into users’ hands as soon as possible after fixing a bug or adding a feature, even if the change is small. This approach gets features and fixes out to customers as soon as possible, makes subsequent deploys smaller (and thus less risky), and helps reveal problems that did not surface during testing, (e.g. scale issues). The philosophy and benefits of continuous delivery go way beyond what’s listed here! In practice, “continuous delivery” means that, in your day-to-day development, you will: Get a Jira issue to work on Write code to address that issue in your favorite text editor Use Git to submit your changes to one of our services Use GitHub to get code review on your work Test the code for your issue Ship your code to production! Making changes and submitting pull requests We use Git to track changes to our source, and host our code on GitHub. At Mixmax, we prefer the following Git practices: Work on a new branch off of master, named < engineer > / < ticket > , e.g. brad/SUP-1337 Use git diff to check our own code for lint errors or dead code before committing Write semantic commit messages that explain why you made changes Use interactive rebasing to clean up our work’s commit history into atomic commits Pull request branches onto master and get PR review from relevant engineers / mentors A few other Mixmax-specific Git tips and tricks: Adding Test Cases and Test Plans It’s a good idea to add test cases via jest as well as to document a test plan in your PR description. This acts as a sanity check when smoke testing your own changes and to ensure that other engineers can easily test your changes. Avoiding large PRs Generally, extremely large PRs (>1000 lines of diff, dozens of files) are not desirable. Reviewing them is difficult, both because it can be difficult to identify the flow of logic in Github’s “files changed” interface and because it takes a remarkable amount of energy and focus to be thorough in reviewing a huge PR. Responding to 50+ comment reviews can also be a real slog! The way to create smaller PRs is to organize your changes into smaller independent groups. For instance, if you’re working on a full-stack feature, consider shipping the backend API before the frontend, since the backend won’t have any dependencies. Nor will anything depend on the backend at the time you deploy it, thus it’ll be low risk to put it into production, and it’ll be easy to add to if your continued work on the frontend motivates changes. (“But how can I make sure the backend works without the frontend?” you may say. One way is to develop the backend alongside tests for the backend. 😉 Another way is to actually develop both backend and frontend simultaneously and use Git Magic™ like interactive staging or rebasing to make it look like you had done the backend first.) If you can eliminate such inter-project dependencies, then you can actually ship these smaller PRs! This doesn’t mean that the work has to be ready for users, just that it makes sense to review on its own, and is low-risk to put into production at an early stage / by itself. If you don’t want users to begin using the work-in-progress feature, you can hide it from them using feature flags. If even the smaller PRs aren’t safe to merge to master, you can and should get review early using a WIP (work in progress) PR. And if that PR starts to turn into a big PR, you can make amendments easier to review by making them in their own PRs—branching off your initial PR branch—rather than pushing them straight to the initial branch. WIP PRs Usually you’ll open a pull request when your code is fully-functional and tested. But you might open a PR earlier if you think your final PR will be huge, and it would be easier to review it incrementally you’re using a technology or framework that’s new to you, and you want tips from other engineers before you commit to a design pattern you wish a designer to begin styling your work while you continue coding In this case, open a Draft PR , optionally put the “work-in-progress” label on the PR, and either tag specific reviewers on GitHub, or use Slack to let the rest of the team know how they can best help you out (by reviewing, designing, etc.). Reviewers can find these helpful too. Also feel free to discuss your draft PRs live! Once you have completed functionality, mark the PR as ready to merge, remove the “work-in-progress” label, and comment saying “ready for final review” in addition to asking for a review via Slack. (The comment lets observers other than reviewers know that they might take a look.) Pull Reminders Many folks choose to use a tool called Pull Reminders to be actively reminded of PR review requests, and stale PRs. This gives a consolidated view of your (and others’) PRs across all the Mixmax repos, and proactive notifications are often nice nudges to review others’ work or pick up on a review you’ve been requested on. Once you’re part of the mixmaxhq organization, you can be added to start receiving updates as well. Reviewing and merging PRs Before asking for review, please practice self-review as described in our internal guide to PR review. For your first few PRs, ask for a review from: your project mentor (if applicable, otherwise one of the suggested reviewers in the upper right of the PR) your product team using the automatic pull assigners in Github your manager (if different than the above). When you’ve addressed a round of feedback, comment with “updated” on the PR to trigger a notification to the reviewer. (The reviewer may receive a notification when you push to the PR, depending on their GitHub settings, but we recommend using comments because people may push to a WIP PR without it yet being ready to review.) If the PR is still blocked on design, apply the “blocked-on-design” label. After the PR is approved, you (the submitter) will merge, since you’ll know if there’s any last things to do before merging (like testing, updating DB indexes, etc.) You can stay on top of reviews happening at Mixmax in the #github-notifications slack channel or by enabling Notifications in your personal Github Settings. Testing Code that we deploy must be tested. At Mixmax, we practice a philosophy of “low-cost testing”. At the minimum, low-cost testing means that you the submitter have thoroughly exercised your code—even before opening your PR. It can also mean writing automated tests and/or testing with other people, depending on how easy a feature is to test and how risky the changes are. Deploying Wait, I deploy immediately? Really? Yes. For most changes, once you’ve tested, you’re good to go. Caveats: When you are deploying major new features / bug fixes… For new features, the PM needs to sign off to make sure we're not introducing any half-baked features. Product and Marketing may wish to coordinate larger feature releases with marketing efforts. Talk with them about that. When your deploy is risky, we hold this deploy until the end of the day EST (3pm PST) so we get some volume, but less than peak. We should not deploy anything except extremely low-risk features after CS gets off (7pm PST) or on Fridays. It’s better that we get usage volume to discover a regression than have it be regressed for most of people’s working day (Pacific midnight - 10am) or the weekend, and CS will not be available to help handle outages. The determination of risk and the decision to deploy involves input from product and CS but ultimately comes down to engineers. If an engineer doesn’t believe the PR to be risky then they can deploy it at will. The bar for confidence just goes up depending on the product area and time of day, as described above. Will users actually get this code immediately? Server-side code is always immediately updated. For client-side code, our application code will attempt to refresh the browser, but might not be able to if it knows that a refresh will disrupt the user. As such, you will need to plan for lag accordingly i.e. make API changes compatible with older clients. Deploying to staging After you’ve passed code review and your code is thoroughly tested locally, it’s time to deploy to staging. When you submit a change to a service (by merging a PR into master), our CI service will then automatically deploy your code to the service’s staging environment. If you submitted a change to a module, our semantic-release bot will publish a new version and you'll need to submit PR(s) to the affected microservices to pick up the new version of the library. Integration testing Before shipping to production, do whatever extra testing you deem necessary on staging. This should be pretty minimal assuming that you tested locally. Automated Integration Testing Our services also make use of automated integration testing via integration-testing-for-robots (or ITFR). Jenkins runs these jest-powered puppeteer tests against staging to test a few common workflows, including loading the app signup page, sending an email from both the app and Gmail, and managing users in a workspace. When a deploy PR is opened, ITFR will run and report its results back to Github. If it fails, you'll need to open the run in Jenkins and inspect the output to determine which test failed and why. Since it's running an actual browser and connecting to actual servers, sometimes real-world issues cause transient failures (e.g. timeouts when trying to interact with Gmail), so often just re-running will resolve those. Once both these integration test suites pass, then you’re ready to deploy! Deploying to Production Once the staging deploy has finished, open a PR titled “deploy” from https://github.com/mixmaxhq/PROJECT/compare/master-production...master . Your bias should be to deploy immediately after testing, modulo the caveats above. If you’re not ready to deploy for some reason, here are ways to signal that: Prefix the PR title “ [ do not merge]” and put relevant TODOs in the PR’s description so that other engineers don’t deploy it either. If another service’s deploy needs to go first, prefix your PR title “ [ after ]”. If two deploys need to go together, but the order doesn’t necessarily matter, prefix each PR with “ [ with ]”. If someone else's code is waiting to deploy, give it a quick review. But you can assume it's good to deploy unless that person had marked the PR as \"do not merge\". You can always ask that person too. Engineers who have left deploy PRs open for an extended period of time, “blocking” staging, may be gently chivvied. 😛 Your code will be deployed to production when you merge the deploy PR! If your code requires creating new indexes or migrating user data or settings, make sure to do these things before deploying. After deploying You can tell when the deploy has finished by watching the build process in Jenkins. Then, keep an eye on #sentry in Slack for a bit, to see if any bugs pop up related to what you deployed. After deploying a bug fix, mark any relevant Sentry bugs as resolved. For client-side bugs, you can mark the Sentry bug as “resolved in next release” as soon as you know how to fix the bug—even before you deploy! Also close any associated Jira issues. The notifications will let Customer Success know to reach out to users to let them know that their issue was fixed! If you just deployed a high-value bug fix or feature, also post in the relevant Slack channel so the team can celebrate. :)", "date": "2020-12-03"},
{"website": "Mix-Max", "title": "How I successfully planned & ran hack days for the distributed Engineering team at Mixmax", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/how-i-successfully-planned-ran-hack-days-for-the-distributed-engineering-team-at-mixmax/", "abstract": "This is the first post of twelve for Mixmax Advent 2020 . “Let’s plan a hack day,” someone said in one of our regular team meetings. We’ve done them before, and they’ve been very successful: get everyone together for some marathon coding, fueled by carbs, caffeine, and adrenaline. We’ll work closely, quickly attacking each problem as it arises and finding the shortest path to a working prototype of something awesome. The problem? In 2020’s pandemic-stricken world, getting everyone together to code side by side is a recipe for likely illness (and possibly worse). This year, Mixmax has gone from being a hybrid company, partially colocated at an office in San Francisco and partially distributed, to being 100% distributed. While working as a distributed team is something we’ve had a lot of practice doing, we’re also used to periodic all-hands get-togethers, during which something like a hack day is relatively straightforward to organize and run. Doing it distributed When I volunteered to plan and run a hack day for the team, I knew I was taking on a challenge with some new unknowns. The “hack” part was straightforward: most people on the team have experience with hack days, and I knew what it would take to make that aspect work: Collect creative ideas related to a chosen theme Form teams around the most appealing ideas Put in a little pre-planning time to hit the ground running Do whatever’s necessary to construct a working prototype Show off what was created, and celebrate the inspiration, creativity, and learning The questions in my mind were related to making it successful without everyone in the same place. A big part of the reason hack days work is that they’re a break from the ordinary. Pulling ourselves out of our usual day-to-day work and tackling something inspiring and creative gives us license to try radical new things, be clever, take risks, and know that it’s all supported in the spirit of learning and growing. Making it unusual In order to make this feel unusual and allow that inspiration to bloom, I worked with our office manager to plan a few surprises for the team: Hack day fuel, personalized We sent everyone participating a preloaded gift card, intended for the purchase of whatever snacks and beverages would best give us that hack day energy. While it’s not the same as sending the VP of Engineering on a pizza run, having a stash of enjoyable food and drinks on hand would help keep spirits up. Hack day radio I organized a group of volunteers to DJ a “Mixmax hack day radio station” using Spotify via JQBX . People signed up for timeslots and chose a format for each one, meaning we had non-stop streaming tunes of many varieties, from early 20th century French jazz to hyperpop to Detroit techno to cumbia. This gave us a good energetic background soundtrack for our hacking, a chance to share our tastes and our musical knowledge, and something to appreciate and chat about when we weren’t deep in the code. A little extra inspiration We also sent everyone on the team a gift box with an assortment of small items inside, one of which was a classic LEGO set. During the event, I challenged each team to use their LEGO set to put together something that conveyed the Mixmax core values . Teams stepped up to the challenge, assembling photos and videos demonstrating their unique takes on the concept. The outcome When it was all over, we shared our work. We made amazing progress on (and built working prototypes of) four concepts: Understanding our product features and their usage via improved analytics and visualizations in a way that will help us focus on what our customers want and need most, and simplify our product to make it better. A key piece of a new, faster, more stable approach (using Go to talk to bee-queue ) to how our system handles processing queued jobs such as the hundreds of millions of Salesforce updates we receive on a daily basis. An amazing amount of progress toward modernizing the code that powers our internal administration system , to make it easier and faster to use and more delightful to work on. A system (written in Go and Rust , using Amazon EventBridge and Lambdas ) to automatically fetch timely data about potential problems within our system, to allow the on-call engineer to make better decisions more quickly and prevent those problems from impacting customers. We also came away feeling inspired, creative, and ready to tackle the next challenges that came our way in our day-to-day work. We’re not done with these hack day projects, either! Every engineer at Mixmax spends 10% of their work time on creative pursuits like these hack day projects that are beneficial to their growth and the improvement of the team overall, so many of these will get polished, released, and become part of our production stack or our internal tooling. Lessons learned The big takeaway from my perspective? Creativity begets creativity . Taking the time to make a hack day feel unusual and push people out of their everyday mindsets can pay off, even when the team can’t be together in person. Next time around, I’ll come up with even more inspiring hack day ideas. Maybe we’ll hang out in something like Rambly or take mini hack-breaks to play a round of Among Us . When the goal is inspiration and creativity, the possibilities are wide open! Do you have great ideas to make your next distributed hack day special? Share them and tag us on Twitter! Do you want to join an exciting, creative team of talented engineers working on software to eliminate busywork? Check out our careers page and come join us!", "date": "2020-12-01"},
{"website": "Mix-Max", "title": "Maintaining an end-to-end integration test suite", "author": ["Srinand Balaji"], "link": "https://www.mixmax.com/engineering/maintaining-an-end-to-end-integration-test-suite/", "abstract": "This is the fourth post of twelve for Mixmax Advent 2020 . About a year ago, we introduced a suite of end-to-end Puppeteer -based tests to our CI pipeline, in order to automate away the manual smoke testing that was required before changes could be deployed to production. In its infancy, this test suite was notoriously flaky, to the point where  engineers were resigned to restarting the test suite a couple of times a day to get the tests to pass on a second attempt (about 1 in every 7 test runs failed for one reason or another). In the past year, we've reduced the false negative rate of the test suite by a significant margin (roughly 1 in every 40 test runs fails now), and in the process, picked up a number of tips and tricks for debugging and maintaining Puppeteer-based tests. Recommendations Automatically retry tests Since you're automating a real(ish) browser, things like the browser being slow or the network having a hiccup can cause integration or end-to-end tests to fail; in practice, we often saw transient failures when one of the microservices that the test suite touched was being deployed to on our staging/testing environment. You can manually retry the test when this happens, but we found it more effective to retry tests upon failure automatically. If you're using the jest-circus test runner, this can be accomplished with a single line of code: jest.retryTimes(2); // Retry twice on failure for a total of 3 test runs. Use a consistent test set up across testing environments If you utilize a consistent test set up, like a Docker container, that can be used with minimal changes in both development and in your CI pipeline, you'll have a higher degree of confidence in your tests passing when they're added to the main test suite. You'll also be able to easily debug errors in your CI pipeline; simply run the tests inside a container from your localhost, but point it at your testing environment to see what's going on with your tests (you can also add console and debugger statements to the tests for further observability). Grab debugging information on failure It's often useful to use Puppeteer’s built-in screenshot functionality to see what’s happening in your application when a test is running: // Take a screenshot every second.\nsetInterval(async () => {\n  await page.screenshot({ path: `./${Date.now()}.png` });\n}, 1000); This works well during development, but often can’t be used for debugging problems specific to the CI pipeline, as it requires access to the machine's filesystem. In that scenario, you can instead try: // Take a low-quality JPEG and dump it to the log in base64 format\nconst data = await this.page.screenshot({ type: 'jpeg', encoding: 'base64', quality: 1 \t});\nconsole.log(`data:image/jpeg;base64,${data}`); when the test fails; the resulting image can be viewed by placing it in the src attribute on any img element, like so: <div>\n  <img src=`data:image/jpeg;base64,${data}`/>\n</div> Try not to make too many assumptions This is less of a concrete item, and more of a general piece of advice for end-to-end/integration test writing, but one that I've found invaluable for ensuring the integrity of your tests and for avoiding flaky tests in the first place; by \"assumptions\", I mean assumptions about the initial and end states of your application before and after your test runs. Say, for example, that you want to write a test for your to-do application that creates a new to-do item - you might set up the test to click a \"create\" button, give the to-do item the title \"Pay bills\", hit a \"save\" button, and ensure that the to-do item titled \"Pay bills\" renders in your list of to-do items. Then, after your test is complete, you'd likely clean up your testing environment by removing the newly created to-do item. But what if the clean up operation fails? Now you have a pre-existing to-do item in your testing environment with the title \"Pay bills\", and at least one portion of your test suite is (erroneously) guaranteed to succeed. Instead, a better practice here is to make very few assumptions about your testing environment, and attempt to isolate your test as much as possible. For example: one of our test suites sends an email to and from the same test user using the Mixmax Chrome extension, and waits for the email to arrive in the test user's inbox. To ensure that the email we're waiting for is the same one we've sent, each test run generates a uuid, and uses that uuid as the email's subject, which allows us to confidently say that our application has sent an email successfully.", "date": "2020-12-04"},
{"website": "Mix-Max", "title": "Saving money, time, and space with MongoDB indexes", "author": ["Shil Sinha"], "link": "https://www.mixmax.com/engineering/saving-money-time-and-space-with-mongodb-indexes/", "abstract": "This is the second post of twelve for Mixmax Advent 2020 . As you may have picked up from previous blog posts , MongoDB is our primary data store and source of truth at Mixmax. At its peak, our largest cluster had 8 shards and stored 6.9TB of uncompressed document data, and over 2.7TB of indexes. As you can imagine, running this cluster wasn’t cheap. To make matters worse, DB writes took over 200ms on average. We noticed that all of our largest indexes were compound indexes that started with a userIds field, followed by an organizationId field. As the name suggests, userIds was an array of user IDs used to denote the set of users who had access to a document, and organizationId was the organization that the document belonged to. An implicit relationship was that all users referenced in the userIds field were members of the organization referenced by organizationId. As Mixmax has grown, so have our customers. What were once 10-20 user organizations were now 100-200. This meant that for a given index, a single document could be responsible for hundreds of index keys. Coupled with the fact that organizations with more users tend to have proportionately more documents overall, it was obvious that this architecture couldn’t scale at a reasonable cost. Looking closer at the data, we saw that in more than 99% of all documents, the userIds array contained more than 90% of the organization’s users. We realized then that we weren’t getting much value out of the leading userIds field; it was marginally more selective than the organizationId field at best, and equally selective at worst. The solution Our path forward was clear, if a bit tedious: we’d have to replace all 16 of our indexes that lead with the userIds field. To keep our total index size as small as possible throughout the process, we went through the indexes in ascending order of size, taking the following steps: Create a new index containing all but the leading userIds field of the existing one. Ensure the new index was in memory by running a few queries explicitly instructed to use it, and confirming by checking wired tiger index stats. Drop the old index. We knew that the leading userIds field was responsible for a lot of our total index size, but weren’t sure how much. We were floored by the results: removing the userIds field resulted in indexes anywhere from 80 to 95 percent smaller (the percentage decrease increased linearly with the size of the original index.) Takeaways The most selective index isn’t always the most optimal If a field in a compound index isn’t used for sorting, and doesn’t significantly reduce the number of documents scanned in production queries, it might be taking up more RAM than it’s worth. Especially if it’s an array field. Chores can be impactful and fun Serially waiting for multi-day index builds to finish wasn’t all that exciting, but seeing folks’ reactions to the resulting changes in index size and costs sure was. It’s worth periodically revisiting your index design As your product evolves, so will your database usage. It’s great if the design decisions you made years ago are still the right call, and it’s worth finding out. What you don’t know could be costing you thousands of dollars a month.", "date": "2020-12-02"},
{"website": "Mix-Max", "title": "Share meeting recordings automatically with Mixmax Rules", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/share-meeting-recordings-automatically-with-mixmax-rules/", "abstract": "This is the fifth post of twelve for Mixmax Advent 2020 . Effective asynchronous communication is one of the most important parts of working in a distributed team. If decisions are made or information is surfaced in a conversation and that conversation isn’t recorded and shared with others in some way, the knowledge is siloed at best and can be lost at worst. Mixmax has always been a distributed company, and with the COVID-19 pandemic still making gatherings unsafe in the U.S., we’ve gone fully remote. This means it’s extra important to make sure our communication methods work for everyone on the team regardless of location, schedule, and time zone, and that we share knowledge effectively. To that end, we’ve taken to recording our meetings that happen via Google Meet. This is easy: at the start of a meeting, we ask for everyone’s consent before starting the recording, and after the meeting is done, Google emails the recording (as a Drive link) to the organizer and the person who started the recording. Thing is, we want these recordings to be shared and visible to people who weren’t there. We could open up the file and relocate it to a central shared folder, or copy the link and send it to specific people or groups by hand. That’s clunky, though. No-one wants to have to remember to do that and then go through all those steps after each meeting. That’s a recipe for frustration and lost information. Getting rid of annoying busywork is where Mixmax shines. On my team, we set up a Mixmax Rule to handle this for us: When I receive an email from meet-recordings-noreply@google.com with a subject line containing “Daily Standup”, post the email to my team’s Slack channel. Now our daily standup recordings are automatically shared with the whole team, so when one or two of us can’t make it for whatever reason, we can all still learn and benefit from what was said. It’s a win for the team, and it makes our work more efficient and pleasant. Want to be part of a team that values good communication and strives to make busywork a thing of the past? Come join us !", "date": "2020-12-05"},
{"website": "Mix-Max", "title": "Practical Metrics with Graphite and Terraform (Part 2)", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/practical-metrics-two/", "abstract": "Over the past few months at Mixmax, we’ve been changing the way we track internal metrics. In this series of blog posts, we’re outlining our journey to metrics bliss. In part 1 we explained why we chose to switch from CloudWatch to Graphite. This post is part 2, where we’ll look at the architecture of our Graphite cluster. Part 3 will dive into how we deploy and maintain that cluster with Terraform. Outlining Requirements After deciding that Graphite was our preferred option for storing and retrieving metrics, the next step was designing a production system that not only accomplished our primary goals, but also optimized for our secondary goals as well. From part 1 , our primary goals required that the Graphite deployment:\n1. Support a high (and increasing) volume of requests without client-side downsampling.\n2. Allow aggregating metrics on multiple dimensions.\n3. Allow alerting on time series metrics. Our secondary goals were that the deployment:\n1. Require minimal maintenance.\n2. Easily scale with our existing tooling.\n3. Have a useable node.js client.\n4. Store data in a fault-tolerant manner.\n5. Support incremental rollout. Graphite handles primary goals 2 and 3 without any configuration, and secondary goals 3 and 5 aren’t dictated by architecture decisions. So when designing our architecture, we needed to ensure that it would: Easily scale to support increasing request volume (to tackle primary goal 1). Minimize maintenance overhead. Be compatible with our existing deployment tools (we terraform all the things ). Have permanent storage and let us access historical data. Arriving at our Architecture With our goals in place, we could tackle the details of designing our architecture. Starting from scratch, there are usually 3 pieces that required to store and visualize Graphite metrics: an aggregator to collect and format metrics from many sources, Graphite for storage, and a visualization layer to retrieve and display Graphite data. We chose to use statsd as an aggregator and Grafana for visualization. Both of those tools are well-used, flexible, and commonly integrated with Graphite. They’re so commonly used, that there are more than a few open source options where they’re deployable together as a package. Sweet! We’re done, right? Unfortunately, these out-of-the-box solutions, while giving a great sandbox for testing, don’t give the flexibility we needed to effectively scale. To illustrate this, here’s an example of one statsd / Graphite / Grafana instance: In this example, we send metrics to statsd on one port, which aggregates and stores that data via Graphite, and the Mixmax engineer visualizing the data visits Grafana and loads it from the same instance. While this implementation maximizes simplicity, it lacks scalability. If the single instance storing metrics hits limitations that require scaling, the only option is to increase the size of the instance. Why is that the case? If we take the previous example and add a completely new instance, we end up with 2 separate deployments that have no way of communicating to each other. While that may reduce load on one instance, we’d then have to manually decide where to send individual metrics and view those metrics in separate Grafanas. To start solving this problem, we needed to separate each component into its own instance, allowing us to scale them independently. With independent instances for each component, if a component were to run into limitations, we could scale it horizontally. However, setting up horizontally scaling (sometimes called “clustered”) Graphite was not as simple as it seemed. With traditional Graphite, each instance must be configured with the addresses of all other instances and be connected by a common “relay.” Each instance is a standalone Graphite instance, the “relay” handles distributing metrics between them, and each instance communicates with the others to fetch all the relevant data for a query (there are other posts about clustering graphite ). Generally, clustered Graphite (with statsd and grafana) with 3 Graphite instances would look something like the following: In the diagram, arrows represent configured connections between separate instances, and dotted lines show instances that may not exist as we scale up or down. Immediately, a few problems arise with this architecture: Each new Graphite instance requires us to modify configuration on all other Graphite instances and the relay. Horizontal scaling requires restarting all Graphite servers to read the new configuration. The relay is still a bottleneck for all incoming data and must be scaled independently. Our design Together, these issues, along with performance limitations of traditional Graphite itself , meant that traditional Graphite would not be easy for us to scale, failing our first primary goal. This led us to look for other options that simplified the architecture and scaling overhead. The go-graphite project offered a higher-performance and more easily scalable alternative. Rather than requiring a relay and connections between each go-graphite instance, we could deploy an api server configured with active go-graphite instances to combine stored metrics. Along with DNS load balancing to distribute incoming metrics, this would allow us to scale horizontally. The go-graphite architecture looks like this: With the above architecture, incoming metrics are aggregated in statsd and sent through a single DNS record, which then forwards the request to one of the go-graphite instances. When a Mixmax engineer loads Grafana, the api combines the requested metrics from all the go-graphite instances and returns the combined response. When we need to scale the cluster, we could add a new instance and only have to update the DNS record and the api configuration allowing zero-downtime scaling. So far, this architecture has scaled to process over 250 million events per day with no signs of stopping! Stay tuned for part 3, where we’ll go over how to deploy a Graphite cluster with terraform. Like designing and deploying scalable systems? Join us!", "date": "2019-05-14"},
{"website": "Mix-Max", "title": "Mixmax's migration from Elastic Beanstalk to Fargate using Terraform", "author": ["Ryan Tasson"], "link": "https://www.mixmax.com/engineering/migrating-from-elastic-beanstalk-to-fargate-using-terraform/", "abstract": "Thanks for tuning in to Mixmax Advent 2020 ! This is the 8th post in the series; click the link to check out the prior posts and check back for more :) In this post, I’d like to detail our journey migrating from Elastic Beanstalk (EB) to Fargate , using Terraform to codify our infrastructure along the way. The intended audience for this blog post are interested engineers, with passing familiarity with Amazon Web Services . We gained a lot of flexibility and insight into our systems, were able to recover reasonably quickly from a recent AWS outage, and saved some cash along the way. I hope you enjoy the ride; I know I have! Introduction: where we came from In the backend, Mixmax started ~5 years ago as a Meteor app. Meteor is a hosted platform-as-a-service (PaaS) that took care of the nitty gritty details of hosting our backend Javascript services, and as a nimble and green startup this solution worked well for Mixmax for some time. However, our requirements for our infrastructure eventually outgrew the technical capabilities of Meteor, and Mixmax examined other solutions that did not require much engineer intervention to quickly host and ship new code. Enter Elastic Beanstalk. Amazon’s PaaS offering was well suited for the Mixmax of the time, allowing us to quickly deploy code without hassle while supporting deeper customization of environments and allowing us to “own” as much of our infrastructure as we wanted. We could SSH in if we wanted, test tweaks on EC2 instances directly, and pre-provision capacity. We designed custom CI pipelines for Mixmax-specific business logic, and Elastic Beanstalk was yielding enough that what few quirks it did have were overlooked or worked around for years. Eventually though, it became clear to our engineers that we had pushed the limits of Elastic Beanstalk and would need to imagine another solution. Deploying new code or rolling back took a very long time at our larger scale (sometimes an hour or more), and both processes were flaky with such large instance pools. Elastic Beanstalk did not scale responsively enough to sudden shifts in traffic, and only supported scaling off one metric at a time; eventually, we ended up pinning minimum instance counts to the same value as the maximum - we weren’t scaling at all and were instead spending lots of money to “paper over” capacity issues. These problems alone caused multiple outages and many more headaches. That was not the only source of issues though. Our infrastructure-as-code tool of choice, Terraform , had poor support for EB when we first migrated, and thus none of these services were implemented in code - they were created by hand. Given how easy it was to accidentally create application drift, consistency was introduced between apps by using one very permissive IAM policy & role, and one very permissive security group attached to everything - a decent approach to consistency, but not decent for security. Finally, service responsiveness & health was largely monitored via load balancers to permit automatic termination of unhealthy instances; this makes sense for web and API services, but was pretty silly for our bee-queue queue workers. I joined Mixmax in November of 2019 as Mixmax’s first engineer dedicated to infrastructure development & devops, and quickly realized I had my work cut out for me. Not everything was bad; in fact, most things were great. Mixmax had successfully containerized all of its applications, and the constant assessment of how we can improve fostered a mature engineering culture of tooling around development, leaning heavily on CI to eliminate toil and error-prone engineering procedures. One thing was clear to everybody though: Elastic Beanstalk had to go. Enter Fargate (and Terraform) Okay, I have to admit, I was more keen on getting things into Terraform than Fargate. Fargate is a fantastic platform - it’s very flexible, a first party service within Amazon with good integrations, and it’s easy to set up. There was also great potential to scale proactively on incoming load too, which promised to save us lots of money and reduce bottlenecks and pain points. However, as a new Mixmax engineer, I had no idea what was actually running - what was talking to what, what permissions they should have, and what a correct configuration looked like. It was more important in my mind to bring this infrastructure into code, and give changes to it the benefit of git blame and contextualized with business requirements in Jira tickets. Fargate was a means to this end; nobody liked Elastic Beanstalk (I certainly didn’t), and it was simple enough to hit the ground running. Terraform modules But first, we needed appropriate abstractions in Terraform. We’d need at least two Terraform modules , one for workers and one for web services. These took a little while to create; it meant looking at what the common Mixmax use case in Elastic Beanstalk was, and translating that to a different set of primitives in Fargate (and Application Load Balancers, and security groups, and IAM roles, and etc.) The first revisions of these modules were designed so that an engineer did not need to know many nitty gritty details about networking, security groups, or IAM permissions; they would “just work”. This turned out to be a nasty anti-pattern; covering every edge case for Mixmax usage in Terraform created really painful and obtuse code, and those edge cases seldom worked well. Eventually, we landed on modules that dependency-injected the important details - subnets, inbound ports, SSL cert ARNs, etc. This was a learning curve for some engineers who hadn’t had to deal with these details before, but it birthed our Fargate cookbook, a first class resource on how to accomplish common tasks with this new infrastructure. It also made testing much easier and cleaner - we had assurances these modules were going to do what they set out to do. Open sourcing these modules is a current focus of mine; keep an eye on this blog for future announcements ;) CI Pipeline improvements Next, we needed to deploy our software onto these new sets of infrastructure. I had the benefit of our applications already being containerized in Docker and published to our Elastic Container Registry ; it was simply a matter of shipping ECS Task Definitions and updating the ECS Fargate Services to use them. Our CI pipeline had to tolerate deploying to both Elastic Beanstalk and Fargate services to facilitate easy testing, cut over, and cut back. Since task definitions contained environment specific details like environment variables, secrets, and file descriptor limits, we opted to consider them deployment artifacts. A task definition for each environment would live within application repositories and would be deployed whenever a release happens, with some mild templating to point to the correct latest container image. I’ll jump ahead a bit. When everything was ported  to Fargate and we no longer needed to support Elastic Beanstalk, our CI pipeline was dramatically simplified; many helper shell scripts were made obsolete, and hacks to support EB were removed. This reduced complexity in our pipeline has reduced the number of deployment oddities substantially. Additionally, deploying to Fargate alone is much faster than deploying to Elastic Beanstalk; getting that important bug fix in front of users, or reverting it, now takes on the order of 5-10 minutes instead of 10-60 minutes. Observability: what the heck is it all doing?? Another substantial problem with adopting Fargate over Elastic Beanstalk was being able to observe our software while it was running to troubleshoot bugs and performance issues. In the Before Times, engineers were comfortable logging into Elastic Beanstalk instances via SSH and directly inspecting running software with things like strace and flamegraphs . They might view application logs directly on the instance, or fetch them from the Elastic Beanstalk console. There also existed a Sentry instance that collected errors from instances, and Grafana for viewing custom statistics in a sophisticated fashion. Sentry continues to serve us well, as does Grafana. But flamegraphs, strace, application/web logs and debugging performance would need to look much different in this world. Application and API query logs would need to get shipped somewhere. More saliently, one cannot log onto Fargate instances; there is no SSH daemon running on our Alpine Docker images . We would need ways to infer what a running task is actually doing. We set up basic Cloudwatch monitors and alerting in our Terraform modules, but we’d need to get sophisticated to match the level of introspection we had on Elastic Beanstalk. Application Performance Monitoring I had used NewRelic in a previous life and found it immensely useful. However, NewRelic is somewhat expensive, and being a new engineer in a small (but growing) SaaS company meant I was not in a position to sell my new employers on this expensive product. I’d already made a large case to move to Fargate, and it was important to prove this out first before other large investments. Fortunately, Elastic , the folks behind Elasticsearch, have created a lovely product called Elastic APM , accomplishing what NewRelic does but also supporting a free license for self-provisioned hardware. I lost a week trying to make this function on AWS’s hosted Elasticsearch, but as it turns out Elastic has not licensed APM as free as in freedom (or open source), but free as in gratis - we had to run it ourselves. Still, this observability was very important for us if we wanted to succeed in porting everything to Fargate and understand what it was all doing. Almost immediately Elastic APM began paying dividends. We found literal ` sleep ` s in our application code, one line fixes that doubled performance on heavy endpoints, and redesigned many endpoints and a lot of logic to behave in a smarter fashion. We also added custom transactions to our bee-queue wrapper to make our job queues more performant. While the Elasticsearch instances necessary to host APM cost a decent amount of money, they saved us more than that in performance improvements alone - and additionally helped us fix bugs and streamline the flow of data through our microservices. This continues to be a resounding success. API & Application Logging With Elastic Beanstalk, we were logging all API requests via an intermediate Nginx server, whose logs were eventually shipped to Elasticsearch. However, in Fargate, we had no intermediate Nginx server. We set up our Terraform module to configure ALB logs and shipped those to S3. This worked for a while, but was a bit clunky; we could query them with Athena , but Athena was somewhat expensive to search against our busiest services and wasn’t terribly intuitive. We eventually set up these S3 buckets to be available to our data warehouse; now they’re available for querying with Redash . Shoutout to the Mixmax Data Team! Additionally, we were fetching application logs in raw text format from Elastic Beanstalk instances. This works, but isn’t terribly sophisticated - if you wanted to fetch every instance’s logs and compare them, you would have to fetch raw text files and run your own analysis on localhost. In contrast, Fargate seamlessly ships ` stdout ` to Cloudwatch Logs with very little configuration. Cloudwatch Logs also has nice integrations with Elasticsearch. With this setup, we could query to our hearts content. Flamegraphs Lastly, one capability we had on Elastic Beanstalk was the ability to generate flamegraphs . Where Elastic APM shows us what one particular long-running API call is doing, flamegraphs show us what a single node.js process is doing - garbage collection, handling IO, etc. In Elastic Beanstalk, we would collect performance data with perf , copy it to localhost, and generate the flamegraph. Since we could not use perf easily on Fargate, I opted to create a single EC2 ECS instance that runs Netflix’s nifty Flamescope . Engineers can deploy a task on this cluster-of-one, take their performance measurements, and display them in a web interface so others can pry into the details with them. This is not a capability we use often, so this setup happened pretty late in our migration to Fargate. However, it has helped us illuminate one of the last dark corners in our infrastructure. Investigating existing services Alright, so we now had all the tools necessary to dig into what a service was doing. Now we actually needed to port things. To do so, we had to get a good idea of what was deployed already - what it was doing, how it behaved, what were expected errors and what errors might be new. When examining existing services, it became clear that there was a lot of wisdom baked into their configurations. The scaling policies, minimum instance counts, and instance counts were not random; they were the result of methodical tweaking to produce a performant Mixmax. There were also many antipatterns as a result of Elastic Beanstalk; minimum instance counts were often much higher than they needed to be because we couldn’t scale responsively, costing us significant amounts of money. First things first, we had to instrument each service with Elastic APM before even spinning up a Fargate environment. This helped us create a performance baseline to measure each setup against the other. Using this helped determine early in the porting process what an individual service’s performance bottlenecks were; sometimes it was CPU, but in our microservice architecture it was more often IO and external requests to other services. We took note of these bottlenecks and how to measure them so we could create custom autoscaling policies for each. We also took note of what dependencies existed to AWS or other services, and created least privilege security groups and IAM policies that gave the necessary permissions. Cutting over to Fargate Then we had to actually launch instances of our services in Fargate and test them out. Up until this point, I had been off on my own developing abstractions for Mixmax engineers to use; however, once we were at this point I was no longer the expert. Our software developers knew what the software was doing better than I did; they wrote it, after all. This part of the process was intensely collaborative, and everybody learned from each other along the way. Big shoutout to every engineer at Mixmax; literally everyone ported a service and chipped in to this process. However, the process itself was somewhat vanilla. We started with deploying to our staging infrastructure. We ported API and web traffic via weighted Route53 records, and ported “traffic” to our workers by capping the number of instances workers could spawn at once. For example, if there were 5 Elastic Beanstalk workers, we would launch 1 Fargate worker to consume 1/6th of the total worker queue. However, it became clear early in the migration that small amounts of staging traffic would never produce enough feedback for us to feel comfortable porting to production. We began moving 50% or 100% of traffic in staging to Fargate immediately to get good feedback, while keeping the rest of the engineering team informed so they could watch for problems. Every step along the way, Sentry, Grafana and Elastic APM were monitored to ensure the new infrastructure was not producing unexpected errors. Mixmax is an expansive set of software; the feedback we got from staging was okay, but not sufficient to exercise every path a bit of data might take through our system. Mixmax is comfortable with experimentation; as engineers, we have reasonable leeway to try out cool new things so long as we can also reasonably back out of any decisions we make. We gradually biased towards getting early feedback from production too - we would port 0.5% of web traffic to an API, or 1 worker out of 40, and watch Sentry and Elastic APM closely. Whenever there was an issue, major or minor, we immediately cut all traffic back to the existing infrastructure and investigated. For some services, this created a drawn-out migration - but what was most important was the protection of our users data and experience, and we seldom disappointed them in the middle of a migration. After many trials and tribulations, the entire process was done; we were complete. The empty Elastic Beanstalk console was cause for celebration. Disaster Strikes (aka the Oregon Trail) Recently, Amazon Web Services suffered a major outage in its us-east-1 Virginia region , where Mixmax is currently deployed in. This affected many services; even some doorbells and vacuums fell victim to this outage . This greatly affected us as well, yet we were able to recover hours before Amazon itself was stable thanks to the efforts to port our infrastructure to Fargate and Terraform. This was an interesting validation of our efforts here, and I’d like to take a moment to discuss them. At 8:15 AM Eastern Standard Time on November 25th, 2020, an on-call engineer was woken up with an alert that our analytics events were delayed. Our analytics subsystem is based on Kinesis, an AWS service that presents sharded streams of data for consumption by AWS Lambda. In our system, these streams were processed by Lambda and stored in our database for consumption by our web frontend. Due to the integral nature of Kinesis in processing our analytics data, it soon became clear that Kinesis itself was having issues; multiple services that pushed data into Kinesis or read off it were producing tons of unanticipated errors. This caused some cascading failures in downstream systems, notably AWS Cloudwatch . AWS Cloudwatch manages scaling for our Fargate infrastructure, and without scaling our services stuttered and then halted under the pressure of the day’s normal traffic - lots of Mixmax customers eager to complete their work before the Thanksgiving holiday in the United States. After some time, it became clear that the issue was present in us-east-1, and not present in other regions . We quickly began efforts to port our infrastructure to us-west-2, the Oregon region in AWS . We quickly spun up replicas of our infrastructure in us-west-2 with copies of the Terraform we used to originally deploy our infrastructure into us-east-1. Using this code, we spun up a network that would have taken an hour or more within 10 minutes, and the Fargate service and Lambdas that consume from Kinesis within the hour. We additionally began adding EC2 capacity to our ECS cluster in us-east-1, and managed some of the scaling ourselves with services that were impacted from Cloudwatch failures. 3 hours before AWS recovered, at 1:13 PM Eastern Standard Time, we brought our analytics subsystem online once again, and restored a working Mixmax application to our customers. At Mixmax, we believe an outage is a terrible thing to waste. This one was particularly painful, and if we did not learn lessons from it we are doomed to repeat the experience. This has spurred us to invest heavily in a cold/warm second region in Oregon with the cost and efficiency improvements we gained with Fargate. About half this infrastructure was configured on the day of the outage, though we need to formalize more of it and make our failover more seamless and resilient. We dub this current effort the Oregon Trail after a video game many of us played in our youth , though we expect our current efforts to be significantly less painful than the game. Conclusion Wow, you’ve read all of it. Maybe. Sorry, this was longer than I intended. I hope it was enjoyable. Mixmax now enjoys a repeatable and dependable infrastructure, one that is malleable and observable. This process of redeploying everything was long, but we feel that it has levelled Mixmax up, and we exercised this new flexibility to recover relatively quickly during a recent AWS outage. Using this new malleable infrastructure, we are investing in high availability improvements and tooling to run safe experiments with our infrastructure (look out for a future blog post!) I am personally quite proud of the Terraform modules that have generated this infrastructure for us. We are working on open sourcing these modules, so watch this blog for future updates! If working with our fancy new infrastructure and open source software sounds interesting to you, we are always interested in hearing from engineers (and others!) Check out our careers page ! And thanks for reading :) The header image \"Containere på terminalen i Orkanger havn\" by Trondheim Havn is licensed under CC BY-SA 2.0 .", "date": "2020-12-08"},
{"website": "Mix-Max", "title": "How We Hacked* Our Manager", "author": ["Siobhan Dolan"], "link": "https://www.mixmax.com/engineering/how-we-hacked-our-manager/", "abstract": "This is the sixth post of twelve for Mixmax Advent 2020 . At Mixmax, security is critical.  Our users trust us to handle their email and other data securely. We do everything we can to merit our customers’ trust . This means that security is the concern of every engineer, every day. We also continually run security exercises throughout the year. One recent exercise focused on phishing. Phishing involves an attacker tricking a victim into sharing sensitive information, such as their password or other personal information. A traditional attack involves the “spray and pray” method where a malicious message is sent to a large number of potential victims, without any specific targeting, and hoping that someone will fall for the trap.  A targeted version of this, spear-phishing, leverages social engineering to increase the likely success of the attack. With this method, the attacker focuses on a chosen individual or organization, and attempts to impersonate a person or entity that the potential victim trusts. The victim is then more likely to provide sensitive information such as a credit card number, password, or social security number. In some cases, the attacker may be looking to gain access to the email account itself. The damage to victims can be immense both financially and reputationally. For our exercise, we wanted to better understand how a bad actor would go about conducting a spear-phishing attack by running our own mock attack.  First, we picked our mark: our manager. Next, we prepared for our attack by gathering open source intelligence (OSINT) on them.  While we had a bit of an artificial advantage, since we were hacking someone we know well, there’s an abundance of data available via social and professional networks, online public records, census data and more. Furthermore, there are various OSINT tools and frameworks that make gathering this information a trivial step. Our next step was crafting our email bait. Again, this was made easy by existing open source tools. We used GoPhish , which took minutes to set up,  to create a professional looking email. We leveraged the OSINT data we collected on LinkedIn to credibly spoof a person we felt they’d be likely to respond to: their manager. Critically, a spear-phishing attempt will appear to come from someone the victim knows in a position of power, and ask them to do something urgently.  In this case, we had their manager ask them to check on a system, and helpfully included a link purportedly to a monitoring tool. Had we been malicious actors, the link would be the conduit for delivering malware, or a spoofed site asking for credentials we could then record. Unfortunately for us, we chose a mark who was over-prepared for the attack.  Our link wasn’t clicked, and our trap failed. While we weren’t successful, the overall ease and lack of complexity in setting up our attack was a good validation of why we focus so intently on security. And the good news is that you can also use these tools to test your teams’ vulnerabilities to such attacks. By regularly conducting mock phishing attacks, you can help prevent your organization from falling victim to the real thing. To get started, take this quiz to figure out if you’d outwit a phishing attack today. * Well, we tried", "date": "2020-12-06"},
{"website": "Mix-Max", "title": "The Future of Services at Mixmax", "author": ["Garret Meier"], "link": "https://www.mixmax.com/engineering/the-future-of-services-at-mixmax/", "abstract": "This is the seventh post of twelve for Mixmax Advent 2020 . Where we are As we've written about before , we develop and deploy Mixmax in a number of microservices that serve different areas of our product and have fairly separate jobs. At least that was our original goal back when we first moved to a service-oriented architecture. However, over the intervening 4 years, our services have grown both in number and in actual size. Instead of microservices, we've ended up with their larger, more unwieldy friends, macroservices. While continually adding functionality to our existing services often allowed our small team of engineers ( we're hiring ) to develop new features quickly, it also gave many of our services an intertwined set of dependencies - both in code and infrastructure - and obfuscated performance bottlenecks. Where we're going To untangle our infrastructure and improve observability, we're launching a new initiative as an engineering team to right-size our services. Notably, we aren't deploying a blanket microservice policy, but rather separating out our services into more manageable chunks while improving the stability of each service. We've chosen this approach because, as a team we'd rather move intentionally and ensure all the work we put into our services improves both our efficacy as an engineering team and the experience of our customers while using Mixmax. It will also ensure we have the greatest overall impact. At the end of this initiative, we'll certainly have more, smaller services, but rather than aim for more services, we'd like to aim for a healthier system. How we're getting there This begs the question of what makes a healthy system. Oftentimes, service and system health are measured in terms of service-level metrics like memory usage, latency, and CPU usage. However, we've found that those metrics only capture a slice of what makes a service truly healthy. Instead, services should be evaluated on a set of standards including performance characteristics, but also extending to operational and maintenance concerns that make the service easier to understand and improve. We've decided that a healthy service should: make testing easy simplify  operation streamline observation With these three goals as our guidance, each goal is broken down into a set of standards a service can meet or exceed. For example, the testing goal's unit test standard is met when 50% of the service is covered by unit tests. Similarly the goal of streamlining observation has a standard that's met when every related piece of infrastructure is managed through terraform . All told, we have 17 standards spread across the 3 goals listed above, and a healthy service should meet those standards. By ordering these standards according to priority and evaluating whether services meet them, we're able to focus our right-sizing efforts on the services that miss the mark the most and projects that can improve all of our services at once. As a recent example, while evaluating our system's health, we found that our ability to search through services' code didn't meet our needs as a team. After discovering that, we invested in new code search tooling that improved observability and maintainability of our codebase as a whole. Not only do these standards help guide our right-sizing efforts now, but they can also be routinely (even automatically) evaluated to create an ongoing history of system health. The end result Because we've defined these standards, we're able to more clearly focus our efforts on projects that materially improve the health of our system as opposed to a dogmatic commitment to the idea of microservices. In the end, this will lead to smaller, more testable, operational and observable services. With individual, improved services, we'll have a more understandable and healthy system overall.", "date": "2020-12-07"},
{"website": "Mix-Max", "title": "You're going to want to follow this star (schema) for your Data Analytics", "author": ["Kristin Hedgecock and Pablo Seibelt"], "link": "https://www.mixmax.com/engineering/youre-going-to-want-to-follow-this-star-schema-for-your-data-analytics/", "abstract": "This is the tenth post of twelve for Mixmax Advent 2020 . What is a Star Schema? Have you ever had to build a schema for a Data Warehouse and had no idea where to start? Maybe you've tried downloading the data from its source systems and put them into different tables per each entity. That's a good start, but then you have to make sure the data can be easy to use for end users. One way to do it is to create easier to use tables each time a new requirement comes, known as \"report-driven development\". This method is quite problematic, for many reasons, including: Frequent updates are needed for the data to work correctly (e.g. cleanups, relationships, business logic, etc.) and have to be replicated each time you reuse the data. Plus any changes to those definitions will have to be applied to all reports using the same data if you want to avoid conflicts between reports You'll rework the same data many times because you'll have to reapply the cleanups, business logic, etc which applies to the source data each time. Your power users will need to learn each report's data separately, instead of learning some set of reusable tables they can join and create new reports It might be difficult to come up with a model that allows you to make it easy for the users to use the report while at the same time being able to explore the data Instead, we chose to go with a Kimball-style Star Schema model, with some alterations. If you are unfamiliar with Ralph Kimball, he and his team are legends in the Data space, they wrote some of the best books on Data Warehousing and Business Intelligence (Which basically used to be the cool names for Data Engineering and Analysis 😉 ). Essentially, with a star schema you create fact and dimension tables, the facts describing business processes (Customer ordered X - Item moved to Y - Feature Z used by customer - etc), and the dimensions describing the entities that perform those processes (Customers, Items, Stores, etc). One of the requirements for something to be modeled as a dimension is that it changes slowly, Kimball calls them \"Slowly Changing Dimensions\" (SCD). e.g. Something that changes every 5 minutes would not normally be modeled as a dimension, or be included as a dimension attribute. Example Star Schema In this example, we have a table called fct_orders (Fact table of Orders), where each row (the grain) is an order made by a user, in which we have ids for the user who made the order, the item which was ordered and from which store. Then there are dimensions for each of these objects, this way each of these ids can be joined to obtain the attributes. This has a couple of advantages: Any adjustments made to the dimensions to improve accuracy will affect all reports that use these dimensions It will be easier to develop new fact tables without having to think about all the attributes you may want to relate to this data, just add the relevant ids to your fact table and voilà! - Down the line this will save countless hours in new projects You’ll only need to document fields once in the place where you created them initially, without having to re-document them for every new report you create It’s easier to create scalable loading processes for star schemas than other kinds of modeling Avoiding Normalization and Snowflaking An important consideration is that this is very different from a normalized schema, where you try to avoid data duplication and is optimized for production usage. This type of schema is optimized for reporting instead, and is in fact “denormalized”, in our example above, suppose a user can have an associated company, and more than one user can be from the same company ( Similar to how a workspace in Mixmax can allow many users to collaborate together ). In that case, instead of linking the users dimension with a new company dimension, we may add the “company_id” to “fct_orders”, and/or, directly add the relevant company attributes into the users dimension. Which solution to use depends a lot on your particular scenario. Either of these solutions avoids “Snowflaking”, which would mean that you are not using a Star schema, but a Snowflake schema. Alterations on Kimball’s advice All this said, some of the advice from Kimball is based on performance and tooling limitations of their time, we have some notable differences: We only do surrogate keys if we really need them, and we do it by creating a hash id as recommended in the DBT documentation. A surrogate key is an artificial key you create in the DWH instead of using the natural keys from the data source. We are a little bit less strict on the inclusion of measurements in dimensions, we allow some of them to be included if they are helpful We try to keep \"Snowflaking\" at a minimum (Joining two dimensions to each other) but we are not as strict as Kimball here, sometimes we allow it at least temporarily if we need to get something done quick Related to this, we create the star schema using DBT, a modern open-source ETL tool which allows us to run SQL queries in an organized way for creating our data model in our Snowflake Data Warehouse. We run everything with machines on AWS Fargate, and use Airflow to coordinate dbt and non-dbt tasks together harmoniously. Opinion of this model from our Data Analyst Facts, dimensions and measures oh my! As a data analyst, working with a star schema has been life changing. In roles at other companies I worked with data warehouses that lacked a true schema. It was difficult to find fields, identify joins and know which of the 10 fields named the same was the right one. Then walks in Mixmax’s organized data warehouse that allows for easy data modeling (we use dbt!) with simple joins to get the data necessary to move forward. It has made onboarding onto a new Business Intelligence (BI) tool a breeze. Earlier this year we purchased Looker as our BI tool, within just a few weeks we were able to accurately report on revenue reporting in Looker. Much of this is due to ease of the data model and the ability to join multiple data sources with simple keys. We were able to easily expose facts tables with the needed dimension tables to get our customers what they needed quickly. A solid data warehouse structure will not only benefit Data Engineers, but will also allow Data Analysts to easily iterate, make changes and create new data assets.  In addition, your stakeholders will enjoy the  ease of use, short time to answer questions, and the ability to deliver value quicker.", "date": "2020-12-10"},
{"website": "Mix-Max", "title": "A Human-Centric Approach to Retrospectives", "author": ["Ryan Freebern"], "link": "https://www.mixmax.com/engineering/a-human-centric-approach-to-retrospectives/", "abstract": "This is the ninth post of twelve for Mixmax Advent 2020 . Until recently, my engineering team at Mixmax conducted periodic retrospectives the way most teams do: we'd discuss What Went Well , What Could Have Gone Better , and How We Can Improve . This was... fine. We'd unearth some useful tidbits about our execution that we'd be able to take forward and use to be more effective in the coming weeks. The problem was, we weren't feeling inspired by this style of retro. It had begun to feel rote, and that's no way to gain deep insights about a team's performance. When I thought about why, I realized: this retro encourages us to be entirely focused on execution, and what we do has so many more facets than that. Just like a good engineer isn't defined solely by their ability to write code, a team's performance isn't solely defined by their ability to close Jira tasks. A good engineer has to exhibit so many other core skills, like empathy, communication, initiative, support, collaboration, and respect in addition to writing good code, and a good team has to recognize the impact of these skills on their ability to achieve goals. Around this time, I saw this thought-provoking tweet: If you want a new retrospective format, you could try Virginia Satir's temperature reading https://t.co/PsODm6yKZU Our team added a \"team wins\" section after \"individual appreciations\", let me know if you try anything interesting -- and if you try it at all! — Senior Oops Engineer (@ReinH) May 19, 2020 Virginia Satir was an author and therapist who worked with groups of people to help them understand and communicate better. In this video, she goes over a process she calls the \"Temperature Reading\" which she uses to help people assess their current state of being: Satir takes a more holistic approach to understanding each individual's observations and concerns by focusing not on what was done, but rather on the impact of those actions. In order to keep our retros snappy, we've condensed the sections to the following: Appreciations , where we share praise for individuals, tools, processes, or anything else good that helped us, Grievances & Puzzles to Solve , where we talk about anything that bothered us or that we feel needs more investigation, New Information , where we discuss what we've learned that affects our work, Hopes & Wishes , where we outline what we'd like to accomplish or see change in the future. We follow this up by selecting one or two key things to work on going forward, which we keep in mind and try to raise during daily standups or as needed. The upshot of this new retro format has been a renewed sense of camaraderie and understanding of each other as human beings contributing to the team's success as a whole. It refocuses us and allows us to see the big picture about not just what the team is working on, but how and why we're doing what we do. And it helps uncover the messy, human aspects of getting things done that often affect our performance, which is key to understanding a team's journey. Want to work on a team that values each other like this? Mixmax is hiring --come join us! Header image: \"Thermometer\" by renaissancechambara is licensed under CC BY 2.0", "date": "2020-12-09"},
{"website": "Mix-Max", "title": "Engineering levels at Mixmax", "author": ["Trey Tacon"], "link": "https://www.mixmax.com/engineering/engineering-levels-at-mixmax/", "abstract": "This is the last post, post twelve, in the Mixmax Advent 2020 series . For the last post in our series, we’ll talk about how we designed our engineering levels. This document is a live document inside of Mixmax - it’s always being updated as we continue to grow and evolve as a team. If this philosophy resonants with you and you’re interested in the work that we’re doing, come join us! Mixmax Engineering Levels Philosophy Levels at Mixmax represent your rank, not your role. Individual contributors and team leads share the same leveling system, and which role you play speaks more to your particular skills than your seniority at the company (beyond a minimum rank requirement for some roles). These levels are meant to exemplify Mixmax’s core values “building strength in our differences” and “bring out the best in each other.'' The intent of this leveling system is to embrace and support a diverse group of unique engineers. We value both generalists and specialists, from the engineer that performs consistently at the same level across all attributes, to the engineer that is an execution powerhouse, to the engineer that is a bastion of support for the entire org. The goal of this document is to provide clarity on how to advance in rank at Mixmax, and to help communicate what we’re looking for when we bring on new engineers at a certain level. You should be able to have a conversation with your manager about what you need to do in order to move onto the next level and formulate a specific plan to get there. Leveling Up Your overall level is an aggregation of your level across the dimensions that we focus on here at Mixmax. Dimensions Technical This dimension focuses on an engineer’s technical skillset: including, and not limited to, an engineer’s mastery of different technologies, best practices, reading code, code stewardship, quality & testing, design, and debugging. Execution This dimension focuses on the way that an engineer gets things done: planning, scoping, estimation skills, getting unstuck, taking ownership, strategic alignment, product/business understanding, and vision. Collaboration This dimension focuses on the way an engineer works together with teammates: communication skills, asking for and giving feedback, collaborating, sharing knowledge, unblocking others, leading teams, participating in meetings. Support This dimension focuses on how an engineer supports themselves, their teammates, all of engineering and the company as a whole. It focuses on the glue that engineers can use to help be a part of and build incredible relationships and teams. Relevant skills include living the company values, supporting other engineers, recruiting, evangelizing, and leading organizational initiatives. Impact This dimension is an overarching dimension that focuses on the impact radius of an engineer. It looks to see what the consistent impact an engineer has on their peers, their team, their department and the company as a whole. Evaluation Your level in a dimension at Mixmax is determined by which of these behaviours you consistently and regularly apply. Writing a spec does not instantly qualify you for L2 - think of it as needing to be able to demonstrate that you’re an L2 consistently for at least 3 months before you fully qualify. As you level up, we encourage a multifaceted approach - you’ll touch base with your manager at least quarterly to discuss how to iterate on this. If you want to focus in, and hone your superpowers, awesome! If you want to spend time working on your weaknesses, 🙌. We want you to focus on what will make you, as an individual, a phenomenal engineer. Everyone has specific superpowers that need to be continually replenished and developed, and working on weaknesses will increase your ability in other areas that then act as multiplying factors for your superpowers. To reiterate this and to riff on how this ties into our values: We all have our own superpowers, allowing us to build strength in our differences, and so it’s important to always be developing these. We also need to work on our weaknesses, so that we can strengthen them to multiply the power of our superpowers, allow us to do more with less. These together help us bring the best out in each other. Which in turn allows us to live as one team, with one mission. Which we have to do in order to turn our customers into heroes at work. When you have these discussions with your manager, it’s up to both of you to come up with an actionable plan on what you can do to increase your rank, along with a desired timeframe for completion. New Hires New Hires can be brought on at any level - part of the recruiting process is to make a judgement call on what level they operate at based on their experience and their performance in interviews. It’s not necessarily expected that a new hire is consistently performing at their level on day one - generally we would expect a new hire to “ramp up” to their expected level within their first 3 months. Levels L1 An L1 engineer is often new to software development, either from school or a bootcamp program. L1 engineers usually work on highly scoped problems that have been thoroughly reviewed by a more senior member before assignment so that they have guidance on how they should go about implementing the change. L1 engineers are usually not expected to define the architecture of their solutions, or work independently on a project without frequent check-ins. L1s main focus to advance should be familiarity with the codebase, technologies, and coding concepts. L1s have project level impact. Impact As newer engineers, L1s will primarily have an impact on themselves and their tasks, and so their projects. Technical Can work independently on small issues and isolated stories that are well-scoped Demonstrates a basic understanding of core technologies like Express, React and Javascript Writes well structured code and writes automated tests appropriately. Execution Consistently completes tasks on time and sees them through to completion Works to push tickets through review, testing, etc. Conducts test parties for their features when appropriate. Raises issues to the team when blocked on a ticket. Collaboration Participates in team, guild, and co-working discussions. Provides feedback to team members and their manager appropriately. Receives feedback constructively and works to resolve issues raised. Support Conducts themselves according to the Mixmax code of conduct at all times. Is familiar with Mixmax values, and makes efforts to follow them. Contributes to organizational initiatives, such as participating in cultural breakout sessions. Actively looks to grow through their own skills through goal-setting and soliciting feedback. Expected timeframe to next level We expect L1s to transition to L2 within one to two years. L2 An L2 engineer is capable of working on a project collaboratively with other team members, including defining specifications, defining architecture and solutions to projects, and ensuring that their work in a project covers all aspects of these specifications. L2 engineers should be active participants in PR reviews, team, guild, and engineering-wide meetings, and should be able to act as mentors to L1 engineers. L2s are increasing their team-level impact through such mentorship and teamwork. Impact An L2’s impact radius is their team. They’re beginning to take responsibility for an area of our system (with guidance) and through taking initiative (e.g fixes bugs unprompted). They help mentor L1s and inspire teamwork. Technical Proposes architecture and solutions for small to mid-sized projects Familiar with all core services of the Mixmax platform Strong familiarity with core technologies used at Mixmax Execution Can manage the workload of a project, including specification writing, creating an issue backlog, and executing on completion of the project. Able to estimate workload and execute successfully within a weekly timeframe Proactively works to unblock themselves on issues rather than relying on another team member. Scope of work is focused on small to medium sized projects Collaboration Presents new ideas and topics at guild meetings and co-working Regularly reviews other engineers code and adds constructive feedback on code style, design issues, and potential risks. Works with Mixmax members outside of engineering on problems Able to work directly with customers to resolve issues when appropriate Able to assess risk on major changes and PRs and minimize risk on code merges and deploys. Represents Mixmax Engineering to the greater community, eg presenting at meetups and conferences Support Lives our core values at Mixmax. Helps support their peers and teammates by providing consistent feedback to them, espousing our culture of continuous feedback. Assume good intent and trust their teammates. Demonstrates empathy for teammates and consideration for the impact they have on others, e.g. reaching out if they sense something wrong. Able to constructively voice concerns. Expected timeframe to next level We expect L2s to progress to L3 within two to three years. L3 An L3 engineer is more of a technical mentor to other engineers on the team, and may serve as a team lead on projects. L3 members should be able to coordinate the work done by a group of individuals in order to hit a project’s goal, whether they are formally leading a team or playing a key technical role on the team. L3s continue to demonstrate strong team-level impact. Impact An L3’s impact radius is their team + their peers. Share their experience and expertise to help others grow. They lead and coach within their team, where possible, and they’re trusted with team decisions. Beginning to broaden their impact. They consider the effects of their work on other teams, as well as identifying and helping to resolve problems facing their team and others. Fully takes responsibility for a service or component. Technical Serves as a technical mentor for other Mixmax engineers through project leadership, specification definition, and teamwork. Thoughtful, in-depth investigation into deep technical issues. Execution Consistently uses data in making decisions on architectural and product direction Estimates risk of future work, identifying potential roadblocks and incorporating them into their estimates Self-manages their time - capable of scheduling time for learning and other initiatives themselves without manager support Scope of work incorporates larger multi-week projects Documents the work they do and ensures information they maintain is kept up to date. Collaboration Actively works to unblock others through helping with questions, offering technical mentorship and advice, and bringing out the best in other engineers. Effectively leads meetings and organizes groups of engineers Has a deep understanding of how customers use Mixmax, and applies that in the work they do. Can be relied on to represent the company when talking to key customers or on sales calls. Support Helps newer teammates live our values. Gives incremental and consistent feedback to all team members. Champions or owns new cultural initiatives. Assists recruiting efforts in growing the Mixmax team, either through interviewing or other activities. Writes insightful documentation and processes. Expected timeframe to next level If an engineer wants to focus on moving to the next level, this will typically take two+ years to grow their influence to L4. L4 L4 engineers are capable of defining the architecture and coordinating the moving pieces of large projects. L4 engineers should serve as mentors and leaders for the engineering organization, either through technical mentorship, leading more substantial projects that may span a quarter, or as direct managers. L4s are expanding their team-level impact by understanding the effect of their work on their team. They’re also beginning to understand and help solve problems facing their team and other teams, by beginning to identify and lead engineering initiatives within their team and the org. Impact In addition to their team and their peers, an L4’s impact radius is their guild (routinely have initiative and domain level impact). Identify and advocate for foundational work and practice improvements in their domain. Leads initiatives & meetings within team and domain. Regularly leads multi-person, multi-week projects Has a consistent record of very strong ownership for their area (e.g. figuring out on-call schedules, new monitoring initiatives, research initiatives to drive their areas forward). Technical Proposes long-term architectural directions for major projects and technical initiatives at Mixmax. Participates in defining an architectural vision for an area of the application long term. Execution Can be relied on to define an execution plan for a major project, incorporating past learnings and accommodating for unforeseen issues and learnings. Can work with a team to define high level, externally facing goals for projects spanning many weeks Scope of work is focused on major initiatives for a quarter. Collaboration Serves as a technical mentor to other team members and can add constructive feedback and guidance to other teams’ specifications and proposals. Works with key stakeholders (including cross-functionally) in assessing needs for projects and ensuring they are addressed by solutions. Support Gives guidance & unblocks others on their team and area. Actively works to help grow and evolve our culture. Helps junior engineers find opportunities to get involved in recruiting and evangelizing (i.e. via presentations opportunities both inside and outside the company). Routinely carves out opportunities for more junior engineers. Helps others maintain resilience during periods of change. Expected timeframe to next level Things start to get fuzzier here, and we’d often expect this process, if desired by the engineer, to take two+ years to grow their influence to L5. L5 L5 engineers are able to manage an entire area of the Mixmax application and propose and execute on long-term architectural vision for that area (i.e. major technology or architecture upgrades, redesign of core systems). L5s are expected to regularly have domain and initiative level impact. Many of the responsibilities of an L5 (such as defining long-term technical vision) might be something we don’t do a lot as a company, but we anticipate this will be an increasingly key function as the company grows. Impact An L5’s impact radius is, at minimum, the entire engineering department. Thought leader for technical decisions, influencing architecture and prioritization across multiple teams. Lead initiatives across domains, even outside their core expertise. Coordinates large & complex projects, including with outside partners. Technical Leads the definition and execution of major technology initiatives (introducing new technology, redesigning areas of the application, etc.) Has best-of-class knowledge in a particular aspect of Mixmax-deployed technology. Execution Can work with key stakeholders, including the management team of Mixmax to define a technical long-term vision and execute on it. Capable of managing the workload of a large team on a quarterly level and defining resourcing plans to meet those goals. Collaboration Implements new initiatives to improve quality and minimize risk in the Mixmax codebase. Regularly serves a mentorship role to several senior engineers on the Mixmax team. Works with all aspects of the business to help shape future direction. Support Routinely and consistently supports multiple teams moving forward. Is a bastion of support for engineers to come to for advice, both professional and personal. This means that this individual is a go-to person for many engineers. Helps act as a buffer for junior engineers, routinely reaching out to support them however they can. Trusted to de-escalate conflicts and build consensus between team members. Actively spreads the Mixmax brand to prospective candidates. Expected timeframe to next level The air is starting to get thin up here, this will often take three+ years to grow into an L6 (if the engineer decides to grow their influence more). L6 As we get bigger, we will probably need engineers who work with an even broader impact and more seniority than L5s. Right now, we don’t have a solid model for how a L6’s responsibilities would be different from an L5. Part of the responsibilities of an engineer hoping to move to L6 should be to work with us to define this role. As a starting point, however, the impact of an L6 engineer should move outside of the engineering organization, and into the broader ecosystem we work in. They should genuinely be considered thought leaders outside of Mixmax for a particular area - whether that’s due to speaking engagements, open source contributions, or direct interaction with Mixmax customers. L6 engineers should have visibility and influence on key strategic initiatives at Mixmax and will generally feel comfortable working among department heads on important projects.", "date": "2020-12-12"},
{"website": "Mix-Max", "title": "A Brief History of Mixmax, Pt. 1", "author": ["Logan Davis"], "link": "https://www.mixmax.com/engineering/history-of-mixmax-part-1/", "abstract": "This is the eleventh post in our Mixmax Advent series and the first part of an ongoing series documenting the evolution of Mixmax’s architecture and infrastructure over time. We’re a long way from where we started way back in 2014! In this post, we’ll cover the first two years of Mixmax history. Mixmax started in mid-2014 as a prototype of a web-based email editor. The idea was that you could embed “smart enhancements” into your email, such as an interactive slideshow or meeting links picked from your Google Calendar availability. The content could then be sent to someone or shared via a URL. Even the company name, “Mixmax”, was chosen to suggest that you could “mix” your content and “maximize” its impact. Over the years, we’ve needed to constantly evolve our infrastructure and technical choices to keep up with growing demand. Mixmax has gone from a prototype Chrome extension with just a few users to a platform that tens of thousands of professionals use daily to do their job. Check out these old screenshots of baby Mixmax! The original Mixmax prototype was written in a Javascript framework called MeteorJS . We chose Meteor because it was, at the time, the go-to framework choice to build and host a simple web app. Meteor, in turn, had chosen to bundle Node and Mongo, because they, too, were very popular in 2014.  😉 Choosing Meteor allowed us to focus exclusively on feature development in the first year of the company and thus get to product-market-fit quickly. It enabled us to deliver a polished experience at a small scale. Its core “reactivity” feature helped us make an app that was realtime, which continues to be a core part of our product experience today. It also made it easy to share code between the client and server (“isomorphic code”), which shortened development time for us, as a lot of the prototype’s code ran on both server and client. From that original Meteor prototype to where we are today, our architecture history can be roughly summarized as breaking apart our original “Meteor monolith” into a new microservice architecture. The below timeline explains each step and why we made the technology choices we did. Timeline Jul 2014: Our first prototype was built using Meteor and hosted on Modulus.io . We chose Modulus because it was the only PaaS that supported single-command Meteor app deployment (not even Heroku did). Our data was stored in MongoDB hosted with Compose.io , on the recommendation of the Meteor team (since Meteor and Compose were both in the same YC class). We used Codeship for deployment because they were the easiest to get set up at the time. Sep 2014: We rewrote our first prototype into something a bit less hacky, and took the opportunity to make the code more modular so that we could start pulling parts of it out into their own services. We knew at this point that Meteor wasn’t going to scale as a monolith, as we had many outages due to server CPU saturation. Feb 2015: We started to break apart the Meteor monolith by moving our contact management code into its own service. Next we split out services for insertable email apps (to better integrate 3rd-party npm modules) and send email (for stability). We decided to continue to use Node because we had team Javascript proficiency and it was much easier to port code from the prototype (ie we didn’t need to rewrite it in another language). For async operations, we decided to continue using Fibers (via a wrapper library called synchronize) from Meteor. Meteor used Fibers to make isomorphic Javascript more readable . These decisions allowed us to mostly copy-and-paste code from the Meteor monolith codebase, without needing to rewrite using Promises. We also decided to stick with Mongo since the schema-less design allowed us to develop more quickly. To send email, we used a distributed queue called Kue, which used Redis as a broker. We made this choice due to our experience with the tools, and because it was easy to host Redis deployments on Compose.io, where we were already hosting Mongo. May 2015: The next code to be pulled out of the Meteor monolith was the email editor . This solved a big performance complaint from customers that Mixmax was too slow to load in Gmail (because Meteor couldn’t render server-side). We chose to implement the frontend in Backbone because we had team expertise in it. Backbone also uniquely supported attaching views to server-side rendered content which other frameworks - including Angular and ReactJS - did not support this at the time. We even built a small library to add reactivity support to Backbone, so it felt more like a Meteor app. Aug 2015: We created a “notifications service” to abstract publishing data to our Redis. Down the line, this would end up powering Mixmax’s rules engine, which at the time was but a twinkle in our eye. Oct 2015: We moved most of the microservices from modulus.io to AWS Elastic Beanstalk. This move was mostly because modulus.io didn’t offer enough monitoring capability. The main webapp stayed on modulus.io for websocket support. Dec 2015: We moved the main webapp from Modulus to Meteor Galaxy . Galaxy had just launched and we were building a relationship with the Meteor team (as their biggest app!), so it made sense to just host with them. This moved us entirely off modulus.io. Feb 2016: We noticed emails weren’t being sent sometimes and realized our distributed queue, Kue, was losing job data (!) and therefore users’ emails (!!!). We searched for alternatives and found one we liked, a queue called Bull. We switched all our backend queues to Bull, which solved the immediate data loss issue. Jun 2016: While on a team offsite, literally in a car, we implemented Elasticsearch as the backing store for an inbox history feature called the Live Feed, served from a new microservice. We initially hosted Elasticsearch using Compose.io , but quickly moved it to Elastic Cloud, since Compose.io had stability issues. ...and that’s the first two years of Mixmax! These days, it’s hard to believe it took us so long to put anything on AWS, but there you have it: it was two years of rapid iteration, PaaS migrations, and growing a business from scratch. In the next installment, we’ll cover how Mixmax matured in its new post-monolith era!", "date": "2020-12-11"},
{"website": "Mix-Max", "title": "Modern Data Warehouse Infrastructure: The Database", "author": ["Pablo Seibelt"], "link": "https://www.mixmax.com/engineering/modern-data-warehouse-infrastructure-the-database/", "abstract": "Photo by Aron Visuals on Unsplash For a modern Data Warehouse that lives in the cloud, which is what most modern organizations would want nowadays, there are a lot of useful tools, and more coming out each day, which is great. However, sometimes it’s difficult to decide which tools to use for each task, which things are worth building inhouse or buying externally, and the answers may not be the same for each organization. Which is why it’s most important to understand the concepts and how these types of infrastructure pieces fit together. In this first post I'll talk about the Data Warehouse itself, the Database where we will store and retrieve the data. The most basic and important piece would be the Data Warehouse, there are 4 well known options which are easy to set up in the cloud: Amazon’s Redshift, Google’s BigQuery, Azure’s SQL Datawarehouse, and Snowflake which can run in any of those 3 cloud services. These are not the only options, there’s open source options like Druid or Hadoop, and also more enterprise-level DW solutions such as Teradata or Vertica. Which one you choose depends on your particular business, the requirements you have, the type of usage you will give it, your existing infrastructure and your budget. Technically you can also make a Data Warehouse in MySQL or PostgreSQL as well but i’ll advise against it, they are meant for transactional workloads (e.g. running an application) so they are pretty good at getting one particular row, but not quite good for summarizing huge amounts of data. I have experience with Redshift and Snowflake so i can talk about those 2 databases, there are a couple of things that can make your life easier depending on which database you choose, for example, comparing Snowflake to Redshift: Character columns On Amazon Redshift text columns are limited to up to 65,535 characters, which can be a problem if you need to add e.g. tickets and emails or source code which may sometimes be over that size. Snowflake on the other hand allows for 16,777,216, no matter what varchar limit you specify (it’s only informative). At the same time this means you have to check for text overflow errors in ETL manually for Snowflake if it doesn’t reach that character size. Semi-Structured Data Until Dec 2020, JSON data was not supported natively on Amazon Redshift, although now it has been added as a preview feature. Snowflake has support for semi-structured data including JSON and it’s queries are blazing fast. User-defined functions Amazon Redshift allows for SQL, Python and Labmda User-defined functions (UDF), Snowflake has UDF support for SQL, Javascript and external functions (AWS Lambda and Azure Function) Processing vs Storage scaling Snowflake allows to separate the billing of processing (Virtual Warehouses) and Storage, allowing for a data source that’s not regularly queried to cost much less right from the very smallest Data Warehouse. Redshift has allowed for this capability of separating compute and storage since Dec 2019, and added more node types since, but it is a much more rigid model in comparison, one of the biggest advantages is to be able to assign as much or as little processing power depending on the user running it, so e.g. in Snowflake if a Data Scientist will need to do very complex calculations you can give them a very powerful connection, while the people viewing dashboards can keep a small one and everyone will be running smooth without spending huge amounts of money. Another advantage of the virtual warehouse model is that you can shut it down when it’s not used, allowing you to only pay for the hours it’s used. Another big advantage here for Snowflake is that scaling takes just a few minutes while on Redshift provisioning or deprovisioning capacity is much more complex. Other decision criteria The most important thing is to be very mindful of your context when choosing a Data Warehouse, and be mindful that migrating to a new one later is a huge endeavor, it is possible you start using features exclusive to one Database and it will be incompatible with others, so do not randomly choose one. Some factors to take into account would be: Ease of use : Does it have tools for importing semi-structured data? Is it compatible with ETL tools? (e.g. DBT) SQL Support : Even if it’s not an SQL Database (Like Hadoop) does it allow for SQL access? In Hadoop’s example, there are many SQL interfaces that work with it. Integrations and drivers : Does it have JDBC/ODBC drivers? How easy is it to connect to a BI Tool? To Python/R? etc Cloud Services integration : Where does your Infrastructure live already? e.g. if you have everything in Google’s cloud, maybe BigQuery will be easy for you to integrate, meanwhile Amazon’s Redshift would require you to move data to S3. Pricing : This is one of the hardest things to compare, because each cloud DW has their own pricing model which is often not trivial to compare. Performance and parallelism : This can be complex to estimate without doing some tests as well, because each organization’s needs can be different and each Database has it’s pros and cons, unlike SQL transactional databases which are relatively similar, each DW solution uses quite different tricks to achieve performance gains. Flexibility : How easy and fast is it to scale it up/down in processing or storage? How easy it is to copy data from one database to another? Authentication : Does it support SSO? How easy is it to set up? Special requirements : Do you need real-time processing? Host on-premise instead of in the cloud? Do you need the Database to be open source? Do your customers need to access this data or only internal employees? Conclusion I hope that these tips can help you decide which Database will be the right one for your organization. I am very skeptical of “This is the perfect tool” claims, each provider optimizes different things, so you will need to analyze your particular needs to decide which is best for you. In our particular case, on Mixmax we chose Snowflake because of the advantages we saw for our use case, plus the fact that most of our infrastructure is on AWS.", "date": "2021-02-09"}
]