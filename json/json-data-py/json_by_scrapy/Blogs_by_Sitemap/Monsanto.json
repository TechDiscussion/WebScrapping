[
{"website": "Monsanto", "title": "Lerna", "author": ["\n                                        Kevin Ou\n                                    "], "link": "http://engineering.monsanto.com/2019/10/10/lerna-monorepo/", "abstract": "The Problem If you are familiar with Babel , you may had seen the various plugins, utilities, and core packages that are available to install. To give you a better sense of how many packages are out there, Babel has published nearly 150 packages to NPM. Our team also publishes packages to NPM. We have a corresponding GitHub repository for each package. While we were only at 10% of Babel’s total package count, it was already difficult for us to maintain the growing number of repos. For example, a problem that we faced was keeping our local dev environment up to date. If a package was updated, we had to pull it down and rebuild with the latest changes. Since 14 or so developers had contributed to the project, multiple packages may have been updated in a single day. As a result, we had to spend a lot of time  switching repos, checking out the right branch, pulling down the code, npm installing, etc. This led me to the question: What are other large JavaScript projects like Babel doing differently? Do they have a solution in place that scales as the number of packages grow? Introducing: Lerna I investigated their GitHub repository to gain some insights on what they were doing differently and noticed this in their FAQ. I had learned that Babel is organized as a monorepo. In this design, all ~150 packages maintained by Babel are stored in the same repository. It may sound like a monolithic design, but I would actually consider it to be a hybrid between that and a multi-repo design. To clarify some terminology before going further. Monolith: All of the dependent code are stored in one repository and are deployed together. Multi-repo: Codebase is spread across multiple repositories. Packages can be deployed independent of one another. Monorepo: Contains multiple packages in one repository. Packages can be deployed independent of one another. Our team initially started off with a multi-repo design. Another issue that we ran into with this pattern was that if we wanted to automate our deploys, we had to create a 1:1 relationship between our repositories and our Jenkin jobs. Not only did we have to maintain the growing number of repos, we now had to maintain the growing number of jobs and deploy keys as well. The Solution We can use Lerna to help with some of the troubles listed above. To start with, we only had to create a single GitHub repository and a single Jenkins job, which removes a ton of setup overhead. Since all of the packages are now under one git tree, we are one git pull away from keeping all of the packages up to date. We technically haven’t used Lerna yet; those are just inherent advantages of having a single GitHub repo. The part that we need Lerna for is to be able to manage the packages within that single repo. Specifically, going back to the idea of only deploying the packages that have changed. Lerna provides a way to perform scripts on either all or just a subset of the packages. It comes with scripts such as lerna bootstrap or lerna version , which handles the install and publishing of packages respectively. Custom scripts can be achieved by using either the lerna run <script> command, which runs the associated npm script for each package that contains that script, or the lerna exec <script> command, which runs the provided script for each package. All of these scripts can be further enhanced by passing in additional flags. Since some of the scripts run on all of the packages by default, we can use the custom flags --since and --include-merged-tags together to target only the packages that have changed. Additional scripts and flags can be found in their official documentation. Putting it all together These are the scripts that are executed by our Jenkins job. npm install lerna bootstrap --since --include-merged-tags lerna run lint --since --include-merged-tags lerna run test --since --include-merged-tags git reset --hard lerna publish patch --include-merged-tags --yes lerna exec --since --include-merged-tags -- node ‘../../saveConfig.js’ First, we install Lerna and other root level dependencies via npm install . Then, we install the dependencies only for the packages that have changed. We validate that it passes both our lint and test checks. We also make sure the working directory is clean before publishing. Once the packages are published, we execute the custom script saveConfig located at the project root. The custom script saves our package metadata to our database. Learnings The first time I migrated a project to use Lerna, I moved the packages to the new repo by copy pasting everything over. Turns out, there was a script, lerna import , that does just that and more. There is a learning curve in setting up and using the scripts for the first time. Lerna will do a lot of the heavy lifting for you, it is just a matter of learning what is available. With Lerna, more than one package may be published from a single build script. This might make recovering from failed builds tricky depending on what step failed. Didn’t pass lint and test step? No problem, fix those and try again. Some packages suceeded but some failed while publishing to NPM? A little bit trickier. lerna publish will attempt to push tags to GitHub and publish to NPM. If connectivity issues occur, it may lead to the situation described above. To recover from this, you can use the command lerna publish from-package to republish only the failed packages. Final Thoughts Switching over to a Lerna monorepo has saved our team countless time and effort. We extended the use to also manage and deploy our lambda functions. There are various other neat things you can do with it that is not covered in this article. A monorepo design is problably not the best design for most projects. I would suggest switching over to a monorepo design if there are multiple packages and managing them all becomes too difficult. Existing projects could always be migrated to a monorepo at any time; Lerna provides a simple way of creating the repo and migrating existing packages via the lerna init and lerna import commands. Here is what Babel thinks of it: Popular projects that are using Lerna include React, Angular, Jest, and many others. For a more in-depth explaination on the actual structure and the various lerna commands, their documentation is a great resource. tl;dr By using Lerna, we get the advantages of a single repo, while still having the capabilities of deploying and publishing only the packages that have changed. posted on October 10, 2019 by Kevin Ou ← Previous Post posted on October 10, 2019 by Kevin Ou Kevin Ou", "date": "2019-10-10"},
{"website": "Monsanto", "title": "Parrot: A Swift Mock Generator", "author": ["\n                                        Brett Lindsay\n                                    "], "link": "http://engineering.monsanto.com/2018/12/19/parrot-swift-mock-generator/", "abstract": "The Problem When I started iOS and Swift programming professionally, I was coming from a team where we were learning test-driven JavaScript web development with Node and React. Chai , Mocha , and Sinon were the standard set of unit testing tools for my team at the time. While running tests and writing assertions in Swift and Xcode are similar enough, the contrast was painfully obvious when stubbing out functionality. Since Swift has no run-time reflection, Xcode provides no tools to help stub out functionality. The typical pattern for enabling stubbed out functionality involves: Writing a protocol for the class or struct to be mocked Injecting that protocol as a dependency Writing a mock class conforming to that protocol And finally, injecting an instance of the mock into the class or struct being tested. These steps are then duplicated many times because many classes and structs have multiple dependencies that each require their own mocks. Clearly, stubbing out functionality in Swift takes quite a few more steps compared to sinon.stub() . Some other issues also present themselves quickly once you start modifying others’ projects with unit tests and even just modifying your own tests and code. Multiple contributors in a code base means mock classes could have inconsistent manual implementations. Some will have a Boolean myFunctionCalled variable, while others will have an Integer myFunctionCallCount variable; one could use a tuple lastParametersCalledWith vs. an array of tuples, myFunctionParameters or myFunctionCalledWith . Without checks or automation, developers don’t always hold themselves or others accountable to their own rules, patterns, or standards that help speed up development. Any changes to a protocol means fixing compile errors for their mocks. Poorly written mocks could be missing expected properties of mocks completely There Has to Be a Better Way Parrot started out as two separate attempts at the same problem which ended up not overlapping at all. I had written a rough script to find the first protocol definition in a given file and psuedo-parse the code in order to generate a mock. It was a bare-bones Swift script (I personally really like scripting with Swift). A friend in my group of developers had started writing code that would scan a directory recursively and identify the Swift files. It was exciting realizing what we could make together. We took a couple days putting the pieces together, and we had our first pass at a Swift mock generator. \nThe initial implementation definitely had some glaring holes in it, and it certainly isn’t “finished” today. \nEven so, the benefits are evident. Generating your mocks with Parrot guarantees they will all be written in the same format, with the same interface to assert against in tests, and there won’t be any missing implementations. It’s meant to generate before the compile time of your tests, so any protocol changes in the project will be reflected in your mock definitions automatically. See How It Works Your browser does not support the video tag. Adding Parrot to a Project This and more details about usage can be found in the Parrot README . Other Options When we started Parrot, there were not similar tools to be found and the ones that did exist had taken directions we didn’t want to follow. At the time of writing this post, another option exists: Swift Mock Generator for Xcode . It’s used as an Xcode extension. Readers should feel free to compare and contrast these two tools. Contribute Parrot is far from perfect and has room for enhancements. From what I’ve seen, the majority of generated mocks work right out of the box while the ones that don’t only need a small amount of modification. I hope Parrot makes Swift testing a much better experience for all who use it, and I’m looking forward to its future enhancement. https://github.com/MonsantoCo/Parrot posted on December 19, 2018 by Brett Lindsay ← Previous Post Next Post → posted on December 19, 2018 by Brett Lindsay Brett Lindsay", "date": "2018-12-19"},
{"website": "Monsanto", "title": "Integrating custom workflow with Slack slash commands", "author": ["\n                                        Prasanna Crasta\n                                    "], "link": "http://engineering.monsanto.com/2018/10/12/integrating-custom-workflow-with-slack-slash-commands/", "abstract": "Like most modern IT organizations, we at Bayer heavily use Slack within our software development community. Recently I had an opportunity to integrate a custom workflow within our slack team. The goal was to be able to search our company developer documentation using Slack and recommend a particular link by posting it back in a channel. As I went about this integration, I learnt quite a few things which I thought would be worth a blog post. By the way, if you need to read up on slash commands I would encourage you to check out this link So, what are we waiting for? Let’s get started. Goal: To create a slash command which upon invocation with some text (say “http cat codes”) will return matching document links along with some interactive buttons (only visible to the user). The user can then click the corresponding buttons which will submit the link to the document in the channel. Technologies used NodeJS Javascript NPM modules used express body-parser query-string @slack/client @slack/interactive-messages Creating a simple node app Let’s spin up a NodeJS server using the express web framework. Whenever a user will use our slash command, this app will receive a url encoded POST request. save below snippet as server.js const express = require ( \" express \" ) const bodyParser = require ( \" body-parser \" ) const slashCommand = require ( \" ./slashCommand \" ) //will be used for handling the slash command request const slackInteractions = require ( \" ./slackInteractions \" ) //will be used for handling button interactions. const app = express () const urlencodedParser = bodyParser . raw ({ type : \" application/x-www-form-urlencoded \" , inflate : true , limit : \" 100kb \" }) app . post ( \" / \" , urlencodedParser , slashCommand ) app . use ( \" /slack/actions \" , slackInteractions (). expressMiddleware ()) app . use ( \" /* \" , ( err , req , res , next ) => { console . error ( err ) // handle uncaught errors next () }) const hostname = process . env . NODE_ENV === \" production \" ? undefined : \" 127.0.0.1 \" // unlike default, only reachable from this machine const port = parseInt ( process . env . PORT || 3000 , 10 ); const server = app . listen ( port , hostname , () => { const address = server . address () const url = `http:// ${ address . host || \" localhost \" } : ${ port } ` console . info ( `Listening at ${ url } /` ) }) I am using body-parser to parse application/x-www-form-urlencoded type of data posted to the / url. Note the use of this middleware before passing on the request to our slashCommand handler function. I have a second route for handling slack actions, this is handled by the slackInteractions middleware. Remember to not use any parser middleware for this route, because the slackInteractions middleware expects a raw body to calculate the signature & verification. Implementing the command handler Below is the basic implementation. The first thing we need to do is verify that the request is a genuine one coming from slack (I’ll get to the implementation later in this section ). If the request is validated we will parse the text that was submitted with the command, carry out the search and return the results as message attachments. save as slashCommand.js (we are using this in server.js) const queryString = require ( \" query-string \" ) const verifySlackRequest = require ( \" ./verifySlackRequest \" ) const performSearch = require ( \" ./performSearch \" ) const slashCommand = async ( req , res ) => { if ( verifySlackRequest ( req )) { const message = queryString . parse ( req . body . toString ()) const data = await performSearch ( message , res ) return res . json ( data ) } return res . status ( 401 ). send ( \" request can not be fulfilled \" ) } module . exports = slashCommand Verifying the incoming slack request Slack signs every request that is sent as part of slash command and includes this signature as a request header x-slack-signature . It is upon us to verify the request is a genuine one coming from slack by recreating the signature using a special Signing Secret assigned to our app. We will get this secret when we create a new app in Slack (we will get to this configuration step later). The verification process involves recreating the signature by combining the Signing Secret of our app, the request timestamp and the request body using a standard HMAC-SHA256 keyed hash. The signature that we created is then matched against the signature passed in the request header. You can read more details about the process here . To recreate the signature we will make use of the NodeJS crypto module. const crypto = require ( \" crypto \" ) const verifySlackRequest = ( req ) => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const timestamp = req . headers [ \" x-slack-request-timestamp \" ] const headerSignature = req . headers [ \" x-slack-signature \" ] const basestring = `v0: ${ timestamp } : ${ req . body . toString ()} ` const hash = crypto . createHmac ( \" sha256 \" , SLACK_SIGNING_SECRET ) . update ( basestring ) . digest ( \" hex \" ) const calculatedSignature = `v0= ${ hash } ` return headerSignature === calculatedSignature } module . exports = verifySlackRequest Performing search and returning results This part involves querying a backend API that would return matching results, formatting these results as slack message attachments and including additional interactive buttons. To keep the example simple instead of writing the backend API we will match the search text against static mock data. save as performSearch.js (we are using this in server.js) const mockData = [ { \" id \" : 0 , \" title \" : \" http cat 300 \" , \" url \" : \" https://http.cat/300 \" }, { \" id \" : 1 , \" title \" : \" http cat 301 \" , \" url \" : \" https://http.cat/301 \" }, { \" id \" : 2 , \" title \" : \" http cat 302 \" , \" url \" : \" https://http.cat/302 \" }, { \" id \" : 3 , \" title \" : \" http cat 303 \" , \" url \" : \" https://http.cat/303 \" }, { \" id \" : 4 , \" title \" : \" http cat 304 \" , \" url \" : \" https://http.cat/304 \" }, { \" id \" : 5 , \" title \" : \" http cat 400 \" , \" url \" : \" https://http.cat/400 \" }, { \" id \" : 6 , \" title \" : \" http cat 401 \" , \" url \" : \" https://http.cat/401 \" }, { \" id \" : 7 , \" title \" : \" http cat 402 \" , \" url \" : \" https://http.cat/402 \" }, { \" id \" : 8 , \" title \" : \" http cat 403 \" , \" url \" : \" https://http.cat/403 \" }, { \" id \" : 9 , \" title \" : \" http cat 404 \" , \" url \" : \" https://http.cat/404 \" } ] const createMessageWithAttachments = ( searchText , messageAttachments , buttonValues ) => { const text = `Found ${ messageAttachments . length } document(s) matching _* ${ searchText } *_` let attachments = [] if ( messageAttachments . length > 0 ) { attachments = messageAttachments . slice ( 0 , 5 ) attachments . push ({ title : ' Click one the below button(s) to post link to the channel ' , callback_id : ' single_link_btn ' , actions : buttonValues . map (({ title , url }, index ) => ({ name : ' results ' , text : `Link ${ index + 1 } ` , type : ' button ' , style : ' primary ' , value : ` ${ url } ?title= ${ title } ` })) }) } return { text , attachments } } const noResultsMessage = ( searchText ) => ({ text : `no results found matching _* ${ searchText } *_` }) const performSearch = ( message ) => { const searchText = message . text const matchingDocs = mockData . filter (( item ) => item . title . includes ( searchText )) const fields = matchingDocs . map (( doc , index ) => ({ title : ` ${ index + 1 } . ${ doc . title } ` , title_link : ` ${ doc . url } ` })) const buttonValues = matchingDocs . map (( doc ) => { const { title } = doc return { title , url : ` ${ doc . url } ?title= ${ encodeURIComponent ( title )} ` } }) if ( fields . length > 0 ) { return Promise . resolve ( createMessageWithAttachments ( searchText , fields , buttonValues )) } return Promise . resolve ( noResultsMessage ( searchText )) } module . exports = performSearch Handler for interactive buttons Okay, this is the final piece of code that we need. This is for handling the request when the user clicks one of the link buttons as shown above . We need to supply the signing secret to the adapter. Note: the callbackId single_link_btn should match the callback id specified for the button links when we generated the original message. So, in this case if the user clicks a button Link 1 it will submit a message back in the channel with link 1, this time visible to everyone in that channel and remove the original message that was posted (only visible to the user who initiated the command). save as slackInteractions.js (we are using this in server.js) const { createMessageAdapter } = require ( ' @slack/interactive-messages ' ) const querystring = require ( ' query-string ' ) module . exports = () => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const slackInteractions = createMessageAdapter ( SLACK_SIGNING_SECRET ) slackInteractions . action ({ callbackId : ' single_link_btn ' , type : ' button ' }, ( payload , respond ) => { const { actions , channel } = payload const url = actions [ 0 ]. value . split ( ' ?title= ' )[ 0 ] const title = decodeURIComponent ( actions [ 0 ]. value . split ( ' ?title= ' )[ 1 ]) respond ({ replace_original : true , response_type : ' in_channel ' , delete_original : true , text : ' see this link ' , attachments : [ { color : ' #36a64f ' , title , title_link : url } ] }) }) return slackInteractions } Now that we are done you could run the app on a server where it is publicly accessible. (say e.g. https://my-super-useless-https-cats-search-api.com) Configuring Slack App Create a new app using the slack api here . Provide a name for your app (say HTTP Cats Search) and choose your slack workspace. Next in the Add Features and functionality section of your slack app Enable Slash Commands and provide details for command & request url Give a descriptive command (say /http-cats ) Provide a request url (say https://my-super-useless-https-cats-search-api.com ) Note: we serve the request at base url as configured in server.js In addition provide a short description & usage hints. Enable Interactive Components and provide request url This would be the second route we configured in server.js.\n So in our case https://my-super-useless-https-cats-search-api.com/slack/actions Finally install the app in your workspace. Make a note of the Signing Secret assigned to this slack app and provide that as an env variable to your node app. At this time, you should have a /http-cats command available in your workspace and if you invoke it (say /http-cats 40 ) you should get back the results like above . Click one of the link buttons and that link should be posted in the channel visible to everyone. See the demo below I hope you enjoyed this post, and are now ready to create your own custom slash commands for your workflow needs. posted on October 12, 2018 by Prasanna Crasta ← Previous Post Next Post → const express = require ( \" express \" ) const bodyParser = require ( \" body-parser \" ) const slashCommand = require ( \" ./slashCommand \" ) //will be used for handling the slash command request const slackInteractions = require ( \" ./slackInteractions \" ) //will be used for handling button interactions. const app = express () const urlencodedParser = bodyParser . raw ({ type : \" application/x-www-form-urlencoded \" , inflate : true , limit : \" 100kb \" }) app . post ( \" / \" , urlencodedParser , slashCommand ) app . use ( \" /slack/actions \" , slackInteractions (). expressMiddleware ()) app . use ( \" /* \" , ( err , req , res , next ) => { console . error ( err ) // handle uncaught errors next () }) const hostname = process . env . NODE_ENV === \" production \" ? undefined : \" 127.0.0.1 \" // unlike default, only reachable from this machine const port = parseInt ( process . env . PORT || 3000 , 10 ); const server = app . listen ( port , hostname , () => { const address = server . address () const url = `http:// ${ address . host || \" localhost \" } : ${ port } ` console . info ( `Listening at ${ url } /` ) }) const express = require ( \" express \" ) const bodyParser = require ( \" body-parser \" ) const slashCommand = require ( \" ./slashCommand \" ) //will be used for handling the slash command request const slackInteractions = require ( \" ./slackInteractions \" ) //will be used for handling button interactions. const app = express () const urlencodedParser = bodyParser . raw ({ type : \" application/x-www-form-urlencoded \" , inflate : true , limit : \" 100kb \" }) app . post ( \" / \" , urlencodedParser , slashCommand ) app . use ( \" /slack/actions \" , slackInteractions (). expressMiddleware ()) app . use ( \" /* \" , ( err , req , res , next ) => { console . error ( err ) // handle uncaught errors next () }) const hostname = process . env . NODE_ENV === \" production \" ? undefined : \" 127.0.0.1 \" // unlike default, only reachable from this machine const port = parseInt ( process . env . PORT || 3000 , 10 ); const server = app . listen ( port , hostname , () => { const address = server . address () const url = `http:// ${ address . host || \" localhost \" } : ${ port } ` console . info ( `Listening at ${ url } /` ) }) const queryString = require ( \" query-string \" ) const verifySlackRequest = require ( \" ./verifySlackRequest \" ) const performSearch = require ( \" ./performSearch \" ) const slashCommand = async ( req , res ) => { if ( verifySlackRequest ( req )) { const message = queryString . parse ( req . body . toString ()) const data = await performSearch ( message , res ) return res . json ( data ) } return res . status ( 401 ). send ( \" request can not be fulfilled \" ) } module . exports = slashCommand const queryString = require ( \" query-string \" ) const verifySlackRequest = require ( \" ./verifySlackRequest \" ) const performSearch = require ( \" ./performSearch \" ) const slashCommand = async ( req , res ) => { if ( verifySlackRequest ( req )) { const message = queryString . parse ( req . body . toString ()) const data = await performSearch ( message , res ) return res . json ( data ) } return res . status ( 401 ). send ( \" request can not be fulfilled \" ) } module . exports = slashCommand const crypto = require ( \" crypto \" ) const verifySlackRequest = ( req ) => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const timestamp = req . headers [ \" x-slack-request-timestamp \" ] const headerSignature = req . headers [ \" x-slack-signature \" ] const basestring = `v0: ${ timestamp } : ${ req . body . toString ()} ` const hash = crypto . createHmac ( \" sha256 \" , SLACK_SIGNING_SECRET ) . update ( basestring ) . digest ( \" hex \" ) const calculatedSignature = `v0= ${ hash } ` return headerSignature === calculatedSignature } module . exports = verifySlackRequest const crypto = require ( \" crypto \" ) const verifySlackRequest = ( req ) => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const timestamp = req . headers [ \" x-slack-request-timestamp \" ] const headerSignature = req . headers [ \" x-slack-signature \" ] const basestring = `v0: ${ timestamp } : ${ req . body . toString ()} ` const hash = crypto . createHmac ( \" sha256 \" , SLACK_SIGNING_SECRET ) . update ( basestring ) . digest ( \" hex \" ) const calculatedSignature = `v0= ${ hash } ` return headerSignature === calculatedSignature } module . exports = verifySlackRequest const mockData = [ { \" id \" : 0 , \" title \" : \" http cat 300 \" , \" url \" : \" https://http.cat/300 \" }, { \" id \" : 1 , \" title \" : \" http cat 301 \" , \" url \" : \" https://http.cat/301 \" }, { \" id \" : 2 , \" title \" : \" http cat 302 \" , \" url \" : \" https://http.cat/302 \" }, { \" id \" : 3 , \" title \" : \" http cat 303 \" , \" url \" : \" https://http.cat/303 \" }, { \" id \" : 4 , \" title \" : \" http cat 304 \" , \" url \" : \" https://http.cat/304 \" }, { \" id \" : 5 , \" title \" : \" http cat 400 \" , \" url \" : \" https://http.cat/400 \" }, { \" id \" : 6 , \" title \" : \" http cat 401 \" , \" url \" : \" https://http.cat/401 \" }, { \" id \" : 7 , \" title \" : \" http cat 402 \" , \" url \" : \" https://http.cat/402 \" }, { \" id \" : 8 , \" title \" : \" http cat 403 \" , \" url \" : \" https://http.cat/403 \" }, { \" id \" : 9 , \" title \" : \" http cat 404 \" , \" url \" : \" https://http.cat/404 \" } ] const createMessageWithAttachments = ( searchText , messageAttachments , buttonValues ) => { const text = `Found ${ messageAttachments . length } document(s) matching _* ${ searchText } *_` let attachments = [] if ( messageAttachments . length > 0 ) { attachments = messageAttachments . slice ( 0 , 5 ) attachments . push ({ title : ' Click one the below button(s) to post link to the channel ' , callback_id : ' single_link_btn ' , actions : buttonValues . map (({ title , url }, index ) => ({ name : ' results ' , text : `Link ${ index + 1 } ` , type : ' button ' , style : ' primary ' , value : ` ${ url } ?title= ${ title } ` })) }) } return { text , attachments } } const noResultsMessage = ( searchText ) => ({ text : `no results found matching _* ${ searchText } *_` }) const performSearch = ( message ) => { const searchText = message . text const matchingDocs = mockData . filter (( item ) => item . title . includes ( searchText )) const fields = matchingDocs . map (( doc , index ) => ({ title : ` ${ index + 1 } . ${ doc . title } ` , title_link : ` ${ doc . url } ` })) const buttonValues = matchingDocs . map (( doc ) => { const { title } = doc return { title , url : ` ${ doc . url } ?title= ${ encodeURIComponent ( title )} ` } }) if ( fields . length > 0 ) { return Promise . resolve ( createMessageWithAttachments ( searchText , fields , buttonValues )) } return Promise . resolve ( noResultsMessage ( searchText )) } module . exports = performSearch const mockData = [ { \" id \" : 0 , \" title \" : \" http cat 300 \" , \" url \" : \" https://http.cat/300 \" }, { \" id \" : 1 , \" title \" : \" http cat 301 \" , \" url \" : \" https://http.cat/301 \" }, { \" id \" : 2 , \" title \" : \" http cat 302 \" , \" url \" : \" https://http.cat/302 \" }, { \" id \" : 3 , \" title \" : \" http cat 303 \" , \" url \" : \" https://http.cat/303 \" }, { \" id \" : 4 , \" title \" : \" http cat 304 \" , \" url \" : \" https://http.cat/304 \" }, { \" id \" : 5 , \" title \" : \" http cat 400 \" , \" url \" : \" https://http.cat/400 \" }, { \" id \" : 6 , \" title \" : \" http cat 401 \" , \" url \" : \" https://http.cat/401 \" }, { \" id \" : 7 , \" title \" : \" http cat 402 \" , \" url \" : \" https://http.cat/402 \" }, { \" id \" : 8 , \" title \" : \" http cat 403 \" , \" url \" : \" https://http.cat/403 \" }, { \" id \" : 9 , \" title \" : \" http cat 404 \" , \" url \" : \" https://http.cat/404 \" } ] const createMessageWithAttachments = ( searchText , messageAttachments , buttonValues ) => { const text = `Found ${ messageAttachments . length } document(s) matching _* ${ searchText } *_` let attachments = [] if ( messageAttachments . length > 0 ) { attachments = messageAttachments . slice ( 0 , 5 ) attachments . push ({ title : ' Click one the below button(s) to post link to the channel ' , callback_id : ' single_link_btn ' , actions : buttonValues . map (({ title , url }, index ) => ({ name : ' results ' , text : `Link ${ index + 1 } ` , type : ' button ' , style : ' primary ' , value : ` ${ url } ?title= ${ title } ` })) }) } return { text , attachments } } const noResultsMessage = ( searchText ) => ({ text : `no results found matching _* ${ searchText } *_` }) const performSearch = ( message ) => { const searchText = message . text const matchingDocs = mockData . filter (( item ) => item . title . includes ( searchText )) const fields = matchingDocs . map (( doc , index ) => ({ title : ` ${ index + 1 } . ${ doc . title } ` , title_link : ` ${ doc . url } ` })) const buttonValues = matchingDocs . map (( doc ) => { const { title } = doc return { title , url : ` ${ doc . url } ?title= ${ encodeURIComponent ( title )} ` } }) if ( fields . length > 0 ) { return Promise . resolve ( createMessageWithAttachments ( searchText , fields , buttonValues )) } return Promise . resolve ( noResultsMessage ( searchText )) } module . exports = performSearch const { createMessageAdapter } = require ( ' @slack/interactive-messages ' ) const querystring = require ( ' query-string ' ) module . exports = () => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const slackInteractions = createMessageAdapter ( SLACK_SIGNING_SECRET ) slackInteractions . action ({ callbackId : ' single_link_btn ' , type : ' button ' }, ( payload , respond ) => { const { actions , channel } = payload const url = actions [ 0 ]. value . split ( ' ?title= ' )[ 0 ] const title = decodeURIComponent ( actions [ 0 ]. value . split ( ' ?title= ' )[ 1 ]) respond ({ replace_original : true , response_type : ' in_channel ' , delete_original : true , text : ' see this link ' , attachments : [ { color : ' #36a64f ' , title , title_link : url } ] }) }) return slackInteractions } const { createMessageAdapter } = require ( ' @slack/interactive-messages ' ) const querystring = require ( ' query-string ' ) module . exports = () => { const SLACK_SIGNING_SECRET = process . env . SLACK_SIGNING_SECRET const slackInteractions = createMessageAdapter ( SLACK_SIGNING_SECRET ) slackInteractions . action ({ callbackId : ' single_link_btn ' , type : ' button ' }, ( payload , respond ) => { const { actions , channel } = payload const url = actions [ 0 ]. value . split ( ' ?title= ' )[ 0 ] const title = decodeURIComponent ( actions [ 0 ]. value . split ( ' ?title= ' )[ 1 ]) respond ({ replace_original : true , response_type : ' in_channel ' , delete_original : true , text : ' see this link ' , attachments : [ { color : ' #36a64f ' , title , title_link : url } ] }) }) return slackInteractions } posted on October 12, 2018 by Prasanna Crasta Prasanna Crasta", "date": "2018-10-12"},
{"website": "Monsanto", "title": "Formatting with Prettier", "author": ["\n                                        Thomas Atkins\n                                    "], "link": "http://engineering.monsanto.com/2018/09/26/using-prettier/", "abstract": "We first started using Prettier on our team a few months ago for one of our primary projects. This was kind of a trial run to see if we liked it. After a while of having it automatically format the code on our main project, I began to miss it when I needed to make changes in our other projects. The value was quite clear. With automatic formatting, there are no more time consuming discussions about formatting: spacing, semi-colons, line breaks, etc. There are not a lot of options to customize formatting with Prettier since it is an opinionated formatter. That’s a good thing. Once the team agrees on a few formatting choices, it is just done automatically. Plus, we can have it set up to format on each commit so the repo is consistently formatted. Code reviews become easier to read, without a lot of extra formatting differences obscuring the real point of the pull request. The documentation is excellent. There are great explanations for the ‘what’, ‘why’ and ‘how’ of Prettier. They also document several possibilities to get it integrated into your project, so you will probably find something to meet your project’s needs. Setup is very simple. I’ll go over the configuration we used. npm install --save-dev --save-exact prettier The save-exact is intentional as even minor versions will change the formatting. npm install --save-dev husky pretty-quick Husky is there for the pre-commit git hook, so that all files committed to the repo can be formatted automatically. By adding this as a dependency, everyone developing on the project will have the same git hook. Nice! Pretty Quick is used to run Prettier on only changed and staged files. It is much quicker than formatting the whole project each time. And it would also allow you to apply changes gradually across the project if you wish. To use these tools we added these scripts to our package.json : {\n  \"scripts\": {\n    \"precommit\": \"pretty-quick --staged\",\n    \"postcommit\": \"git update-index -g\",\n    \"pretty\": \"prettier --write \\\"./**/*.{js,jsx,json}\\\"\",\n  }\n} I also found I needed the postcommit script, which wasn’t on the Prettier docs. When using the command line for git commits, this wasn’t necessary. But our team mostly uses the IntelliJ IDEA editor, and without this postcommit the IDE wouldn’t recognize the current git state after husky and pretty-quick did their thing. The pretty npm script isn’t strictly necessary, but I found it handy to run Prettier on the entire project at once. Prettier can also format css, less, sass, and md files. So you might be interested in including other file types in the glob pattern that I used above. Next, set up the configuration. There are already some sensible defaults, that may just work for your tastes. So you might not even need this step. But you may find you want to customize with the options Prettier provides. I chose to configure with just a .prettierrc.js file. You can also use json or yaml files if you prefer. Our current configuration: module.exports = {\n    printWidth: 120,\n    singleQuote: true,\n    trailingComma: 'es5',\n    semi: false,\n    tabWidth: 4,\n    bracketSpacing: false,\n    jsxBracketSameLine: true,\n} Pretty straight forward. There are a few more customizable options, that are well documented . I also threw in a .prettierignore , which works as you’d expect - ignores specified files from being formatted. If you are already using eslint for code quality rules, there are modules that make that integration possible. Prettier can  handle your formatting rules, while eslint can continue to apply your existing code quality rules. This is documented on their site as well. Also there looks to be great integration with the popular VS Code and Atom editors, which makes it easy to save and format as you go. So really that’s about it. It came down to a couple of installs, a few npm scripts in the package.json, and an optional config file. In the end I’m really happy with the results and impressed at how easy it is to set up. You should give it a try, I think you’ll like it! posted on September 26, 2018 by Thomas Atkins ← Previous Post Next Post → {\n  \"scripts\": {\n    \"precommit\": \"pretty-quick --staged\",\n    \"postcommit\": \"git update-index -g\",\n    \"pretty\": \"prettier --write \\\"./**/*.{js,jsx,json}\\\"\",\n  }\n} {\n  \"scripts\": {\n    \"precommit\": \"pretty-quick --staged\",\n    \"postcommit\": \"git update-index -g\",\n    \"pretty\": \"prettier --write \\\"./**/*.{js,jsx,json}\\\"\",\n  }\n} module.exports = {\n    printWidth: 120,\n    singleQuote: true,\n    trailingComma: 'es5',\n    semi: false,\n    tabWidth: 4,\n    bracketSpacing: false,\n    jsxBracketSameLine: true,\n} module.exports = {\n    printWidth: 120,\n    singleQuote: true,\n    trailingComma: 'es5',\n    semi: false,\n    tabWidth: 4,\n    bracketSpacing: false,\n    jsxBracketSameLine: true,\n} posted on September 26, 2018 by Thomas Atkins Thomas Atkins", "date": "2018-09-26"},
{"website": "Monsanto", "title": "Automated Regression Testing", "author": ["\n                                        Ben Frey\n                                    "], "link": "http://engineering.monsanto.com/2018/08/21/automated-regression-testing/", "abstract": "Over the lifespan of a large product, how are you ensuring that your existing code still functions as you expect? How many of those scenarios are you having to cover with manual testing? My team has been utilizing Cucumber for many years to handle our regression testing for back-end services. More recently, we started using Selenium to do UI regression testing. Using these tools has enabled us to do just minimal manual testing each sprint while increasing the chances that we will find negative impacts to other parts of the application. Cucumber and Selenium both have Node libraries available with decent documentation. In this post, I want to walk through a simple example to show how you might test a UI interaction using both libraries. Project Setup We’ll set up our Node.js project with a structure like this: /features\n    google.feature\n    /step_definitions\n        googleSteps.js\n    /support\n        worldConstructor.js\npackage.json The Cucumber documentation recommends your code live within /features and any sub directories. Before we can write any tests, we need a browser to run them in, and something to drive the process. Cucumber provides hooks for running logic at certain events during your tests.  We want to initialize our chromedriver before any tests are run, so we will use the BeforeAll hook.  Afterwards we need to shut down the driver. require ( ' chromedriver ' ); const { BeforeAll , AfterAll } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); }); AfterAll ( function () { return driver . quit (); }); We’ve started up our driver, but we need a way for the steps we are going to write to use the driver. In Cucumber.js, a World is an object available as this to all of our step functions. Let’s make the driver available in the World and use the synchronous call setWorldConstructor to put our World in scope: require ( ' chromedriver ' ); const { BeforeAll , AfterAll , setWorldConstructor } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); function World ( driver ) { this . driver = driver ; } let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); setWorldConstructor ( World . bind ( undefined , driver )); }); AfterAll ( function () { return driver . quit (); }); Let’s try running it and we should see the driver quickly open and close. Cucumber can be run directly with ./node_modules/.bin/cucumber-js or via node with node node_modules/cucumber/bin/cucumber.js This is the expected console output at this point: Writing a feature file Let’s test something that we’ve probably all done at one time or another: a Google search. First we’ll specify our feature name and tag it: @all\nFeature: Google Search Tags allow you to specify a scope for features and scenarios. A common pattern my team has used is to switch between API and UI specific tests. Tags are optional and can be added at the feature level and for each scenario individually. You can specify which tags you’d like to run (or not run) by appending an option like this when you run Cucumber: --tags \"@all and not @ignore\" For the search, we want to enter some text then click the search button. Due to the dynamic nature of search results, we’ll just verify that the browser navigates to a different URL and that the search result count indicates multiple results, rather than look for a specific hit. Features are written in a BDD Given -> When -> Then format. Given steps are used to set up a test scenario by setting up data or getting the website to a specific state. When steps are actions that a user would take in the test scenario. Then steps are the verifications/assertions of the state after the action is taken. Any Given , When , or Then after the first becomes an And step to make it more readable. For this feature file, any tests we write will likely have navigation to google.com as the first step, regardless of what other components we are testing. Cucumber provides a Background header for this purpose.  Any steps under Background will be run for every Scenario in the file. @all\nFeature: Google Search\n\n  Background: load google homepage\n    Given the user has navigated to the Google search homepage\n\n  Scenario: conduct search\n    When the user types cucumber in the search bar\n    And the user clicks Google Search\n    Then the browser navigates to the search result page\n    And the search result has more than 1 hit Running Cucumber again indicates that it picked up our scenario, but it couldn’t find any step definitions. It also provides an example snippet that can be used directly to write our step definitions. Writing step definitions The snippet that the Cucumber runner provided is a good start, but since Selenium WebDriver deals in promises we can omit the callback and return our promises to Cucumber. Also, we will want the ability to capture variables from the step text, enabled by using regular expressions in place of strings in our step definitions. We don’t need a variable for this first step, but rather than switch between string and regex matchers for different steps, let’s stick to regex. For our first step (navigating to Google search page) we can use the Selenium WebDriver get method. const { Given } = require ( ' cucumber ' ); Given ( /^the user has navigated to the Google search homepage$/ , function () { return this . driver . get ( ' https://www.google.com ' ); }); When Cucumber matches your feature steps to step definitions, it will ignore the first word. Given , When , Then are just proxies of the same internal method in Cucumber. Moving through our feature file, the next unimplemented step is for typing in the search bar. To do that, we need to locate the search bar in the DOM. We could use the Selenium WebDriver’s findElement method, but Cucumber would move from the page load step to the search bar step without waiting for page to completely resolve. If the search bar wasn’t loaded in time, our second step would fail. A good practice in your Selenium steps is to always make sure an element is available before trying to interface with it. I recommend doing this even for steps that are after other verification steps ( Then s), as you may later re-order your steps or use them in different combinations in other tests. Instead of findElement , we can use wait(until.elementLocated) which will wait until the specifed element is located or a set amount of time has passed. If the element is located before the timeout occurs, wait will return that element in a WebElementPromise. With the element located, we can then invoke sendKeys . Since we may want to switch up what gets typed into the search bar for different tests, let’s use a regex capture group. Our step function then takes the captured variables as parameters in the order they appear in the regex, followed by a data table if you have one, then a callback if you use the callback interface. const { When } = require ( ' cucumber ' ); /* step function takes the captured variables as parameters in the order they appear in the  */ /* regex, then a data table if you have one, then a callback if using the callback interface. */ When ( /^the user types ( .+ ) in the search bar$/ , function ( searchText ) { /* wait a max time of 5 seconds. */ return driver . wait ( until . elementLocated ({ xpath : `//input[@title='Search']` }), 5000 ) /* sendKeys takes an argument list of strings and/or Keys which will get sent in the */ /* sequence provided. Strings will get broken down into individual characters. */ . then ( element => element . sendKeys ( searchText )); }); The element locator can be a webdriver By or a shorthand object that By understands. I prefer XPath locators due to their flexibilty and readability, but By supports CSS selectors as well. The Elements tab in Chrome Developer Tools allows you to search the DOM by XPath and is a good place to test your locator strings. Our next step is to click on the search button. Like the previous step, let’s make the step more generic by making the button text be a variable. Let’s refactor the wait call into a separate function since we will want to re-use it here to find the button. We can then call click on the located button. /* `this.driver` is only available within our Cucumber functions, so we must pass the driver */ /* as a parameter to helper functions. */ const locateElement = ( driver , elementLocator ) => { return driver . wait ( until . elementLocated ( elementLocator ), 5000 , ' Element not found within time limit ' ); }; When ( /^the user clicks ( .+ ) $/ , function ( buttonValue ) { return locateElement ( this . driver , { xpath : `//input[@type='button'][@value=' ${ buttonValue } ']` }). click (); }); Our first verification is that the browser ends up on the search result page. We can use getCurrentUrl and an assertion library like chai to do the check. const { expect } = require ( ' chai ' ); const { Then } = require ( ' cucumber ' ); Then ( /^the browser navigates to the search result page$/ , function () { return this . driver . getCurrentUrl () . then ( url => expect ( url ). to . have . string ( `https://www.google.com/search` )); }); Finally, we want to check that the results page indicates at least a certain number of hits. getText will return all text contained within that element and its children. Then ( /^the search result has more than (\\d + ) hit$/ , function ( minCount ) { return locateElement ( this . driver , { xpath : `//div[@id='resultStats']` }). getText () . then (( text ) => { const resultCount = parseInt ( text . replace ( /About ([\\d , ] + ) results.+/ , ' $1 ' ). replace ( /,/ , '' ), 10 ); expect ( resultCount ). to . be . above ( minCount ); }); }); Running the Cucumber now should yield a passing scenario. posted on August 21, 2018 by Ben Frey ← Previous Post Next Post → /features\n    google.feature\n    /step_definitions\n        googleSteps.js\n    /support\n        worldConstructor.js\npackage.json /features\n    google.feature\n    /step_definitions\n        googleSteps.js\n    /support\n        worldConstructor.js\npackage.json require ( ' chromedriver ' ); const { BeforeAll , AfterAll } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); }); AfterAll ( function () { return driver . quit (); }); require ( ' chromedriver ' ); const { BeforeAll , AfterAll } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); }); AfterAll ( function () { return driver . quit (); }); require ( ' chromedriver ' ); const { BeforeAll , AfterAll , setWorldConstructor } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); function World ( driver ) { this . driver = driver ; } let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); setWorldConstructor ( World . bind ( undefined , driver )); }); AfterAll ( function () { return driver . quit (); }); require ( ' chromedriver ' ); const { BeforeAll , AfterAll , setWorldConstructor } = require ( ' cucumber ' ); const seleniumWebdriver = require ( ' selenium-webdriver ' ); function World ( driver ) { this . driver = driver ; } let driver ; BeforeAll ( function () { driver = new seleniumWebdriver . Builder () . forBrowser ( ' chrome ' ) . build (); setWorldConstructor ( World . bind ( undefined , driver )); }); AfterAll ( function () { return driver . quit (); }); @all\nFeature: Google Search @all\nFeature: Google Search @all\nFeature: Google Search\n\n  Background: load google homepage\n    Given the user has navigated to the Google search homepage\n\n  Scenario: conduct search\n    When the user types cucumber in the search bar\n    And the user clicks Google Search\n    Then the browser navigates to the search result page\n    And the search result has more than 1 hit @all\nFeature: Google Search\n\n  Background: load google homepage\n    Given the user has navigated to the Google search homepage\n\n  Scenario: conduct search\n    When the user types cucumber in the search bar\n    And the user clicks Google Search\n    Then the browser navigates to the search result page\n    And the search result has more than 1 hit const { Given } = require ( ' cucumber ' ); Given ( /^the user has navigated to the Google search homepage$/ , function () { return this . driver . get ( ' https://www.google.com ' ); }); const { Given } = require ( ' cucumber ' ); Given ( /^the user has navigated to the Google search homepage$/ , function () { return this . driver . get ( ' https://www.google.com ' ); }); const { When } = require ( ' cucumber ' ); /* step function takes the captured variables as parameters in the order they appear in the  */ /* regex, then a data table if you have one, then a callback if using the callback interface. */ When ( /^the user types ( .+ ) in the search bar$/ , function ( searchText ) { /* wait a max time of 5 seconds. */ return driver . wait ( until . elementLocated ({ xpath : `//input[@title='Search']` }), 5000 ) /* sendKeys takes an argument list of strings and/or Keys which will get sent in the */ /* sequence provided. Strings will get broken down into individual characters. */ . then ( element => element . sendKeys ( searchText )); }); const { When } = require ( ' cucumber ' ); /* step function takes the captured variables as parameters in the order they appear in the  */ /* regex, then a data table if you have one, then a callback if using the callback interface. */ When ( /^the user types ( .+ ) in the search bar$/ , function ( searchText ) { /* wait a max time of 5 seconds. */ return driver . wait ( until . elementLocated ({ xpath : `//input[@title='Search']` }), 5000 ) /* sendKeys takes an argument list of strings and/or Keys which will get sent in the */ /* sequence provided. Strings will get broken down into individual characters. */ . then ( element => element . sendKeys ( searchText )); }); /* `this.driver` is only available within our Cucumber functions, so we must pass the driver */ /* as a parameter to helper functions. */ const locateElement = ( driver , elementLocator ) => { return driver . wait ( until . elementLocated ( elementLocator ), 5000 , ' Element not found within time limit ' ); }; When ( /^the user clicks ( .+ ) $/ , function ( buttonValue ) { return locateElement ( this . driver , { xpath : `//input[@type='button'][@value=' ${ buttonValue } ']` }). click (); }); /* `this.driver` is only available within our Cucumber functions, so we must pass the driver */ /* as a parameter to helper functions. */ const locateElement = ( driver , elementLocator ) => { return driver . wait ( until . elementLocated ( elementLocator ), 5000 , ' Element not found within time limit ' ); }; When ( /^the user clicks ( .+ ) $/ , function ( buttonValue ) { return locateElement ( this . driver , { xpath : `//input[@type='button'][@value=' ${ buttonValue } ']` }). click (); }); const { expect } = require ( ' chai ' ); const { Then } = require ( ' cucumber ' ); Then ( /^the browser navigates to the search result page$/ , function () { return this . driver . getCurrentUrl () . then ( url => expect ( url ). to . have . string ( `https://www.google.com/search` )); }); const { expect } = require ( ' chai ' ); const { Then } = require ( ' cucumber ' ); Then ( /^the browser navigates to the search result page$/ , function () { return this . driver . getCurrentUrl () . then ( url => expect ( url ). to . have . string ( `https://www.google.com/search` )); }); Then ( /^the search result has more than (\\d + ) hit$/ , function ( minCount ) { return locateElement ( this . driver , { xpath : `//div[@id='resultStats']` }). getText () . then (( text ) => { const resultCount = parseInt ( text . replace ( /About ([\\d , ] + ) results.+/ , ' $1 ' ). replace ( /,/ , '' ), 10 ); expect ( resultCount ). to . be . above ( minCount ); }); }); Then ( /^the search result has more than (\\d + ) hit$/ , function ( minCount ) { return locateElement ( this . driver , { xpath : `//div[@id='resultStats']` }). getText () . then (( text ) => { const resultCount = parseInt ( text . replace ( /About ([\\d , ] + ) results.+/ , ' $1 ' ). replace ( /,/ , '' ), 10 ); expect ( resultCount ). to . be . above ( minCount ); }); }); posted on August 21, 2018 by Ben Frey Ben Frey", "date": "2018-08-21"},
{"website": "Monsanto", "title": "Integrating Jira with GitHub", "author": ["\n                                        Celinton Rayen\n                                    "], "link": "http://engineering.monsanto.com/2018/05/21/jira-git-integration/", "abstract": "Most software development is successful only with collaborative effort (Developers, Product Manager/Owner, Business Stakeholders and the list grows). And so is the collaboration of the tools used in the process. Agree?\nAlright, I have seen teams start their sprint cycle on the kanban/scrum board with great energy and enthusiasm. Developers are sometimes too busy working on the complex stories/tasks, that they  forget to switch the workflow status of the story on the board. On the last day of the sprint, magically the story would have been closed. Really? Also, in some instances, the developers commit their work in GitHub repo, move the story to DONE in the kanban board. What is the problem here? I foresee couple of challenges: The reports might not reflect the actual effort during the sprint. The story/task would have been marked as Done, though the VCS might not reflect the same. (As there is no link between GitHub and Jira)\nLet me take you through how Jira and GitHub together make our life simpler during the development process. Linking Jira with GitHub Jira connects to GitHub using OAuth. With that said, it is a simple two step process that makes this successful. Pre-requisite - Install the DVCS connector (if missing) For this, you should be an admin in Jira. If not, please approach a Jira admin. To install this,\n      1. Click on the administrative settings in the toolbar.\n      2. Choose add-ons.\n      3. Search for DVCS connector and install. 1. Register OAuth application in GitHub. Open the account/org in GitHub which is the owner of the repository you\n    want to link in Jira. (For this, you should be the owner of\n    the account/team/org)\n      1. Goto settings Tab.\n      2. Click OAuth Applications.\n      3. Click Register a new application.\n      4. Provide the details.\n      5. In the homepage URL and Autorization callback URL,\n         enter your Jira homepage url.\n      6. Once you click the Register application . This will generate a Client ID and Client Token . Keep this handy we will use it shortly. 2. Configure GitHub for Jira 1. Login as an admin in Jira\n  2. Click development tools\n  3. Select GitHub/Enterprise (whatever your VCS is).\n  4. In the Host URL section, enter your GitHub base url.\n  4. Enter your team name.\n  5. Provide the Client ID & Secret we generated in the earlier step.\n  6. Make sure both Auto link and Enable smart commits are checked.\n  7. Save the changes. Now your account would start the sync up process.\nNOTE : Make sure you create the OAuth client ID & secret under the account which owns the repository. Step 6 in Configure GitHub for Jira is very important.\nNow the setup is complete. Let’s see how we can be smarter with smart commits .\nSmart commits are the secret to making life easier. Smart commits are none other than our regular commits with some smart tags. Whenever a developer commits his/her work, he/she logs the commit messages in a given format. For smart commits, listed below are few frequently used formats. Create a branch in the repo with the story id prefixed. Format: < STORY-ID > Branch_name Example: PLAY-114_Create_User_Module\nwhere PLAY-114 is the story ID. Committing the changes. Format: < STORY-ID > #< workflow-stage > #comment < Commit_message >\nExample: git -commit -m \"PLAY-146 #in-progress #comment Test commit for Jira hooks\"\nThe above example would map the commit to PLAY-146, move the story\nto In Progress stage, and log a\ncomment in the story Test commit for Jira hooks Creating pull requests. Once the pull request is created for the linked story, the details of the pull requests, should be available in Jira. NOTE : While creating the pull request, the smart commit #review would\n automatically switches the story to Review stage in Jira board. Conclusion With the integration of two great tools, it saves a huge time and effort in tracking the product development. It also makes things organized. Happy Integrating! posted on May 21, 2018 by Celinton Rayen ← Previous Post Next Post → posted on May 21, 2018 by Celinton Rayen Celinton Rayen", "date": "2018-05-21"},
{"website": "Monsanto", "title": "The Marvels of Webpack 4", "author": ["\n                                        Håkon Graham Dahlsveen\n                                    "], "link": "http://engineering.monsanto.com/2018/05/18/webpack4/", "abstract": "Webpack 4 (The Quick and Dirty) The release of Webpack 4 (dubbed Legato) brings several exciting changes ,\nthe biggest being that Node 4 is no longer supported, and development has shifted to support the newer ES6 specification.\nES6 has better optimization with V8, which results in quite a nice boost in performance (numbers below!).\nIn addition, the developers of Webpack boast that v4 is now a zero configuration bundling tool!\nThis upgrade is huge win because, in the past, one of the biggest pain points in getting started with Webpack was setting up the webpack.config.js file. But with the new zero configuration model, webpack is delivering a bunch of niceties out of the box, ready to go. They’ve completely removed the need for CommonsChunkPlugin and now deliver their own solution to handle shared chunks automatically for the user. Production & Development Wave buh-bye to having split config files with 2 different sets of plugins for production and development.\nPreviously, a common solution to not uglifying the code and getting it ready for production was to effectively\nhave 2 different webpack.config files with different sets of plugins. This has been replaced with the new mode (–mode) flag. Now you simply have to add these scripts to your package.json: webpack --mode production\nwebpack --mode development These scripts will both generate a bundled file (main.js), but the production flag will do a slew of optimization (hoisting, tree shaking, minification) on your code and modules. Development mode on the other hand is not minified, and is optimized for speed rather than bundle size. What They Really Mean When the developers boast that Webpack 4 is zero config, they really mean that you don’t HAVE to have a config file in order to bundle your javascript code. But, in reality not all browsers know how to handle all of ES6 yet, and if you’re using react and .jsx you still have to introduce a config file with the babel-loader. Another important change in v4 is that the entry and output points are now set to the project’s ./src/index.js and ./dist/main.js folders respectively by default. This can be overridden by making the following changes to the scripts property of your package.json scripts: webpack --mode <environment> <entry path> --output <output path> Metrics Now for the juicy data (probably all you cared about).\nAn important note: Previously, the project was not using gzip compression, but with the migration to Webpack 4 I added it to the config. Before (Webpack 3): After (Webpack 4): The results are output in different UoMs (Webpack changed their output to the IEC binary standard). This means Webpack\n4 displays the bundle size in MiB rather than MB, so (2^20 vs 10^6 bytes). This ultimately results in a difference in \nbundle size of ~148KiB between the main.js and bundle.js files you see above. As you can see, the performance boost of webpack 4 is pretty nice! Bundling time is significantly reduced. NB: There are many variables that affect bundling time, so take these results with a grain of salt, but, many other users have reported significantly faster bundle times .\nAlso evident from the output is how much work gzip is doing for us in reducing the bundle size to 317KiB from the 1.14MiB bundle, that’s a vast amount of space saved. Hidden Gems While playing with webpack 4, and now questioning the sizes of everything on the OS, I found a nifty plugin which displays the contents of the bundle as a size map! (I know, it’s pretty awesome, and you would love to see it). So there it is! All the stuff in the bundle. Not surprisingly most of the bundle size is from node_modules. Perhaps more surprisingly was the size of the moment module, seeing as none of my team members remembered ever using moment for anything. It turns out moment is a sub dependency of an internal monsanto navigation bar module. Recap Webpack 4 is fast Webpack 4 is zero config (erhm..) Webpack 4 –mode makes your .config super clean gzip saves you a bunch of space webpack-bundle-analyzer gives you insight into what’s in the bundle posted on May 18, 2018 by Håkon Graham Dahlsveen ← Previous Post Next Post → webpack --mode production\nwebpack --mode development webpack --mode production\nwebpack --mode development webpack --mode <environment> <entry path> --output <output path> webpack --mode <environment> <entry path> --output <output path> posted on May 18, 2018 by Håkon Graham Dahlsveen Håkon Graham Dahlsveen", "date": "2018-05-18"},
{"website": "Monsanto", "title": "DevOops!", "author": ["\n                                        Martin Gorostegui\n                                    "], "link": "http://engineering.monsanto.com/2018/04/23/devoops/", "abstract": "It all started on a Thursday evening. After months of hard work, we were having the first release for the brand-new version of one of our finance-related products, developed with open source, event-driven microservices, microfrontends, cloud, HCD, agile and DevOps as key principles. With this effort, we’re gradually replacing an actively used Java monolith launched in 2012, so for the first release we decided to include some basic non-core features, together with complex changes made on the authentication flows in order to allow a seamless single sign-on experience when moving between the monolith and the cloud product. First issue Since a monolith deployment was needed for this release, production artifacts deployment was planned for 7 PM in order to avoid impacting end users. At around 6 PM first issue was discovered, there was a typo in a URL defined in one of the monolith properties files.  Correcting that value would have required generating a new ear file and postponing the release due to the approval process that’s in place for that. Luckily for us, the mistyped URL was pointing to an entry in an API gateway that was created with self-service support, so we were able to quickly create a temporary new route in order to match the mistyped URL and avoid any further impact. Second issue At around 9 PM, the deployment was completed, and smoke tests didn’t bring any bad news.  But the following morning, things changed. One of the basic non-core features we were taking to production is a user creation request screen that triggers an email (through a monolith API) to a group of persons that analyze the request and decide on it. Well, that group reported they were not getting any requests. That was unusual and an indication that the new screen was probably not working as expected. We started troubleshooting by analyzing the network traffic from the client side and quickly realized that although the email was not arriving as expected, the request was returning without errors to end users, giving the wrong impression that it was being processed correctly. From here we would have to analyze API gateway and monolith logs, and maybe even take actions at the email server. But first we wanted to avoid future requests from being sent until we had fixed the issue. So we made use of one of the benefits we get with the new product’s development model and quickly created and deployed a static maintenance screen for just this function, without impacting any other features. Afterward, we decided to explore the API gateway monitoring capabilities and, once again, this piece of software saved us. We were able to: See the past requests’ traces Identify the root cause of the issue (an API misconfiguration causing requests to be forwarded to an undesired proxy which was returning a login form html content as response body) Save the non-arriving user creation requests for reprocessing. Third (and last) issue By noon, a third unexpected problem appeared. There were reports of failed logins with the new authentication flows, and members of our dev teams were also receiving alarms from the centralized logging solution we have in place about HTTP errors when contacting the identity provider. We ran some searches from the log viewer web console, performed an analysis (1 out of 3 logins attempts were failing, only in production, for all kind of users and from all the app server cluster nodes) and then shared that information with the identity management team.  They detected that one of the identity server cluster’s nodes was down causing login attempts to fail if they were started on a healthy node but the flow continued on the failing node. Retrospective As days went by we ended up with different thoughts and questions in our minds. Reflecting on past years’ experiences we had many reasons to be happy and thankful in relation to operations and infrastructure: We were able to quickly take actions ourselves (no black boxes) and avoid cumbersome release processes, we had an API gateway product created with focus on developers’ needs and a robust centralized logging solution with valuable log statements added in the application’s code. On the other hand, we regretted spending hours fixing issues caused by typos and misconfigurations. Why didn’t we detect those earlier? How can we avoid suffering this in the future? Maybe environment configuration automation could help, as well as externalizing the monolith properties, avoiding monolith dependencies if possible, and not overestimating non-coding configuration tasks during our sprints. But the most disturbing thoughts we started having were related to future possible scenarios. What would we do if the issue was not a simple one in a non-core feature but a complex one in the core of our product? And if we were in peak season with hundreds or even thousands of transactions failing? Would the same troubleshooting steps and tools be enough? The answer to this last question was definitely no. You build it, you run it There’s so much being said about the DevOps term’s definition these days that it may be hard for teams to focus in achieving the right mindset as you get started. For me, after the experience just shared, the You build it, you run it quote summarizes the key philosophy needed to continue improving in that area for our new product’s teams. Designing and prioritizing backlogs has to change when you’re supporting our own deployments in production, and that needs to be understood by dev teams, product owners and business stakeholders as well. In line with my previous words, during the last few months we’ve been putting that quote into practice in different ways: Meeting in order to make sure every artifact’s owner team is clearly defined, and that we all understand the relationship between autonomy, responsibility and ownership. It’s not acceptable to have many teams working in a code but none of them showing ownership for maintaining it. Conversations with dev teams, product owners and business stakeholders in order to adjust backlogs priorities according to DevOps needs. I wish we had pushed for those earlier in order to avoid sprints full of DevOps stories only. Developed screens to be able to easily query our product’s event store and retrying events in case of failures. By doing this we streamline troubleshooting by avoiding searching for logs, opening up ssh tunnels, running sql queries and manually pushing messages. Smart, concise messaging notifications being used on top of basic monitoring in order to be able to detect potential problems as early as possible. It’s great to identify and solve issues before end users tell you about it! Programmatic and automatic remediation flows in order to avoid having to run manual actions when the number of affected items goes up. Just imagine having to spend whole days running manual remediation actions for thousands of items one by one and you’ll agree it’s worth investing here. Also, the chances of running into this scenario increase when working in an async event-driven application as we are. Increased emphasis on valuable code testing for early bug detection and increased confidence in code. We still have lots of room for improvement, but I’m convinced that we’re moving in the right direction, and that’s in part thanks to the events that took place that Thursday evening. posted on April 23, 2018 by Martin Gorostegui ← Previous Post Next Post → posted on April 23, 2018 by Martin Gorostegui Martin Gorostegui", "date": "2018-04-23"},
{"website": "Monsanto", "title": "Client Side Git Hooks - Part 2", "author": ["\n                                        Pat Gaffney\n                                    "], "link": "http://engineering.monsanto.com/2018/03/05/using-client-side-git-hooks-2/", "abstract": "Client Side Git Hooks - Part 2 Last year I wrote about how my team was using client-side git hooks to run sanity checks on our code before we commit. The example from last year was a pre-commit hook designed to perform static analysis on Javascript code using ESLint. One year later and we’re still using them everyday, but we’ve made a few improvements to make the experience a whole lot smoother. Let’s walk through the problems we ran into over the past year, and the solutions we came up with. The Missing Linter Problem The previous version of this script had a run_linter function which directly invoked ESLint: run_linter () { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } What happens if you’ve recently performed the sacred rm -rf node_modules ritual? This is, after all, an ancient tradition for debugging Javascript issues. Well, the node interpreter immediately throws an error because it cannot find the module you asked it to execute. It retreats with a positive exit code, so now you cannot commit until you npm install — assuming you gathered that from the error the node interpreter threw. Not the greatest experience. We can write a simple function to ensure that the linter is installed before attempting to invoke it, and if not, present our colleague with the solution immediately. Bash has a series of file test operators that make this simple enough. While we’re here, let’s stop using node to interpret a *.js file in the node module directly, and instead invoke ESLint through the executable that npm already created for us in the .bin directory . test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then run_linter else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } run_linter () { ./node_modules/.bin/eslint --fix --ext .js,.jsx src/ test / --cache print_outro } This solves our problem well enough, but using the relative path to execute ESLint still feels wrong. Well, if you’ve using npm 5.2 or greater, you might have noticed that an additional binary was installed — npx . npx allows you to run locally-installed packages without relative paths or npm run <script> . We can now safely eliminate that relative path. run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache print_outro } The -q flag tells npx to suppress all of its output — this does not affect the invoked subcommand. Even if you’re not using version 5.2 or greater of npm , you can still install the standalone version of npx . The Unstaged Changes Problem I make a lot of small commits during development. It tends to make managing the historical logistics of a project easier — rebasing, squashing, and cherry-picking commits are all easier to perform with many small commits rather than a few large commits. There are a great many pros to this method of source control — the one downside being that I often still have a dirty git status after staging my next commit. In other words, I’m editing 5 different files, but only 3 of these files are in the staging area ready to be committed. The problem with having unstaged changes is they’re not exempt from our pre-commit hook. If any of these files irritate the linter, we won’t be able to commit. Obviously I only want the linter to analyze the files I’m about to commit, so let’s stash all files with changes not staged before we invoke the linter, then pop that same stash immediately after. # Stash will be named \"pre-commit-YYYY-MM-SS@HH:MM:SS\" STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } # ...run linter... pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } Lets walk through these two functions: git stash push : save the local modifications to a new stash entry. --quiet : suppress all normal output --keep-index : all changes already staged are left intact. --include-untracked : all untracked (new) files are also stashed. git reset --hard --quiet : ensure we have a clean working directory. git stash pop : remove the last stash and apply it to the current working tree. --index : re-instate the index’s changes as well as a working tree’s. --quiet : suppress all normal output To summarize, stash_unchaged stashes all files not currently in the staging area, and pop_unstaged pops that stash off the stack and back into the working directory. They operate as the inverse of each other. As a safety net, we give our stash a predictable name so that we can find it if trouble arises. The Missing Auto-Fix Problem ESLint has a wonderful --fix flag that will fix any errors that it knows how to solve — most formatting errors can be fixed by just applying this flag. But given our current implementation, if the linter performs any auto-fixes, they won’t be applied to our commit. Our files are already in the staging area, so any changes made to these files will not be automatically applied to the index — they must be added manually. To make matters more confusing, our previous git reset --hard (in pop_unstaged ) would erase these changes before popping our stash. We could manually add all changes made by ESLint to the staging area, but what if some random file had an error in it? Once again, we’re only concerned with the files in this commit — we want this hook to operate under the assumption that all errors in other files will be fixed in the upcoming commits. For this, we need the git update-index command. The update-index command is what the git authors would call a plumbing command (lower-level), as opposed to a porcelain command (user-friendly). Most documentation around update-index will inevitably steer you towards using git add — the more user-friendly command for updating the index. But, in this scenario, update-index has a --again flag that is a perfect solution: it updates the index only with unstaged changes made to files already in the index. # ...stash_unchanged... run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache git update-index --again pop_unstaged } Since we’re now running multiple commands after eslint , we should probably merge the run_linter function with the print_outro function so we can properly inspect the exit code of ESLint. # ...stash_unchanged... run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } Putting It All Together Our team uses client-side hooks like this every day. The pre-commit hook is really just the beginning, there are over 15 different git commands that have corresponding hooks. The updated script is listed below. STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" print_intro () { user = $( git config --get user.name ) printf '\\033[1;34m➡ Time to pay the troll toll, %s...\\033[0m\\n' \" $user \" test_for_linter } test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then stash_unchanged else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } print_intro posted on March 5, 2018 by Pat Gaffney ← Previous Post Next Post → run_linter () { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } run_linter () { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then run_linter else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } run_linter () { ./node_modules/.bin/eslint --fix --ext .js,.jsx src/ test / --cache print_outro } test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then run_linter else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } run_linter () { ./node_modules/.bin/eslint --fix --ext .js,.jsx src/ test / --cache print_outro } run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache print_outro } run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache print_outro } # Stash will be named \"pre-commit-YYYY-MM-SS@HH:MM:SS\" STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } # ...run linter... pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } # Stash will be named \"pre-commit-YYYY-MM-SS@HH:MM:SS\" STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } # ...run linter... pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } # ...stash_unchanged... run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache git update-index --again pop_unstaged } # ...stash_unchanged... run_linter () { npx -q eslint --fix --ext .js,.jsx src/ test / --cache git update-index --again pop_unstaged } # ...stash_unchanged... run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } # ...stash_unchanged... run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" print_intro () { user = $( git config --get user.name ) printf '\\033[1;34m➡ Time to pay the troll toll, %s...\\033[0m\\n' \" $user \" test_for_linter } test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then stash_unchanged else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } print_intro STASH_NAME = \"pre-commit- $( date '+%Y-%m-%d@%H:%M:%S' ) \" print_intro () { user = $( git config --get user.name ) printf '\\033[1;34m➡ Time to pay the troll toll, %s...\\033[0m\\n' \" $user \" test_for_linter } test_for_linter () { if [ -x node_modules/.bin/eslint ] ; then stash_unchanged else echo '\\033[1;31m𝙓 You do not have eslint installed!\\033[0m' echo '\\033[1;30m  Run \"npm i\" to install the linter.\\033[0m' exit 1 fi } stash_unchanged () { printf '\\033[1;30m➡ Stashing unstaged changes as %s...\\033[0m;\\n' \" $STASH_NAME \" git stash push --quiet --keep-index --include-untracked \" $STASH_NAME \" run_linter } run_linter () { if npx -q eslint --fix --ext .js,.jsx src/ test / --cache ; then echo '\\033[1;32m✔ Eslint checks out! Pod bay doors opening...\\033[0m' git update-index --again pop_unstaged exit 0 else echo '\\033[1;32m𝙓 Linter threw up! Pod bay doors sealed...\\033[0m' pop_unstaged exit 1 fi } pop_unstaged () { git reset --hard --quiet && git stash pop --index --quiet } print_intro posted on March 5, 2018 by Pat Gaffney Pat Gaffney", "date": "2018-03-05"},
{"website": "Monsanto", "title": "Introduction to React Native", "author": ["\n                                        Kara Waldemer\n                                    "], "link": "http://engineering.monsanto.com/2018/01/11/react-native-intro/", "abstract": "Reposted with permission from The Ginger Techie . React Native is a cross-platform solution to writing native mobile applications. Facebook open sourced React Native in March 2015. They built it because, like many companies, Facebook needed to make their product available in the web as well as on various mobile platforms and it was a struggle to maintain the specialized teams needed to build the different deployables. After trying a number of different techniques, React Native was Facebook’s solution to the problem. What makes React Native Different There are already solutions to create apps for mobile devices: from writing native code in proprietary languages to writing “mobile web apps” or hybrid solutions. So why do developers need another? Why should they give React Native the time of day? Unlike other options available React Native allows developers to write native applications on both iOS and Android using JavaScript with React in a single codebase. It takes the same design principles used by React in the web and lets you write mobile UIs using the familiar component model. In addition, unlike other options that let you use web technologies to create hybrid applications, React Native runs on the device using the same fundamental building blocks used by the platform specific solutions making it a more seamless experience for users. What it does well There are a number of benefits to developing with React Native over other mobile solutions: One Codebase One of the nicest features of React Native is that you can write a mobile app that runs natively on both iOS and Android using a single codebase and, largely, in a single language. Developers can write a single React component using the building blocks provided by React Native and they will run on  different devices using their platforms unique components behind the scenes. The React code that developers build runs on an embedded instance of JavaScriptCore on the device. From there, React Native is able to make calls to the native APIs to create the native version of the component. React Native allows developers to write code once and have it run using native components and styling on both platforms. Native Components There are cases, however, where you might not want to rely on the native styling and you want a more unified look across platforms. In that case, React Native allows you to write style sheets where you can overwrite the defaults. Integration with native components There are instances where, for one reason or another, a developer will need to make use of native components when writing mobile apps. Whether you want to reuse an existing library written in a proprietary language or React Native simply does not provide a solution that works for your use case, you are likely going to run into a case where native code is required. Developers at Facebook had that issue as well and built a solution to it into React Native. React Native allows developers to create JavaScript wrappers which act as bridges to make native libraries and code available for use in the JavaScript code. Native hardware This is a really helpful tool to make available to developers. A thing to remember is that this works on a platform by platform basis. You would have to develop native functionality for each platform in order to make this work properly. Hot Reloading If you have ever done mobile development, one of the biggest frustrations you have probably faced is constant recompiling. Whenever you make a change to any code or change where a UI element is located, you have to wait for the project to completely recompile before you can check your changes. React Native fixes this issue. It takes the ability to hot reload that all web developers are familiar with and makes it available on the mobile simulators and emulators. So now with the click of a few keys, developers are able to view their changes as they make them. Hot reload An important thing to keep in mind though is that if you change any of the native configurations or code, you will have to recompile. Since most of your code can be written in JavaScript this is usually a much less painful process than when writing a purely native solution. What still needs work As with most technologies that make developers lives easier, React Native is not without its pitfalls. Performance The biggest complaint I have seen from developers about React Native is that there can be some real performance issues when developing with it. Facebook has made a lot of progress in improving the performance but it is still something to keep in mind when exploring options. The crux of the issue boils down to the communication between the APIs controlling the underlying hardware and the JavaScript running on JavaScriptCore. This comes with some overhead because it needs to cross the thread boundary. The more times you have to make that journey, the more overhead is incurred. Since it is built using React, changes can be made in the virtual DOM and only sent to the native API for rendering when all changes are complete. The problem of over communication is largely mitigated through this model. However, there are still cases where more frequent calls become necessary. In particular, animations are a costly action as they require constant back and forth communication to make the incremental changes needed in the view. Facebook has recently released a major update to how React Native handles animations which makes this process much less painful. However, it is still important to note that you will likely see performance issues in apps that use a lot of animation. Maturity As we have seen, React Native is a fairly young technology and while it has seen a lot of growth since its initial release, its “youth” can still cause developers grief. The components provided by Facebook are fairly good and they utilize the underlying native building blocks well. There are still fewer options available than many developers would like. I have found many third party libraries that have tried to fill in some of these gaps, and some of them are really well done, but others could use a lot of work. With its relative youth in mind, it is important to note that you will quite likely need to fall back on writing some native code to achieve the full functionality you want. Usability I have found that it is both a positive and a negative that React Native allows developers to write mobile apps in JavaScript with familiar React concepts. On the one hand, as a web developer, the learning curve for me to get in to it and get started was a lot smaller than if I had to learn both iOS and Android development separately. On the other hand, I found that it was really easy to fall back into old habits and write “web” code to run natively. What looks good and provides the best experience in the web does not always transfer to a mobile experience. I have to remind myself of this when I am creating and styling my components. It is also important to thoroughly review any third party libraries you plan to use to make sure the user experience is appropriate. Conclusion Overall, I have been really excited by my experiences working with React Native. It has come a long way since its first release over two years ago and it certainly has a lot of room to grow. React Native has made the jump from web to mobile development much less painful than it could have been and I am looking forward to following it further. For a glimpse into what React Native code looks like, using Redux, head over to my GitHub and check out my demo project . posted on January 11, 2018 by Kara Waldemer ← Previous Post Next Post → posted on January 11, 2018 by Kara Waldemer Kara Waldemer", "date": "2018-01-11"},
{"website": "Monsanto", "title": "Serverless Dynamic Web Pages in AWS", "author": ["\n                                        Mike Okner\n                                    "], "link": "http://engineering.monsanto.com/2017/11/20/serverless-dynamic-pages/", "abstract": "Background Recently, I was looking to create a status page app to monitor a few important\ninternal services.  I wanted this app to be as lightweight, reliable, and\nhassle-free as possible, so using a “serverless” architecture that doesn’t\nrequire any patching or other maintenance was quite appealing. I also don’t deploy anything in a production AWS environment outside of some\nsort of template (usually CloudFormation) as a rule.  I don’t want to have to\ncome back to something I created ad hoc in the console after 6 months and try\nto recall exactly how I architected all of the resources.  I’ll inevitably\nforget something and create more problems before solving the original one.  So\nbuilding the status page in a template was a requirement. The Design I settled on a design using two Lambda functions, both written in Python 3.6. The first Lambda function makes requests out to a list of important services\nand writes their current status to a DynamoDB table.  This function is executed\nonce per minute via CloudWatch Event Rule. The second Lambda function reads each service’s status & uptime information\nfrom DynamoDB and renders a Jinja template.  This\nfunction is behind an API Gateway that has been configured to return text/html instead of its default application/json Content-Type. The CloudFormation Template AWS provides a Serverless Application\nModel template\ntransformer to streamline the templating of Lambda + API Gateway designs, but\nit assumes (like everything else about the API Gateway) that you’re actually\nserving an API that returns JSON content.  So, unfortunately, it won’t work for\nthis use-case because we want to return HTML content.  Instead, we’ll have to\nenumerate every resource like usual. The Skeleton We’ll be using YAML for the template in this example.  I find it easier to read\nthan JSON, but you can easily convert between the two with a\nconverter if you disagree. --- AWSTemplateFormatVersion : ' 2010-09-09' Description : Serverless status page app Resources : # [...Resources] The Status-Checker Lambda Resource This one is triggered on a schedule by CloudWatch, and looks like: # Status Checker Lambda CheckerLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : checker.handler Role : Fn::GetAtt: - CheckerLambdaRole - Arn Runtime : python3.6 Timeout : 45 CheckerLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com CheckerLambdaTimer : Type : AWS::Events::Rule Properties : ScheduleExpression : rate(1 minute) Targets : - Id : CheckerLambdaTimerLambdaTarget Arn : Fn::GetAtt: - CheckerLambda - Arn CheckerLambdaTimerPermission : Type : AWS::Lambda::Permission Properties : Action : lambda:invokeFunction FunctionName : !Ref CheckerLambda SourceArn : Fn::GetAtt: - CheckerLambdaTimer - Arn Principal : events.amazonaws.com Let’s break that down a bit. The CheckerLambda is the actual Lambda function.  The Code section is a\nlocal path to a ZIP file containing the code and its dependencies.  I’m using\nCloudFormation’s packaging\nfeature to automatically push the deployable to S3. The CheckerLambdaRole is the IAM role the Lambda will assume which grants it\naccess to DynamoDB in addition to the usual Lambda logging permissions. The CheckerLambdaTimer is the CloudWatch Events Rule that triggers the\nchecker to run once per minute. The CheckerLambdaTimerPermission grants CloudWatch the ability to invoke the\nchecker Lambda function on its interval. The Web Page Gateway The API Gateway handles incoming requests for the web page, invokes the Lambda,\nand then returns the Lambda’s results as HTML content. Its template looks like: # API Gateway for Web Page Lambda PageGateway : Type : AWS::ApiGateway::RestApi Properties : Name : Service Checker Gateway PageResource : Type : AWS::ApiGateway::Resource Properties : RestApiId : !Ref PageGateway ParentId : Fn::GetAtt: - PageGateway - RootResourceId PathPart : page PageGatewayMethod : Type : AWS::ApiGateway::Method Properties : AuthorizationType : NONE HttpMethod : GET Integration : Type : AWS IntegrationHttpMethod : POST Uri : Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${WebRenderLambda.Arn}/invocations RequestTemplates : application/json : | { \"method\": \"$context.httpMethod\", \"body\" : $input.json('$'), \"headers\": { #foreach($param in $input.params().header.keySet()) \"$param\": \"$util.escapeJavaScript($input.params().header.get($param))\" #if($foreach.hasNext),#end #end } } IntegrationResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : \" 'text/html'\" ResponseTemplates : text/html : \" $input.path('$')\" ResourceId : !Ref PageResource RestApiId : !Ref PageGateway MethodResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : true PageGatewayProdStage : Type : AWS::ApiGateway::Stage Properties : DeploymentId : !Ref PageGatewayDeployment RestApiId : !Ref PageGateway StageName : Prod PageGatewayDeployment : Type : AWS::ApiGateway::Deployment DependsOn : PageGatewayMethod Properties : RestApiId : !Ref PageGateway Description : PageGateway deployment StageName : Stage There’s a lot going on here, but the real meat is in the PageGatewayMethod section.  There are a couple properties that deviate from the default which is\nwhy we couldn’t use the SAM transformer. First, we’re passing request headers through to the Lambda in the RequestTemplates section.  I’m doing this so I can validate incoming auth\nheaders.  The API Gateway can do some types of auth, but I found it easier to\ncheck auth myself in the Lambda function since the Gateway is designed to\nhandle API calls and not browser requests. Next, note that in the IntegrationResponses section we’re defining the Content-Type header to be 'text/html' (with single-quotes) and defining the\nResponseTemplate to be $input.path('$') .  This is what makes the request\nrender as a HTML page in your browser instead of just raw text. Due to the StageName and PathPart values in the other sections, your actual\npage will be accessible at https://someId.execute-api.region.amazonaws.com/Prod/page .  I have the page\nbehind an existing reverse-proxy and give it a saner URL for end-users.  The\nreverse proxy also attaches the auth header I mentioned above.  If that header\nisn’t present, the Lambda will render an error page instead so the proxy can’t\nbe bypassed. The Web Page Rendering Lambda This Lambda is invoked by calls to the API Gateway and looks like: # Web Page Lambda WebRenderLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : web.handler Role : Fn::GetAtt: - WebRenderLambdaRole - Arn Runtime : python3.6 Timeout : 30 WebRenderLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com WebRenderLambdaGatewayPermission : Type : AWS::Lambda::Permission Properties : FunctionName : !Ref WebRenderLambda Action : lambda:invokeFunction Principal : apigateway.amazonaws.com SourceArn : Fn::Sub: - arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${__ApiId__}/*/*/* - __ApiId__ : !Ref PageGateway The WebRenderLambda and WebRenderLambdaRole should look familiar. The WebRenderLambdaGatewayPermission is similar to the Status Checker’s\nCloudWatch permission, only this time it allows the API Gateway to invoke this\nLambda. The DynamoDB Table This one is straightforward. # DynamoDB table DynamoTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : name AttributeType : S ProvisionedThroughput : WriteCapacityUnits : 1 ReadCapacityUnits : 1 TableName : status-page-checker-results KeySchema : - KeyType : HASH AttributeName : name The Deployment We’ve made it this far defining every resource in a template that we can check\nin to version control, so we might as well script the deployment as well rather\nthan manually manage the CloudFormation Stack via the AWS web console. Since I’m using the packaging feature, I first run: $ aws cloudformation package \\ --template-file template.yaml \\ --s3-bucket <some-bucket-name> \\ --output-template-file template-packaged.yaml Uploading to 34cd6e82c5e8205f9b35e71afd9e1548  1922559 / 1922559.0  (100.00%)\nSuccessfully packaged artifacts and wrote output template to file template-packaged.yaml. Then to deploy the template (whether new or modified), I run: $ aws cloudformation deploy \\ --region '<aws-region>' \\ --template-file template-packaged.yaml \\ --stack-name '<some-name>' \\ --capabilities CAPABILITY_IAM Waiting for changeset to be created..\nWaiting for stack create/update to complete Successfully created/updated stack - <some-name> The End And that’s it!  You’ve just created a dynamic web page that will never require\nyou to SSH anywhere, patch a server, recover from a disaster after Amazon\nterminates your unhealthy EC2, or any other number of pitfalls that are now the\nproblem of some ops person at AWS.  And you can reproduce deployments and make\nchanges with confidence because everything is defined in the template and can\nbe tracked in version control. Plus, you’re paying for execution time on-demand so it’s absurdly cheap to run! posted on November 20, 2017 by Mike Okner ← Previous Post Next Post → --- AWSTemplateFormatVersion : ' 2010-09-09' Description : Serverless status page app Resources : # [...Resources] --- AWSTemplateFormatVersion : ' 2010-09-09' Description : Serverless status page app Resources : # [...Resources] # Status Checker Lambda CheckerLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : checker.handler Role : Fn::GetAtt: - CheckerLambdaRole - Arn Runtime : python3.6 Timeout : 45 CheckerLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com CheckerLambdaTimer : Type : AWS::Events::Rule Properties : ScheduleExpression : rate(1 minute) Targets : - Id : CheckerLambdaTimerLambdaTarget Arn : Fn::GetAtt: - CheckerLambda - Arn CheckerLambdaTimerPermission : Type : AWS::Lambda::Permission Properties : Action : lambda:invokeFunction FunctionName : !Ref CheckerLambda SourceArn : Fn::GetAtt: - CheckerLambdaTimer - Arn Principal : events.amazonaws.com # Status Checker Lambda CheckerLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : checker.handler Role : Fn::GetAtt: - CheckerLambdaRole - Arn Runtime : python3.6 Timeout : 45 CheckerLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com CheckerLambdaTimer : Type : AWS::Events::Rule Properties : ScheduleExpression : rate(1 minute) Targets : - Id : CheckerLambdaTimerLambdaTarget Arn : Fn::GetAtt: - CheckerLambda - Arn CheckerLambdaTimerPermission : Type : AWS::Lambda::Permission Properties : Action : lambda:invokeFunction FunctionName : !Ref CheckerLambda SourceArn : Fn::GetAtt: - CheckerLambdaTimer - Arn Principal : events.amazonaws.com # API Gateway for Web Page Lambda PageGateway : Type : AWS::ApiGateway::RestApi Properties : Name : Service Checker Gateway PageResource : Type : AWS::ApiGateway::Resource Properties : RestApiId : !Ref PageGateway ParentId : Fn::GetAtt: - PageGateway - RootResourceId PathPart : page PageGatewayMethod : Type : AWS::ApiGateway::Method Properties : AuthorizationType : NONE HttpMethod : GET Integration : Type : AWS IntegrationHttpMethod : POST Uri : Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${WebRenderLambda.Arn}/invocations RequestTemplates : application/json : | { \"method\": \"$context.httpMethod\", \"body\" : $input.json('$'), \"headers\": { #foreach($param in $input.params().header.keySet()) \"$param\": \"$util.escapeJavaScript($input.params().header.get($param))\" #if($foreach.hasNext),#end #end } } IntegrationResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : \" 'text/html'\" ResponseTemplates : text/html : \" $input.path('$')\" ResourceId : !Ref PageResource RestApiId : !Ref PageGateway MethodResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : true PageGatewayProdStage : Type : AWS::ApiGateway::Stage Properties : DeploymentId : !Ref PageGatewayDeployment RestApiId : !Ref PageGateway StageName : Prod PageGatewayDeployment : Type : AWS::ApiGateway::Deployment DependsOn : PageGatewayMethod Properties : RestApiId : !Ref PageGateway Description : PageGateway deployment StageName : Stage # API Gateway for Web Page Lambda PageGateway : Type : AWS::ApiGateway::RestApi Properties : Name : Service Checker Gateway PageResource : Type : AWS::ApiGateway::Resource Properties : RestApiId : !Ref PageGateway ParentId : Fn::GetAtt: - PageGateway - RootResourceId PathPart : page PageGatewayMethod : Type : AWS::ApiGateway::Method Properties : AuthorizationType : NONE HttpMethod : GET Integration : Type : AWS IntegrationHttpMethod : POST Uri : Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${WebRenderLambda.Arn}/invocations RequestTemplates : application/json : | { \"method\": \"$context.httpMethod\", \"body\" : $input.json('$'), \"headers\": { #foreach($param in $input.params().header.keySet()) \"$param\": \"$util.escapeJavaScript($input.params().header.get($param))\" #if($foreach.hasNext),#end #end } } IntegrationResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : \" 'text/html'\" ResponseTemplates : text/html : \" $input.path('$')\" ResourceId : !Ref PageResource RestApiId : !Ref PageGateway MethodResponses : - StatusCode : 200 ResponseParameters : method.response.header.Content-Type : true PageGatewayProdStage : Type : AWS::ApiGateway::Stage Properties : DeploymentId : !Ref PageGatewayDeployment RestApiId : !Ref PageGateway StageName : Prod PageGatewayDeployment : Type : AWS::ApiGateway::Deployment DependsOn : PageGatewayMethod Properties : RestApiId : !Ref PageGateway Description : PageGateway deployment StageName : Stage # Web Page Lambda WebRenderLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : web.handler Role : Fn::GetAtt: - WebRenderLambdaRole - Arn Runtime : python3.6 Timeout : 30 WebRenderLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com WebRenderLambdaGatewayPermission : Type : AWS::Lambda::Permission Properties : FunctionName : !Ref WebRenderLambda Action : lambda:invokeFunction Principal : apigateway.amazonaws.com SourceArn : Fn::Sub: - arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${__ApiId__}/*/*/* - __ApiId__ : !Ref PageGateway # Web Page Lambda WebRenderLambda : Type : AWS::Lambda::Function Properties : Code : ./lambda.zip Environment : Variables : TABLE_NAME : !Ref DynamoTable Handler : web.handler Role : Fn::GetAtt: - WebRenderLambdaRole - Arn Runtime : python3.6 Timeout : 30 WebRenderLambdaRole : Type : AWS::IAM::Role Properties : ManagedPolicyArns : - arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : ' 2012-10-17' Statement : - Action : - sts:AssumeRole Effect : Allow Principal : Service : - lambda.amazonaws.com WebRenderLambdaGatewayPermission : Type : AWS::Lambda::Permission Properties : FunctionName : !Ref WebRenderLambda Action : lambda:invokeFunction Principal : apigateway.amazonaws.com SourceArn : Fn::Sub: - arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${__ApiId__}/*/*/* - __ApiId__ : !Ref PageGateway # DynamoDB table DynamoTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : name AttributeType : S ProvisionedThroughput : WriteCapacityUnits : 1 ReadCapacityUnits : 1 TableName : status-page-checker-results KeySchema : - KeyType : HASH AttributeName : name # DynamoDB table DynamoTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : name AttributeType : S ProvisionedThroughput : WriteCapacityUnits : 1 ReadCapacityUnits : 1 TableName : status-page-checker-results KeySchema : - KeyType : HASH AttributeName : name $ aws cloudformation package \\ --template-file template.yaml \\ --s3-bucket <some-bucket-name> \\ --output-template-file template-packaged.yaml Uploading to 34cd6e82c5e8205f9b35e71afd9e1548  1922559 / 1922559.0  (100.00%)\nSuccessfully packaged artifacts and wrote output template to file template-packaged.yaml. $ aws cloudformation package \\ --template-file template.yaml \\ --s3-bucket <some-bucket-name> \\ --output-template-file template-packaged.yaml Uploading to 34cd6e82c5e8205f9b35e71afd9e1548  1922559 / 1922559.0  (100.00%)\nSuccessfully packaged artifacts and wrote output template to file template-packaged.yaml. $ aws cloudformation deploy \\ --region '<aws-region>' \\ --template-file template-packaged.yaml \\ --stack-name '<some-name>' \\ --capabilities CAPABILITY_IAM Waiting for changeset to be created..\nWaiting for stack create/update to complete Successfully created/updated stack - <some-name> $ aws cloudformation deploy \\ --region '<aws-region>' \\ --template-file template-packaged.yaml \\ --stack-name '<some-name>' \\ --capabilities CAPABILITY_IAM Waiting for changeset to be created..\nWaiting for stack create/update to complete Successfully created/updated stack - <some-name> posted on November 20, 2017 by Mike Okner Mike Okner", "date": "2017-11-20"},
{"website": "Monsanto", "title": "Don’t Bite Off Too Much At Once – “Crawl, Walk & Run” ", "author": ["\n                                        Madhuri Gururaj\n                                    "], "link": "http://engineering.monsanto.com/2017/09/18/find-your-needle-in-the-haystack/", "abstract": "In the data driven world that we live in today, it is common to find ourselves spending hours to find information on our own data –from knowing what data we have (data catalog), its definition (both business & technical), its flow and footprint (source of truth), how much data we have (volumes and metrics), and so on. Knowledge workers spend 15-35% of their time searching for information 1 . To get a good 360-degree view of our data assets, it is critical that we have a systematic approach to collecting and managing information about our data (Metadata). Companies are investing in Metadata and practices to reuse data, have visibility to data lineage, compliance and extend the life of their data assets. As Monsanto transforms its business across the globe into becoming more digital, it has become fundamental for our customers, business partners and technical resources to be able to efficiently access well organized and classified information so they spend less time on preparing and managing datasets in files and more time on drawing deeper insights and driving informed business decisions. A core group of data gurus at Monsanto representing R&D, Global Supply Chain and Commercial deliberated for months to identify our top 10 enterprise Metadata capabilities.  After evaluating and scoring many off-the-shelf products against these ranked capabilities, the core team found that none of these products both aligned with these capabilities and had a reasonable price tag. The decision was made to build a newer version of “FAKS” (an existing internal and US only metadata solution) using MediaWiki which is the Open Source engine that drives Wikipedia. It was quickly rolled out with some basic set of features and templates to gather and manage business and technical definitions about our datasets. From here on the focus was championing the product while rolling out new capabilities that added value. The key enablers that endorsed and helped bring visibility to the platform included many road shows to listening to our customer for new features and POCs to enable those capabilities. Leverage New Technologies Utilizing Open Source toolsets, vendor-provided APIs and AI accelerated the development process by letting us concentrate on Metadata-specific functionality instead of having to re-invent the wheel for capabilities like usage metrics, querying capabilities, content management that allowed multiple users to contribute (wikis), etc. Branding Helps Unleash Your Identity By re-using the look and feel of Wikipedia, we began receiving feedback that the UI felt outdated and was not user friendly (largely due to Wikipedia not having a major UI change in nearly 20 years). Partnering with the UX (user experience) team to help redesign the UI and brainstorm on branding strategies was one of the most impactful decisions we made. The redesign completely changed the end user experience, and branding the product as “Haystack” also gave it a new identity and engendered growing excitement from the end user base. Your Customer is No.1 Today’s users are constantly interacting in an eco-system of diverse set of tools and windows. Enabling cross-platform integration allows us to leverage best features from different application into one common user experience. Haystack has an AI Slack bot, we call him “Alfred”, that queries the MediaWiki API and retrieves the data from the page and responds with the content in a Slack chat session. This can be utilized in both a Slack public channel as well as a direct message to the bot. The interest in this feature has escalated where several teams have approached asking to expand our AI bot to include functionality from other platforms. Similarly, another capability allows searches across Haystack and CKAN data catalog in one request.\nSimilar to Amazon’s Alexa or Apple’s Siri, we are not far from being able to enhance Haystack’s Alfred to be an intelligent personal assistant to answer questions like “Alfred…tell me what is the source of truth for Corn’s Relative Maturity?” Slack: #haystack Its All About The Numbers! Usage and adoption metrics are collected about the new platform and made available via Piwik (a self-service Open Source UI), that allows the user to view and customize Dashboards. By calling the Piwik API’s, Haystack’s AI bot is calling them and pushing the results to the #Haystack Slack Channel. Agile Mindset To keep the users excited and engaged with Haystack, we constantly collaborated with users, data stewards, engineers and leads to review and solicit feedback on a prioritized list of key capabilities and gaps. This enabled us to effectively and continuously deliver small features of high value some of which were not only challenging but required creativity. Keep An Evergreen Product Roadmap In phase 1, our goal was to lay the foundation down for the Metadata platform and acquire a 360-degree view of Monsanto’s metadata.  We focused on Business Metadata (Glossaries) and Technical Metadata (Relational Data Catalog) and how to tie them together. Although this combination is already very powerful, we are still missing a large piece of the puzzle: – Data Lineage.\nAs we continue to work on enhancements to the platform capabilities in phase 2 including work on Data Lineage, we will conduct workshops to roll out a structured process of content management and formalize Haystack governance. ## Special thanks to John Cooper for his help with the technical content in this post. https://www.reachengine.com/mindshare-nyc-why-metadata-matters-2/ . ↩ posted on September 18, 2017 by Madhuri Gururaj ← Previous Post Next Post → https://www.reachengine.com/mindshare-nyc-why-metadata-matters-2/ . ↩ posted on September 18, 2017 by Madhuri Gururaj Madhuri Gururaj", "date": "2017-09-18"},
{"website": "Monsanto", "title": "Declaratively securing REST APIs to specific clients", "author": ["\n                                        Prasanna Crasta\n                                    "], "link": "http://engineering.monsanto.com/2017/09/01/declaratively-securing-rest-apis-to-specific-clients/", "abstract": "What problem are we trying to solve? As an API producer I would like to restrict my REST APIs to specific clients. These restrictions should be dynamically configurable with a simple interface. I feel if I skip the background then I won’t do justice to the problem, I promise it will be a quick read. The Platforms Engineering team that I am part of has built an entitlement system (developed using NodeJS & Express) which has become the quintessential backbone providing user authorization data. This entitlement system exposes a bunch of REST APIs that other internal applications can integrate with. We expose a lot of “public” APIs to all of our users, meaning as long as you have a valid OAuth token we allow the APIs to be called. However there are certain APIs that we need to keep private i.e. only certain (OAuth) clients could call them. Initially we had just couple of clients that needed access to these private endpoints. So the simplest and typical agile approach was to embed the check for client restriction at the endpoint level itself. i.e. something like router = require ( 'express' ). Router (); router . put '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' , 'CLIENT-B' ]. indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; router . get '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' ]). indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; This enabled the MVP (minimum viable product) to be shipped out and satisfy the immediate need. However, soon, with the increase in popularity of this entitlement system, more clients needed access to these private endpoints. We still couldn’t just expose them to all of our users, so we needed a more flexible way to manage this. We also didn’t want to have to hardcode a new client id every time and redeploy. That’s for the history…. Design philosophy Given that our system is built on NodeJS & Express we decided to build an express middleware that would intercept every request to our APIs and perform request validation against a cached route configuration made available to the middleware at server startup and can be refreshed on demand. What does the configuration look like? It’s a plain json document like below [ { url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] } { url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] } { url : '/endpoint-1' methods : [ 'GET' ] clientIds : [] } ] As you might have guessed, with above configuration we are trying to limit POST, PUT & DELETE requests to '/endpoint-1' to the clients CLIENT-A & CLIENT-B Aiming for minimal configuration Our main goal was to keep the configuration minimal, because we all know how quickly the configuration can get messy and a nightmare to handle. So our design aimed for evaluating the restriction matches starting from most distinct to partial matches. What do I mean by this? e.g. If I want to secure all POST, PUT DELETE request to CLIENT-A & CLIENT-B to below endpoints: 1. /endpoint-1\n2. /endpoint-1/{id}\n3. /endpoint-1/{id}/sub-endpoint Then I simply define the config at the most partial match level, so in the above case restricting at /endpoint-1/ will cause this level plus any child routes to be restricted these clients e.g: url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] You might have noted, I have limited the access to the base route '/' to just one client CLIENT-A, that way we do not accidentally expose a new endpoint that we mean to keep private. url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] So how we make another endpoint, say /endpoint-2 , public? Good question! just configure as below: url : '/endpoint-2' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [] I hope by now you get the idea on how we provide the routes configuration. If you have been with me so far, I am sure you would be interested in knowing how the validations are actually performed. And this is the code that actually does the trick. findMatchingRoute = ( url , method ) -> matchedRoute = appRoutes . find ( route ) -> pattern = route . url . replace ( /\\?/g , matchPattern ) + queryParamPattern new RegExp ( pattern ). test ( url ) and method in route . methods if not matchedRoute matchUrl = if url . lastIndexOf ( '/' ) > 0 url . slice ( 0 , url . lastIndexOf ( '/' )) else \"/\" if url isnt matchUrl then findMatchingRoute ( matchUrl , method ) else matchedRoute else matchedRoute clientIdHasAccess = ( clientId , originalUrl , method = 'POST' ) -> route = findMatchingRoute ( originalUrl , method ) not route or route . clientIds . length is 0 or clientId in route . clientIds validator = ( req , res , next ) -> if clientIdHasAccess ( req . headers [ clientKey ], req . originalUrl , req . method ) next () else res . status ( 403 ). send ( \"Invalid Client\" ) Basically for each request we perform the below algorithm: Find a matching route for the current request using pattern matching. If a matched route config is found, then check for any client restrictions on it. If client restrictions exist, then check if the current request is made by one of the allowable clients. If yes, then just let the call through by invoking the next middleware function. Otherwise throw a 403 FORBIDDEN error. And use the above module in the core project like so: app = require ( 'express' )() app . use '/' , validator So far we have solved two problems: 1. Minimal configuration of restricted routes.\n2. No more hard coding of client ids. One more problem still remains and that is how do we get new clients added to the configuration at runtime? Well that wasn’t such a hard problem to solve, we chose to store our config in an external database, so we can modify the configuration as new client or routes get added. We load this config at startup of the entitlement server, cache this configuration in the validating module and refresh that cache at a set interval (say 10 mins). So there you go. now we have all our requirements satisfied. We have a way to secure our APIs to specific clients by means of an easy configuration and one that can be refreshed at runtime. And guess what, this module is now available as open source. Please check out the project express-client-validator on Github. posted on September 1, 2017 by Prasanna Crasta ← Previous Post Next Post → router = require ( 'express' ). Router (); router . put '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' , 'CLIENT-B' ]. indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; router . get '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' ]). indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; router = require ( 'express' ). Router (); router . put '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' , 'CLIENT-B' ]. indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; router . get '/endpoint-1' , ( req , res , next ) -> if [ 'CLIENT-A' ]). indexOf ( req . headers . client_id ) < 0 res . status ( 403 ). send ( 'Invalid client' ); else proceedWithLogic ; [ { url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] } { url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] } { url : '/endpoint-1' methods : [ 'GET' ] clientIds : [] } ] [ { url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] } { url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] } { url : '/endpoint-1' methods : [ 'GET' ] clientIds : [] } ] 1. /endpoint-1\n2. /endpoint-1/{id}\n3. /endpoint-1/{id}/sub-endpoint 1. /endpoint-1\n2. /endpoint-1/{id}\n3. /endpoint-1/{id}/sub-endpoint url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] url : '/endpoint-1' methods : [ 'POST' , 'PUT' , 'DELETE' ] clientIds : [ 'CLIENT-A' , 'CLIENT-B' ] url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] url : '/' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [ 'CLIENT-A' ] url : '/endpoint-2' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [] url : '/endpoint-2' methods : [ 'GET' , 'PUT' , 'POST' , 'DELETE' ] clientIds : [] findMatchingRoute = ( url , method ) -> matchedRoute = appRoutes . find ( route ) -> pattern = route . url . replace ( /\\?/g , matchPattern ) + queryParamPattern new RegExp ( pattern ). test ( url ) and method in route . methods if not matchedRoute matchUrl = if url . lastIndexOf ( '/' ) > 0 url . slice ( 0 , url . lastIndexOf ( '/' )) else \"/\" if url isnt matchUrl then findMatchingRoute ( matchUrl , method ) else matchedRoute else matchedRoute clientIdHasAccess = ( clientId , originalUrl , method = 'POST' ) -> route = findMatchingRoute ( originalUrl , method ) not route or route . clientIds . length is 0 or clientId in route . clientIds validator = ( req , res , next ) -> if clientIdHasAccess ( req . headers [ clientKey ], req . originalUrl , req . method ) next () else res . status ( 403 ). send ( \"Invalid Client\" ) findMatchingRoute = ( url , method ) -> matchedRoute = appRoutes . find ( route ) -> pattern = route . url . replace ( /\\?/g , matchPattern ) + queryParamPattern new RegExp ( pattern ). test ( url ) and method in route . methods if not matchedRoute matchUrl = if url . lastIndexOf ( '/' ) > 0 url . slice ( 0 , url . lastIndexOf ( '/' )) else \"/\" if url isnt matchUrl then findMatchingRoute ( matchUrl , method ) else matchedRoute else matchedRoute clientIdHasAccess = ( clientId , originalUrl , method = 'POST' ) -> route = findMatchingRoute ( originalUrl , method ) not route or route . clientIds . length is 0 or clientId in route . clientIds validator = ( req , res , next ) -> if clientIdHasAccess ( req . headers [ clientKey ], req . originalUrl , req . method ) next () else res . status ( 403 ). send ( \"Invalid Client\" ) app = require ( 'express' )() app . use '/' , validator app = require ( 'express' )() app . use '/' , validator 1. Minimal configuration of restricted routes.\n2. No more hard coding of client ids. 1. Minimal configuration of restricted routes.\n2. No more hard coding of client ids. posted on September 1, 2017 by Prasanna Crasta Prasanna Crasta", "date": "2017-09-01"},
{"website": "Monsanto", "title": "React Hot Loading with Webpack 2", "author": ["\n                                        John Glynn\n                                    "], "link": "http://engineering.monsanto.com/2017/08/15/react-hotloading-with-webpack-2/", "abstract": "React components, hot loaded?\nUsing the Webpack Hot Module Replacement (HMR) plugin with webpack-dev-middleware, webpack-hot-middleware and\nreact-hot-loader, you can get hot loading of React components the same as regular Javascript modules. What is hot loading and why would you want it? Hot loading is the replacement of code in a running application without\nrequiring a restart or reloading of the application. It is very useful when developing because you can see your changes\nas you make them thus giving immediate feedback. React hot loading depends on a working Webpack HMR setup so we’ll \nstart with that. First, we’ll start with a minimal Webpack configuration. A single entry entry : [ ' ./public/scripts/main ' , ] and a Webpack 2 style module with a rule for Babel module : { rules : [ { test : / \\. json$/ , use : ' json-loader ' }, { test : / \\. jsx ? $/ , use : [{ loader : ' babel-loader ' }], exclude : [ path . resolve ( __dirname , ' node_modules ' ) ] } ] } Using a common rule for the loader simplifies the Webpack config and environment differences can be broken out in the .babelrc file. For example, { \"env\" : { \"test\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ], \"plugins\" : [ [ \"istanbul\" , { \"exclude\" : [ \"test\" ]}] ] }, \"prod\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ] } } } Each specialization is accessed by setting BABEL_ENV in the shell environment. Adding HMR is easy. Create a plugins array or extend yours with the HMR plugin. plugins : [ new webpack . HotModuleReplacementPlugin () ] To actually use HMR requires a bit more work. Webpack-dev-middleware needs to be setup and another package,\nwebpack-hot-middleware, will need to be used to handle the hot loading plugin for the middleware. The following steps \nare needed to configure express; usually in ./bin/www in your project or a file required from there: // HMR configuration if ( process . env . NODE_ENV !== ' production ' ) { const webpack = require ( ' webpack ' ) const webpackDevMiddleware = require ( ' webpack-dev-middleware ' ) const webpackConfig = require ( ' ./webpack.dev.config ' ) const middlewareOptions = { stats : { colors : true }, noInfo : false , lazy : false , headers : { \" Access-Control-Allow-Origin \" : \" http://localhost \" }, publicPath : webpackConfig . output . publicPath } const compiler = webpack ( webpackConfig ); const webpackDevMiddlewareInstance = webpackDevMiddleware ( compiler , middlewareOptions ); app . use ( webpackDevMiddlewareInstance ) const webpackHotMiddleware = require ( ' webpack-hot-middleware ' ) app . use ( webpackHotMiddleware ( compiler )) } Lastly, you will need to add another entry point for the hot loader client entry : [ ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] If you use a proxy, you will also need to be aware of the HMR callback at localhost/__webpack_hmr . \n This is the purpose of the Access-Control-Allow-Origin header specified in the middleware options. At this point, we have HMR working. A change in the source will cause a partial rebuild of the Webpack bundle into a \nnew small file with the changed code, a mapping file and the original bundle. This is why the entry point for the hot \nloader client was added to the array on top of the original entry point. For example, Original build: webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. After a modification: webpack: Compiling...\nwebpack building...\nwebpack built 3ca752230fba1bc44ed9 in 6329ms\nHash: 3ca752230fba1bc44ed9\nVersion: webpack 2.6.1\nTime: 6329ms\n                               Asset      Size  Chunks                    Chunk Names\n                           bundle.js   26.2 MB       0  [emitted]  [big]  main\n0.2582a08558ae8c6cb3bf.hot-update.js   71.5 kB       0  [emitted]         main\n2582a08558ae8c6cb3bf.hot-update.json  43 bytes          [emitted]         \nchunk    {0} bundle.js, 0.2582a08558ae8c6cb3bf.hot-update.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0}\n   [14] ./~/react-dom/index.js 59 bytes {0}\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0}\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0}\n [1776] ./~/react-hot-loader/index.js 41 bytes {0}\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0}\n [1978] ./~/strip-ansi/index.js 161 bytes {0}\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0}\n     + 2002 hidden modules\nwebpack: Compiled successfully. But React at this point is not hot loading yet. A manual reload is still needed. The approach taken with \nreact-hot-loader is much the same as the one taken by HMR with webpack-hot-middleware; a wrapper with its own entry\npoint. The React examples from here on use React Router v4. It is highly recommended to use v4 if you are using \nrouting since they are real React components and will hot load properly. I will ignore import statements and propType definitions for brevity. Here is our example application’s main component: export default class Root extends React.Component {\n    render () {\n        const {store, history} = this.props;\n        return (\n          <Provider store={store}>\n              <I18nextProvider i18n={ i18n }>\n                  <ConnectedRouter history={history}>\n                      <Switch>\n                          <Route path={`${APP_BASE_URL_PREFIX}`} component={RootComponent} />\n                      </Switch>\n                  </ConnectedRouter>\n              </I18nextProvider>\n          </Provider>\n        );\n    }\n} The React hot loader needs to wrap the top application component with its own called AppContainer, like so: const render = () => {\n    ReactDOM.render(\n      <AppContainer>\n          <Root store={ store } history={ history }/>\n      </AppContainer>,\n      document.querySelector('.contents')\n    );\n};\nrender();\nif (module.hot) {\n    module.hot.accept('./Root', () => {\n        render();\n    });\n} You may need to split the files and/or create a new entry point for the Webpack configuration. Also, if you have \nanonymous functions called to initialize variables before the call to render() , you may need to put those in a\nseparate file and export them for hot loading to work completely. Next, you will need to ensure that Babel is configured for hot loading. With the sample .babelrc above, the dev \nsetting should look like { \"dev\" : { \"plugins\" : [ \"react-hot-loader/babel\" ], \"presets\" : [ \"react\" , [ \"es2015\" , { \"modules\" : false }], \"stage-2\" ] } } The modification to the es2015 preset is because Webpack already has built-in support for ES2015 and will ignore \nchanges in ES6 code if Babel transpiles it again. The last step is to add the hot loader entry point entry : [ ' react-hot-loader/patch ' , ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] Now, when your application is started, you should see both the React and the Webpack hot loaders listed and they should\nbe in the correct order. webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [911] ./~/react-hot-loader/patch.js 41 bytes {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi react-hot-loader/patch webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. References: http://gaearon.github.io/react-hot-loader/ https://github.com/glenjamin/webpack-hot-middleware https://webpack.js.org/concepts/ https://codeburst.io/react-router-v4-unofficial-migration-guide-5a370b8905a posted on August 15, 2017 by John Glynn ← Previous Post Next Post → entry : [ ' ./public/scripts/main ' , ] entry : [ ' ./public/scripts/main ' , ] module : { rules : [ { test : / \\. json$/ , use : ' json-loader ' }, { test : / \\. jsx ? $/ , use : [{ loader : ' babel-loader ' }], exclude : [ path . resolve ( __dirname , ' node_modules ' ) ] } ] } module : { rules : [ { test : / \\. json$/ , use : ' json-loader ' }, { test : / \\. jsx ? $/ , use : [{ loader : ' babel-loader ' }], exclude : [ path . resolve ( __dirname , ' node_modules ' ) ] } ] } { \"env\" : { \"test\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ], \"plugins\" : [ [ \"istanbul\" , { \"exclude\" : [ \"test\" ]}] ] }, \"prod\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ] } } } { \"env\" : { \"test\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ], \"plugins\" : [ [ \"istanbul\" , { \"exclude\" : [ \"test\" ]}] ] }, \"prod\" : { \"presets\" : [ \"react\" , \"es2015\" , \"stage-2\" ] } } } plugins : [ new webpack . HotModuleReplacementPlugin () ] plugins : [ new webpack . HotModuleReplacementPlugin () ] // HMR configuration if ( process . env . NODE_ENV !== ' production ' ) { const webpack = require ( ' webpack ' ) const webpackDevMiddleware = require ( ' webpack-dev-middleware ' ) const webpackConfig = require ( ' ./webpack.dev.config ' ) const middlewareOptions = { stats : { colors : true }, noInfo : false , lazy : false , headers : { \" Access-Control-Allow-Origin \" : \" http://localhost \" }, publicPath : webpackConfig . output . publicPath } const compiler = webpack ( webpackConfig ); const webpackDevMiddlewareInstance = webpackDevMiddleware ( compiler , middlewareOptions ); app . use ( webpackDevMiddlewareInstance ) const webpackHotMiddleware = require ( ' webpack-hot-middleware ' ) app . use ( webpackHotMiddleware ( compiler )) } // HMR configuration if ( process . env . NODE_ENV !== ' production ' ) { const webpack = require ( ' webpack ' ) const webpackDevMiddleware = require ( ' webpack-dev-middleware ' ) const webpackConfig = require ( ' ./webpack.dev.config ' ) const middlewareOptions = { stats : { colors : true }, noInfo : false , lazy : false , headers : { \" Access-Control-Allow-Origin \" : \" http://localhost \" }, publicPath : webpackConfig . output . publicPath } const compiler = webpack ( webpackConfig ); const webpackDevMiddlewareInstance = webpackDevMiddleware ( compiler , middlewareOptions ); app . use ( webpackDevMiddlewareInstance ) const webpackHotMiddleware = require ( ' webpack-hot-middleware ' ) app . use ( webpackHotMiddleware ( compiler )) } entry : [ ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] entry : [ ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. webpack: Compiling...\nwebpack building...\nwebpack built 3ca752230fba1bc44ed9 in 6329ms\nHash: 3ca752230fba1bc44ed9\nVersion: webpack 2.6.1\nTime: 6329ms\n                               Asset      Size  Chunks                    Chunk Names\n                           bundle.js   26.2 MB       0  [emitted]  [big]  main\n0.2582a08558ae8c6cb3bf.hot-update.js   71.5 kB       0  [emitted]         main\n2582a08558ae8c6cb3bf.hot-update.json  43 bytes          [emitted]         \nchunk    {0} bundle.js, 0.2582a08558ae8c6cb3bf.hot-update.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0}\n   [14] ./~/react-dom/index.js 59 bytes {0}\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0}\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0}\n [1776] ./~/react-hot-loader/index.js 41 bytes {0}\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0}\n [1978] ./~/strip-ansi/index.js 161 bytes {0}\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0}\n     + 2002 hidden modules\nwebpack: Compiled successfully. webpack: Compiling...\nwebpack building...\nwebpack built 3ca752230fba1bc44ed9 in 6329ms\nHash: 3ca752230fba1bc44ed9\nVersion: webpack 2.6.1\nTime: 6329ms\n                               Asset      Size  Chunks                    Chunk Names\n                           bundle.js   26.2 MB       0  [emitted]  [big]  main\n0.2582a08558ae8c6cb3bf.hot-update.js   71.5 kB       0  [emitted]         main\n2582a08558ae8c6cb3bf.hot-update.json  43 bytes          [emitted]         \nchunk    {0} bundle.js, 0.2582a08558ae8c6cb3bf.hot-update.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0}\n   [14] ./~/react-dom/index.js 59 bytes {0}\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0}\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0}\n [1776] ./~/react-hot-loader/index.js 41 bytes {0}\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0}\n [1978] ./~/strip-ansi/index.js 161 bytes {0}\n [2016] multi webpack-hot-middleware/client ./public/scripts/main 52 bytes {0}\n     + 2002 hidden modules\nwebpack: Compiled successfully. export default class Root extends React.Component {\n    render () {\n        const {store, history} = this.props;\n        return (\n          <Provider store={store}>\n              <I18nextProvider i18n={ i18n }>\n                  <ConnectedRouter history={history}>\n                      <Switch>\n                          <Route path={`${APP_BASE_URL_PREFIX}`} component={RootComponent} />\n                      </Switch>\n                  </ConnectedRouter>\n              </I18nextProvider>\n          </Provider>\n        );\n    }\n} export default class Root extends React.Component {\n    render () {\n        const {store, history} = this.props;\n        return (\n          <Provider store={store}>\n              <I18nextProvider i18n={ i18n }>\n                  <ConnectedRouter history={history}>\n                      <Switch>\n                          <Route path={`${APP_BASE_URL_PREFIX}`} component={RootComponent} />\n                      </Switch>\n                  </ConnectedRouter>\n              </I18nextProvider>\n          </Provider>\n        );\n    }\n} const render = () => {\n    ReactDOM.render(\n      <AppContainer>\n          <Root store={ store } history={ history }/>\n      </AppContainer>,\n      document.querySelector('.contents')\n    );\n};\nrender();\nif (module.hot) {\n    module.hot.accept('./Root', () => {\n        render();\n    });\n} const render = () => {\n    ReactDOM.render(\n      <AppContainer>\n          <Root store={ store } history={ history }/>\n      </AppContainer>,\n      document.querySelector('.contents')\n    );\n};\nrender();\nif (module.hot) {\n    module.hot.accept('./Root', () => {\n        render();\n    });\n} { \"dev\" : { \"plugins\" : [ \"react-hot-loader/babel\" ], \"presets\" : [ \"react\" , [ \"es2015\" , { \"modules\" : false }], \"stage-2\" ] } } { \"dev\" : { \"plugins\" : [ \"react-hot-loader/babel\" ], \"presets\" : [ \"react\" , [ \"es2015\" , { \"modules\" : false }], \"stage-2\" ] } } entry : [ ' react-hot-loader/patch ' , ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] entry : [ ' react-hot-loader/patch ' , ' webpack-hot-middleware/client ' , ' ./public/scripts/main ' , ] webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [911] ./~/react-hot-loader/patch.js 41 bytes {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi react-hot-loader/patch webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. webpack built 2582a08558ae8c6cb3bf in 24655ms\nHash: 2582a08558ae8c6cb3bf\nVersion: webpack 2.6.1\nTime: 24655ms\n    Asset     Size  Chunks                    Chunk Names\nbundle.js  26.2 MB       0  [emitted]  [big]  main\nchunk    {0} bundle.js (main) 9.29 MB [entry] [rendered]\n    [0] ./~/react/react.js 59 bytes {0} [built]\n   [14] ./~/react-dom/index.js 59 bytes {0} [built]\n  [910] ./public/scripts/main.jsx 2.39 kB {0} [built]\n  [911] ./~/react-hot-loader/patch.js 41 bytes {0} [built]\n  [912] (webpack)-hot-middleware/client.js 6.68 kB {0} [built]\n [1077] ./public/scripts/appNavbar.js 755 bytes {0} [built]\n [1078] ./public/scripts/commonLib.js 547 bytes {0} [built]\n [1111] ./public/scripts/i18n.js 2 kB {0} [built]\n [1120] ./public/scripts/navbarHeaderSettings.js 1.24 kB {0} [built]\n [1160] ./public/scripts/store/main.js 1.08 kB {0} [built]\n [1323] ./~/history/es/index.js 460 bytes {0} [built]\n [1776] ./~/react-hot-loader/index.js 41 bytes {0} [built]\n [1782] ./~/react-hot-loader/lib/patch.js 209 bytes {0} [built]\n [1978] ./~/strip-ansi/index.js 161 bytes {0} [built]\n [2016] multi react-hot-loader/patch webpack-hot-middleware/client ./public/scripts/main 52 bytes {0} [built]\n     + 2002 hidden modules\nwebpack: Compiled successfully. posted on August 15, 2017 by John Glynn John Glynn", "date": "2017-08-15"},
{"website": "Monsanto", "title": "So You Want to Create Excel in the Browser?", "author": ["\n                                        Sean Kemmis\n                                    "], "link": "http://engineering.monsanto.com/2017/06/29/so-you-want-to-create-excel/", "abstract": "Just Use <table> , Right? As developers at Monsanto, we’ve all probably been tasked with recreating a tabular (excel-style) view in our web apps for users, and a majority of us have probably used the <table> tag — or something like bulma or bootstrap columns — to get the job done, and usually, that’s enough. But what about when we need to render hundreds (or thousands!) of cells? What if we want to lock columns, or headers? What if we need to do all three? Well, table and the flex-boxed <div> tags leave a lot to be desired in all of those use cases. In order to fix headers or columns, you either have to render them separately and do a lot of positioning magic on your own, or go through a lot of css trickery. This only gets worse when the table is so large it has to scroll horizontally and/or vertically, and scrolling performance degrades quickly when you have hundreds of rows. My team and I recently needed to generate a heat map, with around 500 rows and between 14 and 30 columns, with each cell being one of 7 colors. When we first implemented it with an HTML table, the performance while scrolling was abysmal, and the fact that we lost the header when scrolling horizontally and the main label column when scrolling horizontally made you lose all context of what row 400—cell 25 meant. So with that, we had three main requirements: Performant scrolling Ability to fix header row(s) Ability to fix label column(s) Without a lot of trickery there’s just no simple way to match all three of those requirements using <table> elements. react-virtualized to the rescue For the first requirement, we’d already been using Brian Vaughn’s react-virtualized — an open source React library that selectively renders elements in a list based on scroll position — for scrolling a horizontal list of React components to great effect. You tell react-virtualized’s List component how to render each individual row, and it tells each row whether it should even exist (am I on the screen?) as well as what position it should be in right now based on your scroll location. Using some trickery of bulma I was able to match the second requirement by rendering a header row of div elements, then underneath render my react-virtualized List , but the third requirement yet eluded me. Fortunately, Brian Vaughn has a working demo of our exact requirements , using a combination of his Grid component along with a ScrollSync component. Implementation While the working demo was what I based everything I did on, I found it to be a little difficult to follow along with if you haven’t already been using react-virtualized , and so I set out to make a few jsFiddle’s to show four ways of rendering the same table, with the fourth way meeting our above requirements. Every table I talk about below will use the same basic set up, except the 4th: const rowCount = 500 const colCount = 30 const cellHeight = 50 const cellWidth = 72 const headers = new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `H ${ idx } ` ) const makeRow = row => new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) In this case, I’m making 500 rows with 30 columns.\nThe header exists as an array of items, [ 'H0', 'H1', ...'H29' ] The body is an array of rows, where each row is an array of [ 'R0C0', 'R0C1', ... 'R0C29' ] Using HTML Table const Table = () => < table > < thead > { headers . map ( header => < th > { header } </ th >) } </ thead > < tbody > { rows . map ( row => < tr > { row . map ( rowData => < td > { rowData } </ td >) } </ tr >) } </ tbody > </ table > ReactDOM . render ( < Table />, document . getElementById ( 'container' ) ); Fiddle Here Simple HTML table, rendering each header item in a <th> tag and rendering each cell as a <td> element as needed through maps. Using react-virtualized Grid Re-Creating the Table with a single Grid Element const headers = new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `H ${ idx } ` ) const makeRow = row => new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) const Cell = ({ columnIndex , key , rowIndex , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { rowIndex === 0 ? headers [ columnIndex ] : rows [ rowIndex - 1 ][ columnIndex ] } </ div > const Table = () => < div > < Grid cellRenderer= { Cell } columnCount= { colCount } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { 500 } rowCount= { rowCount } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } width= { colCount * cellWidth } /> </ div > ReactDOM . render ( < Table />, document . getElementById ( 'container' ) ); Fiddle Here The same table, but rendered one cell at a time by using the Grid component from react-virtualized to give an idea of how it works. cellRenderer is a function that returns a DOM element and receives as props columnIndex , key , rowIndex , and style . I use the columnIndex and rowIndex to determine which cell I’m rendering right now. I use the key and the style props passed in by Grid — The style prop here contains things such as height/width and position on screen, key is used so React knows it’s rendering a unique item. columnCount and rowCount are, simply, the number of columns and rows this Grid will contain. columnHeight and columnWidth are fairly self explanatory, but they can also be functions. In that case, the function would receive an object as an argument, containing a columnIndex value that you could use to determine if specific columns need to be wider or higher than others. rowHeight and rowWidth behave exactly the same as the above. height and width is the overall height and width of the entire Grid – in this case, 500px high and the number of my columns times the width of each cell wide. To see how the virtualization works, I’d suggest inspecting the DOM in that fiddle and scrolling through the table. You will see elements vanish from the DOM and new ones take its place. This is react-virtualized working its magic, and only rendering the components you should be able to see. We’ve successfully met our first requirement, but now what about the rest? Re-Creating the Table With Two Grid Elements const Cell = ({ columnIndex , key , rowIndex , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { rows [ rowIndex ][ columnIndex ] } </ div > const HeaderCell = ({ columnIndex , key , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { headers [ columnIndex ] } </ div > const Table = () => < div > < Grid cellRenderer= { HeaderCell } columnCount= { colCount } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { cellHeight } rowCount= { 1 } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } width= { colCount * cellWidth } /> < Grid cellRenderer= { Cell } columnCount= { colCount } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { 500 } rowCount= { rowCount } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } width= { colCount * cellWidth + scrollbarWidth } /> </ div > ReactDOM . render ( < Table />, document . getElementById ( 'container' ) ); Fiddle Here The same table as the single Grid , but now we’ve locked the header. When we scroll, the header will stay in place. We get the same scrolling performance benefit, but now we’re meeting the second requirement. The data has always been two independent pieces, but now our jsx matches that. The body that scrolls both horizontally and vertically The fixed header which only scrolls horizontally The only changes we had to make to the original table was adding a second Grid that only renders the header, and adding a specific cellRenderer function for both body cells and header cells. Re-Creating the Table with Four Grid Elements This is where things start getting complicated. The setup here is a little different from the rest. const headers = new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `H ${ idx + 1 } ` ) // the fixed header that only scrolls horizontally const makeRow = row => new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx + 1 } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) // the main body const fixedCol = new Array ( rowCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ idx + 1 } C0` ) // the fixed column that only scrolls vertically const fixedCell = ' R0 C0 ' // The fixed cell that never moves I’m splitting my data into four pieces, my rows , my fixedCol (the column that scrolls only horizontally), the fixedCell (the overlap between the fixed header row and the fixed column), and my headers which is my fixed header row. In order to keep that data separate, I had to remove the overlap from headers by dropping the columnCount by 1, and do the same for fixedCol . I’ll go a bit more into this, but first, the JSX. const Cell = ({ columnIndex , key , rowIndex , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { rows [ rowIndex ][ columnIndex ] } </ div > const HeaderCell = ({ columnIndex , key , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { headers [ columnIndex ] } </ div > const FixedColCell = ({ rowIndex , key , style , }) => < div className= 'grid-cell' key= { key } style= { style } > { fixedCol [ rowIndex ] } </ div > const FixedCell = () => < div > { fixedCell } </ div > const Table = () => < ScrollSync > { ({ onScroll , scrollTop , scrollLeft }) => < div > < div style= { { position : 'absolute' , top : 0 , left : 0 , } } > < Grid cellRenderer= { FixedCell } columnCount= { 1 } columnHeight= { cellHeight } columnWidth= { cellWidth } height= { cellHeight } rowCount= { 1 } rowHeight= { cellHeight } rowWidth= { cellWidth } width= { cellWidth } /> </ div > < div style= { { position : 'absolute' , top : cellHeight , left : 0 , } } > < Grid cellRenderer= { FixedColCell } className= { 'no-scroll' } columnCount= { 1 } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { tableHeight } rowCount= { rowCount - 1 } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } scrollTop= { scrollTop } width= { cellWidth } /> </ div > < div style= { { position : 'absolute' , top : 0 , left : cellWidth , } } > < Grid cellRenderer= { HeaderCell } className= { 'no-scroll' } columnCount= { colCount - 1 } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { cellHeight } rowCount= { 1 } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } scrollLeft= { scrollLeft } width= { tableWidth } /> </ div > < div style= { { position : 'absolute' , top : cellHeight , left : cellWidth , } } > < Grid cellRenderer= { Cell } columnCount= { colCount - 1 } columnHeight= { rowCount * cellHeight } columnWidth= { cellWidth } height= { tableHeight + scrollbarWidth } onScroll= { onScroll } rowCount= { rowCount - 1 } rowHeight= { cellHeight } rowWidth= { colCount * cellWidth } width= { tableWidth + scrollbarWidth } /> </ div > </ div > } </ ScrollSync > Fiddle Here Once again we’re rendering the same table, but now we have a locked header and column, successfully meeting all three of our requirements, but what is actually going on? We have to think of this table as four individual pieces. The body that scrolls both horizontally and vertically The fixed header which only scrolls horizontally The fixed column which only scrolls vertically The overlapping cell(s) between the fixed header and fixed column(s) that never move. The easiest way to handle (in my opinion) that is to split our data into those 4 independent items, my headers , my rows , my fixedCol , and my fixedCell elements, and create independent render functions for those elements, HeaderCell , Cell , FixedColCell , and FixedCell (maybe being redundant for the sake of clarity) Now we have to render four Grid elements, but they have to be correctly positioned on the screen relative to each other. The FixedCell should be at left: 0, top: 0 which is the upper-left of our rendering area. The Header should start one cell’s width to the right of FixedCell , so left: cellWidth, top: 0 The FixedCol should start one cell’s height beneath the FixedCell , so left: 0, top: cellHeight The Body should start one cell’s height beneath FixedCell and one cell’s width to the right of FixedCell , so left: cellWidth, top: cellHeight From there, we render our four Grid elements, telling each one how to render their respective cells and how many rows and columns each should have.\nIt’s important to note we have to disable scroll bars on the header and fixed column cells, and in order to do that I passed in a className prop to those two Grid elements that has the overflow: hidden !important rule attached. If that’s all we did (ignoring the ScrollSync component for the time being), then we’d be able to scroll the body but the header and columns wouldn’t scroll with us. By wrapping everything in ScrollSync , we can get a few props, onScroll , scrollTop , and scrollLeft passed in to the underlying component (in this case, our 4 grids) onScroll is a callback that we attach to the main body Grid , so that it gets called whenever the body scrolls scrollLeft is the horizontal scrolling position that we need to attach to the header Grid . scrollTop is the vertical scrolling position that we need to attach to the fixed column Grid . By attaching these, whenever we scroll the body, the onScroll callback is called, updating scrollLeft and scrollTop values, and the two components those are attached to will scroll with us. And there we have it, a 4 piece Grid , aligned correctly, with a scrollable body and a fixed header and fixed column. You can get even more advanced from here, and lock multiple columns or rows to create any scrollable grid layout. posted on June 29, 2017 by Sean Kemmis ← Previous Post Next Post → const rowCount = 500 const colCount = 30 const cellHeight = 50 const cellWidth = 72 const headers = new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `H ${ idx } ` ) const makeRow = row => new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) const rowCount = 500 const colCount = 30 const cellHeight = 50 const cellWidth = 72 const headers = new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `H ${ idx } ` ) const makeRow = row => new Array ( colCount ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) const headers = new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `H ${ idx + 1 } ` ) // the fixed header that only scrolls horizontally const makeRow = row => new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx + 1 } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) // the main body const fixedCol = new Array ( rowCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ idx + 1 } C0` ) // the fixed column that only scrolls vertically const fixedCell = ' R0 C0 ' // The fixed cell that never moves const headers = new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `H ${ idx + 1 } ` ) // the fixed header that only scrolls horizontally const makeRow = row => new Array ( colCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ row } C ${ idx + 1 } ` ) const rows = new Array ( rowCount ). fill ( 0 ). map (( val , idx ) => makeRow ( idx )) // the main body const fixedCol = new Array ( rowCount - 1 ). fill ( 0 ). map (( val , idx ) => `R ${ idx + 1 } C0` ) // the fixed column that only scrolls vertically const fixedCell = ' R0 C0 ' // The fixed cell that never moves posted on June 29, 2017 by Sean Kemmis Sean Kemmis", "date": "2017-06-29"},
{"website": "Monsanto", "title": "Drone Drone Drone", "author": ["\n                                        Justin Schlechte\n                                    "], "link": "http://engineering.monsanto.com/2017/06/20/drone-drone-drone/", "abstract": "CI at Monsanto Continuous integration is a given in any modern software shop.\nWhile there are a plethora of options today, in a traditional Java shop, CI seems to mean Jenkins.\nOver the past few years, however, there has been a big shift toward the cloud and team autonomy.\nAs part of that shift, teams began choosing different tech stacks and different deployment pipelines to best support their users.\nWhen our team made the hop to the cloud, we opted to use Drone CI as our CI system. What is Drone? Drone CI is a CI system built on Docker.\nEach build step is run in a temporary container.\nBuild configurations specify which image to use for each step.\nAnd build configurations are stored in the source code.\nWhen it all comes together, you have a flexible build system based on lightweight containers that can spin up additional support containers during the build. Why Choose Drone? Late 2015, my team began development on its first cloud based application.\nAt the time I had recently heard about Drone CI from a friend at another company and convinced my team to give it a shot.\nWe were hoping that Drone would alleviate some of the issues that we had experienced with Jenkins. For one, as our tech stack changed, Jenkins agents had to have ALL of the tools that any branch of any repository required.\nI know technically you can prescribe builds to run on specific agents, but then you end up with A LOT of agents just waiting for orders.\nDrone, however, runs each build step in a seperate container, so there wouldn’t be just One Agent Image To Rule Them All. Also, as with any healthy software team, pipelines evolved.\nJenkins made it difficult to manage different pipelines for different branches of each of the many repositories we maintained.\nDrone keeps its build configuration in the repository making branch specific build processes easier to manage. Finally, we develop a lot of APIs that depend on databases or other resources.\nIn the past, we would run integration tests against the existing dev environment.\nThis was especially troublesome as we had data model transitions.\nDrone would hopefully allow us to spin up ephemeral resources for testing purposes. Analysis Configuration Configuring Drone was extremely simple.\nAfter the host and agents are running, iteraction with Drone itself tends to be fairly small.\nWe see the build status on our PRs in GitHub and build-breakers are notified in Slack.\nPipelines are managed in the .drone.yml files.\nDrone is unopinionated about your pipeline, so we were able to develop a process that fits our environment and our application.\nWe have had hiccups along the way, but we’ve ultimately found ourselves managing our .drone.yml files very little after our pipeline was stabilized.\nWe found the ability to spin up service containers to be extremely valuable since we have integration tests that depend on ephemeral instances of Postgres, Kafka, MongoDB, ElasticSearch, and more. Example Host Configuration DRONE_OPEN : true DRONE_ORGS : MyGitHubOrg DRONE_GITHUB : true DRONE_ADMIN : my_github_user,your_github_user DRONE_GITHUB_URL : https://github.my-company.com DRONE_GITHUB_CLIENT : **** DRONE_GITHUB_SECRET : **** DRONE_GITHUB_PRIVATE_MODE : true DRONE_SECRET : a_secret_password_for_the_agent_server_communications Example Agent Configuration DRONE_SERVER : wss://drone.my-company.com/ws/broker DRONE_SECRET : a_secret_password_for_the_agent_server_communications Example .drone.yml This is a pretty near approximation to how we’ve been structuring our pipeline for node apps. pipeline : check_signature : image : alpine commands : - ' [ $DRONE_YAML_VERIFIED = true ] && [ $DRONE_YAML_SIGNED = true ]' patch_version : image : node commands : - npm version patch -m '[npm-release][skip ci] %s' when : events : push branch : release prerelease_version : image : node commands : - npm version prerelease -m '[npm-release][skip ci] %s' when : events : push branch : develop test : image : node commands : - npm install - npm test when : events : [ push , pull_request ] push_tags : image : plugins/git-push remote_name : origin local_branch : refs/tags/* branch : refs/tags/* when : events : push branch : release push_to_release : image : plugins/git-push remote_name : origin branch : release when : events : push branch : release push_to_develop : image : plugins/git-push remote_name : origin branch : develop when : events : push branch : - develop - release # redacted deployment, artifact archival, notifications Plugins (aka Scripts in Docker Containers) Creating a new Drone plugin is conceptually straightforward as well.\nWrite a script and get parameters from the environment.\nPut that script in a docker image and post to your docker registry. Secrets We’ve only used versions 0.4 and 0.5 so far.\nSecret management in version 0.5 is far better that version 0.4, but they revamped the secret system again in version 0.6.\nIn version 0.5, secrets are stored on the Drone host and are exposed only to certain images.\nAs an additional layer of security, your build configuration file has to be signed for the secrets to be injected. Should You Use Drone? Yes, unless… You don’t know what docker is.  Drone is most potent when placed in the hands of those who know and understand docker. You are unwilling to ask for help.  The Drone developers are active on Gitter and have been very helpful. You refuse to use beta software.  At this time, the developers are quickly moving toward a 1.0 release, but the software is still in beta. You are already super happy with Jenkins. Conclusion Our team has benefitted greatly from Drone, and I encourage you to take a look.\nReminder though, Drone is still beta software and things have been changing fairly quickly in the last few months as the development team approaches a version 1.0.\nAs beta software, we sometimes ended up source diving to better understand how Drone would behave in specific situations. posted on June 20, 2017 by Justin Schlechte ← Previous Post Next Post → DRONE_OPEN : true DRONE_ORGS : MyGitHubOrg DRONE_GITHUB : true DRONE_ADMIN : my_github_user,your_github_user DRONE_GITHUB_URL : https://github.my-company.com DRONE_GITHUB_CLIENT : **** DRONE_GITHUB_SECRET : **** DRONE_GITHUB_PRIVATE_MODE : true DRONE_SECRET : a_secret_password_for_the_agent_server_communications DRONE_OPEN : true DRONE_ORGS : MyGitHubOrg DRONE_GITHUB : true DRONE_ADMIN : my_github_user,your_github_user DRONE_GITHUB_URL : https://github.my-company.com DRONE_GITHUB_CLIENT : **** DRONE_GITHUB_SECRET : **** DRONE_GITHUB_PRIVATE_MODE : true DRONE_SECRET : a_secret_password_for_the_agent_server_communications DRONE_SERVER : wss://drone.my-company.com/ws/broker DRONE_SECRET : a_secret_password_for_the_agent_server_communications DRONE_SERVER : wss://drone.my-company.com/ws/broker DRONE_SECRET : a_secret_password_for_the_agent_server_communications pipeline : check_signature : image : alpine commands : - ' [ $DRONE_YAML_VERIFIED = true ] && [ $DRONE_YAML_SIGNED = true ]' patch_version : image : node commands : - npm version patch -m '[npm-release][skip ci] %s' when : events : push branch : release prerelease_version : image : node commands : - npm version prerelease -m '[npm-release][skip ci] %s' when : events : push branch : develop test : image : node commands : - npm install - npm test when : events : [ push , pull_request ] push_tags : image : plugins/git-push remote_name : origin local_branch : refs/tags/* branch : refs/tags/* when : events : push branch : release push_to_release : image : plugins/git-push remote_name : origin branch : release when : events : push branch : release push_to_develop : image : plugins/git-push remote_name : origin branch : develop when : events : push branch : - develop - release # redacted deployment, artifact archival, notifications pipeline : check_signature : image : alpine commands : - ' [ $DRONE_YAML_VERIFIED = true ] && [ $DRONE_YAML_SIGNED = true ]' patch_version : image : node commands : - npm version patch -m '[npm-release][skip ci] %s' when : events : push branch : release prerelease_version : image : node commands : - npm version prerelease -m '[npm-release][skip ci] %s' when : events : push branch : develop test : image : node commands : - npm install - npm test when : events : [ push , pull_request ] push_tags : image : plugins/git-push remote_name : origin local_branch : refs/tags/* branch : refs/tags/* when : events : push branch : release push_to_release : image : plugins/git-push remote_name : origin branch : release when : events : push branch : release push_to_develop : image : plugins/git-push remote_name : origin branch : develop when : events : push branch : - develop - release # redacted deployment, artifact archival, notifications posted on June 20, 2017 by Justin Schlechte Justin Schlechte", "date": "2017-06-20"},
{"website": "Monsanto", "title": "Using Client-Side Git Hooks", "author": ["\n                                        Pat Gaffney\n                                    "], "link": "http://engineering.monsanto.com/2017/05/17/using-client-side-git-hooks/", "abstract": "Client-Side Git Hooks Git hooks are a simple solution to an ongoing problem — sometimes we push ugly, breaking code. Sometimes we get wrapped up in the problem we’re solving and forget that other humans are going to have to read and understand this in the future. Our fellow developers are nice people, they shouldn’t be subjected to our forgetfulness. Often our solutions to this problem are overtly complex. We could set up our test suite to run every time we save a file, but that would get out of hand fairly quickly. We could let our editors paint code red as we type expressions our linter finds unsatisfactory, but do we really want our editor to yell at us? We could set up elaborate events to trigger on some automation server that runs the test suite and lints the code — but why should we catch this problem only after our code has been committed to history and deployed? Git hooks aim to automate this problem. Hooks are just scripts to be triggered when important events occur. When a specific git command is issued, a check is made to determine if there is an associated script to run. Hooks can be written for the client or the server, but we’re going to focus on the client here. Sanity Checks Let’s say we’re writing a React front-end for a web application and we’re linting our code with ESLint . In the age of JSX, it’s important for our team that we maintain a consistant style for our rendered markup. To sanity-check the code we’ve written, we can write a pre-commit hook to be triggered everytime someone on our team issues git commit . #!/bin/bash function print_intro { user = $( git config --get user.name ) echo '\\033[1;34m➡ Time to pay the troll toll, ' \" $user \" '...\\033[0m' run_linter } function run_linter { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } function print_outro { if [ $? == 0 ] ; then echo '\\033[1;32m✔︎ Linter checks out! Pod bay doors opening...\\033[0m' exit 0 else echo '\\033[1;31m𝙓 Linter threw up! Pod bay doors forever sealed...\\033[0m' exit 1 fi } print_intro If you’re hazy on your shell scripting, don’t worry — there’s very little going on here. First, we grab the user’s name from their git config . A small alert is printed, and we tell our linter to run. Finally, in print_outro we determine if the linter exited successfully via $? . If it exited with errors — a non-zero status — our script shares in the disappointment and returns a general error status of 1 to the calling shell. The important thing to note here is that when the linter exited gracefully, we followed suit. We only exit with an error if the linter did. This is how virtually all git hooks operate. The pre-commit hook is fired before the user even types a commit message. It has full access to the snapshot of the repository at that moment. Exiting from this hook with a non-zero status will abort the commit — effectively preventing the user from commiting their changes. This can be frustrating, so it’s important to alert the user that something went wrong. We also write all of the linter’s output to stdout so the errors can be easily detected. git leaves the staging area unchanged, so any errors can be fixed and another commit can be issued in quick succession. There are client-side hooks for almost all of the events git triggers — pre-commit is just the tip of the iceberg. Hook Installation The only trouble with client-side hooks is installing them. git looks for hooks in the hooks subdirectory of the $GITDIR — by default the relative path is .git/hooks/ . When a new repository is initialized, the hooks directory is filled with a series of sample hooks that are turned off by default. Any executable file in the hooks/ directory that is named for the event it should be triggered against will be picked up by git . In order to keep all developers on our team up-to-date with the proper hooks, we’ve found it best to check them into git . We have a hooks/ directory at the root of all our repositories where hooks reside. We use a simple installation script to create symbolic links between the version-controlled hooks in this directory and their home in .git/hooks/ : #!/bin/bash # Symlink all the githooks. # ln -s :: create symbolic link # ln -v :: verbose mode- print detail # ln -f :: overwrite target file if it exists GITDIR = \"../.git/hooks\" CURDIR = $( basename \" $PWD \" ) echo '\\033[1;34m➡ Installing all the githooks...\\033[0m' if [[ $CURDIR != \"hooks\" ]] ; then echo '\\033[1;31m✘ You need to execute this command from the hooks/ directory\\033[0m' exit 1 fi # Create symlink for each githook into $GITDIR for file in $( ls -A ) do\n    if [[ $file == \"installhooks.sh\" ]] ; then\n        continue\n    elif [[ -x $file ]] ; then ln -svf \" $PWD / $file \" \" $GITDIR \" else echo '\\033[1;34m➡ Skipping' \" $file \" 'because you cannot execute it\\033[0m' fi\ndone echo '\\033[1;32m✔︎ Githooks successfully installed...\\033[0m' Once again, this shell script is fairly simple. It first checks to ensure you’re issuing it from the hooks/ directory (this is important for the symlink), then loops through the files, installing those whose executable bit is flipped. Because all the hooks are checked into git , any changes made to a hook are circulated to all developers on our team the next time they run git pull . The symlinks take care of the rest — ensuring that the hooks reside where git will find them. If for some reason shell scripts aren’t your thing, no worries, you can write your hooks in any language that will exit with a Unix status code. Popular Amongst Friends Client-side hooks are ideal for these types of repetitive tasks: linters, tests, formatting merge commits, and so on. They provide a benefit best served over time — our team’s pre-commit hooks keep our code in a consistent, working state. The script written in this post was improved upon in a second post . posted on May 17, 2017 by Pat Gaffney ← Previous Post Next Post → #!/bin/bash function print_intro { user = $( git config --get user.name ) echo '\\033[1;34m➡ Time to pay the troll toll, ' \" $user \" '...\\033[0m' run_linter } function run_linter { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } function print_outro { if [ $? == 0 ] ; then echo '\\033[1;32m✔︎ Linter checks out! Pod bay doors opening...\\033[0m' exit 0 else echo '\\033[1;31m𝙓 Linter threw up! Pod bay doors forever sealed...\\033[0m' exit 1 fi } print_intro #!/bin/bash function print_intro { user = $( git config --get user.name ) echo '\\033[1;34m➡ Time to pay the troll toll, ' \" $user \" '...\\033[0m' run_linter } function run_linter { node node_modules/eslint/bin/eslint.js --fix --ext .js,.jsx src/ test /\n\n    print_outro } function print_outro { if [ $? == 0 ] ; then echo '\\033[1;32m✔︎ Linter checks out! Pod bay doors opening...\\033[0m' exit 0 else echo '\\033[1;31m𝙓 Linter threw up! Pod bay doors forever sealed...\\033[0m' exit 1 fi } print_intro #!/bin/bash # Symlink all the githooks. # ln -s :: create symbolic link # ln -v :: verbose mode- print detail # ln -f :: overwrite target file if it exists GITDIR = \"../.git/hooks\" CURDIR = $( basename \" $PWD \" ) echo '\\033[1;34m➡ Installing all the githooks...\\033[0m' if [[ $CURDIR != \"hooks\" ]] ; then echo '\\033[1;31m✘ You need to execute this command from the hooks/ directory\\033[0m' exit 1 fi # Create symlink for each githook into $GITDIR for file in $( ls -A ) do\n    if [[ $file == \"installhooks.sh\" ]] ; then\n        continue\n    elif [[ -x $file ]] ; then ln -svf \" $PWD / $file \" \" $GITDIR \" else echo '\\033[1;34m➡ Skipping' \" $file \" 'because you cannot execute it\\033[0m' fi\ndone echo '\\033[1;32m✔︎ Githooks successfully installed...\\033[0m' #!/bin/bash # Symlink all the githooks. # ln -s :: create symbolic link # ln -v :: verbose mode- print detail # ln -f :: overwrite target file if it exists GITDIR = \"../.git/hooks\" CURDIR = $( basename \" $PWD \" ) echo '\\033[1;34m➡ Installing all the githooks...\\033[0m' if [[ $CURDIR != \"hooks\" ]] ; then echo '\\033[1;31m✘ You need to execute this command from the hooks/ directory\\033[0m' exit 1 fi # Create symlink for each githook into $GITDIR for file in $( ls -A ) do\n    if [[ $file == \"installhooks.sh\" ]] ; then\n        continue\n    elif [[ -x $file ]] ; then ln -svf \" $PWD / $file \" \" $GITDIR \" else echo '\\033[1;34m➡ Skipping' \" $file \" 'because you cannot execute it\\033[0m' fi\ndone echo '\\033[1;32m✔︎ Githooks successfully installed...\\033[0m' posted on May 17, 2017 by Pat Gaffney Pat Gaffney", "date": "2017-05-17"},
{"website": "Monsanto", "title": "Building resilient scalable workflows", "author": ["\n                                        Stephen Grodeon\n                                    "], "link": "http://engineering.monsanto.com/2017/05/15/building-resilient-scalable-workflows/", "abstract": "The backstory When I first came to Monsanto, our typical tech stack was Java with Spring/Hibernate and a whole slew of common libraries that made our life easier. On my team, we were using many bioinformatics tools to analyze DNA which are CPU and memory intensive. Our workload was mostly idle with random huge spikes of work; so we needed to design systems that could handle large volumes of heavy lifting. Enter the data center and parallel computing. What went wrong Our data center at the time promised to have 1000+ nodes of compute power, so we designed workflows to be massively parallel. The first problem was that our workflows are sequential. We take the results of one command and feed them into the next one, then aggregate all the results at the end. We tend to run up to nearly 100 workflows at a time, so the idea was to run different commands from different workflows at once. Each step was designed as an asynchronous job we submitted to the grid with various throttling mechanisms in place to prevent overloading external dependencies, such as NFS. We decided to use Oracle BPM to orchestrate the various commands which we exposed with cxf soap services. In theory, we had a system that was very parallel with tons of error handling. In reality, we had a mix and match of so many architectures that within just a few years became extremely unstable and overloaded. The redesign New Architecture Diagram With our volume of work only increasing over the years, we were under pressure to redesign this system to handle larger volumes of work faster and more reliably. For our API, we decided Node.js would be a better fit for us. We can spin up Node.js micro services really fast, and I personally prefer working with the operating system without a strongly typed language (many of the bioinformatics tools we use don’t have APIs so we use the command line.) To handle our large volumes of work, we have our API submit jobs to Amazon SQS and have small worker applications that poll for any work. We only send identifiers in our queue message, so any additional information will be retrieved from an AWS data-store by our worker. These workers are usually a specialized EC2 instance that live under an auto-scaling group that has an algorithm installed on it. We typically have one of these workers polling multiple queues, one for each command our workflow uses. We only install one set of tools on an EC2 instance so different tools can scale independently. Using this design, we can spin up additional EC2 instances when a large burst of work comes in, then shut them down when we’re finished. Rather than paying for 1000 nodes in our data center, we’re only paying for the servers we need when we need them. We also gained the benefit of having the workers reserved exclusively for our use, rather than competing with other grid clients for grid nodes. During peak usage times on our internal grid, it was not unusual to sit in the queue for an hour or more. Increasing performance One issue we encountered is with the spin up time of an EC2 instance. From start to finish our servers were taking about 5 minutes to spin up, and the work they needed to perform could take anywhere from seconds to hours. We didn’t mind the spin up time on long running jobs, but for the small jobs this was unacceptable. After lots of tweaking, we decided the best approach was to take a rather large EC2 instance and spin up a bunch of docker containers. We leave 1 EC2 instance running all the time and will spin up additional EC2 instances based on the size of the queue. We scale up using small increments if the queue is slightly backed up, and scale up using large percentages if we are extremely backed up. Our Auto-Scaling policy During our load testing, we submitted 20,000 jobs to our queue. Using the old data center it would have taken 2 days to process this many jobs, and we couldn’t really calculate how much it cost. With our new AWS infrastructure we completed the same 20,000 jobs in 90 minutes with $32 of EC2 charges. One of the biggest reasons for the performance difference is we have too much demand on our grid nodes internally, and the grid architecture itself was a huge performance bottleneck. Getting resilience Getting great performance is one thing, but when you’re submitting high volumes of jobs, errors have a tendency to pop up frequently. The vast majority of errors we see are timeouts and various temporary issues that can be solved with simple retry mechanisms. Using our queue pattern, there is a very easy way to solve this that has minimal coding impact. SQS has guaranteed delivery which you can back with a redrive policy. After we consume a message, we do all the work, if it was successful we delete the message and call it a day. If an error pops up the message is not deleted, it will eventually time out and SQS will automatically send it out for delivery again. If after a certain amount of retries it still fails, we leave it in a dead letter queue for further investigation. In our cases this is usually bad data the submitter sent us and we have to manually investigate, so if it ever hits the dead letter queue it is something we would have needed to look at anyways. All of the components in this new design communicate using queues rather than directly relying on service calls. This allows any application to go down without causing an outage to any dependent applications. During the downtime the jobs will continue to queue up and consuming from the queue will resume when the application is brought back to life. Final thoughts Working with parallelization has been an interesting experience for me and having used multiple approaches, I can really appreciate how much easier it is to design massively parallel systems using AWS. Since we’ve adopted Node.js micro services with SQS, we’ve had very few infrastructure problems and we deliver entire workflows at a pace we never would have thought possible in our Java days. We have embraced the queue in / queue out pattern in nearly all of our work and enjoy the freedom to scale whenever and wherever we want to. posted on May 15, 2017 by Stephen Grodeon ← Previous Post Next Post → posted on May 15, 2017 by Stephen Grodeon Stephen Grodeon", "date": "2017-05-15"},
{"website": "Monsanto", "title": "Exploring GitHub Enterprise", "author": ["\n                                        Patrick Byrne\n                                    "], "link": "http://engineering.monsanto.com/2017/01/13/github-enterprise-org-explorer/", "abstract": "Don’t care about the backstory? Skip ahead A story of two GitHubs Earlier this year Monsanto began the process of migrating from github.com to a private GitHub Enterprise instance.  Frankly, we love almost everything about it. GitHub.com Let’s start out with the things that were amazing about github.com when we first started using it. Being able to access our source code without a VPN tunnel was a huge boon for all of our developers that don’t happen to work at our main campus.  The global nature of github.com is what allowed us to start our migration to other people’s computers (i.e. the cloud). On top of that, github.com has an amazing search feature.  It’s fast, it’s accurate, and it just works.  The ability to make some of our code open source, something that our dev teams had been trying to get approved for years, finally became a possibility.  Many of our teams were happy to abandon whatever they were using for issue tracking in favor of per project issue tracking. There were some issues though. In github.com we had to do crazy things like make all our repos private, add all org members to a special team with read only access to things, and then remember to add that team to every repo we created.  Lots of time was wasted trying to figure out why someone couldn’t clone a repo, but someone else could.  In addition, we started to quickly creep up on our private repo limit.  GitHub pages also were usually a no go since, even if the repo was private, the published pages were still open to the world.  There was the possibility that something critical could be set to public by accident.  When people left the company, they were never actually removed from our org!  Most of this was eventually solved with some clever automation, but some of these issues left a bad taste behind. GitHub Enterprise Some of us were tasked with investigating some alternatives to github.com.  The evaluation process could probably be its own post, but eventually we settled on GitHub Enterprise (GHE). Instantly some of our bigger issues were resolved. GHE uses SAML to connect directly to our federated identity source.  No more needing to worry about adding new users.  We had as many repos as we had hard drive space for.  Pages became an awesome way to add documentation to our repos that are both interesting and interactive.  Repos could be public by default so that all our devs could browse and search code, and nobody had to worry about accidentally exposing sensitive info.  We could group our code into orgs that actually represented a product group, or a team. But that’s where the next set of problems started.  Exploration and discovery in GHE was pretty painful.  If you had a good idea what you were looking for, the search would usually get you there, but if you needed to do a bit of exploration, you were probably going to hit a wall. Exploration and discovery of organizations is painful in GHE. There is no built in UI to simply list the orgs that are in a GHE instance.  By default, org membership is private and has to be changed by each individual user for each org they are part of. Even if you found the org you wanted, you had no way of knowing who to ask about it.  Our final hope rested with the GitHub API. Enter the GitHub Org Explorer Org Explorer is a React + material-ui based tool with a couple of built in services, all served from a minimal Express instance.  The service layer simply queries your GHE API so there isn’t any persistent data store. Exploring the organizations in your GHE instance Dummy data provided by the npm-expansion and lorem-ipsum modules The main screen of the app provides some simple client side sorting and filtering.  The org’s avatar and description are displayed, and the org’s login (the part that shows in the URL) is humanized and titlized.  Unfortunately, the GitHub API doesn’t provide the org’s display name on the /v3/orgs endpoint.  Each org has a link to itself in GHE as well as a link to the Org Explorer’s details page for that org. Getting down to details Personal info has been blurred to protect the innocent. The details page displays some of the more hidden information that can be pulled from the API.  To be able to pull back most of this info, you’ll need to be running the app with a Personal Access Token for someone that’s a member of every org.  An easy way to do this is the GHE CLI command ghe-org-admin-promote which will add site admins to all orgs as org admins.  Honestly this is our biggest complaint with GHE.  We would love to see some GHE specific APIs to help out site admins.  Even being an org admin, the API still doesn’t let us find who org owners are, for example.  Because of this work around, GHE site admins are pulled into their own member’s sections.  Hopefully this will eventually be something that can be removed if an improved API is ever provided. Running it yourself Specifics are provided in the project’s README, but the short and sweet of it is run npm install , set some environment variables, and run npm start .  By default the app will start up on localhost:3099 and you’ll be ready to go.  To get the avatar icons to display you’ll need to be logged into your GHE UI.  There’s no built in security so you’ll want to either run this inside your firewall, or behind a reverse proxy like nginx . posted on January 13, 2017 by Patrick Byrne ← Previous Post Next Post → posted on January 13, 2017 by Patrick Byrne Patrick Byrne", "date": "2017-01-13"},
{"website": "Monsanto", "title": "Easy Node Authentication With Ping", "author": ["\n                                        Brenden McKamey\n                                    "], "link": "http://engineering.monsanto.com/2016/03/11/passport-ping-oauth2/", "abstract": "Easy Node Authentication With Ping Introduction Adding authentication and login capability in Node can be painful. The purpose of this tutorial is to showcase the capabilities of passport-ping-oauth2 within a basic Node application, and it will teach you how to leverage the module within your own Node applications. If you don’t already have Node.js installed, you will need it, and you can install it here . Note If you don’t have PingFederate in your environment, but you’d still like to learn about authentication in Node. I’d highly recommend using these tutorials instead. What we’ll be building: We will build an application that will allow you to leverage a PingFederate OAuth client to handle user authentication and the storage of the corresponding OpenID Connect data. End Product Sneak Peek: A basic login screen that of course uses Ping. After a user has logged in, they’ll be able to see their OpenID Connect profile data: That’s it, pretty simple! So let’s dive in. Application Structure - app ------ models ------------ user.js ------ routes.js\n- config ------ auth.js ------ database.js ------ passport.js\n- views ------ pages ------------ index.ejs ------------ profile.ejs ------ partials ------------ header.ejs\napp.json\nindex.js\npackage.json Go ahead and create all the files and folders above and we’ll fill them in as we go along. Packages package.json This file handles all the packages needed for our application. { \"name\" : \"PingPassportTutorial\" , \"description\" : \"Easy Node Authentication With Ping.\" , \"version\" : \"0.1.5\" , \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"node index.js\" }, \"dependencies\" : { \"ejs\" : \"2.3.3\" , \"express\" : \"4.13.3\" , \"mongoose\" : \"*\" , \"passport\" : \"*\" , \"passport-ping-oauth2\" : \"*\" , \"connect-flash\" : \"*\" , \"bcrypt-nodejs\" : \"*\" , \"morgan\" : \"*\" , \"body-parser\" : \"*\" , \"cookie-parser\" : \"*\" , \"method-override\" : \"*\" , \"express-session\" : \"*\" }, \"engines\" : { \"node\" : \"0.12.7\" }, \"repository\" : { \"type\" : \"git\" , \"url\" : \"https://github.com/exampleuser/examplerepo\" }, \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"license\" : \"MIT\" } Most of these are pretty self-explanatory; however, here is a list of the dependencies and the functionality they provide. EJS is a client-side templating language. Express is a web development framework for node.js. Mongoose is an object modeling tool designed to work in an asynchronous environment. Passport is authentication middleware for Node.js. Connect-flash is flash message middleware for Connect and Express. Application Setup index.js This file will be the glue for our entire application. var express = require ( ' express ' ); var app = express (); var mongoose = require ( ' mongoose ' ); var passport = require ( ' passport ' ); var flash = require ( ' connect-flash ' ); var morgan = require ( ' morgan ' ); var cookieParser = require ( ' cookie-parser ' ); var bodyParser = require ( ' body-parser ' ); var session = require ( ' express-session ' ); var configDB = require ( ' ./config/database.js ' ); mongoose . connect ( configDB . url ); require ( ' ./config/passport ' )( passport ); app . use ( morgan ( ' dev ' )); app . use ( cookieParser ()); app . use ( bodyParser ()); app . set ( ' port ' , ( process . env . PORT || 5000 )); app . use ( express . static ( __dirname + ' /public ' )); // views is directory for all template files app . set ( ' views ' , __dirname + ' /views ' ); app . set ( ' view engine ' , ' ejs ' ); app . use ( session ({ secret : ' tutorialsecret ' })); app . use ( passport . initialize ()); app . use ( passport . session ()); app . use ( flash ()); require ( ' ./app/routes.js ' )( app , passport ); app . listen ( app . get ( ' port ' ), function () { console . log ( ' Node app is running on port ' , app . get ( ' port ' )); }); Database Config config/database.js module . exports = { ' url ' : ' your-settings-here ' // looks like mongodb://<user>:<pass>@mongo.onmodulus.net:27017/Mikha4ot }; Fill this in with your own database. If you don’t have a MongoDB database lying around, you can install it locally. You can find instructions here: An Introduction to MongoDB . Routes app/routes.js module . exports = function ( app , passport ) { // route for home page app . get ( ' / ' , function ( req , res ) { res . render ( ' pages/index.ejs ' ); // load the index.ejs file }); // route for showing the profile page app . get ( ' /profile ' , isLoggedIn , function ( req , res ) { res . render ( ' pages/profile.ejs ' , { user : req . user , flash : req . session . flash }); }); // route for logging out app . get ( ' /logout ' , function ( req , res ) { req . logout (); res . redirect ( ' / ' ); }); // route for signing in app . get ( ' /auth/oauth2 ' , passport . authenticate ( ' oauth2 ' , { scope : [ ' openid ' , ' profile ' , ' email ' ] })); // route for call back app . get ( ' /auth/oauth2/callback ' , passport . authenticate ( ' oauth2 ' , { failureRedirect : ' /login ' }), function ( req , res ) { res . redirect ( ' /profile ' ); }); }; // route middleware to make sure a user is logged in function isLoggedIn ( req , res , next ) { // if user is authenticated in the session, carry on if ( req . isAuthenticated ()) return next (); // if they aren't redirect them to the home page res . redirect ( ' / ' ); } Views views/partials/header.ejs <title> My Simple Passport Tutorial </title> <link rel= \"stylesheet\" type= \"text/css\" href= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css\" /> <script src= \"https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\" ></script> <script type= \"text/javascript\" src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js\" ></script> <link rel= \"stylesheet\" type= \"text/css\" href= \"/stylesheets/main.css\" /> This header file is ported into each of the views, so we add all of our application wide CSS and JS here. views/pages/index.ejs <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > <p> Login with: </p> <a href= \"/auth/oauth2\" class= \"btn btn-primary\" > Ping </a> </div> </body> </html> Just a simple button that points to our route for ‘/auth/oauth2’ and triggers the passport authentication. views/pages/profile.ejs <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > < % if ( flash ) { % > <div class= \"row\" > <div class= \"<%= flash.type %>\" > <button type= \"button\" class= \"close\" data-dismiss= \"alert\" aria-hidden= \"true\" > &times; </button> <p><strong>< %= flash . title % ></strong></p> <p>< %= flash . messages % ></p> </div> </div> < % } % > <div class= \"page-header text-center\" > <h1><span class= \"fa fa-anchor\" ></span> Profile Page </h1> <a href= \"/logout\" class= \"btn btn-default btn-sm\" > Logout </a> </div> <div class= \"row\" > <div class= \"col-md-6 col-md-offset-3 well\" > <h3 class= \"text-primary\" > Ping </h3> <dl> <dt> ID: </dt> <dd>< %= user . ping . id % ></dd> <dt> Email: </dt> <dd>< %= user . ping . email % ></dd> <dt> Name: </dt> <dd>< %= user . ping . name % ></dd> < % if ( user . ping . entitlements !== null && user . ping . entitlements . length > 0) { %> <dt> Entitlements: </dt> <dd> <ul> < % for ( var i = 0; i < user . ping . entitlements . length ; ++ i ) { % > <li>< %= user . ping . entitlements [ i ] % ></li> < % } % > </ul> </dd> < % } % > </dl> </div> </div> </div> </body> </html> A bit more complicated, but you can pass flash messages like this to any of your routes that are setup to handle them. req . session . flash = { type : ' alert alert-dismissible alert-danger ' , title : ' Access Denied ' , messages : ' Your access was denied because you do not have an entitlement for this application; however, we ' ll still show you your profile data . ' };\nres.redirect(303, ' / profile ' ); The profile view also renders our database data and loops through the array of entitlements. User Model app/models/user.js We will be storing id , token , email , name , and entitlements in our database. Normally there isn’t a need to store the token; however, for demonstration purposes you’ll be able to see how the passport library decrypts and passes the base 64 encoded data back. // app/models/user.js // load the things we need var mongoose = require ( ' mongoose ' ); // define the schema for our user model var userSchema = mongoose . Schema ({ ping : { id : String , token : String , email : String , name : String , entitlements : [ String ] } }); // create the model for users and expose it to our app module . exports = mongoose . model ( ' User ' , userSchema ); You’ll notice that id, token, email, and name are just normal strings, but entitlements is an array of strings. Okay, on to the fun stuff. Creating our Ping OAuth Client First we’re going to create a new OAuth client. After you’ve logged into your PingFederate administration console, click on OAuth Settings and then click on Client Management . Add a new client. You’ll be using settings somewhat like this; however you’ll need your own Default Access Token Manager and OpenID Connect Policy settings corresponding with your environment data. Remember to save the Client Secret ! Go here if you need assistance with the Default Access Token Manager or OpenID Connect Policy . Okay, that wasn’t too bad, so let’s make it all work with two more files. Passport Strategy config/auth.js Populate your auth.js file appropriately from the Ping Configuration you just completed. It should look something like this, except with your environment specific information: module . exports = { ' pingAuth ' : { ' authorizationURL ' : ' https://www.example.com/as/authorization.oauth2 ' , ' tokenURL ' : ' https://www.example.com/as/token.oauth2 ' , ' clientID ' : ' EXAMPLE_CLIENT_ID ' , ' clientSecret ' : ' EXAMPLE_CLIENT_SECRET ' , ' callbackURL ' : ' http://localhost:5000/auth/oauth2/callback ' , } }; config/passport.js This is the last piece of the puzzle, the file that handles all authentication and data storage. var passport = require ( ' passport ' ); var OAuth2Strategy = require ( ' passport-ping-oauth2 ' ). Strategy ; var User = require ( ' ../app/models/user ' ); var configAuth = require ( ' ./auth ' ); module . exports = function ( passport ) { // used to serialize the user for the session passport . serializeUser ( function ( user , done ) { done ( null , user . id ); }); // used to deserialize the user passport . deserializeUser ( function ( id , done ) { User . findById ( id , function ( err , user ) { done ( err , user ); }); }); passport . use ( new OAuth2Strategy ({ authorizationURL : configAuth . pingAuth . authorizationURL , tokenURL : configAuth . pingAuth . tokenURL , clientID : configAuth . pingAuth . clientID , clientSecret : configAuth . pingAuth . clientSecret , callbackURL : configAuth . pingAuth . callbackURL }, function ( accessToken , refreshToken , params , profile , done ) { User . findOne ({ ' ping.id ' : profile . cn }, function ( err , user ) { if ( err ) { return done ( err ); } if ( user ) { return done ( null , user ); } else { var newUser = new User (); newUser . ping . id = profile . id ; newUser . ping . token = accessToken ; newUser . ping . name = profile . displayName ; newUser . ping . email = profile . email ; newUser . save ( function ( err ) { if ( err ) { throw err ; } return done ( null , newUser ); }); } }); })); }; Run Your Application! You should be able to run your application now whether it be locally or running on a server. If you’re running it locally, you’ll need to run npm install from your root folder and then run node index.js to launch the application. If you’re running it on a server, deploy the code and run your application from the server. Conclusion You’ve just built a simple application that handles Ping authentication for you and allows your users to seamlessly access your application without the need for them to sign up for another account. Awesome! You can build out your tokens to handle whatever profile data is available to Ping, and you can easily add this library to existing Node applications in order to abstract authentication work away from your application. Lastly Hopefully you enjoyed this basic tutorial, and if you have any enhancements for passport-ping-oauth2 that you would like to contribute, submit a pull request . posted on March 11, 2016 by Brenden McKamey ← Previous Post Next Post → - app ------ models ------------ user.js ------ routes.js\n- config ------ auth.js ------ database.js ------ passport.js\n- views ------ pages ------------ index.ejs ------------ profile.ejs ------ partials ------------ header.ejs\napp.json\nindex.js\npackage.json - app ------ models ------------ user.js ------ routes.js\n- config ------ auth.js ------ database.js ------ passport.js\n- views ------ pages ------------ index.ejs ------------ profile.ejs ------ partials ------------ header.ejs\napp.json\nindex.js\npackage.json { \"name\" : \"PingPassportTutorial\" , \"description\" : \"Easy Node Authentication With Ping.\" , \"version\" : \"0.1.5\" , \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"node index.js\" }, \"dependencies\" : { \"ejs\" : \"2.3.3\" , \"express\" : \"4.13.3\" , \"mongoose\" : \"*\" , \"passport\" : \"*\" , \"passport-ping-oauth2\" : \"*\" , \"connect-flash\" : \"*\" , \"bcrypt-nodejs\" : \"*\" , \"morgan\" : \"*\" , \"body-parser\" : \"*\" , \"cookie-parser\" : \"*\" , \"method-override\" : \"*\" , \"express-session\" : \"*\" }, \"engines\" : { \"node\" : \"0.12.7\" }, \"repository\" : { \"type\" : \"git\" , \"url\" : \"https://github.com/exampleuser/examplerepo\" }, \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"license\" : \"MIT\" } { \"name\" : \"PingPassportTutorial\" , \"description\" : \"Easy Node Authentication With Ping.\" , \"version\" : \"0.1.5\" , \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"node index.js\" }, \"dependencies\" : { \"ejs\" : \"2.3.3\" , \"express\" : \"4.13.3\" , \"mongoose\" : \"*\" , \"passport\" : \"*\" , \"passport-ping-oauth2\" : \"*\" , \"connect-flash\" : \"*\" , \"bcrypt-nodejs\" : \"*\" , \"morgan\" : \"*\" , \"body-parser\" : \"*\" , \"cookie-parser\" : \"*\" , \"method-override\" : \"*\" , \"express-session\" : \"*\" }, \"engines\" : { \"node\" : \"0.12.7\" }, \"repository\" : { \"type\" : \"git\" , \"url\" : \"https://github.com/exampleuser/examplerepo\" }, \"keywords\" : [ \"passport\" , \"auth\" , \"authn\" , \"authentication\" , \"authz\" , \"authorization\" , \"oauth\" , \"oauth2\" , \"ping\" ], \"license\" : \"MIT\" } var express = require ( ' express ' ); var app = express (); var mongoose = require ( ' mongoose ' ); var passport = require ( ' passport ' ); var flash = require ( ' connect-flash ' ); var morgan = require ( ' morgan ' ); var cookieParser = require ( ' cookie-parser ' ); var bodyParser = require ( ' body-parser ' ); var session = require ( ' express-session ' ); var configDB = require ( ' ./config/database.js ' ); mongoose . connect ( configDB . url ); require ( ' ./config/passport ' )( passport ); app . use ( morgan ( ' dev ' )); app . use ( cookieParser ()); app . use ( bodyParser ()); app . set ( ' port ' , ( process . env . PORT || 5000 )); app . use ( express . static ( __dirname + ' /public ' )); // views is directory for all template files app . set ( ' views ' , __dirname + ' /views ' ); app . set ( ' view engine ' , ' ejs ' ); app . use ( session ({ secret : ' tutorialsecret ' })); app . use ( passport . initialize ()); app . use ( passport . session ()); app . use ( flash ()); require ( ' ./app/routes.js ' )( app , passport ); app . listen ( app . get ( ' port ' ), function () { console . log ( ' Node app is running on port ' , app . get ( ' port ' )); }); var express = require ( ' express ' ); var app = express (); var mongoose = require ( ' mongoose ' ); var passport = require ( ' passport ' ); var flash = require ( ' connect-flash ' ); var morgan = require ( ' morgan ' ); var cookieParser = require ( ' cookie-parser ' ); var bodyParser = require ( ' body-parser ' ); var session = require ( ' express-session ' ); var configDB = require ( ' ./config/database.js ' ); mongoose . connect ( configDB . url ); require ( ' ./config/passport ' )( passport ); app . use ( morgan ( ' dev ' )); app . use ( cookieParser ()); app . use ( bodyParser ()); app . set ( ' port ' , ( process . env . PORT || 5000 )); app . use ( express . static ( __dirname + ' /public ' )); // views is directory for all template files app . set ( ' views ' , __dirname + ' /views ' ); app . set ( ' view engine ' , ' ejs ' ); app . use ( session ({ secret : ' tutorialsecret ' })); app . use ( passport . initialize ()); app . use ( passport . session ()); app . use ( flash ()); require ( ' ./app/routes.js ' )( app , passport ); app . listen ( app . get ( ' port ' ), function () { console . log ( ' Node app is running on port ' , app . get ( ' port ' )); }); module . exports = { ' url ' : ' your-settings-here ' // looks like mongodb://<user>:<pass>@mongo.onmodulus.net:27017/Mikha4ot }; module . exports = { ' url ' : ' your-settings-here ' // looks like mongodb://<user>:<pass>@mongo.onmodulus.net:27017/Mikha4ot }; module . exports = function ( app , passport ) { // route for home page app . get ( ' / ' , function ( req , res ) { res . render ( ' pages/index.ejs ' ); // load the index.ejs file }); // route for showing the profile page app . get ( ' /profile ' , isLoggedIn , function ( req , res ) { res . render ( ' pages/profile.ejs ' , { user : req . user , flash : req . session . flash }); }); // route for logging out app . get ( ' /logout ' , function ( req , res ) { req . logout (); res . redirect ( ' / ' ); }); // route for signing in app . get ( ' /auth/oauth2 ' , passport . authenticate ( ' oauth2 ' , { scope : [ ' openid ' , ' profile ' , ' email ' ] })); // route for call back app . get ( ' /auth/oauth2/callback ' , passport . authenticate ( ' oauth2 ' , { failureRedirect : ' /login ' }), function ( req , res ) { res . redirect ( ' /profile ' ); }); }; // route middleware to make sure a user is logged in function isLoggedIn ( req , res , next ) { // if user is authenticated in the session, carry on if ( req . isAuthenticated ()) return next (); // if they aren't redirect them to the home page res . redirect ( ' / ' ); } module . exports = function ( app , passport ) { // route for home page app . get ( ' / ' , function ( req , res ) { res . render ( ' pages/index.ejs ' ); // load the index.ejs file }); // route for showing the profile page app . get ( ' /profile ' , isLoggedIn , function ( req , res ) { res . render ( ' pages/profile.ejs ' , { user : req . user , flash : req . session . flash }); }); // route for logging out app . get ( ' /logout ' , function ( req , res ) { req . logout (); res . redirect ( ' / ' ); }); // route for signing in app . get ( ' /auth/oauth2 ' , passport . authenticate ( ' oauth2 ' , { scope : [ ' openid ' , ' profile ' , ' email ' ] })); // route for call back app . get ( ' /auth/oauth2/callback ' , passport . authenticate ( ' oauth2 ' , { failureRedirect : ' /login ' }), function ( req , res ) { res . redirect ( ' /profile ' ); }); }; // route middleware to make sure a user is logged in function isLoggedIn ( req , res , next ) { // if user is authenticated in the session, carry on if ( req . isAuthenticated ()) return next (); // if they aren't redirect them to the home page res . redirect ( ' / ' ); } <title> My Simple Passport Tutorial </title> <link rel= \"stylesheet\" type= \"text/css\" href= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css\" /> <script src= \"https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\" ></script> <script type= \"text/javascript\" src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js\" ></script> <link rel= \"stylesheet\" type= \"text/css\" href= \"/stylesheets/main.css\" /> <title> My Simple Passport Tutorial </title> <link rel= \"stylesheet\" type= \"text/css\" href= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css\" /> <script src= \"https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js\" ></script> <script type= \"text/javascript\" src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js\" ></script> <link rel= \"stylesheet\" type= \"text/css\" href= \"/stylesheets/main.css\" /> <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > <p> Login with: </p> <a href= \"/auth/oauth2\" class= \"btn btn-primary\" > Ping </a> </div> </body> </html> <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > <p> Login with: </p> <a href= \"/auth/oauth2\" class= \"btn btn-primary\" > Ping </a> </div> </body> </html> <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > < % if ( flash ) { % > <div class= \"row\" > <div class= \"<%= flash.type %>\" > <button type= \"button\" class= \"close\" data-dismiss= \"alert\" aria-hidden= \"true\" > &times; </button> <p><strong>< %= flash . title % ></strong></p> <p>< %= flash . messages % ></p> </div> </div> < % } % > <div class= \"page-header text-center\" > <h1><span class= \"fa fa-anchor\" ></span> Profile Page </h1> <a href= \"/logout\" class= \"btn btn-default btn-sm\" > Logout </a> </div> <div class= \"row\" > <div class= \"col-md-6 col-md-offset-3 well\" > <h3 class= \"text-primary\" > Ping </h3> <dl> <dt> ID: </dt> <dd>< %= user . ping . id % ></dd> <dt> Email: </dt> <dd>< %= user . ping . email % ></dd> <dt> Name: </dt> <dd>< %= user . ping . name % ></dd> < % if ( user . ping . entitlements !== null && user . ping . entitlements . length > 0) { %> <dt> Entitlements: </dt> <dd> <ul> < % for ( var i = 0; i < user . ping . entitlements . length ; ++ i ) { % > <li>< %= user . ping . entitlements [ i ] % ></li> < % } % > </ul> </dd> < % } % > </dl> </div> </div> </div> </body> </html> <!doctype html> <html> <head> < % include ../ partials / header . ejs % > </head> <body> <div class= \"container\" > < % if ( flash ) { % > <div class= \"row\" > <div class= \"<%= flash.type %>\" > <button type= \"button\" class= \"close\" data-dismiss= \"alert\" aria-hidden= \"true\" > &times; </button> <p><strong>< %= flash . title % ></strong></p> <p>< %= flash . messages % ></p> </div> </div> < % } % > <div class= \"page-header text-center\" > <h1><span class= \"fa fa-anchor\" ></span> Profile Page </h1> <a href= \"/logout\" class= \"btn btn-default btn-sm\" > Logout </a> </div> <div class= \"row\" > <div class= \"col-md-6 col-md-offset-3 well\" > <h3 class= \"text-primary\" > Ping </h3> <dl> <dt> ID: </dt> <dd>< %= user . ping . id % ></dd> <dt> Email: </dt> <dd>< %= user . ping . email % ></dd> <dt> Name: </dt> <dd>< %= user . ping . name % ></dd> < % if ( user . ping . entitlements !== null && user . ping . entitlements . length > 0) { %> <dt> Entitlements: </dt> <dd> <ul> < % for ( var i = 0; i < user . ping . entitlements . length ; ++ i ) { % > <li>< %= user . ping . entitlements [ i ] % ></li> < % } % > </ul> </dd> < % } % > </dl> </div> </div> </div> </body> </html> req . session . flash = { type : ' alert alert-dismissible alert-danger ' , title : ' Access Denied ' , messages : ' Your access was denied because you do not have an entitlement for this application; however, we ' ll still show you your profile data . ' };\nres.redirect(303, ' / profile ' ); req . session . flash = { type : ' alert alert-dismissible alert-danger ' , title : ' Access Denied ' , messages : ' Your access was denied because you do not have an entitlement for this application; however, we ' ll still show you your profile data . ' };\nres.redirect(303, ' / profile ' ); // app/models/user.js // load the things we need var mongoose = require ( ' mongoose ' ); // define the schema for our user model var userSchema = mongoose . Schema ({ ping : { id : String , token : String , email : String , name : String , entitlements : [ String ] } }); // create the model for users and expose it to our app module . exports = mongoose . model ( ' User ' , userSchema ); // app/models/user.js // load the things we need var mongoose = require ( ' mongoose ' ); // define the schema for our user model var userSchema = mongoose . Schema ({ ping : { id : String , token : String , email : String , name : String , entitlements : [ String ] } }); // create the model for users and expose it to our app module . exports = mongoose . model ( ' User ' , userSchema ); module . exports = { ' pingAuth ' : { ' authorizationURL ' : ' https://www.example.com/as/authorization.oauth2 ' , ' tokenURL ' : ' https://www.example.com/as/token.oauth2 ' , ' clientID ' : ' EXAMPLE_CLIENT_ID ' , ' clientSecret ' : ' EXAMPLE_CLIENT_SECRET ' , ' callbackURL ' : ' http://localhost:5000/auth/oauth2/callback ' , } }; module . exports = { ' pingAuth ' : { ' authorizationURL ' : ' https://www.example.com/as/authorization.oauth2 ' , ' tokenURL ' : ' https://www.example.com/as/token.oauth2 ' , ' clientID ' : ' EXAMPLE_CLIENT_ID ' , ' clientSecret ' : ' EXAMPLE_CLIENT_SECRET ' , ' callbackURL ' : ' http://localhost:5000/auth/oauth2/callback ' , } }; var passport = require ( ' passport ' ); var OAuth2Strategy = require ( ' passport-ping-oauth2 ' ). Strategy ; var User = require ( ' ../app/models/user ' ); var configAuth = require ( ' ./auth ' ); module . exports = function ( passport ) { // used to serialize the user for the session passport . serializeUser ( function ( user , done ) { done ( null , user . id ); }); // used to deserialize the user passport . deserializeUser ( function ( id , done ) { User . findById ( id , function ( err , user ) { done ( err , user ); }); }); passport . use ( new OAuth2Strategy ({ authorizationURL : configAuth . pingAuth . authorizationURL , tokenURL : configAuth . pingAuth . tokenURL , clientID : configAuth . pingAuth . clientID , clientSecret : configAuth . pingAuth . clientSecret , callbackURL : configAuth . pingAuth . callbackURL }, function ( accessToken , refreshToken , params , profile , done ) { User . findOne ({ ' ping.id ' : profile . cn }, function ( err , user ) { if ( err ) { return done ( err ); } if ( user ) { return done ( null , user ); } else { var newUser = new User (); newUser . ping . id = profile . id ; newUser . ping . token = accessToken ; newUser . ping . name = profile . displayName ; newUser . ping . email = profile . email ; newUser . save ( function ( err ) { if ( err ) { throw err ; } return done ( null , newUser ); }); } }); })); }; var passport = require ( ' passport ' ); var OAuth2Strategy = require ( ' passport-ping-oauth2 ' ). Strategy ; var User = require ( ' ../app/models/user ' ); var configAuth = require ( ' ./auth ' ); module . exports = function ( passport ) { // used to serialize the user for the session passport . serializeUser ( function ( user , done ) { done ( null , user . id ); }); // used to deserialize the user passport . deserializeUser ( function ( id , done ) { User . findById ( id , function ( err , user ) { done ( err , user ); }); }); passport . use ( new OAuth2Strategy ({ authorizationURL : configAuth . pingAuth . authorizationURL , tokenURL : configAuth . pingAuth . tokenURL , clientID : configAuth . pingAuth . clientID , clientSecret : configAuth . pingAuth . clientSecret , callbackURL : configAuth . pingAuth . callbackURL }, function ( accessToken , refreshToken , params , profile , done ) { User . findOne ({ ' ping.id ' : profile . cn }, function ( err , user ) { if ( err ) { return done ( err ); } if ( user ) { return done ( null , user ); } else { var newUser = new User (); newUser . ping . id = profile . id ; newUser . ping . token = accessToken ; newUser . ping . name = profile . displayName ; newUser . ping . email = profile . email ; newUser . save ( function ( err ) { if ( err ) { throw err ; } return done ( null , newUser ); }); } }); })); }; posted on March 11, 2016 by Brenden McKamey Brenden McKamey", "date": "2016-03-11"},
{"website": "Monsanto", "title": "Cloud Foundry User Administration", "author": ["\n                                        Russell Caswell\n                                    "], "link": "http://engineering.monsanto.com/2016/01/29/cf-users/", "abstract": "#Cloud Foundry User Administration In the third installation of our open source [Cloud Foundry (CF) toolbox series](/2015/07/22/building-an-open-source-cloud-foundry-toolbox/ we unveil CF-Users , a web based solution for managing Cloud Foundry user credentials and authorization.\nEnabling team collaboration and self-sufficiency is a core principal of Cloud Foundry, one which heavily relies on the concepts of Organizations, Spaces, and Roles .  These built in mechanisms allow teams full access to manage roles and permissions without the chance of impacting other teams using the shared platform.  This seemingly simple ability is increasingly vital as your organization grows, allowing more responsibility to be pushed to the teams themselves and reducing reliance on a centralized ops team. The Catch So what’s the catch?  Out of the box the only tool Cloud Foundry provides for managing user and permissions is the CF CLI .  Although incredibly useful, the cli restricts access to user management commands to only admin level users. This essentially ignores the Org and Space level manager roles which a user may have.  In addition, user management through the cli is a multi-step process requiring multiple commands to accomplish most tasks.  As our utilization of Cloud Foundry grew, we quickly found that relying on the command line interface to add users and configure multiple roles to individuals was a cumbersome task, which was taking increasingly more time. More importantly it was actually an anti-pattern to the DevOps goals we were trying to achieve. Enter CF-Users Although the cli doesn’t allow users to take full advantage of CF’s role functionality, all the necessary mechanisms to do so are still available under the hood.  With CF-Users we created a web-ui which ties into these mechanisms and exposes them in a secure way to the development teams.  In addition to moving user management from an ops role back to the teams, the number of steps required to manage users is also greatly reduced by offering the ability to simultaneously add a user and configure permissions.  This takes what is a multistep process at the command line and reduces it to one form of entry and submittal, which greatly reduces the steps needed to add a user. CF-Users also offers two different workflows for modifying a user’s roles within Cloud Foundry, allowing assignment of roles from the most convenient perspective needed.  For instance, if a new organization or space has been added, the User Role Admin page may be used to assign roles to multiple users at that level without the need to navigate several web pages.   Conversely, if only one user needs permissions added, the Edit User screen may be used to assign multiple roles for that user.  Efficiency is greatly improved, simplifying the process to a few mouse clicks. Using the application Add User Organization managers and space managers both may add users with CF-Users.  However, organization managers may only assign roles within organizations that that they manage.  Likewise, space managers may only assign roles within spaces that they manage.  Adding a user is as easy as entering the username and optionally password (depending on the type of user account created), select roles for the user, and click the Create User button. Edit User This screen allows organization managers and space managers to modify the privileges assigned to users.  The user roles may be selected or deselected by clicking the appropriate checkboxes.   The privileges are updated as these checkboxes are clicked and feedback is given with a Growl message when the action is completed. User Role Admin This screen allows privilege modification from the perspective of the organization and the spaces rather than from the user perspective.  This is accomplished by allowing the selection of the organization and/or space and displaying a list of users who can be assigned roles at the chosen level.  As with edit user, the privileges are updated as the checkboxes are clicked and feedback is given with a Growl message when the action is completed. In conclusion If you would like to deploy CF-Users yourself, go to the github page to download and deploy it.  If you have any enhancements you would like to contribute, submit a pull request .  We welcome enhancements to this project to make it more effective and improve efficiency of user administration. posted on January 29, 2016 by Russell Caswell ← Previous Post Next Post → posted on January 29, 2016 by Russell Caswell Russell Caswell", "date": "2016-01-29"},
{"website": "Monsanto", "title": "Algebraic Data Types", "author": ["\n                                        John Klingler\n                                    "], "link": "http://engineering.monsanto.com/2016/01/11/algebraic-data-types/", "abstract": "Algebraic data types , usually abbreviated ADTs, are a useful construct in programming to increase code clarity and reduce the likelihood of errors. Generally speaking, they’re somewhat similar to enums in Java, but Scala lends much more power to the construct. ADTs define a fixed set of all possible values of a given type. The classic example of an ADT in Scala is the Option type. An Option represents something that may or may not have a value. No value is None and some value is Some() . This is the functional version of returning an object or null from a method in Java, and is very useful for Java interoperability to eliminate NullPointerException ’s. To see how ADTs may be implemented in our code, we will begin with the example of the classic counting game FizzBuzz . The game begins with participants taking turns counting up from 1, but saying fizz instead of numbers which are multiples of 3, buzz instead of numbers which are multiples of 5, and fizzbuzz for numbers which are both. Now let’s look at how FizzBuzz could be written in Java: public static void printIt ( Integer i ) { if ( i % 3 == 0 && i % 5 == 0 ) { System . out . println ( \"FizzBuzz\" ); } else if ( i % 3 == 0 ) { System . out . println ( \"Fizz\" ); } else if ( i % 5 == 0 ) { System . out . println ( \"Buzz\" ); } else { System . out . println ( String . valueOf ( i )); } } public static void main ( String [] args ) { for ( int i = 1 ; i <= 100 ; i ++) { printIt ( i ); } } Nothing out of the ordinary here. If we just translated this into Scala, we’d have something similar to: def printIt ( i : Int ) : Unit = if ( i % 3 == 0 && i % 5 == 0 ) println ( \"FizzBuzz\" ) else if ( i % 3 == 0 ) println ( \"Fizz\" ) else if ( i % 5 == 0 ) println ( \"Buzz\" ) else println ( String . valueOf ( i )) for ( i <- 1 to 100 ) { printIt ( i ) } As experienced Scala developers, we would recognize there are more functional ways to accomplish this task. One way is: val isMod : ( Int , Int ) => Boolean = ( m , i ) => i % m == 0 val mod3 = isMod ( 3 , _ ) val mod5 = isMod ( 5 , _ ) val fizzBuzzIt : Int => String = { case i if mod3 ( i ) && mod5 ( i ) => \"FizzBuzz\" case i if mod3 ( i ) => \"Fizz\" case i if mod5 ( i ) => \"Buzz\" case i => i . toString } Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). foreach ( println ) So that works just fine and does exactly what we needed. But what if we want to share the FizzBuzzification functionality as a library? We could just publish our fizzBuzzIt function. However, if users wanted to do something different depending on whether it’s a Fizz or a Buzz, they’d have to resort to string matching which is brittle and definitely not type safe. One typo would result in incorrect behavior or, even worse, failures at runtime. While proper unit tests may catch these sorts of errors, it is just as likely they copied over the error or defined a constant with the typo. The tests would never catch it. Here is where we can leverage ADTs to get the job done. First off, we need to define the type of our collection of values: sealed abstract class FizzBuzzADT ( i : Int ) { override def toString : String = i . toString } There are two things to note here. First, we’ve defined a sealed class which tells the compiler that only classes within the same file are allowed to inherit from this class. This effectively prevents anyone from adding additional values to our type. This is important because adding more values somewhere might lead to unanticipated problems like a MatchError being thrown in a pattern match that doesn’t handle this new value. Secondly, we’ve declared shared functionality for all values with the toString method, standard OO stuff. Now we need to define all of the possible values of our ADT. Remember, these all have to be in the same file as the trait: case class Fizz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Fizz\" } case class Buzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Buzz\" } case class FizzBuzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"FizzBuzz\" } case class JustInt ( i : Int ) extends FizzBuzzADT ( i ) We’ve defined Fizz , Buzz , FizzBuzz , and JustInt . It might not yet be clear why we’ve defined constructor arguments for Fizz , Buzz , and FizzBuzz , but it gives us some opportunities that we’ll discuss in a minute. Now all we need is a way to create our values given any integer. The FizzBuzzADT companion object is the perfect place for this: object FizzBuzzADT { def apply ( i : Int ) : FizzBuzzADT = i match { case _ if i % 3 == 0 && i % 5 == 0 => FizzBuzz ( i ) case _ if i % 3 == 0 => Fizz ( i ) case _ if i % 5 == 0 => Buzz ( i ) case _ => JustInt ( i ) } } Here you see that the logic for determining whether the integer is a “Fizz”, “Buzz”, “FizzBuzz”, or just an integer hasn’t changed or gone away; instead of just printing the value, we’re mapping the integers to one of the values of our ADT. So to run our new FizzBuzz algorithm and print the values, we can use: Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach ( println ) So you’re probably thinking that this doesn’t look all that different from our “idiomatic Scala” implementation from above. Scala is supposed to be more concise. What did all that extra code buy us? Well, remember the constructor arguments to the case classes? Let’s say, for example, that we have a collection of FizzBuzzADT ’s (but no access to the collection of integers from which it was created) and we only want to print something out if the original integer was even. We can do this using pattern matching: val even : Int => Boolean = i => i % 2 == 0 // Pretend the Stream[Int] came from somewhere else Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case a @Fizz ( i ) if even ( i ) => println ( a ) case a @Buzz ( i ) if even ( i ) => println ( a ) case a @FizzBuzz ( i ) if even ( i ) => println ( a ) case a @JustInt ( i ) if even ( i ) => println ( a ) case _ => // Be quiet } Certainly, we could have accomplished this example more effectively, by exposing the original integer as a value in the abstract class, but the purpose here was to demonstrate the power and flexibility of pattern matching with ADTs. For another example, let’s suppose for a second that I don’t like Fizz. In fact, I hate it; I never want to see it. Let’s get rid of the Fizz: Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case Fizz ( _ ) => // Down with the bloody bad Fizz case x => println ( x ) } In this case, we don’t really need pattern matching and we can accomplish the same thing more concisely: Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). filter (! _ . isInstanceOf [ Fizz ]). foreach ( println ) Sure, you could just as easily have ridded the world of Fizz with the implementation above: Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). filter ( _ != \"Fizz\" ). foreach ( println ) But are you sure there’s not a typo? What if it’s decided that Fizz should be spelled Fisz? These all end up being runtime errors and who likes those? We have to write tests for these scenarios and even those tests are prone to the same problems. The ADT turns these runtime errors into compile time errors. Because we can’t even run tests if the code doesn’t compile, we’ve eliminated a whole class of tests. These tests weren’t truly valuable anyway as they validated library integration, not business functionality. So, in that sense, ADT’s also let us spend more brain cycles on business functionality. Our simple FizzBuzz example demonstrates how ADTs can replace runtime with compile-time errors and transform a brittle algorithm to be type-safe. Hopefully I’ve demystified algebraic data types and shown how they’re valuable constructs you can employ in your code to build more robust applications. All code shown can be found here or just git clone git@github.com:MonsantoCo/algebraic-datatypes.git . posted on January 11, 2016 by John Klingler ← Previous Post Next Post → public static void printIt ( Integer i ) { if ( i % 3 == 0 && i % 5 == 0 ) { System . out . println ( \"FizzBuzz\" ); } else if ( i % 3 == 0 ) { System . out . println ( \"Fizz\" ); } else if ( i % 5 == 0 ) { System . out . println ( \"Buzz\" ); } else { System . out . println ( String . valueOf ( i )); } } public static void main ( String [] args ) { for ( int i = 1 ; i <= 100 ; i ++) { printIt ( i ); } } public static void printIt ( Integer i ) { if ( i % 3 == 0 && i % 5 == 0 ) { System . out . println ( \"FizzBuzz\" ); } else if ( i % 3 == 0 ) { System . out . println ( \"Fizz\" ); } else if ( i % 5 == 0 ) { System . out . println ( \"Buzz\" ); } else { System . out . println ( String . valueOf ( i )); } } public static void main ( String [] args ) { for ( int i = 1 ; i <= 100 ; i ++) { printIt ( i ); } } def printIt ( i : Int ) : Unit = if ( i % 3 == 0 && i % 5 == 0 ) println ( \"FizzBuzz\" ) else if ( i % 3 == 0 ) println ( \"Fizz\" ) else if ( i % 5 == 0 ) println ( \"Buzz\" ) else println ( String . valueOf ( i )) for ( i <- 1 to 100 ) { printIt ( i ) } def printIt ( i : Int ) : Unit = if ( i % 3 == 0 && i % 5 == 0 ) println ( \"FizzBuzz\" ) else if ( i % 3 == 0 ) println ( \"Fizz\" ) else if ( i % 5 == 0 ) println ( \"Buzz\" ) else println ( String . valueOf ( i )) for ( i <- 1 to 100 ) { printIt ( i ) } val isMod : ( Int , Int ) => Boolean = ( m , i ) => i % m == 0 val mod3 = isMod ( 3 , _ ) val mod5 = isMod ( 5 , _ ) val fizzBuzzIt : Int => String = { case i if mod3 ( i ) && mod5 ( i ) => \"FizzBuzz\" case i if mod3 ( i ) => \"Fizz\" case i if mod5 ( i ) => \"Buzz\" case i => i . toString } Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). foreach ( println ) val isMod : ( Int , Int ) => Boolean = ( m , i ) => i % m == 0 val mod3 = isMod ( 3 , _ ) val mod5 = isMod ( 5 , _ ) val fizzBuzzIt : Int => String = { case i if mod3 ( i ) && mod5 ( i ) => \"FizzBuzz\" case i if mod3 ( i ) => \"Fizz\" case i if mod5 ( i ) => \"Buzz\" case i => i . toString } Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). foreach ( println ) sealed abstract class FizzBuzzADT ( i : Int ) { override def toString : String = i . toString } sealed abstract class FizzBuzzADT ( i : Int ) { override def toString : String = i . toString } case class Fizz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Fizz\" } case class Buzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Buzz\" } case class FizzBuzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"FizzBuzz\" } case class JustInt ( i : Int ) extends FizzBuzzADT ( i ) case class Fizz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Fizz\" } case class Buzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"Buzz\" } case class FizzBuzz ( i : Int ) extends FizzBuzzADT ( i ) { override val toString = \"FizzBuzz\" } case class JustInt ( i : Int ) extends FizzBuzzADT ( i ) object FizzBuzzADT { def apply ( i : Int ) : FizzBuzzADT = i match { case _ if i % 3 == 0 && i % 5 == 0 => FizzBuzz ( i ) case _ if i % 3 == 0 => Fizz ( i ) case _ if i % 5 == 0 => Buzz ( i ) case _ => JustInt ( i ) } } object FizzBuzzADT { def apply ( i : Int ) : FizzBuzzADT = i match { case _ if i % 3 == 0 && i % 5 == 0 => FizzBuzz ( i ) case _ if i % 3 == 0 => Fizz ( i ) case _ if i % 5 == 0 => Buzz ( i ) case _ => JustInt ( i ) } } Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach ( println ) Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach ( println ) val even : Int => Boolean = i => i % 2 == 0 // Pretend the Stream[Int] came from somewhere else Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case a @Fizz ( i ) if even ( i ) => println ( a ) case a @Buzz ( i ) if even ( i ) => println ( a ) case a @FizzBuzz ( i ) if even ( i ) => println ( a ) case a @JustInt ( i ) if even ( i ) => println ( a ) case _ => // Be quiet } val even : Int => Boolean = i => i % 2 == 0 // Pretend the Stream[Int] came from somewhere else Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case a @Fizz ( i ) if even ( i ) => println ( a ) case a @Buzz ( i ) if even ( i ) => println ( a ) case a @FizzBuzz ( i ) if even ( i ) => println ( a ) case a @JustInt ( i ) if even ( i ) => println ( a ) case _ => // Be quiet } Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case Fizz ( _ ) => // Down with the bloody bad Fizz case x => println ( x ) } Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). foreach { case Fizz ( _ ) => // Down with the bloody bad Fizz case x => println ( x ) } Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). filter (! _ . isInstanceOf [ Fizz ]). foreach ( println ) Stream ( 1 to 100 : _ * ). map ( FizzBuzzADT ( _ )). filter (! _ . isInstanceOf [ Fizz ]). foreach ( println ) Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). filter ( _ != \"Fizz\" ). foreach ( println ) Stream ( 1 to 100 : _ * ). map ( fizzBuzzIt ). filter ( _ != \"Fizz\" ). foreach ( println ) posted on January 11, 2016 by John Klingler John Klingler", "date": "2016-01-11"},
{"website": "Monsanto", "title": "Optimized Chinese Restaurant Process", "author": ["\n                                        Ryan Richt\n                                    "], "link": "http://engineering.monsanto.com/2015/11/23/chinese-restaurant-process/", "abstract": "TL;DR Bayesian methods provide a theoretically well principled way to accomplish data science tasks, even basic tasks like clustering. Using a variety of performance optimizations we were able to sufficiently reduce the IO, memory and CPU (300,000×!) required to run large scale clustering based on the Chinese Restaurant Process (CRP). CRP is a non-parametric generative Bayesian model of a “mixture” that simultaneously learns the number of clusters, the model of each cluster, and entity assignments into clusters. We have open sourced this project in Scala for use on “count” data. And you can run this sucker with: import com.monsanto.stats.tables._ import com.monsanto.stats.tables.clustering._ val cannedAllTopicVectorResults : Vector [ TopicVectorInput ] = MnMGen . cannedData val cannedCrp = new CRP ( ModelParams ( 5 , 2 , 2 ), cannedAllTopicVectorResults ) val crpResult = cannedCrp . findClusters ( 200 , RealRandomNumGen , cannedCrp . selectCluster ) Iteration 1: cluster count was 365, reseat: 35, score: -29578.83920* Iteration 2: cluster count was 118, reseat: 15, score: -29111.34349* Iteration 3: cluster count was 61, reseat: 7, score: -28919.62995* Iteration 4: cluster count was 40, reseat: 6, score: -28852.91482* Iteration 5: cluster count was 29, reseat: 6, score: -28804.38123* Iteration 6: cluster count was 24, reseat: 5, score: -28741.68993* Iteration 7: cluster count was 16, reseat: 5, score: -28734.04974* Iteration 8: cluster count was 14, reseat: 6, score: -28742.16624 Iteration 9: cluster count was 12, reseat: 5, score: -28739.19560 Iteration 10: cluster count was 10, reseat: 5, score: -28738.64498 … Iteration 190: cluster count was 4, reseat: 10, score: -28724.77273 Iteration 191: cluster count was 3, reseat: 11, score: -28724.77273 Iteration 192: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 193: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 194: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 195: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 196: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 197: cluster count was 3, reseat: 11, score: -28724.77273 Iteration 198: cluster count was 3, reseat: 10, score: -28724.77273 Iteration 199: cluster count was 3, reseat: 13, score: -28724.77273 Iteration 200: cluster count was 3, reseat: 12, score: -28724.77273 Because who really knows what “K” should be anyway? At Monsanto we have a variety of analytics and data science groups working on everything from sales transactions to aerial and satellite imaging to genome (DNA) sequencing. One of the oldest and most common data science problems is clustering: given a set of objects with possibly many properties, what is an appropriate partition of those entities into groups? Below we’ll first describe the statistical method we used to perform clustering and then the software optimizations we implemented to make this scale. Generative Bayesian Models At Monsanto, we are Bayesians, and as the late E.T. Jaynes espoused, we don’t believe in “ad hockeries” like K-means (a numerical method) or ad hoc “machine learning” techniques such as random forests. Instead we have a better way: using only the laws of probability theory. Clustering is actually a difficult problem to cast in the Bayesian paradigm, but new theoretical results and the rise of computing power over the past few decades have made this problem tractable. Proper Bayesian models are “generative,” meaning that they posit an underlying (or latent) “generating” process that creates the data we see. It is precisely writing a computer program to recreate the observed data, perhaps with some input variables missing that we want to recover. Markov-chain Monte Carlo (MCMC) then provides a universal mechanism to “invert” or solve for those input variables for such programs given some data. In the simplest case, say we observe the heights of a room of N people. The generating function could be, a normal distribution with some mean and variance that we draw N samples from. MCMC could then be run on those samples to try to recover the most likely parameters of that normal distribution. We can also construct more complex generating functions (and still solve them with MCMC). Perhaps a better generating function would be to draw Male vs Female from a binomial distribution (probability of being female in the population, like a weighted coin toss), and conditional on the result, draw a height from either the male-specific or female-specific height distribution. Bayesian Clustering Mixture models are a Bayesian way of clustering: your generating function produces a mixed population of entities from an underlying discrete set of components. For instance, imagine I give you a stream of unlabeled bags of M&M’s™ † candies. All you get to observe is a few colored M&Ms™ of each bag. This is multinomial count data: we have a finite discrete “vocabulary” of colors and we will observe some number of counts of each color. A multinomial distribution is just like a weighted (unfair) many-sided die, with one side for each outcome. For Christmas M&M’s™ say we have a 3-sided die with faces indicating {Red, Green White} which lands on Red 40% of the time, Green 40% of the time and White 20% of the time. To generate a draw of size M (say, 35 candies) from this multinomial you just roll this die M times and count up a vector of each possible color. Imagine that the generating function is first buying a bag on a random day. Most of the year you can get bags with classic colors, for 2 weeks you can get Christmas colors, and for 3 days you can get 4th of July colors. So from this distribution of K types of bags say we draw N bags. Then we erase the packaging label of each, and I give you a handful of candies from each bag. Note that there is significant overlap in the colors from each kind of bag. From a small handful of say, Christmas M&M’s™ where you didn’t chance to draw and white colored candies, its hard to say if you they are Christmas or plain! From only these handfuls, your clustering job is to tell me: How many kinds of bags there are A model of the data produced from each kind of bag Which handfuls came from which kinds of bags So this is a clustering process. It is mixture of types of bags, and since we don’t get the see the labels of the bags we have a mixture model. The unknown kinds of bags are the clusters, and the handfuls are our data. The multinomial counts of colors from the handfuls are our features extracted from the data, and we describe each type of bag by an explicit probability model which is a multinomial distribution. Contrast this to say, K-means clustering: Attribute K-means Bayesian Mixture Model Count of clusters Ad hoc - user specified “K” Probabilistic model Membership Measure Ad hoc - euclidean “distance” Probabilistic model Solution Method Ad hoc - EM*-style iterations Markov-chain Monte Carlo Confidence Measure None Probabilities for all aspects (*EM or Expectation Maximization is only guaranteed to converge to local optima) While K-means may work OK in some cases, it leaves much to be desired. The Chinese Restaurant Process OK so we can describe a type of bag by a multinomial distribution. From several multinomials we can use Bayes’ Rule to compute the relative probability that any given handful of candies belongs to each of the possible kinds of bags. But how do we posit kinds of bags in the first place? And how many might there be? The answer, and the probabilistic model for “choosing K” is the “Chinese Restaurant Process.” The Chinese Restaurant Process is a generating function for a mixture model, and the story goes like this: There is a large family-style Chinese restaurant with a seemingly infinite number of infinitely large tables. A line of customers come in, and they join an existing table with probability proportional to how many others are already seated there (so popular tables get more popular), and with some probability they nucleate their own new table. Every diner at the same table eats from the same dish, which is a common probability distribution. Their “bites” of the dish are our observed data points. So this is a generating function for a (clustering) mixture model, where we don’t have to know K in advance and K can be unbounded! The real beauty is that CRP properly probabilistically trades off between more “tighter” clusters and fewer more heterogenous clusters. Setting the “alpha” parameter determines the exchange rate of this trade-off, it doesn’t specify K. You can think of the MCMC solver as running this generating function many times and looking for the highest probability assignments - where diners with similar “bites” are indeed assigned to the same table with the same dish, and we properly trade off the number of tables/dishes with how will the table-mates fit each dish. But instead we use a more efficient sort-of stochastic search that spends more time poking around high-probability regions but can still escape local maxima. The precise low-down on the collapsed Gibbs sampler can best found here . The Optimization We didn’t set out to build our own implementation. Actually there is a great series of DPMM/CRP/Clustering blog posts from the guys over at DatumBox, and that’s where we stared. Open Source FTW! Unfortunately we generated a large test data set with 100,000 “bags” each with Normal(400,100) “candies” sampled from 10,000 “colors” across 10 types of bags (clusters) with exponentially distributed membership, an Exponential(1/10) number of colors per type, and Exponential(1/100) weights of each color. Unfortunately, EXPLOSION!! And this explosion was reproducible on AWS on a behemoth memory optimized r3.8xlarge instance with a java heap size of 150GB! Then we set out on what is a pretty archetypal optimization journey, but if you haven’t done a lot of optimization, it may be of interest. Solve a Different Problem It should also be noted that, we could have subsampled our data, implemented an approximation algorithm, or as we did do, solve another problem completely. MCMC is great for samples but according to Daume 2007, Fast search for Dirichlet process mixture models , it doesn’t seem to be the most efficient search strategy if you only want the single most likely clustering. So (after we were unable to make the original Matlab work) we also reimplemented Daume 2007, which is a variant of A* search with some heuristics for this problem. Turns out even with substantial optimization, and a large beam (look back) size, we always got slower, worse clusterings than with our optimized Gibbs sampler. So it seemed the original problem was indeed the one worth solving. Memory and IO The first thing we noticed was that the in-memory size of the data set was unnecessarily large. To keep Arrays of counts (aka, dense vectors) across 10,000 colors with Exponential(1/10) active colors meant that almost all of the data was zeros. While we love and make heavy use of Breeze we started out with the simplest thing that could possible work for a “sparse vector”: a Map[Int, Int] from color index to count, filled in only for non-zero counts. This would require significant changes to the DatumBox code so we started over in Scala and implemented the collapsed Gibbs sampler for CRP with Dirichlet-Multinomial data in the standard manner . This reduced our memory requirements from at least 150GB down to 2.2GB (68× RAM reduction), and improved startup time since we now needed only to parse in 6MB of data instead of 2000MB (~333× IO reduction). The CPU Saga We were then very exciting to be able to run the Gibbs sampler in reasonable memory. Unfortunately we immediately hit the next problem: a single “reseating” of customers in the restaurant took 32.5 seconds. 32.5 seconds × 100,000 customers × 10,000 iterations = 1000 CPU years . Ouch. Using a combination of the sampler and profiler in VisualVM , manual timings, and micro-benchmarks we crafted a series of 7 versions that drove the reseating time down to 0.0001 seconds. Here are some of the highlights: Initialization If you read papers on CRP, you can see that there are numerous initialization strategies: 1 object per table, log N tables with random assignments, using Daume 2007 output as initialization, etc. While 1 object per table seems like the least likely to be biased based on random early decisions, it is also the slowest. We settled on random tables of 100 entities which gave us another 5× speed up or so, without detectable bias for our data. Mutability Of course we implemented the first version in the idiomatic, purely immutable Scala style, which involves a good deal of data structure copying. We first went whole hog changing everything to mutation and mutable data structures and indeed saw a ~10× speedup. Interestingly, it later turned out that we really only need to make a few data structures mutable (like this one ) and that they could be private to their respective methods or classes. As an example we used to have clusters/tables have a sequence of all of their members, and upon reseating a person we’d need to make a new cluster that was a copy of the old one with the new person. Instead it turned out to be much faster to both make that mutable and invert the order: members now have a mutable Option[Cluster] to which they currently belong and the cluster statistics are mutated upon add/remove. That’s about a 5× speed up. So the overall structure is something we call “immutable on the outside, mutable on the inside.” Eventually we rewrote the codebase back to an immutable Scala style with only these few private mutable arrays for performance. This is a great example of a classic lesson: there is usually only 1 hot path through the code. 90% of your code can remain pretty, idiomatic and immutable; only a small section needs to be uglified with optimization. Caching The collapsed Gibbs sampler for CRP has a central, tight, numerics heavy loop: /*\n * C is just the result of this integral. C tells you the probability\n * that someone is going to sit somewhere and the probability of your\n * uncertainty about what the parameters of that table truly are. If you\n * toss 10 coins and get 6 heads 4 tails, you'd guess it is 60/40, but\n * you wouldn't be very certain. If you had 1000 samples you'd be more\n * certain, and likely be closer to 50/50. C it is accounting for that\n * uncertainty.\n */ def estimateCSmoothingFirst : Double = { // Compute partSumAi and partSumLogGammaAi by iterating through all // values in the WeightedVector's vecMap and computing the sum of the // values and their logGammas. // icky vars for performance in this critical path var partSumAi : Double = 0.0 var partSumLogGammaAi : Double = 0.0 var idx = 1 val len = size * 2 while ( idx < len ) { val v = pairs ( idx ) partSumAi += v + params . beta // add beta to this and the next value to smooth the curve val logGammaSmoothingFirst = if ( v < allTopicVectorResults . length ) cache ( v ) else logGamma ( v + params . beta ) partSumLogGammaAi += logGammaSmoothingFirst idx += 2 } And that logGamma special function, even given a numerical approximation expansion is pretty slow given all its instructions: // Gamma is the continuous version of factorial, but off by 1, and its\n// more accurate to compute it and its log in one step def logGamma ( x : Double ) : Double = { val tmp : Double = ( x - 0.5 ) * Math . log ( x + 4.5 ) - ( x + 4.5 ) val ser : Double = 1.0 + 76.18009173 / ( x + 0 ) - 86.50532033 / ( x + 1 ) + 24.01409822 / ( x + 2 ) - 1.231739516 / ( x + 3 ) + 0.00120858003 / ( x + 4 ) - 0.00000536382 / ( x + 5 ) tmp + Math . log ( ser * Math . sqrt ( 2 * Math . PI )) } Turns out for our data set, this special function would literally be called 1 trillion times! In the naive implementation, we actually call logGamma on a Double . But that Double is really the value of the sum of some counts (an Int ) and a prior probability term that certainly needs to be a Double because we often want values <1. So we pulled a couple of tricks: The value of that prior Double is constant for the whole run, it’s not unknown, so what if we just add it inside a caching function? Now it’s a function of an Int . This is a function of positive Int ’s over count data. So if we can assume some bounded size, there are a very small number of possible output values. Even if we allow the range 0-1,000,000 of input values, that’s a tiny amount of memory and computes the function 1,000,000 times fewer! In fact we can just call the slow version if we’re outside that range with a low-overhead if-check. We can even do better than using a Map[Int, Double] , since this is a function of Int (plus that double we add inside the function) we can just do direct lookups in an Array indexed by the argument. Turns out it’s a lot of conditional logic and possible cache-blowing to check and fill in the map dynamically, we can just pre-fill the whole thing very fast on program startup. There’s another 10×. Boxing The code has a number of Map ’s and Seq ’s of Int ’s and Double ’s which while normally innocuous once you get down to extreme optimization really start to add up with occasional un/boxing overhead. We fell in love with the open source library Debox by Scala math genius Erik Asheim @d6 and recommend it highly. Subbing in these specialized data structures gave us another 5× speedup while keeping our code clean. Micro-benchmarks: Fastest Map Combiner? At some point the slowest part of the code was then computing the updated statistics for each table. These stats operate over the a (sparse) vector of counts summed across all diners sitting at this table. We have our now Debox.Map based sparse vectors, so what is the fastest way to sum a collection over them? Hint: its not “just the Monoid over addition!” First we made a series of alternatives and timed this. Which do you think would be faster, or why are they not all the same? val first = Range ( 1 , 30 ). map ( i => ( i -> 17 )). toMap val second = Range ( 15 , 44 ). map ( i => ( i -> 64.0 )). toMap def v1 () = { // Add tv's vecMap to smoothedCounts vecMap. // MAKE THIS SIMPLER val temp : Vector [( Int , Double )] = first . toVector . map { case ( i , j ) => ( i , j . toDouble ) } ++ second . toVector val temp2 : Map [ Int , Vector [( Int , Double )]] = temp . groupBy ( _ . _1 ) val temp3 : Map [ Int , Vector [ Double ]] = temp2 . mapValues ( xs => xs . map ( _ . _2 )) val topicCountsSums : Map [ Int , Double ] = temp3 . mapValues ( _ . sum ) topicCountsSums . head } def v2 () = { val allDenseKeys = first . keySet ++ second . keySet val diffs = allDenseKeys . map { index => index -> ( first . getOrElse ( index , 0 ) + second . getOrElse ( index , 0.0 )) }. toMap diffs . head } def v3 () = { val sums = mutable . Map . empty [ Int , Double ] first . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + first ( k ) ) second . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + second ( k ) ) sums . head } def v4 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } second . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } sums . head } def v5 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } second . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } sums . head } It turns out that after warmup, v1 is 2× as fast as v2, v3 is 2× as fast as v1, v4 is yet faster (but v5 is not). Who would have thought such big differences here! Turns out, this is still not the fastest way to do it. The slowest part comes in iterating over the lists twice because we need to compute the set of all the keys, or having to do the more expensive getOrElse calls. What if we could do everything in one pass? We settled on a class that implements are final version which makes use of the following facts: Our keys and values are both Int ’s so we can keep them in one specialized Array as key1, value1, key2, value2, … pairs to avoid lookups Even though its asymptotically more work, it’s actually pretty low cost to keep the “maps” as sorted lists of key value pairs (recall that say, hash tables have all O(1) operations and don’t have to do any sorting) We can then simultaneously iterate through both lists of key/value pairs and build up the summed sparse vector in one pass. If we are at the same key in both lists we can output their sum, if we are ahead on one side we know we can output the lower side to the output vector, and we can consume at different rates to ensure we’re always in sync. That looks about like this: // Array(key0, value0, key1, value1, key2, value2, ...\n// plus possibly some unused elements at the end) final class VecMap private ( private val pairs : Array [ Int ], val size : Int ) { def + ( that : VecMap ) : VecMap = { val thisLen = this . size * 2 // Length of used portion of this.pairs array val thatLen = that . size * 2 // Length of used portion of that.pairs array val newPairs : Array [ Int ] = new Array [ Int ]( thisLen + thatLen ) var thisIdx = 0 var thatIdx = 0 var newIdx = 0 while ( thisIdx < thisLen && thatIdx < thatLen ) { val thisKey = this . pairs ( thisIdx ) val thatKey = that . pairs ( thatIdx ) if ( thisKey == thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) + that . pairs ( thatIdx + 1 ) thisIdx += 2 thatIdx += 2 } else if ( thisKey < thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 } else { newPairs ( newIdx ) = thatKey newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 } newIdx += 2 } if ( thisIdx < thisLen ) { // that.pairs is spent. Just finish off this while ( thisIdx < thisLen ) { newPairs ( newIdx ) = this . pairs ( thisIdx ) newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 newIdx += 2 } } else if ( thatIdx < thatLen ) { // this.pairs is spent. Just finish off that while ( thatIdx < thatLen ) { newPairs ( newIdx ) = that . pairs ( thatIdx ) newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 newIdx += 2 } } assert (( newIdx & 1 ) == 0 ) new VecMap ( newPairs , newIdx / 2 ) } ... } Parallelization Finally, of course this is Scala and we have very simple access to parallel collections . Interestingly here, Gibbs sampling is fundamentally sequential, but there are some opportunities for parallelism but several introductions of parallelism actually made CRP run slower! Always measure. But with the right use of parallel collections, such as computing the probabilities that a diner belongs to every possible existing table, we did get another 5× performance bump. Closing We don’t have the time to talk about every aspect of the couple weeks we spent squeezing a 300,000× speed improvement out of our naive CRP clustering implementation, but we hope some of the tools and strategies above might be useful in your work. At any rate, we hope you can make use of our JVM CRP implementation, which we believe to be the only JVM implementation available for large data sets, a foundational data science tool for clustering that we’ve now donated to the open source community. † M&M's™ is a trademark of Mars, Inc used here for illustrative educational purposes. posted on November 23, 2015 by Ryan Richt ← Previous Post Next Post → import com.monsanto.stats.tables._ import com.monsanto.stats.tables.clustering._ val cannedAllTopicVectorResults : Vector [ TopicVectorInput ] = MnMGen . cannedData val cannedCrp = new CRP ( ModelParams ( 5 , 2 , 2 ), cannedAllTopicVectorResults ) val crpResult = cannedCrp . findClusters ( 200 , RealRandomNumGen , cannedCrp . selectCluster ) import com.monsanto.stats.tables._ import com.monsanto.stats.tables.clustering._ val cannedAllTopicVectorResults : Vector [ TopicVectorInput ] = MnMGen . cannedData val cannedCrp = new CRP ( ModelParams ( 5 , 2 , 2 ), cannedAllTopicVectorResults ) val crpResult = cannedCrp . findClusters ( 200 , RealRandomNumGen , cannedCrp . selectCluster ) /*\n * C is just the result of this integral. C tells you the probability\n * that someone is going to sit somewhere and the probability of your\n * uncertainty about what the parameters of that table truly are. If you\n * toss 10 coins and get 6 heads 4 tails, you'd guess it is 60/40, but\n * you wouldn't be very certain. If you had 1000 samples you'd be more\n * certain, and likely be closer to 50/50. C it is accounting for that\n * uncertainty.\n */ def estimateCSmoothingFirst : Double = { // Compute partSumAi and partSumLogGammaAi by iterating through all // values in the WeightedVector's vecMap and computing the sum of the // values and their logGammas. // icky vars for performance in this critical path var partSumAi : Double = 0.0 var partSumLogGammaAi : Double = 0.0 var idx = 1 val len = size * 2 while ( idx < len ) { val v = pairs ( idx ) partSumAi += v + params . beta // add beta to this and the next value to smooth the curve val logGammaSmoothingFirst = if ( v < allTopicVectorResults . length ) cache ( v ) else logGamma ( v + params . beta ) partSumLogGammaAi += logGammaSmoothingFirst idx += 2 } /*\n * C is just the result of this integral. C tells you the probability\n * that someone is going to sit somewhere and the probability of your\n * uncertainty about what the parameters of that table truly are. If you\n * toss 10 coins and get 6 heads 4 tails, you'd guess it is 60/40, but\n * you wouldn't be very certain. If you had 1000 samples you'd be more\n * certain, and likely be closer to 50/50. C it is accounting for that\n * uncertainty.\n */ def estimateCSmoothingFirst : Double = { // Compute partSumAi and partSumLogGammaAi by iterating through all // values in the WeightedVector's vecMap and computing the sum of the // values and their logGammas. // icky vars for performance in this critical path var partSumAi : Double = 0.0 var partSumLogGammaAi : Double = 0.0 var idx = 1 val len = size * 2 while ( idx < len ) { val v = pairs ( idx ) partSumAi += v + params . beta // add beta to this and the next value to smooth the curve val logGammaSmoothingFirst = if ( v < allTopicVectorResults . length ) cache ( v ) else logGamma ( v + params . beta ) partSumLogGammaAi += logGammaSmoothingFirst idx += 2 } // Gamma is the continuous version of factorial, but off by 1, and its\n// more accurate to compute it and its log in one step def logGamma ( x : Double ) : Double = { val tmp : Double = ( x - 0.5 ) * Math . log ( x + 4.5 ) - ( x + 4.5 ) val ser : Double = 1.0 + 76.18009173 / ( x + 0 ) - 86.50532033 / ( x + 1 ) + 24.01409822 / ( x + 2 ) - 1.231739516 / ( x + 3 ) + 0.00120858003 / ( x + 4 ) - 0.00000536382 / ( x + 5 ) tmp + Math . log ( ser * Math . sqrt ( 2 * Math . PI )) } // Gamma is the continuous version of factorial, but off by 1, and its\n// more accurate to compute it and its log in one step def logGamma ( x : Double ) : Double = { val tmp : Double = ( x - 0.5 ) * Math . log ( x + 4.5 ) - ( x + 4.5 ) val ser : Double = 1.0 + 76.18009173 / ( x + 0 ) - 86.50532033 / ( x + 1 ) + 24.01409822 / ( x + 2 ) - 1.231739516 / ( x + 3 ) + 0.00120858003 / ( x + 4 ) - 0.00000536382 / ( x + 5 ) tmp + Math . log ( ser * Math . sqrt ( 2 * Math . PI )) } val first = Range ( 1 , 30 ). map ( i => ( i -> 17 )). toMap val second = Range ( 15 , 44 ). map ( i => ( i -> 64.0 )). toMap def v1 () = { // Add tv's vecMap to smoothedCounts vecMap. // MAKE THIS SIMPLER val temp : Vector [( Int , Double )] = first . toVector . map { case ( i , j ) => ( i , j . toDouble ) } ++ second . toVector val temp2 : Map [ Int , Vector [( Int , Double )]] = temp . groupBy ( _ . _1 ) val temp3 : Map [ Int , Vector [ Double ]] = temp2 . mapValues ( xs => xs . map ( _ . _2 )) val topicCountsSums : Map [ Int , Double ] = temp3 . mapValues ( _ . sum ) topicCountsSums . head } def v2 () = { val allDenseKeys = first . keySet ++ second . keySet val diffs = allDenseKeys . map { index => index -> ( first . getOrElse ( index , 0 ) + second . getOrElse ( index , 0.0 )) }. toMap diffs . head } def v3 () = { val sums = mutable . Map . empty [ Int , Double ] first . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + first ( k ) ) second . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + second ( k ) ) sums . head } def v4 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } second . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } sums . head } def v5 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } second . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } sums . head } val first = Range ( 1 , 30 ). map ( i => ( i -> 17 )). toMap val second = Range ( 15 , 44 ). map ( i => ( i -> 64.0 )). toMap def v1 () = { // Add tv's vecMap to smoothedCounts vecMap. // MAKE THIS SIMPLER val temp : Vector [( Int , Double )] = first . toVector . map { case ( i , j ) => ( i , j . toDouble ) } ++ second . toVector val temp2 : Map [ Int , Vector [( Int , Double )]] = temp . groupBy ( _ . _1 ) val temp3 : Map [ Int , Vector [ Double ]] = temp2 . mapValues ( xs => xs . map ( _ . _2 )) val topicCountsSums : Map [ Int , Double ] = temp3 . mapValues ( _ . sum ) topicCountsSums . head } def v2 () = { val allDenseKeys = first . keySet ++ second . keySet val diffs = allDenseKeys . map { index => index -> ( first . getOrElse ( index , 0 ) + second . getOrElse ( index , 0.0 )) }. toMap diffs . head } def v3 () = { val sums = mutable . Map . empty [ Int , Double ] first . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + first ( k ) ) second . keySet . foreach ( k => sums ( k ) = sums . getOrElse ( k , 0.0 ) + second ( k ) ) sums . head } def v4 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } second . foreach { kv => sums ( kv . _1 ) = sums . getOrElse ( kv . _1 , 0.0 ) + kv . _2 } sums . head } def v5 () = { val sums = mutable . Map . empty [ Int , Double ] first . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } second . foreach { kv => val k = kv . _1 ; sums ( k ) = sums . getOrElse ( k , 0.0 ) + kv . _2 } sums . head } // Array(key0, value0, key1, value1, key2, value2, ...\n// plus possibly some unused elements at the end) final class VecMap private ( private val pairs : Array [ Int ], val size : Int ) { def + ( that : VecMap ) : VecMap = { val thisLen = this . size * 2 // Length of used portion of this.pairs array val thatLen = that . size * 2 // Length of used portion of that.pairs array val newPairs : Array [ Int ] = new Array [ Int ]( thisLen + thatLen ) var thisIdx = 0 var thatIdx = 0 var newIdx = 0 while ( thisIdx < thisLen && thatIdx < thatLen ) { val thisKey = this . pairs ( thisIdx ) val thatKey = that . pairs ( thatIdx ) if ( thisKey == thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) + that . pairs ( thatIdx + 1 ) thisIdx += 2 thatIdx += 2 } else if ( thisKey < thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 } else { newPairs ( newIdx ) = thatKey newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 } newIdx += 2 } if ( thisIdx < thisLen ) { // that.pairs is spent. Just finish off this while ( thisIdx < thisLen ) { newPairs ( newIdx ) = this . pairs ( thisIdx ) newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 newIdx += 2 } } else if ( thatIdx < thatLen ) { // this.pairs is spent. Just finish off that while ( thatIdx < thatLen ) { newPairs ( newIdx ) = that . pairs ( thatIdx ) newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 newIdx += 2 } } assert (( newIdx & 1 ) == 0 ) new VecMap ( newPairs , newIdx / 2 ) } ... } // Array(key0, value0, key1, value1, key2, value2, ...\n// plus possibly some unused elements at the end) final class VecMap private ( private val pairs : Array [ Int ], val size : Int ) { def + ( that : VecMap ) : VecMap = { val thisLen = this . size * 2 // Length of used portion of this.pairs array val thatLen = that . size * 2 // Length of used portion of that.pairs array val newPairs : Array [ Int ] = new Array [ Int ]( thisLen + thatLen ) var thisIdx = 0 var thatIdx = 0 var newIdx = 0 while ( thisIdx < thisLen && thatIdx < thatLen ) { val thisKey = this . pairs ( thisIdx ) val thatKey = that . pairs ( thatIdx ) if ( thisKey == thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) + that . pairs ( thatIdx + 1 ) thisIdx += 2 thatIdx += 2 } else if ( thisKey < thatKey ) { newPairs ( newIdx ) = thisKey newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 } else { newPairs ( newIdx ) = thatKey newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 } newIdx += 2 } if ( thisIdx < thisLen ) { // that.pairs is spent. Just finish off this while ( thisIdx < thisLen ) { newPairs ( newIdx ) = this . pairs ( thisIdx ) newPairs ( newIdx + 1 ) = this . pairs ( thisIdx + 1 ) thisIdx += 2 newIdx += 2 } } else if ( thatIdx < thatLen ) { // this.pairs is spent. Just finish off that while ( thatIdx < thatLen ) { newPairs ( newIdx ) = that . pairs ( thatIdx ) newPairs ( newIdx + 1 ) = that . pairs ( thatIdx + 1 ) thatIdx += 2 newIdx += 2 } } assert (( newIdx & 1 ) == 0 ) new VecMap ( newPairs , newIdx / 2 ) } ... } † M&M's™ is a trademark of Mars, Inc used here for illustrative educational purposes. posted on November 23, 2015 by Ryan Richt Ryan Richt", "date": "2015-11-23"},
{"website": "Monsanto", "title": "What Makes a Data Scientist - Part 3", "author": ["\n                                        Seth Dobrin\n                                    "], "link": "http://engineering.monsanto.com/2015/11/16/what-makes-a-data-scientist-part-3/", "abstract": "Reposted from LinkedIn . The role of Data Scientist is often ill defined. A real Data Scientist\nis part developer, mathematician, data monger, big data engineer, data\nvisualization master, creative thinker, and domain expert. The way I\nlook at is she knows more math than a developer and is a better coder\nthan your current analytics team. She can visualize data better than a\nbig data engineer and can build a data system better than any data viz\nteam. She has more domain expertise than a business analysts or\nproduct owner and is more adept at defining capabilities than your\nbusiness partner. Finally, she can think about data in unique ways and\ncreate previously unrealized value from data. This is the unicorn. You\nmay already have one or two under your nose or you may have have “Data\nScientists” on your team that actually aren’t. Combine this with the relative newness of formal data science training\nprograms and it is no wonder that finding true a Data Scientist has\nbeen likened to finding unicorns. On the left or right coast this can\nbe challenging enough, now apply this to the Midwest. Similar to any\nsoftware decision, you have two main options: build or buy. Or in this\ncase, look outside and recruit for talent or invest in your current\ntalent pool. Both options come with their own set of challenges. With the option of buy/hire, the pool of true Data Scientists is\nsmall, but applying the requirement for domain knowledge can make the\npool dry up entirely, especially in our focus of agriculture. On top\nof that is the challenge of location. Attracting these individuals to\nperceived less desirable locations like the Midwest (which, by the\nway, is a really nice place to live even having grown up on the right\ncoast and having lived on the left coast for more than a decade)\ncreates even steeper barriers. You can look at remote employees, but\nhistorically, large companies have not been good at this and are just\nlearning how to do this effectively while still providing\nopportunities for development and career growth. With the right focus,\nrole definition and willingness to invest in expanding their domain\nknowledge, this is not an insurmountable challenge The other option of build, or developing the current talent pool, has\na separate set of challenges. Is the current talent pool even capable\nof developing the needed skill set? With the current work load how do\nyou help her find time at work to add the necessary skills to her\narsenal?  On the up side, she likely is already located where you want\nher and she is more likely to have a base of necessary domain\nexpertise. With a willingness to carve out time and pay for\ndevelopment, combined with identifying those in your current pool with\nthe desire and potential to develop these skills, this is not an\nintractable course of action either. Not surprisingly, the most correct course is a mixture of both. Bringing in external talent. Start by identifying those critical\nspots that you want to find a true Data Scientist and perhaps lack\nspecific domain or technical expertise. Be prepared to spend the\ntime. Create an intriguing role highlighting the challenges and the\nopportunities. Smart people want to tackle large, multidimensional\nchallenges. Define the role with a menu of requirements: two from this\ncategory, four from that category and stick to it, don’t\ncompromise. Unlike other roles where you can likely set you recruiter\nloose, you need to take an active role in identifying the candidates\nand building a relationship with them. Help them understand your data,\nyour challenges, and the opportunity for them. Most importantly, be\npatient and, again, don’t compromise. Investing in existing talent. It is unlikely that your entire\ncurrent team will be able to master the necessary skills to grow into\nData Scientists. Just like when looking externally, spend the time to\nbuild relationships with your team. Understand your team members and\ntheir capabilities and desires. Once you find that handful who have\nthe desire and ability, be clear about your expectations and clearly\ndefine the new roles and new opportunities. There are a lot of online\nresources for your teams, both free and paid. Take advantage of\nthese. Similar to attracting good talent, set a menu of skills that\nare needed and work with you team and your leaders to set a clear path\nto success. Some will succeed and some will not, you need to be okay\nwith that. Buying or building, that’s the historical software trade off. When\nestablishing a team of true Data Scientists, you face a similar\nquestion of hire externally or develop internal talent. The most\ncorrect answer is a combination of both. Looking externally and\ndeveloping talent both take time and resources. Neither is a silver\nbullet, but both need to be approached with a no compromise attitude\nand clear expectations. Finding or helping to create that unicorn\ntakes direct involvement and commitment from top executives, managers,\nand the people doing the work. In either case, make sure you are not\njust gluing a horn on to a mule can calling it a unicorn. Through hard\nwork, the truly talented team members will have fun and create\nvalue. posted on November 16, 2015 by Seth Dobrin ← Previous Post Next Post → posted on November 16, 2015 by Seth Dobrin Seth Dobrin", "date": "2015-11-16"},
{"website": "Monsanto", "title": "Anatomy of Analytics", "author": ["\n                                        Naghman Waheed\n                                    "], "link": "http://engineering.monsanto.com/2015/11/13/analytics-part-1/", "abstract": "Years ago, when I made the switch from procedural to object oriented\nprogramming, concepts such as function overloading fascinated me.  A\nsingle function changing and behaving based on the types of values\npassed to it spoke volumes to its simplicity and elegance. Feeling a\nbit nostalgic I am going to say, “Those were the good old days!”\nToday, the word Analytics is certainly an overloaded term in the\ndata world!  It is used to define work done by data scientists to\nbusiness analysts and from execution of sophisticated statistical\nalgorithms to the use of simple visualization tools. Oh, and let’s not\nforget its synonymous use with the term “Big Data.” I wish I could\nmuster up the same sense of euphoria for the overloaded term Analytics that I once felt for the use of overloaded functions\nin a class definition. In this multi-part series of posts I will share practical knowledge\nabout using data to gain insights that I have acquired during my\ncareer as a data and information architect. In addition, I will\nhighlight the discipline that is required to create the supporting\narchitecture and challenges encountered along the way when\ninstantiating a comprehensive analytical platform. The first step on\nthis journey is simply defining the consumers of information and\nunderstanding what they need. In short, the first commandment is\nsimply: know thy information consumer. Figure 1 captures the essence\nof this concept by defining four distinct types of consumers of data\n— information consumer, information pro-consumer, data\nscientist, and machine. Figure 1 - Information Consumer Types One thing to note in Figure 1 is that the word Analytics is\nsparingly used. This was deliberate. All information presented has\nanalytical value. More on that in the next post. For now, let’s just\nsay before the word Analytics took on a life of its own, many of\nthe terms above sufficed to describe how different consumers analyzed\ndata and information.  It was not long ago that the terms such as\n“slicing and dicing” and “drill down analysis” were commonly used to\ndescribe capabilities which allowed users to make important\ndecisions. Today those capabilities are still an integral component of\nBusiness Intelligence and offered by many vendors as a standard\nofferings in their respective suite of products. A few observations on information consumers in each category: Information Consumers are generally the business users and\nexecutives that want access to timely and accurate reports and\ndashboards through intuitive and user friendly interfaces. The\ninformation consumed by this group of consumers needs to be highly\naccurate, trusted, and reproducible.  Additional characteristics of\ninformation consumed by this set of users could be simple aggregated\nviews of past sales or financial data for a specific region or a view\nof forecasted sales numbers generated through some sophisticated\nprediction algorithm. Information Pro-Consumers are commonly known by more familiar\nterms such as business analysts and power users. This group is\nconcerned with performing analysis on data and generally their focus\nis to understand the past in order to predict the future. In addition\nto accuracy and timeliness this group often demands low latency and\nhigh accessibility to data. More over, this group prefers to have\nmultiple tools at their disposal to be able to perform aggregations,\ncalculations, slicing, and dicing as well as ad hoc analysis on data. Data Scientists do not need any introduction. Lately, much has\nbeen said and written about this set of users. A quick search on the\nweb will prove that. But this series is not about defining who they\nare. Instead it is about what they would like to do with\ndata. Regardless of whether these users are statisticians with\ncomputer science degrees or computational experts in some specific\nfield, their needs revolve around having access to any and every piece\nof data — big or small, structured or unstructured.  Often the\ndiscovery process and experimentation undertaken by this group may\ninclude diverse techniques ranging from simple data mining to applying\nvarious statistical algorithms to testing some hypothesis.  In the\nend, applying sophisticated statistical techniques and/or machine\nlearning algorithms to create insights is the primary motivation\nbehind the work of this user community. A few posts from one of my\ncolleagues are an interesting read on this particular data consumer\ntype: What makes a data scientist – Part 1 & What makes a data scientist – Part 2 . Machines operate on data and need data to operate. Despite this\nfact, in the data world the needs of this voiceless group is often\noverlooked.  With increased mechanization and automation,\nsystem-to-system communication is only increasing in importance and\nscale. Whether it is data collected through sensors and passed on to\nother systems for processing or data made available via APIs for\ngeneral consumption by external users or partners, the fact remains\nthat most companies are still trying to shore up their ability to\nservice this important category of consumers. In conclusion, the message is simple. It is critical to understand the\nneeds of those you serve. Such a basic yet crucial piece of\ninformation should be a precursor to everything else that\nfollows. Whether it is creating a reference architecture or standing\nup a scalable analytical platform, one thing is immediately apparent\nfrom Figure 1: the platform will need to cover use cases ranging from\nmachine learning to ad hoc analysis, data analysis to sophisticated\nanalytics, and from simple information consumption through APIs to\nmachine-to-machine communication. A key mandate for a successful\nanalytical platform is to make sure that the need of each consumer\ntype is addressed holistically in helping them turn data into useful\ninformation and insights. posted on November 13, 2015 by Naghman Waheed ← Previous Post Next Post → posted on November 13, 2015 by Naghman Waheed Naghman Waheed", "date": "2015-11-13"},
{"website": "Monsanto", "title": "Building a simple Spray application - Part 2", "author": ["\n                                        Scott MacDonald\n                                    "], "link": "http://engineering.monsanto.com/2015/11/12/simple-spray-part-2/", "abstract": "Last time we spoke we went through the simple building blocks to creating a Spray\napplication.  As you go forward though, you will want to have more\noptions in composing your routes, and making sure the routes are\nconstructed correctly. You can feel free to download the code and\nfollow along.  I’ve separated the different sections into their own\npackages, so when running the code change the import\ndemo.PACKAGE.ServiceActor in Main.scala to the appropriate package. Organizing your routes Last time we ended up with a single Trait that contained all our\nroutes, and a single Actor that extended it.  Summarized, it looked\nsomething like this: import akka.actor.Actor import spray.routing.HttpService import scala.concurrent._ class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( routes ) } trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes implicit def executionContext : ExecutionContextExecutor = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } ~ pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } } For a simple situation, doing everything in one trait is fine.\nHowever you’re probably going to end up doing something much larger,\nand stuffing everything together will get very confusing with all the\nnesting that goes on.  Also there’s not really a good relationship\nbetween junk and stuff , so separating that is probably a good\nidea. ###Separating your routes Separating the routes into their own traits is as straighforward as it\nsounds import demo.Stuff._ import spray.http.MediaTypes import spray.httpx.SprayJsonSupport._ import spray.routing.HttpService trait StuffRoute extends HttpService { implicit def executionContext = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } } trait JunkRoute extends HttpService { val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } Note that in each trait I’ve defined a routes property.  This is\nsimply a convention, you can call it whatever you wish.  However using\nthis convention has a small price.  Recall our service actor: class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } We extend the route trait and pass in the routes property to the runRoute method to setup the required receive action.  If we\nalways call the property routes on all our traits, we’re going to\nhit some conflicts when we extend all of these traits.  The solution\nis to no longer extend them, but to instantiate them inside our actor\ninstead.  This also means we need to extend from HttpServiceActor instead of Actor since we are no longer extending a trait that\ncontains the framework we need.  The final code looks like this: import spray.routing.HttpServiceActor class ServiceActor extends HttpServiceActor { override def actorRefFactory = context val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( stuff . routes ~ junk . routes ) } As you can see, we utilize the ~ operator to combine the routes. ###Ordering matters While it’s a good idea to separate your different contexts this way,\nyou can possibly run into an issue where you have path conflicts.  To\nshow this, append the code below to the end of each trait’s routes property: ~ get { complete ( \"root from stuff|junk\" ) } Run the app and access your root, you should only see the message from\nthe stuff service.  Change the ordering of the routes in the runRoute method and try again, you should see the junk service’s\nresponse.  It get’s worse, now try to access the /stuff context,\nyou’ll notice the same root message again. Remember that Spray looks for the first match, so the order you link\nthe routes matters.  If you have a general fallback in a trait early\nin the chain, it will never resolve to the other paths that are\nprobably more appropriate. If you need a general fallback strategy, make sure its in a different\ntrait at the end of the chain .  The most common mistake here is\ntacking on a custom 404 for a context if nothing matches.  If put at\ntoo high a level, it will actually block all other contexts. Now that we know how to split up our services into more managable\nchunks, let’s revisit how paths are built. ##Constructing paths more cleanly Our junk context currently can support the paths junk/mine and junk/yours .  It does so by nesting pathPrefix & pathEnd statements\nlike so: val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ... There is a way to condense this somewhat. While path(“junk/mine”) doesn’t work, you can separate the strings with the / character into\nsomething that does work. val routes = path ( \"junk\" / \"mine\" ) { get { complete ( \"MINE!\" ) } } ~ path ( \"junk\" / \"yours\" ) { get { complete ( \"YOURS!\" ) } } But the main benefit here is that it allows an easy way to grab path\nparameters.  To illustrate this, let’s go back to our stuff route. Currently the GET operation just returns a single item, let’s change\nthat to return something by id, pretending we’re talking to a data\nstore.  Instead of putting another string after the / we’ll instead\njust put in a Spray type matcher. val stuffMap = Map ( 1 -> Stuff ( 1 , \"my stuff\" ), 2 -> Stuff ( 2 , \"your stuff\" )) val routes = { respondWithMediaType ( MediaTypes . `application/json` ) { path ( \"stuff\" / IntNumber ) { ( id ) => get { complete ( stuffMap ( id )) } } ~ path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } Notice the (id) => expression, the matchers will map to that\ndeclaration in the order they are defined.  So you could do something\nlike the following: path ( \"stuff\" / IntNumber / \"substuff\" / IntNumber / \"subsubstuff\" / IntNumber ) { ( id , subId , subsubId ) => .... } Go ahead and modify your code with these changes and give it a try.\nYou should get different answers for a call to stuff/1 vs stuff/2 . You might also want to try stuff/3 , you’ll see we get an internal\nserver error, but that’s not what we really want.  Which brings us\nto…. ##Rejecting requests In the case above, a 404 - Not Found seems to be the response we\nreally want.  Luckilly the complete method is overloaded to allow us\nto do this easily by passing a status ( import\nspray.http.StatusCodes._ ) as the first parameter.  Let’s see how\nthis looks in our stuff service. import spray.http.StatusCodes._ ... path ( \"stuff\" / IntNumber ) { ( id ) => get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } ~ ... Go ahead and try that out, when you call the url stuff/3 you should\nget the correct 404 - Not Found error with our message included. ###Validating Requests In The Path In certain cases you might want a quick way to send a 400 - Bad\nRequest defined in your path structure.  For instance, we send a 404 - Not Found if the id isn’t available, but what if we wanted\nto tell the user that an id less than 1 wasn’t only not found, but\nwas an incorrect usage?  The validate directive gives us a quick way\nto do this.  Our code now looks like this: ... path ( \"stuff\" / IntNumber ) { ( id ) => validate ( id > 0 , \"Id must be greater than 0!\" ) { get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } } ... ###Using Exception Handlers There’s another way to handle common cases.  Let’s say we want an IllegalArgumentException to automatically be translated into a 400 - Bad Request .  In fact let’s modify our POST call to stuff to make that happen if it receives and id less than 1 ... path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => if ( stuff . id <= 0 ) throw new IllegalArgumentException ( \"Id cannot be less than 1!\" ) complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } .... If you run this code as it is, you’ll end up with an 500 - Internal\nServer Error .  However, you can attach an ExceptionHandler to your ServiceActor class to automatically take care of this for you. import spray.http.StatusCodes._ import spray.routing. { ExceptionHandler , HttpServiceActor } class ServiceActor extends HttpServiceActor { override def actorRefFactory = context implicit def exceptionHandler = ExceptionHandler { case ex : IllegalArgumentException => complete ( BadRequest -> ex . getMessage ) } val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( junk . routes ~ stuff . routes ) } Try running the service now and do a POST with an id less than 1,\nyou should see the correct HTTP response and message. ##In the end…. Spray gives you a lot of flexibility to setup your routes and\nresponses in a way that works best for your application.  I recommend\nstarting with the basic nesting and refactoring to other formats as\nyou get more complex.  Decide early on your strategy of returning\nerrors, so you’re not just shimming it in later.  Correct error\nresponses will save you tons of support hours. In the next installment I’ll go over Unit Testing and including\nSwagger documentation (and how that might affect your route\nconstruction). posted on November 12, 2015 by Scott MacDonald ← Previous Post Next Post → import akka.actor.Actor import spray.routing.HttpService import scala.concurrent._ class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( routes ) } trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes implicit def executionContext : ExecutionContextExecutor = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } ~ pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } } import akka.actor.Actor import spray.routing.HttpService import scala.concurrent._ class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( routes ) } trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes implicit def executionContext : ExecutionContextExecutor = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } ~ pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } } import demo.Stuff._ import spray.http.MediaTypes import spray.httpx.SprayJsonSupport._ import spray.routing.HttpService trait StuffRoute extends HttpService { implicit def executionContext = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } } trait JunkRoute extends HttpService { val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } import demo.Stuff._ import spray.http.MediaTypes import spray.httpx.SprayJsonSupport._ import spray.routing.HttpService trait StuffRoute extends HttpService { implicit def executionContext = actorRefFactory . dispatcher val routes = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } } trait JunkRoute extends HttpService { val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } } class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } import spray.routing.HttpServiceActor class ServiceActor extends HttpServiceActor { override def actorRefFactory = context val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( stuff . routes ~ junk . routes ) } import spray.routing.HttpServiceActor class ServiceActor extends HttpServiceActor { override def actorRefFactory = context val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( stuff . routes ~ junk . routes ) } ~ get { complete ( \"root from stuff|junk\" ) } ~ get { complete ( \"root from stuff|junk\" ) } val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ... val routes = pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ... val routes = path ( \"junk\" / \"mine\" ) { get { complete ( \"MINE!\" ) } } ~ path ( \"junk\" / \"yours\" ) { get { complete ( \"YOURS!\" ) } } val routes = path ( \"junk\" / \"mine\" ) { get { complete ( \"MINE!\" ) } } ~ path ( \"junk\" / \"yours\" ) { get { complete ( \"YOURS!\" ) } } val stuffMap = Map ( 1 -> Stuff ( 1 , \"my stuff\" ), 2 -> Stuff ( 2 , \"your stuff\" )) val routes = { respondWithMediaType ( MediaTypes . `application/json` ) { path ( \"stuff\" / IntNumber ) { ( id ) => get { complete ( stuffMap ( id )) } } ~ path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } val stuffMap = Map ( 1 -> Stuff ( 1 , \"my stuff\" ), 2 -> Stuff ( 2 , \"your stuff\" )) val routes = { respondWithMediaType ( MediaTypes . `application/json` ) { path ( \"stuff\" / IntNumber ) { ( id ) => get { complete ( stuffMap ( id )) } } ~ path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } } } path ( \"stuff\" / IntNumber / \"substuff\" / IntNumber / \"subsubstuff\" / IntNumber ) { ( id , subId , subsubId ) => .... } path ( \"stuff\" / IntNumber / \"substuff\" / IntNumber / \"subsubstuff\" / IntNumber ) { ( id , subId , subsubId ) => .... } import spray.http.StatusCodes._ ... path ( \"stuff\" / IntNumber ) { ( id ) => get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } ~ ... import spray.http.StatusCodes._ ... path ( \"stuff\" / IntNumber ) { ( id ) => get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } ~ ... ... path ( \"stuff\" / IntNumber ) { ( id ) => validate ( id > 0 , \"Id must be greater than 0!\" ) { get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } } ... ... path ( \"stuff\" / IntNumber ) { ( id ) => validate ( id > 0 , \"Id must be greater than 0!\" ) { get { stuffMap . get ( id ) match { case Some ( stuff ) => complete ( stuff ) case None => complete ( NotFound -> s \"No stuff with id $id was found!\" ) } } } } ... ... path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => if ( stuff . id <= 0 ) throw new IllegalArgumentException ( \"Id cannot be less than 1!\" ) complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } .... ... path ( \"stuff\" ) { post { entity ( as [ Stuff ]) { stuff => if ( stuff . id <= 0 ) throw new IllegalArgumentException ( \"Id cannot be less than 1!\" ) complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } } .... import spray.http.StatusCodes._ import spray.routing. { ExceptionHandler , HttpServiceActor } class ServiceActor extends HttpServiceActor { override def actorRefFactory = context implicit def exceptionHandler = ExceptionHandler { case ex : IllegalArgumentException => complete ( BadRequest -> ex . getMessage ) } val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( junk . routes ~ stuff . routes ) } import spray.http.StatusCodes._ import spray.routing. { ExceptionHandler , HttpServiceActor } class ServiceActor extends HttpServiceActor { override def actorRefFactory = context implicit def exceptionHandler = ExceptionHandler { case ex : IllegalArgumentException => complete ( BadRequest -> ex . getMessage ) } val stuff = new StuffRoute { override implicit def actorRefFactory = context } val junk = new JunkRoute { override implicit def actorRefFactory = context } def receive = runRoute ( junk . routes ~ stuff . routes ) } posted on November 12, 2015 by Scott MacDonald Scott MacDonald", "date": "2015-11-12"},
{"website": "Monsanto", "title": "What Makes a Data Scientist - Part 2", "author": ["\n                                        Seth Dobrin\n                                    "], "link": "http://engineering.monsanto.com/2015/11/11/what-makes-a-data-scientist-part-2/", "abstract": "Reposted from LinkedIn . A colleague of mine eloquently and rightfully declares that “if one\nperson or one team can solve a problem by themselves, it is probably\nnot a problem worth solving.” Given this context, as a follow on to my\npost “ What Makes a Data Scientist - Part 1 ,” it is fitting to\nexplore the role of teams in defining a Data Scientist. As discussed\nin the first post, the spectrum of skills that a Data Scientist needs\nto posses is too vast for a single person to be an expert in all. As\npart of a team she will lead a group of individuals who have more\ndepth than her in some areas and less in others. How she utilizes and\nleverages that team as well as her ability to reach across functional\nboundaries will define the level of success her and her team actually\nattain. Based on the spectrum of work a Data Scientist does she can actually\nland in two different organizations: an analytics team or a data team,\nboth of which should have Data Scientists. The expectation for Data\nScientists varies depending on which side of the fence she lands. The\nteam with a data-focused Data Scientist is responsible for data\narchitecture, data engineering, data intake, data transformation,\nontology, metadata, statistical transformation, and descriptive\nanalytics & reporting. A Data Scientist who lands on the\nanalytics-focused team is responsible for descriptive analytics,\nmodel-based analytics, and data journalism. This is a critical point,\nif the Data Scientist building the model were responsible for doing\nthis foundational work on the data, her natural inclination would be\nto bring it together to solve her problem. The outcome of having a\nseparate team led by Data Scientists focused on the data is that the\ndata can then be managed with the broader enterprise in mind and not\njust the problem at hand. To be clear, one person is not expected to be an expert in all of\nthese areas regardless of which side of the fence she lands. The\nexpectation is that she has a solid understanding of the spectrum and\ndepth in at least 2-3 areas, as well as the ability to execute (the\nconcept of “E-shaped” skills). The Data Scientist cannot be the only\none with an E-shaped skill set, her entire team must have this type of\nskill set as well. If you are familiar with Agile development\nconcepts, think of the Data Scientist as Data and Analytics Product\nOwner (PO) and the others as “Team Members”. With this structure you\ncan build Agile teams of 3-4 individuals that possess the necessary\nspectrum of skills and domain expertise. Since my current focus is leading a data team, I am going to focus on\nthe Data Scientist that is on the data side of the fence. In this\nexample she must be able to lead and direct her team as well as work\nacross organizational boundaries. Her role in this position is to\nunderstand the data and understand the application of these data to\nsolve a complex modeling problem. Leveraging her domain expertise, she\nworks with the analytics team to understand its problem and put it in\nthe context of the larger organization. To do this the team needs to\narchitect and engineer the solution based on a set of use cases. While\nthe solution is being architected and engineered, the team needs to\nidentify, source, consume, and transform the data. The team also needs\nto ensure the data have appropriate context in the form of an ontology\nand metadata. This is important in building a reusable enterprise data\nasset. Essentially, under her guidance the team must prepare the data\nand publish it to the Enterprise in such a way that it stands on its\nown. This last part is where her foundation in analytics comes\nin. Additionally, her depth of knowledge in statistical\ntransformation, descriptive analytics, and reporting comes in to play\nat this point. Data sets are a commodity, but unlike other commodities data sets are\nreusable. Even more importantly, the value of data multiplies with\nevery use. Data gains value when leveraged by analytical models. These\nmodels are simultaneously generating new data and driving decisions. A\nData Scientist maximizes her value when she treats data as a reusable\ncommodity for the enterprise. By building and nurturing these data\nassets she enables reuse of the data for application development,\nother analytical questions, and reporting. Circling back to the\nstatement “if one person or one team can solve a problem by themselves\nit is probably not a problem worth solving,” having an Agile team\nbuilt around the spectrum of data science skills is critical to\naddressing decisions that deliver real business value for an\norganization while simultaneously building a repertoire of reusable\ndata assets. posted on November 11, 2015 by Seth Dobrin ← Previous Post Next Post → posted on November 11, 2015 by Seth Dobrin Seth Dobrin", "date": "2015-11-11"},
{"website": "Monsanto", "title": "What Makes a Data Scientist - Part 1", "author": ["\n                                        Seth Dobrin\n                                    "], "link": "http://engineering.monsanto.com/2015/11/09/what-makes-a-data-scientist-part-1/", "abstract": "Reposted from LinkedIn . When having conversations about data and analytics, the new fad is\nhiring Data Scientists. But I find that most people don’t know what\nthat means, and most people we call Data Scientists are not.\nTypically, individuals who are called Data Scientists fall into one of\nthree camps.  The first and least prevalent is a real Data\nScientist. The second camp is composed of database engineers,\narchitects, or data analysts. And the third is comprised of\nstatisticians and/or mathematicians. That’s not to say that the two\nlatter camps can’t become Data Scientists, but to do so requires a\nwillingness to learn and take risks, as well as an investment in time\nby the individual and her employer. So what is a Data Scientist? She is an individual who possesses domain\nknowledge in a relevant area, a spectrum of skills and experiences\nthat range from raw data architecture to what I will call data\njournalism.  Perhaps even more importantly this breadth of skills and\nexperiences needs to be with modern, relevant technologies and\ntechniques. These skills and experiences are as follows and in future\nposts I will dig into what each of these mean and why some combination\nof them are required. The specific categories of skills and\nexperiences are as follows: database architecture & engineering, data\nintake & transformation, ontology & metadata, statistical\ntransformation & analytics foundation, descriptive analytics &\nreporting, model-based analytics and finally data journalism. Data\nScientists are the data and analytics version of an Agile Development\nteam member with E-shaped skills. This means she should have a depth\nof skills in 2-3 areas, a breadth of skills that spans most of the\ngamut of skills listed above and the ability to execute. If you have\nseparate data and analytics teams in your organization, you need Data\nScientists in both organizations. The depth of her skills in\ndescriptive analytics & reporting determines which group she belongs\nin, a data team or an analytics team. As mentioned above the skills and experiences need to be with relevant\nand modern technologies and techniques. What do I mean by this? A Data\nScientist possesses an understanding of legacy type platforms, such as\nrelation databases management systems (RDBMS) and data warehouses\n(DW), and she is able to manage processes such as extract, transform\nand load (ETL). Additionally, she will have a foundation in\ntraditional supervised and unsupervised statistical methods. While\nthese are valuable skills and an important part of her toolbox, they\nare only the beginning. Her toolbox should also consist of an\nunderstanding and depth of skill in the so called “Not only SQL”, or\nNoSQL, platforms. Theses include columnar data stores, document\nstores, key-value stores, graph databases, and finally multi-model\ndatabases.  More modern analytics skill include techniques such as\nmachine learning, neural networks, and operational research.  To\nleverage most of these modern tools, she needs to know at least one\nprogramming language, and preferably more, such as Python, R, Java, or\nScala.  All of these skills and experiences should be on web-scale,\ncloud-based environments. Wow what a list and what expectations! It is important to remember\nthat very few people, if any, will master the entire spectrum of\nskills, platforms, and techniques. What is important is that anyone\nyou call a Data Scientist has a breadth of skills and experiences that\nspan from data architecture to data journalism with depth in at least\n2-3 areas. As technologies change it is important that they be a\nlearner so they can maintain their understanding of relevant\ntechnologies. posted on November 9, 2015 by Seth Dobrin ← Previous Post Next Post → posted on November 9, 2015 by Seth Dobrin Seth Dobrin", "date": "2015-11-09"},
{"website": "Monsanto", "title": "AWS re:Invent 2015", "author": ["\n                                        David Dooling\n                                    ", "\n                                        Ryan Richt\n                                    ", "\n                                        Mark Sparks\n                                    ", "\n                                        Danny Williams\n                                    "], "link": "http://engineering.monsanto.com/2015/10/14/aws-reinvent/", "abstract": "Monsanto sent several of our IT staff to AWS re:Invent last week. We learned a lot and were excited to hear about the new\nservices announced, especially those around the EC2 Container Service,\nKinesis, IoT, AWS Config, and the data import/export device Snowball.\nWe also gave a few talks and look forward to connecting and\ncollaborating with the many that came up after the talks and spoke to\nus. If you couldn’t attend in person, have a look and let us know what you think . ARC401 Cloud First: New Architecture for New Infrastructure David Dooling and Ryan Richt gave a talk about our Cloud First\napproach, architecture, and automation. SlideShare ARC401 Cloud First: New Architecture for New Infrastructure MBL203 Drones to Cars: Connecting the Devices in Motion to the Cloud Danny Williams and Mark Sparks on our IoT team paired up with\nConstantin Gonzalez, Jan Metzner, and Ricardo DeMatos from AWS to\npresent our IoT platform Field Drive and the newly announced AWS IoT service. SlideShare MBL203 Drones to Cars: Connecting the Devices in Motion to the Cloud posted on October 14, 2015 by David Dooling Ryan Richt Mark Sparks Danny Williams ← Previous Post Next Post → posted on October 14, 2015 by David Dooling Ryan Richt Mark Sparks Danny Williams David Dooling Ryan Richt Mark Sparks Danny Williams", "date": "2015-10-14"},
{"website": "Monsanto", "title": "Demystifying IAM Roles", "author": ["\n                                        David Dooling\n                                    "], "link": "http://engineering.monsanto.com/2015/10/07/demystifying-iam-roles/", "abstract": "TL;DR The step-by-step process for creating IAM Roles and associating them\nwith EC2 Instances in CloudFormation. Create the assumeRolePolicyStatement and assumeRolePolicyDocument Create an AWS::IAM::Role for your EC2 instance, associating that role with the assumeRolePolicyDocument Create an AWS::IAM::InstanceProfile associated with your AWS::IAM::Role Create a PolicyStatement that defines the allowed action Put that PolicyStatement in a PolicyDocument Create an AWS::IAM::Policy for your PolicyDocument and associate it with all the AWS::IAM::Role, i.e.,\ninstances, that need that role Finally, when you create your instance resource, associate your AWS::IAM::InstanceProfile with your instance\nusing the IamInstanceProfile property Managing EC2 Instance Privileges If you have used AWS EC2 for any length of time you have come\nacross the need for an EC2 instance to create or access another AWS\nresource on your behalf.  For example, perhaps you have needed an EC2\ninstance to be able to publish notifications on an SNS topic, get a\nscript or configuration from a private S3 bucket, or even spin up or\nterminate another EC2 instance it was monitoring. When first encountering this problem, you may have decided to put your\nAPI key and secret somewhere on the instance and then point the CLI or\nSDK to those credentials.  While this is expedient, it is not very\nsafe.  As the name implies, your API secret is not something you\nreally want getting out.  If you choose this pattern and hard code\nyour API key and secret everywhere, if one of the instances gets\ncompromised you are going to have a lot of cleaning up to do…\nand probably a lot of explaining too. A more secure approach would be to create an IAM User for\neach task that needs to be accomplished.  This user would then only be\nassigned the permissions it needed to accomplish its task.  An API key\nand secret could be generated for that user and then used on the\ninstances that needed that permission.  Then, should an instance be\ncompromised, anyone who got a hold of that key would only be able to\ndo a specific task and you would only need to deactivate one key and\nclean up and reconfigure a hopefully smaller list of instances.  This\napproach still has its disadvantages.  First, there is more overhead\nand management of IAM Users.  Second, best practices dictate that you\nshould rotate your API keys regularly.  If you need lots of users for\nlots of different tasks, it can be quite a bit of work to adhere to\nthat best practice. An even better approach for securely granting privileges to your EC2\ninstances is to use IAM Roles for EC2 .  With IAM Roles,\nyou create a role with the permission your EC2 instance needs and\nassign that role to your instance.  AWS, through the instance metadata , provides an API key and secret to your\ninstance that has the desired permissions.  AWS automatically manages\nthe API key rotation and all the AWS SDKs and CLI know to look in the\ninstance metadata for the API key and secret.  Sounds great, right?\nWell, it is great, but creating roles and assigning them to instances\nis a bit more complicated than it sounds, especially when using AWS CloudFormation . Since at Monsanto we use CloudFormation extensively ,\nwe’ve thought a lot about how best to leverage IAM Roles when we\ncreate templates using our CloudFormation Template Generator (CFTG) .  We think we’ve come\nup with a straightforward, easy to comprehend way to create IAM Roles\nand assign them to EC2 instances in CloudFormation templates.  So how\ndo we do it? Boilerplate The first thing you want to do when dealing with IAM Roles and EC2\ninstances in CloudFormation is get some boilerplate code out of the\nway.  Specifically, you need to create a PolicyStatement with a\nPolicyDocument that allows EC2 instances to assume IAM Roles.  Using\nCFTG, that looks like this: val allow : String = \"Allow\" // 0. Common Policy Statments and Documents val assumeRolePolicyStatement = PolicyStatement ( Effect = allow , Principal = Some ( DefinedPrincipal ( Map ( \"Service\" -> Seq ( \"ec2.amazonaws.com\" )))), Action = Seq ( \"sts:AssumeRole\" ) ) val assumeRolePolicyDocument = PolicyDocument ( Statement = Seq ( assumeRolePolicyStatement ) ) The first thing we do is create a val for the string \"Allow\" since\nwe are going to use that string so often.  Then we create our assumeRolePolicyStatement with EC2 as the principal and AssumeRole\nas the action.  We then create the associated PolicyDocument, assumeRolePolicyDocument , that holds the PolicyStatement.  You will\nsee this pattern when dealing with IAM roles in CloudFormation over\nand over: creating standalone PolicyStatements that then get added to\nPolicyDocuments.  It is the PolicyDocument that is referenced by other\nresources. Roles for each instance type The next thing you do is to create an “empty” role for each type of\ninstance you need, associating the role with the assumeRolePolicyDocument just created.  This association tells AWS\nwhat kind of role this is.  In this case, a role that allows EC2\ninstances to assume roles.  See how it starts to get confusing?  Below\nis the CFTG code for two different instance types, a NAT instance role\nand a bastion instance role. val natRoleResource = `AWS::IAM::Role` ( name = \"NATRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) val bastionRoleResource = `AWS::IAM::Role` ( name = \"BastionRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) You can see that there is not much in these roles as currently\ndefined.  Other than giving them a name and associating them with the assumeRolePolicyDocument , we only specify a path.  Here, we give the\nroot path, / , but you could use the path to restrict where this\npolicy an be applied. Instance profiles for each instance role For each instance role you created above, you next need to create an\nInstanceProfile. There is a one-to-one mapping between instance roles\nand InstanceProfiles.  Using CFTG, that looks like: val natInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"NATProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( natRoleResource )) ) val bastionInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"BastionProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( bastionRoleResource )) ) Again, not much to see here.  Other than setting the name and the\nassociation with the role, we set a non-restrictive path. Policy statements for each action Now we finally get to actually defining what actions we want the\ninstances to be able to assume.  In doing this, we change our point of\nview.  Rather than thinking about the instance types, we think about\nthe actions we want any instance to take.  A single instance type\ncould need more than one type of action.  Example CFTG code is below. val natTakeOverPolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ), Resource = Some ( \"*\" ) ) val staxS3PolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"s3:GetObject\" ), Resource = Some ( `Fn::Join` ( \"\" , Seq ( \"arn:aws:s3:::\" , `AWS::StackName` , \"/*\" ))) ) Above we define two different sets of actions using two\nPolicyStatements.  The first PolicyStatement, natTakeOverPolicyStatement , defines the types of actions a NAT box\nwould need to terminate and recreate it high-availability (HA) NAT\npartner and take over its route while it is down.  Unfortunately, it\ndoes not restrict the application of these policies to only its HA NAT\npartner.  Those actions can be executed on any EC2 instance ( Resource\n= Some(\"*\") ) in your account.  It would be better to restrict these\nactions to the VPC in which the NAT instances reside .  The\nsecond PolicyStatement allows read-only access to the S3 bucket with\nthe same name as the CloudFormation stack. Policy documents to hold the statements Remember near the beginning when we said you will often see\nPolicyStatements associated with PolicyDocuments?  Well, that is all\nwe are doing here. val natTakeOverPolicyDocument = PolicyDocument ( Statement = Seq ( natTakeOverPolicyStatement )) val staxS3PolicyDocument = PolicyDocument ( Statement = Seq ( staxS3PolicyStatement )) It is worth noting that a PolicyDocument can hold more than one\nPolicyStatement, as implied by the value being a Seq() , should that\nbe useful to you. Creating IAM Policies The next step is to create IAM Policies.  The role of an IAM Policy is\nto associate a PolicyDocument with one or more of the instance roles.\nIn other words, there is a one-to-one mapping of an IAM Policy to a\nPolicyDocument but the IAM Policy can hold more than one instance\nrole. val natTakeoverPolicyResource = `AWS::IAM::Policy` . from ( name = \"NatTakeoverPolicy\" , PolicyDocument = natTakeOverPolicyDocument , PolicyName = \"NatTakeover\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ))), Users = None ) val staxS3PolicyResource = `AWS::IAM::Policy` . from ( name = \"StaxS3Policy\" , PolicyDocument = staxS3PolicyDocument , PolicyName = \"StaxS3\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ), ResourceRef ( bastionRoleResource ))), Users = None ) Above we are using the AWS::IAM::Policy helper method from to\ncreate the IAM Policy resource.  You can see in each case we give the\nIAM::Policy resource a name, associate it with a single\nPolicyDocument, and give the policy a name.  We do not associate the\npolicy to any groups or users.  In the case of natTakeoverPolicyResource , we associate it with a single instance\nrole, natRoleResource .  For the staxS3PolicyResource , we associate\nit with two instance roles, both the natRoleResource and the bastionRoleResource . Putting it all together We now have several IAM Policies that associate a set of actions as\ncontained within the PolicyDocument with instance roles.  Those\ninstance roles, in turn, are associated with InstanceProfiles.  It is\nthose InstanceProfiles that are used when creating EC2 instances. def natInstance ( number : Int , subnet : `AWS::EC2::Subnet` ) = { val name = \"NAT\" + number + \"Instance\" Builders . ec2 ( name = name , InstanceType = ParameterRef ( natInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( awsNATAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), SubnetId = subnet , Tags = AmazonTag . fromName ( name ), Metadata = Some ( Map ( \"Comment1\" -> ( s \"Create NAT $nat1\" ))), IamInstanceProfile = Some ( natInstanceProfileResource ), SourceDestCheck = Some ( \"false\" ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) val nat1Instance = natInstance ( 1 , pubSubnet1 ) val nat2Instance = natInstance ( 2 , pubSubnet2 ) Here, since we are creating two NAT instances, we have defined a\nsimple method to create them using the CFTG EC2 builder method, ec2 ,\nand I have removed the UserData. Ignoring the unimportant bits, you\nsee that the IamInstanceProfile is set to natInstanceProfileResource , the InstanceProfile we created above\nand, using the IAM Policy, associated with the natTakeOverPolicyDocument and the staxS3PolicyDocument through the natRoleResource . Similarly the bastion instance uses the bastionInstanceProfileResource as its IamInstanceProfile. val bastion = \"BastionInstance\" val bastionInstance = Builder . ec2 ( name = bastion , InstanceType = ParameterRef ( bastionInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), IamInstanceProfile = Some ( bastionInstanceProfileResource ), SecurityGroupIds = Seq (), SubnetId = pubSubnet1 , Tags = AmazonTag . fromName ( bastion ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) JSON The JSON created by all that CFTG code is shown below. \"NATRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"NATProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"StaxS3Policy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:s3:::\" , { \"Ref\" : \"AWS::StackName\" }, \"/*\" ]] } }] }, \"PolicyName\" : \"BastionRoleStaxS3\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }, { \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NatTakeoverPolicy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ], \"Resource\" : \"*\" }] }, \"PolicyName\" : \"NatRoleNatTakeover\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NAT1Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT1Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #1\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"NAT2Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT2Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #2\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"BastionInstance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AmazonLinuxAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"BastionInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"BastionInstance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"IamInstanceProfile\" : { \"Ref\" : \"BastionProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Type\" : \"AWS::EC2::Instance\" } Note that some of those variables we defined ended up being serialized\ninto JSON several times in the resulting template.  Using CFTG, you\ncan avoid this duplication and possible future error by only have one\nplace to update when changes need to be made. Wrapping Up We hope this walk through of how we create and assign IAM Roles in\nCloudFormation is helpful.  If you have a different approach or ideas\non how to improve our approach, we’d love to hear them . posted on October 7, 2015 by David Dooling ← Previous Post Next Post → val allow : String = \"Allow\" // 0. Common Policy Statments and Documents val assumeRolePolicyStatement = PolicyStatement ( Effect = allow , Principal = Some ( DefinedPrincipal ( Map ( \"Service\" -> Seq ( \"ec2.amazonaws.com\" )))), Action = Seq ( \"sts:AssumeRole\" ) ) val assumeRolePolicyDocument = PolicyDocument ( Statement = Seq ( assumeRolePolicyStatement ) ) val allow : String = \"Allow\" // 0. Common Policy Statments and Documents val assumeRolePolicyStatement = PolicyStatement ( Effect = allow , Principal = Some ( DefinedPrincipal ( Map ( \"Service\" -> Seq ( \"ec2.amazonaws.com\" )))), Action = Seq ( \"sts:AssumeRole\" ) ) val assumeRolePolicyDocument = PolicyDocument ( Statement = Seq ( assumeRolePolicyStatement ) ) val natRoleResource = `AWS::IAM::Role` ( name = \"NATRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) val bastionRoleResource = `AWS::IAM::Role` ( name = \"BastionRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) val natRoleResource = `AWS::IAM::Role` ( name = \"NATRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) val bastionRoleResource = `AWS::IAM::Role` ( name = \"BastionRole\" , AssumeRolePolicyDocument = assumeRolePolicyDocument , Path = Some ( \"/\" ) ) val natInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"NATProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( natRoleResource )) ) val bastionInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"BastionProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( bastionRoleResource )) ) val natInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"NATProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( natRoleResource )) ) val bastionInstanceProfileResource = `AWS::IAM::InstanceProfile` ( name = \"BastionProfile\" , Path = \"/\" , Roles = Seq ( ResourceRef ( bastionRoleResource )) ) val natTakeOverPolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ), Resource = Some ( \"*\" ) ) val staxS3PolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"s3:GetObject\" ), Resource = Some ( `Fn::Join` ( \"\" , Seq ( \"arn:aws:s3:::\" , `AWS::StackName` , \"/*\" ))) ) val natTakeOverPolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ), Resource = Some ( \"*\" ) ) val staxS3PolicyStatement = PolicyStatement ( Effect = allow , Action = Seq ( \"s3:GetObject\" ), Resource = Some ( `Fn::Join` ( \"\" , Seq ( \"arn:aws:s3:::\" , `AWS::StackName` , \"/*\" ))) ) val natTakeOverPolicyDocument = PolicyDocument ( Statement = Seq ( natTakeOverPolicyStatement )) val staxS3PolicyDocument = PolicyDocument ( Statement = Seq ( staxS3PolicyStatement )) val natTakeOverPolicyDocument = PolicyDocument ( Statement = Seq ( natTakeOverPolicyStatement )) val staxS3PolicyDocument = PolicyDocument ( Statement = Seq ( staxS3PolicyStatement )) val natTakeoverPolicyResource = `AWS::IAM::Policy` . from ( name = \"NatTakeoverPolicy\" , PolicyDocument = natTakeOverPolicyDocument , PolicyName = \"NatTakeover\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ))), Users = None ) val staxS3PolicyResource = `AWS::IAM::Policy` . from ( name = \"StaxS3Policy\" , PolicyDocument = staxS3PolicyDocument , PolicyName = \"StaxS3\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ), ResourceRef ( bastionRoleResource ))), Users = None ) val natTakeoverPolicyResource = `AWS::IAM::Policy` . from ( name = \"NatTakeoverPolicy\" , PolicyDocument = natTakeOverPolicyDocument , PolicyName = \"NatTakeover\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ))), Users = None ) val staxS3PolicyResource = `AWS::IAM::Policy` . from ( name = \"StaxS3Policy\" , PolicyDocument = staxS3PolicyDocument , PolicyName = \"StaxS3\" , Groups = None , Roles = Some ( Seq ( ResourceRef ( natRoleResource ), ResourceRef ( bastionRoleResource ))), Users = None ) def natInstance ( number : Int , subnet : `AWS::EC2::Subnet` ) = { val name = \"NAT\" + number + \"Instance\" Builders . ec2 ( name = name , InstanceType = ParameterRef ( natInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( awsNATAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), SubnetId = subnet , Tags = AmazonTag . fromName ( name ), Metadata = Some ( Map ( \"Comment1\" -> ( s \"Create NAT $nat1\" ))), IamInstanceProfile = Some ( natInstanceProfileResource ), SourceDestCheck = Some ( \"false\" ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) val nat1Instance = natInstance ( 1 , pubSubnet1 ) val nat2Instance = natInstance ( 2 , pubSubnet2 ) def natInstance ( number : Int , subnet : `AWS::EC2::Subnet` ) = { val name = \"NAT\" + number + \"Instance\" Builders . ec2 ( name = name , InstanceType = ParameterRef ( natInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( awsNATAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), SubnetId = subnet , Tags = AmazonTag . fromName ( name ), Metadata = Some ( Map ( \"Comment1\" -> ( s \"Create NAT $nat1\" ))), IamInstanceProfile = Some ( natInstanceProfileResource ), SourceDestCheck = Some ( \"false\" ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) val nat1Instance = natInstance ( 1 , pubSubnet1 ) val nat2Instance = natInstance ( 2 , pubSubnet2 ) val bastion = \"BastionInstance\" val bastionInstance = Builder . ec2 ( name = bastion , InstanceType = ParameterRef ( bastionInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), IamInstanceProfile = Some ( bastionInstanceProfileResource ), SecurityGroupIds = Seq (), SubnetId = pubSubnet1 , Tags = AmazonTag . fromName ( bastion ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) val bastion = \"BastionInstance\" val bastionInstance = Builder . ec2 ( name = bastion , InstanceType = ParameterRef ( bastionInstanceTypeParameter ), KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), IamInstanceProfile = Some ( bastionInstanceProfileResource ), SecurityGroupIds = Seq (), SubnetId = pubSubnet1 , Tags = AmazonTag . fromName ( bastion ), UserData = Some ( `Fn::Base64` ( `Fn::Join` ( \"\" , Seq [ Token [ String ]](...)))) ) \"NATRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"NATProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"StaxS3Policy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:s3:::\" , { \"Ref\" : \"AWS::StackName\" }, \"/*\" ]] } }] }, \"PolicyName\" : \"BastionRoleStaxS3\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }, { \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NatTakeoverPolicy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ], \"Resource\" : \"*\" }] }, \"PolicyName\" : \"NatRoleNatTakeover\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NAT1Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT1Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #1\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"NAT2Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT2Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #2\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"BastionInstance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AmazonLinuxAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"BastionInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"BastionInstance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"IamInstanceProfile\" : { \"Ref\" : \"BastionProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Type\" : \"AWS::EC2::Instance\" } \"NATRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionRole\" : { \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] }] }, \"Path\" : \"/\" }, \"Type\" : \"AWS::IAM::Role\" } , \"BastionProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"NATProfile\" : { \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::InstanceProfile\" } , \"StaxS3Policy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:s3:::\" , { \"Ref\" : \"AWS::StackName\" }, \"/*\" ]] } }] }, \"PolicyName\" : \"BastionRoleStaxS3\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }, { \"Ref\" : \"BastionRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NatTakeoverPolicy\" : { \"Properties\" : { \"PolicyDocument\" : { \"Statement\" : [{ \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeRouteTables\" , \"ec2:CreateRoute\" , \"ec2:ReplaceRoute\" , \"ec2:StartInstances\" , \"ec2:StopInstances\" ], \"Resource\" : \"*\" }] }, \"PolicyName\" : \"NatRoleNatTakeover\" , \"Roles\" : [{ \"Ref\" : \"NATRole\" }] }, \"Type\" : \"AWS::IAM::Policy\" } , \"NAT1Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT1Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #1\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"NAT2Instance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSNATAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"NATInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"NAT2Instance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"SourceDestCheck\" : \"false\" , \"IamInstanceProfile\" : { \"Ref\" : \"NATProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet2\" } }, \"Metadata\" : { \"Comment1\" : \"Create NAT #2\" }, \"Type\" : \"AWS::EC2::Instance\" } , \"BastionInstance\" : { \"Properties\" : { \"ImageId\" : { \"Fn::FindInMap\" : [ \"AmazonLinuxAMI\" , { \"Ref\" : \"AWS::Region\" }, \"AMI\" ] }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ ... ]] } }, \"KeyName\" : { \"Ref\" : \"KeyName\" }, \"InstanceType\" : { \"Ref\" : \"BastionInstanceType\" }, \"Tags\" : [{ \"Key\" : \"Name\" , \"Value\" : { \"Fn::Join\" : [ \"-\" , [ \"BastionInstance\" , { \"Ref\" : \"AWS::StackName\" }]] } }], \"IamInstanceProfile\" : { \"Ref\" : \"BastionProfile\" }, \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\" } }, \"Type\" : \"AWS::EC2::Instance\" } posted on October 7, 2015 by David Dooling David Dooling", "date": "2015-10-07"},
{"website": "Monsanto", "title": "Cloud Different", "author": ["\n                                        David Dooling\n                                    "], "link": "http://engineering.monsanto.com/2015/10/06/cloud-different/", "abstract": "Over the past several years, cloud computing of all types has gained a\nlot of momentum. Because of the massive increase in mobile computing,\ni.e., phones and tablets, more and more people are consuming cloud\nservices like GMail, DropBox, Instagram, and Office365. These Software\nas a Service (SaaS) solutions are convenient for people on the go\nbecause the information they need is always easily accessible through\na web browser or mobile app. This is possible because the user’s data\nand preferences are stored “in the cloud”, using servers and services\nmanaged by someone else, typically the operator of the SaaS\nsolution. While these SaaS solutions are convenient for end users,\nthey do not allow for much customization. To develop custom\ncapabilities in the cloud, you must turn to Infrastructure as a\nService (IaaS). IaaS consists of virtualized compute, storage, and networking managed\nby someone else that you can consume, typically on a pay-per-use\nbasis. The automation enabled by consuming infrastructure via services\nallows you to spin up an entire data-center-like environment in less\nthan ten minutes! Many IaaS providers like Amazon Web Services,\nMicrosoft Azure, and Google Cloud Platform provide additional\ncapabilities like managed message queues, notifications, and even\ndatabases delivered as a service. Using these building blocks of IT,\nyou can rapidly create solutions for your customers without purchasing\na single piece of hardware. So what’s the catch? While self-service infrastructure and paying only\nfor what you use sounds great, typically cloud infrastructure is less\nreliable and over time becomes more expensive than on-site\nresources. To reap the benefits of IaaS, you must think differently\nabout your solution architecture. If you don’t, you’ll end up with a\nless reliable, more expensive solution. To avoid these pitfalls, when architecting solutions in the cloud, you\nmust design with the following five principles in mind. Automation : You must automate every aspect of your\ninfrastructure and solution, ensuring your solution can be spun up\nreliably, rapidly, and repeatably. Fault Tolerant : Since cloud infrastructure is less reliable,\nyou must create solutions that are fault tolerant, responding to\nand recovering from failures automatically. Horizontally Scalable : Your solution must be horizontally\nscalable so it can scale up to meet demand and scale back down to\ncontrol costs. Secure : You must secure every aspect of your solution\nenvironment. Security is your job, not someone else’s! Cost Effective : You have to use commodity, cost-effective\ncomponents so that executing on the first four principles does not\nbreak the bank. Done properly, cloud solutions can be more reliable and less costly. Just as how you architect solutions in the cloud must change,\ncontinuing to use the technology and processes that have been\noptimized for internal operations is not going to lead to success in\nthe cloud.  At Monsanto, when we asked ourselves how we could be\nsuccessful in the cloud, we knew our people would have to adopt new\ntechnologies and develop more agile processes. We would have to\ntransition from different teams managing different part of a custom\napplications life cycle, i.e., one team for development, another for\ntesting, another for operation, and yet another for support, we should\nhave teams operating in a DevOps model, owning a capability from\ncradle to grave and providing it as a service to their customers. The cloud presents an amazing opportunity to increase the speed with\nwhich companies can deliver capabilities to their customers. To\ncapitalize on the opportunity, you must embrace the cloud’s\ndifferences and use them to your advantage.  The architectural\nprinciples outlined above coupled with more agile process and a\nholistic DevOps model allow teams to create solutions that are more\nsecure, more reliable, more performant, and more cost effective than\non-premises solutions.  In addition, your teams will be able to\ndeliver solutions that delight your users much more quickly than\nbefore. To learn more about how to architect “cloud first” solutions, learn\nabout our journey to the cloud, and see how we use some of the tools\nand libraries we have open sourced, be sure to attend our talk at AWS\nre:Invent, Cloud First: New Architecture for New Infrastructure , this\nThursday 8 October 2015 at 2:45 p.m. PT in Palazzo N. posted on October 6, 2015 by David Dooling ← Previous Post Next Post → posted on October 6, 2015 by David Dooling David Dooling", "date": "2015-10-06"},
{"website": "Monsanto", "title": "Better Spray metrics with Kamon", "author": ["\n                                        Daniel Solano Gómez\n                                    "], "link": "http://engineering.monsanto.com/2015/09/24/better-spray-metrics-with-kamon/", "abstract": "At Monsanto, we have adopted Kamon for monitoring our microservices\nimplemented in Scala with Spray .  Kamon provides an integration that will automatically instrument our services to\ngenerate traces for each incoming request.  This is great, but we wanted more.\nSome of the things we wanted to improve included: Providing better response metrics Detecting requests that time out Reporting Spray can server statistic through Kamon To accomplish this, we have created and open sourced the spray-kamon-metrics library.  This library contains two independent pieces\nof functionality: TracingHttpService , a drop-in replacement for spray-routing’s HttpService class. TracingHttpService provides better trace metrics\nand handles timed out requests. KamonHttp , a drop-in replacement for spray-can’s Http Akka I/O\nextension.  It will transparently collect Spray can’s server metrics. For the rest of the post, we will explore in greater detail what the library\ndoes and how it works.  Finally, we will wrap up by presenting some ideas for\nfuture development.  If you like, you can visit the project’s page on\nGitHub for more details about how to integrate the\nlibrary into your application. Improving service metrics with TracingHttpService The TracingHttpService fulfils the first two of our goals: Providing better response metrics Detection and tracing of request timeouts Providing better response metrics Kamon’s Spray integration is immensely useful.  However, we felt like the\ndefault behaviour makes it difficult to really understand the application that\nis being measured.  In particular: It creates traces for each response, but they all have a default name of UnnamedTrace .  The intention is for application developers to give\nmeaningful names to each response.  However, it would be nice if the library\nprovided a more meaningful default. There are metrics collected under the http-server category, but they only\ncontain the trace name and resulting status code, and it is not easily to\ncorrelate the http-server metrics with corresponding traces, especially if\nwe have not given the traces meaningful names. While we could have resolved these issues to some extent by providing a name\ngenerator , the core problem was that, even with more meaningful names,\nthere is no way to add tags to a trace that has already been established.  As a\nresult, the dimensionality of the metrics that are produced are restricted to\ntrace name and response status code.  We want more, including: What was the method of the request, e.g. GET or POST ? What was the path of the request? Did the request time out? At first we considered modifying the kamon-spray library, but it works by using\nAspectJ.  That’s great because it means we can use it without making any\nchanges to our application.  Unfortunately, it also means that in order to be\nable to use it, you need to have knowledge of both AspectJ and a very deep\nunderstanding of the code you are trying to instrument (in this case, Spray\nrouting).  We had neither, so we opted to try something else.  However, in the\nlong run, moving spray-kamon-metrics’ functionality into kamon-spray seems like\na good idea. Next, we looked to see if we could just create a new directive that captures\nthe information we wanted.  Thus, we could theoretically just do something like: class ServiceActor extends HttpServiceActor { override def receive = runRoute { withKamonMetrics { serviceRoute } } } This works most of the time, but if the request results in an error or a\nrejection, it fails.  The reason for this is that in the case of rejections and\nerrors not handled explicitly by the route, the route does not produce the\nresulting HttpResponse .  As shown in figure 1 , when you use runRoute , it seals your route with implicitly given rejection and exception\nhandlers.  It is these handlers that actually generate the HttpResponse object that is sent to the client. Figure 1 : How HttpService handles rejections and exceptions While it is possible to provide custom handlers, it becomes very difficult to\nmanage state (in particular, start time) across all of these different places.\nWe could make the directive itself provide rejection and exception handling,\nbut that departs from the norm and also does not solve the problem with\nmanaging state. In the end, what we decided to do is to replace HttpService with TracingHttpService , which is largely identical to HttpService , the biggest\ndifference being in how it seals routes: // from HttpService def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) // from TracingHttpService (simplified, no timeout handling included) def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext = r val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( duration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } As we can see, HttpService.sealRoute is simply a higher order function that wraps\na route with exception and rejection handlers.  In the case of TracingHttpService , sealRoute just adds another wrap to the mix.  It still\nwraps the route with the handlers, but it adds its own wrapper around that.\nBefore the internally sealed route runs, it records the start time and starts\nbuilding a set of tags.  Once the internal route completes, it records the\ntiming to a Kamon histogram. Because we include these various tags, the metrics we collect are now much\nricher.  It is now possible to filter and analyse the metrics based on the\ntags, allowing us to answer questions such as ‘which types of request\n(method/path) are resulting in errors?’ and ‘What is the average response time\nfor a particular type of request?’. Additionally, creating our own version of HttpService allowed us to tackle\nthe next issue we had:  How do we know when a request times out? Detecting requests that time out Due to the asynchronous nature of the Spray server, the way it handles request\ntimeouts may be surprising to newcomers. Figure 2 : How Spray times out routes Figure 2 presents an overview of how Spray works (in particular, the\ntimeout route itself can time out, resulting in an invocation of a last ditch\ntimed out timeout route).  In particular there are couple of things to note: A route will continue running until it completes, regardless of how long it\nis taking.  This may have an impact performance and resource utilisation.\nUnfortunately, Spray does not include any sort of mechanism for cooperative\ncancellation. Spray invokes the timeout handler via a different mechanism than a standard\nrequest. As a result of this, the instrumentation built into kamon-spray is completely\nblind to the timeout mechanism.  It will record a requested that timed out as\nif it completed normally, and it will not generate a trace.  In fact, that is a\nreason for EmptyTraceContext present while closing the trace with token showing up in your logs. We want to measure both cases: If a request times out, we want to make a note of it so we can see which\nrequests are timing out and with what status code (is the timeout too short\nfor what we need to do, or is hanging due to an error?). We also want to know about the timeout responses, as they should be\naggregated to response time and error count metrics. To help us account for timeouts, we modify our sealRoute implementation: def sealRoute ( route : Route , timeoutNanos : Long , isTimeout : Boolean ) ( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext => val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString val timedOut = duration > timeoutNanos tagBuilder += \"timed-out\" -> timedOut . toString val realDuration = if ( isTimeout ) duration + timeoutNanos else duration Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( realDuration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } A couple of notes about this method: This method is called both when handling the regular route and when handling\nthe timeout route.  The isTimeout flag lets us know which if the two we\nare handling. We do not know for a fact that a non-timeout response timed out.  We use the\nheuristic that if the duration is greater than the request timeout, the\nrequest probably timed out. Measuring an accurate duration for a timeout route is similarly tricky.  The\nduration we calculate is only for the timeout route and does not include\nthe time that elapsed between when the request arrived and the timeout route\nwas invoked.  As an approximation, we simply add the configured request\ntimeout length to the measured duration. With this code in place, now our TracingHttpService implementation will\nnow measure: Durations for all regular and timeout responses For each response: The request method and path The response status code Did the response time out? This data gives us a much better picture of what clients are experiencing and\nhelps us identify problematic routes within our services.  Now, we just want to\nknow a little more about the metrics the Spray can server itself is collecting. Getting Spray can server metrics with KamonHttp The Spray can server automatically collects some statistic about its operation\nand provides a method for retrieving them .  Hooking up these\nmetrics to Kamon  essentially requires two steps: When a server socket is established, set up a job that periodically asks the\nserver for its latest statistics. Each time those statistics are collected, update a Kamon entity with their\nvalues. This is not difficult to do, but neither is it a one-liner that can be\ntrivially done.  Arguably, the trickiest bit is setting up the job that will\nmonitor the server.  To review, you set up a new server by sending an Http.Bind message the Spray can’s Http Akka I/O extension. IO ( Http ) ! Http . Bind ( myServiceActor , interface = \"localhost\" , port = 80 ) The tricky bit is that we need to capture the reference to the actor that\nreplies to this message, which the application may never do.  Furthermore, if\nthe application does want to get a reference to the responder, the library\nshould not interfere with that.  What is the solution? Proxying Spray The solution we settled on was to create a new Akka I/O extension that proxies\nthe Spray extension, which sounds much more complex than it actually is.  To do\nthis,  there is a little bit of boilerplate to ensure that Akka will find the\nextension, but, beyond that, an Akka extension is just an actor.  For KamonHttp , this actor is called SprayProxy , a slightly simplified version\nof which is listed below: class SprayProxy extends Actor { import constext.system private val ioActor = IO ( Http ) override def receive = { case x : Http.Bind => val proxied = sender () val monitor = context . actorOf ( SprayMonitor . props ( proxied )) ioActor . tell ( x , monitor ) case x => ioActor . forward ( x ) } } This class instantiates an instance of Spray’s extension and stores it in ioActor .  From that point forward, this actor does one of two things: If a Http.Bind message arrives, instantiate a SprayMonitor actor, which\nwe will cover next.  As part of this, we pass in a reference to the actor\nthat sent the original message.  Finally, we send the Http.Bind message to\nSpray, but do so in a matter so that Spray believes that the monitor actor\nwe just created was the original sender. For any other message, we simply forward it to Spray without changing the\nsender, rendering our extension invisible. Monitoring Spray Next, let us review what this Spray monitor does.  This actor is slightly more\ncomplex.  It exists in one of two states: The initial state is binding , which means we have sent the Http.Bind message to Spray and we are waiting to hear the result of the operation. In the bound state, the server is up and running and we periodically poll\nit for its statistics. These two states exist as methods on the actor, each of which returns a Receive .  Let’s take a look at binding first. def bindind : Receive = { case x : Http.CommandFailed => proxied . forward ( x ) context . stop ( self ) case x @ Http . Bound ( address ) => proxied . forward ( x ) context . become ( bound ( address )) } In this state, we have handle two possible messages: Http.Bound and Http.CommandFailed .  These indicate whether Spray succeeded in binding a new\nserver.  In both cases, we forward the message to the original sender of the Http.Bind message, rendering our proxy effectively invisible.  In the case\nwere the bind fails, we simply shut down.  In the case where the bind\nsucceeded, we become into the bound state, which we will examine next. def bound ( address : InetSocketAddress ) : Receive = { import context.dispatcher val httpListener = sender () context . watch ( httpListener ) val updateTask = context . system . scheduler . schedule ( 15 seconds , 15 seconds , httpListener , Http . Getstats ) val metricsName = s \"${address.getHostName}:${address.getPort}\" val metrics = Kamon . metrics . entity ( SprayServerMetrics , metricsName ) { case _: Terminated => updateTask . cancel () Kamon . metrics . removeEntity ( metricsName , SprayServerMetrics . category ) context . stop ( self ) case s : Stats => metrics . updateStats ( s ) } } When we become bound , a few things take place: We capture the reference to the sender of the Http.Bound message.  This is\nthe HTTP listener actor which handles all connections to that particular\nserver. We start watching the listener.  When it dies, that means the server has\ndied, so we should stop monitoring and shut down. We schedule a task that will send the listener a Http.GetStats every 15\nseconds (this is configurable in the real code).  Keep in mind that when we\ncreate this task, it uses the self implicit value as the sender, meaning\nthat as far as the listener is concerned, it is the monitor that is sending\nthese messages. We instantiate a Kamon entity we created specifically for recording the\nSpray server metrics.  Its name is generated from the host name and port\nwhere the server is listening. Finally we have the partial function that handles the two types of messages\nthat our actor will receive from this point forward: When we get a Terminated message, that means the server has stopped.\nAs a result, we clean up by cancelling the recurring task, removing the\nKamon entity, and finally stopping ourselves. In the case where we get new Stats , we update the Kamon entity with\nthe new values. All in all, this code is relatively straightforward.  However, we are not quite\ndone, yet.  Updating the Kamon entity’s metrics was not as straightforward as\nwe initially thought. Reporting the statistics to Kamon It might seem like having dealt with all of the Akka bits, we would be out of\nthe woods.  However, it turned out that what Spray is reporting is not entirely\nin line with how Kamon’s instruments behave.  Spray reports the following\nstatistics: connections , the total number of connections over time open-connections , the number of currently open connections max-open-connections , the maximum number of open connections ever requests , the total number of requests over time open-requests , the number of currently open requests max-open-requests , the maximum number of open requests ever request-timeouts , the total number of request timeouts over time uptime , the current uptime of the server, in nanoseconds Most of these fit nicely into one of Kamon’s instrument types: connections , requests , and request-timeouts all work will as Kamon\ncounters, which must be strictly increasing, and are reported as time-series\ndata open-connections and open-requests are conceptually gauges, but given\nthat we are already dealing with sampled data, we decide to map these to\nKamon histograms. This leaves: We could have treated max-open-connections and max-open-requests like\ntheir non-max counterparts, but given that these values rarely change, we\nreally only want to report the latest value. uptime , which is conceptually a type of counter, but we do not want that to\nbe reported as time-series data, i.e.  the server was up 15 seconds in the\nlast 15 seconds.  We really want to report the latest value. So how, did we deal with this?  Well, for the most part, we just defined our\nown entity recorder .  However, it does contains a couple of twists\nworth sharing: class SprayServerMetrics ( instrumentFactory : InstrumentFactory ) extends GenericEntityRecorder ( instrumentFactory ) { private val stats = new AtomicReference [ Stats ]( new Stats ( 0. nanoseconds , 0 , 0 , 0 , 0 , 0 , 0 , 0 )) private val connections = counter ( \"connections\" ) private val openConnections = histogram ( \"open-connections\" ) private val requests = counter ( \"requests\" ) private val openRequests = histogram ( \"open-requests\" ) private val requestTimeouts = counter ( \"request-timeouts\" ) override def collect ( collectionContext : CollectionContext ) : EntitySnapshot def updateStats ( newStats : Stats ) : Unit } We keep a copy of the most recent statistics, starting with a value of all\nzeroes.  Next, for the metrics that map nicely to Kamon’s instruments, we do\nthat. override def collect ( collectionContext : CollectionContext ) : EntitySnapshot = { val parentSnapshot = super . collect ( collectionContext ) val metrics = parentSnapshot . metrics ++ Map ( counterKey ( \"uptime\" , Time . Nanoseconds ) → CounterSnapshot ( stats . uptime . toNanos ), counterKey ( \"max-open-connections\" ) → CounterSnapshot ( stats . maxOpenConnections ), counterKey ( \"max-open-requests\" ) → CounterSnapshot ( stats . maxOpenRequests ) ) new DefaultEntitySnapshot ( metrics ) } When it is time for Kamon to collect the values for our entity, that is were\nthings get a little tricky.  We need to do a combination of both the default\nbehaviour for the Kamon-friendly statistics along with some custom behaviour\nfor those that are not.  Luckily this was not too difficult: We invoke the parent class’s collect method to get a snapshot that\nincludes all of the Kamon-friendly statistics. From that snapshot, we can get the recorded instrument snapshot and append\nto it fabricated instrument snapshots with the values that we want.  We use\nreport uptime , max-open-connections , and max-open-requests as\ncounters. Finally, we construct a new snapshot with our custom metrics. Last, we just need to be sure to deal with the updates: def updateStats ( newStats : Stats ) : Unit = { openConnections . record ( newStats . openConnections ) openRequests . record ( newStats . openRequests ) connections . increment ( newStats . totalConnections - stats . totalConnections ) requests . increment ( newStats . totalRequests - stats . totalRequests ) requestTimeouts . increment ( newStats . requestTimeouts - stats . requestTimeouts ) stats = newStats } The implementation here should not come as a surprise. Both open-connections and open-requests are histograms, so we just\nrecord their new values. For connections , requests , and request-timeouts , we simply record how\nmuch each of these has increased since the last time. Finally, we keep a copy of the statistics.  Note that we do not have to do\nanything for the other values as those metrics are generated during collect . And that is how we take the statistics from Spray to report them from a custom\nKamon entity recorder. Moving forward We are generally pretty happy with our work in spray-kamon-metrics, but that is\nnot to say there is not room for improvement.  A few of ideas come to mind: Figure out if there is a better way to handle request timeouts. See if it is possible to implement any of this using AspectJ, making it as\nsimple to use as kamon-spray. Especially if we can acheive the last goal, perhaps it would be good to\nmerge this into kamon-spray itself. Can you think of anything else you like the library to do? posted on September 24, 2015 by Daniel Solano Gómez ← Previous Post Next Post → class ServiceActor extends HttpServiceActor { override def receive = runRoute { withKamonMetrics { serviceRoute } } } class ServiceActor extends HttpServiceActor { override def receive = runRoute { withKamonMetrics { serviceRoute } } } // from HttpService def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) // from TracingHttpService (simplified, no timeout handling included) def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext = r val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( duration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } // from HttpService def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) // from TracingHttpService (simplified, no timeout handling included) def sealRoute ( route : Route )( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext = r val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( duration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } def sealRoute ( route : Route , timeoutNanos : Long , isTimeout : Boolean ) ( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext => val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString val timedOut = duration > timeoutNanos tagBuilder += \"timed-out\" -> timedOut . toString val realDuration = if ( isTimeout ) duration + timeoutNanos else duration Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( realDuration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } def sealRoute ( route : Route , timeoutNanos : Long , isTimeout : Boolean ) ( implicit eh : ExceptionHandler , rh : RejectionHandler ) : Route = { mapRequestContext { ctx : RequestContext => val path = ctx . request . uri . path . toString () val method = ctx . request . method . name val start = System . nanoTime () val tagBuilder = Map . newBuilder [ String , String ] tagBuilder += \"path\" -> path tagBuilder += \"method\" -> method ctx . withHttpResponseMapped { response => val duration = System . nanoTime () - start tagBuilder += \"status-code\" -> response . status . intValue . toString val timedOut = duration > timeoutNanos tagBuilder += \"timed-out\" -> timedOut . toString val realDuration = if ( isTimeout ) duration + timeoutNanos else duration Kamon . metrics . histogram ( \"spray-service-response-duration\" , tagBuilder . result (), Time . Nanoseconds ) . record ( realDuration ) response } } { ( handleExceptions ( eh ) & handleRejections ( sealRejectionHandler ( rh )))( route ) } } IO ( Http ) ! Http . Bind ( myServiceActor , interface = \"localhost\" , port = 80 ) IO ( Http ) ! Http . Bind ( myServiceActor , interface = \"localhost\" , port = 80 ) class SprayProxy extends Actor { import constext.system private val ioActor = IO ( Http ) override def receive = { case x : Http.Bind => val proxied = sender () val monitor = context . actorOf ( SprayMonitor . props ( proxied )) ioActor . tell ( x , monitor ) case x => ioActor . forward ( x ) } } class SprayProxy extends Actor { import constext.system private val ioActor = IO ( Http ) override def receive = { case x : Http.Bind => val proxied = sender () val monitor = context . actorOf ( SprayMonitor . props ( proxied )) ioActor . tell ( x , monitor ) case x => ioActor . forward ( x ) } } def bindind : Receive = { case x : Http.CommandFailed => proxied . forward ( x ) context . stop ( self ) case x @ Http . Bound ( address ) => proxied . forward ( x ) context . become ( bound ( address )) } def bindind : Receive = { case x : Http.CommandFailed => proxied . forward ( x ) context . stop ( self ) case x @ Http . Bound ( address ) => proxied . forward ( x ) context . become ( bound ( address )) } def bound ( address : InetSocketAddress ) : Receive = { import context.dispatcher val httpListener = sender () context . watch ( httpListener ) val updateTask = context . system . scheduler . schedule ( 15 seconds , 15 seconds , httpListener , Http . Getstats ) val metricsName = s \"${address.getHostName}:${address.getPort}\" val metrics = Kamon . metrics . entity ( SprayServerMetrics , metricsName ) { case _: Terminated => updateTask . cancel () Kamon . metrics . removeEntity ( metricsName , SprayServerMetrics . category ) context . stop ( self ) case s : Stats => metrics . updateStats ( s ) } } def bound ( address : InetSocketAddress ) : Receive = { import context.dispatcher val httpListener = sender () context . watch ( httpListener ) val updateTask = context . system . scheduler . schedule ( 15 seconds , 15 seconds , httpListener , Http . Getstats ) val metricsName = s \"${address.getHostName}:${address.getPort}\" val metrics = Kamon . metrics . entity ( SprayServerMetrics , metricsName ) { case _: Terminated => updateTask . cancel () Kamon . metrics . removeEntity ( metricsName , SprayServerMetrics . category ) context . stop ( self ) case s : Stats => metrics . updateStats ( s ) } } class SprayServerMetrics ( instrumentFactory : InstrumentFactory ) extends GenericEntityRecorder ( instrumentFactory ) { private val stats = new AtomicReference [ Stats ]( new Stats ( 0. nanoseconds , 0 , 0 , 0 , 0 , 0 , 0 , 0 )) private val connections = counter ( \"connections\" ) private val openConnections = histogram ( \"open-connections\" ) private val requests = counter ( \"requests\" ) private val openRequests = histogram ( \"open-requests\" ) private val requestTimeouts = counter ( \"request-timeouts\" ) override def collect ( collectionContext : CollectionContext ) : EntitySnapshot def updateStats ( newStats : Stats ) : Unit } class SprayServerMetrics ( instrumentFactory : InstrumentFactory ) extends GenericEntityRecorder ( instrumentFactory ) { private val stats = new AtomicReference [ Stats ]( new Stats ( 0. nanoseconds , 0 , 0 , 0 , 0 , 0 , 0 , 0 )) private val connections = counter ( \"connections\" ) private val openConnections = histogram ( \"open-connections\" ) private val requests = counter ( \"requests\" ) private val openRequests = histogram ( \"open-requests\" ) private val requestTimeouts = counter ( \"request-timeouts\" ) override def collect ( collectionContext : CollectionContext ) : EntitySnapshot def updateStats ( newStats : Stats ) : Unit } override def collect ( collectionContext : CollectionContext ) : EntitySnapshot = { val parentSnapshot = super . collect ( collectionContext ) val metrics = parentSnapshot . metrics ++ Map ( counterKey ( \"uptime\" , Time . Nanoseconds ) → CounterSnapshot ( stats . uptime . toNanos ), counterKey ( \"max-open-connections\" ) → CounterSnapshot ( stats . maxOpenConnections ), counterKey ( \"max-open-requests\" ) → CounterSnapshot ( stats . maxOpenRequests ) ) new DefaultEntitySnapshot ( metrics ) } override def collect ( collectionContext : CollectionContext ) : EntitySnapshot = { val parentSnapshot = super . collect ( collectionContext ) val metrics = parentSnapshot . metrics ++ Map ( counterKey ( \"uptime\" , Time . Nanoseconds ) → CounterSnapshot ( stats . uptime . toNanos ), counterKey ( \"max-open-connections\" ) → CounterSnapshot ( stats . maxOpenConnections ), counterKey ( \"max-open-requests\" ) → CounterSnapshot ( stats . maxOpenRequests ) ) new DefaultEntitySnapshot ( metrics ) } def updateStats ( newStats : Stats ) : Unit = { openConnections . record ( newStats . openConnections ) openRequests . record ( newStats . openRequests ) connections . increment ( newStats . totalConnections - stats . totalConnections ) requests . increment ( newStats . totalRequests - stats . totalRequests ) requestTimeouts . increment ( newStats . requestTimeouts - stats . requestTimeouts ) stats = newStats } def updateStats ( newStats : Stats ) : Unit = { openConnections . record ( newStats . openConnections ) openRequests . record ( newStats . openRequests ) connections . increment ( newStats . totalConnections - stats . totalConnections ) requests . increment ( newStats . totalRequests - stats . totalRequests ) requestTimeouts . increment ( newStats . requestTimeouts - stats . requestTimeouts ) stats = newStats } posted on September 24, 2015 by Daniel Solano Gómez Daniel Solano Gómez", "date": "2015-09-24"},
{"website": "Monsanto", "title": "Learn implicits: Type classes", "author": ["\n                                        Jorge Montero\n                                    ", "\n                                        Jessica Kerr\n                                    "], "link": "http://engineering.monsanto.com/2015/09/23/implicits-typeclasses/", "abstract": "In this series on Scala implicits, we have seen \n[some](/2015/05/14/implicits-intro/, \n[everyday](/2015/06/15/implicits-futures/, \n[uses](/2015/07/31/implicit-conversions/ of implicits. \nOne crucial pattern combines these techniques: Type classes. Caution : please excuse the name. “Type classes” resembles neither types nor classes in a way useful for understanding.\n The pattern is called “type classes” for historical reasons. Type classes extend functionality of classes without actually changing them, and with full type safety.\nThis pattern is often used to add functionality to classes that we do not control. It’s also useful for cross-cutting concerns.\n Serialization is a cross-cutting concern, and spray-json provides a practical example of the type class pattern.\n Let’s see how it implements JSON serialization/deserialization for built-in classes. The documentation says that any object can have the .toJson method \nif we add a couple of imports. import spray.json._\nimport DefaultJsonProtocol._ The two imports add some implicits to the compiler’s magic hat. import spray.json._ brings in everything in the spray json package object , including: implicit def pimpAny[T](any: T) = new PimpedAny (any) \nprivate[json] class PimpedAny[T] (any: T) {\n    def toJson (implicit writer: JsonWriter[T]): JsValue = writer.write(any)\n} After the last few articles, we are ready for this. The first line is a view that turns anything\n into a PimpedAny . Now every object in the world implements toJson .\n We [warned](/2015/07/31/implicit-conversions/ against this kind of breadth in views,\n but here we are safe from surprises: the only way to activate this view is by calling .toJson . The intermediate class is invisible; you never even have to see its terrible name. The useful type is the return value of .toJson . Calling toJson transforms an object into a JsValue , a representation of JSON data.\n Two methods on JsValue , prettyPrint and compactPrint, return a String we can transmit or save. Can we now serialize every single object, just like that? No. Not that easy. Here’s the declaration again: def toJson ( implicit writer: JsonWriter[T] ): JsValue This toJson method takes an implicit parameter , a JsonWriter of T.\nSo for any type T we want to convert to Json, there must be a JsonWriter[T], \nand it must be in the magic hat, in scope where toJson is called. What is a JsonWriter[T], and where would the compiler find one? JsonWriter is a trait with a single method, write. trait JsonWriter[T] {\n  def write(obj: T): JsValue } spray-json defines this trait, along with JsonReader for deserialization, and JsonFormat for both together. JsonFormat is the one we create, typically.\nSpray-json has built-in JsonFormat implementations for many common types; these lurk in DefaultJsonProtocol. \nWe bring all of them into implicit scope when we import DefaultJsonProtocol._ . It’s those JsonFormats that know how to serialize\nand deserialize JSON. For instance, there is an implicit JsonFormat[String]. In type class parlance, “There is an instance of the JsonFormat type class for String.” We can use it like this: import spray.json._\nimport DefaultJsonProtocol._ \nval pony = \"Fluttershy\"\nval json = pony. toJson The implicits resolve to: val pony = \"Fluttershy\"\nval json = new PimpedAny \\[String\\](pony). toJson ( DefaultJsonProtocol.StringJsonFormat ) This desugared syntax looks like serialization in a language without implicits. This use of the type class pattern adds a whole feature (serialization) to any class we want, in a generic way,\nwithout changing the classes. All the usual types (String, Int, Seq, Map, Option, etc) have serialization code in DefaultJsonFormat. For our own class T, we make .toJson work when we define an implicit val of type JsonFormat[T] .\nThis is called “providing an instance of the JsonFormat type class for T.” We can write these by hand, or use helper methods from spray-json.\nThere’s even a project that makes the compiler generate it all for case classes;\n the details are way outside the scope of this post. Here’s the kicker: when we make a JsonFormat[MyClass], we get more than serialization/deserialization for MyClass.\nWe can now call toJson on MyClass, Seq[MyClass], on Map[String,MyClass], on Option[Map[MyClass,List[MyClass]]] – without writing any extra code! This is the killer feature of the type class pattern: it composes.\nOne generic definition of JsonFormat[List[T]] means a List of any JsonFormat-able T is also JsonFormat-able. T could be String, Int, Long, MyClass – you name it, if we can format it, we can also format Lists of it.\nHere’s the trick: instead of an implicit val for JsonFormat of List, there is an implicit def in DefaultJsonFormat: implicit def listFormat[ T : JsonFormat ] = new RootJsonFormat[List[T]] {\n      def write(list: List[T]) = ..\n      def read(value: JsValue ): List[T] = ..\n    } What is this doing? First, we have to understand some new syntax: inside the type parameter, there is a colon, followed by a type class . \nThis is called Context Bounds (good luck finding the documentation without knowing this special name).\nThis is shorthand for “ a type T such that there exists in the magic hat a JsonFormat[T] ”.\nThe context-bounds notation above expands to: implicit def listFormat[ T ]( implicit _ : JsonFormat[T] ) = new RootJsonFormat[List[T]] {\n      def write(list: List[T]) = ..\n      def read(value: JsValue ): List[T] = ..\n    } The implicit parameter ensures that the write function inside listFormat will be able to call .toJson on the elements in the List. This implicit def does not work the same way as a [view](/2015/07/31/implicit-conversions/, which converts a single type to another.\nInstead, it is a supplier of implicit values. It can give the compiler a JsonFormat[List[T]],as long as the compiler supplies a JsonFormat[T]. One definition composes with any other JsonFormats in the magic hat. \nThe compiler calls as many of these implicit functions, as many times as needed, to produce the implicit parameter it desperately desires. This works on types as complicated as we want. Let’s say we want to serialize an Option[Map[String,List[Int]]]: import spray.json._\nimport DefaultJsonProtocol._\nval a:Option[Map[String,List[Int]]] = Some(Map(\"Applejack\" -> List(1,2,3,4),\n                                               \"Fluttershy\" -> List(2,4,6,8))) \nval json = a. toJson println(json.prettyPrint) The compiler uses implicit functions for Option, Map, and List, along with implicit vals for String and Int,\nto compose a JsonFormat[Option[Map[String,List[Int]]]]. \nThat gets passed into .toJson , and only then does serialization occur.\nIf we use the JsonFormats explicitly, the code above becomes: import spray.json._\nimport spray.json.{DefaultJsonProtocol => Djp}\nval a:Option[Map[String,List[Int]]] = Some(Map(\"Applejack\" -> List(1,2,3,4),\n                                               \"Fluttershy\" -> List(2,4,6,8)))\nval json = new PimpedAny\\[Option[Map[String,List[Int]]]](a). toJson (Djp.optionFormat(\n            Djp.mapFormat(\n                Djp.StringJsonFormat,\n                Djp.listFormat(\n                    Djp.IntJsonFormat\n                )\n            )))\nprintln(json.prettyPrint) Whew, that’s a lot of magic. The compiler does all that composition for us. This property of implicit parameters makes the type class pattern very useful. That much magic also means it’s hard to understand. \n While you’ll rarely need to create your own types in the style of JsonFormat,\nyou’ll often want to create new type class instances, such as JsonFormat[MyClass]. Other times you need to find the right ones to import.\nEither way, familiarity with the pattern is essential when using spray-json and many other libraries. Spray-routing, a library for writing RESTful services, uses this pattern for a lot of things, including returning data, and to avoid some pitfalls of method overloading.\nThey call it the ‘magnet pattern’ and try to get you to read a post much, much longer than this one .\nUltimately it’s the same pattern, used for different properties. In some ways, the type class pattern is the culmination of Scala’s implicit feature. If this post makes sense, then you’re well on your way to Scala mastery. posted on September 23, 2015 by Jorge Montero Jessica Kerr ← Previous Post Next Post → posted on September 23, 2015 by Jorge Montero Jessica Kerr Jorge Montero Jessica Kerr", "date": "2015-09-23"},
{"website": "Monsanto", "title": "Cluster Lightly", "author": ["\n                                        Caleb Courier\n                                    "], "link": "http://engineering.monsanto.com/2015/09/21/leaflet-light-cluster/", "abstract": "As the amount of available technology increases so does the amount of available data.  Of this newly available data,\nmuch of it is concerned with two major questions: When? and Where? The ability to associate data throughout time and space has allowed the development of applications that can make\ninformed decisions based on someone’s position at a particular time.\nThese machine-made suggestions can be as commonplace as what route to take home during rush hour from applications such\nas Waze or Google Maps, or as seemingly complex as deciding where to get brunch by typing something ridiculous like\n“open breakfast diners with really big omelets near me” into a search engine on your smartphone. Machine made decisions\nlike this are made possible by the growing amount of geospatial data, i.e., information that is related to a specific\nlocation.  The problem that we face now is not where to get the data or even how to store it necessarily, but rather\nhow to visually represent this large amount of data in a meaningful way.  This is where clustering comes into play. Clustering geospatial data is not a new concept. It has been used in GIS related fields for centuries for reasons\nsuch as Dr. John Snow’s method to track cholera outbreaks in the mid 1800’s to modern day classification algorithms\nused in image analysis.  However, as developers, one of the challenges we face is when and how to actually cluster the\ndata we have.  Plenty of libraries and modules exist that will take your data into a black box and spit out some\nclustered objects, and these work phenomenally under certain conditions.  For example, if you don’t have a terrible\namount of data, say just a few thousand objects, or if you don’t mind handing your data over to a third party via a\nwebservice then these are great options.  On the other hand, if you’re dealing with millions of objects or points you\nprobably won’t be able to, and won’t want to, do heavy processing with them on the client.  This is where LightCluster\ncomes in. In short, LightCluster is an extension for the Leaflet.js mapping library that allows you to use a server side\nclustering solution for geospatial data.  Unlike other javascript clustering libraries, LightCluster doesn’t want all\nthe data up front in order to run clustering algorithms on the client; instead it expects a minimized cluster object\nconsisting of a lat-long pair to know where to place it, a bounding box to show the area of the data it represents, and\na count of how many objects it’s representing.  You may be thinking “Hold on now, what exactly does it do then?”. The\nanswer to that is: Whatever you tell it to.  Upon initialization this simple extension will take in an anonymous\nfunction which you define for updating the data and, depending on the options you pass in, it will use your update\nfunction when handling all of the click, drag, and zoom events on the map.  It also keeps track of how many\npoints/objects are in the cluster and will reduce the cluster to a standard leaflet marker if the count is only one.\nBecause of this simplification on the client you can define and execute all the intense processing on the server side\nwhere it should be.  Granted, this requires you to develop your own clustering solution in order to generate these\nlight weight cluster objects.  The good news is it’s not as hard as it sounds.  One extremely simple solution is to use geohash .  Since geohash already represents points/areas in a\nunique way that can be directly related at various amounts of precision, most of the work of correlating your data into\nrelated geospatial regions is already done.  Since this method is so simple it’s actually what I used to generate the\nsample data used in the demo project . Of course if you would\nrather use something more traditional or statistical feel free.  Some of the more commonly used clustering algorithms\ninclude DBSCAN and K-means both of which you can find code implementations with a simple Google search. No matter what\nyou do, LightCluster will take it and perform its simple task of rendering and updating keeping your client code light\nand your map performant. posted on September 21, 2015 by Caleb Courier ← Previous Post Next Post → posted on September 21, 2015 by Caleb Courier Caleb Courier", "date": "2015-09-21"},
{"website": "Monsanto", "title": "Cloud Foundry Metrics", "author": ["\n                                        Mark Seidenstricker\n                                    "], "link": "http://engineering.monsanto.com/2015/08/20/cf-metrics/", "abstract": "In the second installation of our open source [Cloud Foundry (CF) toolbox series](/2015/07/22/building-an-open-source-cloud-foundry-toolbox/ we would like to introduce CF-Metrics , a comprehensive solution for Cloud Foundry monitoring and alerting based solely on open source projects.  In a world of unreliable cloud infrastructure and distributed micro service architectures, monitoring and alerting are as critical now as they’ve ever been.  Even when you learn to expect failure (or better yet embrace it ) and build self healing platforms like Cloud Foundry, you still need monitoring and alerting to analyze why there was a failure and how to make it not happen again in the future. Lowering the Cost of Entry When we first started with Cloud Foundry at Monsanto, monitoring was one of the key areas the team needed to tackle to feel comfortable running the platform. Maybe it’s a placebo effect, but knowing I have the ability to graph any metric at will keeps me from loosing sleep at night.  And although this topic remains highly requested in meetups and mailing lists, publicized documentation and more importantly downloadable working implementations are still relatively sparse. By releasing CF-Metrics, we hope to provide a solution which will help new community users get off the ground quicker by: Being one of the quickest and most painless ways to get up and running with a functional CF monitoring solution Re-using existing components to eliminate the need to run custom plugins and/or forked releases The Setup CF-Metrics is a combination of open source tools (Heka, InfluxDB, and Grafana) running as a docker compose application.  Packaging the tools via docker makes it compact and highly portable to a variety of hosting solutions.  We purposefully elected not to package the application as a BOSH release since it’s best practice not to run your monitoring solution on the platform that it is monitoring.\n<img src=/img/cf-metrics-arch.png  width=700 height=400/> Gathering the metrics To gather metrics we are utilizing two components that have been a part of BOSH and Cloud Foundry for a long time time: CF collector and BOSH monitor.  CF collector is in charge of grabbing metrics from two built-in endpoints (/varz and /healthz) published by every job in the Cloud Foundry release.  By default it does this every thirty seconds and returns a wealth of statistics about the performance, utilization, and health of each job.  BOSH monitor talks to the BOSH agent installed on every VM in the releases it deploys and gathers OS related metrics such as CPU, swap, and disk utilization. These components have the capability to forward their consolidated metric streams to a number of 3rd party tools via configuration in their release ymls.  For the purposes of our project, we are choosing to forward BOSH and CF metrics via the common graphite protocol and BOSH events via the new BOSH monitor consul plugin. Processing To process the metric and event stream, we chose Mozilla’s open source stream processing system Heka .  Heka is lightweight and its lua sandbox functionality gives it the ability to perform a large variety of functions.  Once the metric and event data is sent to Heka, it is decoded from graphite to Heka format and then streamed through a variety of filters to accomplish our specific tasks. Alerting Alerting through Heka is done through configuration of sandbox filters.  There are filters which trigger on specific message types flowing through the system and perform logic to determine if an alert is required.  It then uses the built-in alert module to inject an alert message into the Heka stream and set a timer to throttle additional alerts of that type for the specified time.  Alert messages are then picked up by a separate sandbox and sent to a Heka output such as an email address or a slack channel. We’ve included some alert filters we find useful such as swap utilization and DEA memory ratios, but creating your own is as easy as creating a new Heka filter with your custom logic.  Heka also has built in anomaly detection modules so if you’re adventurous you can go beyond traditional threshold based alerting. Storing and Trending Once the metric and event data has flowed through the filters a heka encoder and output are used to dump the metrics we want to persist to a InfluxDB database.  This data is then accessible via Grafana to create ad-hoc graphs or custom dashboards like the one below Ready, Set, Go! For a deeper look at CF-Metrics head over to the github page to try it out for yourself.  If you make a feature enhancement or useful dashboard we’d love a PR to include it in the project.  Or if you have your own solution for Cloud Foundry monitoring and alerting, hopefully we’ve encouraged you to pay it forward and share it with the community. posted on August 20, 2015 by Mark Seidenstricker ← Previous Post Next Post → posted on August 20, 2015 by Mark Seidenstricker Mark Seidenstricker", "date": "2015-08-20"},
{"website": "Monsanto", "title": "Building a simple Spray application", "author": ["\n                                        Scott MacDonald\n                                    "], "link": "http://engineering.monsanto.com/2015/08/11/simple-spray/", "abstract": "You’ve probably heard people talk about Spray by now; maybe you’re\neven using it for JSON serialization.  Spray also provides a\nlightweight server package to allow you easily create REST services.\nThe hardest part of getting started with any new technology is getting\npast that initial contact of setup and concepts.  I’ll try to reduce\nthat hurdle by walking you through a simple app that hits most of the\nfeatures you might regularly use. Because the spray documentation isn’t great about saying what\ndependencies you need, or imports for that matter, I’ll be including\nthem so you don’t have to guess.  Also, the final source code is at simple-spray . Setting up the project’s build.sbt Your build.sbt are going to need the following dependencies val akka = \"2.3.9\" val spray = \"1.3.2\" resolvers += Resolver . url ( \"TypeSafe Ivy releases\" , url ( \"http://dl.bintray.com/typesafe/ivy-releases/\" ))( Resolver . ivyStylePatterns ) libraryDependencies ++= Seq ( // -- Logging -- \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\" , \"com.typesafe.scala-logging\" %% \"scala-logging-slf4j\" % \"2.1.2\" , // -- Akka -- \"com.typesafe.akka\" %% \"akka-testkit\" % akka % \"test\" , \"com.typesafe.akka\" %% \"akka-actor\" % akka , \"com.typesafe.akka\" %% \"akka-slf4j\" % akka , // -- Spray -- \"io.spray\" %% \"spray-routing\" % spray , \"io.spray\" %% \"spray-client\" % spray , \"io.spray\" %% \"spray-testkit\" % spray % \"test\" , // -- json -- \"io.spray\" %% \"spray-json\" % \"1.3.1\" , // -- config -- \"com.typesafe\" % \"config\" % \"1.2.1\" , // -- testing -- \"org.scalatest\" %% \"scalatest\" % \"2.2.1\" % \"test\" ) Creating a bare bones service Spray utilizes Akka Actors to receive the service calls, a pattern\nthat is used to setup the routes is to create Scala Traits.  So a\nframework would look like this: import akka.actor.Actor import spray.routing.HttpService class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } trait SampleRoute extends HttpService { val route = { get { complete ( \"I exist!\" ) } } } As you can see, the trait contains a route value that get’s passed\ninto the runRoute method.  You don’t have to do it this way, but using\ntraits can be a good way of organizing your routes. So we have a service setup, let’s actually run it and try it out.  We\nneed one more object, a Main class to bind to the port. import akka.actor. { ActorSystem , Props } import akka.io.IO import spray.can.Http object Main extends App { implicit val system = ActorSystem ( \"simple-service\" ) val service = system . actorOf ( Props [ SampleServiceActor ], \"simple-service\" ) //If we're on cloud foundry, get's the host/port from the env vars lazy val host = Option ( System . getenv ( \"VCAP_APP_HOST\" )). getOrElse ( \"localhost\" ) lazy val port = Option ( System . getenv ( \"VCAP_APP_PORT\" )). getOrElse ( \"8080\" ). toInt IO ( Http ) ! Http . Bind ( service , host , port = port ) } Our main class set’s up the Akka ActorSystem, and attaches our\nSampleServiceActor to it.  We then bind the service to the host and\nport (I put in code to show how to get it from Cloud Foundry). At this point in IntelliJ you should be able to run the Main class.\nIt should just take a second to launch, then in your browser you can\nlook at http://localhost:8080/ and see the message we returned. So now we have the basic framework up, let’s make it more interesting\nby…. Adding paths/routes So currently we’re just hitting the root of the service, let’s add a\nsubresource called stuff . val route = { get { complete ( \"I exist!\" ) } ~ get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } } The path operation allows us to name a route, and the tilde operator\nallows you to chain things together (it’s a common usage in the Spray\nlibraries).  In theory http://localhost:8080/stuff should show us our\nnew message.  But instead we still see the I exist! message. What’s going on is Spray goes down the list one by one until it finds\na complete .  Since the root has a get on it, it resolves that\nfirst.  By reordering we can fix this problem val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ get { complete ( \"I exist!\" ) } } Doing a POST So I’m sure you’ve noticed the get by now that signifies which verb\nwe want to use.  The other verbs (post/put/delete/etc) are also\nsupported. Let’s do a POST to the stuff resource. val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ post { path ( \"stuff\" ) { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } Obviously we’re not really getting any data (that will come in the\nnext section), but we can just do a POST and see this message.  There\nis a problem though, we’ve got some repetition since we do a path(“stuff”) twice. It turns out Spray is very flexible about how you nest things!  So we\ncan easily do some code reduction by putting the path first and have\nit contain the get and post . val route = { path ( \"stuff\" ) { get { complete ( \"That's my stuff!\" ) } ~ post { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } That’s better!  Note the tilde chaining the get & post together. Reading and returning JSON So we’re ready to deal with some data, so we’ll change our get and post to use JSON objects. For our purposes, the object is going to be really simple. I’ll also\ncreate the format to use spray.json ’s serialization. import spray.json.DefaultJsonProtocol case class Stuff ( id : Int , data : String ) object Stuff extends DefaultJsonProtocol { implicit val stuffFormat = jsonFormat2 ( Stuff . apply ) } Now we’ll have our get return some fake data.  I’ll list the whole trait again so there’s no confusion. trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes val route = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { complete ( \"stuff posted!\" ) } } } ~ get { complete ( \"I exist!\" ) } } } The complete call takes the case class stuff and automatically takes care of creating the JSON for it. Please note the function respondWithMediaType . You can use this to\ngo a different route if someone wants XML vs JSON.  I recommend that\nyou always put this on anything that returns JSON so if browser\ndemands that type, your service doesn’t erroneously return that it\ncan’t do that. Now that we’re returning data, let’s modify our post call to read\ndata.  We’ll return the data with 100 added to the id, and the text posted added to the data. post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } Note another nesting done here with the entity(as[Stuff]) which will\ndeserialize the JSON into an object variable provided as a parameter\nto the expression block. Routes with a depth of greater than 1 So you might decided to do a route junk/mine and junk/yours , and\nyour first thought would be to do path(“junk/mine”) .  While\nintuitive, that unfortunately will not work.  When you go into more\ncomplex paths, Spray utilizes the pathPrefix and pathEnd directives. pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } It gets very ‘nesty’ but allows you a lot of flexibility in creating\nyour routes.  There’s a great more you can do with this, but we’ll\nleave that for a future blog post or the Spray documentation.  We are trying to keep this simple. Getting query parameters At some point, you’re going to want to handle query parameters, both\nrequired and optional.  We’ll create a route named params that will\ntake in a required parameter req and an optional parameter opt . path ( \"params\" ) { get { parameters ( 'req, 'opt. ?) { ( req , opt ) => complete ( s \"Req: $req, Opt: $opt\" ) } } } Notice the single tick declaration, and for opt the .?. The .?\nsignifies the parameter is optional. You can list as many parameters\nas you want, and they will be provided to the expression as variables\n(the variable names don’t need to be named the same as the query\nparams).  If you run this and go to\nhttp://localhost:8080/params?req=hi&opt=bye your response should be Req: hi, Opt: Some(bye) So you can see the optional parameter translates to a Scala Option type. Now hit the url without any query parameters\n(http://localhost:8080/params).  You’ll notice the message is our root\nlevel get I exist! . This is because it didn’t match against the\nrequired parameters so it fell through to our root get .  However, if\nyou comment out our root get you’ll see the following message: Request is missing required query parameter 'req' Which is much more useful.  The lesson here is to be careful about how\nthings can flow through and design your routes appropriately. Getting headers Similarly to getting parameters, you can retrieve headers through the headerValueByName directive. path ( \"headers\" ) { get { headerValueByName ( \"ct-remote-user\" ) { userId => complete ( userId ) } } } You also can get an optional header through the\noptionalHeaderValueByName directive that will return an Option. Returning futures instead of data Finally, you might have your service call a reactive framework like\nSlick 3, or your worker might return a Scala Future.  Luckily, the complete call handles this without any problems. path ( \"reactive\" ) { get { complete ( future { \"I'm reactive!\" }) } } So you can make your code as reactive as you want and never have to do\nan Await.result! Packaging your app for deployment Now we have our services, we’re ready to deploy them.  For this we\nneed to instruct SBT on how to package the distribution.  In the\nbuild.sbt I started with, there was this line lazy val root = ( project in file ( \".\" )). enablePlugins ( JavaAppPackaging ) That uses the sbt-native-packager plugin provided by typesafe.  In\nyour project/plugins.sbt, you’ll want to add the following addSbtPlugin ( \"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.0.0\" ) If you run sbt dist you’ll end up with a zip file under\ntarget/universal which you can send to the deployment mechanism of\nyour choice (for instance, Cloud Foundry). Finally done As I stated above (ok, WAY above), the code is at simple-spray .  Feel free\nto fork it into your own repository and experiment with it.  But the\nbuilding blocks I’ve given you should be able to handle a great deal\nof your requests. I’ll follow up later with more advanced spray features focusing on\npathing and unit testing your spray services. posted on August 11, 2015 by Scott MacDonald ← Previous Post Next Post → val akka = \"2.3.9\" val spray = \"1.3.2\" resolvers += Resolver . url ( \"TypeSafe Ivy releases\" , url ( \"http://dl.bintray.com/typesafe/ivy-releases/\" ))( Resolver . ivyStylePatterns ) libraryDependencies ++= Seq ( // -- Logging -- \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\" , \"com.typesafe.scala-logging\" %% \"scala-logging-slf4j\" % \"2.1.2\" , // -- Akka -- \"com.typesafe.akka\" %% \"akka-testkit\" % akka % \"test\" , \"com.typesafe.akka\" %% \"akka-actor\" % akka , \"com.typesafe.akka\" %% \"akka-slf4j\" % akka , // -- Spray -- \"io.spray\" %% \"spray-routing\" % spray , \"io.spray\" %% \"spray-client\" % spray , \"io.spray\" %% \"spray-testkit\" % spray % \"test\" , // -- json -- \"io.spray\" %% \"spray-json\" % \"1.3.1\" , // -- config -- \"com.typesafe\" % \"config\" % \"1.2.1\" , // -- testing -- \"org.scalatest\" %% \"scalatest\" % \"2.2.1\" % \"test\" ) val akka = \"2.3.9\" val spray = \"1.3.2\" resolvers += Resolver . url ( \"TypeSafe Ivy releases\" , url ( \"http://dl.bintray.com/typesafe/ivy-releases/\" ))( Resolver . ivyStylePatterns ) libraryDependencies ++= Seq ( // -- Logging -- \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\" , \"com.typesafe.scala-logging\" %% \"scala-logging-slf4j\" % \"2.1.2\" , // -- Akka -- \"com.typesafe.akka\" %% \"akka-testkit\" % akka % \"test\" , \"com.typesafe.akka\" %% \"akka-actor\" % akka , \"com.typesafe.akka\" %% \"akka-slf4j\" % akka , // -- Spray -- \"io.spray\" %% \"spray-routing\" % spray , \"io.spray\" %% \"spray-client\" % spray , \"io.spray\" %% \"spray-testkit\" % spray % \"test\" , // -- json -- \"io.spray\" %% \"spray-json\" % \"1.3.1\" , // -- config -- \"com.typesafe\" % \"config\" % \"1.2.1\" , // -- testing -- \"org.scalatest\" %% \"scalatest\" % \"2.2.1\" % \"test\" ) import akka.actor.Actor import spray.routing.HttpService class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } trait SampleRoute extends HttpService { val route = { get { complete ( \"I exist!\" ) } } } import akka.actor.Actor import spray.routing.HttpService class SampleServiceActor extends Actor with SampleRoute { def actorRefFactory = context def receive = runRoute ( route ) } trait SampleRoute extends HttpService { val route = { get { complete ( \"I exist!\" ) } } } import akka.actor. { ActorSystem , Props } import akka.io.IO import spray.can.Http object Main extends App { implicit val system = ActorSystem ( \"simple-service\" ) val service = system . actorOf ( Props [ SampleServiceActor ], \"simple-service\" ) //If we're on cloud foundry, get's the host/port from the env vars lazy val host = Option ( System . getenv ( \"VCAP_APP_HOST\" )). getOrElse ( \"localhost\" ) lazy val port = Option ( System . getenv ( \"VCAP_APP_PORT\" )). getOrElse ( \"8080\" ). toInt IO ( Http ) ! Http . Bind ( service , host , port = port ) } import akka.actor. { ActorSystem , Props } import akka.io.IO import spray.can.Http object Main extends App { implicit val system = ActorSystem ( \"simple-service\" ) val service = system . actorOf ( Props [ SampleServiceActor ], \"simple-service\" ) //If we're on cloud foundry, get's the host/port from the env vars lazy val host = Option ( System . getenv ( \"VCAP_APP_HOST\" )). getOrElse ( \"localhost\" ) lazy val port = Option ( System . getenv ( \"VCAP_APP_PORT\" )). getOrElse ( \"8080\" ). toInt IO ( Http ) ! Http . Bind ( service , host , port = port ) } val route = { get { complete ( \"I exist!\" ) } ~ get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } } val route = { get { complete ( \"I exist!\" ) } ~ get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } } val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ get { complete ( \"I exist!\" ) } } val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ get { complete ( \"I exist!\" ) } } val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ post { path ( \"stuff\" ) { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } val route = { get { path ( \"stuff\" ) { complete ( \"That's my stuff!\" ) } } ~ post { path ( \"stuff\" ) { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } val route = { path ( \"stuff\" ) { get { complete ( \"That's my stuff!\" ) } ~ post { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } val route = { path ( \"stuff\" ) { get { complete ( \"That's my stuff!\" ) } ~ post { complete ( \"stuff posted!\" ) } } ~ get { complete ( \"I exist!\" ) } } import spray.json.DefaultJsonProtocol case class Stuff ( id : Int , data : String ) object Stuff extends DefaultJsonProtocol { implicit val stuffFormat = jsonFormat2 ( Stuff . apply ) } import spray.json.DefaultJsonProtocol case class Stuff ( id : Int , data : String ) object Stuff extends DefaultJsonProtocol { implicit val stuffFormat = jsonFormat2 ( Stuff . apply ) } trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes val route = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { complete ( \"stuff posted!\" ) } } } ~ get { complete ( \"I exist!\" ) } } } trait SampleRoute extends HttpService { import spray.httpx.SprayJsonSupport._ import Stuff._ import spray.http.MediaTypes val route = { path ( \"stuff\" ) { respondWithMediaType ( MediaTypes . `application/json` ) { get { complete ( Stuff ( 1 , \"my stuff\" )) } ~ post { complete ( \"stuff posted!\" ) } } } ~ get { complete ( \"I exist!\" ) } } } post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } post { entity ( as [ Stuff ]) { stuff => complete ( Stuff ( stuff . id + 100 , stuff . data + \" posted\" )) } } pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } pathPrefix ( \"junk\" ) { pathPrefix ( \"mine\" ) { pathEnd { get { complete ( \"MINE!\" ) } } } ~ pathPrefix ( \"yours\" ) { pathEnd { get { complete ( \"YOURS!\" ) } } } } path ( \"params\" ) { get { parameters ( 'req, 'opt. ?) { ( req , opt ) => complete ( s \"Req: $req, Opt: $opt\" ) } } } path ( \"params\" ) { get { parameters ( 'req, 'opt. ?) { ( req , opt ) => complete ( s \"Req: $req, Opt: $opt\" ) } } } Req: hi, Opt: Some(bye) Req: hi, Opt: Some(bye) Request is missing required query parameter 'req' Request is missing required query parameter 'req' path ( \"headers\" ) { get { headerValueByName ( \"ct-remote-user\" ) { userId => complete ( userId ) } } } path ( \"headers\" ) { get { headerValueByName ( \"ct-remote-user\" ) { userId => complete ( userId ) } } } path ( \"reactive\" ) { get { complete ( future { \"I'm reactive!\" }) } } path ( \"reactive\" ) { get { complete ( future { \"I'm reactive!\" }) } } lazy val root = ( project in file ( \".\" )). enablePlugins ( JavaAppPackaging ) lazy val root = ( project in file ( \".\" )). enablePlugins ( JavaAppPackaging ) addSbtPlugin ( \"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.0.0\" ) addSbtPlugin ( \"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.0.0\" ) posted on August 11, 2015 by Scott MacDonald Scott MacDonald", "date": "2015-08-11"},
{"website": "Monsanto", "title": "Learn implicits: Views as class extensions", "author": ["\n                                        Jorge Montero\n                                    "], "link": "http://engineering.monsanto.com/2015/07/31/implicit-conversions/", "abstract": "This is the third post in our series on Scala implicits. The earlier posts cover [implicit type parameters](/2015/05/14/implicits-intro/ and [implicit parameters with futures](/2015/06/15/implicits-futures/. In this post, we discuss implicit conversions. Implicit conversions, also called views, are a powerful and dangerous friend. \nThey are useful for avoiding boilerplate, but used improperly they lead to confusion. Even if you didn’t know they existed, I bet you’ve used them already. Let’s look at a very simple example, using the scala REPL: scala> val s = \"fluttershy\"\ns: String = fluttershy\n\nscala> s.getClass.getName\nres1: String = java.lang.String\n\nscala> val cap = s. capitalize cap: String = Fluttershy\n\nscala> cap.getClass.getName\nres2: String = java.lang.String so we have a plain Java String, and we capitalize it. Seems simple. I just called a method on an object. \nExcept java.lang.String does not have a capitalize method! What sorcery is this? As IntelliJ tells us, the capitalize method is a part of StringLike . Somehow our String got converted into a StringLike, to call capitalize . But we didn’t do anything! Scala automatically imports scala.Predef everywhere. Among many other things, Predef contains: implicit def augmentString(x : String) : scala.collection.immutable.StringOps The return type of this method, StringOps ,\nhas the StringLike trait which includes the capitalize method. So what does that implicit def mean? Any time we try to call a method that doesn’t exist (or when a parameter doesn’t match the expected type),\n the compiler attempts to use a view to make it match.\nA view is a single-parameter function or constructor, declared with the implicit keyword in front of it. The implicit keyword tells the compiler\nthat it can use this function automatically, for as long as it is in scope. It’s almost as if Scala added methods without changing java.lang.String. No manual wrapping: it’s almost invisible. Sounds convenient! All this power comes with downsides. If a programmer is not familiar with all the views in scope, the code is harder to interpret.\nThere’s also the temptation to define very wide conversions. Everyone does it, and later regrets it.\nLet’s say that some classes take a lot of Options: case class Octopus(name : Option[String], tentacles : Option[Int], phoneNumber : Option[String])\n\nval a = Octopus(Some(name), Some(tentacles), Some(phone)) If we always have the data, those Options are just noise, so someone who recently learned views might write something like this: implicit def optionify[T](t : T):Option[T] = Option(t) Which lets this call work: val a = Octopus(name, tentacles, phone) Sounds great, right? We never have to wrap any values anymore! What’s the worst that could happen? Wherever that implicit function is in scope, any syntax error that could be fixed by wrapping anything into an Option will be \n“fixed” that way, whether it makes sense or not. val aList = List(\"a\",\"b\",\"c\")\nval anInt = 42\nval something = Octopus(\"Angry Bob\",7,\"(888)-444-3333\")\n\naList.isEmpty\nanInt.isEmpty\nsomething.isEmpty List and Option define isEmpty . If you think you have a List, but you really have an Octopus, \nthe compiler will use your view, give you an Option[Octopus], and isEmpty will compile! That’s not what we wanted when we defined our view,\nbut there it is. Add a few more implicits like that to the same scope, and suddenly you might as well be working in a language without types:\n the compiler stops being useful. To use this view responsibly,  add it to the scope very carefully, just for the\ncode than needs it: object AutoOption { implicit def def optionify[T](t:T):Option[T] = Option(t)\n}\n\nclass PutsThingsIntoOptionsAllTheTime{\n  import AutoOption._\n\n  ... put code that uses the implicit conversion here ...\n\n} In general, views that accept anything at all will be confusing. For example, Scala lets you call + on anything. Predef includes: implicit def final class any2stringadd[A](private val self: A) extends AnyVal {\n  def +(other: String): String = String.valueOf(self) + other\n} So this gives every class a + method that lets it concatenate to a String. scala> Set(\"1\",\"2\",\"3\") + \"a gazebo\"\nres0: scala.collection.immutable.Set[String] = Set(1, 2, 3, a gazebo)\n\nscala> Set(1,2,3) + \"a gazebo\"\nres1: String = Set(1, 2, 3)a gazebo\n\nscala> \"a gazebo\" + Set(1,2,3)\nres2: String = a gazeboSet(1, 2, 3)\n\nscala> Set[Any](1,2,3) + \"a gazebo\"\nres3: scala.collection.immutable.Set[Any] = Set(1, 2, 3, a gazebo)\n\nscala> Some(\"gazebo\") + 3\n<console>:8: error: type mismatch;\n found   : Int(3)\n required: String If this isn’t crazy enough for you, check out this Scala puzzler . This has annoyed so many people so much that there are plans to remove it in\na future version of Scala. If the language authors create troublesome views, the rest of us should take warning. When creating views, aim to have the seamlessness of capitalize . The view aims to be invisible. Notice that capitalize returns a String; when we benefit from the view, we never see the intermediate StringOps type. Other bonus methods do the same: trait StringLike {\n  def capitalize : String\n  def stripMargin(marginChar : Char) : String\n  def stripPrefix(prefix : String)\n} Calling the method does not surprise us.\nThe one way the user can tell that we are using a custom implicit conversion is this subtle underline in IntelliJ: capitalize is underlined because the method is added by a view.\nThe 42 is underlined because a Scala Int is converted to a Java Integer using another view defined in Predef . While overly wide views in an overly wide scope can lead to confusion,\nviews are an invaluable way to extend class functionality with a strong type system, without a bunch of explicit wrapping. posted on July 31, 2015 by Jorge Montero ← Previous Post Next Post → case class Octopus(name : Option[String], tentacles : Option[Int], phoneNumber : Option[String])\n\nval a = Octopus(Some(name), Some(tentacles), Some(phone)) case class Octopus(name : Option[String], tentacles : Option[Int], phoneNumber : Option[String])\n\nval a = Octopus(Some(name), Some(tentacles), Some(phone)) val a = Octopus(name, tentacles, phone) val a = Octopus(name, tentacles, phone) val aList = List(\"a\",\"b\",\"c\")\nval anInt = 42\nval something = Octopus(\"Angry Bob\",7,\"(888)-444-3333\")\n\naList.isEmpty\nanInt.isEmpty\nsomething.isEmpty val aList = List(\"a\",\"b\",\"c\")\nval anInt = 42\nval something = Octopus(\"Angry Bob\",7,\"(888)-444-3333\")\n\naList.isEmpty\nanInt.isEmpty\nsomething.isEmpty scala> Set(\"1\",\"2\",\"3\") + \"a gazebo\"\nres0: scala.collection.immutable.Set[String] = Set(1, 2, 3, a gazebo)\n\nscala> Set(1,2,3) + \"a gazebo\"\nres1: String = Set(1, 2, 3)a gazebo\n\nscala> \"a gazebo\" + Set(1,2,3)\nres2: String = a gazeboSet(1, 2, 3)\n\nscala> Set[Any](1,2,3) + \"a gazebo\"\nres3: scala.collection.immutable.Set[Any] = Set(1, 2, 3, a gazebo)\n\nscala> Some(\"gazebo\") + 3\n<console>:8: error: type mismatch;\n found   : Int(3)\n required: String scala> Set(\"1\",\"2\",\"3\") + \"a gazebo\"\nres0: scala.collection.immutable.Set[String] = Set(1, 2, 3, a gazebo)\n\nscala> Set(1,2,3) + \"a gazebo\"\nres1: String = Set(1, 2, 3)a gazebo\n\nscala> \"a gazebo\" + Set(1,2,3)\nres2: String = a gazeboSet(1, 2, 3)\n\nscala> Set[Any](1,2,3) + \"a gazebo\"\nres3: scala.collection.immutable.Set[Any] = Set(1, 2, 3, a gazebo)\n\nscala> Some(\"gazebo\") + 3\n<console>:8: error: type mismatch;\n found   : Int(3)\n required: String posted on July 31, 2015 by Jorge Montero Jorge Montero", "date": "2015-07-31"},
{"website": "Monsanto", "title": "Cloud Foundry Portal", "author": ["\n                                        Mark Seidenstricker\n                                    "], "link": "http://engineering.monsanto.com/2015/07/29/cf-portal/", "abstract": "This segment of our open source Cloud Foundry (CF) toolbox series introduces CF-Portal , a basic read-only view into your Cloud Foundry application landscape.  CF-Portal was one of the first applications we made for our CF toolbox, since unlike the branded CF offerings, the community project lacks any pre-packaged UI. Why not re-use an existing solution? There are open source UI’s available in the community, most notably the admin-ui project, which we do utilize for our environments.  But the admin-ui project is truly an admin interface, requiring logins and admin level privileges to interact with the system.  CF-Portal is meant to supplement the admin-ui project by providing an anonymous, read-only view into your Cloud Foundry application landscape for consumption by CF end-users. Does cloud foundry even need a UI? The CF CLI is powerful, extensible, and, for a large chunk of CF developers, it’s their sole point of interaction with the system.  But there are still use cases where having a UI comes in handy. Sometimes we need to expose data which can be accessed by non-developers, for example people who don’t have an account in CF such as release managers or a level one support agent.  And even with access to the CF CLI, some tasks such as searching or aggregating output are difficult and would require elevated permissions. Design Requirements When creating CF-Portal we wanted to fulfill three basic requirements Anonymous Access - The portal should be available without requiring user authentication.  Because of this we also must ensure that the application displays only non-sensitive data and has read-only capabilities. Searchable View of the Application Landscape - The portal should allow any user to view how many applications are in any given (CF org space) as well as quickly determine which org and space a specific application belongs to. Bare Bones Troubleshooting - The portal should quickly allow users to determine the state of an application such as if it’s up or down and how long it’s been running.  It should also show basic audit information such as recent application logs and audit events. The Result The portal has two views, a summary view of all applications deployed to the Cloud Foundry environment and a detail view for each app showing some basic application specific stats.  The data for the summary view is gathered every 5 minutes and cached locally for best responsiveness.  The top right corner includes a search bar to quickly find the high level details of an application and clicking on any row allows you to drill down to the detail page for that application. If you’re used to the CF CLI, its similar to what you’d get from running cf apps in every space. The detail page shows real-time information about the application in question, similar to what you would see by running cf app <app-name> .  In addition it collects recent logs and audit events which you might see by issuing a few more CLI commands: cf events <app-name> & cf logs <app-name> . If you’re using Cloud Foundry and you have the need for an intuitive UI, please jump over to the project repo and give CF-Portal a try.  And as always we encourage comments and pull requests to continue making our tools better. posted on July 29, 2015 by Mark Seidenstricker ← Previous Post Next Post → posted on July 29, 2015 by Mark Seidenstricker Mark Seidenstricker", "date": "2015-07-29"},
{"website": "Monsanto", "title": "Testing without mocking in Scala", "author": ["\n                                        Jessica Kerr\n                                    "], "link": "http://engineering.monsanto.com/2015/07/28/avoiding-mocks/", "abstract": "For unit testing in Java, mocking frameworks replace classes necessary for the code under \ntest, but not under test themselves. These mock frameworks don’t transfer easily to Scala. That’s OK: the functional side of Scala can make mocking unnecessary. As someone told me the other day at PolyConf : mocks are the sound of your code crying out, “please structure me differently!” Don’t use mocks? Structure code differently? Easier said than done. What follows is a practical example of removing the need for a mock object, and at the same time separating concerns of interface and business logic. Say there’s an IdentityService that returns a username based on an access token. Internally, it calls out to AccessTokenService, retrieving\ninformation about the access token. Then it interprets the result: success provides an identity; anything else means proceed \nanonymously (return no identity). The code looks like: class IdentityClient( jsonClient: JsonClient )  {\n\n  def fetchIdentity(accessToken: String) : Future[Option[Identity]] = { jsonClient .getWithoutSession( Path(\"identities\"), Params(\"access_token\" -> accessToken ) ) .map {\n      case JsonResponse(OkStatus, json, _, _) => Some(Identity.from(json))\n      case _ => None\n    } }\n} The tests want to say, “If the inner call returns success, provide the returned identity; if it fails, return none.” To unit-test that, we need to mock the JsonClient , and its getWithoutSession method, and check the arguments… ugh, mocking. The secret here is to recognize that part of the method under test is about the interface , and part of it is business logic . The interface is only testable in integration tests. That’s where we check our assumptions about the path structure, the input and the output of the other service. \nThe business logic part of this is unit-testable once we separate the two. Ports and Adapters Instead of passing in a general JsonClient ,\n let’s pass in a function that contains all the interface code. That function needs an access token, and it returns a future response. class IdentityClient( howToCheck: String => Future[JsonResponse] ) {\n\n  def fetchIdentity(accessToken: String) : Future[Option[Identity]] = { howToCheck (accessToken).map { case JsonResponse(OkStatus, json, _, _) => Some(IdentityMapper(json)) case _ => None } }\n} Meanwhile, the real interface code is shipped off to a handy object somewhere: object RealAccessTokenService {\n  def reallyCheckAccessToken( jsonClient:JsonClient )(accessToken: String): Future[JsonResponse] = jsonClient .getWithoutSession( Path() / \"identity\", Params(\"access_token\" -> accessToken) ) } The production code can instantiate the AccessToken client using that object, but the test is free to provide a fake function implementation , without duplicating any specifics about this particular interface: it(\"returns an Identity when we have it\") {\n  val jsonBody = Map(\"identity\" -> Map(\"id\" -> \"external_username\")).toJson\n  val identityClient \n    = new IdentityClient( _ => Future(JsonResponse(OkStatus, jsonBody)) )\n\n  Await.result(identityClient.fetchIdentity(\"an_access_token\"), 1.second) ===\n    Some(Identity(\"external_username\"))\n} This test constructs the expected response and then provides a function that returns that, no matter what. \nIt isn’t checking the arguments, although it could. We can pass a function that does whatever we want, for the purposes of our test.\n There’s no JsonClient object to mock. (Technically, the function we passed is a fake, which is different from a mock. It works here.) This is a minimal example, and the test isn’t perfect. Yet, it shows how passing a “ how ” instead of passing an object can make testing easier in Scala. Check the sample code before and after to see the difference. This example illustrates a ports-and-adapters architecture. By removing the interface code, we created a port – like a hole, like a Java interface. Then the RealJsonClient contains an adapter :\n a plug for the hole that hooks up to a real-life system. The function passed in the test is an adapter that fits the same hole. Or: Flow of Data The ports-and-adapters style lets us drop in different implementations for I/O that happens in the middle of our code. If we can avoid I/O in the middle of the code, even better. There’s a cleaner way of restructuring this same code, because it’s possible to view it as: gather data, then make decisions, then output. Instead of passing in how to call the AccessTokenService, why not make the call and pass the results into the function under test? object dentityClient  {\n  def fetchIdentity( accessTokenInfo: Future[JsonResponse] ) : Future[Option[dentity]] = { accessTokenInfo.map { case JsonResponse(OkStatus, json) => Some(Identity.from(json)){ case _ => None{ } }\n} The function is data-in, data-out. No need to instantiate a class; an object will do. The code is even easier to test now. No mocks, no fakes, construct some input and pass it in . import dentityClient._\n\nit(\"returns an Identity when we have it\") {\n  val jsonBody = Map(\"identity\" -> Map(\"id\" -> \"external_username\")).toJson\n  val accessTokenInfo = Future(JsonResponse(OkStatus, jsonBody))\n\n  Await.result(fetchIdentity( accessTokenInfo ), 1.second) ===\n  Some(Identity(\"external_username\"))\n} In the real world, we use the same RealAccessTokenService to gather the input before calling \nour data-in, data-out function. Instead of passing “how to gather input” we’re passing the input as data. This is the simplest structure. Sample code here . Whenever you see mocking in Scala, look for an opportunity to separate decision-making code from interface code . Consider these styles instead. Thanks to Duana for asking me these questions and providing the example. posted on July 28, 2015 by Jessica Kerr ← Previous Post Next Post → class IdentityClient( jsonClient: JsonClient )  {\n\n  def fetchIdentity(accessToken: String) : Future[Option[Identity]] = { jsonClient .getWithoutSession( Path(\"identities\"), Params(\"access_token\" -> accessToken ) ) .map {\n      case JsonResponse(OkStatus, json, _, _) => Some(Identity.from(json))\n      case _ => None\n    } }\n} class IdentityClient( howToCheck: String => Future[JsonResponse] ) {\n\n  def fetchIdentity(accessToken: String) : Future[Option[Identity]] = { howToCheck (accessToken).map { case JsonResponse(OkStatus, json, _, _) => Some(IdentityMapper(json)) case _ => None } }\n} object RealAccessTokenService {\n  def reallyCheckAccessToken( jsonClient:JsonClient )(accessToken: String): Future[JsonResponse] = jsonClient .getWithoutSession( Path() / \"identity\", Params(\"access_token\" -> accessToken) ) } it(\"returns an Identity when we have it\") {\n  val jsonBody = Map(\"identity\" -> Map(\"id\" -> \"external_username\")).toJson\n  val identityClient \n    = new IdentityClient( _ => Future(JsonResponse(OkStatus, jsonBody)) )\n\n  Await.result(identityClient.fetchIdentity(\"an_access_token\"), 1.second) ===\n    Some(Identity(\"external_username\"))\n} object dentityClient  {\n  def fetchIdentity( accessTokenInfo: Future[JsonResponse] ) : Future[Option[dentity]] = { accessTokenInfo.map { case JsonResponse(OkStatus, json) => Some(Identity.from(json)){ case _ => None{ } }\n} import dentityClient._\n\nit(\"returns an Identity when we have it\") {\n  val jsonBody = Map(\"identity\" -> Map(\"id\" -> \"external_username\")).toJson\n  val accessTokenInfo = Future(JsonResponse(OkStatus, jsonBody))\n\n  Await.result(fetchIdentity( accessTokenInfo ), 1.second) ===\n  Some(Identity(\"external_username\"))\n} posted on July 28, 2015 by Jessica Kerr Jessica Kerr", "date": "2015-07-28"},
{"website": "Monsanto", "title": "PaaSify your Apps", "author": ["\n                                        Mark Seidenstricker\n                                    "], "link": "http://engineering.monsanto.com/2015/07/22/building-an-open-source-cloud-foundry-toolbox/", "abstract": "The age of cloud computing is already here, and companies who don’t adapt are at serious risk of getting left in the dust.  At Monsanto we have fully embraced the transformation this revolution has brought and are diligently on the path towards a modern IT landscape focused around public cloud, microservices, open source technologies, and 12 factor apps .  Transformations however, rarely happen overnight and like most enterprise companies we were starting the journey with a datacenter full of legacy and not-so-cloudy apps.  So our first step was to adopt Cloud Foundry , an open platform as a service which allowed our dev teams to start learning/adopting cloud designs on top of an agile platform in the safety of our private datacenter. We’ve been using Cloud Foundry for over a year now, and the core project is fairly comprehensive with extensions and tooling being contributed by the community on a regular basis.  But there are still a few areas which lack open source solutions to address the capabilities seen in the branded CF offerings.  In addition to using some of the existing community tools we’ve also added a few of our own and we’ll be opening the first three projects from this toolbox to the community. Starting with CF-Portal , we will be doing a multi-part blog post over the coming weeks for each tool corresponding with its public release.  We hope these are useful to others using the platform, and we look forward to feedback and contribution to continue growing the Cloud Foundry community toolbox. CF-Portal : A Single Pane of Glass for Cloud Foundry Apps CF-Users : Self Service Team Management for Cloud Foundry CF-Metrics : Open Source Monitoring and Alerting for Cloud Foundry posted on July 22, 2015 by Mark Seidenstricker ← Previous Post Next Post → posted on July 22, 2015 by Mark Seidenstricker Mark Seidenstricker", "date": "2015-07-22"},
{"website": "Monsanto", "title": "CloudFormation Template Generator", "author": ["\n                                        Ryan Richt\n                                    "], "link": "http://engineering.monsanto.com/2015/07/10/cloudformation-template-generator/", "abstract": "TL;DR CloudFormation gives you a declarative specification to stand up complex AWS topologies. You can simplify creation of templates with potentially thousands of lines using our open source, type-safe library to generate templates with the full power of Scala. We use a variety of strategies to simplify creation of resources as well as encode consistency checks in Scala’s type system. Hand-crafted Template Woes At Monsanto we have a reasonably complex infrastructure topology for managing Microservice applications in AWS. You may have seen our post open sourcing our Stax tool for simplifying interactions with AWS and managing our VPCs. Or maybe you’re jumping into the new AWS Service Catalog . Stax, AWS Service Catalog, and underlying CloudFormation all work on a BYOT model - bring your own template. Our problem was, as our topology become more complex and many members of our team spent more and more time making changes, our template ballooned to ~5,000 lines of JSON. Worse, CloudFormation entries are not self-contained but also contain internal references: \"MyAutoScaleGroup\" : { \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\" , \"Properties\" : { \"LoadBalancerNames\" : [{ \"Ref\" : \"MyLoadBalancerA\" }, { \"Ref\" : \"MyLoadBalancerB\" }], ... meaning that changes to a template may require simultaneous edits to far-flung parts of the template you might not even know exist. Today we have reduced our template to ~500 lines of Scala that make use of our CloudFormation Template Generator library which is now open source under the BSD 3-clause license. Goals In designing CFTG, we wanted to eliminate repetition and many common errors that we saw in working with complex templates: Support generation of repetitive elements Constrain “References” to entities that actually exist Check the types of Maps and Functions like Fn::If Lift all of the stringly-typed parameters like AMI’s or CIDR blocks into strong types Explicitly model optional fields Disallow the many mutually exclusive parameter settings described in the CloudFormation documentation We also wanted to create a “layered” set of abstractions. At the bottom most layer, we have objects that map directly to CloudFormation JSON objects and we are continuing to work on higher-order Builder functions to create groups of meaningful functionality, such as autoscaling groups with reasonable launch configs. In early versions and after aggressive refactoring we also noticed that the majority of our remaining template code concerned Security Groups and In/Egress Rules so we also created a DSL for ingress rules and a construct for implicit security groups we call “SecurityGroupRoutables.” Why didn’t we use Terraform or roll-our-own API calls? While these might be great solutions for some, Terraform did not (and still does not yet) support all of the resources types we needed to use. We also wanted the full power of a Turing-complete language like Scala to abstract complex elements. As fans of “immutable infrastructure” we also liked the idea of using CloudFormation where we have an authoritative, declarative specification of our entire environment without drift. Low-Level Bits We love Spray-JSON and its type-class based system for mapping classes to JSON. All of our Resource classes look something like this: case class `AWS :: EC2 :: Subnet `(\n  name:             String,\n  VpcId:            Token[ResourceRef[` AWS :: EC2 :: VPC `]],\n  AvailabilityZone: Token[String],\n  CidrBlock:        Token[CidrBlock],\n  Tags:             Seq[AmazonTag],\n  override val Condition: Option[ConditionRef] = None\n  ) extends Resource[` AWS :: EC2 :: Subnet `]{\n\n  def when(newCondition: Option[ConditionRef] = Condition) = copy(Condition = newCondition)\n}\nobject ` AWS :: EC2 :: Subnet ` extends DefaultJsonProtocol {\n  implicit val format: JsonFormat[` AWS :: EC2 :: Subnet `] = jsonFormat6(` AWS :: EC2 :: Subnet `. apply ) } The Resource companion objects always define an implicit Spray-JSON type class instance. OK so it’s not quite a perfect mapping to the JSON. In CloudFormation, top level entities are held in a map with a user specified name as a key. In CFTG each Resource or Parameter has a “name” property that holds this label inside each object so that it can be be more easily referenced from others. But we do try to map as closely to CloudFormation as possible. We have non-idiomatic Scala capital field names in Resources to match that CloudFormation standard. We use Scala’s back-tick labels to preserve the CloudFormation AWS::EC2::Subnet style names for better search-ability and familiarity with raw CloudFormation. We also support Conditions on resource which result in conditional creation, including a “when” convenience function that you can also use through: import com.monsanto.arch.cloudformation.model.simple.Builders._ object Example { ... val conditionalResource = when ( someCondition ){ someResource } } More Types This AWS::EC2::Subnet Resource also highlights a few other central concepts in CFTG: our reference types, our common usage of wrapper types for things like CIDR Blocks and Amazon’s Tags, and the mysterious “Token” type. In CloudFormation, you can reference Resources, Parameters, Mappings and Conditions by name. In CFTG we force all of these to be by object reference using ResourceRef , ConditionRef , ParameterRef , and MappingRef . These references contain a type parameter of what they point to. For instance, to define an AWS::EC2::Subnet you have to pass in a VpcId: Token[ResourceRef[AWS::EC2::VPC]] . In stock CloudFormation this parameter is just a string, as are CIDR blocks, AMI IDs, etc., but in CFTG we have strict types for almost everything. Many of these types do have implicit conversions to make them easier to use: you can always pass a Resource instance, like an AWS::EC2::Instance to something that takes a ResourceRef[AWS::EC2::Instance] and it will be converted for you. Similarly for classes like CidrBlock which is defined like this: case class IPAddressSegment ( value : Int ){ require ( value <= 255 && value >= 0 ) } object IPAddressSegment { implicit def fromInt ( i : Int ) : IPAddressSegment = IPAddressSegment ( i ) } case class IPMask ( value : Int ){ require ( value <= 32 && value >= 0 ) } object IPMask { implicit def fromInt ( i : Int ) : IPMask = IPMask ( i ) } case class CidrBlock ( a : IPAddressSegment , b : IPAddressSegment , c : IPAddressSegment , d : IPAddressSegment , mask : IPMask ) ... This makes it valid to write simply: val myBlock = CidrBlock ( 10 , 10 , 0 , 0 , 16 ) but have the type checking of: val myBlock = CidrBlock ( IPAddressSegment ( 10 ), IPAddressSegment ( 10 ), IPAddressSegment ( 0 ), IPAddressSegment ( 0 ), IPMask ( 16 ) ) You’ll note this is one of the few places we “cheat” with run-time (but remember this is template generation runtime, not template instantiation runtime) with some Design-by-contract style checks for valid numerical ranges of IP segments. Typed Functions OK, so what is that funny Token[T] thing? It might be a terrible name ( Pull Requests welcome ), but it’s purpose is to abstract over “a literal T ” or “an Amazon function that returns a T .” In CFTG, we have versions of the built-in Amazon functions like: Fn::GetAtt Fn::Join Fn::FindInMap Fn::Base64 Fn::Equals Fn::Not Fn::And Fn::Or Fn::If Consider the implementation of Fn::If : case class `Fn::If` [ R : JsonFormat ]( conditionName : Token [ String ], valIfTrue : Token [ R ], valIfFalse : Token [ R ] ) extends AmazonFunctionCall [ R ]( \"Fn::If\" ){ type CFBackingType = ( Token [ String ], Token [ R ], Token [ R ]) val arguments = ( conditionName , valIfTrue , valIfFalse ) ... } Here R is the logical return type of the Fn::If in CFTG (logical because this is our notion of a return type, not stock CloudFormation’s). For instance I could have a: val gatewayServiceELBSecGroup = // Security group with 80 ingress rules val gatewayServiceELBSSLSecGroup = // Security group with 443 ingress rules val serviceElbOrElbSSL : Token [ ResourceRef [ `AWS::EC2::SecurityGroup` ]] = `Fn::If` [ ResourceRef [ `AWS::EC2::SecurityGroup` ]]( \"ServiceELBSSLCertNameIsNotDefined\" , gatewayServiceELBSecGroup , gatewayServiceELBSSLSecGroup ) I’ve shown the explicit return type parameter of this Fn::If which here is a Token[ResourceRef[`AWS::EC2::SecurityGroup`]] , not just a string! In this way I could pass this value confidently to an AWS::EC2::Instance constructor that requires a Security Group. Back to Tokens, while you can explicitly wrap a Resource (and sometimes other values) you can always just pass a Resource or a Function return value and it will be promoted into a Token value using the implicit conversions: sealed trait Token [ R ] object Token extends DefaultJsonProtocol { implicit def fromAny [ R: JsonFormat ]( r : R ) : AnyToken [ R ] = AnyToken ( r ) implicit def fromOptionAny [ R: JsonFormat ]( or : Option [ R ]) : Option [ AnyToken [ R ]] = or . map ( r => Token . fromAny ( r )) implicit def fromString ( s : String ) : StringToken = StringToken ( s ) implicit def fromFunction [ R ]( f : AmazonFunctionCall [ R ]) : FunctionCallToken [ R ] = FunctionCallToken [ R ]( f ) implicit def fromSome [ R ]( oR : Some [ R ])( implicit ev1 : R => Token [ R ]) : Some [ Token [ R ]] = oR . map ( ev1 ). asInstanceOf [ Some [ Token [ R ]]] implicit def fromOption [ R ]( oR : Option [ R ])( implicit ev1 : R => Token [ R ]) : Option [ Token [ R ]] = oR . map ( ev1 ) implicit def fromResource [ R <: Resource [ R ]]( r : R )( implicit conv : ( R ) => ResourceRef [ R ]) : Token [ ResourceRef [ R ]] = fromAny ( conv ( r )) implicit def fromSeq [ R <: Resource [ R ]]( sR : Seq [ R ])( implicit toRef : R => ResourceRef [ R ]) : Seq [ Token [ ResourceRef [ R ]]] = sR . map ( r => fromAny ( toRef ( r ))) This includes the ability to automatically wrap options of things (including a more specific conversion to maintain Somes as such, more on that later), each of a sequence of Resources, etc. Patterns for Encoding Complex Constraints There are several Amazon resources with documentation like: AWS::Route53::RecordSet … AliasTarget Alias resource record sets only: Information about the domain to which you are redirecting traffic. … TTL The resource record cache time to live (TTL), in seconds. If you specify this property, do not specify the AliasTarget property. For alias target records, the alias uses a TTL value from the target. … If this is relatively simple, we can (did) just create a class with a private constructor and a set of defined factory methods, like this real example: class `AWS :: Route53 :: RecordSet ` private (\n  val name:            String,\n  val RecordName:      Token[String],\n  val RecordType:      Route53RecordType,\n  val HostedZoneName:  Option[Token[String]],\n  val HostedZoneId:    Option[Token[String]],\n  val ResourceRecords: Option[Seq[Token[String]]] = None,\n  val TTL:             Option[Token[String]]      = None,\n  val AliasTarget:     Option[Route53AliasTarget] = None,\n  override val Condition: Option[ConditionRef]    = None\n  ) extends Resource[` AWS :: Route53 :: RecordSet `]{\n  ...\n  }\nobject ` AWS :: Route53 :: RecordSet ` {\n  ...\n  def generalRecord(...) = new ` AWS :: Route53 :: RecordSet `(...)\n  def aliasRecord(...) = new ` AWS :: Route53 :: RecordSet `(...) } So we try to do this in a variety of places in CFTG to ensure we are creating AWS resources that are “correct by construction.” Elsewhere, we use a more sophisticated pattern, that is obvious in Scala but we haven’t seen documented anywhere, that we call the “Valid Combo Pattern.” For this AWS resource: AWS::EC2::Route Creates a new route in a route table within a VPC. The route’s target can be either a gateway attached to the VPC or a NAT instance in the VPC. … GatewayId … Required: Conditional. You must specify only one of the following properties: GatewayId, InstanceId, NetworkInterfaceId, or VpcPeeringConnectionId. Here to avoid writing a long constructor method four times, we specify a set of implicits values, one for each valid combo, from a class with a private constructor that others cannot create new instances of: @implicitNotFound ( \"A Route can only have exactly ONE of GatewayId, InstanceId, NetworkInterfaceId or VpcPeeringConnectionId set\" ) class ValidRouteCombo [ G , I ] private () object ValidRouteCombo { implicit object valid1T extends ValidRouteCombo [ Some [ Token [ ResourceRef [ `AWS::EC2::InternetGateway` ]]] , None. type ] implicit object valid2T extends ValidRouteCombo [ None. type , Some [ Token [ ResourceRef [ `AWS::EC2::Instance` ]]]] ... } Then our factory method uses type parameters and implicits to make sure that a caller is filling in a valid set of Somes and Nones according to the defined implicits: object `AWS :: EC2 :: Route ` extends DefaultJsonProtocol {\n...\n  def apply[\n    G <: Option[Token[ResourceRef[` AWS :: EC2 :: InternetGateway `]]],\n    I <: Option[Token[ResourceRef[` AWS :: EC2 :: Instance `]]]\n  ](\n    name:                         String,\n    RouteTableId:                 Token[ResourceRef[` AWS :: EC2 :: RouteTable `]],\n    DestinationCidrBlock:         CidrBlock,\n    GatewayId:                    G = None,\n    InstanceId:                   I = None,\n    Condition: Option[ConditionRef] = None\n   )(implicit ev1: ValidRouteCombo[G, I]) = \n   \t\tnew ` AWS :: EC2 :: Route `( name , RouteTableId , DestinationCidrBlock , GatewayId , InstanceId , Condition ) } You can see the implicit parameter ev1 (evidence 1) witnesses that G,I are part of one of the valid combinations that we encoded above, namely that only one of the options can be a Some. Now we could implement the other ValidRouteCombo ’s for NetworkInterfaceId and VpcPeeringConnectionId each with one line of new code instead of 9 lines each of another constructor for each. Further, the Valid Combo Pattern encodes much more explicitly in the type system what is valid than the private constructor + factories approach. YAML Support In addition to our templates including lots of Amazon resources, they eventually accumulated many large and increasing complex CloudConfig configuration files to setup individual hosts. Some of the components formerly hand-coded in CloudFormation JSON were duplicated across many files. We support 2 features to abstract these common pieces and compose them back together: object LoggingSupport { private val logrotateYaml = \"/cloudconfig/logrotate.yaml\" private val logspoutYaml = \"/cloudconfig/logspout.yaml\" private val journalspewYaml = \"/cloudconfig/journalctl.yaml\" val loggingUnits = yaml \"\" // yes this is supposed to be \"\"\", but I can't figure out how to make Markdown happy -- $logspoutYaml -- $logrotateYaml \"\" } ... yaml \"\" // this one too |# cloud - config | | coreos : | units: | $ { LoggingSupport.loggingUnits } \"\" First, we provide a YAML string interpolator that allows you to compose bits of YAML together, accounting for indenting. You can see this in the bottom section where we include a YAML list into the CoreOS units map. Second, as you can see at the top, instead of a bit of YAML, you can instead provide a file path to your YAML files, in this case files under src/main/resources/cloudconfig, and the contents of that file will be transcluded (with indenting) into the interpolating YAML. Higher-Order Builders For simpler topologies, we have a set of convenience methods to more easily create common entities. For instance, many types of Resources, like AWS::EC2::Instance ’s require a constructor parameter for VPC and Subnet in which that instance will be contained. Our methods allow you to express a template in a way that visually resembles a diagram of your architecture. Logical/network containment is express in nested code blocks: withVpc ( ParameterRef ( vpcCidrParameter )){ implicit vpc => withAZ ( ParameterRef ( availabilityZone1Parameter )){ implicit az1 => withSubnet ( \"Public\" , 1 , ParameterRef ( publicSubnet1CidrParameter )){ implicit pubSubnet1 => ec2 ( \"myInstance\" , InstanceType = \"t2.micro\" , KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), Tags = AmazonTag . fromName ( \"myInstance\" ), UserData = None ) ) } ++ withSubnet ( \"Private\" , 1 , ParameterRef ( privateSubnet1CidrParameter )){ implicit priSubnet1 => ... } } ++ withAZ ( ParameterRef ( availabilityZone2Parameter )){ implicit az2 => withSubnet ( \"Public\" , 2 , ParameterRef ( publicSubnet2CidrParameter )){ implicit pubSubnet2 => ... } ++ withSubnet ( \"Private\" , 2 , ParameterRef ( privateSubnet2CidrParameter )){ implicit priSubnet2 => ... } } } This method above returns a Template whose JSON serialization is a CloudFormation template. The convenience methods withVPC , withAZ and withSubnet each take a few parameters, internally create a resource of the specified type and then take a block or function passing that resource along. Each of these expect you to return a Template and note that Template s can be composed with ++ . While withVPC is stand alone, some of the others also take implicit parameters, for instance: def withSubnet ( visibility : String , ordinal : Int , cidr : Token [ CidrBlock ]) ( f : ( `AWS::EC2::Subnet` ) => Template )( implicit vpc : `AWS::EC2::VPC` , az : AZ ) : Template = { ... } Subnets require a VPC and an AZ. To express this most succinctly, above we marked the passed along resources (vpc, az1, pubSubnet1, etc) as implicit . Turns out you can add the implicit modifier in front of parameters of lambdas in Scala, as in Seq(1,2,3).map(implicit x => x + 1) to bring x into implicit scope. Then, methods like ec2() , elb , asg() (autoscaling group) or securityGroup also use these implicit parameters so you dont have to keep passing them around: trait EC2 { def ec2 ( name : String , InstanceType : Token [ String ], KeyName : Token [ String ], ImageId : Token [ AMIId ], SecurityGroupIds : Seq [ ResourceRef [ `AWS::EC2::SecurityGroup` ]], Tags : Seq [ AmazonTag ], Metadata : Option [ Map [ String , String ]] = None , IamInstanceProfile : Option [ Token [ ResourceRef [ `AWS::IAM::InstanceProfile` ]]] = None , SourceDestCheck : Option [ String ] = None , UserData : Option [ `Fn::Base64` ] = None , Monitoring : Option [ Boolean ] = None , Volumes : Option [ Seq [ EC2MountPoint ]] = None , Condition : Option [ String ] = None )( implicit subnet : `AWS::EC2::Subnet` , vpc : `AWS::EC2::VPC` ) = // IMPLICITS! SecurityGroupRoutable from `AWS::EC2::Instance` ( name , InstanceType , KeyName , subnet , ImageId , Tags , SecurityGroupIds , Metadata , IamInstanceProfile , SourceDestCheck , UserData , Monitoring , Volumes ) } We also have a few methods to do things like create EIPs or CloudWatch alarms from EC2 instances as in: val myEIP = myEC2Instance . withEIP ( \"NAT1EIP\" ) val myCloudWatch = myEC2Instance . alarmOnSystemFailure ( \"NATInstanceAlarm\" , \"nat-instance\" ) val myEC2AndOutput = myEC2Instance . andOutput ( \"NAT1EIP\" , \"NAT 1 EIP\" ) You can see these and other RichXYZ method pimps in com.monsanto.arch.cloudformation.model.simple.Builders. SecurityGroupRoutables Above you might have noticed this sneaky SecurityGroupRoutable type. After using all of the techniques above, you’ll still be left with lots of code to make security groups and to associate various ones to various resources. We’ve begun to create a new abstraction, SecurityGroupRoutable , currently supporting AWS::EC2::Instance ’s, AWS::ElasticLoadBalancing::LoadBalancer ’s and AWS::AutoScaling::LaunchConfiguration ’s / AWS::AutoScaling::AutoScalingGroup ’s. Instead of manually defining security groups for logically similar instances, we discovered it was less work overall to specify ingress rules point to point between all necessary Resource instances (or to define them in a function). So a SecurityGroupRoutable[R <: Resource[R]] is a wrapper around a Resource that itself needs or can be associated with a security group, as well as an auto-generated security group specific for that Resource that is injected into the resource. In other words, every Resource that is wrapped in an SGR gets its own security group associated only to itself. Then instead of defining ingress/egress rules in the definition of that security group, we use the fact that CloudFormation also permits the definition of AWS::EC2::SecurityGroupEgress and AWS::EC2::SecurityGroupIngress rules as stand-alone resources that point to security groups. Several of the higher-level methods for creating Resource ’s above now return SGRs instead of bare Resource ’s. Note that SGRs have a .template method that returns a template containing both the Resource and its SecurityGroup . Ingress Rule DSL So now that we have methods to automate all of the above, the last major source of duplication and friction is in creating those Ingress and Egress rules between Security Groups or SecurityGroupRoutables . To solve this problem, we created a little DSL to specify the creation of many of these rules at once, and in an easy to grok syntax. A secondary goal was that these rules might be more easily audited by a non-developer security team. val securityGroupA = securityGroup ( \"A\" , \"Group A\" ) val securityGroupB = securityGroup ( \"B\" , \"Group B\" ) securityGroupA ->- 22 ->- securityGroupB securityGroupA ->- 22 / UDP ->- securityGroupB securityGroupA ->- 22 / ICMP ->- securityGroupB securityGroupA ->- 22 / ALL ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) / ICMP ->- securityGroupB securityGroupA ->- Seq ( 22 , 5601 ) ->- securityGroupB securityGroupA ->- Seq ( 22 , ( 5601 to 5602 ) / UDP , 45 / ICMP , 14 / TCP ) ->- securityGroupB securityGroupA ->- 22 -<- securityGroupB securityGroupA -<- 22 ->- securityGroupB So yes, we do have this one symbolic operator in CFTG, and we read it as “flow.” Each expression like x ->- 22 / TCP ->- y generates a list of ingress rules, in this case it generates a single rule that allows ingress traffic on port 22 over the TCP protocol from security group x to security group y . As you can see above, you can leave off / TCP as it is the default. We also support other protocols such as ` / UDP or / ICMP or the wildcard / ALL . We read / as \"over,\" as in \"22 over TCP\" like you'd say \"JSON over HTTP.\" (OK so I guess we have two, TWO symbolic operators.) We also support port ranges, which use Scala ranges of ports like (1 to 1024) , as well as cherry-picked sequences of ports like Seq(22, 5601)`. Ranges also support protocols, and sequence elements can both have protocols and themselves be nested ranges. You can flow in either direction, because who wants to have to remember which ways the arrows are “supposed to go”, meaning you can write the same ingress rule as either: a ->- 22 ->- b //or b -<- 22 -<- a More importantly, you can use arrows facing in opposite directions to express bi-directional flows, meaning a pair of ingress rule lists, allowing ingress into each of the two security groups: // either server can accept SSH traffic from the other a ->- 22 -<- b //same as a -<- 22 ->- b Summary Clearly this library is far from complete. Writing this post, it is obvious that we should replace Fn::If ’s first parameter (now a string Condition name) with a ConditionRef , it would be nice if we generated Egress rules in addition to Ingress rules from “flows,” etc. And if anyone has a great idea for modeling cross-cutting concerns like ASGs in a beautiful way, pull requests are always welcome! We hope though that CFTG is sufficiently advanced that you can use it in your work. We certainly use it to generate our complex Microservice environment templates at Monsanto, and are delighted to share it with all of you. posted on July 10, 2015 by Ryan Richt ← Previous Post Next Post → \"MyAutoScaleGroup\" : { \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\" , \"Properties\" : { \"LoadBalancerNames\" : [{ \"Ref\" : \"MyLoadBalancerA\" }, { \"Ref\" : \"MyLoadBalancerB\" }], ... \"MyAutoScaleGroup\" : { \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\" , \"Properties\" : { \"LoadBalancerNames\" : [{ \"Ref\" : \"MyLoadBalancerA\" }, { \"Ref\" : \"MyLoadBalancerB\" }], ... case class `AWS :: EC2 :: Subnet `(\n  name:             String,\n  VpcId:            Token[ResourceRef[` AWS :: EC2 :: VPC `]],\n  AvailabilityZone: Token[String],\n  CidrBlock:        Token[CidrBlock],\n  Tags:             Seq[AmazonTag],\n  override val Condition: Option[ConditionRef] = None\n  ) extends Resource[` AWS :: EC2 :: Subnet `]{\n\n  def when(newCondition: Option[ConditionRef] = Condition) = copy(Condition = newCondition)\n}\nobject ` AWS :: EC2 :: Subnet ` extends DefaultJsonProtocol {\n  implicit val format: JsonFormat[` AWS :: EC2 :: Subnet `] = jsonFormat6(` AWS :: EC2 :: Subnet `. apply ) } case class `AWS :: EC2 :: Subnet `(\n  name:             String,\n  VpcId:            Token[ResourceRef[` AWS :: EC2 :: VPC `]],\n  AvailabilityZone: Token[String],\n  CidrBlock:        Token[CidrBlock],\n  Tags:             Seq[AmazonTag],\n  override val Condition: Option[ConditionRef] = None\n  ) extends Resource[` AWS :: EC2 :: Subnet `]{\n\n  def when(newCondition: Option[ConditionRef] = Condition) = copy(Condition = newCondition)\n}\nobject ` AWS :: EC2 :: Subnet ` extends DefaultJsonProtocol {\n  implicit val format: JsonFormat[` AWS :: EC2 :: Subnet `] = jsonFormat6(` AWS :: EC2 :: Subnet `. apply ) } import com.monsanto.arch.cloudformation.model.simple.Builders._ object Example { ... val conditionalResource = when ( someCondition ){ someResource } } import com.monsanto.arch.cloudformation.model.simple.Builders._ object Example { ... val conditionalResource = when ( someCondition ){ someResource } } case class IPAddressSegment ( value : Int ){ require ( value <= 255 && value >= 0 ) } object IPAddressSegment { implicit def fromInt ( i : Int ) : IPAddressSegment = IPAddressSegment ( i ) } case class IPMask ( value : Int ){ require ( value <= 32 && value >= 0 ) } object IPMask { implicit def fromInt ( i : Int ) : IPMask = IPMask ( i ) } case class CidrBlock ( a : IPAddressSegment , b : IPAddressSegment , c : IPAddressSegment , d : IPAddressSegment , mask : IPMask ) ... case class IPAddressSegment ( value : Int ){ require ( value <= 255 && value >= 0 ) } object IPAddressSegment { implicit def fromInt ( i : Int ) : IPAddressSegment = IPAddressSegment ( i ) } case class IPMask ( value : Int ){ require ( value <= 32 && value >= 0 ) } object IPMask { implicit def fromInt ( i : Int ) : IPMask = IPMask ( i ) } case class CidrBlock ( a : IPAddressSegment , b : IPAddressSegment , c : IPAddressSegment , d : IPAddressSegment , mask : IPMask ) ... val myBlock = CidrBlock ( 10 , 10 , 0 , 0 , 16 ) val myBlock = CidrBlock ( 10 , 10 , 0 , 0 , 16 ) val myBlock = CidrBlock ( IPAddressSegment ( 10 ), IPAddressSegment ( 10 ), IPAddressSegment ( 0 ), IPAddressSegment ( 0 ), IPMask ( 16 ) ) val myBlock = CidrBlock ( IPAddressSegment ( 10 ), IPAddressSegment ( 10 ), IPAddressSegment ( 0 ), IPAddressSegment ( 0 ), IPMask ( 16 ) ) case class `Fn::If` [ R : JsonFormat ]( conditionName : Token [ String ], valIfTrue : Token [ R ], valIfFalse : Token [ R ] ) extends AmazonFunctionCall [ R ]( \"Fn::If\" ){ type CFBackingType = ( Token [ String ], Token [ R ], Token [ R ]) val arguments = ( conditionName , valIfTrue , valIfFalse ) ... } case class `Fn::If` [ R : JsonFormat ]( conditionName : Token [ String ], valIfTrue : Token [ R ], valIfFalse : Token [ R ] ) extends AmazonFunctionCall [ R ]( \"Fn::If\" ){ type CFBackingType = ( Token [ String ], Token [ R ], Token [ R ]) val arguments = ( conditionName , valIfTrue , valIfFalse ) ... } val gatewayServiceELBSecGroup = // Security group with 80 ingress rules val gatewayServiceELBSSLSecGroup = // Security group with 443 ingress rules val serviceElbOrElbSSL : Token [ ResourceRef [ `AWS::EC2::SecurityGroup` ]] = `Fn::If` [ ResourceRef [ `AWS::EC2::SecurityGroup` ]]( \"ServiceELBSSLCertNameIsNotDefined\" , gatewayServiceELBSecGroup , gatewayServiceELBSSLSecGroup ) val gatewayServiceELBSecGroup = // Security group with 80 ingress rules val gatewayServiceELBSSLSecGroup = // Security group with 443 ingress rules val serviceElbOrElbSSL : Token [ ResourceRef [ `AWS::EC2::SecurityGroup` ]] = `Fn::If` [ ResourceRef [ `AWS::EC2::SecurityGroup` ]]( \"ServiceELBSSLCertNameIsNotDefined\" , gatewayServiceELBSecGroup , gatewayServiceELBSSLSecGroup ) sealed trait Token [ R ] object Token extends DefaultJsonProtocol { implicit def fromAny [ R: JsonFormat ]( r : R ) : AnyToken [ R ] = AnyToken ( r ) implicit def fromOptionAny [ R: JsonFormat ]( or : Option [ R ]) : Option [ AnyToken [ R ]] = or . map ( r => Token . fromAny ( r )) implicit def fromString ( s : String ) : StringToken = StringToken ( s ) implicit def fromFunction [ R ]( f : AmazonFunctionCall [ R ]) : FunctionCallToken [ R ] = FunctionCallToken [ R ]( f ) implicit def fromSome [ R ]( oR : Some [ R ])( implicit ev1 : R => Token [ R ]) : Some [ Token [ R ]] = oR . map ( ev1 ). asInstanceOf [ Some [ Token [ R ]]] implicit def fromOption [ R ]( oR : Option [ R ])( implicit ev1 : R => Token [ R ]) : Option [ Token [ R ]] = oR . map ( ev1 ) implicit def fromResource [ R <: Resource [ R ]]( r : R )( implicit conv : ( R ) => ResourceRef [ R ]) : Token [ ResourceRef [ R ]] = fromAny ( conv ( r )) implicit def fromSeq [ R <: Resource [ R ]]( sR : Seq [ R ])( implicit toRef : R => ResourceRef [ R ]) : Seq [ Token [ ResourceRef [ R ]]] = sR . map ( r => fromAny ( toRef ( r ))) sealed trait Token [ R ] object Token extends DefaultJsonProtocol { implicit def fromAny [ R: JsonFormat ]( r : R ) : AnyToken [ R ] = AnyToken ( r ) implicit def fromOptionAny [ R: JsonFormat ]( or : Option [ R ]) : Option [ AnyToken [ R ]] = or . map ( r => Token . fromAny ( r )) implicit def fromString ( s : String ) : StringToken = StringToken ( s ) implicit def fromFunction [ R ]( f : AmazonFunctionCall [ R ]) : FunctionCallToken [ R ] = FunctionCallToken [ R ]( f ) implicit def fromSome [ R ]( oR : Some [ R ])( implicit ev1 : R => Token [ R ]) : Some [ Token [ R ]] = oR . map ( ev1 ). asInstanceOf [ Some [ Token [ R ]]] implicit def fromOption [ R ]( oR : Option [ R ])( implicit ev1 : R => Token [ R ]) : Option [ Token [ R ]] = oR . map ( ev1 ) implicit def fromResource [ R <: Resource [ R ]]( r : R )( implicit conv : ( R ) => ResourceRef [ R ]) : Token [ ResourceRef [ R ]] = fromAny ( conv ( r )) implicit def fromSeq [ R <: Resource [ R ]]( sR : Seq [ R ])( implicit toRef : R => ResourceRef [ R ]) : Seq [ Token [ ResourceRef [ R ]]] = sR . map ( r => fromAny ( toRef ( r ))) class `AWS :: Route53 :: RecordSet ` private (\n  val name:            String,\n  val RecordName:      Token[String],\n  val RecordType:      Route53RecordType,\n  val HostedZoneName:  Option[Token[String]],\n  val HostedZoneId:    Option[Token[String]],\n  val ResourceRecords: Option[Seq[Token[String]]] = None,\n  val TTL:             Option[Token[String]]      = None,\n  val AliasTarget:     Option[Route53AliasTarget] = None,\n  override val Condition: Option[ConditionRef]    = None\n  ) extends Resource[` AWS :: Route53 :: RecordSet `]{\n  ...\n  }\nobject ` AWS :: Route53 :: RecordSet ` {\n  ...\n  def generalRecord(...) = new ` AWS :: Route53 :: RecordSet `(...)\n  def aliasRecord(...) = new ` AWS :: Route53 :: RecordSet `(...) } class `AWS :: Route53 :: RecordSet ` private (\n  val name:            String,\n  val RecordName:      Token[String],\n  val RecordType:      Route53RecordType,\n  val HostedZoneName:  Option[Token[String]],\n  val HostedZoneId:    Option[Token[String]],\n  val ResourceRecords: Option[Seq[Token[String]]] = None,\n  val TTL:             Option[Token[String]]      = None,\n  val AliasTarget:     Option[Route53AliasTarget] = None,\n  override val Condition: Option[ConditionRef]    = None\n  ) extends Resource[` AWS :: Route53 :: RecordSet `]{\n  ...\n  }\nobject ` AWS :: Route53 :: RecordSet ` {\n  ...\n  def generalRecord(...) = new ` AWS :: Route53 :: RecordSet `(...)\n  def aliasRecord(...) = new ` AWS :: Route53 :: RecordSet `(...) } @implicitNotFound ( \"A Route can only have exactly ONE of GatewayId, InstanceId, NetworkInterfaceId or VpcPeeringConnectionId set\" ) class ValidRouteCombo [ G , I ] private () object ValidRouteCombo { implicit object valid1T extends ValidRouteCombo [ Some [ Token [ ResourceRef [ `AWS::EC2::InternetGateway` ]]] , None. type ] implicit object valid2T extends ValidRouteCombo [ None. type , Some [ Token [ ResourceRef [ `AWS::EC2::Instance` ]]]] ... } @implicitNotFound ( \"A Route can only have exactly ONE of GatewayId, InstanceId, NetworkInterfaceId or VpcPeeringConnectionId set\" ) class ValidRouteCombo [ G , I ] private () object ValidRouteCombo { implicit object valid1T extends ValidRouteCombo [ Some [ Token [ ResourceRef [ `AWS::EC2::InternetGateway` ]]] , None. type ] implicit object valid2T extends ValidRouteCombo [ None. type , Some [ Token [ ResourceRef [ `AWS::EC2::Instance` ]]]] ... } object `AWS :: EC2 :: Route ` extends DefaultJsonProtocol {\n...\n  def apply[\n    G <: Option[Token[ResourceRef[` AWS :: EC2 :: InternetGateway `]]],\n    I <: Option[Token[ResourceRef[` AWS :: EC2 :: Instance `]]]\n  ](\n    name:                         String,\n    RouteTableId:                 Token[ResourceRef[` AWS :: EC2 :: RouteTable `]],\n    DestinationCidrBlock:         CidrBlock,\n    GatewayId:                    G = None,\n    InstanceId:                   I = None,\n    Condition: Option[ConditionRef] = None\n   )(implicit ev1: ValidRouteCombo[G, I]) = \n   \t\tnew ` AWS :: EC2 :: Route `( name , RouteTableId , DestinationCidrBlock , GatewayId , InstanceId , Condition ) } object `AWS :: EC2 :: Route ` extends DefaultJsonProtocol {\n...\n  def apply[\n    G <: Option[Token[ResourceRef[` AWS :: EC2 :: InternetGateway `]]],\n    I <: Option[Token[ResourceRef[` AWS :: EC2 :: Instance `]]]\n  ](\n    name:                         String,\n    RouteTableId:                 Token[ResourceRef[` AWS :: EC2 :: RouteTable `]],\n    DestinationCidrBlock:         CidrBlock,\n    GatewayId:                    G = None,\n    InstanceId:                   I = None,\n    Condition: Option[ConditionRef] = None\n   )(implicit ev1: ValidRouteCombo[G, I]) = \n   \t\tnew ` AWS :: EC2 :: Route `( name , RouteTableId , DestinationCidrBlock , GatewayId , InstanceId , Condition ) } object LoggingSupport { private val logrotateYaml = \"/cloudconfig/logrotate.yaml\" private val logspoutYaml = \"/cloudconfig/logspout.yaml\" private val journalspewYaml = \"/cloudconfig/journalctl.yaml\" val loggingUnits = yaml \"\" // yes this is supposed to be \"\"\", but I can't figure out how to make Markdown happy -- $logspoutYaml -- $logrotateYaml \"\" } ... yaml \"\" // this one too |# cloud - config | | coreos : | units: | $ { LoggingSupport.loggingUnits } \"\" object LoggingSupport { private val logrotateYaml = \"/cloudconfig/logrotate.yaml\" private val logspoutYaml = \"/cloudconfig/logspout.yaml\" private val journalspewYaml = \"/cloudconfig/journalctl.yaml\" val loggingUnits = yaml \"\" // yes this is supposed to be \"\"\", but I can't figure out how to make Markdown happy -- $logspoutYaml -- $logrotateYaml \"\" } ... yaml \"\" // this one too |# cloud - config | | coreos : | units: | $ { LoggingSupport.loggingUnits } \"\" withVpc ( ParameterRef ( vpcCidrParameter )){ implicit vpc => withAZ ( ParameterRef ( availabilityZone1Parameter )){ implicit az1 => withSubnet ( \"Public\" , 1 , ParameterRef ( publicSubnet1CidrParameter )){ implicit pubSubnet1 => ec2 ( \"myInstance\" , InstanceType = \"t2.micro\" , KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), Tags = AmazonTag . fromName ( \"myInstance\" ), UserData = None ) ) } ++ withSubnet ( \"Private\" , 1 , ParameterRef ( privateSubnet1CidrParameter )){ implicit priSubnet1 => ... } } ++ withAZ ( ParameterRef ( availabilityZone2Parameter )){ implicit az2 => withSubnet ( \"Public\" , 2 , ParameterRef ( publicSubnet2CidrParameter )){ implicit pubSubnet2 => ... } ++ withSubnet ( \"Private\" , 2 , ParameterRef ( privateSubnet2CidrParameter )){ implicit priSubnet2 => ... } } } withVpc ( ParameterRef ( vpcCidrParameter )){ implicit vpc => withAZ ( ParameterRef ( availabilityZone1Parameter )){ implicit az1 => withSubnet ( \"Public\" , 1 , ParameterRef ( publicSubnet1CidrParameter )){ implicit pubSubnet1 => ec2 ( \"myInstance\" , InstanceType = \"t2.micro\" , KeyName = ParameterRef ( keyNameParameter ), ImageId = `Fn::FindInMap` [ AMIId ]( MappingRef ( amazonLinuxAMIMapping ), `AWS::Region` , \"AMI\" ), SecurityGroupIds = Seq (), Tags = AmazonTag . fromName ( \"myInstance\" ), UserData = None ) ) } ++ withSubnet ( \"Private\" , 1 , ParameterRef ( privateSubnet1CidrParameter )){ implicit priSubnet1 => ... } } ++ withAZ ( ParameterRef ( availabilityZone2Parameter )){ implicit az2 => withSubnet ( \"Public\" , 2 , ParameterRef ( publicSubnet2CidrParameter )){ implicit pubSubnet2 => ... } ++ withSubnet ( \"Private\" , 2 , ParameterRef ( privateSubnet2CidrParameter )){ implicit priSubnet2 => ... } } } def withSubnet ( visibility : String , ordinal : Int , cidr : Token [ CidrBlock ]) ( f : ( `AWS::EC2::Subnet` ) => Template )( implicit vpc : `AWS::EC2::VPC` , az : AZ ) : Template = { ... } def withSubnet ( visibility : String , ordinal : Int , cidr : Token [ CidrBlock ]) ( f : ( `AWS::EC2::Subnet` ) => Template )( implicit vpc : `AWS::EC2::VPC` , az : AZ ) : Template = { ... } trait EC2 { def ec2 ( name : String , InstanceType : Token [ String ], KeyName : Token [ String ], ImageId : Token [ AMIId ], SecurityGroupIds : Seq [ ResourceRef [ `AWS::EC2::SecurityGroup` ]], Tags : Seq [ AmazonTag ], Metadata : Option [ Map [ String , String ]] = None , IamInstanceProfile : Option [ Token [ ResourceRef [ `AWS::IAM::InstanceProfile` ]]] = None , SourceDestCheck : Option [ String ] = None , UserData : Option [ `Fn::Base64` ] = None , Monitoring : Option [ Boolean ] = None , Volumes : Option [ Seq [ EC2MountPoint ]] = None , Condition : Option [ String ] = None )( implicit subnet : `AWS::EC2::Subnet` , vpc : `AWS::EC2::VPC` ) = // IMPLICITS! SecurityGroupRoutable from `AWS::EC2::Instance` ( name , InstanceType , KeyName , subnet , ImageId , Tags , SecurityGroupIds , Metadata , IamInstanceProfile , SourceDestCheck , UserData , Monitoring , Volumes ) } trait EC2 { def ec2 ( name : String , InstanceType : Token [ String ], KeyName : Token [ String ], ImageId : Token [ AMIId ], SecurityGroupIds : Seq [ ResourceRef [ `AWS::EC2::SecurityGroup` ]], Tags : Seq [ AmazonTag ], Metadata : Option [ Map [ String , String ]] = None , IamInstanceProfile : Option [ Token [ ResourceRef [ `AWS::IAM::InstanceProfile` ]]] = None , SourceDestCheck : Option [ String ] = None , UserData : Option [ `Fn::Base64` ] = None , Monitoring : Option [ Boolean ] = None , Volumes : Option [ Seq [ EC2MountPoint ]] = None , Condition : Option [ String ] = None )( implicit subnet : `AWS::EC2::Subnet` , vpc : `AWS::EC2::VPC` ) = // IMPLICITS! SecurityGroupRoutable from `AWS::EC2::Instance` ( name , InstanceType , KeyName , subnet , ImageId , Tags , SecurityGroupIds , Metadata , IamInstanceProfile , SourceDestCheck , UserData , Monitoring , Volumes ) } val myEIP = myEC2Instance . withEIP ( \"NAT1EIP\" ) val myCloudWatch = myEC2Instance . alarmOnSystemFailure ( \"NATInstanceAlarm\" , \"nat-instance\" ) val myEC2AndOutput = myEC2Instance . andOutput ( \"NAT1EIP\" , \"NAT 1 EIP\" ) val myEIP = myEC2Instance . withEIP ( \"NAT1EIP\" ) val myCloudWatch = myEC2Instance . alarmOnSystemFailure ( \"NATInstanceAlarm\" , \"nat-instance\" ) val myEC2AndOutput = myEC2Instance . andOutput ( \"NAT1EIP\" , \"NAT 1 EIP\" ) val securityGroupA = securityGroup ( \"A\" , \"Group A\" ) val securityGroupB = securityGroup ( \"B\" , \"Group B\" ) securityGroupA ->- 22 ->- securityGroupB securityGroupA ->- 22 / UDP ->- securityGroupB securityGroupA ->- 22 / ICMP ->- securityGroupB securityGroupA ->- 22 / ALL ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) / ICMP ->- securityGroupB securityGroupA ->- Seq ( 22 , 5601 ) ->- securityGroupB securityGroupA ->- Seq ( 22 , ( 5601 to 5602 ) / UDP , 45 / ICMP , 14 / TCP ) ->- securityGroupB securityGroupA ->- 22 -<- securityGroupB securityGroupA -<- 22 ->- securityGroupB val securityGroupA = securityGroup ( \"A\" , \"Group A\" ) val securityGroupB = securityGroup ( \"B\" , \"Group B\" ) securityGroupA ->- 22 ->- securityGroupB securityGroupA ->- 22 / UDP ->- securityGroupB securityGroupA ->- 22 / ICMP ->- securityGroupB securityGroupA ->- 22 / ALL ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) ->- securityGroupB securityGroupA ->- ( 1 to 65536 ) / ICMP ->- securityGroupB securityGroupA ->- Seq ( 22 , 5601 ) ->- securityGroupB securityGroupA ->- Seq ( 22 , ( 5601 to 5602 ) / UDP , 45 / ICMP , 14 / TCP ) ->- securityGroupB securityGroupA ->- 22 -<- securityGroupB securityGroupA -<- 22 ->- securityGroupB a ->- 22 ->- b //or b -<- 22 -<- a a ->- 22 ->- b //or b -<- 22 -<- a // either server can accept SSH traffic from the other a ->- 22 -<- b //same as a -<- 22 ->- b // either server can accept SSH traffic from the other a ->- 22 -<- b //same as a -<- 22 ->- b posted on July 10, 2015 by Ryan Richt Ryan Richt", "date": "2015-07-10"},
{"website": "Monsanto", "title": "Stax", "author": ["\n                                        Phil Cryer\n                                    "], "link": "http://engineering.monsanto.com/2015/07/08/stax/", "abstract": "When we were looking to build out an infrastructure in AWS ( Amazon Web Services ), we took some time to understand how the aws-cli (Universal Command Line Interface for Amazon Web Services) worked. With ideas from other projects found on GitHub , we started wrapping commands in simple BASH scripts, and pointed it to CloudFormation templates written in JSON. From there stax was born, and we have released it as open source under the BSD 3-clause license . It’s easy to get started with stax to create and manage CloudFormation stacks (aka stax) in AWS ( Amazon Web Services ) and the project provides several CloudFormation templates, or you can use your own. For launching new AWS instances with stax, you can use Linux (Debian GNU/Linux 7 and Ubuntu 14.04 are supported) and Apple OS X (tested on 10.10 and 10.9). When running with a default template, you will end up with a VPC setup like the following: NOTICE: As you can see, stax creates an AWS environment from scratch, so it needs to be run under an IAM user or role with permissions to create all AWS resources in your template, including potentially IAM roles. You should understand the risk associated with giving users these permissions in your AWS account before using stax. CoreOS hosts run Docker containers on the Gateway and Service layer, while Amazon Linux instances serve as NAT devices to direct traffic for the internal hosts. Additionally, one of the biggest requests we had was to simplify the connectivity to nodes once the stax was created. For this we have the stax connect function, that will ssh through the bastion/ssh host, and log you on to either a host on the gateway layer, or the services layer. The Gateway layer: $ ./stax connect gateway\n     _\n    | | every cloud has a silicon lining\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  getting public IP (EIP)\n[ ---- ]    public IP (EIP): 52.6.248.224\n[ ---- ]    writing to /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.bastion\n[ ---- ]  get gateway and service IPs\n[ ---- ]    querying aws for gateway hosts\n[ ---- ]    getting service leaders\n[ ---- ]    querying aws for service hosts\n[ ---- ]  creating ssh_config\n[ ---- ]    writing /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]    created /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]  connecting to stax: gateway\nLast login: Tue Apr 28 18:57:07 2015 from 10.183.1.208\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-131 ~ $ So you’re logged into a gateway CoreOS box, and from the current /etc/motd you can see what version of CoreOS you are running. Now we’ll login to the Service layer: $ ./stax connect service\n     _\n    | | all glory to the hypnotoad!\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  connecting to stax: service\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-111 ~ $ Keeping in mind that this is only available when you’re in development, no external access is allowed to internal nodes directly without the bastion/ssh host, and shutting it down when running in production is highly recommended. Lastly a quick run of stax without arguments will show you the usage, giving you an idea of all lf the tasks the tool is capable of: $ ./stax\nUsage: stax [OPTIONS] COMMAND\n\nOptions:\n  -c,--config=CONFIG       Use file CONFIG rather than config/vpc-default.json\n  -d,--debug               Turn on verbose messages\n  -h,--help                Output this message\n  -j,--jump=IP             SSH through host with IP address IP\n  -m,--module=MOD          Use config/MOD.json and template/MOD.json\n  -t,--template=TEMPLATE   Use file TEMPLATE rather than template/vpc-default.json\n  -v,--version             Print name and version information\n  -y,--yes                 Do not prompt for confirmation\n\nIf an argument is required for a long option, so to the short. Same for\noptional arguments.\n\nCommands:\n  add                Add functionality to an existing VPC\n  check              Run various tests against an existing stax\n  connect [TARGET]   Connect to bastion|gateway|service in the VPC stax over SSH\n  create             Create a new VPC stax in AWS\n  describe           Describe the stax created from this host\n  delete             Delete the existing VPC stax\n  dockerip-update    Fetch docker IP addresses and update related files\n  fleet              Run various fleetctl commands against the fleet cluster\n  help               Output this message\n  history            View history of recently created/deleted stax\n  list               List all completely built and running stax\n  rds PASSWORD       Create an RDS instance in the DB subnet\n  rds-delete RDSIN   Delete RDS instance RDSIN\n  remove ADD         Remove the previously added ADD\n  services           List servers that are available to run across a stax\n  start SERVICE      Start service SERVICE in the fleet cluster\n  test               Automated test to exercise functionality of stax\n  validate           Validate CloudFormation template\n\nFor more help, check the docs: https://github.com/MonsantoCo/stax To try it out, just clone the repo and follow the documentation in the readme. git clone https://github.com/MonsantoCo/stax\nmore stax/README.md We hope that this tool will be useful to the community and look forward to feedback on what people think of it. We’re open to issues you might create or pull requests to make stax better. posted on July 8, 2015 by Phil Cryer ← Previous Post Next Post → $ ./stax connect gateway\n     _\n    | | every cloud has a silicon lining\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  getting public IP (EIP)\n[ ---- ]    public IP (EIP): 52.6.248.224\n[ ---- ]    writing to /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.bastion\n[ ---- ]  get gateway and service IPs\n[ ---- ]    querying aws for gateway hosts\n[ ---- ]    getting service leaders\n[ ---- ]    querying aws for service hosts\n[ ---- ]  creating ssh_config\n[ ---- ]    writing /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]    created /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]  connecting to stax: gateway\nLast login: Tue Apr 28 18:57:07 2015 from 10.183.1.208\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-131 ~ $ $ ./stax connect gateway\n     _\n    | | every cloud has a silicon lining\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  getting public IP (EIP)\n[ ---- ]    public IP (EIP): 52.6.248.224\n[ ---- ]    writing to /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.bastion\n[ ---- ]  get gateway and service IPs\n[ ---- ]    querying aws for gateway hosts\n[ ---- ]    getting service leaders\n[ ---- ]    querying aws for service hosts\n[ ---- ]  creating ssh_config\n[ ---- ]    writing /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]    created /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.ssh_config\n[ ---- ]  connecting to stax: gateway\nLast login: Tue Apr 28 18:57:07 2015 from 10.183.1.208\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-131 ~ $ $ ./stax connect service\n     _\n    | | all glory to the hypnotoad!\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  connecting to stax: service\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-111 ~ $ $ ./stax connect service\n     _\n    | | all glory to the hypnotoad!\n ___| |_ __ ___  __\n/ __| __/ _` \\ \\/ /\n\\__ \\ || (_| |>  <\n|___/\\__\\__,_/_/\\_\\  0.9\n\n[ ---- ]  checking if stax build is complete\n[ ---- ]  describe stax\n[ NAME ]  vpc-stax-42179-unrepulsed\n[ ---- ]    querying aws\n[ ---- ]    query complete\n[ ---- ]    see /home/phil/devel/stax/run/vpc-stax-42179-unrepulsed.json for details\n[ ---- ]    stack vpc-stax-42179-unrepulsed build complete\n[ ---- ]  connecting to stax: service\nCoreOS stable (633.1.0)\ncore@ip-10-183-0-111 ~ $ $ ./stax\nUsage: stax [OPTIONS] COMMAND\n\nOptions:\n  -c,--config=CONFIG       Use file CONFIG rather than config/vpc-default.json\n  -d,--debug               Turn on verbose messages\n  -h,--help                Output this message\n  -j,--jump=IP             SSH through host with IP address IP\n  -m,--module=MOD          Use config/MOD.json and template/MOD.json\n  -t,--template=TEMPLATE   Use file TEMPLATE rather than template/vpc-default.json\n  -v,--version             Print name and version information\n  -y,--yes                 Do not prompt for confirmation\n\nIf an argument is required for a long option, so to the short. Same for\noptional arguments.\n\nCommands:\n  add                Add functionality to an existing VPC\n  check              Run various tests against an existing stax\n  connect [TARGET]   Connect to bastion|gateway|service in the VPC stax over SSH\n  create             Create a new VPC stax in AWS\n  describe           Describe the stax created from this host\n  delete             Delete the existing VPC stax\n  dockerip-update    Fetch docker IP addresses and update related files\n  fleet              Run various fleetctl commands against the fleet cluster\n  help               Output this message\n  history            View history of recently created/deleted stax\n  list               List all completely built and running stax\n  rds PASSWORD       Create an RDS instance in the DB subnet\n  rds-delete RDSIN   Delete RDS instance RDSIN\n  remove ADD         Remove the previously added ADD\n  services           List servers that are available to run across a stax\n  start SERVICE      Start service SERVICE in the fleet cluster\n  test               Automated test to exercise functionality of stax\n  validate           Validate CloudFormation template\n\nFor more help, check the docs: https://github.com/MonsantoCo/stax $ ./stax\nUsage: stax [OPTIONS] COMMAND\n\nOptions:\n  -c,--config=CONFIG       Use file CONFIG rather than config/vpc-default.json\n  -d,--debug               Turn on verbose messages\n  -h,--help                Output this message\n  -j,--jump=IP             SSH through host with IP address IP\n  -m,--module=MOD          Use config/MOD.json and template/MOD.json\n  -t,--template=TEMPLATE   Use file TEMPLATE rather than template/vpc-default.json\n  -v,--version             Print name and version information\n  -y,--yes                 Do not prompt for confirmation\n\nIf an argument is required for a long option, so to the short. Same for\noptional arguments.\n\nCommands:\n  add                Add functionality to an existing VPC\n  check              Run various tests against an existing stax\n  connect [TARGET]   Connect to bastion|gateway|service in the VPC stax over SSH\n  create             Create a new VPC stax in AWS\n  describe           Describe the stax created from this host\n  delete             Delete the existing VPC stax\n  dockerip-update    Fetch docker IP addresses and update related files\n  fleet              Run various fleetctl commands against the fleet cluster\n  help               Output this message\n  history            View history of recently created/deleted stax\n  list               List all completely built and running stax\n  rds PASSWORD       Create an RDS instance in the DB subnet\n  rds-delete RDSIN   Delete RDS instance RDSIN\n  remove ADD         Remove the previously added ADD\n  services           List servers that are available to run across a stax\n  start SERVICE      Start service SERVICE in the fleet cluster\n  test               Automated test to exercise functionality of stax\n  validate           Validate CloudFormation template\n\nFor more help, check the docs: https://github.com/MonsantoCo/stax git clone https://github.com/MonsantoCo/stax\nmore stax/README.md git clone https://github.com/MonsantoCo/stax\nmore stax/README.md posted on July 8, 2015 by Phil Cryer Phil Cryer", "date": "2015-07-08"},
{"website": "Monsanto", "title": "Doing Docker Metrics", "author": ["\n                                        Stuart Wong\n                                    "], "link": "http://engineering.monsanto.com/2015/06/17/docker-metrics/", "abstract": "As our usage of Docker grows and we provision more container hosts, collecting metrics and monitoring containers and hosts has become a necessity. This post will walk you through how to glue together a few components to deploy a monitoring solution for Docker. All components are intentionally plug-and-play, so if things need to be changed any component can be (relatively) easily swapped out for an alternative. Some assumptions First, we assume that Docker is installed, configured and running on your hosts. We further assume that you can connect to your Docker hosts with a web browser. It’s worth noting some of our other requirements at this point since there may be some questions as to why certain decisions were made. Ease of deployment - We believe in fail fast, and succeed fast. We wanted an immediate solution to get up and running quickly, run some tests and make a quick, but informed decision. Scalability - Success tends to have its own set of problems, namely scale. The solution needed to scale at all levels, handling both data velocity, volume and query demands. Availability - This goes without saying since we plan on doing notifications from the system and not just support analysis and planning. With that said, lets get to the good stuff! Components for Docker Metrics and Monitoring There are three components to our Docker monitoring setup: cAdvisor, InfluxDB, and Grafana. cAdvisor or Container Advisor - A Google developed tool that provides host and container metrics. It’s a host daemon that collects, aggregates, processes and exports information about running containers. We run it within a container on each host with various host volumes exposed which allow it to collect metrics from the Docker containers running on the host. It provides an available dashboard with a default 60 second data aggregation interval. It has a few options for back ends which can be used to store the data for longer term retrieval and analysis. InfluxDB - An open source distributed time series database which we use for longer term storage and analysis of the data from cAdvisor. It’s still under development but works fine and does have scale-out capabilities, though for the time being we are using a single container instance until we can upgrade to a stable 0.9.x version.\nWe would have liked to standardize on the soon to be released 0.9.x version, however, at this time cAdvisor does not yet support the InfluxDB 0.9.x API . Though there are other non-completed features which also affect the UI component, this is the primary reason we will be using the 0.8.x version at this time. Grafana - A nice open source web based UI which allows us to visualize all the metrics data. We create various dashboards which allow us to run queries against InfluxDB and chart them accordingly in a very nice layout. Putting things together Now that you have an overview of the different components lets put things together. We start first with our InfluxDB container using the below fleet/systemd unit: [Unit]\nDescription=InfluxDB Service\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull tutum/influxdb:latest\n\nExecStart=/usr/bin/docker run --publish=8083:8083 --publish=8086:8086 \\\n  --name=%p tutum/influxdb:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineID=[your_machine_ID] You can just extract and use the above docker run command if you prefer. I provide the above systemd unit for a more complete, “production-ready” example. You will notice the [X-Fleet] section is specific to fleet , which is our container scheduler on CoreOS hosts (hence the MachineID which comes from cat /etc/machine-id ). In a true production environment you would not hardcode the host using MachineID as I’ve done above, but in my test setup I’m not using any service discovery mechanisms. For further production readiness you would use a data volume container or other means of persisting the storage when the container is restarted. To run the unit and create the required database for cAdvisor: fleetctl start influxdb.service sleep 10\ncurl -X POST 'http://[influxdb_hostname]:8086/db?u=root&p=root' -d '{\"name\": \"cadvisor\"}' The sleep 10 is there to allow InfluxDB a few seconds to be ready as Docker will need to pull (download) the image, and the InfluxDB itself time to start. The curl statement creates a database for storing the cAdvisor data, called “cadvisor” with a default configuration - the credentials to log into InfluxDB are as shown, i.e. root/root .  Be sure to replace [influxdb_hostname] with the actual IP or DNS resolvable hostname of your InfluxDB container host. Note that the InfluxDB dashboard can be accessed via the exposed UI port 8083 , i.e., type http://[influxdb_hostname]:8083 in your web browser. Port 8086 is for API access , as used by the curl statement and cAdvisor. If you are doing clustering you will need to also expose the cluster ports, 8090 and 8099. We now start the cAdvisor container across all our hosts, using the this fleet/systemd unit: [Unit]\nDescription=Google Container Advisor (cAdvisor)\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull google/cadvisor:latest\n\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 --name=%p google/cadvisor:latest --logtostderr \\\n  -storage_driver=influxdb -storage_driver_host=[influxdb_hostname]:8086 \\\n  -storage_driver_db=cadvisor -storage_driver_user=root -storage_driver_password=root\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nGlobal=true To run the unit file: fleetctl start cadvisor.service The Global=true in the fleet specific section indicates that this unit will be started across all Docker container hosts. Again, ensure you replace the [influxdb_hostname] in the Docker run statement with the appropriate value for your InfluxDB host. Here we are exposing required Docker host volumes to the container so it can read the host and container metrics from the Docker host. We are also publishing the cAdvisor port to enable the built-in dashboard (on port 8080) . You can refer to the cAdvisor documentation on the specific command line options but here we are just providing the InfluxDB details, i.e., the storage driver type (‘influxdb’), host, database name (‘cadvisor’), database user (‘root’) and database password (‘root’). You can access the cAdvisor dashboard for each host by entering the URL http://[cadvisor_hostname]:8080 in your web browser. Install the Grafana dashboard using the fleet/systemd unit: [Unit]\nDescription=Grafana UI Service\n\nRequires=docker.service\nWants=influxdb.service\nAfter=docker.service\nAfter=influxdb.service\nBindsTo=influxdb.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull grafana/grafana:latest\n\nExecStart=/usr/bin/docker run --publish=3000:3000 \\\n  --env INFLUXDB_HOST=%H --env INFLUXDB_PORT=8086 --env INFLUXDB_NAME=cadvisor \\\n  --env INFLUXDB_USER=root --env INFLUXDB_PASS=root \\\n  --name=%p grafana/grafana:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineOf=influxdb.service To run the unit file: fleetctl start grafana.service We use the official Grafana 2.x container image , using environment variables to connect to the cadvisor InfluxDB database. By using the fleet specific MachineOf unit statement we pin the container to the same host as the InfluxDB container. The Grafana dashboard can now be accessed via your web browser at http://[grafana_hostname]:3000 . Connecting to Grafana and creating dashboards Once your full stack is up, you’ll then need to connect Grafana to your cadvisor database within InfluxDB and create some useful graphs. Start by opening a web page to http://[grafana_hostname]:3000 and log in using the default Grafana credentials, admin/admin . You will need to change this password and enable the appropriate authentication and authorization mechanisms for a production deployment . Create a new data source connection to the cadvisor InfluxDB database by first exposing the data source menu by clicking on the Grafana fireball icon in the top left hand corner of the UI, then selecting Data Sources -> Add New Enter the appropriate information: Add data source settings Name: influxdb Type: InfluxDB 0.8.x Default (checked) Http settings Url: http://[influxdb_hostname]:8086 Basic Auth (enabled) User: root Password: root InfluxDB Details Database: cadvisor User: root Password: root Now comes what may be the hardest part: creating useful dashboards. Click on the Home icon (top left corner) and select +New to create a new dashboard. Hover over, and select the thin green icon bar (top far left, below fireball) and Add Panel -> Graph from the displayed sub-menus. Select the no title (click here) and edit from the displayed sub-menu. Now we can create a quick first graph. In the series section fill in ‘stats’, then ‘Limit’ in the alias section. Use ‘fs_limit’ as the value for mean in the select section. Click on +Add query to add an additional query/graph line and enter ‘Usage’ in the alias section and ‘fs_usage’ as the value for mean in the select section here. You will see values being plotted as soon as we enter values, and by now you will realize we are doing a simple file system limit vs usage graph . To complete our graph let’s give it a better name, and more meaningful unit values. Click on General and give your graph a name, for example ‘File System’. Then click on ‘Axes & Grid’ and use the ‘byte’ unit for the Left Y axis unit. Once complete, ensure you click the Save icon (near top left of screen) to save your dashboard. By default Grafana 2.x saves dashboards to it’s embedded sqlite3 database though they can be exported and imported as well. You can also use other supported storage backends. Conclusion You’ve now built a Docker metrics collection system with a single Grafana dashboard for file system statistics. We’ve only just begun so relevant dashboards are still being built, but check out the Grafana reference docs to get better acquainted with it’s capabilities. You can look out for future posts on more of the components being used in our platform solution such as Heapster , which we are looking at for tying together all the cAdvisor agents and provide a cluster-view, and our take on persistent storage in a Docker environment. I hope this has proven useful to you. Please keep in touch and let us know your thoughts and what you might be working on as well. Good luck! posted on June 17, 2015 by Stuart Wong ← Previous Post Next Post → [Unit]\nDescription=InfluxDB Service\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull tutum/influxdb:latest\n\nExecStart=/usr/bin/docker run --publish=8083:8083 --publish=8086:8086 \\\n  --name=%p tutum/influxdb:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineID=[your_machine_ID] [Unit]\nDescription=InfluxDB Service\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull tutum/influxdb:latest\n\nExecStart=/usr/bin/docker run --publish=8083:8083 --publish=8086:8086 \\\n  --name=%p tutum/influxdb:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineID=[your_machine_ID] fleetctl start influxdb.service sleep 10\ncurl -X POST 'http://[influxdb_hostname]:8086/db?u=root&p=root' -d '{\"name\": \"cadvisor\"}' fleetctl start influxdb.service sleep 10\ncurl -X POST 'http://[influxdb_hostname]:8086/db?u=root&p=root' -d '{\"name\": \"cadvisor\"}' [Unit]\nDescription=Google Container Advisor (cAdvisor)\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull google/cadvisor:latest\n\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 --name=%p google/cadvisor:latest --logtostderr \\\n  -storage_driver=influxdb -storage_driver_host=[influxdb_hostname]:8086 \\\n  -storage_driver_db=cadvisor -storage_driver_user=root -storage_driver_password=root\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nGlobal=true [Unit]\nDescription=Google Container Advisor (cAdvisor)\n\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nRestartSec=5s\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull google/cadvisor:latest\n\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 --name=%p google/cadvisor:latest --logtostderr \\\n  -storage_driver=influxdb -storage_driver_host=[influxdb_hostname]:8086 \\\n  -storage_driver_db=cadvisor -storage_driver_user=root -storage_driver_password=root\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nGlobal=true fleetctl start cadvisor.service fleetctl start cadvisor.service [Unit]\nDescription=Grafana UI Service\n\nRequires=docker.service\nWants=influxdb.service\nAfter=docker.service\nAfter=influxdb.service\nBindsTo=influxdb.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull grafana/grafana:latest\n\nExecStart=/usr/bin/docker run --publish=3000:3000 \\\n  --env INFLUXDB_HOST=%H --env INFLUXDB_PORT=8086 --env INFLUXDB_NAME=cadvisor \\\n  --env INFLUXDB_USER=root --env INFLUXDB_PASS=root \\\n  --name=%p grafana/grafana:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineOf=influxdb.service [Unit]\nDescription=Grafana UI Service\n\nRequires=docker.service\nWants=influxdb.service\nAfter=docker.service\nAfter=influxdb.service\nBindsTo=influxdb.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=-/usr/bin/docker rm -f %p\nExecStartPre=/usr/bin/docker pull grafana/grafana:latest\n\nExecStart=/usr/bin/docker run --publish=3000:3000 \\\n  --env INFLUXDB_HOST=%H --env INFLUXDB_PORT=8086 --env INFLUXDB_NAME=cadvisor \\\n  --env INFLUXDB_USER=root --env INFLUXDB_PASS=root \\\n  --name=%p grafana/grafana:latest\n\nExecStop=/usr/bin/docker stop %p\nExecStopPost=-/usr/bin/docker rm -f %p\n\n[Install]\nWantedBy=multi-user.target\n\n[X-Fleet]\nMachineOf=influxdb.service fleetctl start grafana.service fleetctl start grafana.service posted on June 17, 2015 by Stuart Wong Stuart Wong", "date": "2015-06-17"},
{"website": "Monsanto", "title": "Learn implicits: Scala Futures", "author": ["\n                                        Jorge Montero\n                                    "], "link": "http://engineering.monsanto.com/2015/06/15/implicits-futures/", "abstract": "Implicits are difficult to understand because they have many different uses.\nIn an earlier post ,\nwe looked at implicit parameters and type tags.\nNow, we’ll take a look at another usage pattern every Scala programmer sees: \nimplicits as an alternative to passing the same argument over and over. Scala Futures use implicit parameters in this way. There is much said about futures elsewhere. The gist of it is that a Future contains a value that may\nor may not have been computed yet.\nFutures let us spin off work into other threads, add more operations that should be performed on the result,\ndefine what should happen after failure, and (if we really must) wait for the operation to complete. Everything we do asynchronously happens on some other thread. \nCreating a future, adding operations after success, adding failure handling – in each case, we need to tell it what thread to run on. \nThe futures library lets us specify this using implicit parameters. For illustration, let’s define some data types and a fake Data Access Object with the following operations: case class Employee(id:Int, name:String)\ncase class Role(name:String, department :String)\ncase class EmployeeWithRole(id :Int, name :String, role :Role)\n\ntrait EmployeeGrabberBabber {\n  def rawEmployee (id :Int) :Employee\n  def rawRole (e :Employee) :Role\n  def employee (id: Int)( implicit e :ExecutionContext ) :Future[Employee]\n  def role (employee :Employee)( implicit e :ExecutionContext ) :Future[Role]\n} I have an implementation for that trait, but it’s not that important. The first two methods do synchronous IO: Whenever we call them, our thread will patiently wait until we get the requested information, leaving our thread blocked. The second pair uses Futures: employee returns a Future[Employee], which will eventually provide an Employee, or error out.\n  We do not wait for the operation to complete before returning; the caller gets the power of deciding whether to wait,\n   whether to attach more actions, whether to handle errors. With the first set of methods , if we wanted to get an Employee, and then get their Role, and then combine that into an EmployeeWithRole: val employee = grabber. rawEmployee (100)\nval role = grabber. rawRole (employee)\nval bigEmployee = EmployeeWithRole(employee.id,employee.name,role) This is imperative programming. It holds up the calling thread until the entire calculation is made. You probably don’t want to do this in a web application \nor in an event thread in a native UI toolkit. In contrast, the asynchronous methods return instantly. We can keep right on defining what to do with the value – inside the context of the Future. val bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100).flatMap { e =>\n    grabber. role (e).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }\n  } This code puts both operations together without blocking. Except, the code above does not work on its own. Remember those implicit parameters defined above in EmployeeGrabberBabber? def employee (id :Int)( implicit e :ExecutionContext ) :Future[Employee]\ndef role (employee :Employee)( implicit e: ExecutionContext ) : Future[Role] We did not define them, as the compiler helpfully reminds us. Error: Cannot find an implicit ExecutionContext . You might pass\nan (implicit ec: ExecutionContext) parameter to your method\nor import scala.concurrent.ExecutionContext.Implicits.global.\n    grabber. employee (100).flatMap { e =>\n                    ^ That’s a useful error message! While we could just add the import, we’d not learn much from doing that, so let’s dig deeper. Creating a Future starts an asynchronous operation on another thread. The ExecutionContext provides the thread pool that Future will use.\nDifferent execution contexts wrap different thread pools, with different properties.\nThe one that the errors suggest, Scala’s global execution context , suits us for now. The Future-creating methods declare two parameter lists. We can be perfectly clear about which ExecutionContext each should use by passing it explicitly: val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100)( ec ).flatMap { e =>\n    grabber. role (e)( ec ).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }\n  } But that doesn’t work either! Error: Cannot find an implicit ExecutionContext . You might pass\nan (implicit ec: ExecutionContext) parameter to your method\nor import scala.concurrent.ExecutionContext.Implicits.global.\n      grabber. employee (100)( ec ).flatMap { e =>\n                                      ^ The flatmap method on Future also wants an ExecutionContext! We gave that Future another operation to perform, and it needs a thread pool to run that on. \nFuture.map has the same problem, so pass the ExecutionContext there, too. This is getting tedious. val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100)( ec ).flatMap { e =>\n    grabber. role (e)( ec ).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }( ec )\n  }( ec ) So now it’s happy, and it’s very clear which ExecutionContext every operation runs in. \nBut I’m not happy, because it’s repetitive and cluttered. It gets even more cluttered,\n the more things we call on a Future. Just look at these signatures from the Future API : def onSuccess\\[U\\](pf : PartialFunction[T, U])( implicit executor : ExecutionContext )\n    def onFailure\\[U\\](callback : PartialFunction[Throwable, U])( implicit executor : ExecutionContext ) \n    def onComplete\\[U\\](func : Try[T] => U)( implicit executor : ExecutionContext )\n    def foreach\\[U\\](f : T => U)( implicit executor : ExecutionContext ) \n    def transform\\[S\\](s : T => S, f : Throwable => Throwable)( implicit executor : ExecutionContext ) : Future[S] \n    def map\\[S\\](f : T => S)( implicit executor : ExecutionContext ) : Future[S]\n    def flatMap\\[S\\](f : T => Future[S])( implicit executor : ExecutionContext ) : Future[S] \n    def filter(pred : T => Boolean)( implicit executor : ExecutionContext ) : Future[T] \n    def withFilter(p : T => Boolean)( implicit executor : ExecutionContext ) : Future[T]\n    def collect\\[S\\](pf : T => S)( implicit executor : ExecutionContext ) : Future[S] \n    def recover\\[U >: T\\](pf : PartialFunction[Throwable, U])( implicit executor : ExecutionContext ) : Future[U] \n    def recoverWith\\[U >: T\\](pf : PartialFunction[Throwable, Future[U]])( implicit executor : ExecutionContext ) : Future[U]\n    def andThen\\[U\\](pf : PartialFunction[Try[T], U])( implicit executor : ExecutionContext ) : Future[T] ExecutionContexts everywhere. They’re important, and sometimes we need to be specific about where each operation should run,\n but the common case is that they can all run in the same pool. It is tedious, cluttered, and error-prone to repeat\nthat same bit of information over and over. When a function has multiple parameter lists, Scala permits the implicit keyword at the beginning of the last parameter list. def employee (id:Int)( implicit e:ExecutionContext ) :Future[Employee] This instructs the Scala compiler to pull those arguments out of its magic hat, instead of requiring them to be passed each time. If it’s ok to use all those threads in the same pool, \nthen we can supply the execution context implicitly , which puts it in the magic hat: implicit val ec : ExecutionContext = scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100).flatMap { e =>\n    grabber. role (e).map { r =>\n      EmployeeWithRole(e.id,e.name,r) \n    }\n  } Here, the implicit keyword is serving a different (but related) purpose than it did in the parameter list. \nThe implicit val goes into the compiler’s magic hat for as long as that value is in scope.\n The compiler can use it anywhere it needs to supply an implicit parameter of type ExecutionContext, over and over. This lets us use for-comprehensions too, which take the place of flatmap and map: implicit val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval employeeWithRole = for { employee <- grabber. employee (200L)\n                            role <- grabber. role (employee) } \n                            yield EmployeeWithRole(employee.id,employee.name,role) Much cleaner. This implicit-parameter-supplying feature only works if there is exactly one value of the needed type in the compiler’s magic hat when the method that\ndeclares the implicit parameter is called.  If none are available, you get that\n“Cannot find an implicit” compile error. If more than one are available,\nyou get an “ambiguous implicit” error Of the many ways to get values into the magic hat, three make sense for the\nExecutionContext. The simplest is to declare an implicit val , as above.  It\nstays in the magic hat as long as the val is in scope.  This is common inside an Akka actor: class SomeActor extends Actor { implicit val ec: ExecutionContext = context.dispatcher\n} Another is to declare an implicit parameter : that value is in the magic hat\ninside the method. This is good practice, because it lets the caller decide\nwhat to use. For example, this e is available to the Future constructor: def employee (id:Int)( implicit e :ExecutionContext ) :Future[Employee] = Future{...} Finally, the most common way to punt on the selection of the execution context is to \nimport an implicit val in file scope: import scala.concurrent.ExecutionContext.Implicits.global Inside scala.concurrent.ExecutionContext.Implicits , global is an implicit val , so into the magic hat it goes. Adding this to the top of the file \nchooses the default execution context for asynchronous operations. Any way you do it, one implicit value declaration saves repetition, \nproviding a default per scope while allowing an override at each function call. \nThe authors of the Future library put the ExecutionContext into an implicit parameter for this reason:\nit’s common to repeat the same value, common to pass it down through various methods,\nand essential that it be explicitly passed sometimes, at the caller’s discretion.\nIn this way, Scala lets library designers keep the interface clean and flexible at the same time. posted on June 15, 2015 by Jorge Montero ← Previous Post Next Post → case class Employee(id:Int, name:String)\ncase class Role(name:String, department :String)\ncase class EmployeeWithRole(id :Int, name :String, role :Role)\n\ntrait EmployeeGrabberBabber {\n  def rawEmployee (id :Int) :Employee\n  def rawRole (e :Employee) :Role\n  def employee (id: Int)( implicit e :ExecutionContext ) :Future[Employee]\n  def role (employee :Employee)( implicit e :ExecutionContext ) :Future[Role]\n} val employee = grabber. rawEmployee (100)\nval role = grabber. rawRole (employee)\nval bigEmployee = EmployeeWithRole(employee.id,employee.name,role) val bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100).flatMap { e =>\n    grabber. role (e).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }\n  } def employee (id :Int)( implicit e :ExecutionContext ) :Future[Employee]\ndef role (employee :Employee)( implicit e: ExecutionContext ) : Future[Role] Error: Cannot find an implicit ExecutionContext . You might pass\nan (implicit ec: ExecutionContext) parameter to your method\nor import scala.concurrent.ExecutionContext.Implicits.global.\n    grabber. employee (100).flatMap { e =>\n                    ^ val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100)( ec ).flatMap { e =>\n    grabber. role (e)( ec ).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }\n  } Error: Cannot find an implicit ExecutionContext . You might pass\nan (implicit ec: ExecutionContext) parameter to your method\nor import scala.concurrent.ExecutionContext.Implicits.global.\n      grabber. employee (100)( ec ).flatMap { e =>\n                                      ^ val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100)( ec ).flatMap { e =>\n    grabber. role (e)( ec ).map { r =>\n      EmployeeWithRole(e.id,e.name,r)\n    }( ec )\n  }( ec ) def onSuccess\\[U\\](pf : PartialFunction[T, U])( implicit executor : ExecutionContext )\n    def onFailure\\[U\\](callback : PartialFunction[Throwable, U])( implicit executor : ExecutionContext ) \n    def onComplete\\[U\\](func : Try[T] => U)( implicit executor : ExecutionContext )\n    def foreach\\[U\\](f : T => U)( implicit executor : ExecutionContext ) \n    def transform\\[S\\](s : T => S, f : Throwable => Throwable)( implicit executor : ExecutionContext ) : Future[S] \n    def map\\[S\\](f : T => S)( implicit executor : ExecutionContext ) : Future[S]\n    def flatMap\\[S\\](f : T => Future[S])( implicit executor : ExecutionContext ) : Future[S] \n    def filter(pred : T => Boolean)( implicit executor : ExecutionContext ) : Future[T] \n    def withFilter(p : T => Boolean)( implicit executor : ExecutionContext ) : Future[T]\n    def collect\\[S\\](pf : T => S)( implicit executor : ExecutionContext ) : Future[S] \n    def recover\\[U >: T\\](pf : PartialFunction[Throwable, U])( implicit executor : ExecutionContext ) : Future[U] \n    def recoverWith\\[U >: T\\](pf : PartialFunction[Throwable, Future[U]])( implicit executor : ExecutionContext ) : Future[U]\n    def andThen\\[U\\](pf : PartialFunction[Try[T], U])( implicit executor : ExecutionContext ) : Future[T] def employee (id:Int)( implicit e:ExecutionContext ) :Future[Employee] implicit val ec : ExecutionContext = scala.concurrent.ExecutionContext.Implicits.global\nval bigEmployee: Future[EmployeeWithRole] =\n  grabber. employee (100).flatMap { e =>\n    grabber. role (e).map { r =>\n      EmployeeWithRole(e.id,e.name,r) \n    }\n  } implicit val ec =  scala.concurrent.ExecutionContext.Implicits.global\nval employeeWithRole = for { employee <- grabber. employee (200L)\n                            role <- grabber. role (employee) } \n                            yield EmployeeWithRole(employee.id,employee.name,role) class SomeActor extends Actor { implicit val ec: ExecutionContext = context.dispatcher\n} def employee (id:Int)( implicit e :ExecutionContext ) :Future[Employee] = Future{...} import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.ExecutionContext.Implicits.global posted on June 15, 2015 by Jorge Montero Jorge Montero", "date": "2015-06-15"},
{"website": "Monsanto", "title": "etcd Clustering in AWS", "author": ["\n                                        T.J. Corrigan\n                                    "], "link": "http://engineering.monsanto.com/2015/06/12/etcd-clustering/", "abstract": "Overview For the last few months our team has been focused on building a robust, highly automated Docker container infrastructure in AWS . We choose to use CoreOS as our base operating system because it is lightweight and container-centric. We are using fleet , another CoreOS project, to handle scheduling containers across a cluster of machines and keeping them running even if the original host they are running on is terminated. Both CoreOS and fleet need a shared view of the current state of all the machines and containers running in the cluster. This is where etcd , yet another CoreOS project, comes in to play. etcd is a distributed, consistent key-value store used for storing shared configuration and information in a cluster. In a large production environment, etcd is designed to run on a subset of machines in the system, preferably either three or five hosts. source The Bootstrapping Problem etcd requires an initial bootstrapping to form a cluster. This can be accomplished in several ways . Initially we used the etcd discovery service , but we saw strange behavior when using this with AWS Auto Scaling Groups, namely ghost IP addresses in the list the service would return. Plus, the discovery service does not handle the post-bootstrap problem of members joining and leaving the cluster. In the end, we chose the static method to reduce dependencies on external systems. Our initial approach, using etcd 0.4, was to create 3 dedicated EC2 instances in AWS via CloudFormation . This allowed us access to the IPs of these machines to use in the cloud-config in a block like: coreos : etcd : addr : localhost:4001 peer-addr : localhost:7001 peers : $ip_from_this_machine$:7001,$ip_from_other_machine$:7001,$ip_from_another_machine$:7001 While this approach works adequately there are a few disadvantages: Robustness These etcd server machines are critically important to the infrastructure and require special treatment. We were using static IPs, setting CloudWatch alarms, and doing extra monitoring. Phil Cryer , a colleague of mine, has been championing the concept of [Pets vs Cattle] (https://blog.engineyard.com/2014/pets-vs-cattle) and how we should avoid this sort of ‘special’ design, especially in an environment like AWS where Amazon doesn’t guarantee the health of any given EC2 instance. CloudFormation Updates Occasionally we needed to make changes to our infrastructure. To do this we would use CloudFormation to update our configuration. If there were any changes to these etcd machines, AWS would reboot them to apply the changes, potentially all at the same time. If this happened our cluster would become unavailable and may have trouble re-clustering. The Solution In thinking of potential solutions we turned to a feature we were already using for our worker machines, AWS Auto Scaling Groups . In this case we don’t really want to scale up and down the number of etcd servers but do want to maintain a fixed cluster size, even if a host were to fail. However, this presented a new challenge in figuring out how to coordinate the bootstrapping of etcd. Around this time CoreOS released etcd2 into their alpha channel builds. This new version brought with it changes to bootstrapping and dynamic reconfiguration which gives us the flexibility we needed to manage cluster membership with Auto Scaling Groups. Bootstrapping Our first concern was to automate the bootstrapping process. Since we no longer had fixed IPs like in our previous approach we needed a mechanism to discover the other leaders. \nFortunately, the Amazon CLI provides us with the tools we need. However, since we are using CoreOS we couldn’t just install the cli but needed to create a container for the job. The next concern was how to get the credentials needed to use the cli. Here we used an IAM Instance Role to give our server machines read-only permissions to ec2:Describe* and autoscaling:Describe* . With these tools, we can accomplish what we need with a simple BASH script: ec2_instance_id = $( curl -s http://169.254.169.254/latest/meta-data/instance-id ) ec2_instance_ip = $( curl -s http://169.254.169.254/latest/meta-data/local-ipv4 ) asg_name = $( aws autoscaling describe-auto-scaling-groups --region us-east-1 \\ | jq --raw-output \".[] map(select(.Instances[].InstanceId | contains( \\\" $ec2_instance_id \\\" ))) | .[].AutoScalingGroupName\" ) etcd_peer_urls = $( aws ec2 describe-instances --region us-east-1 --instance-ids \\ $( aws autoscaling describe-auto-scaling-groups --region us-east-1 --auto-scaling-group-name $asg_name | jq .AutoScalingGroups[0].Instances[].InstanceId | xargs ) \\ | jq -r '.Reservations[].Instances | map(\"http://\" + .NetworkInterfaces[].PrivateIpAddress + \":2379\")[]' ) This script starts off by querying the EC2 instance ID and IP address from AWS using their instance metadata endpoint . With this information we are able to retrieve the name of the Auto Scaling Group that this particular instance belongs to by using the CLI and jq . From this we are then able to query for all the IPs of the machines in this Auto Scaling Group. We then write this information to file: ETCD_INITIAL_CLUSTER_STATE = new ETCD_NAME = $ec2_instance_id ETCD_INITIAL_CLUSTER = \" $etcd_initial_cluster \" and then instruct etcd to load this information when it starts up. With these changes we were reliably able to boostrap etcd from an autoscaling group without any hardcoding! Maintaining Cluster Membership Normally etcd is expecting that a machine would either remove itself from the cluster before exiting or would rejoin at a later time (e.g., in the event of a restart). We wanted to build something more robust where we could kill a machine and replace it with an entirely new machine without any hiccups in availability. Of course there were a few challenges… Detecting New vs Existing Cluster The first thing we discovered is that you needed to explicitly tell etcd if this was a new cluster or if you were joining an existing cluster for etcd to work correctly. After a bit of trial and error we arrived at: etcd_existing_peer_urls = etcd_existing_peer_names = etcd_good_member_url = for url in $etcd_peer_urls ; do\n    case \" $url \" in * $ec2_instance_ip * ) continue ;; esac etcd_members = $( curl -f -s $url /v2/members ) if [[ $? == 0 && $etcd_members ]] ; then etcd_good_member_url = \" $url \" echo \"etcd_members= $etcd_members \" etcd_existing_peer_urls = $( echo \" $etcd_members \" | jq --raw-output .[][].peerURLs[0] ) etcd_existing_peer_names = $( echo \" $etcd_members \" | jq --raw-output .[][].name ) break fi\ndone\n\nif [[ $etcd_existing_peer_urls && $etcd_existing_peer_names != * \" $ec2_instance_id \" * ]] ; then echo \"joining existing cluster\" else echo \"creating new cluster\" fi The basic idea here is that we try to connect to each machine in the Auto Scaling Group to see if any of them are currently running etcd and if so, what are the members of the cluster. We assume that if no one responds this must be a new cluster. Now if someone does respond back with a list of potential members we could still potentially be in a bootstrapping situation. Remember that the first machine to come up will still likely know about the other machines in the Auto Scaling Group and will already know their instance IDs or IPs. So if our instance ID is in the list we assume we are just late to the party but still part of the initial bootstrapping. Adding / Removing Members Once we know that we are joining an existing cluster and the members of the cluster, we can begin the steps to add the new member to the existing cluster. # eject bad members from cluster peer_regexp = $( echo \" $etcd_peer_urls \" | sed 's/^.*http:\\/\\/\\([0-9.]*\\):[0-9]*.*$/contains(\\\\\"\\1\\\\\")/' | xargs | sed 's/  */ or /g' ) bad_peer = $( echo \" $etcd_members \" | jq --raw-output \".[] | map(select(.peerURLs[] | $peer_regexp | not )) | .[].id\" ) if [[ $bad_peer ]] ; then\n    for bp in $bad_peer ; do echo \"removing bad peer $bp \" curl -f -s \" $etcd_good_member_url /v2/members/ $bp \" -XDELETE done\nfi etcd_initial_cluster = $( curl -s -f \" $etcd_good_member_url /v2/members\" | jq --raw-output '.[] | map(.name + \"=\" + .peerURLs[0]) | .[]' | xargs | sed 's/  */,/g' )$( echo \", $ec2_instance_id =http:// ${ ec2_instance_ip } :2380\" ) echo \"adding instance ID $ec2_instance_id with IP $ec2_instance_ip \" curl -f -s -XPOST \" $etcd_good_member_url /v2/members\" -H \"Content-Type: application/json\" -d \"{ \\\" peerURLs \\\" : [ \\\" http:// $ec2_instance_ip :2380 \\\" ], \\\" name \\\" : \\\" $ec2_instance_id \\\" }\" The first step is to try and detect whether any members of the cluster have been terminated. This can be deduced by comparing the list of members reported by etcd to the list of machines in the Auto Scaling Group. Once we find the bad host(s) we can go ahead and send a REST call to one of the good members of the cluster to remove the dead machine. Afterwards we can add the new machine to the cluster through another REST call before starting etcd. etcd Bugs At this point we thought we had a great pattern for dealing with adding and removing machines from the cluster and started some scale testing. Whenever we terminated a machine we saw that the cluster remained healthy, with one unhealthy node, until we tried to remove the dead node. After removing the dead node using the API, the cluster became unhealthy and would not accept writes. After a few minutes in this state, the cluster sorted things out and became healthy again. Once healthy, we were able to add the new machine and write to the cluster. I filed a bug report with the CoreOS team about this minutes-long unhealthy state after dead node removal and very quickly got a response and a solution (big kudos to the CoreOS team!). I’ve tested out their new builds and am happy to report we now have a reliable solution. Their fixes are merged in and hopefully we’ll see them in a new release in the next week or two. Conclusion We have now built a fully automated solution to build etcd clusters on AWS Auto Scaling Groups. I’m happy to announce that we have open-sourced our efforts ( GitHub / DockerHub ) and hope they will be of use to the broader community. We welcome issues, contributions, comments, and questions. posted on June 12, 2015 by T.J. Corrigan ← Previous Post Next Post → coreos : etcd : addr : localhost:4001 peer-addr : localhost:7001 peers : $ip_from_this_machine$:7001,$ip_from_other_machine$:7001,$ip_from_another_machine$:7001 coreos : etcd : addr : localhost:4001 peer-addr : localhost:7001 peers : $ip_from_this_machine$:7001,$ip_from_other_machine$:7001,$ip_from_another_machine$:7001 ec2_instance_id = $( curl -s http://169.254.169.254/latest/meta-data/instance-id ) ec2_instance_ip = $( curl -s http://169.254.169.254/latest/meta-data/local-ipv4 ) asg_name = $( aws autoscaling describe-auto-scaling-groups --region us-east-1 \\ | jq --raw-output \".[] map(select(.Instances[].InstanceId | contains( \\\" $ec2_instance_id \\\" ))) | .[].AutoScalingGroupName\" ) etcd_peer_urls = $( aws ec2 describe-instances --region us-east-1 --instance-ids \\ $( aws autoscaling describe-auto-scaling-groups --region us-east-1 --auto-scaling-group-name $asg_name | jq .AutoScalingGroups[0].Instances[].InstanceId | xargs ) \\ | jq -r '.Reservations[].Instances | map(\"http://\" + .NetworkInterfaces[].PrivateIpAddress + \":2379\")[]' ) ec2_instance_id = $( curl -s http://169.254.169.254/latest/meta-data/instance-id ) ec2_instance_ip = $( curl -s http://169.254.169.254/latest/meta-data/local-ipv4 ) asg_name = $( aws autoscaling describe-auto-scaling-groups --region us-east-1 \\ | jq --raw-output \".[] map(select(.Instances[].InstanceId | contains( \\\" $ec2_instance_id \\\" ))) | .[].AutoScalingGroupName\" ) etcd_peer_urls = $( aws ec2 describe-instances --region us-east-1 --instance-ids \\ $( aws autoscaling describe-auto-scaling-groups --region us-east-1 --auto-scaling-group-name $asg_name | jq .AutoScalingGroups[0].Instances[].InstanceId | xargs ) \\ | jq -r '.Reservations[].Instances | map(\"http://\" + .NetworkInterfaces[].PrivateIpAddress + \":2379\")[]' ) ETCD_INITIAL_CLUSTER_STATE = new ETCD_NAME = $ec2_instance_id ETCD_INITIAL_CLUSTER = \" $etcd_initial_cluster \" ETCD_INITIAL_CLUSTER_STATE = new ETCD_NAME = $ec2_instance_id ETCD_INITIAL_CLUSTER = \" $etcd_initial_cluster \" etcd_existing_peer_urls = etcd_existing_peer_names = etcd_good_member_url = for url in $etcd_peer_urls ; do\n    case \" $url \" in * $ec2_instance_ip * ) continue ;; esac etcd_members = $( curl -f -s $url /v2/members ) if [[ $? == 0 && $etcd_members ]] ; then etcd_good_member_url = \" $url \" echo \"etcd_members= $etcd_members \" etcd_existing_peer_urls = $( echo \" $etcd_members \" | jq --raw-output .[][].peerURLs[0] ) etcd_existing_peer_names = $( echo \" $etcd_members \" | jq --raw-output .[][].name ) break fi\ndone\n\nif [[ $etcd_existing_peer_urls && $etcd_existing_peer_names != * \" $ec2_instance_id \" * ]] ; then echo \"joining existing cluster\" else echo \"creating new cluster\" fi etcd_existing_peer_urls = etcd_existing_peer_names = etcd_good_member_url = for url in $etcd_peer_urls ; do\n    case \" $url \" in * $ec2_instance_ip * ) continue ;; esac etcd_members = $( curl -f -s $url /v2/members ) if [[ $? == 0 && $etcd_members ]] ; then etcd_good_member_url = \" $url \" echo \"etcd_members= $etcd_members \" etcd_existing_peer_urls = $( echo \" $etcd_members \" | jq --raw-output .[][].peerURLs[0] ) etcd_existing_peer_names = $( echo \" $etcd_members \" | jq --raw-output .[][].name ) break fi\ndone\n\nif [[ $etcd_existing_peer_urls && $etcd_existing_peer_names != * \" $ec2_instance_id \" * ]] ; then echo \"joining existing cluster\" else echo \"creating new cluster\" fi # eject bad members from cluster peer_regexp = $( echo \" $etcd_peer_urls \" | sed 's/^.*http:\\/\\/\\([0-9.]*\\):[0-9]*.*$/contains(\\\\\"\\1\\\\\")/' | xargs | sed 's/  */ or /g' ) bad_peer = $( echo \" $etcd_members \" | jq --raw-output \".[] | map(select(.peerURLs[] | $peer_regexp | not )) | .[].id\" ) if [[ $bad_peer ]] ; then\n    for bp in $bad_peer ; do echo \"removing bad peer $bp \" curl -f -s \" $etcd_good_member_url /v2/members/ $bp \" -XDELETE done\nfi etcd_initial_cluster = $( curl -s -f \" $etcd_good_member_url /v2/members\" | jq --raw-output '.[] | map(.name + \"=\" + .peerURLs[0]) | .[]' | xargs | sed 's/  */,/g' )$( echo \", $ec2_instance_id =http:// ${ ec2_instance_ip } :2380\" ) echo \"adding instance ID $ec2_instance_id with IP $ec2_instance_ip \" curl -f -s -XPOST \" $etcd_good_member_url /v2/members\" -H \"Content-Type: application/json\" -d \"{ \\\" peerURLs \\\" : [ \\\" http:// $ec2_instance_ip :2380 \\\" ], \\\" name \\\" : \\\" $ec2_instance_id \\\" }\" # eject bad members from cluster peer_regexp = $( echo \" $etcd_peer_urls \" | sed 's/^.*http:\\/\\/\\([0-9.]*\\):[0-9]*.*$/contains(\\\\\"\\1\\\\\")/' | xargs | sed 's/  */ or /g' ) bad_peer = $( echo \" $etcd_members \" | jq --raw-output \".[] | map(select(.peerURLs[] | $peer_regexp | not )) | .[].id\" ) if [[ $bad_peer ]] ; then\n    for bp in $bad_peer ; do echo \"removing bad peer $bp \" curl -f -s \" $etcd_good_member_url /v2/members/ $bp \" -XDELETE done\nfi etcd_initial_cluster = $( curl -s -f \" $etcd_good_member_url /v2/members\" | jq --raw-output '.[] | map(.name + \"=\" + .peerURLs[0]) | .[]' | xargs | sed 's/  */,/g' )$( echo \", $ec2_instance_id =http:// ${ ec2_instance_ip } :2380\" ) echo \"adding instance ID $ec2_instance_id with IP $ec2_instance_ip \" curl -f -s -XPOST \" $etcd_good_member_url /v2/members\" -H \"Content-Type: application/json\" -d \"{ \\\" peerURLs \\\" : [ \\\" http:// $ec2_instance_ip :2380 \\\" ], \\\" name \\\" : \\\" $ec2_instance_id \\\" }\" posted on June 12, 2015 by T.J. Corrigan T.J. Corrigan", "date": "2015-06-12"},
{"website": "Monsanto", "title": "Secret STDIN Slurper", "author": ["\n                                        David Dooling\n                                    "], "link": "http://engineering.monsanto.com/2015/05/27/secret-stdin-slurper/", "abstract": "BASH may be looked down upon by\nall the programming language hipsters out there, but when you are\ndoing system-level tasks it can be quite convenient: it’s on every\nsystem, it has no dependencies to install, and it can be a powerful\nlanguage when used properly. The Setup As mentioned in a previous post ,\nwe use AWS CloudFormation to\nstand up infrastructure in AWS in an automated way.  In one use case,\nwe create a standard environment that makes it easy to deploy\nmicroservices as Docker containers onto an Auto Scaling group running CoreOS .  If there is a container you want to\nrun on every container, e.g., a log forwarder, you can simply put the systemd unit\ninformation in the instance User Data of the Auto Scaling group’s Launch Configuration in cloud-config format. Things are a bit trickier if you only want to run a container on a\nsubset of the instances.  You would typically do this with a scheduler\nlike fleet or Mesos .  If the container is one of your\nmicroservices, you can just configure deployment in your continuous\nintegration server, e.g., Jenkins .  If,\nhowever, the container is more of an infrastructure service that you\nonly want running on a single node, something like Kibana for a system\ndashboard, you need to find a different way to inject that container\ninto your scheduler. As a first pass, we decided to have the helper BASH script we use to\nspin up and manage state of our CloudFormation stacks simply wait for\nthe stack creation to complete and then issue fleet commands via SSH.\nAlongside the template used to create the stack, we created a file\nthat contained the list of unit files we wanted the script to spin up.\nSo if our template is called vpc-default.json , then we would create\na file called vpc-default.services with contents something like\nthis: kibana\nroute-updater\nreaper After the stack creation was complete, the script would read this file\nand issue the appropriate fleet commands over SSH on one of the nodes\nin the fleet cluster. The Problem One of our engineers initially coded up the solution something like\nthis: services_file = \"vpc-default.services\" while read service ; do echo \"launching service: $service \" if ! ssh service fleetctl submit $service ; then echo \"ERROR: failed to submit service: $service \" return 1 fi\n    if ! ssh service fleetctl start $service ; then echo \"ERROR: failed to submit service: $service \" return 1 fi echo \"successfully launched service: $service \" done < \" $services_file \" After setting the value of the variable services_file to the name of\nthe services file (in the actual script this is done dynamically), the\nexecution enters a while loop.  Each time through the while loop,\nit read ’s a line from STDIN and puts what is read into the variable service .  But we don’t want the values read from STDIN , we want\nthem read from the file, so we redirect STDIN to come from $services_file , that’s the < \"$services_file\" on the last line. Unfortunately, this doesn’t work.  When we run the script on a file\nwith the contents show above, all we get is: launching service: kibana\nsuccessfully launched service: kibana The loop only executes one time instead of three!?!  What is going on?\nWhy is the loop terminating early and/or what happens to the second\nand third line of the file? To try to figure this out, let’s simplify the script.  Let’s just echo\nthe each line as we read it: services_file = \"vpc-default.services\" while read service ; do echo \"service: $service \" done < \" $services_file \" When we run this, we get: service:kibana\nservice:route-updater\nservice:reaper as expected.  Something must be going wrong inside our original loop,\nbut what could it be?  We know running echo is OK, so the problem\nmust be with the SSH fleet commands.  Somehow, they are causing the\nloop to exit without processing the last two lines of the file.  So\neither they are making reading the second line evaluate to false,\nwhich seems unlikely, or those commands are somehow consuming the\ncontents of STDIN themselves.  We can see if the latter is a possibility\nby replacing the SSH commands with a simpler command that consumes STDIN , cat : services_file = \"vpc-default.services\" while read service ; do echo \"service: $service \" cat done < \" $services_file \" When you run the above bit of code, the output is: service:kibana\nroute-updater\nreaper Ah-ha! The while loop read gobbles up the the first line of the\nfile, but then the cat within the loop consumes the rest of the file\nand the next time through the loop, read returns false and the\nscript moves along.  But does the same thing happens with SSH?  We can\ntry that too: services_file = \"vpc-default.services\" while read service ; do echo \"ssh service: $service \" ssh service cat done < \" $services_file \" Running this outputs: ssh service:kibana\nroute-updater\nreaper So SSH is indeed consuming the contents of STDIN .  How?  Basically,\nSSH connects the STDIN of the calling process to the STDIN of the\nprocess on the remote machine so you can do cool things like pipe\nstuff over SSH.  If the remote process does something with STDIN ,\nlike cat above, great.  If it just ignores STDIN like the fleet\ncommands, then lines two through the end of the file get lost\nsomewhere on the remote machine. The Solution The solution to this problem is quite simple and good advice whenever\nwriting loops in BASH: read the contents of the file into a variable\nbefore the loop ever executes and use a for loop instead of a while loop.  This looks something like: services_file = \"vpc-default.services\" services = $( < \" $services_file \" ) for service in $services ; do echo \"launching service: $service \" if ! ssh service fleetctl submit $service ; then echo \"ERROR: failed to submit service: $service \" return 1 fi\n    if ! ssh service fleetctl start $service ; then echo \"ERROR: failed to submit service: $service \" return 1 fi echo \"successfully launched service: $service \" done When you run this, you get the expected output: launching service: kibana\nsuccessfully launched service: kibana\nlaunching service: route-updater\nsuccessfully launched service: route-updater\nlaunching service: reaper\nsuccessfully launched service: reaper Then, fleet will make sure your services are running and all is right\nwith the world.  What happens if you need to read a very large file\nwhere the act of doing so will cause harm to the normal operations of\nyour computer?  You may want to investigate the SSH -n command line\noption. posted on May 27, 2015 by David Dooling ← Previous Post Next Post → kibana\nroute-updater\nreaper kibana\nroute-updater\nreaper launching service: kibana\nsuccessfully launched service: kibana launching service: kibana\nsuccessfully launched service: kibana service:kibana\nservice:route-updater\nservice:reaper service:kibana\nservice:route-updater\nservice:reaper service:kibana\nroute-updater\nreaper service:kibana\nroute-updater\nreaper ssh service:kibana\nroute-updater\nreaper ssh service:kibana\nroute-updater\nreaper launching service: kibana\nsuccessfully launched service: kibana\nlaunching service: route-updater\nsuccessfully launched service: route-updater\nlaunching service: reaper\nsuccessfully launched service: reaper launching service: kibana\nsuccessfully launched service: kibana\nlaunching service: route-updater\nsuccessfully launched service: route-updater\nlaunching service: reaper\nsuccessfully launched service: reaper posted on May 27, 2015 by David Dooling David Dooling", "date": "2015-05-27"},
{"website": "Monsanto", "title": "Modifying JSON on the command line", "author": ["\n                                        Jessica Kerr\n                                    "], "link": "http://engineering.monsanto.com/2015/05/22/jq-change-json/", "abstract": "If you need to parse through some JSON data at the command line, jq is here for you. jq is its own programming language. There are tons of examples of how to use jq to extract data from JSON; \nthis post shows how we use it to modify JSON. AWS CloudFormation turns a JSON stack definition (plus a JSON configuration file)\ninto a whole interconnected bunch of AWS resources. I frequently want to update my configuration. \nUsing jq , I can do this from the command line. That means I can script it for automated tests. The configuration file looks like this: [{\n  \"ParameterKey\": \" Project \",\n  \"ParameterValue\": \"<changeMe>\"\n }, \n {\n  \"ParameterKey\": \"DockerInstanceType\",\n  \"ParameterValue\": \"m3.medium\"\n}] The JSON is an array of objects, each with ParameterKey and ParameterValue. I want to change the ParameterValue for a particular ParameterKey . Here’s the syntax: cat config.json | \n  jq ' map (if .ParameterKey == \" Project \"\n          then . + {\"ParameterValue\"=\"jess-project\"} else . end\n         )' > populated_config.json This says, “ For each object in the array :\ncheck if ParameterKey is “ Project ”. If so, combine that object with this other one (right-hand-side values win, so my ParameterValue overrides the existing one). If not, leave the object alone.” \nThe output file now contains [{\n   \"ParameterKey\": \" Project \",\n   \"ParameterValue\": \" jess-project \"\n },\n {\n   \"ParameterKey\": \"DockerInstanceType\",\n   \"ParameterValue\": \"m3.medium\"\n}] Hooray! The jq map function, combined with a conditional, let me change a particular value. Since I do this often, and I’m on a Mac, I made a crude bash function and threw it in my .bash_profile so it will always be available: function populate-config() { \n  jq \" map (if .ParameterKey == \\\" $1 \\\" \n          then . + {\\\"ParameterValue\\\":\\\" $2 \"} \n          else . \n          end)\"\n} Now I can say cat config.json | \n  populate-config Project jess-project |\n  populate-config DockerInstanceType t2.micro > populated_config.json Piping to the populate-config function over and over lets me change multiple values. CAUTION: The jq map function works on arrays. It’s easy in this config file, because the JSON happens to be an array. If instead I’m changing a value within an object, such as: {\n  \" honesty \": \"Apple Jack\",\n  \"laughter\": \"Pinkie Pie\",\n  \"loyalty\": \"Rainbow Dash\"\n} then I must convert the object’s properties to an array , map over that array and then convert the array back into an object . cat ponies.json | \n  jq ' to_entries | map (if .key == \" honesty \" \n          then . + {\"value\":\" Trixie \"} \n          else . \n          end\n         ) | from_entries ' this gives: {\n  \" honesty \": \" Trixie \",\n  \"laughter\": \"Pinkie Pie\",\n  \"loyalty\": \"Rainbow Dash\"\n} That sneaky Trixie. Try running cat ponies.json | jq to_entries to see how this works. FURTHER INVESTIGATION: You might ask, how do I modify arrays of objects nested inside other objects? If you find the answer, please ping us on twitter @MonPlatformEng because I’m wondering this too. posted on May 22, 2015 by Jessica Kerr ← Previous Post Next Post → [{\n  \"ParameterKey\": \" Project \",\n  \"ParameterValue\": \"<changeMe>\"\n }, \n {\n  \"ParameterKey\": \"DockerInstanceType\",\n  \"ParameterValue\": \"m3.medium\"\n}] cat config.json | \n  jq ' map (if .ParameterKey == \" Project \"\n          then . + {\"ParameterValue\"=\"jess-project\"} else . end\n         )' > populated_config.json [{\n   \"ParameterKey\": \" Project \",\n   \"ParameterValue\": \" jess-project \"\n },\n {\n   \"ParameterKey\": \"DockerInstanceType\",\n   \"ParameterValue\": \"m3.medium\"\n}] function populate-config() { \n  jq \" map (if .ParameterKey == \\\" $1 \\\" \n          then . + {\\\"ParameterValue\\\":\\\" $2 \"} \n          else . \n          end)\"\n} cat config.json | \n  populate-config Project jess-project |\n  populate-config DockerInstanceType t2.micro > populated_config.json {\n  \" honesty \": \"Apple Jack\",\n  \"laughter\": \"Pinkie Pie\",\n  \"loyalty\": \"Rainbow Dash\"\n} cat ponies.json | \n  jq ' to_entries | map (if .key == \" honesty \" \n          then . + {\"value\":\" Trixie \"} \n          else . \n          end\n         ) | from_entries ' {\n  \" honesty \": \" Trixie \",\n  \"laughter\": \"Pinkie Pie\",\n  \"loyalty\": \"Rainbow Dash\"\n} posted on May 22, 2015 by Jessica Kerr Jessica Kerr", "date": "2015-05-22"},
{"website": "Monsanto", "title": "Using Big Data for Good", "author": ["\n                                        David Dooling\n                                    "], "link": "http://engineering.monsanto.com/2015/05/20/using-big-data-for-good/", "abstract": "Reading through blog posts, press releases, and source code you can easily get\nthe impression that everyone working in the Big Data space is focused on\ndetermining what ads you are most likely to click.  From Facebook to Google to\nTwitter, the modern players in IT collect massive amounts of information on\ntheir customers and then analyze that data to provide ever more relevant ads.\nAs Jeff Hammeracher, formerly of Facebook and now with Cloudera, famously said,\n“The best minds of my generation are thinking about how to make people click\nads.”  The result of this nearly singular focus is that many of the tools\ndeveloped to operate in the Big Data space aren’t easily applied to systems\nthat are much more complicated than counting words and providing links to ads.\nAs anyone who has tried to use NoSQL databases for complicated work flows on\ndistributed systems can tell you, pushing the limits of these systems that\neschew features for speed can be fraught with peril, forcing you to do a lot of\nengineering on your own. But engineering is not the only problem. Sometimes, the data just aren’t\nenough.  For example, when I was working at The Genome\nInstitute , now the McDonnell Genome Institute, we\ncollected petabytes and petabytes of genomic data, comprising the sum of data\nand analysis from The Human Genome Project ,\nsubsequent model genome references like mouse and chimpanzee, the first whole\ncancer genomes , the 1000 Genomes Project , The Cancer Genome\nAtlas , the Pediatric Cancer Genome\nProject , the Human\nMicrobiome Project , and many others.  While all of these\nefforts generated a lot of data and insights, they didn’t cure cancer.\nSometimes the systems we are dealing with, whether they be cancerous cells\ninside the complex environment that is the human body or producing fruit in the\ncomplex environment of soil, pests, and weather, provide such challenges to our\nunderstanding that simply collecting more and more data does not bring us\ncloser to answering our questions or solving our problems.  In these complex\nsystems, we are much more limited by our understanding of the many variables\ninvolved and how they interact than we are by our ability to collect data.  In\nthese cases, we need to marry Big Data techniques with models of these systems. Unfortunately, even our models of systems as complex as the human body and the\nenvironment are often too simple to be predictive.  This provides a real\nopportunity for Big Data in the land of complex science.  To date, Big Data has\nfocused primarily on collecting as much data as you can, worrying about how you\nwill use it later.  For these complex systems, this approach often just leads\nto more noise.  By combining the scientific method with the machinery of Big\nData, we can design large-scale experiments capable of collecting massive\namounts of data that provide answers to specific questions, chosen to create\nincreased understanding of these complex systems.  In other words, these\nexperiments can be designed to create better models.  These models can then be\nfurther refined with the next round of experiments, and so on.  This is exactly\nthe approach being used in genomics by the McDonnell Genome Institute and DOE’s Joint Genome Institute as they try to map features\nof human, plant, and microbial genomes to biological function. Executing these directed Big Data experiments has only recently become possible\nin genomics because of the ever decreasing cost of computing and storage and\nthe precipitous drop in the cost of DNA\nsequencing over the last decade (see\ngraph below).  For those outside the genomics world, similar changes are\noccurring now with the Internet of Things\n(IoT) .  As\neveryday objects like refrigerators and thermostats are becoming\nInternet-connected smart devices, the cost of incorporating this technology in\nfarming equipment large, e.g., combines, and small, e.g., water sensors, is\ndropping rapidly.  The resulting ability to collect a wide variety of data\ntypes rapidly, cheaply, and at scale completely changes our ability to measure\nthe environment in which a plant grows. At Monsanto, we are in a unique position to use directed Big Data\nexperiments on a large scale.  We can combine plant and microbial\ngenomics, soil science, and weather models informed by data collected\nfrom Internet-connected planters, sensors, and combines into our IoT\nplatform to make significant improvements in our understanding of how\nto create healthier plants and more optimal yields.  We call this\neffort “unlocking digital yield” and it provides the basis\nof how we intend to meet the needs of an ever hungrier planet.\nExecuting these experiments requires expertise across science,\nstatistics, and IT, both software and hardware.  We’ll be writing more\ndetailed posts about our approach in these areas soon, but until then\nhere are some slides from a talk Rob Long, one of our Data Architects,\ngave at StampedeCon 2014 about how we use HBase and Solr to analyze our large genomics\ndata sets. Managing Genomes At Scale: What We Learned - StampedeCon 2014 from StampedeCon Despite the prevailing notions in the popular press, there are a lot\nof great opportunities in Big Data beyond just selling ads.  There are\ncomplicated problems that not only do we not have the answers for, we\nare still trying to figure out what the solutions will look like.\nSolving these problems requires a diverse set of skills, and working\nin close collaboration with a variety of experts is one of the great\nthings about working in IT at Monsanto.  When you couple that with\nsolving real-world problems that affect tens of millions of people\nlike cancer, or hundreds of millions of people like producing safe,\nabundant food, working on Big Data can mean a lot more than just\ngetting a big paycheck. posted on May 20, 2015 by David Dooling ← Previous Post Next Post → Managing Genomes At Scale: What We Learned - StampedeCon 2014 from StampedeCon posted on May 20, 2015 by David Dooling David Dooling", "date": "2015-05-20"},
{"website": "Monsanto", "title": "Learn implicits: Scala reflection", "author": ["\n                                        Jorge Montero\n                                    "], "link": "http://engineering.monsanto.com/2015/05/14/implicits-intro/", "abstract": "Implicits are arguably the most unique and\nmisunderstood language feature of Scala.  Unlike other advanced features in the\nlanguage, they are very hard to avoid: most major libraries in Scala, starting\nfrom Scala collections, make heavy use of implicits. That use is not invisible\nto the users of the library, especially when we choose to look at the code. The\nother tricky part about implicits is that there are so many ways to use them,\neach with a different reason and pattern.  They can’t be fully explained in one\ncoherent post. This post explains one important use pattern on implicit parameters. It’s a\ngood place to start for understanding the hows and whys of Scala implicits. Reflection lets us ask about type information at runtime.  Java provides the .class method on all objects, but it is limited: type parameters are\ninvisible at runtime.  Scala’s TypeTags can give us details about type\nparameters that the Java compiler would normally erase.  And what is the\neasiest and most common way of obtaining a TypeTag? An implicit parameter. So for this exercise, we’ll see a little\nmethod that takes a List, and returns the type\nname of the contents of the list . import scala.reflect.runtime.universe._\n\ndef getInnerType[ T ] (list:List[ T ])(implicit tag:TypeTag[ T ]) = tag. tpe.toString Using that method, we can report on the inner type of a list: val stringList: List[String] = List(\"A\")\nval stringName = getInnerType (stringList)\nprintln( s\"a list of $stringName\") will print out a list of java . lang . String Great, we defeated erasure! But how did this work? How did that\nimplicit TypeTag get there? Compiler magic! The easiest way to think about implicit parameters is that they are\nextra parameters to a function that can be populated by the compiler\ninstead of being passed manually. In the case of TypeTags and\nClassTags, we do not have to do anything to make them work: the\ncompiler will always be able to provide an implicit TypeTag or\nClassPath parameter for all real classes (as opposed to generics,\nwhich we’ll cover in a minute). The context of the call to getInnerType knows that stringList is a List[String] , so the compiler fills in the implicit for us.  The\ncompiler fills the implicit as if we had called the method like this: import scala.reflect.runtime.universe._\n\nval stringList: List[String] = List(\"A\")   \nval stringName = getInnerType (stringList) (typeTag(String)) println( s\"a list of $stringName\") That typeTag method is defined in the scala.reflect.runtime.universe\nobject, and it triggers the same compiler magic as any request for an\nimplicit TypeTag[T] .  TypeTag is special: instances of TypeTag are\nbrought into being by Scala’s compiler. This contrasts with every\nother implicit parameter, supplied (one way or another) by the\nprogrammer. That’s why this is a great implicit to start with: see how\nto ask for it, before you learn how to supply it. The compiler is able to provide a TypeTag in this instance, because it\nknows the fully qualified type of stringList.  When getInnerType is\ninvoked, the compiler knows exactly what kind of List the parameter\nstringList is.  This would not work if instead of some specific type\n(String) the code calling getInnerType used a generic type .  For example, we will now add a gratuitous method that just delegates to getInnerType import scala.reflect.runtime.universe._\n\ndef gratuitousIntermediateMethod [ T ](list:List[ T ]) = getInnerType (list) val stringList: List[String] = List(\"A\") \nval stringName = gratuitousIntermediateMethod (stringList)\nprintln( s\"a list of $stringName\") This code fails to compile though, with this scary compilation error. Error:(36, 83) No TypeTag available for T\n  def gratuitousIntermediateMethod`[`T`]`(list: List`[`T`]`) = getInnerType(list) When the compiler tries to work on gratuitousIntermediateMethod, T is a generic type, so the compiler does not know\nwhat type it represents.  Therefore, it cannot provide the implicit\nparameter.  Only the callers to gratuitousIntermediateMethod will know\nwhat the type is, and therefore be able to provide the TypeTag. So to make this work, the intermediate method needs to request the\nTypeTag itself, so that the compiler can pass the TypeTag down to\ngetInnerType: import scala.reflect.runtime.universe._\n\ndef gratuitousIntermediateMethod [ T ](list:List[ T ])(implicit tag :TypeTag[ T ]) =\n   getInnerType(list) val stringList: List[String] = List(\"A\") \nval stringName = gratuitousIntermediateMethod (stringList)\nprintln( s\"a list of $stringName\") This works just like our example without the gratuitous method. a list of java . lang . String We need parameters that carry the TypeTag from where the type is\nconcrete to where the typeTag is needed.  This kind of pattern,\ncarrying implicits over, happens with other kinds of implicit\nparameters: Watch for future posts that reveal other examples of this\nimplicit-handoff pattern. Notice that the implicit parameter to getInnerType appears in a second\nparameter list.  Implicit parameters are always separated from\nexplicit parameters this way. We could declare getInnerType with only\none parameter list, all explicit parameters. Then the code would look\nlike: import scala.reflect.runtime.universe._\n\ndef getInnerType[ T ](list:List[ T ], tag :TypeTag[ T ]) = tag.tpe.toString val stringList: List[String] = List(\"A\")\nval stringName = getInnerType(stringList, typeTag(String))\nprintln( s\"a list of $stringName\") Comparing the code with implicits and the one without, there is one\nmajor difference: The top level code doesn’t have a trace of the\nTypeTag, so we do not have to know it is required. The compiler takes\ncare of it. We pay a little bit in complexity in the code that\nreceives the implicit parameter, in exchange for simpler calling code.\nThe library author suffers once, while the library clients benefit\nmany times.  No wonder implicit parameters are used all over the place\nin Scala! posted on May 14, 2015 by Jorge Montero ← Previous Post Next Post → import scala.reflect.runtime.universe._\n\ndef getInnerType[ T ] (list:List[ T ])(implicit tag:TypeTag[ T ]) = tag. tpe.toString val stringList: List[String] = List(\"A\")\nval stringName = getInnerType (stringList)\nprintln( s\"a list of $stringName\") a list of java . lang . String a list of java . lang . String import scala.reflect.runtime.universe._\n\nval stringList: List[String] = List(\"A\")   \nval stringName = getInnerType (stringList) (typeTag(String)) println( s\"a list of $stringName\") import scala.reflect.runtime.universe._\n\ndef gratuitousIntermediateMethod [ T ](list:List[ T ]) = getInnerType (list) val stringList: List[String] = List(\"A\") \nval stringName = gratuitousIntermediateMethod (stringList)\nprintln( s\"a list of $stringName\") Error:(36, 83) No TypeTag available for T\n  def gratuitousIntermediateMethod`[`T`]`(list: List`[`T`]`) = getInnerType(list) import scala.reflect.runtime.universe._\n\ndef gratuitousIntermediateMethod [ T ](list:List[ T ])(implicit tag :TypeTag[ T ]) =\n   getInnerType(list) val stringList: List[String] = List(\"A\") \nval stringName = gratuitousIntermediateMethod (stringList)\nprintln( s\"a list of $stringName\") a list of java . lang . String a list of java . lang . String import scala.reflect.runtime.universe._\n\ndef getInnerType[ T ](list:List[ T ], tag :TypeTag[ T ]) = tag.tpe.toString val stringList: List[String] = List(\"A\")\nval stringName = getInnerType(stringList, typeTag(String))\nprintln( s\"a list of $stringName\") posted on May 14, 2015 by Jorge Montero Jorge Montero", "date": "2015-05-14"},
{"website": "Monsanto", "title": "We are Monsanto Engineering", "author": ["\n                                        Ryan Richt\n                                    "], "link": "http://engineering.monsanto.com/2015/04/06/we-are-monsanto-engineering/", "abstract": "At Monsanto, we are engineers: in our history we’ve engineered molecules , materials , and medicines . Now as a purely agricultural company, we not only engineer plants, but the systems that develop, manufacture, monitor and deploy them. Monsanto might not be the first company that springs to mind when you think of cutting-edge software engineering. We’re a bit different: instead of being highly focused on a narrow segment of IT like social media analytics, Monsanto is all about diversity (and not just this kind ). In the coming posts, we will share our experiences and code for dealing with a multitude of topics. We have several image analysis and computer vision teams, on projects like our robotic automated greenhouse. We develop both the hardware and software to acquire hyper-spectral images both in controlled environments as well as in the field. We have also have several computational biology teams, who mine petabytes of DNA sequence data for plants, microbes and insects. Over the past decade our distributed analytics pipelines have matured from hundreds to more than a thousand processor cores. Crossing over between research and manufacturing, we have a globally distributed cloud-based Internet of Things (IoT) platform to collect and analyze realtime planting, harvesting, and logistics data from a multitude of sensors, some of which we’ve built on top of RasPi, Arduino, and custom PCBs. On the commercial side, our IFS team has built a predictive analytical model and cloud platform that combined with hundreds of data points per second from our precision planters optimizes yield using only software . Monsanto is a large company, but we’ve aggressively pursued cutting-edge technology. In addition to the requisite relational and Oracle based applications, we’re also running a variety of NoSQL DBs in production. Since 2010, we’ve developed applications on top of Hadoop and CouchDB (and we’ve released a monadic CouchDB connector, Stoop ). At an upcoming talk we’ll be showcasing one of our Neo4J production applications . We’re also using Riak, Rabbit, Kafka, and Cassandra in major production applications, in addition to our own multidimensional array database Mandolin . While Java makes up the core of our applications at Monsanto, and we have pockets of Clojure, Python, R, Perl, .Net and other languages, Scala is now our standard back-end language. Most, if not all, of our new development is occurring in Scala. In our first few posts, we are excited to highlight and release our code for rapidly developing microservices applications on Scala/Docker/AWS. An upcoming post will introduce and release our synthetic “type system” we implemented in Scala’s type system to generate AWS CloudFormation. Having used Scala at Monsanto since 2010, we can’t wait to share not only our libraries but other explorations in monads, type-level programming, and general type-foolery of the form (type members and refinements FTW!): // TODO: Also add QueryParams object / type case class GET [ PathParameters , P <: Path { type Params = PathParameters } , Body <: CanonicalModel [ Body ] , PathMapperOut , R ]( path : P { type Params = PathParameters }, pathFactory : PathMapper [ PathMapperOut , PathParameters ], bodyFactory : CanonicalModelFactory [ Body ] ) ( f : ( PathMapperOut , Body ) => R ){ def run ( url : String , item : Body ) : Option [ R ] = path . parse ( url ). map ( bits => f ( pathFactory . tupled ( bits ), item )) } IT at Monsanto is characterized by its amazing diversity. With modern tools, we are tackling exciting problems in domains as diverse as genomics and computer vision. Now we are opening up these tools and experiences to the community. We hope you’ll join us back here for our next post on automation in AWS. posted on April 6, 2015 by Ryan Richt ← Previous Post Next Post → posted on April 6, 2015 by Ryan Richt Ryan Richt", "date": "2015-04-06"},
{"website": "Monsanto", "title": "Stoop: our first open source release", "author": ["\n                                        Phil Cryer\n                                    "], "link": "http://engineering.monsanto.com/2015/04/05/stoop-our-first-open-source-release/", "abstract": "We are happy to annouce the release of our first open source software, Stoop . Based on an existing Haskell library , Stoop is a Scala DSL for interfacing with CouchDB. Implementation can be easily switched between talking to an actual Couch or a fake (mock) one for testing. We’ve released this under the Modified BSD License, so check it out and let us know if you have any questions, if you find bugs, or even if you have a pull request to fix something! name := \"Stoop\" version := \"0.9.10-SNAPSHOT\" scalaVersion := \"2.10.3\" scalacOptions += \"-deprecation\" scalacOptions += \"-feature\" resolvers += \"Sonatype snapshots\" at \"http://oss.sonatype.org/content/repositories/snapshots/\" resolvers += \"Scalaz Bintray Repo\" at \"http://dl.bintray.com/scalaz/releases\" resolvers += \"spray repo\" at \"http://repo.spray.io\" libraryDependencies ++= Seq ( \"org.scalaz\" %% \"scalaz-core\" % \"7.1.0\" , \"io.spray\" %% \"spray-json\" % \"1.2.5\" , \"org.scalaz\" %% \"scalaz-effect\" % \"7.1.0\" , \"org.scalaz\" %% \"scalaz-concurrent\" % \"7.1.0\" , \"org.scalaz.stream\" %% \"scalaz-stream\" % \"0.6a\" ) libraryDependencies += \"net.databinder.dispatch\" %% \"dispatch-core\" % \"0.10.0\" libraryDependencies += \"org.scalatest\" % \"scalatest_2.10\" % \"2.0\" % \"test\" libraryDependencies += \"org.scalacheck\" %% \"scalacheck\" % \"1.10.1\" % \"test\" publishMavenStyle := true posted on April 5, 2015 by Phil Cryer Next Post → posted on April 5, 2015 by Phil Cryer Phil Cryer", "date": "2015-04-05"}
]