[
{"website": "JustTakeAway", "title": "Run your tests faster with parallel cucumber….", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/10/12/run-your-tests-faster-with-parallel-cucumber/", "abstract": "Here in the JUST EAT web team, we have quite a few UI tests to run on each build we produce. We know that in order for these tests to remain useful to the team, it’s important to keep the speed of execution to a minimum. There are a few ways that we can improve speed of execution for our tests. One approach is to not have many UI tests, and to write fast running, in memory, integration tests instead (this is something we’re currently working on – I’ll write a new blog post about it in a few months)! Another approach is to parallelise the UI tests. Recently, we’ve been experimenting with the parallel_tests gem in Ruby to do just that. First of all you need to install the parallel_test gem in your directory. You can find more info here: GitHub . Once you’ve successfully installed the parallel_tests gem, then you’re ready to roll. Run features in parallel The following basic command will run all the features within the ‘features’ directory: $ bundle exec parallel_cucumber features/ But if you want to run parallel_tests using a specific profile or tags, here’s an example of how your can run your tests: $ bundle exec parallel_cucumber features/ -n 6 -o '-t  @smoke -t ~@manual' You can specify the number of processes to speed up tests executions by using option ‘-n’, as in the above example. By default, it uses 4 processes. Outcome of the parallel tests The following reports indicate the summary of the test execution before and after implementing parallel_tests in our project. We were able to reduce the test execution time from 45% to 62% as a percentage for different test builds. The test execution time varies with each test builds and also depends on few factors  such as… number of feature files number of tests in each feature file number of headless and browser tests in each build Overall, we’ve reduced the test execution time by 56% on average. Before After Generating HTML Reports We can generate HTML report for the each process. We chose to create a profile called ‘parallel’ in our cucumber.yml file as follows: parallel: --format html --out report<%= ENV['TEST_ENV_NUMBER']%>.html This will create HTML report with name as report.html, report2.html and so on. The variable ENV[‘TEST_ENV_NUMBER’] gives details about all the processes that cucumber scenarios are executing. Re-running Failed Scenarios This section describes how you can re-run only the failed scenarios with parallel cucumber. Set up Update your existing parallel test profile to log failing scenarios in cucumber.yml as follows: parallel: --format html --out report<%= ENV['TEST_ENV_NUMBER']%>.html --format ParallelTests::Cucumber::FailuresLogger --out cucumber_failures.log This will create cucumber_failures.log with all the failed scenarios which we can use to rerun. Now, run your features with that ‘parallel’ profile: $ bundle exec parallel_cucumber features/ -n 6 -o '-t %tags% -t ~@manual ENVIRONMENT=%environment% -p parallel' This will run your entire features in six different processes and will log the failed scenarios in the ‘cucumber_failures.log’ file. Re-run failed scenarios Since we have ‘cucumber_failures.log’ file with all the failed scenarios, we can use that file to re-run failed cucumber scenarios. $ bundle exec cucumber @cucumber_failures.log --format html --out test_results_rerun.html ENVIRONMENT=%environment% test_results_rerun.html gives the report of re-running failed scenarios. Also, you need to make sure you delete cucumber_failures.log before each test execution. I’ve created the following powershell task (as the cleanup task) in the TeamCity to delete the log file before the execution of parallel test command. $filePath = \"$scriptDirectory\\cucumber_failures.log\"\nif (Test-Path $filePath) {\n Remove-Item $filePath\n} Here’s an example of how the steps have been executed for smoke tests in the TeamCity… But you can create a separate rake tasks to run parallel tests, rerun failing tests and cleanup failure logs depend on your work environment. Summary There’s a couple of things to bear in mind with this tool. The first – the best way to speed up your UI tests is to not have many UI tests! However, if you are going down this route, then you need to keep in mind that one feature will run on one process. This tool will not break down your scenarios which are written in one feature file, into many processes, so you don’t get much control over how the feature files are split into processes unfortunately. Thanks for reading! Deepthi", "date": "2015-10-12"},
{"website": "JustTakeAway", "title": "Acceptance Testing in Appium", "author": ["Written by Jack and Sneha"], "link": "https://tech.justeattakeaway.com/2015/09/21/acceptance-testing-in-appium/", "abstract": "Why use acceptance testing? A well-tested application measures the quality of the application and helps achieve good compatibility between the users and devices (iOS and Android devices in our case). To get the most out of our testing efforts and test coverage, a robust and cross-platform tool is required to automate the application. Appium is a pretty good choice which fits the bill. Acceptance testing gives us confidence in two main areas: that new features have been built to the correct specification; and existing features continue to function after new integrations. It’s notoriously difficult to prove that developers have built the feature they were asked for. One reason for this is that the use of natural language to describe feature specifications can result in ambiguities and misunderstandings between developers and stakeholders. One approach being undertaken by a number of teams in International Engineering at JUST EAT is to produce executable specifications. This has numerous benefits in terms of understanding the requirements of a project through user stories but, in particular, it give us specific steps that can be tested to verify a feature has been implemented correctly. These step sets are called scenarios, and capture the flow of a specific task that a user wants to perform. The following are some examples from our own specifications… Scenario Outline: Make a successful order – Login to JUST EAT through Facebook Given I am on the home screen And I search for a restaurant And I add items to basket And I login through < social > account < email > and < password > When I checkout with cash payment Then the order should be placed successfully Examples: | social | email | password | | facebook | test@test.com | test | Scenario: View order details on Order History screen Given I am on the home screen And I have previously ordered from this device And I navigate to order history When I select an order | Test Restaurant | Then I am shown the details of that order These specifications are written in Gherkin syntax, which is supported by the Cucumber suite of test tools. The specific tools we chose to use are outlined in the next section. Summary of technology The iOS Webkit Debug Proxy allows access to webviews and their content. It provides an interface to the various components of a web page within a webview using the same contract as that used for the native application. For the JUST EAT app we use it to facilitate automation of the checkout flow so that we can enter delivery details, make an order and read the details of the order confirmation page. As part of the initialisation of the Appium process, it launches the iOS Simulator. Appium communicates with the simulator using Apple’s UI Automation layer. This layer provides Javascript APIs to access the UI of the application at runtime. Appium then wraps this interface in another API that conforms to the JSON Wire Protocol. The benefit of using this abstraction is that it standardises UI access for different platforms meaning the same tests can be run for a variety of platforms. While the Appium server provides the interface to access the UI of the running application, Cucumber JS is used to execute our scenarios defined in Gherkin syntax. The code that backs these steps contains the procedures to send commands to Appium. NodeJS underlies most of the technologies listed above. NodeJS implements a non-blocking I/O model and an event loop that uses a single thread and performs I/O asynchronously. Mocha, Chai and Chai-as-Promised were among other modules used to provide additional testing support. Page object model Since we have the apps on iOS and Android platforms we created a single test suite which can run the same set of tests on both platforms, to avoid duplication of test code and save time, and Page Object Model helped us achieve this. Page object models each represent a screen or group of controls in the user interface. For example, the home screen of the iOS app can be represented by a page object that provides a text field and a search button as its interactive elements. These elements can be obtained by ID, Xpath, Class name or by Name, and stored as properties on the model to be used during the test. This adds a level of abstraction and the tests use an instance of the page object to interact with the UI elements rather than worrying about the element locators in the test itself. The abstraction is useful because the model is tightly coupled to the user’s perception of the user interface, and so is more appropriate for acceptance testing. Another benefit is that we can add elements to a given model over time, as needed by the tests, instead of including all the elements on the screen that the tests do not interact with. Below is a diagram of the test project structure, indicating where the page objects are located. Acceptance-Tests\n    |_ features\n       |_ PageObjects\n       |  |\n       |  |_ Android\n       |  |  |_ HomeScreen.js\n       |  |  |_ MenuScreen.js\n       |  |\n       |  |_ iOS\n       |     |_ HomeScreen.js\n       |     |_ MenuScreen.js\n       |\n       |_ step_definitions\n       |_ support PageObjects/Android/HomeScreen.js var HomeScreenObjects = function(){\n   this.searchButton = {\n        type: \"id\",\n        value:”search_button\"\n    };\n};\nexports.homeScreen = HomeScreenObjects; PageObjects/iOS/HomeScreen.js var HomeScreenObjects = function(){\n   this.searchButton = {\n        type: “class name\",\n        value:”UIAButton\"\n    };\n}; We direct the tests to use the page objects from the right folder, based on the platform we are running the tests on. var pageObjectsPath = platform == \"Android\" ? \"../PageObjects/Android/\" : \"../PageObjects/iOS/\";\n// Imports\nvar HomeScreen = require(pageObjectsPath + “HomeScreen.js\").homeScreen;\nthis.clickSearchButton = function clickSearchButton(driver){\n    var home = new HomeScreen();\n    return this.driver\n            .element(home.searchButton.type, home.searchButton.value)\n            .click();\n}; A potential issue with a cross-platform test suite is that you may have non-uniform UI designs across platforms. Luckily, our UX designers and developers make sure the flow is uniform across iOS and Android, meaning that we don’t have to add lots of “if(platform)” statements in the test code. Continuous integration A major use of automated acceptance test is verifying the validity of new and existing features during automated build processes. To this end, we created a new script that runs the tests on TeamCity. The script itself takes a number of arguments to allow configuration of the test environment for different build jobs: Platform Name specifies which platform it runs on, i.e. iOS or Android. Device Name specifies the type of device to run on, e.g. iPhone 6 Plus, Samsung Galaxy S4. Platform Version allow a particular SDK to be targeted, e.g. iOS 7.1, Android 4.0.4. App Path specifies a path to the app executable under test. Environment was a custom option introduced to allow the selection of a particular QA environment, e.g. QA19, QA99, Staging. Country Code lets the tests know which flavour of the app is under test. Cucumber Flags allows some additional configuration to be passed to the grunt task runner. To integrate it with TeamCity we took the following steps… We created a new build template and added a number of configurable parameters for the script arguments. We added build features to allow consumption of the JUnit report and archiving of the HTML report. We added an artifact dependency to ensure that the most recent valid build would always be under test. Reporting Reporting test results is an important step in continuous integration as it allows tracking of test results over time, as well as providing a quick insight for interested parties without requiring them to read test metadata files such as XML and JSON. We use a selection of Javascript tools to produce and convert the output of Cucumber JS. Grunt provides the framework in which to execute Cucumber JS and consume the test output, through a simple task runner with various configurable settings. The JSON output produced is simple and readable but not necessarily compatible with continuous integration reporting tools. To this end we use protractor-cucumber-junit which converts the JSON into two formats: HTML provides a simple and readable page that any non-technical user can access for a quick overview of test results. JUnit XML is almost universally consumable by CI tools allowing native presentation of results in your CI front-end of choice, as well as enabling CI tools to track trends in testing over time, code coverage, and so on. Simulation across platforms iOS simulators are used, which are included as part of Xcode and hence can only be run from a mac. Xcode can run only one instance of instruments at a time and therefore, we cannot parallelise the iOS test run. Genymotion is being used for Android emulators as it’s more promising than the Android emulator that comes with the Android sdk. Read an awesome blog on how to use Android emulators on CI, by Andrew Barnett, here [http://tech.just-eat.com/2015/02/26/using-android-emulators-on-ci/] Problems and challenges We’re using simulators and emulators to run our acceptance tests – as they give us more coverage in terms of devices and also an easy way to test any small changes. Nevertheless, not all types of user interactions can be tested on emulators and the iOS simulators in particular have problems with certain Appium gestures. Further, during the test execution there are no real events like battery consumption and interrupts and hence, we do not get accurate results regarding the performance of the app. Since we are in initial stages of automating the apps, we find simulators/emulators as a good stop-gap while experimenting with continuous integration. In the future it would be desirable to use a cloud-hosted service such as SauceLabs. This would allow tests to be run across many devices and OS versions in parallel.", "date": "2015-09-21"},
{"website": "JustTakeAway", "title": "My life as a robot", "author": ["Written by Dave Williams"], "link": "https://tech.justeattakeaway.com/2015/08/26/my-life-as-a-robot/", "abstract": "Be there when you can’t be there Every quarter the Tech teams at JUST EAT take part in a 3-day Hackathon to get away from the day job and generate amazing innovation ideas. As it happens, when the most recent Hackathon occurred I was scheduled to be in Australia visiting our newly acquired companies, Menulog and Eat Now. Never being one to miss an opportunity for a bit of fun we hatched a cunning plan to allow me to attend. Given how widely distributed our Tech teams are (with a significant presence in Bristol, the City of London, Borehamwood and Kiev in Ukraine) we’ve spoken from time to time about using telepresence robots but were always put off by the price. Recently though, a colleague spotted a new product which seemed to fit the bill and cost a lot less than other models we’d seen. What better time than Hackathon week to give this a try? Speaking to the team at Savvy Robotics they agreed to loan us a Double Robotics telepresence robot, which is a gyroscopically balanced robotic stand which uses an iPad Air to create a fully mobile videoconference. You can drive it from pretty much any device or web browser anywhere in the world. For the purposes of the Hackathon, this was going to become Robo-Dave. ​ Not wanting to spoil the surprise for colleagues we tested Robo-Dave in an out-of-the-way comms room in our London offices, whilst I sat in my Hotel in Sydney using their free wifi. Hackathon day came and I returned to my hotel in Sydney before midnight to join the meeting. Logging into Robo-Dave I immediately recognised where I was parked and (with my minder in tow) I began driving up to our video wall where the demos were being held. It was a good job I had the minder, as free hotel wifi and a 10,000 mile fibre-optic journey meant things were quite laggy. The video and audio reception were great on my end and good enough for my colleagues in London that people were able to recognise me and have a chat. ​ Having managed to maintain the element of surprise it’s fair to say that the Double generated a lot of interest, from people saying hello to others stopping me to video or photograph, and I really felt like I was present at the Hackathon. The first rule of Hackathons is to have fun, and I’d like to think our little experiment added to that… the fact that (unbeknown to me) I spent much of the meeting wearing a sequinned red fedora and a ‘kick me’ sign, suggests I may be right. Having returned from Australia we carried out some more testing for staff, working from home and between our offices in Bristol, Kiev and London. Everyone who had the opportunity to use the robot was positive about the experience. The lag was minimal (no minder required) and the video quality was even better. Most importantly we discovered it’s not just a gimmick, there’s a subtle distinction in the way that people interact with you and the experience is deeply anthropomorphic. You feel like you’re there when obviously you’re not. In fact one of our Heads of Technology commented that he’d bumped into a colleague in the corridor and had a chat about an idea for our payments team. Note it was ‘he’ who ‘bumped into’ his colleague… you just don’t get that chance interaction through Hangouts or Skype. We were so pleased with our loaner unit that we’ve ordered a Double so we can carry out a full pilot in each of our UK offices. A massive thank you to the team at JUST EAT, and the great folks at Savvy Robotics for making this happen.", "date": "2015-08-26"},
{"website": "JustTakeAway", "title": "Why we are moving to Sketch", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/08/17/why-we-are-moving-to-sketch/", "abstract": "I’ve always liked to describe Sketch as the love child of Photoshop and Illustrator. Imagine stripping back Photoshop to the basic UI tools you need, making it vector-based and having additional features that make your life a lot easier as a UI Designer. Subtle differences such as being able to change the corner radius of a shape after it’s been created, quick application of gaussian blur to layers for iOS apps and great exporting capabilities are a few examples that make the tool more practical than Photoshop. Photoshop was originally designed for photo editing and this becomes evident when comparing to a tool like Sketch. Just as a disclosure: this software would be ideal for UI Designers who believe in sustainable and flat design that follows native patterns. If you find yourself using textures and extensive layering and effects in your designs, then this isn’t the tool for you. Sketch has been adopted by many major players in the industry, such as Apple, Facebook, and Google to name a few, and this shows the extent by which the tool has taken the industry by storm. Here at JUST EAT we made the transition to Sketch not only for the functionality detailed below, but because it was so easily integrated into our product cycle. The small price – a mere £80 one off payment – meant that we could extend the tool out to our native engineers who are beginning to use Sketch files as UI reference, instead of requiring a dull design spec with hex codes, font sizes, padding values and so on. Brace yourselves, this may be the end of redlining.There are a number of additional features in Sketch, which are made specifically with UI design in mind. You can set font styles to text (similar to InDesign), which means that changing the body style across all screens can be done with one click. Say you get feedback that the font weight for body is too heavy, this can now be a much simpler task than having to trawl through and set the new style to each layer. Additionally all screen designs can be within one Sketch file thanks to artboards (as seen in Illustrator) and can be exported all at once. Photoshop has recently added this feature, which shows that they consider Sketch as a credible competitor. Sketch is also great because they open up the tool to many third party developers, which means if you think “wouldn’t it be nice if I could do this in Sketch?” it’s probably already available online . My favourite plugin helps me quickly export assets for Android with a simple click of a button, instead of manually creating a single asset in five different sizes. Some more tips for designers: Zoom in and out by small percentages by holding down cmd and scroll your mouse. When you click on an item it selects the top level group layer. In order to click on the item layer you need to hold down cmd. Masking with a fade tutorial Most get overly excited by symbols and create master elements such as the header and tab nav but it actually causes more issues. It’s best to use this for icons instead. Preview designs live on a phone instantly with the Mirror app Save time designing by using these free UI kits When you paste an item in Sketch you tend to lose it – hold down alt and it will show red guides that helps you locate the item. Some tips for developers: Select an element by holding down cmd + click to reveal right panel with values of the dimensions, font styling, colours, opacity, angle, corner radius, and so on. Hold alt to see the distance between elements for correct padding and margins Export iOS assets by defining the outputs in the right panel (we use PDF as opposed to the traditional 1x 2x 3x) Export Android assets using this plugin If you haven’t already done so, go ahead and download the free trial !", "date": "2015-08-17"},
{"website": "JustTakeAway", "title": "A journey – From NoSQL to Elasticsearch", "author": ["Written by Anton Jefcoate"], "link": "https://tech.justeattakeaway.com/2015/09/14/a-journey-from-nosql-to-elasticsearch/", "abstract": "JUST EAT’s UK restaurant search was based on postcode districts, but this was a very rigid approach, not very accurate for large districts and was only updated periodically. What we wanted was a way to customise the delivery areas, per restaurant, at a greater level of accuracy with near real time updates. With the power of Elasticsearch and some very clever maths we were able to create and search restaurant delivery areas using polygons. NoSQL implementation We ran a ‘Search Lifecycle’ scheduled job (multiple times per day) that collated all of our restaurants and the districts they delivered to, creating an inverted index with the district as the key and a collection of the restaurant IDs as the value. When a consumer searched for SE1 1AA we searched only on the district, so stripping 1AA and searching for restaurants that deliver to SE1, which mapped to a key in our inverted index. This allowed fast retrieval of search results by postcode. search lifecycle indexing workflow NoSQL limitations One of the main drawbacks of the NoSQL implementation was that delivering to whole districts is not the same as how the restaurants work day-to-day – in reality a restaurant is willing to travel a certain distance, internally we mapped this to districts. Districts can be very large, a restaurant that delivers up to, say, two miles from its premise may cover a certain section of a district, but they may not necessarily want to deliver to all of it. There were two ways to fix this… Change to use more accurate postcodes ie. sectors like SE1 1xx (where the xx part is ignored) . Use polygons to map out exact areas where a restaurant will deliver. We decided on option two, polygons. The second option allows us to have any custom shape that can be drawn on a map which is more accurate than districts and easier to manage than sectors, the main disadvantage is that it requires a wider change to both our indexing process and our Search API. Polygon all the things When looking at different implementations of polygon search we looked at Postgres and Elasticsearch , we decided on Elasticsearch because of the added features that it provides on top of polygon mapping . There were a number of factors we needed to work out before we could implement full postcode searching… How do we transform our current districts into polygons? How do we search an Elasticsearch index for restaurants based on a full postcode? Transforming districts into polygons We purchased polygons for all districts in the UK and saved this to a DynamoDB table, with the district as the key and the collection of points as the value, creating an operation in an existing API to return the polygon based on the district. We already had an API operation for saving delivery areas (districts) for a restaurant, and didn’t want to have to change how this behaved so we published an SNS message using the JustSaying message bus, the message contains the restaurant ID and list of districts. We have an existing worker service which listens to restaurant messages and performs long running tasks, we added a new handler to listen to listen to this message and publish an enriched message to be consumed by a different worker service. This service is used for transforming the message into a GeoShape to be saved into the Elasticsearch index. elasticsearch document indexing workflow Searching full postcodes using Elasticsearch First we had to design the mapping for our index. We decided on using the nested type to store the delivery areas because it allows us to add additional metadata to each area for future features. {\n   \"properties\" : {\n      \"deliveryAreas\" : {\n         \"properties\" : {\n            \"area\" : {\n               \"type\" : \"geo_shape\"\n            }\n         },\n         \"type\" : \"nested\"\n      },\n      \"restaurantId\" : {\n         \"type\" : \"integer\"\n      }\n   }\n} Searching for a full postcode requires us to turn the postcode into a set of latitude and longitude coordinates, so SE11AA becomes latitude 51.501961 and longitude -0.091652. We use the GeoShape Filter to search the nested child documents. {\n   \"_source\" : false,\n   \"query\" : {\n      \"filtered\" : {\n         \"filter\" : {\n            \"nested\" : {\n               \"filter\" : {\n                  \"geo_shape\" : {\n                     \"deliveryAreas.area\" : {\n                        \"relation\" : \"intersects\",\n                        \"shape\" : {\n                           \"coordinates\" : [ -0.0916520, 51.501961 ],\n                           \"type\" : \"point\"\n                        }\n                     }\n                  }\n               },\n               \"path\" : \"deliveryAreas\"\n            }\n         },\n         \"query\" : {\n            \"match_all\" : {}\n         }\n      }\n   }\n} You may notice the _source = false part of the query. When we search for restaurants we only need to know the IDs as the consumer only wants to know which restaurants deliver to them, not which delivery area they fall into, as the restaurant ID is the ID of the document no fields are required. Setting _source to false improves performance as less data is returned for each search. Custom polygons Once we had all districts mapped and indexed into our Elasticsearch cluster we wanted to address the original feature that polygons were meant to address, delivery distance limiting. delivery areas without distance limit Based on the location (latitude and longitude) of the restaurant and a mile limit (as the crow flies) we wanted to clip the existing districts to create new polygons, but how did we achieve this? We used Trigonometry to create a circular polygon from the restaurant location and a mile limit and with the coordinates from the district polygon and with the help from an open source C# library Clipper we were able to create a new set of polygons based on the point intersections. delivery areas with distance limit GeoShape queries work based on coordinates, the coordinates for the polygon could be a district, a circle or even the shape of a penguin, as long as the consumer’s location falls into that shape it doesn’t matter what it is. What have we learnt? We were relative novices when implementing our first Elasticsearch index, we had lots of obstacles to overcome, here are just a few of the things that we learnt along the way… If you only require the ID to be returned in a query then omit the _source field, reducing the overhead of the query and improving performance. Create a read alias for your live index, allowing you to hot swap to a different index in case it needs to be rebuilt. Create a write alias, this can be made to have multiple indices so you can write to more than one index at a time. Or, with the use of the read alias you can rebuild your entire index without affecting the reads, this can be achieved by setting the write alias to a new index on a node that is not currently in the same cluster and having replication disabled. Setting a decent refresh interval can greatly increase how quickly you can index your documents, you can see some of the performance benefits from this blog post , we decided on 60 seconds, the gains seem to flatline much after that. Dynamic mapping can be a curse, especially with polygons. A polygon is an array of arrays of arrays of coordinates, if you do not explicitly set the mapping to geo_shape then it will set it as an array of numbers and your queries will not work. We’ve only scratched the surface of what can be achieved with Elasticsearch, our indexing architecture is a foundation to easily extend and adapt new search features.", "date": "2015-09-14"},
{"website": "JustTakeAway", "title": "Managing OKRs at JUST EAT tech with our own web app", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/08/03/managing-okrs-at-just-eat-tech-with-our-own-web-app/", "abstract": "Summary This blog post shares some lessons learnt from the process of building the JUSTOKRs priority and progress tracking web app and the process of rolling it out across JUST EAT’s large technology organisation. Context There are now just over two hundred people in the JUST EAT technology team, where technology includes development, testing, product management, UX, ICT etc. For perspective, six years ago we had seven people … so we’ve grown pretty quickly. We now operate in thirteen countries, across a mix of group built and acquired technology platforms with many different cultural, marketing and legislative challenges. In late 2014 the UK Engineering area was made up of ten teams working on a range of components, including: Customer facing desktop and mobile websites Customer facing Android and iOS apps Restaurant facing devices and apps Customer care tools Public and private APIs that provide data to the other components Challenges As we continued to grow our teams we identified some challenges: Managing cross team dependencies We’d seen a few cases of implicit dependencies between the API teams and the teams which consumed their APIs, which became apparent very late in development – meaning that a consumer team became blocked whilst an API team caught up. Appropriate & timely communication As the organisation grew we found it increasingly important to communicate what the teams were working on to a range of stakeholders at different levels of detail. For example communicating at a high level to country operational management and in depth within the technology organisation. Downstream delivery horizon As a technology organisation we have many downstream consumers who want to know roughly when we’ll deliver something we’re building e.g. a marketing team who may want to run a marketing campaign on a new feature or the customer care team who need training on a new tool we’re building for them. JUST EAT had started using the OKR (Objective & Key Result) methodology popularised by Google, at a high level, a couple of months previously (see more here ).  The management team determined that we should adopt OKRs within technology to address these challenges and started by building the mother-of-all-spreadsheets to track team dependencies. The spreadsheet approach worked well but many people found it unwieldy and it was difficult for stakeholders outside of technology to understand. At this point I felt that a system was needed to track OKRs rather than a spreadsheet and carried out a review of systems available in the market.  Nothing quite met the bill and therefore I started building a tool that would help us manage progress in the same way as the mother-of-all-spreadsheets but which would be much easier to use. I’d previously built a project using node.js for the backend APIs and Ember.js for the frontend and really enjoyed the convention over configuration and ( stability without stagnation ) approaches of Ember and being able to code APIs and front end in the same language. JUSTOKRs We’ve made the JUSTOKRs available as open source here .  It’s structured around a few concepts: High level Objectives such as “Ensure first time users come back”; Key Results such as “Apple Watch app available on launch date” Deliverables , which are the concrete things that teams might want to build to deliver against the key result including apps, APIs etc. The tool offers a high level overview of OKRs for stakeholders, showing when key results are expected to deliver, to help downstream teams such as marketing and customer care to plan their campaigns and agent training: [Summary view with pixelated text] Plus views for individual teams to manage their priorities and understand dependencies on other teams: [Team view with pixelated text ] A few learnings along the way The asynchronous nature of node.js and callback hell Node.js applications are single threaded and therefore it’s important to use the asynchronous model to keep them performant. However for someone needing to write APIs which often combine a number of steps that need to be completed in sequence, (eg authentication, database query and database update) writing pure asynchronous code can result in code which is really difficult to write and comprehend. My solution, which I’m sure that node purists would eschew, has been to use the va_sync library to join steps together in a sequential flow.  For this application at least, sacrificing a little speed for code readability is the right compromise. Active Directory Authentication Initially a basic tokenised authentication mechanism was used, but as tool started to be rolled out we adopted Active Directory for authentication in order to use the same mechanism as other internal tools and to reduce administrative load. We went through a few different iterations along the way of trying to get node APIs and front end code to communicate to AD but the final solution was actually very straightforward and just involved a call out to AD with it calling back to a node API. Multi tenanting for international and other parts of the business The tool was originally built for the teams developing the platform for the UK, but as we started to show that it was useful there were requests from other parts of technology who wanted access to the tool. I looked at how we could make the app multi tenanted so that different areas of technology (eg UK, International & Partner Product Development) could all build and maintain their own OKRs within the tool but could get visibility on what other areas are working on if wanted. The tool has now been adopted by all areas of technology and we’re already finding that by using a shared tool we’re seeing opportunities to collaborate and share knowledge across different areas, in a way we may not have done when each area was using their own systems (aka unwieldy home-made brother-and-sisters-of-all-spreadsheets) to manage priorities and progress. Timeline view When I showed the tool to one of our international teams they said that they’d like to be able to show their in country stakeholders a more visual view of work in progress.  I needed to do a couple of iterations to get to the right view but ended up with a Kanban style view to show progress, which makes it easy to understand at a glance how each team are doing. [Timeline view with pixelated text ] It’s worth noting that we use this view in conjunction with JIRA – JIRA is used to manage detailed technical tasks, whilst the JUSTOKRs tool (and the timeline view specifically) provides a higher level view of progress in stakeholder friendly language. Looking Forward Getting to Ember 2.0 – as the tool has been developed the core Ember team have been moving toward Ember 2.0 . This has been done by incremental changes in a point release every six weeks with a set of suggested changes, shown in the console and in Ember Inspector , the in-browser debugger. I kept up with many of these releases and made the changes needed to take advantage of simplified syntax, additional features and removed deprecated features. However the release of 1.11 required a pretty substantial number of changes to JUSTOKRs app and coincided with me moving to another role within JUST EAT. Doing the work to get to 1.11 and beyond, including the dramatically faster Glimmer rendering engine , is a job for another day… or maybe another contributor (hint, hint)… Summary In general, making the decision to build a tool in house is something that should not be taken lightly. It can be time consuming in terms of getting it built and adopted. However, building a tool in house means that you can tailor it to the exact needs of the organisation and get very quick feedback on new features implemented and turnaround changes very swiftly. From an organisational perspective the tool is now used across all of technology and helps engineers, product managers and programme managers to plan and sequence their work and stakeholders around the business to understand progress.  It contributes toward making dependencies explicit and toward our constant improvement in delivery pace – all helping to “enable our customers to love their takeaway experience”. Thanks Finally I’d like to thank firstly everyone within JUST EAT who has used the tool & given feedback and secondly the broader Ember community – without your support the tool couldn’t exist.", "date": "2015-08-03"},
{"website": "JustTakeAway", "title": "Hailo Tech Talk @ JUST EAT", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/07/24/hailo-tech-talk-just-eat/", "abstract": "Sharing tech stories Last week, Hailo kindly hosted our very own Pete Mounce , who went to give a talk about how we do performance testing with fake load and monitoring here at JUST EAT. This week we were delighted to welcome Jose Nascimento, Head of Technology and Boyan Dimitrov, Platform Automation Lead from Hailo to come give a tech talk at our offices. We loved hearing about their Micro-services architecture and the effect it has on their culture, as well as the business benefits they get and what capabilities it gives their engineers. JUST EAT Tech are hoping to welcome other companies in for talks on a monthly basis, in order to promote sharing within the tech community. Thanks to Jose, Boyan and Pete we are off to a flying start! You can see the talk here: https://www.youtube.com/watch?v=ab9TyP7mH4M&feature=youtu.be&t=204", "date": "2015-07-24"},
{"website": "JustTakeAway", "title": "Google I/O from a designer’s perspective", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/07/13/googleio/", "abstract": "I’m a UX and UI Designer at JUST EAT primarily working on the consumer facing apps and leading the Android experience. One evening I casually applied for the tickets to Google I/O, with the sort of attitude you have when you’re revealing a scratch card – hopeful but in the back of your mind you know it won’t come true. But to my surprise a month later, I received an email from Google inviting me to the conference held in San Francisco. I hastily informed JUST EAT about this and to my luck, they agreed on sending me over to the States to learn how we can better utilise Google’s technology and to spread this knowledge with hopes of inspiring others in the company. People assume the conference is tech heavy but since Android L was released last year, which we all know was revolutionary in the design world, Google has been a great resource for design and experience not just specifically for Android but any interactive product. This is why I wanted to talk about my experience there from a designer’s perspective and what you non-techy people can gain from attending this prestigious event. From day 1 Google made me feel welcome and provided a personable service despite communicating through typically cold email channels. Perhaps this was due to their tone of voice, the way they were attentive with frequent emails (but not too much that it was like spam) or maybe it was how they created great ways of mingling with other attendees. Prior to the conference they set up a community on Slack for women in tech to interact over, which was followed up by dinner and drinks the evening before the conference. This was great especially since I’d attended alone, and it seems like that was the case for most girls I met that night. There was a good mix of us; engineers, startup owners, aspiring students, designers, product owners to name a few, who’d flown from all over the world to attend this conference. It seems like Women in tech is growing fast. According to one of the organisers (and also a Google Engineer), the number of women attending Google I/O has increased from 8% last year to 23%, which is outstanding. Google believe that having a diversity of perspectives leads to better decision-making and more relevant products, therefore strive to create a wholly inclusive workplace around the globe where women can pursue their dreams. We also contribute to this here at JUST EAT through GGM (Geek Girl Meet) to help others learn, network and most importantly be inspired by the incredible women working in the tech industry today. The conference had an early start, with a queue that went twice around the Moscow Centre building, people itching to get into the main hall to find out about Google’s latest innovations. We were greeted with a screen that stretched nearly around the entire room, which we saw two tech fans use to have a face off in an intense game of Pong, followed by an animated countdown video through outer space and finally, the much awaited keynote. Anyone interested in tech should be fully aware of what was announced during keynote because there was full coverage and there was also a live stream . What I found the most invaluable were the talks held after and I’ve selected a few that I found relevant to me as a designer. Talk: Icons Made Easy Google have released over 700 icons for free in any format you require – even as a web font. This means that we have access to an icon library through a single line of HTML. Web fonts typically work with numeric character references, which Google wanted to further simplify by using a typographic feature called ligatures. This means we can render an icon simply by using its textual name, for instance “search” instead of “e600” which is much easier to understand and less prone to errors. Grab yourself these icons from the Material Design guidelines . Talk: Adaptive UI Whether it be a website, a mobile or TV app, the product needs to be able to scale from small to extra-large screens in order to provide consistent experience to users. Google want to emphasise the importance of this by providing a new section on adaptive UI in their Material Design guideline . It includes a flexible grid designers can use to ensure consistency across layouts, as well as breakpoint details and examples of animations to show how content reflows between different sized screens. This is a great tool designers can use to communicate with engineers to ensure static designs are translated well. Talk: Google Play Store Google Analytics is great for measuring success (or failure) for any change of a product and Google have further extended this to the Play store. We can now A/B test our app listing pages to try out different screenshots and app descriptions, with statistics around which method encouraged the most installs. Kongregate used this feature in a private pilot program and saw a 30 percent increase in its conversion rate. This success rate shows how powerful this additional functionality can be so we hope to take advantage of this at JUST EAT in hopes of gaining further installs of our apps. Watch the talk All day session: App Review One on one app review sessions with a designer from the Material Design team were open to anyone throughout the two days. Having recently released a total revamp of the JUST EAT app to adopt this new design language, of course I booked myself a session. I managed to get a full 20 minute session with one of the Senior Designers who helped shape Material Design from the very start. Overall he found the app to be simple, intuitive and most importantly correctly adopted the new UI. It was reassuring to hear that we’d done a good job at interpreting the guidelines. However, being a Google employee of course he had to comment on our search functionality. Left screen shows the current JUST EAT Android mobile homescreen and the right is the Google app . If you compare the two, you can immediately notice that Google’s search functionality at the top is much simpler. In contrast, our app surfaces too many options for the user to interact with when essentially all we need from them is their postcode and to hit go. Simplifying this opens up further opportunities such as more freeing up screen real estate to push suggested content for different entries into the conversion flow. Above is a mockup of what was suggested and as you can see, it somewhat resembles the Google Play Store, which isn’t necessarily a bad thing. In fact, it’s great because it means the users are most likely going to be familiar with the layout and essentially deal with less friction down the flow. Another talk “ Expressing brand in Material ” advised us that you can express uniqueness and brand through other means such as the tone of voice, choice of font, colours, transitions and so on. We hope to take this feedback onboard and implement this change in the near future. Talk: Android Wear Smart watches are leading the wearables revolution and Google’s contribution is Android Wear. This new product line was released in last year’s Google I/O and since then they have been making efforts to help us jump onboard. During this talk they shared with us new implementation support for developers, improvements to the overall watch experience for users as well as plenty of resources for designers. This is something that we would love to do in the future here at JUST EAT, so here are some quick mockups of how we envision the JUST EAT watch app… Watch apps should require minimum manual entry by the user and its sole purpose is to surface small digestible information to the user. Therefore the best use of an Android watch for a service like JUST EAT only happens once a user has placed an order. Typical use cases include notifying the user that their order has been accepted/cancelled, giving directions to the takeaway for collection orders and letting the user quickly rate their meal through a few simple taps and voice input. By bridging the gap between a user placing an order and receiving it, we believe the user of the future will have an overall better experience ordering from JUST EAT. You can also view an interactive prototype to get a better idea of the rate your meal flow. Watch the talk Talk: Speechless I/O The conference ended with us on a high as Google showed even more personality through their final talk “Speechless at I/O” which felt more like a game show. This involved speakers from Google pitching a product they’ve never heard of and using ridiculous slides (like above) they’d never seen before. The products were shouted out by the audience and it was great watching a respectful engineer talk seriously about “Smart Underwear”. Conclusion It was great seeing this human side to such a large company and being a part of this friendly and exciting community with people from all levels and different backgrounds. It really enforces their slogan to “be together, not the same” which they are able to reflect even within the company and not just as a marketing campaign. All sessions I attended, especially the app review session and speechless I/O, made it an invaluable experience that you can’t gain from the online streams and media coverages. I’d love to go again if I get another opportunity and would highly recommend if anyone else does too.", "date": "2015-07-13"},
{"website": "JustTakeAway", "title": "The journey of Apple Pay at JUST EAT", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2015/07/14/the-journey-of-apple-pay-at-just-eat/", "abstract": "Introduction Apple Pay has recently been released in UK and at JUST EAT we worked on the integration in the iOS app to better support all of our customers and to ease the experience to both existing and new users. Until version 10 of our iOS UK app, the checkout for completing an order was wrapped into a webview and the flow was as follows: Since Apple pushes developers to implement Apple Pay in a way that the checkout doesn’t force the user to log in the checkout flow had to be reworked, and we took the opportunity to make the majority of the checkout flow native. This enabled us to support both checkout flows: standard checkout (now with a more native flavour) Apple Pay checkout The latter is clearly a fantastic solution for completing the checkout in very few steps with a great and simple UX. Thanks to the information provided by Apple Pay (inserted by the user when registering a debit/credit card) the user details native screen is no longer necessary and more importantly for the user, there is no need to log in to the platform. A further detail on the checkout is that we support two different so-called “service types” for the orders: delivery and collection. Defined as so: typedef NS_ENUM(NSUInteger, JEServiceType)\n{\n    JEServiceTypeUnknown = 0,\n    JEServiceTypeDelivery,\n    JEServiceTypeCollection\n}; On a side note, these changes soon became a challenge during the development as JUST EAT need to treat Apple Pay users (guest users) in a similar manner to users that have registered previously to our service. How we designed around Apple Pay At the time of writing there are already a few very good articles about a basic integration with Apple Pay. Probably the best reference worth mentioning is the NSHipster post . Clearly also the Apple Documentation is a great start and the “Apple Pay Within Apps” video from WWDC 2015 explains really clearly all the relevant steps to have your app ready for Apple Pay. Rather than discussing the basic concepts (creating the merchant ID, configuring the PKPaymentRequest object, handling the presentation of the PKPaymentAuthorizationViewController , sending the token to the Payment Service Provider, etc.), we think it’d be more useful to walk you through the architectural aspects we considered when designing the solution on iOS using Objective-C. In the architecture we are proposing, the relevant components for handling an Apple Pay payment are the following: ApplePayService ApplePayPaymentHandler ApplePayPaymentRequestFactory Some additional components are also present in the big picture: CheckoutService ABRecordRefConverter PaymentFlowController We haven’t used the iOS SDK provided by our PSP (Payment Service Provider) to communicate directly to it from the iOS app, but rather we rely on a payment API to complete this communication. At JUST EAT we like dependency injection and composition when possible. Developing this new feature with these concepts in mind helped to develop components that are isolated, easily pluggable, easy to test and (sometimes) reusable. The above components have well-defined responsibilities. We’ll provide simplified code for the interfaces. Let’s go through them in a constructive order: N.B. Don’t be alarmed if you see the usage of the JEFuture or the JEProgress symbols. Lots of parts in our codebase rely on JustPromises (the library about Future and Promises we open sourced on GitHub ). You’ll also see the usage of some DTOs and the JE prefix. CheckoutService : responsible for handling the basic flow for the checkout. This is very much platform dependant. It could include the logic to perform the necessary actions in the backend to prepare the order to be completed with Apple Pay. In our case we need to store the user delivery notes (things like “The door bell doesn’t work please call me when you arrive.”) and the preferred time for the delivery. @interface JECheckoutService : NSObject\n/**\n* Composition of operations that are needed to prepare the order to be payed.\n*/\n- (JEFuture *)checkoutWithProgress:(JEProgress *)progress\n                          basketID:(NSString *)basketID\n               orderContactDetails:(JEOrderContactDetailsDTO *)orderContactDetails\n                      deliveryDate:(NSDate *)deliveryDate\n                     deliveryNotes:(NSString *)deliveryNotes;\n- (void)cancelCheckoutForBasketWithID:(NSString *)basketID;\n@end ApplePayPaymentRequestFactory : responsible for creating the PKPaymentRequests representing a transaction. In our case objects of this kind are initialised with a delivery method and a card fee. The input parameter for the method returning a PKPaymentRequest is a representation of the basket. @interface JEApplePayPaymentRequestFactory : NSObject\n- (instancetype)initWithServiceType:(JEServiceType)serviceType\n                            cardFee:(NSDecimalNumber *)cardFee NS_DESIGNATED_INITIALIZER;\n- (PKPaymentRequest *)paymentRequestForBasket:(JEBasketDTO *)basketDTO;\n- (NSArray *)summaryItemsForBasket:(JEBasketDTO *)basketDTO;\n@end You might wonder why summaryItemsForBasket: is public rather than keep it private. The reason is related to the fact that the block parameter of paymentAuthorizationViewController:didSelectShippingAddress:completion: has the following signature: (void (^)(PKPaymentAuthorizationStatus status, NSArray *shippingMethods, NSArray *summaryItems)) It might very well be that some items are not available to be shipped to a specific address, and therefore we need a way to provide the updated list of summary items for the new shipping address the user selected. ApplePayPaymentHandler : objects of this class are responsible for handling the payment from beginning to end. It’s initialised with a CheckoutService that covers the initial part of the flow (i.e. the steps that happen with the standard checkout). A PKPaymentRequest and a basket representation are provided (along with some other minor details) to objects of this class when asked to actually process a payment. @interface JEApplePayPaymentHandler : NSObject\n- (instancetype)initWithServiceType:(JEServiceType)serviceType\n                    checkoutService:(JECheckoutService *)checkoutService NS_DESIGNATED_INITIALIZER;\n/**\n* Handle an authorised PKPayment payment.\n*\n* @param payment The payment object provided by Apple Pay via the PassKit framework after the user authorised it.\n* @param basket The basket representation to use for creating the order.\n* @param paymentProvider The payment provider.\n* @param deliveryDate The delivery time of the order.\n* @param deliveryNotes THe delivery notes of the order.\n*\n* @return A future for the payment handling. Future can be successful, failed or canceled if 'cancelLastPaymentAttempt' is called during the initial state of the payment and it can therefore be aborted.\n*/\n- (JEFuture *)handlePayment:(PKPayment *)payment\n         forOrderWithBasket:(JEBasketDTO *)basket\n            paymentProvider:(NSString *)paymentProvider\n               deliveryDate:(NSDate *)deliveryDate\n              deliveryNotes:(NSString *)deliveryNotes;\n/**\n* Attempt to stop the payment process for a given payment.\n*/\n- (JEFuture *)attemptPaymentCancellation:(PKPayment *)payment;\n@end ApplePayService : this service is responsible for implementing the PKPaymentAuthorizationViewControllerDelegate protocol, for checking if Apple Pay is enabled on the device and for cancelling the payment (if it’s not too late). It’s initialised with a view controller (used to display the PKPaymentAuthorizationViewController ), a JEApplePayPaymentRequestFactory and a JEApplePayPaymentHandler . The logic for handling the selection of shipping address or the shipping method is here. @protocol JEApplePayServiceDelegate\n@optional\n- (void)applePayServiceDidPresentApplePaySheet:(JEApplePayService *)service;\n- (void)applePayService:(JEApplePayService *)service didCompletePaymentProcessWithSuccessForOrderWithID:(NSString *)orderID;\n- (void)applePayService:(JEApplePayService *)service didCompletePaymentProcessWithError:(NSError *)error;\n- (void)applePayService:(JEApplePayService *)service didFinishWithPaymentStatus:(JEApplePayServicePaymentStatus)paymentStatus;\n@end\n@interface JEApplePayService : NSObject\n@property (nonatomic, weak) id delegate;\n- (instancetype)initWithPresentingViewController:(UIViewController *)presentingViewController\n                           paymentRequestFactory:(JEApplePayPaymentRequestFactory *)paymentRequestFactory\n                                  paymentHandler:(JEApplePayPaymentHandler *)paymentHandler NS_DESIGNATED_INITIALIZER;\n+ (BOOL)isApplePayAvailableOnDevice;\n/**\n* Request the receiver to handle an Apple Pay payment.\n*\n* @param basket The basket representation for the order to be paid.\n* @param paymentProvider The payment provider from the retrieved payment option.\n* @param deliveryDate The delivery date of the order.\n* @param deliveryNotes The delivery notes of the order.\n*/\n- (void)handlePaymentForOrderWithBasket:(JEBasketDTO *)basket\n                        paymentProvider:(NSString *)paymentProvider\n                           deliveryDate:(NSDate *)deliveryDate\n                          deliveryNotes:(NSString *)deliveryNotes;\n/**\n* Attempt to stop the payment process for a given basket.\n*/\n- (void)attemptPaymentCancellationForBasket:(JEBasketDTO *)basket;\n@end ABRecordRefConverter : just a bunch of class methods for isolating the logic for transforming the ABRecordRef to easy-to-use DTOs. Until iOS 8.4, the delegate method paymentAuthorizationViewController:didSelectShippingAddress:completion: of PKPaymentAuthorizationViewControllerDelegate provides a ABRecordRef . Starting with iOS 9.0, this method is depracated and a similar one using a wrapper object ( PKContact ) on top of the Contacts framework is used. ABRecordRefConverter basically does the work that Apple did for us in iOS 9. @interface JEABRecordRefConverter : NSObject\n+ (JEOrderContactDetailsDTO *)orderContactDetailsForABRecordRef:(ABRecordRef)recordRef\n                                         requireShippingDetails:(BOOL)requireShippingDetails\n                                                          error:(NSError **)error;\n+ (JEAddressDTO *)addressForABRecordRef:(ABRecordRef)recordRef error:(NSError **)error;\n@end Similar to what happened in the ApplePayPaymentRequestFactory , you might wonder why the addressForABRecordRef: method is necessary. The reason is that the second parameter of paymentAuthorizationViewController:didSelectShippingAddress:completion: is an ABRecordRef populated with only the address information for privacy reasons. PaymentFlowController : it is responsible for creating the necessary stack and interactions between of all the above components. It acts as the delegate of the ApplePayService to handle the navigation flow and it should be intended to be the starting point and glue around our standard checkout flow and the Apple Pay one. /* simplified code from JEPaymentFlowController */\nJEApplePayPaymentRequestFactory *paymentRequestFactory =\n[[JEApplePayPaymentRequestFactory alloc] initWithServiceType:self.serviceType\n                                                     cardFee:self.applePayPaymentOption.fee];\nself.checkoutService = [JECheckoutService new];\nJEApplePayPaymentHandler *paymentHandler =\n[[JEApplePayPaymentHandler alloc] initWithServiceType:self.serviceType\n                                      checkoutService:self.checkoutService];\nself.applePayService =\n[[JEApplePayService alloc] initWithPresentingViewController:self.navigationController\n                                      paymentRequestFactory:paymentRequestFactory\n                                             paymentHandler:paymentHandler];\nself.applePayService.delegate = self;\n[self.applePayService handlePaymentForOrderWithBasketID:self.basketID\n                                        paymentProvider:self.applePayPaymentOption.paymentProvider\n                                           deliveryDate:deliveryDate\n                                          deliveryNotes:deliveryNotes]; We strongly believe in unit testing (and automation and integration testing as well) and we strive to cover our code with the necessary tests for every new feature we develop. Payments are clearly a hot topic and a crucial part of our business, therefore structuring this feature in small and separated components allowed us to easily keep the code coverage for the above components close to 100%. PKPaymentRequest’s pitfalls The way the payment request is populated dictates what the Apple Pay sheet will display. How to populate the shipping and billing address properties is not completely straightforward. An excerpt of code we have in production is as follows: /* basic PKPaymentRequest creation */\nPKPaymentRequest *paymentRequest = [[PKPaymentRequest alloc] init];\npaymentRequest.supportedNetworks = @[PKPaymentNetworkMasterCard, PKPaymentNetworkVisa];\npaymentRequest.merchantCapabilities = PKMerchantCapability3DS;\npaymentRequest.countryCode = @\"GB\"; // ISO 3166-1 alpha-2 country code\npaymentRequest.currencyCode = @\"GBP\"; // ISO 4217 currency code\npaymentRequest.requiredBillingAddressFields = PKAddressFieldAll;\nif (__ORDER_IS_FOR_DELIVERY__)\n{\n    paymentRequest.requiredShippingAddressFields = PKAddressFieldAll;\n    paymentRequest.shippingType = PKShippingTypeDelivery;\n}\nelse if (__ORDER_IS_FOR_COLLECTION__)\n{\n    paymentRequest.requiredShippingAddressFields = PKAddressFieldPhone | PKAddressFieldEmail;\n}\nreturn paymentRequest; Using the above configuration the Apple Pay sheets for delivery and collection orders appear like so: Note that, in the case of collection orders, even if we set the requiredShippingAddressFields property to something meaningful (which isn’t PKAddressFieldNone ), the associated cell in the sheet is not displayed. At JUST EAT we need to know upfront if the order is for delivery or collection in order to let the user fill the basket accordingly (e.g. some items might not be available for delivery) and for this reason we couldn’t leverage the built-in capabilities of Apple Pay to handle different shipping methods. Moreover, since Apple defines the shipping type like so: typedef NS_ENUM(NSUInteger, PKShippingType) {\n    PKShippingTypeShipping,\n    PKShippingTypeDelivery,\n    PKShippingTypeStorePickup,\n    PKShippingTypeServicePickup\n} NS_ENUM_AVAILABLE(NA, 8_3); for collection orders the correct value to use would be PKShippingTypeStorePickup but the address of the store must be present in the list of addresses the user has entered on the device. This isn’t practical. Going back to how the payment request is configured, it’s critical for JUST EAT to have the phone number and the email of the customer in order for customer services to contact them in case something goes wrong with the order. This applies to delivery orders as well as collection orders. At first we thought that, since for collection orders the shipping address was not necessary, the property requiredShippingAddressFields of the PKPaymentRequest could be set to PKAddressFieldNone and we could grab the phone number and email from the billingAddress property. Unfortunately, even setting the requiredBillingAddressFields to PKAddressFieldAll when fetching the info from the billingAddress property of the PKPayment the phone number and email values are not there. Crucially, to grab the necessary order details info we had to merge the information provided by the two properties ( billingAddress and shippingAddress ) as so: @interface JEAddressDTO : NSObject\n@property (nonatomic, copy, readonly) NSString *line1;\n@property (nonatomic, copy, readonly) NSString *line2;\n@property (nonatomic, copy, readonly) NSString *line3;\n@property (nonatomic, copy, readonly) NSString *city;\n@property (nonatomic, copy, readonly) NSString *postCode;\n+ (instancetype)addressWithLine1:(NSString *)line1\n                           line2:(NSString *)line2\n                           line3:(NSString *)line3\n                            city:(NSString *)city\n                        postCode:(NSString *)postCode;\n@end\n@interface JEOrderContactDetailsDTO : NSObject\n@property (nonatomic, copy, readonly) NSString *fullName;\n@property (nonatomic, copy, readonly) NSString *phoneNumber;\n@property (nonatomic, copy, readonly) NSString *email;\n@property (nonatomic, strong, readonly) JEAddressDTO *address;\n+ (instancetype)orderContactDetailsWithFullName:(NSString *)fullName\n                                    phoneNumber:(NSString *)phoneNumber\n                                          email:(NSString *)email\n                                        address:(JEAddressDTO *)address;\n@end /*\n* simplified code for composing the necessary order details info from\n* billing and shipping addresses within the `JEApplePayPaymentHandler`\n*/\n- (JEOrderContactDetailsDTO *)orderContactDetailsForPayment:(PKPayment *)payment\n                                                serviceType:(BOOL)serviceType\n                                                      error:(NSError **)error\n{\n    BOOL isDeliveryOrder = (serviceType == JEServiceTypeDelivery);\n    NSError *shippingAddressError = nil;\n    JEOrderContactDetailsDTO *contactDetailsFromShippingAddress =\n    [JEABRecordRefConverter orderContactDetailsForABRecordRef:payment.shippingAddress\n                                       requireShippingDetails:isDeliveryOrder\n                                                        error:&shippingAddressError];\n    // at this point `contactDetailsFromShippingAddress` contains\n    if (shippingAddressError && error != NULL)\n    {\n        *error = shippingAddressError;\n        return nil;\n    }\n    JEOrderContactDetailsDTO *orderContactDetails = contactDetailsFromShippingAddress;\n    if (!isDeliveryOrder) // collection orders\n    {\n        NSError *billingAddressError = nil;\n        JEOrderContactDetailsDTO *contactDetailsFromBillingAddress =\n        [JEABRecordRefConverter orderContactDetailsForABRecordRef:payment.billingAddress\n                                           requireShippingDetails:NO\n                                                            error:&billingAddressError];\n        if (billingAddressError && error != NULL)\n        {\n            *error = billingAddressError;\n            return nil;\n        }\n        // compose the order contact details\n        orderContactDetails =\n        [JEOrderContactDetailsDTO orderContactDetailsWithFullName:contactDetailsFromBillingAddress.fullName\n                                                      phoneNumber:contactDetailsFromShippingAddress.phoneNumber\n                                                            email:contactDetailsFromShippingAddress.email\n                                                          address:contactDetailsFromBillingAddress.address];\n    }\n    return orderContactDetails;\n} Integration considerations From our journey into the Apple Pay world we learned that Apple pushes a lot for the Apple Pay button to be prominent in your app’s UI. Unless one starts an iOS application from scratch and provides only Apple Pay as a payment method, other methods are usually already available (cards, PayPal, etc.): showing the Apple Pay button as big as the other buttons leading to different payments is almost a mandatory requirement from Apple. Showing the button as a first option is equally important. What is not so mandatory, but still very nice to have, is to provide the user with the ability to complete the checkout without logging in. This is a good thing for a few reasons: remove the friction and enable happy paths to help your customers pay quicker no need to collect data from the user any more leveraging what’s already provided by Apple Pay Apple and customer happiness As in our case, this is far from being trivial and logic in the backend usually needs to be tweaked accordingly to support what we call “guest” or “anonymous” users. On a side note, it wasn’t completely clear from the beginning that the ABRecordRef object provided in the paymentAuthorizationViewController:didSelectShippingAddress:completion: delegate method does not contain the full user data but just the address, but that is sufficient information to calculate the shipping cost and the availability of the items to a specific address. The entire user information is provided in the PKPayment object via the shippingAddress and billingAddress properties once the user authorised the payment. In iOS9 the API slightly changed mainly due to introduction of the Contacts framework and the deprecation of the AddressBook one. PassKit provides a new class PKContact that is a wrapper around objects from the Contacts framework. This means that the following properties of PKPayment objects: @property (nonatomic, assign, nullable) ABRecordRef billingAddress NS_DEPRECATED_IOS(8_0, 9_0, \"Use billingContact instead\");\n@property (nonatomic, assign, nullable) ABRecordRef shippingAddress NS_DEPRECATED_IOS(8_0, 9_0, \"Use shippingContact instead\"); are deprecated in favour of the following new ones: @property (nonatomic, retain, nullable) PKContact *billingContact NS_AVAILABLE_IOS(9_0);\n@property (nonatomic, retain, nullable) PKContact *shippingContact NS_AVAILABLE_IOS(9_0); This will help a lot since the old ABRecordRef is defined as a CFTypeRef and the related APIs are not easy to consume in an object-oriented world.", "date": "2015-07-14"},
{"website": "JustTakeAway", "title": "Deep linking in Android the easy way", "author": ["Written by Chris Newton"], "link": "https://tech.justeattakeaway.com/2015/06/29/deep-linking-in-android-the-easy-way/", "abstract": "Introduction Deep linking in Android serves as a useful way for other apps, services or web pages to open relevant content within your app. It’s particularly useful if, like JUST EAT, you have a website that hands off content to the app, or for Google App Indexing. App Indexing allows Google searches to deep link straight into the app activities that you expose. It’s easy to implement, but it’s also easy to go overboard and make a mess of your, otherwise pretty, code. Here we present a simple way to structure your code so that it’s headache free and, more importantly, unit-testable. Study case Let’s say we have three activities, A, B and C . A opens B and B opens C . We need to support deep links to each of these activities – both custom schema URIs and web URIs. Activity B needs one argument to work properly, whereas activity C needs two. Activity A needs no arguments. When switching between the activities, arguments are passed between them using the standard Intent Mechanism. The deep links will contain the same arguments in a bespoke format. Deep links that need to be supported include: Activity A: example-scheme://activitya\nhttp(s)://(www.)example.co.uk/a Activity B: example-scheme://activityb?query=[text] Activity C: example-scheme://activityc?query=[text]&choice=[int]\nhttp(s)://(www.)example.co.uk/c?query=[text]&choice=[int] The don’ts Let’s see what a typical setup for this scenario might look like. Let’s start with the Manifest. <activity\n    android:name=\".activities.AActivity\"\n    android:label=\"@string/label_a\" >\n    <intent-filter>\n        <action android:name=\"android.intent.action.MAIN\" />\n        <category android:name=\"android.intent.category.LAUNCHER\" />\n    </intent-filter>\n    <intent-filter>\n        <action android:name=\"android.intent.action.VIEW\"/>\n        <category android:name=\"android.intent.category.DEFAULT\"/>\n        <category android:name=\"android.intent.category.BROWSABLE\"/>\n        <data android:scheme=\"example-scheme\"/>\n        <data android:host=\"activitya\"/>\n        <data android:pathPattern=\"/.*\"/>\n    </intent-filter>\n    <intent-filter>\n        <action android:name=\"android.intent.action.VIEW\"/>\n        <category android:name=\"android.intent.category.DEFAULT\"/>\n        <category android:name=\"android.intent.category.BROWSABLE\"/>\n        <data android:scheme=\"http\" />\n        <data android:scheme=\"https\"/>\n        <data android:host=\"www.example.co.uk\" />\n        <data android:host=\"example.co.uk\" />\n        <data android:path=\"/a\" />\n    </intent-filter>\n</activity>\n<activity\n    android:name=\".activities.BActivity\"\n    android:label=\"@string/label_b\" >\n    <intent-filter>\n        <action android:name=\"android.intent.action.VIEW\"/>\n        <category android:name=\"android.intent.category.DEFAULT\"/>\n        <category android:name=\"android.intent.category.BROWSABLE\"/>\n        <data android:scheme=\"example-scheme\"/>\n        <data android:host=\"activityb\"/>\n        <data android:pathPattern=\"/.*\"/>\n    </intent-filter>\n</activity>\n<activity\n    android:name=\".activities.CActivity\"\n    android:label=\"@string/label_c\" >\n    <intent-filter>\n        <action android:name=\"android.intent.action.VIEW\"/>\n        <category android:name=\"android.intent.category.DEFAULT\"/>\n        <category android:name=\"android.intent.category.BROWSABLE\"/>\n        <data android:scheme=\"example-scheme\"/>\n        <data android:host=\"activityc\"/>\n        <data android:pathPattern=\".*\"/>\n    </intent-filter>\n    <intent-filter>\n        <action android:name=\"android.intent.action.VIEW\"/>\n        <category android:name=\"android.intent.category.DEFAULT\"/>\n        <category android:name=\"android.intent.category.BROWSABLE\"/>\n        <data android:scheme=\"http\" />\n        <data android:scheme=\"https\"/>\n        <data android:host=\"www.example.co.uk\" />\n        <data android:host=\"example.co.uk\" />\n        <data android:path=\"/c\" />\n    </intent-filter>\n</activity> We have just three activities, but our manifest is already bloated and unreadable because of all the intent filters! This is not the end of the bad news though… let’s look at the code needed to parse the deep links – let’s take activity B as an example. private void parseIntent() {\n    final Intent intent = getIntent();\n    final Uri uri = intent.getData();\n    if (uri != null) {\n        if (\"example-scheme\".equals(uri.getScheme()) && \"activityb\".equals(uri.getHost())) {\n            // Cool, we have a URI addressed to this activity!\n            mQuery = uri.getQueryParameter(\"query\");\n        }\n    }\n    if (mQuery == null) {\n        mQuery = intent.getStringExtra(IntentHelper.EXTRA_B_QUERY);\n    }\n} There’s nothing fundamentally wrong with this code, but it’s not unit-testable – integration tests would work fine, but why on earth would we need to fire up the device emulator just to test that our deep links are handled? While moving this code out to a helper class would solve this problem, we would now have a sprinkling of helper classes for all deep-linkable activities, where some of the code is repetitive. Imagine having to implement 20 different deep links with varying amounts of parameters to extract – not a great prospect. Back to the drawing board What can we change here then? How about using a single activity to dispatch deep links to all the others ? How is it going to help? What are the benefits? All the intent filters collapse nicely into one activity tag We can handle all deep links in one place – a parser class instantiated in our dispatcher activity. The parser class is easily unit-testable The logic is deduplicated and simplified We can pass the arguments to activities on the intent we use to launch the target activity, adding no additional logic to the target activity The full solution will be presented here step by step, including how to set up unit tests for our deep links. The Manifest This time, all of our manifest logic is contained to one activity tag. This is what it looks like… <activity\n        android:name=\".activities.LinkDispatcherActivity\"\n        android:noHistory=\"true\"\n        android:launchMode=\"singleInstance\"\n        android:theme=\"@android:style/Theme.NoDisplay\" >\n        <intent-filter>\n            <action android:name=\"android.intent.action.VIEW\"/>\n            <category android:name=\"android.intent.category.DEFAULT\"/>\n            <category android:name=\"android.intent.category.BROWSABLE\"/>\n            <data android:scheme=\"example-scheme\"/>\n            <data android:host=\"activitya\"/>\n            <data android:host=\"activityb\"/>\n            <data android:host=\"activityc\"/>\n            <data android:pathPattern=\".*\"/>\n        </intent-filter>\n        <intent-filter>\n            <action android:name=\"android.intent.action.VIEW\"/>\n            <category android:name=\"android.intent.category.DEFAULT\"/>\n            <category android:name=\"android.intent.category.BROWSABLE\"/>\n            <data android:scheme=\"http\" />\n            <data android:scheme=\"https\"/>\n            <data android:host=\"www.example.co.uk\" />\n            <data android:host=\"example.co.uk\" />\n            <data android:path=\"/a\" />\n            <data android:path=\"/c\" />\n        </intent-filter>\n    </activity> In addition to cutting down on some code, this setup makes it much clearer what is actually going on here. As you can see we can specify multiple hosts or paths, either of which will result in a hit on the respective Intent filter. All of our deep link Intents will go to our LinkDispatcherActivity , so they can be processed and forwarded to the actual activities responsible for a link. <activity\n        android:name=\".activities.LinkDispatcherActivity\"\n        android:noHistory=\"true\"\n        android:launchMode=\"singleInstance\"\n        android:theme=\"@android:style/Theme.NoDisplay\" > A few important things to note about the above piece of code. The noHistory attribute will prevent the activity from appearing in the back stack. Once it’s finished, it’s gone and the user won’t be able to return to it. The theme set on this activity will make it completely transparent to the user. The launchMode is an interesting and important one. The singleInstance mode we specified makes the LinkDispatcherActivity launch in its own exclusive task. Then any activity started from this activity launches in a separate task. What this means in practice is that we avoid problems arising from already having an existing application task. The deep link will launch its own task with its own back stack instead of plugging into an existing one. This also satisfies one of the Google App Indexing requirements – we want the back button to back out to the activity that launched the deep link instead of back down an existing back stack. The code The entirety of the deep link handling code resides in two simple classes. The activities are unaffected – they don’t know whether they were launched from a deep link or a regular Intent, as we take advantage of the code that’s already there – parsing Intent extras. What this means in practice is that we need a class that can take a URI, parse it, and turn it into an Intent that we subsequently start. public class LinkDispatcherActivity extends Activity {\n    private final UriToIntentMapper mMapper = new UriToIntentMapper(this, new IntentHelper());\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        try {\n            mMapper.dispatchIntent(getIntent());\n        } catch (IllegalArgumentException iae) {\n            // Malformed URL\n            if (BuildConfig.DEBUG) {\n                Log.e(\"Deep links\", \"Invalid URI\", iae);\n            }\n        } finally {\n            // Always finish the activity so that it doesn't stay in our history\n            finish();\n        }\n    }\n} The activity itself, as you can see, doesn’t do much. That is because we wish the URI-to-Intent mapping logic to be fully unit-testable. Admittedly you could use a service (not recommended by Google, but safe in our particular case) instead of an activity here, but we found this works well. The UriToIntentMapper contains all of the mapping logic we are interested in. Below is the core code for supporting our deep links. public class UriToIntentMapper {\n    private Context mContext;\n    private IntentHelper mIntents;\n    public UriToIntentMapper(Context context, IntentHelper intentHelper) {\n        mContext = context;\n        mIntents = intentHelper;\n    }\n    public void dispatchIntent(Intent intent) {\n        final Uri uri = intent.getData();\n        Intent dispatchIntent = null;\n        if (uri == null) throw new IllegalArgumentException(\"Uri cannot be null\");\n        final String scheme = uri.getScheme().toLowerCase();\n        final String host = uri.getHost().toLowerCase();\n        if (\"example-scheme\".equals(scheme)) {\n            dispatchIntent = mapAppLink(uri);\n        } else if ((\"http\".equals(scheme) || \"https\".equals(scheme)) &&\n                (\"www.example.co.uk\".equals(host) || \"example.co.uk\".equals(host))) {\n            dispatchIntent = mapWebLink(uri);\n        }\n        if (dispatchIntent != null) {\n            mContext.startActivity(dispatchIntent);\n        }\n    }\n    private Intent mapAppLink(Uri uri) {\n        final String host = uri.getHost().toLowerCase();\n        switch (host) {\n            case \"activitya\":\n                return mIntents.newAActivityIntent(mContext);\n            case \"activityb\":\n                String bQuery = uri.getQueryParameter(\"query\");\n                return mIntents.newBActivityIntent(mContext, bQuery);\n            case \"activityc\":\n                String cQuery = uri.getQueryParameter(\"query\");\n                int choice = Integer.parseInt(uri.getQueryParameter(\"choice\"));\n                return mIntents.newCActivityIntent(mContext, cQuery, choice);\n        }\n        return null;\n    }\n    private Intent mapWebLink(Uri uri) {\n        final String path = uri.getPath();\n        switch (path) {\n            case \"/a\":\n                return mIntents.newAActivityIntent(mContext);\n            case \"/c\":\n                String cQuery = uri.getQueryParameter(\"query\");\n                int choice = Integer.parseInt(uri.getQueryParameter(\"choice\"));\n                return mIntents.newCActivityIntent(mContext, cQuery, choice);\n        }\n        return null;\n    }\n} This is where all the magic happens. We’ve basically moved all logic involved in parsing deep link Intents from all the activities here. Simple. You might say that we should use resources for our string constants here. I personally think for simplicity’s sake this is better and hardly has any memory impact. I should probably mention the IntentHelper class now to make it all clearer… public class IntentHelper {\n    public static String EXTRA_B_QUERY = \"com.justeat.app.deeplinks.intents.Intents.EXTRA_B_QUERY\";\n    public static String EXTRA_C_QUERY = \"com.justeat.app.deeplinks.intents.Intents.EXTRA_C_QUERY\";\n    public static String EXTRA_C_CHOICE = \"com.justeat.app.deeplinks.intents.Intents.EXTRA_C_CHOICE\";\n    public Intent newAActivityIntent(Context context) {\n        Intent i = new Intent(context, AActivity.class);\n        i.addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);\n        return i;\n    }\n    public Intent newBActivityIntent(Context context, String query) {\n        Intent i = new Intent(context, BActivity.class);\n        i.addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);\n        i.putExtra(EXTRA_B_QUERY, query);\n        return i;\n    }\n    public Intent newCActivityIntent(Context context, String query, int choice) {\n        Intent i = new Intent(context, CActivity.class);\n        i.addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);\n        i.putExtra(EXTRA_C_QUERY, query);\n        i.putExtra(EXTRA_C_CHOICE, choice);\n        return i;\n    }\n} As you can see, it’s really simple. This class allows us to reuse code for constructing Intents. The activities use these internally between each other and we can use the exact same code when dispatching our deep links. This means no additional work on activities, as they should already handle the Intent extras. Testing – manual testing There’s a great little tool created by Google to help us test our deep links. It’s simple to use, just replace the example information with your own. In our case an example is: android-app://com.justeat.app.deeplinks/example-scheme/activityb?query=abcd123 After you’re done, just use a QR code reader to test your deep link. Testing – Intent filters One of the things we have to test is our intent filter. We can write tests that check whether our URIs resolve to the LinkDispatcherActivity . For the sake of brevity, I won’t present the entire test suite here, the entire code is hosted on GitHub if you wish to take a look. Some of the code for matching Intent filters was taken from the fantastic Robolectric project and adapted to fit our testing purpose. @Config(constants = BuildConfig.class, sdk = 21)\n@RunWith(RobolectricRunner.class)\npublic class IntentFilterTest {\n    @Test\n    public void activity_a_app_link_resolves() {\n        assertIntentResolves(DeepLinks.getActivityAUri());\n    }\n    @Test\n    public void activity_b_app_link_resolves() {\n        assertIntentResolves(DeepLinks.getActivityBUri(\"bogoQuery\"));\n    }\n    private void assertIntentResolves(String uriString) {\n        //Arrange\n        ShadowApplication.getInstance().getAppManifest();\n        RobolectricPackageManager packageManager = (RobolectricPackageManager) RuntimeEnvironment.application.getPackageManager();\n        packageManager.setQueryIntentImplicitly(true);\n        Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(uriString));\n        //Act\n        List resolveInfo = queryImplicitIntent(intent, -1);\n        ArrayList activityNames = new ArrayList<>();\n        for (ResolveInfo info : resolveInfo) {\n            activityNames.add(info.activityInfo.targetActivity);\n        }\n        //Assert\n        assertThat(LinkDispatcherActivity.class.getName(), isOneOf(activityNames.toArray()));\n    }\n    private List queryImplicitIntent(Intent intent, int flags) {\n        List resolveInfoList = new ArrayList();\n        AndroidManifest appManifest = ShadowApplication.getInstance().getAppManifest();\n        String packageName = appManifest.getPackageName();\n        for (Map.Entry<String, ActivityData> activity : appManifest.getActivityDatas().entrySet()) {\n            String activityName = activity.getKey();\n            ActivityData activityData = activity.getValue();\n            if (activityData.getTargetActivity() != null) {\n                activityName = activityData.getTargetActivityName();\n            }\n            if (matchIntentFilter(activityData, intent)) {\n                ResolveInfo resolveInfo = new ResolveInfo();\n                resolveInfo.resolvePackageName = packageName;\n                resolveInfo.activityInfo = new ActivityInfo();\n                resolveInfo.activityInfo.targetActivity = activityName;\n                resolveInfoList.add(resolveInfo);\n            }\n        }\n        return resolveInfoList;\n    }\n    private boolean matchIntentFilter(ActivityData activityData, Intent intent) {\n        System.out.println(\"Searching for \"+ intent.getDataString());\n        for (IntentFilterData intentFilterData : activityData.getIntentFilters()) {\n            List actionList = intentFilterData.getActions();\n            List categoryList = intentFilterData.getCategories();\n            IntentFilter intentFilter = new IntentFilter();\n            for (String action : actionList) {\n                intentFilter.addAction(action);\n            }\n            for (String category : categoryList) {\n                intentFilter.addCategory(category);\n            }\n            for (String scheme : intentFilterData.getSchemes()) {\n                intentFilter.addDataScheme(scheme);\n            }\n            for (String mimeType : intentFilterData.getMimeTypes()) {\n                try {\n                    intentFilter.addDataType(mimeType);\n                } catch (IntentFilter.MalformedMimeTypeException ex) {\n                    throw new RuntimeException(ex);\n                }\n            }\n            for (String path : intentFilterData.getPaths()) {\n                intentFilter.addDataPath(path, PatternMatcher.PATTERN_LITERAL);\n            }\n            for (String pathPattern : intentFilterData.getPathPatterns()) {\n                intentFilter.addDataPath(pathPattern, PatternMatcher.PATTERN_SIMPLE_GLOB);\n            }\n            for (String pathPrefix : intentFilterData.getPathPrefixes()) {\n                intentFilter.addDataPath(pathPrefix, PatternMatcher.PATTERN_PREFIX);\n            }\n            for (IntentFilterData.DataAuthority authority : intentFilterData.getAuthorities()) {\n                intentFilter.addDataAuthority(authority.getHost(), authority.getPort());\n            }\n            // match action\n            boolean matchActionResult = intentFilter.matchAction(intent.getAction());\n            // match category\n            String matchCategoriesResult = intentFilter.matchCategories(intent.getCategories());\n            // match data\n            int matchResult = intentFilter.matchData(intent.getType(), intent.getScheme(),\n                    intent.getData());\n            if ((matchResult != IntentFilter.NO_MATCH_DATA && matchResult != IntentFilter.NO_MATCH_TYPE)) {\n                System.out.println(\"Matcher result for \" + intent.getType() + \" \" + intent.getScheme() + \" \" +\n                        intent.getDataString() +\": \" + Integer.toHexString(matchResult));\n                System.out.println(intentFilterData.getSchemes());\n                for (IntentFilterData.DataAuthority authority : intentFilterData.getAuthorities()) {\n                    System.out.println(authority.getHost() + \":\" + authority.getPort());\n                }\n                System.out.println(intentFilterData.getPaths());\n                System.out.println(intentFilterData.getPathPatterns());\n                System.out.println(intentFilterData.getPathPrefixes());\n                boolean pathMatchesExactly = false;\n                for (int i = 0; i < intentFilter.countDataPaths(); i++) {\n                    PatternMatcher pm = intentFilter.getDataPath(i);\n                    if (pm.match(intent.getData().getPath())) {\n                        System.out.println(\"Pattern match on path: \" + pm.getPath());\n                    }\n                }\n            }\n            // Check if it's a match at all\n            boolean result = matchActionResult && (matchCategoriesResult == null) &&\n                    (matchResult != IntentFilter.NO_MATCH_DATA && matchResult != IntentFilter.NO_MATCH_TYPE);\n            // No match, discard activity\n            if (!result) continue;\n            // We have a partial match. The matchResult doesn't seem to correspond to reality\n            // as far as path matching goes – no idea why. Fortunately we can check if the path\n            // matches exactly manually.\n            if (!TextUtils.isEmpty(intent.getData().getPath())) {\n                //If the path is not empty, we need to make sure it's an exact match\n                boolean pathMatch = false;\n                for (int i = 0; i < intentFilter.countDataPaths(); i++) {\n                    PatternMatcher pm = intentFilter.getDataPath(i);\n                    if (pm.match(intent.getData().getPath())) {\n                        // Exact match found, return\n                        pathMatch = true;\n                    }\n                }\n                // No exact match found – we only have a partial match on the intent filter.\n                // While this filter may work for general cases,\n                // for links like \"android-app://com.justeat.app.uk/just-eat.co.uk/account\"\n                // to work, the path must be an exact match. Hence we enforce this for all intents\n                // This way we can catch all errors.\n                result = pathMatch;\n            }\n            if (result) return true;\n        }\n        return false;\n    }\n} public class DeepLinks {\n    public static String getActivityAUri() {\n        return \"example-scheme://activitya\";\n    }\n    public static String getActivityBUri(String query) {\n        return String.format(\"example-scheme://activityb?query=%s\", query);\n    }\n    public static String getActivityCUri(String query, int choice) {\n        return String.format(\"example-scheme://activityc?query=%s&choice=%d\", query, choice);\n    }\n    public static String getActivityAWebUrl(boolean https, boolean www) {\n        StringBuilder sb = new StringBuilder(https ? \"https\" : \"http\");\n        sb.append(\"://\");\n        if (www) sb.append(\"www.\");\n        sb.append(\"example.co.uk\");\n        sb.append(\"/a\");\n        return sb.toString();\n    }\n    public static String getActivityCWebUrl(boolean https, boolean www, String query, int choice) {\n        StringBuilder sb = new StringBuilder(https ? \"https\" : \"http\");\n        sb.append(\"://\");\n        if (www) sb.append(\"www.\");\n        sb.append(\"example.co.uk\");\n        sb.append(String.format(\"/c?query=%s&choice=%d\", query, choice));\n        return sb.toString();\n    }\n} As you can see, what we do is we parse the Intent filters from the manifest and match the URI against them. We have to make sure we check for exact matches on the Intent filter (all Intent filter elements match – scheme, host, path etc.). This way we can be certain our deep link URIs resolve correctly to our LinkDispatcherActivity . Testing – Deep link dispatching To have end-to-end coverage of our solution we need to test whether valid URIs dispatch to corresponding activities with the correct data. To achieve that, we need to write a few simple tests around that. Below I’ll present a few example tests. Again, the full suite is presented in our GitHub repository . @Config(constants = BuildConfig.class, sdk = 21)\n@RunWith(RobolectricRunner.class)\npublic class DeepLinkResolutionTest {\n    private IntentHelper mMockIntents;\n    private Activity mMockActivity;\n    private UriToIntentMapper mMapper;\n    @Before\n    public void setUp() {\n        mMockIntents = mock(IntentHelper.class);\n        mMockActivity = mock(LinkDispatcherActivity.class);\n        mMapper = new UriToIntentMapper(mMockActivity, mMockIntents);\n    }\n    @Test\n    public void activity_a_link_dispatches() {\n        //Arrange\n        Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(DeepLinks.getActivityAUri()));\n        Intent mockDispatchIntent = mock(Intent.class);\n        when(mMockIntents.newAActivityIntent(mMockActivity)).thenReturn(mockDispatchIntent);\n        //Act\n        mMapper.dispatchIntent(intent);\n        //Assert\n        verify(mMockActivity).startActivity(eq(mockDispatchIntent));\n    }\n    @Test\n    public void activity_b_link_dispatches() {\n        //Arrange\n        Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(DeepLinks.getActivityBUri(\"bogo\")));\n        Intent mockDispatchIntent = mock(Intent.class);\n        when(mMockIntents.newBActivityIntent(eq(mMockActivity), eq(\"bogo\"))).thenReturn(mockDispatchIntent);\n        //Act\n        mMapper.dispatchIntent(intent);\n        //Assert\n        verify(mMockIntents).newBActivityIntent(any(Context.class), eq(\"bogo\"));\n        verify(mMockActivity).startActivity(eq(mockDispatchIntent));\n    }\n} Conclusion And there you have it – a simple and unit testable way of dealing with deep links. Your code is kept unblemished, thanks to the logic being moved away from activities into simple, single purpose classes. The entire working solution is available for download from our GitHub repository – we encourage you to take a look. It doubles up as a good example of how to set up your unit tests… #minifistpump", "date": "2015-06-29"},
{"website": "JustTakeAway", "title": "Remote Retrospectives", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/07/08/remote-retrospectives-2/", "abstract": "Retros at JUST EAT Here at JUST EAT, our technical teams are split across three locations: Bristol, Kiev and London. Often, we work with remote colleagues either as part of an ongoing project, or for a short standalone period of time. Within our agile teams, we regularly run retrospectives as a way of inspecting what we’ve done over the past few weeks, in the hope of adapting for the better. For the past 6 months, I’ve been working with our Payments team, helping them to facilitate their retros. Retros are held every 3 weeks, and I’ve been using some of my own ideas, plus information from other sources such as Retromat . Recently we have gained two new team members, who are based in Kiev. Unfortunately we do not have the ability to fly everyone to the same location for retros, so I’ve been doing some thinking about how we can integrate remote participants in retrospectives. I’ve facilitated a lot of retros over the course of my career, and I’ve had the opportunity to work with and learn from some awesome people, so I thought I would gather all the things I’ve learned into one place. Learnings from Remote Retros Always defer to the remote party If the majority of the team is in one room with remote participants, I like to have a rule which is that everyone defers to the remote participants. When you’re remote in this situation, it’s often difficult to know when someone has finished speaking, especially when you cannot see their facial expressions. Often, there is a time lag on communications with remote participants, so even by the time they perceive someone has finished speaking, and they begin, it’s likely that someone locally has already begun speaking – leading to what I call Remote Comms Kerfuffle (RCK). With the rule in place, the second you hear even a crackle from the remote participants, everyone in the room shuts up and listens. It’s not perfect, but it definitely helps. Remote facilitator I read an article a while back ( Strong and Agile ) “To be more in tune with communication issues, the facilitator could sit in a room on their own and connect to the session. When the session is running they will then be experience first hand the difficulties that each of the rooms are having which would be less obvious if located in one of the team rooms. This will help the facilitator ask the right questions to stabilise the session.” I thought this was a brilliant idea, so gave it a try a while back. Immediately I was able to see that the people sitting at the back of the room were inaudible, and the remote participants were only getting half the conversation. A quick rearrange of the microphone, and things were much clearer. Next I found how difficult it was to interject the conversation in a way that didn’t make me feel silly, RCK ensued and we wasted time apologising to each other, and then pausing, and then having more RCK. Hence the rule above! Increase the timeframe Often, with remote participants, retros take longer, and this needs to be factored in. There’s the initial connection time (at least 5 minutes at the beginning of the retro to battle with technology in whichever meeting room), then there’s the additional time for resulting RCK (which will most likely always happen to some degree). I tend to add on 15 minutes to retro time to cater for remote participants, though once you get into a regular pattern of remote retrospectives, this can be reduced. Prep the team When we have remote participants, it can be harder for them to grasp the activities we are trying to run through, and they may not have time to ask questions. The day before each retro, I tend to send an email to the remote participants, explaining the activities, ensuring they have the right materials (if needed) and asking them if they have any questions. This means we can be prepared ahead of time, and don’t have to factor this additional communication into the retro time. Shared tools Google docs are really useful as the real time updates are easy to see. I recently found this tool, which I think could be very useful: http://www.ideaboardz.com/for/PRS%20Board/1357023. I usually have a rule of no electronic devices in retros, however – in this case we would need people to bring and use laptops for a portion of the retro, to create cards etc. Essentially, traditional retro materials (post its, whiteboards) will not translate well. You could ask one of the remote team members to organise materials in the remote location, but generally the whole “webcam pointed at a whiteboard” approach has not worked out well for me in the past. Include the remote people It’s always easy to forget the remote people during retros, so we all (especially the facilitator) need to be sure we encourage the remote people to speak up. If it’s been a while since they have spoken – it can help to ask them by name for their opinion on the subject being discussed. Thanks for reading ~ Beccy", "date": "2015-07-08"},
{"website": "JustTakeAway", "title": "Protecting our inputs in a microservices world", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/06/23/protecting-our-inputs-in-a-microservices-world/", "abstract": "At JUST EAT, we’re increasingly moving towards an interconnected web of internal APIs that each own their own data. We’re doing this so we can slice up our centralised database along domain boundaries, and scale parts of our system independently in terms of performance and rate of change. Small things are easier. In this sort of world, it’s more important than ever to have consistent behaviour for non-functional requirements that span across different components. It’s a waste of everyone’s time to keep reinventing the wheel, but slightly differently, each time. These cross-cutting concerns are the usual suspects — security, validation, logging, metrics. This post deals with validation and consistency in how to respond when errors happen (when rules are violated). It’s really important to validate one’s inputs — Garbage In, Garbage Out. Further, it’s valuable to reject invalid input as early as possible, so that the internals of the platform don’t have to worry about invalid data so much (not that they should trust internal clients more than external ones, but at least one can go tap on the shoulder of an engineer that looks after an internal thing). Rejecting invalid input & requests politely By validation, broadly speaking, I’m talking about two classes of error that I think are distinct: Errors because the rules about the request were invalid – for example, the “Email” field was blank, or not an actual email address – rules that don’t require the API to ask anything external Errors because the request was valid, but could not be processed because some sort of business-rule was not obeyed – for example, the credentials did not match, or the order could not be accepted because the restaurant ran out of chicken for the evening while the customer was ordering – rules that do require input from other components Personally, I don’t really want clients of my APIs to have an inconsistent experience. So, if I give them back a 400 Bad Request, I want them to be able to say ‘Oh, I got a bad request, here’s a list of things I need to fix to make it work&rsquor;, rather than ‘Oh, I got a bad request, I’ll go and ask the API team how to fix it — because it’s different from that other operation from that other API&rsquor;. This means my APIs should give back an error response body that is standard, across operations and across APIs. Open Source! I’m really pleased to announce that today, we’re open-sourcing a library that we’ve built to do exactly this – JE.ApiValidation . It contains A standard error response contract DTO The assumption that you’ll be using the excellent FluentValidation library to implement your validation rules Request-validation support for WebApi and OpenRasta , for requests that have a validator defined for them registered in your container: WebApi: plug in to modelstate stage via a global action-executing filter OpenRasta: plug in as an OperationInterceptor Error-during-processing support for WebApi and OpenRasta, for requests that fail a business-rule check WebApi: an exception-filter catches any FluentValidation.ValidationException and transforms that to the standard error response OpenRasta: an OperationInterceptor catches any FluentValidation.ValidationException and transforms that to the standard error response virtual methods for you to override, should you want to log warnings via your logging framework of choice, or publish metrics, or anything else Error contract The standard error response looks like: {\n  \"Code\": 40000,\n  \"Message\": \"some text about what class of error this might be\",\n  \"Errors\": {\n    \"FieldName\": [\n      {\n        \"Code\": 0,\n        \"Message\": \"Error message for the rule being violated\",\n        \"AttemptedValue\": \"What value was rejected\",\n        \"CustomState\": {\n          \"Something\": \"Custom\"\n        }\n      }\n    ]\n  }\n} There are a few points about this design of response: so that clients can use arithmetic to decide what class of error this is, each error response is categorised, and each category has a (hopefully unique) code number. So far, 40000 is “invalid request”, 45000 is “error during processing”. The codes are modelled after http and smtp , which suggest classes of error be denoted by ranges of values, so it’s easier for machines to interpret. it’s very similar to what FluentValidation gives you out of the box. it fulfils the requirement about telling the client as much as possible about what went wrong, rather than forcing them to solve each problem they may encounter one request & code-change at a time it could be improved by adding some data about whether the request should be retried, or is a final-state (for example, if there was a network transient, probably retry it. If the payment failed because the customer’s card expired, there’s no point retrying) Source code & how to install The source code for the library can be found at http://github.com/justeat/JE.ApiValidation. There are a few different nugets here, all installable from nuget.org and the package manager or console: JE.ApiValidation.DTOs – the contract. Take this if you want the notion of the standard contract, but the implementation for it doesn’t suit you (PRs welcome!) JE.ApiValidation.OpenRasta – the OpenRasta OperationInterceptors for request validation and error-processing JE.ApiValidation.WebApi – the WebApi request validation attribute – no dependency on FluentValidation, in case you happen to use DataAnnotations to do your validation already (and don’t want to change that) JE.ApiValidation.WebApi.FluentValidation – the WebApi error-processing exception-filter (that depends on FluentValidation) Getting started There are examples for how to use it: WebApi request validation using JE.ApiValidation.WebApi;\npublic class MyWebApiConfiguration : HttpConfiguration {\n  public MyWebApiConfiguration() {\n    // ...\n    ConfigureRequestValidation();\n  }\n  private void ConfigureRequestValidation() {\n    FluentValidationModelValidatorProvider.Configure(this, provider => provider.ValidatorFactory = new ValidatorFactoryForYourContainer());\n    Filters.Add(new FilterForInvalidRequestsAttribute());\n  }\n}\n// and, assuming a class called \"Widget\" with an string:Email property\npublic class WidgetValidator : AbstractValidator {\n  public WidgetValidator() {\n    RuleFor(x => x.Email)\n      .NotNull()\n      .NotEmpty()\n      .EmailAddress();\n  }\n} WebApi errors during response-processing using JE.ApiValidation.WebApi;\nusing JE.ApiValidation.WebApi.FluentValidation;\npublic class MyWebApiConfiguration : HttpConfiguration {\n  public MyWebApiConfiguration() {\n    // ...\n    ConfigureResponseProcessingErrorHandling();\n  }\n  private void ConfigureResponseProcessingErrorHandling() {\n    Filters.Add(new ResponseProcessingErrorAttribute());\n  }\n// and then, assuming a class verbosely named InternalRepresentationOfAWidget\npublic class WidgetBusinessRules : AbstractValidator {\n  RuleFor(x => x.Something)\n    .Must(x => x.Something == \"something\")\n    .WithMessage(\"Some arcane rule was not met.\");\n}\n// and finally, within a controller or service class (where _validator is injected via your container):\nvoid DoSomething(InternalRepresentationOfAWidget w) {\n  _validator.ValidateAndThrow(w);\n  // ... do things knowing that the business rules passed\n} OpenRasta request validation (link to example) WebApi errors during response-processing (link to example) For the time being, continuous integration and publishing the nugets will be internal; that will change in due course.", "date": "2015-06-23"},
{"website": "JustTakeAway", "title": "MDD – Motto Driven Development", "author": ["Written by Nick Thompson"], "link": "https://tech.justeattakeaway.com/2015/06/09/mdd-motto-driven-development/", "abstract": "Our purpose At JUST EAT our purpose is… ‘Empower consumers to love their takeaway experience’ Whenever a decision is made, this catchy little motto is used to remind everyone involved of our ultimate goal as a company. Our websites, and their development, should be no exception. Within this post we dig into the way in which the purpose has driven, and continues to drive, our web development culture. So how do we empower consumers through web development? The website must allow our customers to order food from restaurants – this is after all our core value proposition. The customer should be empowered to access the website from watches, game consoles, mobile phones, tablets or desktop computers and their choice should be largely unrestricted. To fulfil these requirements the website must utilise technologies that are accessible and available across as many platforms as is feasible. Classic HTML, with form posts being used to alter the state, was identified as providing the high level of compatibility required. JavaScript exposes the website to many cross browser issues, due to the differing implementations of its core library and DOM access. Creating a website that does not rely on JavaScript reduces the impact of inconsistent support and enables graceful failure of a user’s experience on inconsistent platforms. Unfortunately in practice, there are cases where JavaScript is needed. One area of our website that depends on JavaScript is credit card payment authentication – primarily due to the idiosyncratic requirements of the banking system. Although this adds a hard dependency on JavaScript, minimising its necessity, maximises the chances of providing the service to all platforms and localises the scope of failure. How can we help customers love their takeaway experience? The website, like the rest of the company, should attempt to provide an experience that the customer will love. In order to love the experience, it shouldn’t cause the user to feel frustrated, impatient or preclude a #minifistpump. Server posts introduce additional round trip requests adding a delay between each interaction. Further, page refreshes typically result in a different scroll position; displaying a different area of the page which can be disorientating and breaks interaction flow. It’s also difficult to engage the customer when visual cues to system responses can not be communicated during page refresh. To create an experience the customer will truly love, requires that JavaScript be used to enhance the classic HTML with interaction simplifications, when available. The system can leverage JavaScript to communicate to the customer when progress is made with visual transitions. Customers can keep focused on their task by avoiding interruptions to the process flow caused by view refreshes through the use of asynchronous updates. How do we bring both strategies together? To bring both halves of the purpose to life, the website is being developed with a progressive enhancement strategy. Each page of the website is written using an MVC framework and view engine, rendered on the server side into semantic HTML. Interactions are built so that they can be performed using simple get (links) or post requests (form buttons). This provides the core experience and follows the aforementioned strategy to support a huge range of different devices. The website then uses JQuery to provide multi-browser support allowing components of the page to be upgraded when and where the required browser features are detected. An example would be where we would like to show the user a list of food categories available within a menu. The ideal situation is to have that list minimised by default but allow the user to expand it should they be interested in its content. Unfortunately the functionality to hide and expand the list is unavailable in some devices. In order to facilitate the functionality across all users we create a default in which the list is expanded and then during the page load we identify if the show/hide functionality would be available. If it is then we change that default view using JavaScript to the minimised version. This enhances the usability of the site without affecting its accessibility on any browser. This approach also scales up to more involved solutions and is applicable to many other scenarios. This completes the overall approach by allowing us to add richness and interactivity that our users love without impeding their ability to progress should that richness fail to operate correctly. What does this mean for the team? Implementing a core experience in HTML and enhancing with JavaScript may require more time to develop by an engineering team as frontend and backend engineers must work together to define interfaces that will enable the multiple levels of interaction provided to the user. It also requires a greater involvement and more time to design by the user experience team. An understanding of the progressive nature of the interface is needed at design level if a fluid and consistent experience is to be presented to the the user. This holistic understanding and approach requires close collaboration between many disciplines – UX, design, product and engineering. The necessary high level of collaboration has been facilitated with a cross functional team structure. These teams have representatives from each discipline required to implement a feature in a top to bottom slice of the system. This provides the high level of collaboration needed to implement a non-trivial solution such as progressive enhancement and ensures an inside out approach to development. The website’s evolution is a non trivial task and needs a clear direction to achieve our ultimate goal as a company. We can produce something truly outstanding with the right process in place. At JUST EAT, we’re using MDD (Motto Driven Development) to… ‘Empower consumers to love their takeaway experience’", "date": "2015-06-09"},
{"website": "JustTakeAway", "title": "Deployment Automation using Powershell DSC", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/06/01/deployment-automation-using-powershell-dsc/", "abstract": "Immutable Servers using Desired State Configuration At JUST EAT, when we upgrade our platform services within our AWS hosted environment, we don’t just update the package on the instances, we replace the old servers with new ones built from scratch. This is an extension of the phoenix server approach. The longer a server has been provisioned and running, the more likely it is in an unknown state, creating a problem known as Snowflake servers . We continuously deploy our components many times a day replacing almost every part of our platform infrastructure. Our continuous deployment strategy encompasses repeatable environments and extremely fast deployment times through pre-built static images (Amazon images – AMIs) and service orchestration times through decentralised orchestration tools (AWS Cloudformation). It’s difficult to guarantee repeatable deployments of the same server due to dependencies on packages, network availability and changes in the environment descriptions hence we achieve predictability using machine images that have been pre-built and tested within a Continuous Integration pipeline. Using version controlled recipes to define server configurations is an important part of continuous delivery . Desired State Configuration (DSC) is a new management model for Powershell that allows us to deploy and manage configuration data for software services and environments, in turn allowing us to configure how to bring new and existing machines into compliance. DSC provides a set of Windows Powershell language extensions and resources that can be used to specify how you want your server to be configured allowing us to create Pheonix servers that can be recreated reliably from scratch. DSC is the layer that accepts a configuration in the form of a static vendor neutral Management Object Format (MOF) file , a plain-text file in a format developed by the Distributed Management Task Force (DMTF), and implements it. The extensions in Powershell merely provide a Domain Specific Language (DSL) to define a configuration and generate the final MOF which is then executed by DSC. You could potentially write your own administrative interface to DSC. Moreover, since the MOF is based on an open standard, you could implement your own agent that provides the local and native resources to manage an OS. Hence, DSC is an open, cross platform solution to configuration management. The DSC agent on Windows is called the Local Configuration Manager (LCM) and is easily configured via a DSC configuration. You can use DSC to build a LINUX machine using the Open Management Infrastructure . This post aims to introduce DSC and is an introduction to a series of posts that will dive into advanced features of DSC including the different modes in which DSC operates (push and pull), the LCM and building LINUX AMIs. In order to generate machine images (AMIs) in AWS, we will be push ing a configuration to a remote instance and asking DSC to evaluate it. Generating an AMI for EC2 in AWS An Amazon Machine Image (AMI) defines the programs and settings that will be applied when you launch an EC2 instance. We dynamically install and configure our services on EC2 at instance launch time, pulling packages from S3, deploying files and ensuring services are started using CloudFormation . The AMI has the necessary OS features and tools pre-installed in order to speed up stack creation. In order to create an AMI, we are going to require AWS Tools for Powershell installed. The following steps allow you to create an EC2 instance to create an image from, install features using DSC and finally create an image from it using just Powershell. 1) Generate a key-pair and a security group for managing access to the instance whilst you are building it. You will need to open open up ports for WinRM. $keypair = New-EC2KeyPair -KeyName $name -ProfileName $profile\n\"$($keypair.KeyMaterial)\" | Out-File -Encoding ascii -Filepath $keyfile\n\"KeyName: $($keypair.KeyName)\" | Out-File -encoding ascii -Filepath $keyfile -Append\n\"KeyFingerprint: $($keypair.KeyFingerprint)\" | Out-File -Encoding ascii -Filepath $keyfile -Append $groupid = New-EC2SecurityGroup $name -Description $description -ProfileName $profile\n$publicIpAddress = <substitute with your public ip address>\n$ipRanges = @(\"$publicIpAddress/32\")\nGrant-EC2SecurityGroupIngress -GroupName $name -ProfileName $profile -IpPermissions @{IpProtocol = \"tcp\"; FromPort = 5985; ToPort = 5985; IpRanges = $ipRanges}\nGrant-EC2SecurityGroupIngress -GroupName $name -ProfileName $profile -IpPermissions @{IpProtocol = \"tcp\"; FromPort = 5986; ToPort = 5986; IpRanges = $ipRanges} 2) Find the standard Windows 2012 R2 image from the Amazon store and create an EC2 instance from it and set its attached EBS volumes in the exact way you want them created in the final image. Set the key-pair and security group to the ones created in step 1. Set the user data block to the following: $newInstances = Get-EC2Image -ProfileName $profile -Filters @{Name = \"name\"; Values = \"$imagePrefix*\"}\n$imageid = $newInstances[$newInstances.Length-1].ImageId\n$imagename = $newInstances[$newInstances.Length-1].Name\n$userdata = @\"\n<powershell>\nSet-ExecutionPolicy Unrestricted -Force\nEnable-NetFirewallRule FPS-ICMP4-ERQ-In\nSet-NetFirewallRule -Name WINRM-HTTP-In-TCP-PUBLIC -RemoteAddress Any\nNew-NetFirewallRule -Name \"WinRM80\" -DisplayName \"WinRM80\" -Protocol TCP -LocalPort 80\nNew-NetFirewallRule -Name \"WinRM443\" -DisplayName \"WinRM443\" -Protocol TCP -LocalPort 443\nEnable-PSRemoting -Force\nRestart-Service winners\n</powershell>\n\"@\n$userdataBase64Encoded = [System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($userdata))\n$parameters = @{\n            ImageId = $imageid\n            MinCount = 1\n            MaxCount = 1\n            InstanceType = $instanceType\n            KeyName = $keyPairName\n            SecurityGroupIds = $securityGroupId # created in step (1)\n            UserData = $userdataBase64Encoded\n            ProfileName = $profile\n            Region = $region\n            BlockDeviceMapping = $blockDeviceMapping # see docs..\n        }\n$newInstances = New-EC2Instance @parameters Please note that on Windows Server 2012 R2, WinRM is enabled by default, the firewall needs to allow remote management via WinRM. 3) Extract the DNS name from the API and use the key-pair to extract the administrator password as shown below. You can now configure the machine using DSC. $newInstances = Get-EC2Instance -ProfileName $profile -Filter @{Name = \"instance-id\"; Values = $instanceid\n$publicDNSName = $newInstances.Instances[0].PublicDnsName\n$keyfile = Get-KeyFile -KeyPairName $keyPairName -Profile $profile -Folder $folder\n$password = Get-EC2PasswordData -InstanceId $instanceid -PemFile $keyfile -Decrypt -ProfileName $profile\n$credential = Create-CredentialObject -Username \"Administrator\" -Password $password Executing a DSC configuration against an EC2 instance The following is an example configuration we use at JUST EAT to install the web server features on server core: Configuration SomeConfiguration\n{\n  param ([string[]]$computerName = 'localhost')\n  Node $computerName\n  {\n    WindowsFeature Web-AppInit { Ensure = 'Present'; Name = 'Web-AppInit' }\n    WindowsFeature Web-Asp-Net45 { Ensure = 'Present'; Name = 'Web-Asp-Net45' }\n    WindowsFeature Web-Http-Tracing { Ensure = 'Present'; Name = 'Web-Http-Tracing' }\n    WindowsFeature Web-Mgmt-Service { Ensure = 'Present'; Name = 'Web-Mgmt-Service' }\n    WindowsFeature Web-Net-Ext { Ensure = 'Present'; Name = 'Web-Net-Ext' }\n    WindowsFeature Web-Server { Ensure = 'Present'; Name = 'Web-Server' }\n    WindowsFeature Web-WebSockets { Ensure = 'Present'; Name = 'Web-WebSockets' }\n    WindowsFeature Web-Mgmt-Compat { Ensure = 'Present'; Name = 'Web-Mgmt-Compat' }\n  }\n} The keyword Configuration is an extension to the Powershell language. The keyword Node specifies the machine where the configuration will execute. WindowsFeatrue is a DSC Resource provided out-of-the-box with Windows Management Framework 4.0. Within this config we are asking DSC to ensure the features Web-AppInit , Web-Asp-Net45 , Web-Http-Tracing , Web-Mgmt-Service , Web-Net-Ext , Web-Server , Web-WebSockets and Web-Mgmt-Compat are installed on the machine with name $computerName . We set the property Ensure to Present in order to ask DSC to make sure the feature is present. We can set the value to Absent to remove a feature. You can use the Powershell commandlet Get-WindowsFeature to find the feature names. You can see the declarative syntax of DSC at work here; we are specifying the desired state of the machine, we don’t really care how it happens. DSC will check if the feature already exists and install it if not found, which means that you can keep running the configuration above to ensure compliance. This configuration can be extracted into a composite DSC resource, which you can use in multiple configurations. This is quite useful as you can develop a suite of behaviour tests for your own resources which can then be combined to create a robust set of configurations for your images. Creating your own resources is an important part of authoring DSC configurations and we’ll cover this in another post. Once you have defined a configuration, generate the DSC MOF using: SomeConfiguration -ComputerName <dns name of the EC2 instance> -OutputPath $outpath $outpath will contain the DSC MOF file which will be pushed to the EC2 instance. We can push the MOF to the remote machine and ask DSC to force the remote instance to evaluate the configuration using the following script: $securepassword = ConvertTo-SecureString $password -AsPlainText -Force\n$credential = New-Object System.Management.Automation.PSCredential ($username, $securepassword)\n$cimSession = New-CimSession -ComputerName <dns anme of EC2 instance> -Credential $credential -Authentication Negotiate\nStart-DscConfiguration -Verbose -Wait -Path $outpath -Force -CimSession $cimSession And that’s it! -Wait will wait until the configuration is applied and -Verbose will output the verbose logging in the resources to your local console. Once done, stop the EC2 instance and generate the AMI using the following powershell script: Stop-EC2Instance -Instance $instanceId -ProfileName $profile\n$ami = New-EC2Image -InstanceId $instanceId -Name $name -Description \"JustBake generated AMI from configuration $name\" -ProfileName $profile Setting up a task in CI to generate an AMI DSC is already pre-installed on Windows Server 2012 R2 and if you use just DSC to configure your AMIs, then everything is just a set of text files that live in a version control system like Git. Just clone the repo and execute the scripts on a Windows Server 2012 R2 build agent. The only requirement is that you have the AWS SDK for Windows installed. The future of DSC DSC was first mentioned in the Monad Manifesto in 2002 by Jeffery Snover, a distinguished engineer at Microsoft and lead architect for Windows Server and System Center. He describes the delivery of DSC as completing an eleven year journey to deploy automation to the data center in the form of Powershell. The Windows OS, unlike LINUX is built around an enormous set of disparate APIs, meaning that getting something like Chef, Puppet or even DSC working on Windows is not only harder but can be a stressful task. Microsoft is pouring time and money into creating DSC resources that can, through a very simple and consistent interface, configure most of the OS. The next version of the Windows Management Framework, WMF 5.0 , also shipped with Windows 10 will allow installing DSC resources directly from the Powershell Gallery , hence providing a mechanism to share DSC resources, guaranteeing that the coverage of the OS and applications will grow exponentially. Microsoft and other companies out there will be doing a lot of work that you don’t have to! Microsoft recently announced Nanoserver , a purpose-built OS designed to run cloud applications and containers. Powershell DSC will be the preferred way to remotely manage the OS in the future. Microsoft is working with Chef to create a set of Cookbooks based on Powershell DSC in order to provide enterprises with a rock-solid, native automation experience for the provision of compute and storage instances on Azure (and AWS). If you are not already using DSC to configure your Windows instances, then you are going to miss out on the work that Microsoft is doing with the community to save them and their users the work to configure the OS. Useful links The following book by Jez Humble and David Farley is highly recommended if you want to dig deeper into Deployment Pipelines: Continuous Delivery http://martinfowler.com/bliki/DeploymentPipeline.html Advanced Powershell DSC series: http://channel9.msdn.com/Series/Advanced-PowerShell-Desired-State-Configuration-DSC-and-Custom-Resources", "date": "2015-06-01"},
{"website": "JustTakeAway", "title": "How to take screenshots for failing tests with KIF", "author": ["Written by Amarpreet Sandhu"], "link": "https://tech.justeattakeaway.com/2015/05/11/how-to-take-screenshots-for-failing-tests-with-kif/", "abstract": "How to take screenshots for failing tests with KIF In the Consumer iOS app team we use KIF to test our application. KIF (Keep It Functional) is an open source tool for iOS automation from square. More details here: https://github.com/kif-framework/KIF Taking screenshots for failing tests is a good way to understand why the test is failing. Most test automation tools come with some way of taking screenshots. To enable taking screenshots for failing tests in KIF, we need to set an environment variable “KIF_SCREENSHOTS= ‘location’”. Location is the path which tells KIF where to save the screenshots. To make sure that screenshots are saved for all failing tests, irrespective of the machine they run on, ‘location’ should be relative. I used the location ‘$PROJECT_DIR/Failed_Screenshots’. $PROJECT_DIR is an Xcode build setting which stores the value of the Xcode project directory. This will already be set to where the project is on your system. To see the value of $PROJECT_DIR, go to the terminal, navigate to where the Xcode project is, and type ‘xcodebuild -showBuildSettings | grep PROJECT_DIR’. KIF creates the Failed_Screenshots folder automatically, whenever a test fails and saves screenshots for all the failing tests. Below are the steps to enable screenshots from Xcode: How to enable screenshots using Xcode Select the test scheme in Xcode Select Edit Scheme Select Run and set an environment variable KIF_SCREENSHOTS= ‘location_to_Save_screenshots’ Select Close Write a failing test This is a sample test which I created to demonstrate how KIF can take screenshots. This test is looking for an accessibility label which does not exist and always fails. Run the test using Xcode or xcodebuild on command line Check the screenshot in the failed screenshots folder, which you set as the value of the KIF_SCREENSHOTS variable. The screenshot is saved with the same name as the test class and also includes the line number of the failing step. For example, ‘JETESTScreenshots.m, line 17.png’, in this case ‘JETESTScreenshots.m’ is the test class and line 17 is the line number of the failing step. Give the path to the ‘Failed_Screenshots’ folder in the artifact paths field of the project in your CI system, to save the screenshots as artifact for every failing build job. Thanks for reading. Preet", "date": "2015-05-11"},
{"website": "JustTakeAway", "title": "3D Printing in JUST EAT", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/05/05/3d-printing-in-just-eat/", "abstract": "How did it start? A while ago I was telling my colleagues how cool the 3D printer I have at home is and how many things you could print with. My manager suggested getting one for the office and asked me which 3D printer to buy. I did a bit of research to see which one would best suit our office and our goals of improving the tech environment and we decided to buy a FDM printer. A few weeks later the printer was on our desk in a big box. Everybody gathered round us while we were opening it and soon the printer was plugged in and waiting to be used. I did the whole setup very carefully, as I didn’t want it to fail while everybody was watching, then loaded up an .stl file and hit print. And so it began… I decided that it would be a good idea to do a quick presentation and a workshop for those who were interested in this subject, before we started using it. That way I wouldn’t have to supervise everyone and it probably wouldn’t get broken. We then thought about what would help us to improve our work environment and decided to prototype some phone and tablet stands. We discussed the issues with the existing design, came up with an improvement, put it on paper, and then translated into a 3D model. A few hours later the prototype was ready to use. Soon after that, we had a Hackathon coming up and one of the ideas we worked on involved 3D printing. A colleague came to me with the idea and we recruited a designer to help us create the 3D model from scratch. You can check the finished result here . The best part of having this 3D printer is that it isn’t just for business use, you can just come in with a 3D virtual model and you can print it yourself. 3D Printing What is 3D printing? 3D printing is the process of making three dimensional objects from a digital file. The solid object can be created using different processes like extrusion (laying down successive layers of material) or sintering (compacting and forming a solid mass of material by heat and/or pressure, without melting it). It is similar with two dimensions printing, but with an added third dimension, Z-axis. How it works? As the 2D printing starts with creating a “model” using a software, the 3D printing process starts the same. First you create and design the virtual 3D model on a computer. The model is created using a 3D modelling software (e.g. Solidworks, Catia, SketchUp) or scanning an existing object. Secondly you export it to a format that is compatible with the printer (e.g. *.stl, *.obj), and then the software slices the design into small horizontal layers that will be printed one on top of the other until the final object is done. Finally you print the virtual model into a solid object. Printing technologies Almost all types of technology in the world have got multiple ways of implementation. And so does 3D printing. Not all 3D printers use the same technologies to build objects. The big difference is how the layers are created. Some of them produce the layers by melting or heating the materials. The most common technologies using this way of printing are Selective Laser Sintering (SLS) and Fused Deposition Modelling (FDM). Another method of 3D printing is using liquid resins that are treated using different technologies. An example for this method is called Stereolithopraphy (SLA). Selective Laser Sintering – SLS OWNER: Materialgeeza, used under Creative Commons license SLS technology was developed and patented in the 1980s by Dr. Carl Deckard at University of Texas under DARPA’s sponsorship. It is based on a high power laser that heats the powdered material (plastic, metal, ceramic or glass) transforming it into solid shape. The printer has 2 chambers, starting with one empty(A) and one full with powdered material(B). Once the process starts the roller moves one layer of powdered material from chamber B to A, and the laser fuses it by scanning the layers generated by the 3D modelling software. Then the platform is lowered by one layer thickness and another one is applied on top, with the process repeating until the object is created. The powder that is not touched acts like a support structure for the object. Remaining powder can be reused for the next print, so there we be no wasted material. Stereolithopraphy – SLA OWNER: Materialgeeza, used under Creative Commons license SLA technology was developed in 1986 by Charles Hull who founded the company called 3D Systems. This process involves a vat of liquid ultraviolet curable photopolymer resin and an ultraviolet laser in order to build the object layer by layer. The platform that will support the object goes down into the resin layer by layer while the laser beam traces the pattern on the surface of the liquid. By exposing the resin to the laser light will make it solid and so the object is created. The remaining resin can be reused for the next print, so there will be no wasted material. Fused Deposition Modeling – FDM OWNER: Zureks , used under Creative Commons license FDM technology was developed in the late 1980s by S. Scott Crump, and commercialised in 1990. This technology uses different types of filaments that are pulled from a spool into an extrusion nozzle. Once the material gets into the hot nozzle it melts and can be easily extruded into the desired shape. The extruder can move in both horizontal or vertical directions and so can the platform that supports the object (bed). The final object is produced by laying down multiple layers one on top of another. The material hardens immediately after extrusion from the nozzle. Printing flow Everything starts with the computer, where you can create the 3D virtual model of the object and export it to a file. The file is then sent to a slicing software that interprets it and transforms to a gcode format that is easily interpreted by all 3D printers. Finally the printer creates layer over layer of the selected material and produces the solid version of the previously created virtual object. Materials In the last period a lot of different types of materials appeared for this printing technology. Starting from different colours of the same material to flexible or hard materials, you can choose whatever works best for your project. Here is a small list of material types (description of the filaments from e3D-online ): PLA – PLA is a bioplastic usually derived from corn starches. PLA melts at low temperatures, has extremely low warp, bonds easily to cold bed surfaces, and generally prints with fantastic quality and no fumes. It might not be the best at demanding mechanical or thermal applications but it is cheap and easy to print with. The most popular polymer in 3D printing. ABS – One of the most used plastics in our made world, ABS is a copolymer of Acronitrile, Butadiene and Styrene. Lego bricks, car dashes, and electronics enclosures all tend to be ABS. ABS provides a great balance of properties, while still being low cost enough for everyday use. Be aware that not all ABS is the same, and the properties from one ABS to the next can vary wildly. Co-polyesters – PET, PETG and related Co-Polyesters present a great balance of printing much more easily than ABS, but having much better mechanical and thermal performance than PLA. They also come in crystal clear variants which have the best optical clarity in the plastics world. Composites – Composite filaments are composed of a polymer binder with another material added to them to change their appearance and/or physical properties. This can take the form of attractive wood-like materials, where a binder holds together wood fibres, or specialist mechanical properties obtained by adding carbon fibres to a polymer binder. Flexibles – Probably amongst the most fun filament types out there. These filaments are synthetic thermoplastic rubber-like materials. Great for useful products like gaskets, seals, shock absorbers, flexible hinges, and tyres. Also they’re great for just plain fun in printing various aesthetic models with a flexible twist. Nylon – Nylons represent some of the toughest and strongest materials that can be printed. While they take a little more practice to print than other materials the rewards are huge. Interlayer adhesion is probably the best in the 3D printing world and tensile strength is fantastic. Nylons are relatively flexible and can be used for things like living hinges with long life. Nylon is also extremely low friction and makes for great bearings, sliders and gears. Preparing for print Bed levelling: the printed part will stick better on the bed if levelling is done correctly once done properly will last for a while, depending on usage methods: using a piece of paper of a known thickness feeler gauges, micrometer test prints and adjust the bed on the fly eyeballing Cleaning the bed, degreasing Heating bed and extruder – depending on what material you are using you will need to use different temperatures for both the bed and extruder. Variables you can change before printing position of the object on the bed layer height printing speed cooling fan speed (PLA needs cooling fans, ABS doesn’t) infill percentage first layer settings skirt settings supports. Printing sources – Depending on your printer model you can print from various sources like a computer, an SD card, your mobile phone or even from a file that is stored somewhere in the cloud End of printing – After the printing process is done you don’t need to fast remove the object from the bed, as you can easily break it. It is recommended to leave it for a few minutes while it is cooling down. By doing this will make it easier to remove it from the bed surface without damaging it. During the time after you remove the part, make sure that you clean and degrease the bed. Usage of 3D printing 3D printing has been used in a lot of domains for decades, especially for designing and rapid prototyping of different objects. Until a few years ago these technologies were really expensive and slow, but now you can find a variety of models and prices. Prices can start from a few hundred pounds for a personal printer, to thousands for an industrial, really fast one. Depending on the needs you can buy what suits best for you or your business. Industrial printing More and more companies are saving lots of money by using rapid prototyping using 3D printing technologies. In just a few minutes or hours you can check that your future product is exactly what you’ve created in the modelling software. With the advantage of having a variety of materials that you can use for printing, you can mimic everything. The quality of some of the printers is so good that companies are using them directly for mass production of the products and not just for prototyping. Personal printing Starting in 2011, 3D printing became popular for personal use. This happened mostly because desktop 3D printers started to become affordable in prices ranging from £250 to £3000 for high definition ones. You can use them to print small parts that you need around the house, or even as part of your hobbies (home made drones, robots or toys). Do you have to be a 3D modelling expert to create 3D models? No, not at all. While complex and expensive CAD software like Catia and Solidworks have a steep learning curve, there are a number of other programs, many free, that are very easy to learn. The free version of Google SketchUp, for example, is very popular for its ease of use. Thank you for reading! Adrian", "date": "2015-05-05"},
{"website": "JustTakeAway", "title": "App launching: OperationQueue to the rescue", "author": ["Written by Luigi Parpinel"], "link": "https://tech.justeattakeaway.com/2021/02/04/app-launching-operationqueue-to-the-rescue/", "abstract": "The challenge: MAD (Massive App Delegate) Modern apps are complex, the iOS community has come up with a few architectural patterns to manage such complexity. Nonetheless, there is a step in the apps lifecycle where this complexity has not yet been tamed: the app launch. It’s quite common to find bugs in which an app does not behave as expected, when launched from springboard shortcuts, or notifications. These bugs are usually related to: non-linear navigation, this happens when an app is already running and you try to navigate to a screen that can’t be reached from the current screen issues in managing the request because the app is launching from a cold start. As developers we want our app to behave correctly and to launch as fast as it can. The iOS watchdog monitors the app’s launch time, and terminates  the app if the startup is not fast enough. The complexity comes from all the steps that the app needs to perform to be ready to be used. These steps include things like fetching and applying feature flags, executing asynchronous network code, migrating databases, initializing third-party SDKs and other operations such as the handling of universal links, NSUserActivity or quick actions. It’s important to note that some of these operations can be executed concurrently, while others should be done in a specific order. Another source of complexity is time. The AppDelegate is ( was , but this is out of topic for this post…) the first object that you see when you create a new project and this usually means that it is one of the oldest files in your codebase. It is supposed to be used as an interface to communicate with the OS, but it ends up being an over complicated and confusing class, in which you can find every kind of code, from network code to UI code. Divide et impera Let’s define what we want to achieve. We want our app launching code to be: Fast Predictable Encapsulated Decoupled Testable Maintainable To achieve this we can use the divide et impera strategy. The app setup should be divided into small chunks of initialisation code following the single responsibility principle . These chunks will have dependencies between each other so we need to track which chunk depends on what. Conceptually the app is ready to be used when all these chunks have been executed. An important note is that the code of the chunks must be decoupled, but this does not mean that they can’t be dependent on each other, for example one chunk can produce a result that another chunk will consume as its input. Once we have all these blocks of code we need one or more executors to run them. Foundation already provides a great way to achieve this: OperationQueue . Each chunk of code will be an operation . The operations are encapsulated and can be easily tested on their own. Their code is decoupled but it’s very easy to define dependencies between operations in a declarative way and the framework will handle the dependencies for you. Dependencies can be even defined on operations enqueued on different queues running on different threads. Defining dependencies in a declarative way will make it very easy to understand and maintain them. It will make your code predictable because you will always know in which order it will be executed. To make our code faster and to avoid blocking the main thread for too long, we can use multiple queues to execute code in parallel. A possible setup could be: using the main operation queue to execute the operations one by one on the main queue running on the main thread using a background concurrent queue to execute in parallel multiple operations that do not need to be executed on the main queue, such as networking code or database management. The OS will automatically scale the number of concurrently running operations based on the device resources (RAM, number of CPU cores, etc.) and the dependencies between the operations, which will guarantee the fastest possible execution and the most efficient resource usage. Show me some code (or pseudo-code) It’s not easy to provide a real and meaningful example of an app setup, but I want to provide a quick example. A common and complex-enough scenario, is when the user launches the app using a quick action from the home screen. It’s quite easy to handle, isn’t it? func application(_ application: UIApplication,\n                 didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n\n        if let shortcut = launchOptions?[.shortcutItem] as? UIApplicationShortcutItem {\n            doSomething(shortcut)\n        }\n\n        return true\n} Sadly, it is not that easy… because we need to do other stuff first, like initialise the crash reporting sdk. func application(_ application: UIApplication,\n                  didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n\n        AnotherSDK(key: \"Th1s1s4S3cretK3y\")\n        \n        if let shortcut = launchOptions?[.shortcutItem] as? UIApplicationShortcutItem {\n            doSomething(shortcut)\n        }\n\n        return true\n    } Done! More or less.. Other things to consider include: The network call which should be fired as soon asthe app starts to fetch the user’s data. The feature toggles should be fetched before performing the action, but after the SDK initialisation. func application(_ application: UIApplication,\n                     didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n\n        AnotherSDK(key: \"Th1s1s4S3cretK3y\")\n        \n        let userDataTask = URLSession.shared.dataTask(with: self.userDataURL) { [unowned self] _, _, _ in\n            let featureToggleTask = URLSession.shared.dataTask(with: self.toggleURL) { _, _, _ in\n                if let shortcut = launchOptions?[.shortcutItem] as? UIApplicationShortcutItem {\n                    self.doSomething(shortcut)\n                }\n            }\n            featureToggleTask.resume()\n        }\n        userDataTask.resume()\n\n        return true\n    } How easy is it to understand and change coupled code like this? It already starts to look like a Massive App Delegate. In the following diagram you can see how the initialisation code can be split into operations and their dependencies (keep a lookout for circular dependencies to avoid potential deadlocks). The blocks represent the operations. The yellow blocks are the initialisations one and the green “handle shortcut” block is the action that the app should perform when the app is ready. The arrows show the dependencies between the blocks. Now that we know what we want to achieve, let’s see how the code will look. // The returned queue executes one operation at a time on the app’s main thread\nlet mainOperationQueue = OperationQueue.main\n\nlet concurrentOperationQueue: OperationQueue = {\n    let queue = OperationQueue()\n    queue.qualityOfService = .userInitiated\n    return queue\n}()\n    \nfunc application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n\n    let handleQuickAction = BlockOperation { [unowned self] in\n        if let shortcut = launchOptions?[.shortcutItem] as? UIApplicationShortcutItem {\n            self.doSomething(shortcut)\n        }\n    }\n        \n    let sdkInitializationOperation = BlockOperation {\n        AnotherSDK(key: \"Th1s1s4S3cretK3y\")\n    }\n        \n    let fetchUserDataOperation = AsyncBlockOperation { [unowned self] operation in\n        let task = URLSession.shared.dataTask(with: self.userDataURL) { _, _, _ in\n            // Do something with the response\n            operation.finish()\n        }\n        task.resume()\n    }\n\n    let fetchFeatureToggleOperation = AsyncBlockOperation { [unowned self] operation in\n        let task = URLSession.shared.dataTask(with: self.toggleURL) { _, _, _ in\n            // Do something with the response\n            operation.finish()\n        }\n        task.resume()\n    }\n        \n        // Setting up the dependencies\n    handleQuickAction.addDependency(fetchUserDataOperation)\n    handleQuickAction.addDependency(fetchFeatureToggleOperation)\n    \n    fetchFeatureToggleOperation.addDependency(sdkInitializationOperation)\n        \n    mainOperationQueue.addOperations([handleQuickAction], waitUntilFinished: false)\n    concurrentOperationQueue.addOperations([sdkInitializationOperation,\n                                        \n    return true\n} The operations can be moved away from the app delegate and from the application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool. The key point here is that each operation is self-contained, they can be tested separately and the dependency management between them is easy to change, it’s declarative and it is decoupled from their creation, making it easy to modify the execution order and the concurrency model. Note: AsyncBlockOperation is a common Operation subclass that I suggest to add to your codebase. An example can be found here . Conclusion In software engineering there are always multiple solutions to address an issue and this is just one of the ways in which you can better handle the application setup. The most important advantages in this solution are: It is built-in in the iOS SDK. OperationQueue is available since iOS 2.0, it’s been  battle-tested over the years and it’s good to rely on something supported by Apple for a critical part of the app such as the app setup; It is quite easy to implement. Making an operation starting from a block of code is very easy so it should be straightforward to refactor existing code with this approach; The declarative style for dependency management makes it very easy to write predictable code. On top of all of that, every iOS developer should already be familiar with OperationQueue so it should be easy for everyone in your team to work with them without having to learn another framework or paradigm and, maybe, someone will enjoy this powerful tool even more!", "date": "2021-02-04"},
{"website": "JustTakeAway", "title": "URLCache – Default size is not enough", "author": ["Written by Luigi Parpinel"], "link": "https://tech.justeattakeaway.com/2021/02/01/urlcache-default-size-is-not-enough/", "abstract": "Even though networks are getting faster every day, the amount of data transferred between your device and the cloud is also increasing. Here at Just Eat, we always aim to write reliable, and efficient code that results in fast, problem free transactions. Having a good caching strategy is an essential part of this. Having an effective cache could help: 🙂  improve the user experience, by making the app faster and by saving battery life 🔋 🌱  save energy by reducing the amount of overall network traffic 💰  save money on the infrastructure, because the backend load will be reduced without any product change. iOS has built-in support for HTTP caching, so you might think the work has already been done for you. But if you look closely, you might start wondering why some of your network calls get executed every time. If your app is doing that, a simple reason could be because there isn’t enough space to cache the response so make sure you check your URL shared cache size! Investigating the problem and understanding the solution: Let’s go a bit deeper. URLCache will not store a response if it is too big and the app will always perform the request despite the HTTP header directive. URLCache can store data both in memory and on disk. The documentation in NSURLCache.h says that the default sizes are: 4 megabytes in memory 20 megabytes on disk My advice is to inspect the cache size at runtime because you can find different values from the documented ones. Whilst the algorithm used to decide when something is too big to be cached is not public,  I spent some time trying to determine the maximum size for a response to be cached. According to my calculations, the response size must be no bigger than ~5% of the size of the on-disk cache. It’s important to measure the network response size as uncompressed payload. It can make a big difference since most of the network calls nowadays use JSON which is text based format and usually has high entropy. If the default cache size is not enough for your app, you can increase it in one simple line of code: URLCache.shared = URLCache(memoryCapacity: 6*1024*1024, \n                             diskCapacity: 40*1024*1024, \n                                 diskPath: nil) Remember that the sizes are measured in bytes. So in this example the cache is set to: 6 megabytes in memory 40 megabytes on disk My advice is to set it as soon as the app starts before any network calls are fired. This small change allowed us to save up to ~10% of the iOS traffic on one of our busiest endpoints. I hope this little hint can speed up your apps, save some battery on users’ devices, and decrease the load on your backends too! References Photo by Harrison Broadbent on Unsplash NSURLCache on NSHipster", "date": "2021-02-01"},
{"website": "JustTakeAway", "title": "Distributed Jenkins Nodes for iOS CI", "author": ["Written by Dimi Chakarov"], "link": "https://tech.justeattakeaway.com/2021/01/11/distributed-jenkins-nodes-for-ios-ci/", "abstract": "Overview Like many other companies affected by lockdown, we needed to adapt to the new circumstances. That included distributing our CI build agents across our engineers’ homes. While mostly the new setup worked great, we faced a few challenges. This article outlines the solutions we used to help us further improve this service. The agents were connected using VPN and, from time to time, we needed to log in to perform maintenance using VNC. We were using the agents’ IP addresses to connect, but these addresses were not always constant. The process of determining the correct IP address and connecting to the right machine was not as effective as we would have wanted. We needed a change. First, let’s go over what we are using currently: We have a folder of shortcuts to VNC connections (aka screenshares ) for remote login Whenever a node stops responding, we ask their host (the closest engineer with physical access) to provide the new IP We update the corresponding shortcut in the folder We zip and send the folder to anyone that needs it Our screenshares (yes, we love Archer ) There are a few challenges with this approach that we intend to solve in this article: it’s hard to keep track of changing IP addresses we want to minimise our reliance on people manually updating information Plus, we will have an excuse to learn new technologies (did somebody say Combine ?) These are the steps to automate our management of CI agents/nodes and solve the stated problems: determine how we can automate getting the IP addresses provide a way for people to quickly connect to a given node keep the data up to date Let’s get started Automate getting the IP addresses Let’s start with Jenkins. We are going to need Scriptler, so go ahead and install it if you don’t already have it. Scriptler enables you to write scripts in Groovy that can be executed on your Jenkins server and/or agents. Once installed, add a new script with an easy to remember id and name, which prints out a list of agent names and IP addresses. You can use this as a starting point: import hudson.util.RemotingDiagnostics;\n\ndef jenkins = jenkins.model.Jenkins.instance\n\ndef printIp(String name, String config) { \n  def pattern = ~/10\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}/\n  def matcher = config =~ pattern\n  if (matcher.find()) {\n    println name + \": \" + matcher[0]\n  }\n}\n\nfor (node in jenkins.nodes) {\n  host = node.computer\n  def name = node.getDisplayName()\n  if (node.getChannel() != null) {\n    def config = RemotingDiagnostics.executeGroovy(\"\\\"ifconfig\\\".execute().text\", node.getChannel());\n    printIp(name, config)\n  } else {\n    println name + \": DISCONNECTED\"\n  }\n} In this script we are using ifconfig to get all the information for the network interfaces of the Jenkins agents and then printing just the IP address of each one. In order to extract the IP address, we are assuming all IP addresses in your corporate network start with 10 (e.g. 10.15.52.112). Ask your DevOps team for the submask your network is using. Save and run your script to verify it is working correctly. Alternatively, you can open https:// <your-jenkins-url> /scriptler/runScript?id= <your-script-id> .groovy (Don’t forget to replace <your-jenkins-url> and <your-script-id> with your values). The result should be something similar to the following: Malory: 10.⎕.⎕.⎕\nPam: 10.⎕.⎕.⎕\nRay: 10.⎕.⎕.⎕\nSterling: 10.⎕.⎕.⎕\nCheryl: DISCONNECTED\nWoodhouse: 10.⎕.⎕.⎕ The addresses are partial to protect our agents’ privacy. So far so good. Next step is to use the results for accessing the nodes. Generate Screenshares Luckily we can do that using the Jenkins REST APIs. You need to create a token to use them. You can do that by going to https:// <your-jenkins-url> /user/ <your-user-name> /configure. After creating the token, run this in the terminal: curl -X POST -L --user <your-user-name>:<your-api-token> https://<your-jenkins-url>/scriptler/run/<your-script-id>.groovy The result will hopefully be very similar to the one above: Malory: 10.⎕.⎕.⎕\nPam: 10.⎕.⎕.⎕\nRay: 10.⎕.⎕.⎕\nSterling: 10.⎕.⎕.⎕\nCheryl: DISCONNECTED\nWoodhouse: 10.⎕.⎕.⎕ Try connecting to one of them by opening Finder, and pressing Cmd-K. Then type vnc:// followed by the IP address of the machine. If that works, the next step is to create a shortcut ( screenshare ) so you wouldn’t have to go through Finder next time. Open a text editor (yes, vim is fine), and paste this (unmasking the address): <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n        <key>URL</key>\n        <string>vnc://10.⎕.⎕.⎕</string>\n</dict>\n</plist> Save and double click to connect. Et voilà! Now let’s create a script that automates the things we did so far. We will use what we know best – Swift. We will also streamline the process by using Apple’s open source framework Swift Argument Parser . The script should be pretty straightforward – connect to the API, get the data, and generate the screenshares. It should work like so: $ generate_screenshares --user <your-user-name> \\\n --token <your-api-token> \\\n --url https://<your-jenkins-url>/scriptler/run/<your-script-id>.groovy We will skip the trivial parts of the code for brevity. We create a new Command Line Tool project in Xcode, use SPM to import Swift Argument Parser, and paste this in the main.swift file. import Foundation\nimport ArgumentParser\nimport Combine\n\nvar cancellable: AnyCancellable?\n\nstruct GenerateScreenshares: ParsableCommand {\n    static let configuration = CommandConfiguration(abstract: \"A Swift command-line tool to generate screenshares\")\n\n    @Option(name: .long, help: \"Your username on Jenkins\")\n    private var user: String\n\n    @Option(name: .long, help: \"Your token on Jenkins\")\n    private var token: String\n\n    @Option(name: .long, help: \"The full URL to your Groovy script\")\n    private var url: String\n\n    func run() throws {\n        let client = JenkinsAPIClient(user: user, token: token, scriptUrl: url)\n        cancellable = client.nodes()\n            .receive(on: RunLoop.main)\n            .sink { result in\n                switch result {\n                case .failure(let error):\n                    print(error)\n                    GenerateScreenshares.exit(withError: error)\n                case .success(let rawNodeString):\n                    let allNodes = rawNodeString\n                        .components(separatedBy: .newlines)\n                        .filter { $0.count > 0 }\n                        .map { str -> (name: String, ip: String) in\n                            let components = str.components(separatedBy: .whitespaces)\n                            return (name: components[0], ip: components[1])\n                        }\n                    for node in allNodes {\n                        let fileContents = screenshare(ip: node.ip)\n                        let fileManager = FileManager.default\n                        let filePath = fileManager.currentDirectoryPath.appending(\"/\\(node.name).vncloc\")\n                        fileManager.createFile(atPath: filePath,\n                                               contents: fileContents.data(using: .utf8),\n                                               attributes: nil)\n                    }\n                    GenerateScreenshares.exit()\n                }\n            }\n    }\n}\n\nfunc screenshare(ip: String) -> String {\n\"\"\"\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n  <dict>\n    <key>URL</key>\n    <string>vnc://\\(ip)</string>\n  </dict>\n</plist>\n\"\"\"\n}\n\nGenerateScreenshares.main()\nRunLoop.main.run() Our JenkinsAPIClient class is inspired by Vadim Bulavin’s article Modern Networking in Swift 5 with URLSession, Combine and Codable . It goes something like this: class JenkinsAPIClient {\n    private let session: URLSession = URLSession.shared\n    private let user: String\n    private let token: String\n    private let scriptUrl: String\n\n    init(user: String, token: String, scriptUrl: String) {\n        self.user = user\n        self.token = token\n        self.scriptUrl = scriptUrl\n    }\n\n    func nodes() -> AnyPublisher<Result<String, NetworkError>, Never> {\n        let url = URL(string: scriptUrl)!\n        let data = \"\\(user):\\(token)\".data(using: .utf8)\n        let auth = data!.base64EncodedString()\n        var request = URLRequest(url: url)\n        request.httpMethod = \"POST\"\n        request.addValue(\"Basic \\(auth)\", forHTTPHeaderField: \"Authorization\")\n\n        return session.dataTaskPublisher(for: request)\n            .mapError { error in\n                NetworkError.invalidRequest(error: error)\n            }\n            .flatMap { data, response -> AnyPublisher<Data, Error> in\n                guard let response = response as? HTTPURLResponse else {\n                    return Fail(error: NetworkError.invalidResponse).eraseToAnyPublisher()\n                }\n                guard 200..<300 ~= response.statusCode else {\n                    return Fail(error: NetworkError.dataLoadingError(statusCode: response.statusCode, data: data)).eraseToAnyPublisher()\n                }\n                return Just(data)\n                    .catch { _ in Empty().eraseToAnyPublisher() }\n                    .eraseToAnyPublisher()\n        }\n        .map { String(data: $0, encoding: .utf8)! }\n        .map { .success($0) }\n        .catch { error -> AnyPublisher<Result<String, NetworkError>, Never> in\n            return Just(.failure(NetworkError.decodingError(error: error)))\n                .catch { _ in Empty().eraseToAnyPublisher() }\n                .eraseToAnyPublisher()\n        }\n        .eraseToAnyPublisher()\n    }\n} We then compile the script and move it to a suitable folder. Running it without parameters will give us a nice help message. Error: Missing expected argument '--user <user>'\n\nOVERVIEW: A Swift command-line tool to generate screenshares\n\nUSAGE: generate-screenshares --user <user> --token <token> --url <url>\n\nOPTIONS:\n  --user <user>           Your username on Jenkins\n  --token <token>         Your token on Jenkins\n  --url <url>             The full URL to your Groovy script\n  -h, --help              Show help information. When we run it with proper credentials we get a folder containing the generated shortcuts. $ ls\nBabou.vncloc          Krieger.vncloc        Pam.vncloc            Sterling.vncloc\nCyril.vncloc          Lana.vncloc           Ray.vncloc            Woodhouse.vncloc\nKatya.vncloc          Malory.vncloc         Ron.vncloc            generate_screenshares Although the script we wrote is working great, we still have to manually share the generated files with our fellow engineers. Let’s try a different approach. A New Hope The remaining problem we need to solve is the disconnect between getting the IP addresses of the nodes and connecting to them. To solve that, let’s bring these two functions together and create a macOS app that both fetches the latest node data and triggers the connection to a node of our choice. With the release of SwiftUI, Apple made that very easy for an iOS developer. We’ll skip parts of the implementation for brevity. The app is straightforward – get the user credentials, call the API, show the results in a nice way, job done. Let’s see the final result. We are using the same JenkinsAPIClient we developed for the script earlier. We introduce a NodeManager class which our SwiftUI view will observe. class NodeManager: ObservableObject {\n    @Published var nodes: [[Node]] = []\n    @Published var error: NetworkError?\n    private var cancellable: AnyCancellable?\n    var client = JenkinsAPIClient()\n\n    func fetchNodes() {\n        nodes = []\n        error = nil\n        cancellable?.cancel()\n        cancellable = client.nodes()\n            .receive(on: RunLoop.main)\n            .sink { result in\n                switch result {\n                case .failure(let error):\n                    self.error = error\n                case .success(let rawNodeString):\n                    let allNodes = rawNodeString\n                        .components(separatedBy: .newlines)\n                        .filter { $0.count > 0 }\n                        .map { str -> Node in\n                            let components = str.components(separatedBy: .whitespaces)\n                            return Node(name: components[0], ip: components[1])\n                        }\n                    self.nodes = allNodes.chunked(into: 4)\n                    Preferences.lastUpdated = Date()\n                }\n            }\n    }\n} We also created a Preferences screen where the user can update their credentials. The inspiration for that came from the brilliant series SwiftUI for Mac by TrozWare . The layout code is straightforward Swift UI stuff. We are listening for three events: a change in preferences, a click on the reload button, and the onAppear trigger. When any of them occur we are updating the view. We also have a timer which helps us visualise the time since we last updated the list. Some relevant code follows: @ObservedObject var nodeManager: NodeManager = NodeManager()\n    @State private var relativeDate = \"Just now\"\n    private let timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()\n    private let preferencesChanged = NotificationCenter.default\n        .publisher(for: .preferencesChanged)\n        .receive(on: RunLoop.main)\n\n...\n\n        .onAppear {\n            self.nodeManager.fetchNodes()\n        }\n        .onReceive(timer) { _ in\n            self.relativeDate = Self.relativeFormatter.localizedString(for: Preferences.lastUpdated, relativeTo: Date())\n        }\n        .onReceive(preferencesChanged) { _ in\n            self.nodeManager.fetchNodes()\n        } Considerations: Use Keychain to store sensitive data (e.g. the token) If the polling is time consuming, opt for a reload button instead. Don’t forget to display the last updated time, so the user can decide whether the data is stale Shoot for simplicity – list the nodes and link them using vnc:// links so that your nodes are one tap away Handle the unhappy paths (e.g. bad credentials, node can’t be reached, no internet) In summary We created a macOS app which serves as a one-stop shop for connecting to our CI nodes. Our engineers can simply run the app, tap the node they want to log in to, and complete their task. Along the way we also learned how to create a Swift script, which might come in handy later. We used the latest technologies such as Swift UI, Combine, and ArgumentParser. This will hopefully encourage more people to adopt these technologies and improve their tooling. References Header image by Joey Banks on Unsplash Straightforward, type-safe argument parsing for Swift Modern Networking in Swift 5 with URLSession, Combine and Codable SwiftUI for Mac – Part 1 :: TrozWare Thank you for reading. You can reach out on Twitter at @gimly .", "date": "2021-01-11"},
{"website": "JustTakeAway", "title": "Big app little app", "author": ["Written by Huw Jones"], "link": "https://tech.justeattakeaway.com/2020/11/20/big-app-little-app/", "abstract": "Introduction Over the last few years we have changed the structure of our Android consumer application from a large monolithic application to a modular one. This has numerous well-documented benefits such as being easier to maintain and scale, and improved build times. One of the least documented benefits of modularity is the ability to create sample applications that just exercise a small piece of the main application functionality. This post will describe some of the benefits of ‘sample apps’ and also how you might set them up to help with your everyday development. Benefits of a sample application If the app that you work on everyday has a large codebase with a broad range of functionality, you will generally find yourself focusing on small sections of the app throughout your development time. These sections can be found at various points of the user journey. When manually testing, it can be quite painful to build, run, and then navigate to the area you want to test each time you make a change. A modular application based on features makes it very easy to set up a sample app as part of your main project that launches straight into the area of functionality you are working on. This can save a lot of time when building, running, and testing changes compared to the main application. There are other benefits to sample apps. The sample apps generally have a launch activity that has numerous fields so that you can launch your feature with a specific configuration. This can be as simple as an ID used for requesting some data from a server. For more complex features, each small piece of functionality can be enabled or disabled via a switch or field from the sample app launch screen. You might be wondering how we wire the configuration of our sample app so that it runs the real feature code with specific data. I will describe that in the next section. Sample application setup The high level structure of a sample application is typically very simple, normally containing just a single launch activity. The activity often has a couple of responsibilities such as allowing the user to configure the data that is going to be used in the feature module and for us, it creates a Dagger component used for the DI in the module. This gives developers a chance to configure the DI how they see fit when running their feature module code. For example, developers can configure the DI to temporarily divert crashdumps or log output to a temporary, non-prod destination while the module is under test. The sample app modules would ideally just have minimal dependencies on other modules in your project. They would obviously require a dependency on the feature module they are running and also any additional modules specifically implemented to improve the usefulness of the sample app. A sample app launch screen doesn’t have to be pretty, but it does have to be functional. It would generally navigate directly into the feature module by selecting one of possible multiple launch actions. It might look something like this: Enhancing your sample app A few years ago, we made a concerted effort to further improve our UI tests. Looking to increase the coverage and also the reliability, we decided to switch the tests over to use mock data rather than hit the real API. There are a number of different ways you can do this. After some discussion, we opted to use the OkHttp Mock Web Server and json files to model the data at the ‘lowest’ point. In order to easily increase our coverage, the json response files were ‘tokenised’ so that we could replace parts of the response at runtime during a test run. We soon realised that as well as running our sample apps against real data, if we extracted the Mock Web Server functionality out into its own ‘fake-server’ module we could reuse it when running them in ‘mocked mode’ simply by adding it as a dependency to the sample application. This is useful for a number of reasons. Firstly, you can launch your sample application without requiring any network connectivity. Secondly it means you can configure your sample app to launch in a number of different ways simply by setting values in the mocked json responses. This is helpful if you want to test the UI in different states. Here’s a few examples of things you might want to configure: Add delays to responses to check progress UI Add long or short description fields to check for wrapping etc Configure HTTP errors to check error handling and UI You might have different entry points into your module providing different data. For example, you might allow deep links into your feature module but also have a more normal navigation route from another screen in your app. We navigate to each module via a Dispatcher object which might provide different data to the module depending on where we are navigating from. If you use feature flags, you might want to control switching them on and off from the sample app so you can quickly see their impact Simply return data in ‘optional’ json properties so that a particular feature fires Another really valuable usage is if you’re developing a new feature module from scratch. If a contract with the server has been defined for the new feature but the API not implemented, the mobile developers can start implementing the feature whilst the backend developers are completing the code to provide the data. Meaning both sides can work in parallel with, ideally, just a small amount of integration work at the end of the feature development cycle. Conclusion Sample apps offer a number of benefits when developing large Android apps: They speed up development time by reducing the build time when implementing and manually testing new features If a mocking framework is incorporated into the sample app, both client and backend developers can work in parallel on new features The sample app can be made highly configurable in order to test UI scenarios rapidly without needing to find the correct backend data and navigate through the entire application Internal deployment of sample apps – so product, UX, or UI can review changes quickly and easily Whilst there are a few challenges to creating sample apps within your main app project, such as having more build files to maintain, or that it can take longer to load the project in Android Studio. (This can be mitigated somewhat by temporarily unloading the sample apps you are not interested in). Overall I find sample apps provide a really great way to develop a large application efficiently. Resources OkHttp Mock Web Server – https://square.github.io/okhttp/4.x/mockwebserver/okhttp3.mockwebserver/-mock-web-server/ Android Modular Architecture – https://www.youtube.com/watch?v=PZBg5DIzNww", "date": "2020-11-20"},
{"website": "JustTakeAway", "title": "Managing Costs Efficiently With BigQuery", "author": ["Written by Martin Grayson"], "link": "https://tech.justeattakeaway.com/2020/09/25/managing-costs-efficiently-with-bigquery/", "abstract": "At Just Eat, we’ve been big users of the Google Cloud Platform (GCP) for a number of years. We ingest both realtime and batch data , maintaining an in-house data transformation platform that allows any business user to self-serve data using Google BigQuery. Our in-house Airflow-based transformation platform, named Optimus, has seen massive growth over the past two years. We have over 200 users making around 500 enhancements per week. We’re now at the point of having 1500 data pipelines running in our production environment. The insights that we generate in BigQuery (BQ) go on to power a number of functions from personalised food discovery services to forensic understanding (of customers, couriers and restaurants) and power all data driven communications. Needless to say, there is a lot of data processing needed to power these functions in various markets around the globe. In this post we’ll discuss how we efficiently manage the costs that arise through processing this data with BigQuery, covering the various cost models and how we make use of them at Just Eat. This will set the scene for a follow-up post containing some practical engineering tips to efficiently write code tailored for BigQuery. BigQuery Pricing Models When using BQ, you are billed for how much data you store and how much processing you carry out on that data. We’re going to focus on optimising query cost by exploring the different pricing models that Google support. BigQuery pricing models are based on the concept of slots. A slot is a unit of computational power. BigQuery can use one or more slots to process your request and the number of slots being used can change over time as the stages of a query progress. The best way of looking at how much energy (cost) your query has used, is to measure slot-hours. On to pricing models – On-demand – this is BQ’s default cost model. The more data you process, the more you pay. Think of this like your home’s electricity consumption. The power consumed by a device (slots) over a given period of time translates into the number of kilowatt-hours (slot-hours) that you use over the month and so the size of your bill. Flat-rate – this is Google’s way of giving its customers a predictable bill by charging for a fixed number of slots, all the time. This is like an internet contract, where you pay for a fixed download speed (e.g. 50Mb), regardless of whether you need it all the time. Further information on the above can be found on a blog post published by Google. Where The Journey Began Now that we’ve discussed the Google nomenclature, let’s get down to the practical aspects of the pricing models and talk about the journey that Just Eat has been on. We began our data journey with Redshift, it was a great fit due to our existing AWS stack and various integrations. The data team would ingest data, carry out various ETL processes and administer the cluster. But as the business’s need for data grew, increasingly we saw that Data Engineers were being asked to wear many hats, providing downstream teams with all the data they needed to unlock insight. The realisation came that we were asking Engineers to worry too much about the underlying architecture, acting as DBAs. They had become a bottleneck to all the data alchemy that could be happening downstream. While both Redshift and BigQuery have their own strengths, we concluded that in order to truly democratise our data with a limited number of engineers, it would make more sense for us to transition to BigQuery. This would enable us to build tools that would allow users to self-serve and unlock insights without an Engineer blocking their progress. A key part to entering this Bronze Age of data at Just Eat was the introduction of BigQuery and the construction of our Data Lake. To support this goal, we began the development of our GCS (Google Cloud Storage) and BQ backed Data Lake. We started gradually adding in more data and migrating processes until we officially deprecated Redshift in Q1 of 2019. During this early phase of the Data Lake development, on-demand pricing was critical in allowing us to scale to meet the demands of our users without having to expel too much effort making (often inaccurate) usage estimates. Data Explosion On-demand pricing worked very well for the team when the Data Lake was in its infancy. Since going all-in on BigQuery, we utilised on-demand pricing for around a year before reaching critical mass, realising the rate of expenditure was not sustainable. In early 2020, we began to explore flat-rate billing in order to provide us with a more predictable and cost efficient process for the future. When exploring this, our first realisation was that in order to take advantage of slot reservations and segregate user workload we’d need to shuffle our project structure. Prior to this, users were free to switch projects and query data in the Data Warehouse having activated any project. This presents a problem as billing and slots are controlled at project level meaning users could take resources from business critical processes and potentially hop between projects to pinch additional resources. This would mean that a bad neighbour could hog resources and make cost analysis very difficult. Our solution to this was to create a “data-users” project. This project’s purpose is to logically separate user queries from ETL and other programmatic actors that need to run reliably in order to support business processes. This project enabled us to map user queries to a set reservation using an assignment and user permissions to query the DWH were restricted to this new project. Using this meant that cost analysis and billing became much simpler and users could not consume slots required for critical automated processes. Within this initial phase, we settled on this new project structure and three associated reservations; the lion’s share of slots were allocated to critical automated pipelines. Playing The Google Slot Machine This shift to flat rate billing entered production in early 2020, we revisited the concept towards the middle of the year. Our initial goal was to ensure that we could drive a maintainable level of spend for the Data Lake. Flat Rate helped us ensure this but also meant that we regressed to a Stone Age (pre 2019) situation of having to worry about workloads and assist users with understanding how to tune their queries. We began to see performance issues with critical workloads and realised the journey wasn’t over. Tackling this problem required two techniques – education and understanding from our users, and further thinking around how we could flex our capacity. The former is an ongoing activity, but has primarily been centred around helping people understand all the new concepts introduced following migrating to flat-rate billing. Our next post will discuss some of the advice and tuning activities that have been carried out internally. The second technique was to make use of a new feature within BigQuery to dynamically grow our capacity to meet demand. In late February, 2020, Google launched Flex Slots. This feature allowed us to purchase slots for a short, pre-defined duration of time, providing a more flexible approach to growing capacity. Flex slots allowed us to scale the platform during peak times to reduce the throttling effect of having a number of slots reserved for an activity. In Q3 of 2020, we introduced some automation (aptly named Flexy) to help us purchase additional flex slots to meet demand at known times of heavy usage. Purchasing flex slots is working fairly well for our application, but we believe we can reduce a significant amount of resource usage by further understanding jobs that would be more efficiently run using on-demand pricing, rather than burning reserved slots. The next stage in our journey is to identify workloads that are cheaper to execute using on-demand. We will look at changing our in-house tooling to dynamically schedule queries using the most efficient billing method by switching projects on the fly. Monitoring It All All well-shaped objectives should be measurable. Much of the work we’ve described wouldn’t be possible without capturing data points and setting targets to ensure we’re heading in the right direction. Having visibility of query performance and cost at a higher level was critical in giving us the ability to understand where to apply the Pareto principle. In order to identify resource intensive pipelines we introduced a standard labelling scheme within our ETL tool. Labels were mapped to the API call when issuing queries to BQ and these dimensions were then visible within our cost reporting. Subsequently, we were able to identify expensive jobs, slice costs by owner, department or feature. This helps the team be clear on objectives and understand bottlenecks, reducing the noisy neighbour effect by ensuring usage metrics are available for all. Similarly operational metrics around slot usage and capacity were surfaced in Grafana, allowing us to understand real-time usage and monitor any over or under provisioning of the system. The Journey Never Ends In this post we talked about the various milestones and discoveries that we’ve made during our time adopting BigQuery. Balancing cost and usability has meant that we’ve had to adapt the solution as we’ve grown. In the next post, we’ll look at some SQL optimisations and design principles that we’ve applied to help us keep costs in check and ensure our data lake operates efficiently.", "date": "2020-09-25"},
{"website": "JustTakeAway", "title": "Android access ability", "author": ["Written by Andy Barber"], "link": "https://tech.justeattakeaway.com/2020/09/08/android-access-ability/", "abstract": "In theory every app development process should allow the development team to create features that are accessible to all users. When designers and developers create features, they usually first think about the user experience that is provided and how easy and intuitively that feature can be used. This usually accounts for a clear visual appearance and sticking to well known mobile interaction patterns e.g. icons, lists, pinch to zoom, swipe etc. One thing that is often neglected is how the user interacts with the mobile device.  This assumption is a natural one as the majority of people interact first visually and then by touch, but as we all know this is not always the case. There are various reasons why a consumer may not be able to touch or look at the screen when interacting with the app, and we therefore need to keep in mind that our app will be accessed in many different ways. By taking this into consideration when creating features,  we  allow a much broader audience to access our app by introducing a design that works for  all of our customers regardless of their accessibility requirements. When designing the Just Eat Takeaway.com app we took these accessibility considerations into account in both design and development, but there’s always some room for improvement. Here we’ll outline some of the improvements we made on one part of the app (in particular our menu module) and the tools we used to help us on Android. Accessibility scanner app We already made some accessibility considerations during the design phase, looking at touch areas, colour and fonts, but also during the development, adding in content descriptions which are used by the Android screen reader (Talkback). The starting point for the improvements began with an app called Accessibility Scanner, that is free to download on the Google Play Store . This is a great app provided by Google to allow app developers (or anyone for that matter) to review any app and assess how accessible it is according to Google’s guidelines. The app works by scanning the selected apps screen layout while you navigate through it and then produces a report at the end highlighting ways that you can improve accessibility with specific elements, like in the screenshots below.  Tapping on ‘Learn more’ in the suggestions also links you to some great information on improving accessibility for that view. Based on the feedback in accessibility scanner we were able to improve the following areas: Add missing content description (this is the text read out by the screen reader for each element on the screen e.g. buttons & images) Increase the touch target areas of buttons to meet minimums Change the app colours to meet requirements (contrasts etc.) Fix focus orders between elements Content descriptions were added to views that were missing them by using the following view property. android:contentDescription=\"@string/restaurant_logo_content_description\" Or by adding a content description programmatically in cases where we need to dynamically build the string needed e.g. 3 out of 4 review stars, like the example below. ratingBar.rating = String.format(resources.getString(R.string.rating_bar_average_content_description), ratingAverage) The touch targets on some buttons & views needed to be increased to match the 48dp minimum width/height by adding appropriate padding or margin (see the material design docs for more detail on this) Changing the app colours was passed back to the app designers as further consideration needed to be made between changes in colours and branding. If you need to do this yourself there is a great website provided by Google that helps compare colours and assess whether they match accessibility constraints here . Talkback The second stage of the accessibility improvements involved turning on the built in screen reader in Android, called TalkBack, and testing how each screen is read out.  (This can be found in the settings under, Accessibility > Screen Readers > TalkBack) Navigating an app with the Talkback interface is a great way to empathise with people that can’t see the app and how they actually perceive the app experience. Using Talkback Before switching on Talkback it’s a good idea to know how to switch it off.  If you’ve never navigated with it on, it can be challenging to get back to the setting to turn it off.  I found this out the hard way, but eventually realised that there is a nice hardware shortcut. Before you switch on Talkback in the device settings, toggle the option “Talkback shortcut”  this will allow you to switch Talkback on and off by holding down both volume up and down for 3 seconds. The next hurdle is navigating your app with Talkback on.  The easiest way to do this is to swipe the screen left to right to move down the screen between elements (or right to left to go up).  Double tapping the screen will then select a button or link. Reviewing Talkback usage Once we mastered using Talkback we used it to check that navigation between items made sense and the correct information was being read out to the user. It also allowed us to improve the overall navigation order of focus between elements on the screen. For example, if the initial focus item on a screen was the back button, we could use something like the properties below to change this focus to another field. android:focusable=\"true\"\nandroid:focusableInTouchMode=\"true\" We could also control the order of the focus between views on the screen with these view properties. android:nextFocusUp=\"@id/name_text\"\nandroid:nextFocusLeft=\"@id/description_text\"\n...etc We finished by double checking that improvements made to content descriptions after using the Accessibility Scanner app were accurate, when read out by the screen reader. Further work These two tools are a great start to improve app accessibility, but there is always room for improvement. The main point that we came up with for future improvements was to integrate the use of accessibility tools into the development process further and use the Accessibility Scanner & Talkback apps with each feature we develop.  In a recent canary version of Android Studio (4.2 Canary 8) they have also introduced the Accessibility Scanner features directly into the layout editor, which you can use to further integrate these checks into your process here . Another really interesting area for our next stage of improvements is to review how our app can be navigated by accessibility Switches.  These come in various different forms but essentially allow users to navigate between elements on the screen by pressing a switch (usually used by people with limited or no movement). The focus order of the elements on the screen contributes to how well this works and also if your app relies on gesture navigation like swiping, this can cause problems if there isn’t a button to perform the equivalent action.  There is a great article on this by Rebecca Franks on this subject here . The Android platform is always evolving and accessibility is improving with it, so we aim to keep up to date with these improvements and also keep our app accessible to all. There is a really interesting improvement in Android 11 around voice commands in this tweet,  that shows part of this progression. The process of improving an app’s accessibility expands your view on the potential audience of users that want to use your app and allows your app to appeal to everyone.  I hope that our further investigation into improving accessibility will inspire others to do the same. Resources If you are interested in improving your own app then there are plenty of resources available out there, but here are just a few that we came across. Android developer docs on app accessibility testing Talkback Navigation Material colour accessibility check tool Android backstage podcast Switch accessibility in Android app Google video playlist New speech features in Android 11 – Engadget video", "date": "2020-09-08"},
{"website": "JustTakeAway", "title": "Hacking Remotely", "author": ["Written by Ryan Cormack"], "link": "https://tech.justeattakeaway.com/2020/08/12/hacking-remotely/", "abstract": "At Just Eat Takeaway.com we love Hackathons. They’ve been a key part of our culture for as long as I’ve worked here and for many years before that. I’d like to discuss how we make being a part of them so great at work, the benefits that we get from them and how we’ve adapted to working from home. Hackathons in general provide a fantastic opportunity for employees to bring their creativity to the table and explore new ideas. Over three days, newly formed teams work hard to turn their ideas into a demo and over the past few years, we’ve even had some great ideas make it into production. Hackathons also give us the freedom to explore and use a wider range of tech that we may not necessarily come across as part of our everyday jobs or dive into areas of the platform we don’t usually work on.  With over 900 components coupled with access to full-featured AWS accounts and a rich dataset to experiment on, there’s always something to learn. While we were all still super excited about our latest hackathon in May, it did bring some additional challenges. With the outbreak of coronavirus,we’ve adapted fully to  working from home over the past few months, which meant for the first time, Hackathon 28 – became a fully virtual event. Additionally, we all know that these are some of the most challenging times our society has faced so we also decided to focus on a theme that was all about helping others: Help the Planet, Help the Community and Helping Ourselves. I worked alongside two other engineers, Zac and Fahad, as part of this hackathon. We decided to see how we could reduce the number of paper receipts for both our Restaurant Partners and customers to help reduce our impact on the environment. This gave us a fantastic opportunity to learn about different parts of the business that we wouldn’t usually come across, including how our end-to-end order flow works – from the moment the customer places an order right to when this order has been delivered to the door. And with the added challenges of taking part in a first-time virtual hackathon, I want to share  some of my key learnings that I picked up along the way: Different teams do things differently – At Just Eat we don’t have a top down approach for building API’s, writing the front end or sharing packages. This allows teams to do things in a way that works best for them. So this was a great way to share learnings across different functions. Communication is key – Working remotely can be a challenge when working with new people. I think we did a pretty good job of it over the hackathon. Usually, we’re used to communicating freely with those sitting around us at Hackathons, but it’s not the same when everyone is at home. We made strong use of Slack, and jumped on lots of hangouts to ensure we were communicating all the time. Drawing diagrams – Drawing diagrams is hard when you don’t have a whiteboard. I’ve been getting more and more used to using draw.io and there are a variety of similar other tools that people have been using, from Miro to Jamboards . – having things like a writing tablet makes Jamboards much easier to use. Cross team collaboration – I’ve noticed that as more people rely on Slack or other communication tools, we’ve got better at chatting in threads, replying to people and generally being available to answer questions. Working with various teams on our hack was a really nice experience over Slack, with people collaborating well and generally being all round supportive of other teams making Pull Requests into their features. This Hackathon showed how well we’re able to work remotely as a company and how events like hackathons can still work really well when held virtually. We had great brainstorming ideas across teams and countries, designed and implemented totally new architectures and presented these ideas to hundreds of people. We run 3-4 Hackathons per year, and the next one may still be held virtually, so I hope the lessons learned here on remote collaboration will help those taking part in  future online Hackathons!", "date": "2020-08-12"},
{"website": "JustTakeAway", "title": "Lessons learned implementing Sign in with Apple on iOS", "author": ["Written by Andrea Antonioni"], "link": "https://tech.justeattakeaway.com/2020/05/27/lessons-learned-implementing-sign-in-with-apple-on-ios/", "abstract": "At WWDC 2019, Sign in with Apple was introduced as a new way for apps to authenticate users with their Apple ID in a few steps. Following the App Store Guidelines , this new feature is a requirement for all apps that support social logins (with some exceptions). At Just Eat we didn’t see Sign in with Apple as a required feature we needed to implement. Instead, we took it as an opportunity to provide a more secure and easy way to log in for our customers around the world. For my team ( Identity&Protection ) it was the first time we implemented an Apple feature which was brand new for everyone. The documentation was a good starting point but we faced some edge cases that may prevent the user to log in if not handled properly. Moreover, we refactored our code on iOS in order to integrate Sign in with Apple with the existing social providers and the native login. This is what we learned along the journey about implementing Sign in with Apple . The initial deep dive Even if Sign in with Apple might appear easy to understand at first, it is important to read carefully the documentation and get a clear idea of how both the authentication flow and the integration between the app, the backend and Apple work. Here are some good starting points: “Introducing Sign In with Apple” session from WWDC 2019 “What’s New in Authentication” session from WWDC 2019 An example code with a Sign in with Apple implementation by Apple Another good source of information that helped us to understand some edge cases was the Apple Developer Forum . There you can find some really good conversations with Apple engineers explaining some edge cases of Sign in with Apple that aren’t well documented anywhere else. One of the features of Sign in with Apple is the ability for the user to not share their real email. In this case, Apple provides a proxy email to the app ( xxxxx@privaterely.appleid.com ) that can be used to get in touch with the user. Users have the same proxy email in all the apps that belong to the same Development Team. Together with the email, Apple provides a user identifier which is unique for all apps written by a single development team. Apple suggests using the user property as a unique identifier in your server rather than the email, as there are some cases where the app only gets the identifier back in the response that the server can use to match to an existing account. Certificates and App capabilities The setup of certificates and app capabilities always sounds difficult, but that is the first necessary step in the Sign in with Apple implementation journey. We added the Sign in with Apple capability using Xcode and then got the private key and the Key ID from the Certificates, Identifiers & Profiles section of our account in the Apple Developer Portal . This is what is needed by the backend to verify the user identity with the Apple server. That was pretty easy, but we know that things are often more complicated than they might appear. In our Xcode project we have 3 different build configurations: Debug , Internal and App Store . Each of these configurations has different bundle ID and App ID in the Apple Developer Portal. As the name suggests, we use Internal to distribute the app internally for testing purposes. This configuration is signed with an Enterprise certificate and – similarly to Apple Pay – Sign in with Apple is not available in the Enterprise License, so Sign in with Apple cannot work for internal builds. If you have a similar configuration, remember to remove the Sign in with Apple capability from your Internal configuration otherwise you’ll get an error when trying to distribute the app internally. Another edge case to consider is that the App ID is used as the client_id parameter from the backend through the interactions with the Apple server. Debug and App Store have 2 different App IDs, so each of them has its own private key and Key ID: basically, they’re managed as 2 different apps. You can avoid this by grouping Debug with App Store marking the last one as Primary App ID. For more details on this procedure, check this help page from Apple . Integrating Sign in with Apple with Facebook and Google SDKs At Just Eat we use a Modular Architecture on iOS and the Account module is responsible for managing login and signup in the app. We already support Facebook and Google in some countries using a SocialLoginService protocol: enum IdentityProvider {\n    case facebook\n    case google\n}\n\nenum SocialLoginResult {\n    case success(String)\n    case failure(Error)\n    case cancelled\n}\n\nprotocol SocialLoginService {\n    var identityProvider: IdentityProvider { get }\n\n    func login(from viewController: UIViewController, _ handler: @escaping (SocialLoginResult) -> Void)\n}\n\nclass FacebookService: SocialLoginService {\n    var identityProvider: IdentityProvider { return .facebook }\n\n    func login(from viewController: UIViewController, _ handler: @escaping (SocialLoginResult) -> Void) {\n        // Facebook SDK implementation\n    }\n}\n\nclass GoogleService: SocialLoginService {\n    var identityProvider: IdentityProvider { return .google }\n\n    func login(from viewController: UIViewController, _ handler: @escaping (SocialLoginResult) -> Void) {\n        // Google SDK implementation\n    }\n} Both Facebook and Google SDKs provide an access token, which the app sends to our backend in order to retrieve user information (email and name) and to create or log in into an existing account. let socialLoginServices: [SocialLoginService] = [FacebookService(), GoogleService()]\n\nfunc loginViewController(_ viewController: UIViewController, didRequestSocialLoginWith provider: IdentityProvider) {\n    guard let socialLoginService = socialLoginServices.first(where: { $0.identityProvider == provider }) else { return }\n    socialLoginService.login(from: viewController) { [weak self] result in\n        switch result {\n        case .success(let accessToken):\n            print(\"Social SDK accessToken: \\(accessToken)\")\n            self?.login(with: provider, token: accessToken)\n        case .failure, .cancelled:\n            print(\"Sign in flow interrupted\")\n        }\n    }\n}\n\nfunc login(with provider: IdentityProvider, token: String){\n    // Login request to our backend\n} By using the SocialLoginService protocol, we created the SignInWithAppleService class and in IdentityProvider added the case apple only available for iOS 13. enum IdentityProvider {\n    case facebook\n    case google\n\n    @available(iOS 13, *)\n    case apple\n}\n\n@available(iOS 13, *)\nclass SignInWithAppleService: SocialLoginService, ASAuthorizationControllerPresentationContextProviding {\n    private weak var presentingViewController: UIViewController?\n    private var handler: ((SocialLoginResult) -> Void)?\n\n    var identityProvider: IdentityProvider { return .apple }\n\n    func login(from viewController: UIViewController, _ handler: @escaping (SocialLoginResult) -> Void) {\n        // 1\n        self.presentingViewController = viewController\n        self.handler = handler\n\n        // 2\n        let appleIDRequest = ASAuthorizationAppleIDProvider().createRequest()\n        appleIDRequest.requestedScopes = [.email, .fullName]\n\n        let authorizationController = ASAuthorizationController(authorizationRequests: [appleIDRequest])\n        authorizationController.delegate = self\n        authorizationController.presentationContextProvider = self\n        authorizationController.performRequests()\n    }\n\n    private func resetStatus() {\n        presentingViewController = nil\n        handler = nil\n    }\n\n    // MARK: - ASAuthorizationControllerPresentationContextProviding\n\n    func presentationAnchor(for controller: ASAuthorizationController) -> ASPresentationAnchor {\n        if let presentingViewController = presentingViewController,\n            let window = presentingViewController.view.window {\n            return window\n        } else {\n            return UIWindow()\n        }\n    }\n\n    // MARK: - ASAuthorizationControllerDelegate\n\n    func authorizationController(controller: ASAuthorizationController, didCompleteWithAuthorization authorization: ASAuthorization) {\n        defer { resetStatus() }\n\n        guard let appleIDCredential = authorization.credential as? ASAuthorizationAppleIDCredential,\n            let authenticationCode = appleIDCredential.authorizationCode,\n            let authCode = String(data: authenticationCode, encoding: .utf8) else {\n                let error = AccountErrorBuilder.error(forCode: AccountErrorCode.loginFailure.rawValue)\n                handler?(.failure(error))\n                return\n        }\n\n        handler?(.success(authCode))\n    }\n\n    func authorizationController(controller: ASAuthorizationController, didCompleteWithError error: Error) {\n        defer { resetStatus() }\n        print(error)\n    }\n} We need to save both viewController and handler in order to use them in ASAuthorizationControllerPresentationContextProviding and ASAuthorizationControllerDelegate methods. In this case defer comes handy so we can call resetStatus to remove references before leaving the context. We create a request using ASAuthorizationAppleIDProvider . In our case, we set email and fullName in scopes as they’re required by the backend to create an account. After that, we create ASAuthorizationController and perform the request to show the sheet to the user. ASAuthorizationControllerDelegate contains one method for the failure and one for the success. Sign in with Apple doesn’t return an access token like Facebook and Google. Instead, it returns an ASAuthorizationAppleIDCredential object which contains some information to send to the backend for the authorization flow. To support this difference, we had to refactor our code: // 1\npublic enum SocialSignInResult {\n    public enum AuthInfo {\n        case accessToken(String)\n\n        @available(iOS 13.0, *)\n        case userInfo(ASAuthorizationAppleIDCredential)\n    }\n\n    case success(AuthInfo)\n    case cancelled\n    case failure(Error)\n}\n\n@available(iOS 13, *)\nextension SignInWithAppleService: ASAuthorizationControllerDelegate {\n    func authorizationController(controller: ASAuthorizationController, didCompleteWithAuthorization authorization: ASAuthorization) {\n        defer { resetStatus() }\n\n        // 2\n        switch authorization.credential {\n        case let appleIDCredential as ASAuthorizationAppleIDCredential:\n            let authInfo = SocialSignInResult.AuthInfo.userInfo(appleIDCredential)\n            handler?(.success(authInfo))\n        default:\n            let error = AccountErrorBuilder.error(forCode: AccountErrorCode.loginFailure.rawValue)\n            handler?(.failure(error))\n        }\n    }\n\n    func authorizationController(controller: ASAuthorizationController, didCompleteWithError error: Error) {\n        defer { resetStatus() }\n        print(error)\n    }\n}\n\nfunc loginViewController(_ viewController: UIViewController, didRequestSocialLoginWith provider: IdentityProvider) {\n    guard let socialLoginService = socialLoginServices.first(where: { $0.identityProvider == provider }) else { return }\n    socialLoginService.login(from: viewController) { [weak self] result in\n        switch result {\n        case .success(let authInfo):\n            self.handleSocialLoginServiceSuccess(with: provider, authInfo: authInfo)\n        case .failure, .cancelled:\n            print(\"Sign in flow interrupted\")\n        }\n    }\n}\n\n// 3\nprivate func handleSocialLoginServiceSuccess(with provider: IdentityProvider, authInfo: SocialSignInResult.AuthInfo) {\n    switch authInfo {\n    case .accessToken(let token):\n        login(with: provider, token: token)\n    case .userInfo(let appleIDCredentials):\n        guard let authorizationCode = appleIDCredentials.authorizationCode,\n            let authCode = String(data: authorizationCode, encoding: .utf8) else {\n                return\n        }\n        login(with: provider, token: authCode)\n    }\n} In the success case we now return a AuthInfo enum which supports both an access token and the user information represented by a ASAuthorizationAppleIDCredential object. We need to cast the credential property because it changes based on the request we make. In this case we put only ASAuthorizationAppleIDRequest in the request array; in this way,  we will only receive a ASAuthorizationAppleIDCredential as response. If the cast fails we have a AccountErrorBuilder class we use internally to build errors (you can read more about this approach here ). There’s a separate method that handles the AuthInfo we get from a success response. ASAuthorizationAppleIDCredential contains an authenticationCode which is different from an access token you get from Facebook and Google. In this case it’s our backend that based on the social handles the param we send in a different way. Email and Full Name Sign in with Apple comes out with 2 different sheets that are presented in different scenarios: the first is shown to the user during the initial request and the second is shown for a confirmation request. This logic is handled by iOS, but there is a difference we need to consider in the implementation. In fact, user information is shared with the app only in the initial request . Following requests will only contain the user identifier. There are 2 important things to point out here: The backend is able to retrieve email using the authenticationCode (this was not 100% true for us, in fact, we experienced some issues retrieving the email in production and we opened a radar. At the moment we treat email in the same way we do for fullName ) If the full name is required to create an account in your service (like our case), what would happen if you get an error from the API during the initial flow? To solve these issues, we store both email and fullName in the keychain. In this way, we can use them the second time the user will try again. As soon as the app receives a success response from the backend and we are confident an account has been created, we can clean up the keychain. private func handleSocialLoginServiceSuccess(with provider: IdentityProvider, authInfo: SocialSignInResult.AuthInfo) {\n    switch authInfo {\n    case .accessToken(let token):\n        login(with: provider, token: token)\n    case .userInfo(let appleIDCredentials):\n        guard let authorizationCode = appleIDCredentials.authorizationCode,\n            let authCode = String(data: authorizationCode, encoding: .utf8) else {\n                return\n        }\n\n        if let email = appleIDCredentials.email, let fullName = appleIDCredentials.fullName {\n            keychain.signInWithAppleEmail = email\n            keychain.signInWithAppleFullName = fullName\n        }\n\n        login(with: provider, token: authCode) { [weak self] success in\n            guard success else { return }\n            self?.keychain.signInWithAppleFullName = nil\n            self?.keychain.signInWithAppleEmail = nil\n        }\n    }\n} Support for native credentials Sign in with Apple comes with a set of features, one of these is the support for native credentials. It is possible to use the same sheet to search for credentials in the iCloud Keychain and use them to log in the user in their existing account. To support it, we need to change the way we build the request: public enum SocialLoginRequest {\n    case checkExisting\n    case create\n}\n\npublic protocol SocialLoginService {\n    var identityProvider: IdentityProvider { get }\n\n    // 1\n    func login(from viewController: UIViewController, request: SocialLoginRequest, _ handler: @escaping (SocialSignInResult) -> Void)\n}\n\npublic struct Credentials {\n    public let email: String\n    public let password: String\n}\n\n@available(iOS 13, *)\nclass SignInWithAppleService: SocialLoginService, ASAuthorizationControllerPresentationContextProviding {\n    var identityProvider: IdentityProvider { return .apple }\n\n    // ...\n\n    func login(from viewController: UIViewController, request: SocialLoginRequest, _ handler: @escaping (SocialLoginResult) -> Void) {\n        // ...\n\n        let appleIDRequest = ASAuthorizationAppleIDProvider().createRequest()\n        appleIDRequest.requestedScopes = [.email, .fullName]\n\n        // 2\n        var authorizationRequests: [ASAuthorizationRequest] = [appleIDRequest]\n\n        if case .checkExisting = request {\n            authorizationRequests.append(ASAuthorizationPasswordProvider().createRequest())\n        }\n\n        let authorizationController = ASAuthorizationController(authorizationRequests: authorizationRequests)\n        authorizationController.delegate = self\n        authorizationController.presentationContextProvider = self\n        authorizationController.performRequests()\n    }\n}\n\n@available(iOS 13, *)\nextension SignInWithAppleService: ASAuthorizationControllerDelegate {\n    func authorizationController(controller: ASAuthorizationController, didCompleteWithAuthorization authorization: ASAuthorization) {\n        // ...\n\n        switch authorization.credential {\n        case let appleIDCredential as ASAuthorizationAppleIDCredential:\n            SocialSignInResult.AuthInfo.userInfo(appleIDCredential)\n        // 3\n        case let passwordCredential as ASPasswordCredential:\n            let credentials = Credentials(email: passwordCredential.user, password: passwordCredential.password)\n            handler?(.success(.native(credentials)))\n        default:\n            let error = AccountErrorBuilder.error(forCode: AccountErrorCode.loginFailure.rawValue)\n            handler?(.failure(error))\n        }\n    }\n\n    // ...\n} We modified SocialLoginService to add a SocialLoginRequest param that indicates if we want to check for an existing account or to create a new one. To check for native credentials, we need to create a request from ASAuthorizationPasswordProvider and add it to the array. We handle a new case where we can get a ASPasswordCredential object which represents the user’s credentials fetched from the iCloud Keychain. To share those credentials we use a Credential struct. Now we can generate two different types of requests with Sign in with Apple. As Apple suggests, we look for an existing account when we present the login screen, while we create a new account when the user taps on the Sign in with Apple button. class LoginFlowCoordinator {\n\n    func begin() {\n        // ...\n\n        if #available(iOS 13, *){\n            let appleService = SignInWithAppleService()\n            showSignInWithAppleSheet(with: appleService)\n        }\n    }\n\n    @available(iOS 13, *)\n    private func showSignInWithAppleSheet(with appleService: SocialLoginService) {\n        appleService.login(from: loginViewController, request: .checkExisting) { [weak self] result in\n            guard let self = self else { return }\n            switch result {\n            case .success(let authInfo):\n                self.handleSocialLoginServiceSuccess(with: .apple, authInfo: authInfo)\n            case .cancelled, .failure:\n                break\n            }\n        }\n    }\n\n    func loginViewController(_ viewController: UIViewController, didRequestSocialLoginWith provider: IdentityProvider) {\n        guard let socialLoginService = socialLoginServices.first(where: { $0.identityProvider == provider }) else { return }\n        socialLoginService.login(from: viewController, request: .create) { [weak self] result in\n            // ...\n        }\n    }\n} Managing the user session From the Apple documentation: “your app is responsible for managing the user session” . This is because a user can decide to revoke the association between its Apple ID and our app from the Settings app. When this happens the backend will not be notified by Apple: in fact, it is the app’s responsibility to manage this case adding some extra checks. After a session is revoked, Apple suggests logging out the user and showing a login screen. We created the SignInWithAppleRevokedManager class that performs the check in applicationDidBecomeActive and takes care to log out the user and show the login screen. @available(iOS 13, *)\nfinal class SignInWithAppleRevokedManager {\n\n    func checkSessionStatus() {\n        guard let userID = keychain.signInWithAppleUserID else { return }\n        let authorizationAppleIDProvider = ASAuthorizationAppleIDProvider()\n        authorizationAppleIDProvider.getCredentialState(forUserID: userID) { (credentialState, error) in\n            if let error = error {\n                print(error)\n                return\n            }\n\n            if case .revoked = credentialState {\n                DispatchQueue.main.async { [weak self] in\n                    self?.userDidRevokedSession()\n                }\n            }\n        }\n    }\n\n    private func userDidRevokedSession() {\n        guard loginService.isUserLoggedIn, keychain.signInWithAppleUserID != nil else { return }\n        loginService.logout { [weak self] in\n            self?.keychain.signInWithAppleUserID = nil\n            self?.showLoginAlert()\n        }\n    }\n} We need to store the user identifier in the keychain during the login flow and remember to remove it when the user logs out. Before logging the user out, we need to make sure that they are still logged in via Sign in with Apple (as users could have logged in with a different account since the login via Sign in with Apple). Conclusion We faced some issues in production during the rollout but we were able to mitigate them thanks to metrics, logs and our feature flag system . Sign in with Apple is still on its first version and we advise keeping a lookout for edge cases that are not covered in the documentation. Our implementation is now live on the Just Eat platform, we’re receiving very good feedback from users and we think it will become a key feature to offer a secure and easy way to log in to mobile apps.", "date": "2020-05-27"},
{"website": "JustTakeAway", "title": "MVP is DEAD! Long live MVP!", "author": ["Written by Ian Warwick"], "link": "https://tech.justeattakeaway.com/2020/04/14/mvp-is-dead-long-live-mvp/", "abstract": "In this article I am going to describe a modern approach to using the MVP pattern in an Android application. If you are familiar with MVP then still there might be some things here that might be useful. A long time ago in a galaxy far away… We used the MVP pattern and it was a great way of separating UI interaction logic (or Presentation logic) though ever since ViewModel was introduced with the architecture components, for some, MVP became history with the MVVM pattern taking its place. About ViewModels The introduction of ViewModel was pretty much a game changer in the Android developer world. Finally a decent way to represent state for an activity that could survive orientation changes, conceptually locking it into either a Fragment or Activities lifespan. You may think by using a ViewModel you are doing MVVM however this is an incorrect assumption. If we take a look at the documentation for ViewModel we find:- The ViewModel class is designed to store and manage UI-related data in a lifecycle conscious way. The ViewModel class allows data to survive configuration changes such as screen rotations. Instead of seeing ViewModel as a component within the MVVM pattern, it might be best to see it exactly as a useful data storage mechanism for a screen. A modern approach to MVP The design approach at the class level is shown in the diagram below. Looking at the diagram I want to highlight some of the important points in the design:- HelloPresenter has a dependency on HelloModel and HelloView which are both marked with <<interface>> denoting that these are both interfaces. The HelloModel interface is backed by an implementation HelloViewModel which in turn extends ViewModel from the Android architecture components. HelloPresenter has no idea how the interface HelloModel persists data, it can expose all sorts of data via properties to the presenter including things parsed from intents or data that has been loaded (which I will get onto later by using LiveData<T> behind the scenes as one usually would with ViewModel . Config is a separate function whose responsibility is to configure the routing of UI Actions (action flows) and Async Data callbacks (data flows) which I will get into later. Looking back at the class diagram we see a class Config which is responsible for binding async data flows (via LiveData<T> ) from the model to the presenter and UI action flows from the HelloView into the presenter. The next diagram shows how the pattern is integrated into a Fragment . Looking at the diagram we introduce a HelloFragment that is responsible for creating a concrete implementation HelloViewImpl which will take care of interacting with the android views which HelloPresenter can only do through its interface HelloView. Typically in the MVP that I have used the Fragment would implement the view interface itself, however it is best to delegate view interaction to its own class in this case HelloViewImpl to separate view lookups and bindings from an activity or fragment and keep them clean. Why an abstract model? As mentioned previously, instead of allowing access to a ViewModel directly you should use an interface. By using an interface we make sure anything (such as the presenter) interacting with our model makes no assumption on the technology employed to carry out the models responsibility. Which is to give access to data which is often contextual to the current screen. If later some fancier technology comes along to persist screen data, your refactoring becomes more trivial. Why use an abstract view? This goes without saying really in the MVP pattern instead of accessing the Android views directly we hide them behind an interface though instead of making the Fragment or Activity implement the view we delegate it to a class that specifically deals with a view hierarchy. Looking at the diagram HelloViewImpl can be given a view hierarchy from HelloFragment (including passing itself if necessary). Now HelloViewImpl is responsible for doing view look ups, view binding, etc. HelloPresenter may only interact with the view via HelloView which is typical of MVP patterns. Implementation So all this high-level design is insightful however there are some things missing from the design such as data & action flows. Like all good examples we will create a slightly modified “Hello World” that has a button, which when pushed will generate a new greeting message. Let’s start out with the abstract stuff and deal with the implementation later, for that we need a View, a Model and a Presenter as follows. interface HelloView {\n    fun updateGreetingText(greetingText: String)\n    fun onShowNewGreetingButtonPressed(onPressed: () -&gt; Unit)\n}\ninterface HelloModel {\n    fun loadGreeting()\n}\nclass HelloPresenter(val model: HelloModel, val view: HelloView) {\n    fun onCreated(init: Boolean) {\n        // If this is the first time we loaded,\n        // then load the first greeting\n        if (init) {\n            model.loadGreeting()\n        }\n    }\n    fun onGreetingLoaded(greeting: String) {\n        view.updateGreetingText(greeting)\n    }\n    fun onShowNewGreetingButtonPressed() {\n        model.loadGreeting()\n    }\n} Looking at the code, in HelloPresenter we have some very simple functionality in onCreated(init: Boolean) . Firstly we check init which tells us if this is the first time this screen was loaded (which we will see later how this works) and if it is the first time (init is true) then we ask the model to load a greeting by calling model.loadingGreeting() . Next in the presenter we have another very simple function onGreetingLoaded(greeting: String) which will be called when the greeting has loaded (which we will see how this works later). When this function is called we make a call to view.updateGreetingText(greeting) with the loaded greeting text. Finally we have a function onShowNewGreetingButtonPressed() which will be called when the user presses on a button (again this will become clear later when we find out how it is called). When the function is called we make a call to model.loadGreeting() which covers the functionality of loading a new greeting message. HelloModel and HelloView are very simple interfaces, and we will see how they are implemented in the next sections. The concrete view implementation To satisfy our HelloView interface we need a concrete view, and to achieve this we create a class HelloViewImpl which implements the interface. We can use a Fragment but in this case to keep our example simple we will use an Activity . class HelloViewImpl(activity: AppCompatActivity) : HelloView {\n    private val greetingTextView: TextView = activity.findViewById(R.id.greeting_text)\n    private val newGreetingButton: Button = activity.findViewById(R.id.new_greeting_button)\n    override fun updateGreetingText(greetingText: String) {\n        greetingTextView.text = greetingText\n    }\n    override fun onShowNewGreetingButtonPressed(onPressed: () -> Unit) =\n        newGreetingButton.setOnClickListener { onPressed() }\n} In the code snippet above we have the HelloViewImpl which takes the activity as its first argument so it can perform relevant view lookups and actions on those views. It is a fairly simple app so we implement both the functions of HelloView to interact with the view accordingly. For onShowNewGreetingButtonPressed we take the lambda passed as onPressed and just call it when the greeting button is pushed, very simple. How we integrate this into the activity we will see later, let’s first take a look at the concrete model. The concrete model implementation On the other side, we need a concrete model, which we will use the awesome power of the architecture components ViewModel and its best buddy LiveData . internal class HelloViewModel : ViewModel(), HelloModel {\n    val greeting: MutableLiveData = MutableLiveData()\n    override fun loadGreeting() {\n        greeting.value = greetings.random()\n    }\n}\nprivate val greetings = listOf(\n    \"Hello\", \"Yo!\", \"Big up!\", \"Greetings!\", \"Hiya!\" Looking at the code HelloViewModel we have a live data which we will use to post new greetings to. We implement the HelloModel interface function loadGreeting() to select a random greeting from a list and post it on the greeting live data. Configuring action and data flows Now we have a concrete model and a concrete view and our presentation logic, we can wire things together and create an activity which will make all of this finally work. Taking a look at the diagram below we can visualise how communication will happen between the Presenter, View and Model. In the diagram we have two way communication between Presenter and Model and the Presenter and the view. To keep things nice and separate the responsibility of configuration is delegated to a simple extension functionHelloPresenter.configure(…) below. fun HelloPresenter.configure(lifecycle: Lifecycle, vm: HelloViewModel, view: HelloView) {\n    // action flows\n    view.onShowNewGreetingButtonPressed(::onShowNewGreetingButtonPressed)\n    // data flows\n    vm.greeting.observe({ lifecycle }, ::onGreetingLoaded)\n} This function is best kept in a separate file and serves one purpose to connect the model data streams to the presenter (for this we need the concrete model) and for connecting UI event streams from the view. Integrating into the activity Finally we can integrate into an activity and the following code snippet shows how this is done. We could use something like Dagger here though to keep things obvious and simple no DI will be used. class MainActivity : AppCompatActivity() {\n    private lateinit var view: HelloView\n    private lateinit var presenter: HelloPresenter\n    private lateinit var model: HelloViewModel\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_main)\n        model = of(this).get(HelloViewModel::class.java)\n        view = HelloViewImpl(this)\n        presenter = HelloPresenter(model, view)\n        presenter.configure(lifecycle, model, view)\n        presenter.onCreated(savedInstanceState == null)\n    }\n} Following the example in onCreate we create our model (using ViewModelProviders.of ) create our HelloViewImpl passing the activity to it so it can perform relevant lookups and bindings, create our presenter and call the configure(…) function on it effectively setting up the action & data flows and finally calling presenter.onCreated(savedInstanceState == null) kickstarting the presenter logic and letting it know that this is the first time this screen was created (leveraging saveInstanceState nullability). Conclusion And that is my modern approach to MVP using architecture components and following some basic rules of abstraction. The full source code for the example is available here Hope that helps anyone who finds some use in the MVP pattern and is looking for a modern approach to what might be out there already. A thing to keep in mind is Jetpack Compose is around the corner so using a pattern like this (including MVVM) may become redundant as may some of the architecture components. That being said it is still a great pattern and using a model interface instead of directly integrating with ViewModel could save you some trouble later if you wish to move away from architecture components later.", "date": "2020-04-14"},
{"website": "JustTakeAway", "title": "Streamlining Certificate Management with Let’s Encrypt and Ansible", "author": ["Written by Andrew Marwood"], "link": "https://tech.justeattakeaway.com/2020/05/04/streamlining-certificate-management-2/", "abstract": "In this post I’ll show you, with examples, how we’re using Ansible and Route53 to request and renew Let’s Encrypt certificates that we can use to secure our internal tools and APIs at Just Eat. I’ll also describe how we’re automating this with Concourse CI, how we’re storing them securely in AWS Certificate Manager (ACM), and how we’re doing this across our dozens of AWS accounts. In the beginning… Our requirement to use Let’s Encrypt arose from a project to implement Hashicorp Vault. We wanted to secure our Vault instances with Transport Layer Security (TLS) certificates so we could have end to end encryption terminated at the Vault EC2 instances. At the time the only available certificates were self signed certificates from an internally managed Certificate Authority (CA). These would have required us to ensure our Vault users had valid root CA certificates installed to avoid warnings in the browser. Securing our certificates Secure storage of certificates is critical. We’re utilising AWS Secrets Manager to store the certificates with an appropriate IAM resource policy to ensure certificates are retrieved by authenticated and authorised entities only. We can also see from AWS Cloud Trail when these certificates are retrieved and by whom. It is important to point out that although it is not shown in the examples below we’re wrapping our ansible tasks in blocks that use no_log. This prevents certificate material being inadvertently leaked into logs. Orchestration We’re running all of these tasks in an Ansible role from Concourse CI. Concourse CI will run our Ansible role in an ephemeral container that will be destroyed once we have created and stored the certificate. We run the role on a schedule which generates a new certificate for anything due to expire within the next 10 days. We use a simple Concourse CI pipeline to run our Ansible role in each of our AWS accounts and ensuring we have consistent certificates everywhere that they’re needed. The pipeline has a git based trigger based for any changes made to the master branch of the Ansible role, and we also have a time based trigger to run the pipeline in all accounts on a fortnightly schedule. Building out the solution At Just Eat we use Ansible for our infrastructure provisioning as it gives us a way to simply declare the resources we would like and create them in an idempotent way. Fortunately there are a number of open-source Ansible modules available that make the use of Let’s Encrypt fairly straightforward. The step by step process we are using to generate a Let’s Encrypt certificate is laid out below. Generate the ACME account private key The Automatic Certificate Management Environment (ACME) account private key is used later by the acme_certificate module. - name: Generate the acme account key\n  become: yes\n  openssl_privatekey:\n    path: \"{{ account_key_path }}\"\n    owner: letsencrypt\n    group: letsencrypt\n    mode: 0640 Generate the certificate private key We need a private key that will be used for the certificate we are generating. - name: Generate the private key\n  become: yes\n  openssl_privatekey:\n    path: \"{{ private_key_path }}\"\n    owner: letsencrypt\n    group: letsencrypt\n    mode: 0640\n    size: 2048 Note: We’re using 2048 for key length because AWS Load Balancers do not support larger keys if the certificate is stored in ACM. See here for more details: https-listener-certificates Generate the Certificate Signing Request The Certificate Signing Request (CSR) will be used when requesting the certificate from Let’s Encrypt. - name: Generate the certificate signing request\n  become: yes\n  openssl_csr:\n    path: \"{{ csr_path }}\"\n    privatekey_path: \"{{ private_key_path }}\"\n    common_name: \"{{ dns }}\"\n    owner: letsencrypt\n    group: letsencrypt\n    mode: 0640 Create a DNS challenge This is one of the key tasks in the process. It requests the certificate from Let’s Encrypt and returns information about the DNS records we need to create to prove that we own the domain and allow the request to be validated by Let’s Encrypt. Let’s Encrypt has a staging environment ( https://letsencrypt.org/docs/staging-environment/ ) that allows you to get the settings right before trying to issue production certificates. - name: Create a DNS challenge\n  become: yes\n  acme_certificate:\n    account_key_src: \"{{ account_key_path }}\"\n    account_email: \"{{ email }}\"\n    csr: \"{{ csr_path }}\"\n    dest: \"{{ cert_path }}\"\n    challenge: dns-01\n    acme_directory: \"{{ acme_directory_url }}\"\n    acme_version: 2\n    terms_agreed: yes\n    remaining_days: 10\n    force: \"{{ force }}\"\n  register: dns_challenge Note: We use an Ansible var for whether or not to force the dns challenge, but this should be used with care as Let’s Encrypt will rate limit ( https://letsencrypt.org/docs/rate-limits/ ) certificate requests if you request too many certificates with the same parameters in a given time period. Set some facts… From the acme_certificate we register the dns_challenge and extract the dns record and value that we need to create in Route53 to have the certificate request validated. - name: Set dns challenge facts\n  when: dns_challenge is changed\n  block:\n    - name: Set dns_challenge_record fact\n      set_fact:\n        dns_challenge_record: \"{{ dns_challenge.challenge_data[dns]['dns-01'].record }}\"\n\n    - name: Set dns_challenge_record fact\n      set_fact:\n        dns_challenge_value: \"{{ dns_challenge.challenge_data[dns]['dns-01'].resource_value }}\" Create the Route 53 record From here we can create the Route53 record, then wait for it to be created. - name: Get hosted zone id\n  command: |\n    aws route53 list-hosted-zones-by-name \\\n    --dns-name \"{{ hosted_zone }}\" \\\n    --query \"HostedZones[0].Id\" \\\n    --region {{ region }}\n  register: hosted_zones_cmd\n\n- name: Create dns-challenge.json file\n  template:\n    src: dns-challenge.json.j2\n    dest: /tmp/dns-challenge.json\n\n- name: Set record\n  command: |\n    aws route53 change-resource-record-sets \\\n       --hosted-zone-id \"{{ hosted_zones_cmd.stdout }}\" \\\n       --change-batch file:///tmp/dns-challenge.json \\\n       --region {{ region }}\n  register: record_set_cmd\n\n- name: Wait for record change\n  command: |\n    aws route53 wait resource-record-sets-changed \\\n      --id \"{{ (record_set_cmd.stdout|from_json).ChangeInfo.Id }}\" Validate and retrieve the certificate Finally we can validate the certificate challenge and retrieve the certificate and intermediate certificate. - name: Let the challenge be validated and retrieve the cert and intermediate certificate\n  become: yes\n  when: dns_challenge is changed\n  acme_certificate:\n    account_key_src: \"{{ account_key_path }}\"\n    account_email: \"{{ email }}\"\n    csr: \"{{ csr_path }}\"\n    dest: \"{{ cert_path }}\"\n    fullchain_dest: \"{{ fullchain_path }}\"\n    chain_dest: \"{{ chain_path }}\"\n    challenge: dns-01\n    acme_directory: \"{{ acme_directory_url }}\"\n    acme_version: 2\n    terms_agreed: yes\n    remaining_days: 10\n    force: \"{{ force }}\"\n    data: \"{{ dns_challenge }}\" Renewal Let’s Encrypt certificates are valid for 90 days, but due to the way the ansible role has been written, the renewal process is trivial. In the code snippet above, you may note that we have remaining days set to 10. This means that we can run the Ansible role when the certificate is due to expire in less than 10 days and the certificate will be automatically renewed. Next Steps Our initial goal was to create Let’s Encrypt certificates that we can use for Hashicorp Vault, but it quickly became clear that the Ansible role we created took a generic approach which allows us to extend our Let’s Encrypt usage to Amazon Certificate Manager (ACM) to allow the certificates to be used in Cloud Front, API Gateway, and by AWS managed Load Balancers. Conclusion Let’s Encrypt provided a great solution to a number of problems we were facing at Just Eat. We are now able to automate our certificate management and schedule it on a regular basis which mitigates risks and effort associated with expiring certificates. We have also vastly simplified our certificate infrastructure making it much easier for our applications to be configured with TLS endpoints.", "date": "2020-05-04"},
{"website": "JustTakeAway", "title": "Modularisation in the Just Eat Android Consumer Application", "author": ["Written by Ian Warwick"], "link": "https://tech.justeattakeaway.com/2020/02/24/modularisation-in-the-just-eat-android-consumer-application/", "abstract": "In this blog post I am going to share an insight into the technical approach to app modularisation of a monolithic Android app and how we use Dagger in a Gradle multi-module setup. This article does not aim to describe best practices of modern modular application development since this is subjective to how old your application code is, this is more about the patterns and approaches we currently use and if you are a heavy Dagger user like us maybe it could take your monolithic application to the world of modular also. If you want to know more about the benefits and other useful info regarding a modular application my colleague Alberto De Bortoli from the iOS team here at Just-Eat wrote a great blog post on that here https://tech.just-eat.com/2019/12/18/modular-ios-architecture-just-eat/ . What is in a feature module? A typical feature module at Just Eat is a gradle module with an activity (sometimes more than one) that provides a logical slice of the application such as Home, Search, Help, Menu, Checkout and Payment. These types of features we consider core features, usually whole screens of functionality and we have feature teams that maintain them. The following component diagram shows an example of two feature modules, feature-checkout and feature-payment that both share a core gradle module and the main application module justeat-app that depends on both features. In our app we have a core module to share top level dependencies that are frequently used throughout the app such as the okhttp dependency and any other core dependencies that we want to consistently provide to the rest of the application specially those dagger dependencies that we scope as @AppScope dependencies. Taking a look inside our core module, we have a single dagger component – AppComponent . AppComponent has the scope annotation @AppScope which allows us to provide core dependencies once (app wide singletons) which is useful in some scenarios where some third party API’s impose it for performance gains or some other reason (OkHttp, etc) We hang a number of dagger modules from this component to provide these scoped (and other non-scoped) dependencies. Our feature modules pretty much have the same setup, they are mostly geared around an activity, or several activities, sometimes a single activity and many fragments. In the diagram we show that the module feature-checkout contains an Activity CheckoutActivity and its dagger counterpart CheckoutActivityComponent which depends on AppComponent . This allows us to share those @AppScope dependencies downstream to our feature modules so they can benefit from dependencies that are provided from above. Also our feature module’s dagger component is annotated with @FeatureScope which allows features to retain dependencies once they are provided to our feature code effectively giving us a singleton like scope on some dependencies until a configuration change occurs. A practical example This is all fair and well to show as conceptual diagrams however there are some practical concerns that can only be demonstrated with code examples, in particular how we can make our feature components such as CheckoutActivityComponent gain a reference to AppComponent to fulfill its dependency. In a nutshell we create a singleton reference to AppComponent which we initialize in our application, where our app component looks something like this. @Component\n@ApplicationScope\ninterface AppComponent {\n    @Component.Builder\n    interface Builder {\n        @BindsInstance\n        fun application(application: Application): Builder\n        fun build(): AppComponent\n    }\n    fun application(): Application\n    companion object {\n        lateinit var instance: AppComponent\n        fun init(application: Application) {\n            instance = DaggerAppComponent\n                            .builder()\n                            .application(application)\n                            .build()\n        }\n    }\n} Then in a custom Application class we can initialize our core DI component. class ExampleApplication : Application() {\n    override fun onCreate() {\n        super.onCreate()\n        AppComponent.init(this)\n    }\n} Now for our downstream components (specifically feature components) it is a simple matter of gaining a reference to AppComponent.instance and passing it to our feature component. The following example shows a simplified dagger component for our example feature module feature-checkout . @Component(dependencies = [AppComponent::class])\n@FeatureScope\ninterface CheckoutActivityComponent {\n    @Component.Builder\n    interface Builder {\n        @BindsInstance\n        fun activity(activity: Activity): Builder\n        fun appComponent(component: AppComponent): Builder\n        fun build(): CheckoutActivityComponent\n    }\n} Then in our Activity we can bootstrap the feature component and set its parent component class CheckoutActivity : AppCompatActivity() {\n    private lateinit var component: CheckoutActivityComponent\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        component = DaggerCheckoutActivityComponent\n            .builder()\n            .appComponent(AppComponent.instance)\n            .build()\n    }\n} And that is the most basic practical example of the approach we use in our modular application at Just Eat. In reality our app is far more complex, has many modules hanging off AppComponent and each feature component such as CheckoutActivityComponent also have local dagger modules that provide dependencies on the feature level. When Feature X wants to depend on Feature Y? We won’t allow this. Our modular approach does not always start out with the best intentions of sharing common code especially in situations where say, Feature X wants to access data where access is only implemented in Feature Y. Generally we do not allow features depending on features. The approach we use is to identify what code from the other feature we want, and move that up above both feature modules. Referring back to our first diagram If feature-payment wants to reuse code from feature-checkout then we would move that code up into an API module such as checkout-api . In this case our checkout-api does not declare a dagger component in order to include its functionality though can include a dagger module that provides dependencies in a way that might be useful for other downstream feature modules to include, alternatively those feature modules can create the dependencies themselves. In some cases where you would need to include checkout-api everywhere to every module then you could provide it from the core component. In the diagram we show that checkout-api has been moved up to core . Downstream modules will now receive dependencies from checkout-api through AppComponent given that it includes a CheckoutModule that provides checkout-api dependencies. Another thing to note is our API style modules we like to keep pure, no UI, no activities or fragments, typically they provide data and contain things like repositories and networking code, effectively the back-end code of features. Similarly if a feature has UI code that we want to share with other features, we could follow the same approach and create a checkout-ui module that we can include to downstream feature-* modules. Dealing with a large number of core APIs and Services In our core module we have more than a few dependencies provided by our core component AppComponent . For instance we have a number of other modules such as network , analytics and logging which we provide through our core component AppComponent , the following diagram provides an overview of our core gradle module dependencies as well as their respective dagger modules which we include into our core AppComponent . Conclusion This article has briefly described the modularisation approach we use in our Android Application at Just Eat. With the advent of new technologies like Jetpack Compose and the fact that the development community have trended toward single activity applications this approach may no longer be a good starting point. However this post could still serve as a means to move from a monolithic activity per feature based application into a modular one, especially if you are invested in Dagger. References Modular iOS Architecture @ Just Eat https://tech.just-eat.com/2019/12/18/modular-ios-architecture-just-eat/ Dagger https://github.com/google/dagger", "date": "2020-02-24"},
{"website": "JustTakeAway", "title": "Bootstrapping UX graduates", "author": ["Written by maxim.millner"], "link": "https://tech.justeattakeaway.com/2020/02/18/bootstrapping-ux-graduates/", "abstract": "The bootcamp How do you introduce three graduates starting their very first UX role into your team? The UX&D team at Just Eat did it with a bootcamp. It was a creative and effective way to introduce us to life at Just Eat – here’s how it was done. In September, I joined Just Eat’s first UX graduate programme along with two others in the same position. The bootcamp followed hard on the heels of our corporate induction. We were given a real design brief to work on which gave us the opportunity to learn in context about the product as well as develop our skills. We had coaching from different members of the team for a few hours each day (UX designers, UX researchers, UI designers and copywriters), and were set exercises and tasks designed to facilitate self-learning. One of the key aims of the bootcamp was to get us up to speed with the way the team works here. UXers at Just Eat work in fast-paced design sprints. We work within multidisciplinary teams focusing on a different problem or set of problems each sprint. Just Eat sprints are fortnightly blocks of work, and do not follow a prescribed format; we have iterated on the format to develop one that works for us. At the end of each sprint, we speak to customers / restaurant partners to investigate problem areas, learn about new design iterations or answer discovery questions. The bootcamp was a way of us learning about and getting used to this way of working, before being fully immersed within it. The structure of our bootcamp sprint The brief The problem space we were given to explore was ‘group ordering’. We know that some customers, in certain situations, want to order food as a group rather than on their own. Group ordering is a real world customer behaviour that the team wanted to learn more about and potentially support, so it was a fitting topic to be used in the bootcamp. Week one: we learned by doing In the first week, UX designers taught us about the first stages of the design sprint. They first taught us about the ‘sprint kickoff’. We ran a mini kickoff session ourselves where we discussed the problem, breaking it up and exploring it in detail. Once we felt we had a good understanding of the problem space, we learned about ideation strategies, like Crazy 8s, which we used to brainstorm some ideas for solutions to the problem. Sketching during rapid ideation Also in week one, UX researchers introduced us to various research methods. They went through how they are used in the industry and what kinds of problems each research method is best suited to exploring. We also learned how to work with our Copywriters during the design process, and learned some basic Sketch skills to turn our best sketched ideas into low fidelity digital prototypes, in preparation for testing the following week. Week two: from static wireframes to user research of an interactive prototype The second week of bootcamp involved taking our low fidelity prototypes and making them interactive with Invision . We were then taught how to design and write a discussion guide capable of testing our hypotheses in user testing. Amy & Jess prototyping Once our discussion guide was finished, we watched a couple of user research demonstrations from UX researchers in the team then each of us took a turn facilitating a session. During the session we carried out semi-structured interviews, probing participants about their takeaway and group ordering experiences. We also took them through some scenarios using our prototype to complete some tasks, looking for usability and UX insights about our designs. Whilst the interviews were going on, the rest of the group were watching at their desks and taking notes on a Miro board. After user research, we analysed our findings as a group thematically, using our notes on a Miro board. From what seemed like an impossible amount of notes, we managed to build some themes and developed these into key insights about our design. Through this process, we learned a lot about collaboration and remote working, as we needed to talk through decisions as we shaped our analysis. We also collaboratively started building a playback slide deck of our findings, that we were to share with the team the following week. Week three: playing back our findings and refining the UI In week three, we finished preparing the slide deck and presented it to the UX team. This was a great way to get us used to sharing our learnings with the team and learn how to distil UX work into a digestible and impactful form. Finally, we spent some time with the UI designers in the team, learning about their processes of creating beautiful interactive designs and how to deliver designs to developers ready for production. We had the opportunity to make our own style guides, by cutting and sticking fonts and colour palettes from the current design language used at Just Eat. We then used our style guides to create higher fidelity prototypes of our group ordering designs, and then learned how to animate these in Principle. What we learned Coming from an academic environment into a large UX team at a major technology company like Just Eat is daunting – there is so much to learn and take in, and the pace of work is something you need to get used to fast. Through the bootcamp we had an exciting, intense first few weeks at Just Eat, but most importantly were able to learn the key skills necessary to make our transition from academia to industry as smooth as possible. Here’s a list of the key skills we learnt during the bootcamp – The design sprint structure at Just Eat An introduction to tools: Miro , Trello , Sketch , InVision , Craft , Principle Collaboration and remote working via Google Hangouts Ideation strategies The importance of phrasing when doing user research Discussion guide writing and user research facilitation How to make an idea into a prototype and take it into research within a week Effective communication for good teamwork How to present UX design and research work to stakeholders and colleagues in a Playback Overall, the bootcamp was a really great way to meet the team in London and Bristol and get up to speed with how the team works by being taught by people of different disciplines. Thank you so much to everyone who took time out of their busy days to facilitate the bootcamp! So, if your company is taking on UX graduates or even some junior UXers consider running a bootcamp during onboarding to get them settled in. It will leave them wonton more!", "date": "2020-02-18"},
{"website": "JustTakeAway", "title": "Building empathy for our Restaurant Partners: Archetypes as a design tool", "author": ["Written by Charlie Phillips"], "link": "https://tech.justeattakeaway.com/2020/01/29/building-empathy-for-our-restaurant-partners-archetypes-as-a-design-tool/", "abstract": "When you think of Just Eat, you probably think of an app where you can order food right to your doorstep (maybe on a particularly hungover Sunday). You might think of the branding, the adverts, the food, the delivery driver at your door, and the experiences you’ve had. You might have ‘Did somebody say Just Eat’ stuck in your head (apologies if so). It’s unlikely you think of the actual restaurants you’ve ordered from, because using an app pus a distance between us as customers and the actual restaurant. Behind the scenes and the experiences you have, there is a diverse range of restaurants that we need to understand and design for in the Restaurant team here at Just Eat. We were lacking a coherent story of our users, their behaviours and the challenges they face. We are super lucky that both UX designers and researchers are frequently encouraged to go out into the field and meet restaurant partners in their natural contexts. However, for those members of the wider business that don’t go out into the field, having artefacts that embody our Restaurant Partners needs is essential. And so, this project was born. Archetypes and personas Let’s start with the basics. What actually is an archetype? And is archetype just a fancy word for a persona? Well: “A behavioural archetype conveys the “who does what, when they do it, & why” of our audience. Steeped in user behaviour, they focus on a group’s needs, motivations and pain-points, and capture how they think, feel and act in particular situations or scenarios.” Personas, on the other hand, can be thought of as a hypothetical archetype of an actual user. They tend to include specific details such as age, sex, occupation, education and interests – which can’t be representative of all users. If you’d like to learn more, here’s an article that sums it up well. For this situation, we chose to go with archetypes because of the credibility of having a behaviour based tool. We have a particularly large user base, so representing this with personas can be tricky. From a design perspective, understanding different behaviours helps us cater to them with our solutions. Also, because we are dealing with lots of different account managers and territory managers, archetypes help them to understand this is not one particular person but a way of aggregating our user types. The value of archetypes I briefly touched on how this project came about in the first paragraph, but it’s important to cover all the ways in which archetypes are an important tool for user centred design. The key points being; Build empathy for our Restaurant Partners throughout the business Help Product & Design teams make decisions with a shared picture of “the user” in mind Define the product roadmap and content strategy Sense check that solutions cater to all user types Understand when, where and how we can make a difference to our restaurant partners experiences How We tackled this project initially in a team of 3, later bringing in the help of our Principal Copywriter. We already had a set of archetypes in the design team for our restaurant partners, however the team didn’t identify with them and struggled to relate them back to their experiences with restaurant partners. So, our first step was to work with what we had and dissect them. We gathered feedback on the current ones, and discussed in detail what information they contained, whether we felt it was relevant and whether we felt the archetypes in general were representative of our estate. What we found from feedback sessions was that the archetypes were hard to distinguish from one another, not particularly representative of our restaurant partners and their behaviours, lacked empathy and were generally not memorable. They included specific details that were hard to link to an archetype, for example, linking level of tech proficiency to success, which we know is not the case. There was also a lack of trust from the team and Territory managers. Involving them in the process and engaging everyone would lead to more trustworthy and robust archetypes, as well as get buy in and increase the likelihood of people using them. Evolution through talking to our Restaurant Partners Between the team, we observed many usability sessions in our research lab with the archetypes in front of us to identify gaps and opportunities . We then decided to formulate a set of ‘profiling questions’ informed based on previous rounds of research to ask every single restaurant partner we spoke to. This would allow us to draw out key differences between our user base. We asked our Restaurant Partner’s a series of questions; What motivates you to wake up every day in the morning to go to your restaurant? Where would you like your restaurant to be in 3–5 years? What does a day-to-day look like? What keeps you awake at night? What are your biggest everyday challenges? As part of this project alone, we spoke to over 50 restaurant partners from a mix of backgrounds; a range of Partner Status’, cuisines and locations both in the lab and out in the field. While we gained insight from asking questions, we also gained a lot from observation. In addition to this, we spent three hours listening in to calls in the call centre. The process we took as a team was very agile — meeting frequently and building up a picture as we went along, whilst iterating along the way. As we gained richer insight, we were able to start thinking about what information we’d actually include on the archetypes for them to be a valuable design tool. We came up with the idea of a behaviour trait spectrum, to help us distinguish the archetypes from one another. This was heavily informed by behaviours and attitudes we’d both observed and heard expressed by our restaurant partners. For example – ‘Happy to get by’ versus ‘Got to grow’, highlighting the difference between Restaurant Partners who are more reactive and focused on day to day operations in contrast to ones who are more proactive, focused on long term strategy and growth. Once we’d identified the different attitudes, we mapped our drafted archetypes to the scales. Validating Cumulatively, the design team has a tonne of experience and insight into our restaurant partners both out in the field and in the usability labs, so testing out the archetypes with them was essential. The other group crucial to validating these archetypes is Account Managers and Territory Managers. Their role involves speaking to Restaurant Partners all day everyday, and they are out there on the front line hearing their challenges and stories. We conducted a workshop with members from this group to gain further insight into our archetypes, their relatability and % of our estate. During these workshops, we were hoping to hear things like; ‘Oh yes this restaurant partner I visited is definitely that one’, to validate that our archetypes were relatable. We also wanted to check if they felt there were any gaps. We asked for stories or quotes around certain archetypes, to add to their realness and build further empathy. Territory and Account managers were able to relate to the new archetypes and give further suggestions, as well as insight into how much of our estate they think they account for. Our Principal Copywriter Mike was able to re-write the archetypes in a compelling way, using first person to help us feel connected to them. New archetypes The new archetypes are framed in a way that makes it easy to step into the restaurant’s shoes easily — particularly through the use of first person and real life quotes. Clearly defining the challenges and needs allows us to understand what the restaurant partner needs from our products and their motivations behind it. We understand their pain points, meaning we can check our solutions to ensure we’re helping them overcome those. Together, the challenges, needs and attitudes and ambition matrix as well as ‘Where you’ll find them’ builds up a picture of our entire product ecosystem. We can gauge their motivations and behaviours around the products we design. Result Greater empathy in the Restaurant pillar: The design team now has referenceable archetypes that are widely used in sprint kick offs and playbacks with teams to build empathy for our restaurant partners and consider different types of restaurants & scenarios that we need to cater to with our designs. Widely shared: Further, the archetypes have been presented on numerous occasions at company and pillar wide meetings and global design all hands, as well as product huddles. They are used outside of the design team to educate stakeholders and build a picture of the estate we design for. Helpful as a training material and resource: They’ve also proved useful for Territory Managers and Account Managers in carrying out their roles; “Really interesting stuff you’re working on. It’s been needed for so long” — Market Activation Manager — Collection • Sales, Customer Marketing User friendly design solutions: From the early ideation phase, archetypes are used by the design team to ensure each solution fits the needs of all user types Next steps Having distributed the Marketplace Restaurant archetypes, we’ll keep sense checking these and evolving them. We’re using these archetypes and the process we took to pave the way for our Canadian team at Skip the Dishes to do the same. The next big thing is Branded Restaurant Group (the chain restaurants that you know and love) archetypes. We identified that Branded restaurants have different roles, behaviours and needs and therefore don’t fit in to the archetypes detailed above. The team worked closely with the strategic account managers to formulate initial archetypes, and delivered a first draft of these archetypes in December. Watch this space for more archetype updates!", "date": "2020-01-29"},
{"website": "JustTakeAway", "title": "As a UX Designer you are not the user expert", "author": ["Written by James Eyke"], "link": "https://tech.justeattakeaway.com/2020/01/17/as-a-ux-designer-you-are-not-the-user-expert/", "abstract": "As user experience designers we are the voice of our users’ needs, but are we the most knowledgeable people in the company about our users? And if we are not, who is? Am I not The Expert? At Just Eat I primarily work on products that are used by restaurant owners or managers. When working on these products I regularly get to go out into the field to talk to these users, or I’ll attend lab based sessions hosted by our amazing researchers. Both of these help me understand the problem areas and the opportunities we have. So that should make me the internal expert on how restaurant owners and managers operate, right? Well no. As much as I’d like to think I’m an expert on how our restaurant owners think, the simple truth is I’m not. I don’t spend everyday talking to them and I certainly don’t own my own restaurant. Who is then? Here at Just Eat we have people who speak with restaurants everyday (but not anyone who actually owns a restaurant). So when I quickly need more insight into a problem, or want someone representing the restaurants in a workshop, or just get a better idea of a restaurant owners day-to-day, I’ll reach out to our experts. At Just Eat the people we have who spend their working day with restaurants are: Call centre operators Specialist operation teams Territory managers Telephone account managers Call centre operators — Every restaurant who calls in with an issue will no doubt go through this team. They will know the types of problems currently coming in and how they are solved and/or escalated. Specialist operation teams — Depending on the product, some of ours have specialist operation teams working directly with restaurants. They will know more about the ins-and-outs of the product, and how restaurants use it. Territory managers — These are the people out in the field, going into restaurants everyday. Signing up new restaurants, working directly with the owner to maximise their opportunities and solving the problems they have. They fully understand our product and how restaurants use it. Telephone account managers — They have a pretty similar role to our territory managers but all their work is done over the phone. So while they don’t get to physically see how restaurants use our tools, they speak to more restaurants, giving them a wider perspective of the general problems. The Expert Okay so I’ve found my experts, now what? When kicking off any new project, I now factor in who my internal experts are and quickly reach out to them. Talking to the call centre operators and looking at their logs, helps me understand the scale of the problem and the steps they take to help the user. I did this recently and discovered a feature that I thought was only available in the UK was also available in France. So I reached out to the French team and learnt more about how they used the tool which helped shape the direction of the product. If the project has a specialist operation team, I’ll look to schedule a journey mapping session as quickly as possible. This is one of the best and quickest ways I’ve found to fully understand the problem area, how it works today (both for our internal and external users) and the opportunity areas. I’ll reach out to Territory Managers and Telephone Account Managers to understand how important the problem is to restaurants, and hopefully be put in contact with restaurants who have specifically raised the matter. With the Territory Managers I’ll also look to book in some visits to restaurants. While with the Telephone Account Managers I’d instead focus on listening in to some calls. When exploring a problem around time and delivery a territory manager mentioned she was having discussions around this with her restaurants. It turned out her restaurants were based in rural Scotland, and before I knew it I was travelling up there. The size of the difference between rural and urban restaurants surprised us all and helped shape how we did research going forward. As the project evolves I will keep working with my internal experts and actively bring them into workshops. This allows me to not only learn more from them, but also have someone in the workshop (who isn’t me) proactively pushing for the user needs. In one workshop we did crazy 8s and all the stakeholders sketched tech solutions, my internal experts sketched physical solutions. This helped open up the problem and help move the stakeholders from their solution led ideas. Let’s wrap this up You are not The Expert, but you are the person who brings everyone together. Find your internal experts, reach out to any sales or support teams you have and build contacts with them. Work with your internal experts through the life of the project. This allows you to focus more quickly on the key discovery questions and opportunity areas. It will help you avoid getting trapped in tech led solutions, particularly those pushed by stakeholders. Do research! This isn’t a proxy for research, but something that will help make it more focused and useful.", "date": "2020-01-17"},
{"website": "JustTakeAway", "title": "Introduction to Jubako", "author": ["Written by Ian Warwick"], "link": "https://tech.justeattakeaway.com/2020/01/20/introduction-to-jubako/", "abstract": "Jubako is a new open-source offering from Just Eat to create rich content screens (ala wall of carousels) similar to our Android app home screen, Netflix or Google Play. At the time of writing Jubako just made a 0.5 alpha release. Although it is only an alpha we have been using some rendition of the approach for several years. Back in April 2019 we added a new feature to the home screen that offers recommended restaurants and personalized cuisines. Our home screen was to become a wall of content where each content section could be anything, though mostly – carousels. Carousels are widgets that allow the user to swipe through a horizontal list of content. Content is represented as tiles and could contain information on products, restaurants or cuisines. The back-end APIs that serve up this content are defined in multiple ways. Some API’s return a list of lists (where each list item represented a carousel) or an API could return a list of links to other lists, or even just return a single list, the possibilities of how data is returned from APIs could never be defined in a fixed manner, so Jubako had to be flexible on how it sources content to be displayed. Given that Jubako was designed to be flexible on how it handles asynchronous data, `LiveData` was used as the data interface. Hello Jubako To achieve Hello World status with Jubako we can use one of many convenience functions that aims to take out the boilerplate for simple scenarios that do not require the complexity of enterprise patterns. class HelloJubakoActivity : AppCompatActivity() {\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_jubako_recycler)\n        recyclerView.withJubako(this).load {\n            for (i in 0..100) {\n                addView { textView(\"Hello Jubako!\") }\n                addView { textView(\"こんにちはジュバコ\") }\n            }\n        }\n    }\n    private fun textView(text: String): TextView {\n        return TextView(this).apply {\n            setText(text)\n            setTextSize(TypedValue.COMPLEX_UNIT_DIP, 48f)\n        }\n    }\n} In this example we use the convenience function `withJubako` to setup a `JubakoRecyclerView` as a target in which to load data into. Following up with a call to `load { }` passing a lambda where we can use the convenience function `addView { }` to simply add android views. We loop a 100 times adding both english Hello Jubako and Japanese (a respectful nod to the origins of the Jubako name!) こんにちはジュバコ Running this example produces the following result, a list of 100 hellos in both English and Japanese. Fig. 1 Hello Jubako (advanced version) Under Jubakos convenience extensions `load { }` and `addView { }` there is a much more verbose way to get the same result, and even though it is more verbose and could be considered boilerplate in common scenarios, the verbose way has many more opportunities for advanced work especially if you want close contact with `RecyclerView` internals such as a `ViewHolder`. The following example shows an expanded version of Hello World example (Ex. 2) recyclerView.withJubako(this).load(SimpleJubakoAssembler {\n    for (i in 0..100) {\n        add(object : ContentDescriptionProvider<Any> {\n            override fun createDescription(): ContentDescription<Any> {\n                return ContentDescription(\n                    viewHolderFactory = viewHolderFactory {\n                        object : JubakoViewHolder<Any>(textView(\"Hello Jubako!\")) {\n                            override fun bind(data: Any?) {}\n                        }\n                    },\n                    data = EmptyLiveData()\n                )\n            }\n        })\n        add(object : ContentDescriptionProvider<Any> {\n            override fun createDescription(): ContentDescription<Any> {\n                return ContentDescription(\n                    viewHolderFactory = viewHolderFactory {\n                        object : JubakoViewHolder<Any>(textView(\"こんにちはジュバコ\")) {\n                            override fun bind(data: Any?) {}\n                        }\n                    },\n                    data = EmptyLiveData()\n                )\n            }\n        })\n    }\n}) The example above produces exactly the same result as in the screenshot (see Fig. 1), here are some key differences:- We pass a `SimpleJubakoAssembler` to `load(…)`. The responsibility of an assembler is to assemble a list of rows to render in Jubako where each row is a `ContentDescriptionProvider`. The `SimpleJubakoAssembler` is really just a convenience implementation of the interface `JubakoAssembler` which provides even more lower level customisation power of Jubako’s content loading method. Instead of calling `addView { }` we add `ContentDescriptionProvider`s instead via `SimpleJubakoAssembler::add` function. A `ContentDescriptionProvider` is your chance to construct and return a `ContentDescription` that effectively describes the content and provides an interface into the contents data source via `LiveData`, in this example we don’t have any data since its a static “Hello Jubako” directly set on the text view, for that reason Jubako requires that we set it to `EmptyLiveData` (although this does not need to be done explicitly as the example illustrates, you can leave it out if you like). `viewHolderFactory { } ` allows the construction of a custom view holder for a `ContentDescription` which must extend `JubakoViewHolder` giving you access to anything view holder related which could be necessary in advanced implementations. Simple Carousels In this next example we lean once again on the convenient Jubako extension functions to build a wall of carousels. Also in this example to add some reality into the mix we will have a makeshift repository that will provide the content we wish to bind and a 500ms delay to show Jubakos loading indicator (see Fig. 2) where the spinner moves down underneath each new carousel loaded until the screen is filled with carousels. Fig. 2 To achieve this requires little code with Jubako calls to `addRecyclerView` convenience function during `load { … }` like in Ex. 3. class SimpleCarouselsActivity : AppCompatActivity() {\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_jubako_recycler)\n        // Set page size to 1 so we can see it loading (descriptions are delayed by 500ms)\n        recyclerView.withJubako(this, pageSize(1)).load {\n            (0 until 100).forEach { i ->\n                addRecyclerView(\n                    //\n                    // Inflate a view for our carousel\n                    //\n                    view = { inflater, parent ->\n                        inflater.inflate(R.layout.simple_carousel, parent, false)\n                    },\n                    //\n                    // Provide a lambda that will create our carousel item view holder\n                    //\n                    itemViewHolder = { inflater, parent, _ ->\n                        SimpleCarouselItemViewHolder(inflater, parent)\n                    },\n                    //\n                    // Specify the data that will be loaded into the carousel\n                    //\n                    data = when {\n                        i % 2 == 0 -> getNumbersEnglish()\n                        else -> getNumbersJapanese()\n                    },\n                    //\n                    // Provide a lambda that will fetch carousel item data by position\n                    //\n                    itemData = { data, position -> data[position] },\n                    //\n                    // Specify a lambda that will provide the count of item data in our carousel\n                    //\n                    itemCount = { data -> data.size },\n                    //\n                    // Specify a viewBinder that will allow binding between data and item holder\n                    //\n                    itemBinder = { holder, data ->\n                        holder.itemView.findViewById<TextView>(R.id.text).text = data\n                    }\n                )\n            }\n        }\n    }\n    inner class SimpleCarouselItemViewHolder(inflater: LayoutInflater, parent: ViewGroup) :\n        RecyclerView.ViewHolder(inflater.inflate(R.layout.simple_carousel_item_text, parent, false))\n    companion object {\n        fun getNumbersEnglish(): LiveData<List<String>> {\n            return object : LiveData<List<String>>() {\n                override fun onActive() {\n                    GlobalScope.launch(Dispatchers.IO) {\n                        delay(500)\n                        postValue(listOf(\"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\"))\n                    }\n                }\n            }\n        }\n        fun getNumbersJapanese(): LiveData<List<String>> {\n            return object : LiveData<List<String>>() {\n                override fun onActive() {\n                    GlobalScope.launch(Dispatchers.IO) {\n                        delay(500)\n                        postValue(listOf(\"ひとつ\", \"ふたつ\", \"みっつ\", \"よっつ\", \"いつつ\", \"むっつ\", \"ななつ\", \"やっつ\"))\n                    }\n                }\n            }\n        }\n    }\n} Since this example is quite large, let us break it down to see what is going on, firstly the `companion object` provides two functions that produce a `LiveData<List>` where these functions `getNumbersEnglish()` and `getNumbersJapanese()` mimic what might be the interface of some data layer (See Ex. 4). companion object {\n    fun getNumbersEnglish(): LiveData<List<String>> {\n        return object : LiveData<List<String>>() {\n            override fun onActive() {\n                GlobalScope.launch(Dispatchers.IO) {\n                    delay(500)\n                    postValue(listOf(\"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\"))\n                }\n            }\n        }\n    }\n    fun getNumbersJapanese(): LiveData<List<String>> {\n        return object : LiveData<List<String>>() {\n            override fun onActive() {\n                GlobalScope.launch(Dispatchers.IO) {\n                    delay(500)\n                    postValue(listOf(\"ひとつ\", \"ふたつ\", \"みっつ\", \"よっつ\", \"いつつ\", \"むっつ\", \"ななつ\", \"やっつ\"))\n                }\n            }\n        }\n    }\n}\n﻿ Both functions are equivalent with the only obvious difference that one returns numbers in Japanese rather than English. Both functions employ a 500ms delay and both use coroutines. What is important here is that we override `LiveData::onActive()` which is essentially our hook to load data – this is where we want to make our API calls or interact with our data layer and `postValue(…)` with our retrieved data as the payload. In the example our data is of type `List`. Next up in Ex.5, like we saw in the Hello World example we attach Jubako to a recycler using `withJubako(…)` though this time, the second argument we specify the page size using `pageSize(1)` function which effectively sets up loading to load 1 page at a time. What this actually does is instruct Jubako loading to populate the recyclerview one content description at a time until either the screen is filled vertically with items, the end of the datasource is reached or if the user scrolls down on an already populated RecyclerView then one item will be loaded until the user scrolls again. Setting the page size to one only helps us show a 500ms delay between each ContentDescription that is loaded from the assembled descriptions (assembling using JubakoAssembler is covered later). // Set page size to 1 so we can see it loading (descriptions are delayed by 500ms)\nrecyclerView.withJubako(this, pageSize(1)).load {\n    (0 until 100).forEach { i ->\n        addRecyclerView(\n            //\n            // Inflate a view for our carousel\n            //\n            view = { inflater, parent ->\n                inflater.inflate(R.layout.simple_carousel, parent, false)\n            },\n            //\n            // Provide a lambda that will create our carousel item view holder\n            //\n            itemViewHolder = { inflater, parent, _ ->\n                SimpleCarouselItemViewHolder(inflater, parent)\n            },\n            //\n            // Specify the data that will be loaded into the carousel\n            //\n            data = when {\n                i % 2 == 0 -> getNumbersEnglish()\n                else -> getNumbersJapanese()\n            },\n            //\n            // Provide a lambda that will fetch carousel item data by position\n            //\n            itemData = { data, position -> data[position] },\n            //\n            // Specify a lambda that will provide the count of item data in our carousel\n            //\n            itemCount = { data -> data.size },\n            //\n            // Specify a viewBinder that will allow binding between data and item holder\n            //\n            itemBinder = { holder, data ->\n                holder.itemView.findViewById<TextView>(R.id.text).text = data\n            }\n        )\n    }\n} In Ex. 5 using the `addRecyclerView` extension function we can simply add a number carousel rows (content descriptions), in this case we add `0 until 100` of them. The convenient `addRecyclerView` function allows us to declaratively specify many aspects of our content such as a `view` our content should be where we get an opportunity to inflate our custom view (with carousels you must have a `JubakoCarouselRecyclerView` present in your inflated layout), as well as:- itemViewHolder – where you can specify a ViewHolder to represent an item data – the `LiveData` we want to provide data for the carousel, in this case we delegate to those companion object functions from earlier, `getNumbersEnglish()` or `getNumbersJapanese()` alternating respectively. itemData – helps Jubako know how to get an item from the data, specially useful if your data source is not a list itemCount – helps Jubako know how many items are in the data, specially useful if data is not a list again! itemBinder – where you can bind the data to the holder given as arguments in the lambda You can see here that most of these things deal with how our data looks, how it is bound and what view we want to represent our data for each content description. Just like the Hello Jubako Example this example can be expanded with a `JubakoAssembler` which provides more capabilities like paginated loading as well as finer control over the creation of content descriptions. Paginated Asynchronous loading and error handling The very latest version of Jubako introduces the ability to assemble content with pagination. What this means is if you have a paginated API, or you want to load and show content in batches you can just implement `ContentAssembler::hasMore()`. When you give Jubako your `ContentAssembler` implementation it will call `ContentAssembler::assemble()` as long as `hasMore()` returns true whenever Jubako is ready to load the next batch of content (when the user scrolls to near the bottom) into the attached recyclerview. Infinite Hello Jubako To illustrate this we can refer to the Infinite Hello Jubako sample where we return pages of 10 “Hello Jubako” greetings infinitely by simply always returning true from `hasMore()`. class InfiniteHelloJubakoActivity : AppCompatActivity() {\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_jubako_recycler)\n        recyclerView.withJubako(this).load(HelloAssembler())\n    }\n    class HelloAssembler : SimpleJubakoAssembler() {\n        var counter = 0\n        //\n        // Always has more == infinite!\n        // Normally you should calculate this value, for instance\n        // start returning false when you get to the end\n        // of your data source\n        //\n        override fun hasMore() = true\n        override suspend fun onAssemble(list: MutableList<ContentDescriptionProvider<Any>>) {\n            // pages of ten\n            for (i in 0 until 10) {\n                val num = ++counter\n                list.addView { textView(it, \"Hello Jubako! $num\") }\n            }\n        }\n        private fun textView(parent: ViewGroup, text: String): TextView {\n            return TextView(parent.context).apply {\n                setText(text)\n                setTextSize(TypedValue.COMPLEX_UNIT_DIP, 48f)\n            }\n        }\n    }\n} Although there may be no practical reason to infinitely display a greeting message it would be trivial, given some condition to return false from `hasMore()` such as when there is no more data left from whichever datasource we are sourcing our data from. As our data loads a loading indicator will be displayed and if an exception is thrown from `::assemble()`, Jubako will display a retry button both of which states can be customized by providing your `ViewHolder` that implements a `ProgressView`. By default Jubako will use `DefaultProgressViewHolder` Carousel data pagination So far we have only covered the pagination feature during content assembly. For Content received as lists, we can also display as a carousel with pagination, although the implementation is slightly different. To explain how this works, in Fig. 3 we have a mockup of a carousel where the blue arrow indicates the loading direction. With carousel pagination when a carousel is fetching the next page to load Jubako will display a loading indicator, and if an error occurs a retry button. Fig. 3 To make use of carousel pagination, where `ContentDescription` provides data as `LiveData` we can assign a special type of live data `PaginatedLiveData<T>` where T represents the type of item and they are fairly easy to implement for a given carousel. To create an implementation of `PaginatedLiveData` is fairly straightforward and has only two lambdas that you need to assign, the first one is `hasMore` which tells Jubako if the paginated live data has more items to load and should return true of false respectively, Ex. 7 shows a simple example where `hasMore` will be true of the loaded data size is less than 100 items. `nextPage` returns a list with a single item “Hello”. val data = PaginatedLiveData<String> {\n    hasMore = { loaded.size < 100 }\n    nextPage = {\n        listOf(\"Hello\")\n    }\n} To see this in action we can modify the simple carousels example to add another carousel assigning this live data (Ex. 8) addRecyclerView(\n    view = { inflater, parent ->\n        inflater.inflate(R.layout.simple_carousel, parent, false)\n    },\n    data = PaginatedLiveData<String> {\n        hasMore = { loaded.size < 100 }\n        nextPage = {\n            delay(2000)\n            listOf(\"Hello\")\n        }\n    },\n    itemViewHolder = { inflater, parent, _ ->\n        SimpleCarouselItemViewHolder(inflater, parent)\n    },\n    itemBinder = { holder, data:String? ->\n        holder.itemView.findViewById<TextView>(R.id.text).text = data\n    }\n)\n﻿ In Ex. 8 we use the extension function `addRecyclerView` inside a `load { }` block to add a carousel assigning our simple `PaginatedLiveData` that will produce items of “Hello” until `hasMore` returns false. In Fig. 4 we see this where the top carousel has loaded two tiles and shows a spinner for the next loading tiles until the screen is filled horizontally the user would be required to scroll to the end before they load more. Fig. 4 Conclusion This article gave a brief introduction to Jubako and its main features. If you are building an app with a wall of carousels Jubako could help you achieve it, with less headaches due to dealing with asynchronous loading of paginated content. It takes the precise interactions of dealing with the RecyclerView API updates, letting you concentrate on the UI and plugging in your data sources. Although it still gives you the ability to get into the nuts and bolts of the RecyclerView API if you need to. There are many examples in the open source project that show the features covered here and many other features such as row updates. We would encourage you to read them before making a decision about whether Jubako is right for your project. References Project and Documentation https://github.com/justeat/jubako Hello Jubako Example https://github.com/justeat/jubako/blob/master/jubako-sample/src/main/java/com/sample/jubako/HelloJubakoActivity.kt Simple Carousels Example https://github.com/justeat/jubako/blob/master/jubako-sample/src/main/java/com/sample/jubako/SimpleCarouselsActivity.kt", "date": "2020-01-20"},
{"website": "JustTakeAway", "title": "Operating Load Testing Infrastructure at Scale", "author": ["Written by Joshua Genders"], "link": "https://tech.justeattakeaway.com/2020/01/17/operating-load-testing-infrastructure-at-scale/", "abstract": "We’ve had a lot of success with load testing and the tooling we’ve built to support it. In this post we: Give an overview how load testing supports operating at a global scale Take a quick peek at the tooling we’ve built to support our test strategy Review some of the lessons learned from operating the platform and test tooling over the last three years At Just Eat we maintain hundreds of software components, with thousands of interactions between them. We serve hundreds of thousands of customers every day, and our platform needs to be as reliable and performant as possible to ensure we can provide the services our food community rely upon. With dozens of teams releasing to this complex environment every day, we needed a way to load and performance test at a scale that mitigates the risks of large scale change in large scale environments. We maintain a set of performance tests across the organisation. These tests loosely fall into three categories: Synthetic Monitoring Our synthetic monitoring continuously executes key business paths which are monitored real-time. They provide a simulated client perspective of the platform. We’ve also found it useful to have continuous traffic on key paths in test environments. Our synthetic tests tend to align with the most crucial user journeys. Component Tests These are the more traditional style of performance tests. They executes against a single component or bounded context. These tests are the earliest to run in the software development life-cycle, providing quick feedback loops. End to End Tests These are larger, more comprehensive tests. They are fewer in number and are characterised as executing against multiple components across multiple bounded contexts. They are used to validate change that requires complex system behavior or for combinations of change across domains to ensure key business paths function at scale. These tests are more expensive to maintain but also tend to find wider-scoped issues that component level performance tests have trouble triggering such as retry-storms. The Situation We execute over 300 hours worth of these tests every day across our development, test and production environments. Orchestration of testing at that scale can become cumbersome without supporting tooling to handle the common tasks like launching and configuring load agents, deploying the test scripts and starting and stopping tests. Our original load test implementations utilised our build and deployment infrastructure to execute tests. However, the networking infrastructure in a typical build server environment isn’t designed to cope with the amount of traffic we generate. Three years ago, we decided to take the opportunity to build a solution that created better control, visibility and ease of use for our engineers, and was able to support the level of complexity in our testing that we needed. So we built Rambo We use a bespoke load testing platform we’ve dubbed ‘ Rambo ’ to manage test configuration, execution and scheduling. Rambo is a container-based scheduler that provides an opinionated implementation to help simplify configuration and uses domain specific language with a custom UI, making it more cognitively friendly to use. Rambo UI Users can manually manage test execution through the Rambo front-end UI. Tests can be grouped into suites to support larger tests that require multiple load agents or greater test coverage. Rambo integrates with our monitoring and logging infrastructure to provide live telemetry for executing tests. Rambo Architecture The UI is an Angular single-page application stored in S3 and served via Cloudfront. The API’s primary function is to serve content for the UI and invoke commands against ECS. ECS manages the container orchestration and execution. Scheduled tests and suites are triggered using CloudWatch Events. Tests are defined and configured in source control, and deployed into Amazon S3, which can then be ingested by the API and load agents. Rambo is deployed into a dedicated VPC with multiple NAT gateways and Elastic IPs. This gives us the greatest control for network configuration ensuring we can effectively maintain the network and diagnose any issues with traffic generation (such as NAT saturation). Large amounts of traffic can be risky to both our production and pre-production environments, so we’ve implemented a number of safety measures to protect our platforms from test traffic. Protected Production Testing In order to run against production we often need to bypass our DDOS protection, which forces us to mitigate the risk of DDOSing our own platform. We implement multiple safety mechanisms to help ensure execution of our load tests. PR validation Our test metadata is stored in source control. We use validation scripts to check: All configuration can be successfully parsed Required parameters are populated Test scripts don’t contain known configuration that causes memory leaks Tests are configured with the correct URLs for their target environments Tests pointing at production are using safe load levels We use a single centralised repository for configuring production tests to mitigate the risk of unknown configurations running against production. Runtime Protection Rambo makes assessments at runtime and takes/prevents actions to ensure the safety of the platform. For example: Access to Rambo is controlled through ADFS Production endpoints cannot be targeted by non-production tests Any unrecognised tasks running in ECS are killed immediately Any duplicate tasks running in ECS are killed immediately Production tests are automatically stopped when approaching peak trading periods Test Preconditions Preconditions are pre-test-execution checks which prevent tests from running if failed. For example, if the environment is not scaled sufficiently to handle the load traffic. We calculate what ‘scaled’ means dynamically by interrogating scaling configuration for our components. Human Factors In a complex system, automated safety measures cannot always be implicitly relied upon; so any large scale load tests are monitored closely by our engineers in addition to our monitoring and alerting. Our Service Operations Centre (SOC) also have an emergency kill switch in case any test traffic is suspected of causing risk to the production platform. Important events like tests starting or preconditions failing are logged and posted to a Slack channel for visibility. We also have PagerDuty alerts for our synthetic monitoring on service level indicators such as latency, order rate and error rate. One of the side-effects of successful large-scale testing is that there’s inevitably contention for shared resources such as test environments. To mitigate this we ensure that we work closely with the SOC and engineers to co-ordinate allocation of resources. Conclusion Rambo is a tool we’ve been using for the last three years and it has: Helped us ensure our features function successfully at scale Allowed us to execute load tests at a moments notice, with a click of a button Drive large scale load tests with minimal support required Enabled synthetic monitoring and continuous testing Enabled a performance-aware software development life-cycle However, when operating at scale and in production, we’ve had to approach the situation with great care. We’ve learned: Use safe defaults, reduce blast radius and guard your production environments because humans inevitably make mistakes Engineers need visibility to make good decisions, and process/constructs to support communication/collaboration Test infrastructure should be given the same attention as production infrastructure Cover your bases: Supplement human systems with automation and vice versa Self-service models allow for process scalability and improved adoption", "date": "2020-01-17"},
{"website": "JustTakeAway", "title": "Modular iOS Architecture @ Just Eat", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2019/12/18/modular-ios-architecture-just-eat/", "abstract": "The journey we took to restructure our mobile apps towards a modular architecture. Overview Modular mobile architectures have been a hot topic over the past 2 years, counting a plethora of articles and conference talks. Almost every big company promoted and discussed modularization publicly as a way to scale big projects. At Just Eat, we jumped on the modular architecture train probably before it was mainstream and, as we’ll discuss in this article, the root motivation was quite peculiar in the industry. Over the years (2016-2019), we’ve completely revamped our iOS products from the ground up and learned a lot during this exciting and challenging journey. There is so much to say about the way we structured our iOS stack that it would probably deserve a series of articles, some of which have previously been posted. Here we summarize the high-level iOS architecture we crafted, covering the main aspects in a way concise enough for the reader to get a grasp of them and hopefully learn some valuable tips. Modular Architecture Lots of information can be found online on modular architectures. In short: A modular architecture is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each one contains everything necessary to execute only one aspect of the desired functionality. Note that modular design applies to the code you own. A project with several third-party dependencies but no sensible separation for the code written by your team is not considered modular. A modular design is more about the principle rather than the specific technology. One could achieve it in a variety of ways and with different tools. Here are some key points and examples that should inform the decision of the ifs and the hows of implementing modularization: Business reasons The company requires that parts of the codebase are reused and shared across projects, products, and teams; The company requires multiple products to be unified into a single one. Tech reasons The codebase has grown to a state where things become harder and harder to maintain and to iterate over; Development is slowed down due to multiple developers working on the same monolithic codebase; Besides reusing code, you need to port functionalities across projects/products. Multiple teams The company structured teams following strategic models (e.g. Spotify model) and functional teams only work on a subset of the final product; Ownership of small independent modules distributed across teams enables faster iterations; The much smaller cognitive overhead of working on a smaller part of the whole product can vastly simplify the overall development. Pre-existing knowledge Members of the team might already be familiar with specific solutions (Carthage, CocoaPods, Swift Package Manager, manual frameworks setup within Xcode). In the case of a specific familiarity with a system, it’s recommended to start with it since all solutions come with pros and cons and there’s not a clear winner at the time of writing. Modularizing code (if done sensibly) is almost always a good thing: it enforces separation of concerns, keeps complexity under control, allows faster development, etc. It has to be said that it’s not necessarily what one needs for small projects and its benefits become tangible only after a certain complexity threshold is crossed. Journey to a new architecture In 2014, Just Eat was a completely different environment from today and back then the business decided to split the tech department into separate departments: one for UK and one for the other countries. While this was done with the best intentions to allow faster evolution in the main market (UK), it quickly created a hard division between teams, services, and people. In less than 6 months, the UK and International APIs and consumer clients deeply diverged introducing country-specific logic and behaviors. By mid-2016 the intent of “merging back” into a single global platform was internally announced and at that time it almost felt like a company acquisition. This is when we learned the importance of integrating people before technology. The teams didn’t know each other very well and became reasonably territorial on their codebase. It didn’t help that the teams span multiple cities. It’s understandable that getting to an agreement on how going back to a single, global, and unified platform took months. The options we considered spanned from rewriting the product from scratch to picking one of the two existing ones and make it global. A complete rewrite would have eventually turned out to be a big-bang release with the risk of regressions being too high; not something sensible or safe to pursue. Picking one codebase over the other would have necessarily let down one of the two teams and caused the re-implementation of some missing features present in the other codebase. At that time, the UK project was in a better shape and new features were developed for the UK market first. The international project was a bit behind due to the extra complexity of supporting multiple countries and features being too market-specific. During that time, the company was also undergoing massive growth and with multiple functional teams having been created internally, there was an increasing need to move towards modularization. Therefore, we decided to gradually and strategically modularize parts of the mobile products and onboard them onto the other codebase in a controlled and safe way. In doing so, we took the opportunity to deeply refactor and, in the vast majority of the cases, rewrite parts in their entirety enabling new designs, better tests, higher code coverage, and – holistically – a fully Swift codebase. We knew that the best way to refactor and clean up the code was by following a bottom-up approach. We started with the foundations to solve small and well-defined problems – such as logging, tracking, theming – enabling the team to learn to think modular . We later moved to isolating big chunks of code into functional modules to be able to onboard them into the companion codebase and ship them on a phased rollout. We soon realized we needed a solid engine to handle run-time configurations and remote feature flagging to allow switching ON and OFF features as well as entire modules. As discussed in a previous article , we developed JustTweak to achieve this goal. At the end of the journey, the UK and the International projects would look very similar, sharing a number of customizable modules, and differing only in the orchestration layer in the apps. The Just Eat iOS apps are far bigger and more complex than they might look at first glance. Generically speaking, merging different codebases takes orders of magnitude longer than separating them, and for us, it was a process that took over 3 years, being possible thanks to unparalleled efforts of engineers brought to work together. Over this time, the whole team learned a lot, from the basics of developing code in isolation to how to scale a complex system. Holistic Design 🤘 The following diagram outlines the modular architecture in its entirety as it is at the time of writing this article (December 2019). We can appreciate a fair number of modules clustered by type and the different consumer apps. Whenever possible, we took the opportunity to abstract some modules having them in a state that allows open-sourcing the code. All of our open-source modules are licensed under Apache 2 and can be found at github.com/justeat . Apps Due to the history of Just Eat described above, we build different apps per country per brand from different codebases All the modularization work we did bottom-up brought us to a place where the apps differ only in the layer orchestrating the modules. With all the consumer-facing features been moved to the domain modules, there is very little code left in the apps. Domain Modules Domain modules contain features specific to an area of the product. As the diagram above shows, the sum of all those parts makes up the Just Eat apps. These modules are constantly modified and improved by our teams and updating the consumer apps to use newer versions is an explicit action. We don’t particularly care about backward compatibility here since we are the sole consumers and it’s common to break the public interface quite often if necessary. It might seem at first that domain modules should depend on some Core modules (e.g. APIClient) but doing so would complicate the dependency tree as we’ll discuss further in the “Dependency Management” section of this article. Instead, we inject core modules’ services, simply making them conformant to protocols defined in the domain module.  In this way, we maintain a good abstraction and avoid tangling the dependency graph. Core & Shared modules The Core and Shared modules represent the foundations of our stack, things like: custom UI framework theming engine logging, tracking, and analytics libraries test utilities client for all the Just Eat APIs feature flagging and experimentation engine and so forth. These modules – which are sometimes also made open-source – should not change frequently due to their nature. Here backward compatibility is important and we deprecate old APIs when introducing new ones. Both apps and domain modules can have shared modules as dependencies, while core modules can only be used by the apps. Updating the backbone of a system requires the propagation of the changes up in the stack (with its maintenance costs) and for this reason, we try to keep the number of shared modules very limited. Structure of a module As we touched on in previous articles, one of our fundamental principles is “ always strive to find solutions to problems that are scalable and hide complexity as much as possible “. We are almost obsessed with making things as simple as they can be. When building a module, our root principle is: Every module should be well tested, maintainable, readable, easily pluggable, and reasonably documented. The order of the adjectives implies some sort of priority. First of all, the code must be unit tested , and in the case of domain modules, UI tests are required too. Without reasonable code coverage, no code is shipped to production. This is the first step to code maintainability , where maintainable code is intended as “code that is easy to modify or extend”. Readability is down to reasonable design, naming convention, coding standards, formatting, and all that jazz. Every module exposes a Facade that is very succinct, usually no more than 200 lines long. This entry point is what makes a module easily pluggable . In our module blueprint, the bare minimum is a combination of a facade class, injected dependencies, and one or more configuration objects driving the behavior of the module (leveraging the underlying feature flagging system powered by JustTweak discussed in a previous article ). The facade should be all a developer needs to know in order to consume a module without having to look at implementation details. Just to give you an idea, here is an excerpt from the generated public interface of the Account module (not including the protocols): public typealias PasswordManagementService = ForgottenPasswordServiceProtocol & ResetPasswordServiceProtocol\npublic typealias AuthenticationService = LoginServiceProtocol & SignUpServiceProtocol & PasswordManagementService & RecaptchaServiceProtocol\npublic typealias UserAccountService = AccountInfoServiceProtocol & ChangePasswordServiceProtocol & ForgottenPasswordServiceProtocol & AccountCreditServiceProtocol\npublic class AccountModule {\n    public init(settings: Settings,\n                authenticationService: AuthenticationService,\n                userAccountService: UserAccountService,\n                socialLoginServices: [SocialLoginService],\n                userInfoProvider: UserInfoProvider) \n    public func startLogin(on viewController: UIViewController) -> FlowCoordinator\n    public func startResetPassword(on viewController: UIViewController, token: Token) -> FlowCoordinator\n    public func startAccountInfo(on navigationController: UINavigationController) -> FlowCoordinator\n    public func startAccountCredit(on navigationController: UINavigationController) -> FlowCoordinator\n    public func loginUsingSharedWebCredentials(handler: @escaping (LoginResult) -> Void)\n} We believe code should be self-descriptive and we tend to put comments only on code that really deserves some explanation, very much embracing John Ousterhout’s approach described in A Philosophy of Software Design . Documentation is mainly relegated to the README file and we treat every module as if it was an open-source project: the first thing consumers would look at is the README file, and so we make it as descriptive as possible. Overall design We generate all our modules using CocoaPods via $ pod lib create which creates the project with a standard template generating the Podfile, podspec, and demo app in a breeze. The podspec could specify additional dependencies (both third-party and Core modules) that the demo app’s Podfile could specify core modules dependencies alongside the module itself which is treated as a development pod as per standard setup. The backbone of the module, which is the framework itself, encompasses both business logic and UI meaning that both source and asset files are part of it. In this way, the demo apps are very much lightweight and only showcase module features that are implemented in the framework. The following diagram should summarize it all. Demo Apps Every module comes with a demo app we give particular care to. Demo apps are treated as first-class citizens and the stakeholders are both engineers and product managers. They massively help to showcase the module features – especially those under development – vastly simplify collaboration across Engineering, Product, and Design, and force a good mock-based test-first approach. Following is a SpringBoard page showing our demo apps, very useful to individually showcase all the functionalities implemented over time, some of which might not surface in the final product to all users. Some features are behind experiments, some still in development, while others might have been retired but still present in the modules. Every demo app has a main menu to: access the features force a specific language toggle configuration flags via JustTweak customize mock data We show the example of the Account module demo app on the right. Internal design It’s worth noting that our root principle mentioned above does not include any reference to the internal architecture of a module and this is intentional. It’s common for iOS teams in the industry to debate on which architecture to adopt across the entire codebase but the truth is that such debate aims to find an answer to a non-existing problem. With an increasing number of modules and engineers, it’s fundamentally impossible to align on a single paradigm shared and agreed upon by everyone. Betting on a single architectural design would ultimately let down some engineers who would complain down the road that a different design would have played out better. We decided to stick with the following rule of thumb: Developers are free to use the architectural design they feel would work better for a given problem. This approach brought us to have a variety of different designs – spanning from simple old-school MVC, to a more evolved VIPER – and we constantly learn from each other’s code. What’s important at the end of the day is that techniques such as inversion of control, dependency injection, and more generally the SOLID principles, are used appropriately to embrace our root principle. Dependency Management We rely heavily on CocoaPods since we adopted it in the early days as it felt like the best and most mature choice at the time we started modularizing our codebase. We think this still holds at the time of writing this article but we can envision a shift to SPM (Swift Package Manager) in 1-2 years time. With a growing number of modules, comes the responsibility of managing the dependencies between them. No panacea can cure dependency hell , but one should adopt some tricks to keep the complexity of the stack under reasonable control. Here’s a summary of what worked for us: Always respect semantic versioning ; Keep the dependency graph as shallow as possible. From our apps to the leaves of the graph there are no more than 2 levels; Use a minimal amount of shared dependencies. Be aware that every extra level with shared modules brings in higher complexity; Reduce the number of third-party libraries to the bare minimum. Code that’s not written and owned by your team is not under your control; Never make modules within a group (domain, core, shared) depend on other modules of the same group; Automate the publishing of new versions. When a pull request gets merged into the master branch, it must also contain a version change in the podspec. Our continuous integration system will automatically validate the podspec, publish it to our private spec repository, and in just a matter of minutes the new version becomes available; Fix the version for dependencies in the Podfile. Whether it is a consumer app or a demo app, we want both our modules and third-party libraries not to be updated unintentionally. It’s acceptable to use the optimistic operator for third-party libraries to allow automatic updates of new patch versions; Fix the version for third-party libraries in the modules’ podspec. This guarantees that modules’ behavior won’t change in the event of changes in external libraries. Failing to do so would allow defining different versions in the app’s Podfile, potentially causing the module to not function correctly or even to not compile; Do not fix the version for shared modules in the modules’ podspec. In this way, we let the apps define the version in the Podfile, which is particularly useful for modules that change often, avoiding the hassle of updating the version of the shared modules in every podspec referencing it. If a new version of a shared module is not backward compatible with the module consuming it, the failure would be reported by the continuous integration system as soon as a new pull request gets raised. A note on the Monorepo approach When it comes to dependency management it would be unfair not to mention the opinable monorepo approach. Monorepos have been discussed quite a lot by the community to pose a remedy to dependency management (de facto ignoring it), some engineers praise them, others are quite contrary. Facebook, Google, and Uber are just some of the big companies known to have adopted this technique, but in hindsight, it’s still unclear if it was the best decision for them. In our opinion, monorepos can sometimes be a good choice. For example, in our case, a great benefit a monorepo would give us is the ability to prepare a single pull request for both implementing a code change in a module and integrating it into the apps. This will have an even greater impact when all the Just Eat consumer apps are globalized into a single codebase. Onwards and upwards Modularizing the iOS product has been a long journey and the learnings were immense. All in all, it took more than 3 years, from May 2016 to October 2019, always balancing tech and product improvements. Our natural next step is unifying the apps into a single global project, migrating the international countries over to the UK project to ultimately reach the utopian state of having a single global app. All the modules have been implemented in a fairly abstract way and following a white labeling approach, allowing us to extend support to new countries and onboard acquired companies in the easiest possible way.", "date": "2019-12-18"},
{"website": "JustTakeAway", "title": "Lessons learned from handling JWT on mobile", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2019/12/04/lessons-learned-from-handling-jwt-on-mobile/", "abstract": "Overview Modern mobile apps are more complicated than they used to be back in the early days and developers have to face a variety of interesting problems. While we’ve put in our two cents on some of them in previous articles, this one is about authorization and what we have learned by handling JWT on mobile at Just Eat. When it comes to authorization, it’s standard practice to rely on OAuth 2.0 and the companion JWT (JSON Web Token). We found this important topic was rarely discussed online while much attention was given to new proposed implementations of network stacks, maybe using recent language features or frameworks such as Combine. We’ll illustrate the problems we faced at Just Eat for JWT parsing, usage, and (most importantly) refreshing. You should be able to learn a few things on how to make your app more stable by reducing the chance of unauthorized requests allowing your users to virtually always stay logged in. What is JWT JWT stands for JSON Web Token and is an open industry standard used to represent claims transferred between two parties. A signed JWT is known as a JWS (JSON Web Signature), in fact, a JWT has either to be JWS or JWE (JSON Web Encryption). RFC 7515 , RFC 7516 , and RFC 7519 describe the various fields and claims in detail. What is relevant for mobile developers is the following: JWT is composed of 3 parts dot-separated: Header, Payload, Signature. The Payload is the only relevant part. The Header identifies which algorithm is used to generate the signature. There are reasons for not verifying the signature client-side making the Signature part irrelevant too. JWT has an expiration date. Expired tokens should be renewed/refreshed. JWT can contain any number of extra information specific to your service. It’s common practice to store JWTs in the app keychain. Here is a valid and very short token example, courtesy of jwt.io which we recommend using to easily decode tokens for debugging purposes. It shows 3 fragments (base64 encoded) concatenated with a dot. eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyLCJleHAiOjE1Nzc3NTA0MDB9.7hgBhNK_ZpiteB3GtLh07KJ486Vfe3WAdS-XoDksJCQ The only field relevant to this document is exp (Expiration Time), part of Payload (the second fragment). This claim identifies the time after which the JWT must not be accepted. In order to accept a JWT, it’s required that the current date/time must be before the expiration time listed in the `exp` claim. It’s accepted practice for implementers to consider for some small leeway, usually no more than a few minutes, to account for clock skew. N.B. Some API calls might demand the user to be logged in (user-authenticated calls), and others don’t (non-user-authenticated calls). JWT can be used in both cases, marking a distinction between Client JWT and User JWT we will refer to later on. The token refresh problem By far the most significant problem we had in the past was the renewal of the token. This seems to be something taken for granted by the mobile community, but in reality, we found it to be quite a fragile part of the authentication flow. If not done right, it can easily cause your customers to end up being logged out , with the consequent frustration we all have experienced as app users. The Just Eat app makes multiple API calls at startup: it fetches the order history to check for in-flight orders, fetches the most up-to-date consumer details, etc. If the token is expired when the user runs the app, a nasty race condition could cause the same refresh token to be used twice, causing the server to respond with a 401 and subsequently logging the user out on the app. This can also happen during normal execution when multiple API calls are performed very close to each other and the token expires prior to those. It gets trickier if the client and the server clocks are sensibly off sync: while the client might believe to be in possession of a valid token, it has already expired. The following diagram should clarify the scenario. Common misbehavior I couldn’t find a company (regardless of size) or indie developer who had implemented a reasonable token refresh mechanism. The common approach seems to be: to refresh the token whenever an API call fails with 401 Unauthorized. This is not only causing an extra call that could be avoided by locally checking if the token has expired, but it also opens the door for the race condition illustrated above. Avoid race conditions when refreshing the token 🚦 We’ll explain the solution with some technical details and code snippets but what’s more important is that the reader understands the root problem we are solving and why it should be given the proper attention. The more we thought about it, we more we convinced ourselves that the best way to shield ourselves from race conditions was by using threading primitives when scheduling async requests to fetch a valid token. This means that all the calls would be regulated via a filter that would hold off subsequent calls to fire until a valid token is retrieved, either from local storage or, if a refresh is needed, from the remote OAuth server. We’ll show examples for iOS, so we’ve chosen dispatch queues and semaphores (using GCD ); fancier and more abstract ways of implementing the solution might exist – in particular by leveraging modern FRP techniques – but ultimately the same primitives are used. For simplicity, let’s assume that only user-authenticated API requests need to provide a JWT, commonly put in the Authorization header: Authorization: Bearer <jwt-token> The code below implements the “Get valid JWT” box from the following flowchart. The logic within this section is the one that must be implemented in mutual exclusion, in our solution, by using the combination of a serial queue and a semaphore. Here is just the minimum amount of code (Swift) needed to explain the solution. typealias Token = String\ntypealias AuthorizationValue = String\nstruct UserAuthenticationInfo {\n    let bearerToken: Token // the JWT\n    let refreshToken: Token\n    let expiryDate: Date // computed on creation from 'exp' claim\n    var isValid: Bool {\n        return expiryDate.compare(Date()) == .orderedDescending\n    }\n }\nprotocol TokenRefreshing {\n    func refreshAccessToken(_ refreshToken: Token, completion: @escaping (Result<UserAuthenticationInfo, Error>) -> Void)\n}\nprotocol AuthenticationInfoStorage {\n    var userAuthenticationInfo: UserAuthenticationInfo?\n    func persistUserAuthenticationInfo(_ authenticationInfo: UserAuthenticationInfo?)\n    func wipeUserAuthenticationInfo()\n} class AuthorizationValueProvider {\n    private let authenticationInfoStore: AuthenticationInfoStorage\n    private let tokenRefreshAPI: TokenRefreshing\n   \n    private let queue = DispatchQueue(label: <#label#>, qos: .userInteractive)\n    private let semaphore = DispatchSemaphore(value: 1)\n    init(tokenRefreshAPI: TokenRefreshing, authenticationInfoStore: AuthenticationInfoStorage) {\n        self.tokenRefreshAPI = tokenRefreshAPI\n        self.authenticationInfoStore = authenticationInfoStore\n    }\n    func getValidUserAuthorization(completion: @escaping (Result<AuthorizationValue, Error>) -> Void) {\n        queue.async {\n            self.getValidUserAuthorizationInMutualExclusion(completion: completion)\n        }\n    }\n} Before performing any user-authenticated request, the network client asks an AuthorizationValueProvider instance to provide a valid user Authorization value (the JWT). It does so via the async method getValidUserAuthorization which uses a serial queue to handle the requests. The chunky part is the getValidUserAuthorizationInMutualExclusion. private func getValidUserAuthorizationInMutualExclusion(completion: @escaping (Result<AuthorizationValue, Error>) -> Void) {\n    semaphore.wait()\n    guard let authenticationInfo = authenticationInfoStore.userAuthenticationInfo else {\n        semaphore.signal()\n        let error = // forge an error for 'missing authorization'\n        completion(.failure(error))\n        return\n    }\n    if authenticationInfo.isValid {\n        semaphore.signal()\n        completion(.success(authenticationInfo.bearerToken))\n        return\n    }\n    tokenRefreshAPI.refreshAccessToken(authenticationInfo.refreshToken) { result in\n        switch result {\n        case .success(let authenticationInfo):\n            self.authenticationInfoStore.persistUserAuthenticationInfo(authenticationInfo)\n            self.semaphore.signal()\n            completion(.success(authenticationInfo.bearerToken))\n        case .failure(let error) where error.isClientError:\n            self.authenticationInfoStore.wipeUserAuthenticationInfo()\n            self.semaphore.signal()\n            completion(.failure(error))\n        case .failure(let error):\n            self.semaphore.signal()\n            completion(.failure(error))\n        }\n    }\n } The method could fire off an async call to refresh the token, and this makes the usage of the semaphore crucial. Without it, the next request to AuthorizationValueProvider would be popped from the queue and executed before the remote refresh completes. The semaphore is initialised with a value of 1, meaning that only one thread can access the critical section at a given time. We make sure to call wait at the beginning of the execution and to call signal only when we have a result and therefore ready to leave the critical section. If the token found in the local store is still valid, we simply return it, otherwise, it’s time to request a new one. In the latter case, if all goes well, we persist the token locally and allow the next request to access the method, in the case of an error, we should be careful and wipe the token only if the error is a legit client error (2xx range). This includes also the usage of a refresh token that is not valid anymore, which could happen, for instance, if the user resets the password on another platform/device. It’s critical to not delete the token from the local store in the case of any other error, such as 5xx or the common Foundation’s NSURLErrorNotConnectedToInternet (-1009), or else the user would unexpectedly be logged out. It’s also important to note that the same AuthorizationValueProvider instance must be used by all the calls: using different ones would mean using different queues making the entire solution ineffective. It seemed clear that the network client we developed in-house had to embrace JWT refresh logic at its core so that all the API calls, even new ones that will be added in the future would make use of the same authentication flow. General recommendations Here are a couple more (minor) suggestions we thought are worth sharing since they might save you implementation time or influence the design of your solution. Correctly parse the Payload Another problem – even though quite trivial and that doesn’t seem to be discussed much – is the parsing of the JWT, that can fail in some cases. In our case, this was related to the base64 encoding function and “adjusting” the base64 payload to be parsed correctly. In some implementations of base64, the padding character is not needed for decoding, since the number of missing bytes can be calculated but in Foundation’s implementation it is mandatory. This caused us some head-scratching and this StackOverflow answer helped us. The solution is – more officially – stated in RFC 7515 – Appendix C and here is the corresponding Swift code: func base64String(_ input: String) -> String {\n    var base64 = input\n        .replacingOccurrences(of: \"-\", with: \"+\")\n        .replacingOccurrences(of: \"_\", with: \"/\")\n    switch base64.count % 4 {\n    case 2:\n         base64 = base64.appending(\"==\")\n   \n    case 3:\n         base64 = base64.appending(\"=\")\n    default:\n         break\n    } \n     return base64\n } The majority of the developers rely on external libraries to ease the parsing of the token, but as we often do, we have implemented our solution from scratch, without relying on a third-party library. Nonetheless, we feel JSONWebToken by Kyle Fuller is a very good one and it seems to implement JWT faithfully to the RFC, clearly including the necessary base64 decode function . Handle multiple JWT for multiple app states As previously stated, when using JWT as an authentication method for non-user- authenticated calls, we need to cater for at least 3 states, shown in the following enum: enum AuthenticationStatus {\n     case notAuthenticated\n     case clientAuthenticated\n     case userAuthenticated\n} On a fresh install, we can expect to be in the .notAuthenticated state, but as soon as the first API call is ready to be performed, a valid Client JWT has to be fetched and stored locally (at this stage, other authentication mechanisms are used, most likely Basic Auth), moving to the .clientAuthenticated state. Once the user completes the login or signup procedure, a User JWT is retrieved and stored locally (but separately to the Client JWT), entering the .userAuthenticated, so that in the case of a logout we are left with a (hopefully still valid) Client JWT. In this scenario, almost all transitions are possible: A couple of recommendations here: if the user is logged in is important to use the User JWT also for the non-user-authenticated calls as the server may personalise the response (e.g. the list of restaurants in the Just Eat app) store both Client and User JWT, so that if the user logs out, the app is left with the Client JWT ready to be used to perform non-user-authenticated requests, saving an unnecessary call to fetch a new token Conclusion In this article, we’ve shared some learnings from handling JWT on mobile that are not commonly discussed within the community. As a good practice, it’s always best to hide complexity and implementation details. Baking the refresh logic described above within your API client is a great way to avoid developers having to deal with complex logic to provide authorization, and enables all the API calls to undergo the same authentication mechanism. Consumers of an API client, should not have the ability to gather the JWT as it’s not their concern to use it or to fiddle with it. We hope this article helps to raise awareness on how to better handle the usage of JWT on mobile applications, in particular making sure we always do our best to avoid accidental logouts to provide a better user experience.", "date": "2019-12-04"},
{"website": "JustTakeAway", "title": "A Smart Feature Flagging System for iOS", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2019/11/26/a-smart-feature-flagging-system-for-ios/", "abstract": "Overview At Just Eat we have experimentation at our heart, and it is very much dependent on feature flagging/toggling. If we may be so bold, here’s an analogy: feature flagging is to experimentation as machine learning is to AI, you cannot have the second without the first one. We’ve developed an in-house component, named JustTweak, to handle feature flags and experiments on iOS without the hassle. We open-sourced JustTweak on github.com in 2017 and we have been evolving it ever since; in particular, with support for major experimentation platforms such as Optimizely and Firebase Remote Config . JustTweak has been instrumental in evolving the consumer Just Eat app in a fast and controlled manner, as well as to support a large number of integrations and migrations happening under the hood. In this article, we describe the feature flagging architecture and engine, with code samples and integration suggestions. What is feature flagging Feature flagging, in its original form, is a software development technique that provides an alternative to maintaining multiple source-code branches, so that a feature can be tested even before it is completed and ready for release. Feature flags are used in code to show/hide or enable/disable specific features at runtime. The technique also allows developers to release a version of a product that has unfinished features, that can be hidden from the user. Feature toggles also allow shorter software integration cycles and small incremental versions of software to be delivered without the cost of constant branching and merging – needless to say, this is crucial to have on iOS due to the App Store review process not allowing continuous delivery. A boolean flag in code is used to drive what code branch will run, but the concept can easily be extended to non-boolean flags, making them more of configuration flags that drive behavior. As an example, at Just Eat we have been gradually rewriting the whole application over time, swapping and customizing entire modules via configuration flags, allowing gradual switches from old to new features in a way transparent to the user. Throughout this article, the term ‘tweaks’ is used to refer to feature/configuration flags. A tweak can have a value of different raw types, namely `Bool`, `String`, `Int`, `Float`, and `Double`. Boolean tweaks can be used to drive features, like so: let isFeatureXEnabled: Bool = ...\nif isFeatureXEnabled {\n    // show feature X\n}\nelse {\n    // don't show feature X\n} Other types of tweaks are instead useful to customise a given feature. Here is an example of configuring the environment using tweaks: let publicApiHost: String = ...\nlet publicApiPort: Int? = ...\nlet endpoint = Endpoint(scheme: \"https\",\n                        host: publicApiHost,\n                        port: publicApiPort,\n                        path: \"/restaurant/:id/menu\")\n// perform a request using the above endpoint object Problem The crucial part to get right is how and from where the flag values (isFeatureXEnabled, publicApiHost, and publicApiPort in the examples above) are fetched. Every major feature flagging/experimentation platform in the market provides its own way to fetch the values, and sometimes the APIs to do so significantly differ (e.g. Firebase Remote Config vs. Optimizely ). Aware of the fact that it’s increasingly difficult to build any kind of non-trivial app without leveraging external dependencies, it’s important to bear in mind that external dependencies pose a great threat to the long term stability and viability of any application. Following are some issues related to third-party experimentation solutions: – third-party SDKs are not under your control – using third-party SDKs in a modular architected app would easily cause dependency hell – third-party SDKs are easily abused and various areas of your code will become entangled with them – your company might decide to move to a different solution in the future and such switch comes with costs – depending on the adopted solution, you might end up tying your app more and more to the platform-specific features that don’t find correspondence elsewhere – it is very hard to support multiple feature flag providers For the above reasons, it is best to hide third-party SDKs behind some sort of a layer and to implement an orchestration mechanism to allow fetching of flag values from different providers. We’ll describe how we’ve achieved this in JustTweak. A note on the approach When designing software solutions, a clear trait was identified over time in the iOS team, which boils down to the kind of mindset and principle been used: Always strive to find solutions to problems that are scalable and hide complexity as much as possible. One word you would often hear if you were to work in the iOS team is ‘ Facade ‘, which is a design pattern that serves as a front-facing interface masking more complex underlying or structural code. Facades are all over the place in our code: we try to keep components’ interfaces as simple as possible so that other engineers could utilize them with minimal effort without necessarily knowing the implementation details. Furthermore, the more succinct an interface is, the rarer the possibility of misusages would be. We have some open source components embracing this approach, such as JustPersist , JustLog , and JustTrack . JustTweak makes no exception and the code to integrate it successfully in a project is minimal. Sticking to the above principle, the idea behind JustTweak is to have a single entry point to gather flag values, hiding the implementation details regarding which source the flag values are gathered from. JustTweak to the rescue JustTweak provides a simple facade interface interacting with multiple configurations that are queried respecting a certain priority. Configurations wrap specific sources of tweaks, that are then used to drive decisions or configurations in the client code. You can find JustTweak on CocoaPods and it’s on version 5.0.0 at the time of writing. We plan to add support for Carthage and Swift Package Manager in the future. A demo app is also available for you to try it out. With JustTweak you can achieve the following: – use a JSON local configuration providing default tweak values – use a number of remote configuration providers, such as Firebase and Optmizely, to run A/B tests and feature flagging – enable, disable, and customize features locally at runtime – provide a dedicated UI for customization (this comes particularly handy for features that are under development to showcase the progress to stakeholders) Here is a screenshot of the `TweakViewController` taken from the demo app. Tweak values changed via this screen are immediately available to your code at runtime. Stack setup The facade class previously mentioned is represented by the TweakManager. There should only be a single instance of the manager, ideally configured at startup, passed around via dependency injection, and kept alive for the whole lifespan of the app. Following is an example of the kind of stack implemented as a static let. static let tweakManager: TweakManager = {\n    // mutable configuration (to override tweaks from other configurations)\n    let userDefaultsConfiguration = UserDefaultsConfiguration(userDefaults: .standard) \n    // remote configurations (optional)\n    let optimizelyConfiguration = OptimizelyConfiguration()\n    let firebaseConfiguration = FirebaseConfiguration() \n    // local JSON configuration (default tweaks)\n    let jsonFileURL = Bundle.main.url(forResource: \"ExampleConfiguration\", withExtension: \"json\")!\n    let localConfiguration = LocalConfiguration(jsonURL: jsonFileURL)   \n   \n    // priority is defined by the order in the configurations array (from highest to lowest)\n    let configurations: [Configuration] = [userDefaultsConfiguration,\n                                           optimizelyConfiguration,\n                                           firebaseConfiguration,\n                                           localConfiguration]\n    return TweakManager(configurations: configurations)\n}() {\n    \"ui_customization\": {\n        \"display_red_view\": {\n            \"Title\": \"Display Red View\",\n            \"Description\": \"shows a red view in the main view controller\",\n            \"Group\": \"UI Customization\",\n            \"Value\": false\n        },\n        ...\n        \"red_view_alpha_component\": {\n            \"Title\": \"Red View Alpha Component\",\n            \"Description\": \"defines the alpha level of the red view\",\n            \"Group\": \"UI Customization\",\n            \"Value\": 1.0\n        },\n        \"label_text\": {\n            \"Title\": \"Label Text\",\n            \"Description\": \"the title of the main label\",\n            \"Group\": \"UI Customization\",\n            \"Value\": \"Test value\"\n        }\n    },\n    \"general\": {\n        \"greet_on_app_did_become_active\": {\n            \"Title\": \"Greet on app launch\",\n            \"Description\": \"shows an alert on applicationDidBecomeActive\",\n            \"Group\": \"General\",\n            \"Value\": false\n        },\n        ...\n    }\n} JustTweak comes with three configurations out-of-the-box: – ‘UserDefaultsConfiguration’ which is mutable and uses UserDefaults as a key/value store – ‘LocalConfiguration’ which is read-only and uses a JSON configuration file that is meant to be the default configuration – ‘EphemeralConfiguration’ which is simply an instance of NSMutableDictionary Besides, JustTweak defines `Configuration` and `MutableConfiguration` protocols you can implement to create your own configurations to fit your needs. In the example project, you can find a few example configurations which you can use as a starting point. You can have any source of flags via wrapping it in a concrete implementation of the above protocols. Since the protocol methods are synchronous, you’ll have to make sure that the underlying source has been initialised as soon as possible at startup. All the experimentation platforms provide mechanisms to do so, for example here is how Optimizely does it. The order of the objects in the configurations array defines the configurations’ priority. The MutableConfiguration with the highest priority, such as UserDefaultsConfiguration in the example above, will be used to reflect the changes made in the UI (TweakViewController). The LocalConfiguration should have the lowest priority as it provides the default values from a local JSON file. It’s also the one used by the TweakViewController to populate the UI. When fetching a tweak, the engine will inspect the chain of configurations in order and pick the tweak from the first configuration having it. The following diagram outlines a possible setup where values present in Optimizely override others in the subsequent configurations. Eventually, if no override is found, the local configuration would return the default tweak baked in the app. Structuring the stack this way brings various advantages: – the same engine is used to customise the app for development, production, and test runs – consumers only interface with the facade and can ignore the implementation details – new code put behind flags can be shipped with confidence since we rely on a tested engine – ability to remotely override tweaks de facto allowing to greatly customise the app without the need for a new release TweakManager gets populated with the tweaks listed in the JSON file used as backing store of the LocalConfiguration instance. It is therefore important to list every supported tweak in there so that development builds of the app can allow tweaking the values. Here is an excerpt from the file used in the `TweakViewController` screenshot above. Testing considerations We’ve seen that the described architecture allows customization via configurations. We’ve shown in the above diagram that JustTweak can come handy when used in conjunction with our AutomationTools framework too, which is open-source. An Ephemeral configuration would define the app environment at run-time greatly simplifying the implementation of UI tests, which is well-known to be a tedious activity. Usage The two main features of JustTweak can be accessed from the TweakManager. 1. Checking if a feature is enabled // check for a feature to be enabled\nlet isFeatureXEnabled = tweakManager.isFeatureEnabled(\"feature_X\")\nif isFeatureXEnabled {\n    // show feature X\n} else {\n    // hide feature X\n} 2. Getting and setting the value of a flag for a given feature/variable. JustTweak will return the value from the configuration with the highest priority that provides it, or nil if none of the configurations have that feature/variable. // check for a tweak value\nlet tweak = tweakManager.tweakWith(feature: <#feature_key#>, variable: <#variable_key#>\")\nif let tweak = tweak {\n    // tweak was found in some configuration, use tweak.value\n}\nelse {\n    // tweak was not found in any configuration\n} The `Configuration` and MutableConfiguration protocols define the following methods: func tweakWith(feature: String, variable: String) -> Tweak?\nfunc set(_ value: TweakValue, feature: String, variable: String)\nfunc deleteValue(feature: String, variable: String) You might wonder why is there a distinction between feature and variable. The reason is that we want to support the Optimizely lingo for features and related variables and therefore the design of JustTweak has to necessarily reflect that. Other experimentation platforms (such as Firebase) have a single parameter key, but we had to harmonise for the most flexible platform we support. Property Wrappers With SE-0258 , Swift 5.1 introduces Property Wrappers. If you haven’t read about them, we suggest you watch the WWDC 2019 Modern Swift API Design talk where Property Wrappers are explained starting at 23:11. In short, a property wrapper is a generic data structure that encapsulates read/write access to a property while adding some extra behavior to augment its semantics. Common examples are `@AtomicWrite` and `@UserDefault` but more creative usages are up for grabs and we couldn’t help but think of how handy it would be to have property wrappers for feature flags, and so we implemented them . ‘@TweakProperty’ and ‘@OptionalTweakProperty’ are available to mark properties representing feature flags. Here are a couple of examples, making the code so much nicer than before. @TweakProperty(fallbackValue: <#default_value#>,\n               feature: <#feature_key#>,\n               variable: <#variable_key#>,\n               tweakManager: tweakManager)\nvar isFeatureXEnabled: Bool\n@TweakProperty(fallbackValue: <#default_value#>,\n               feature: <#feature_key#>,\n               variable: <#variable_key#>,\n               tweakManager: tweakManager)\nvar publicApiHost: String\n@OptionalTweakProperty(fallbackValue: <#default_value_or_nil#>,\n                       feature: <#feature_key#>,\n                       variable: <#variable_key#>,\n                       tweakManager: tweakManager)\nvar publicApiPort: Int? Mind that by using these property wrappers, a static instance of TweakManager must be available. Update a configuration at runtime JustTweak comes with a ViewController that allows the user to edit the tweaks while running the app. That is achieved by using the `MutableConfiguration` with the highest priority from the configurations array. This is de facto a debug menu, useful for development and internal builds but not to include in release builds. #if DEBUG\nfunc presentTweakViewController() {\n    let tweakViewController = TweakViewController(style: .grouped, tweakManager: tweakManager)\n    // either present it modally or push it on a UINavigationController\n}\n#endif Additionally, when a value is modified in any MutableConfiguration, a notification is fired to give the clients the opportunity to react and reflect changes in the UI. override func viewDidLoad() {\n    super.viewDidLoad()\n    NotificationCenter.defaultCenter().addObserver(self,\n                                                   selector: #selector(updateUI),\n                                                   name: TweakConfigurationDidChangeNotification,\n                                                   object: nil)\n}\n@objc func updateUI() {\n    // update the UI accordingly\n} A note on modular architecture It’s reasonable to assume that any non-trivial application approaching 2020 is composed of a number of modules and our Just Eat iOS app surely is too. With more than 30 modules developed in-house, it’s crucial to find a way to inject flags into the modules but also to avoid every module to depend on an external library such as JustTweak. One way to achieve this would be: – define one or more protocols in the module with the set of properties desired – structure the modules to allow dependency injection of objects conforming to the above protocol – implement logic in the module to consume the injected objects For instance, you could have a class wrapping the manager like so: protocol ModuleASettings {\n    var isFeatureXEnabled: Bool { get }\n}\nprotocol ModuleBSettings {\n    var publicApiHost: String { get }\n    var publicApiPort: Int? { get }\n} import JustTweak\npublic class AppConfiguration: ModuleASettings, ModuleBSettings {\n    static let tweakManager: TweakManager = {\n        ...\n    }\n    @TweakProperty(...)\n    var isFeatureXEnabled: Bool\n    @TweakProperty(...)\n    var publicApiHost: String\n    @OptionalTweakProperty(...)\n    var publicApiPort: Int?\n} Future evolution With recent versions of Swift and especially with 5.1, developers have a large set of powerful new tools, such as generics, associated types, opaque types, type erasure, etc. With Combine and SwiftUI entering the scene, developers are also starting adopting new paradigms to write code. Sensible paths to evolve JustTweak could be to have the Tweak object be generic on TweakValue have TweakManager be an ObservableObject which will enable publishing of events via Combine, and use @EnvironmentObject to ease the dependency injection in the SwiftUI view hierarchy. While such changes will need time to be introduced since our contribution to JustTweak is in-line with the evolution of the Just Eat app (and therefore a gradual adoption of SwiftUI), we can’t wait to see them implemented. If you desire to contribute, we are more than happy to receive pull requests . Conclusion In this article, we illustrated how JustTweak can be of great help in adding flexible support to feature flagging. Integrations with external providers/experimentation platforms such as Optimizely, allow remote override of flags without the need of building a new version of the app, while the UI provided by the framework allows local overrides in development builds. We’ve shown how to integrate JustTweak in a project, how to setup a reasonable stack with a number of configurations and we’ve given you some guidance on how to leverage it when writing UI tests. We believe JustTweak to be a great tool with no similar open source alternatives nor proprietary ones and we hope developers will adopt it more and more.", "date": "2019-11-26"},
{"website": "JustTakeAway", "title": "From an IT consultancy to a food tech giant: what I've learned", "author": ["Written by Charlie Phillips"], "link": "https://tech.justeattakeaway.com/2019/07/16/from-an-it-consultancy-to-a-food-tech-giant-what-ive-learned/", "abstract": "I started out in the UX industry working for a consultancy. This was the perfect place for me to build my skills, experience and confidence by working on different client projects. Given I was used to starting new projects that often required me to be at client site, when I accepted a new role I felt well equipped. But the reality is that starting somewhere in house is wildly different from starting a new client project. As will be the case with any large company, there’s a lot to learn in terms of domain and ways of working. Throughout an initial period of discomfort, self-doubt and imposter syndrome, I was sure that it would be 100% worth it. Surrounded by a helpful and supportive team, getting stuck in to running design sprints was the key to feeling comfortable and content. As I approach 3 months at Just Eat, I wanted to put in writing all the revelations I’ve had since being here… Teamwork is pretty darn cool I feel like I now fully understand what teamwork and collaboration looks like. When you’re in house, you work with the same people with the same common goal day in, day out. Each member of the team has a valuable part to play in a project. It’s so rewarding to work effectively with the team and see that work come to life, and to know that you achieved that together. One of the most important parts of teamwork is sharing knowledge. I came into a more experienced team, as the most junior designer. I’ve wracked up a huge amount of new knowledge from the designers and researchers on my team. For example, I had never written a hypothesis before, and had never used google analytics. Where at a consultancy you somewhat depend on your own knowledge to bring to the client and are positioned as an SME, at Just Eat I am one part of a team that I learn from. Having proper design processes in place makes a huge difference to the quality of the work produced Usability labs at Just Eat We’re lucky enough to have user researchers here (it seems strange now to think I did this myself before). Research is a huge discipline, so having that resource and capacity dedicated to thorough research is incredibly important. Every 2 weeks here, our restaurant partners come into the usability lab for research. Research and design led thinking is prioritised and celebrated. We’re trusted to make our own design decisions, and given the agency to carry this out. Want to go to the call centre to get more insight into a problem? Go ahead. Want to go out to the field to understand how this product is being used in context? It’s encouraged. Today, ‘design thinking’ can be used as a bit of a buzz-word — there’s a lot of talk, writing, and a lot of workshops. But when it comes down to it, often user-centred design isn’t prioritised, particularly in a consultancy project based environment where budgets and timelines are tight, and clients are often unaware of the benefits. Acting as an advocate for user-centred design can become wearing, especially when you’re up against the odds. Working for a company that leads with a user centred design mindset which is entrenched in processes is really refreshing, and, at the end of the day, produces better outputs. There’s a whole frickin’ tonne of information at your fingertips As a new person, this has been both a blessing and a curse. In a company as big as Just Eat, there are so many different teams, so many different products, so many different projects and so many people with so much knowledge. The great thing is, when you’re faced with a new problem to tackle, you can get a huge amount of insight to this problem internally. What’s tricky is to know what to look for and where to look. There’s a wealth of information — from data, to the call centre, to people in the business (for example territory managers who go out and speak to restaurants everyday). You get to see the fruits of your labour It’s fascinating and rewarding to be involved in the full product lifecycle, and to be able to see what happened to that feature after you finished working on it. You can learn a lot when something is released and feedback is collected — you can understand what has gone well and what hasn’t, and build on it. You think you know a product until you see it in context I strongly believe since working at Just Eat that testing a product in context is essential, and this is something I’ll carry with me. It’s risky to design for something without seeing it in its natural habitat. You might miss crucial things. For example, we recently worked on a new design for how scheduled orders are received. Restaurants were able to differentiate the scheduled from the normal orders in user testing in our lab, where they were primed. However, the reality is that it’s actually pretty rare for a restaurant to receive a scheduled order. Testing in the field showed that in the context of a restaurant, particularly when busy , there is so much going on that the scheduled orders were assumed to be normal ones, and a much stronger visual cue was required to differentiate. The knock on effect of this would’ve been restaurants cooking food that was not yet required, having to throw it away and therefore losing money and wasting time. Technology puts a distance between the customer and the vendor, particularly in the food industry When we are the user, particularly in the takeaway industry, we’re distanced from the vendor. As a customer, we don’t realise all of the work behind the scenes and what actually happens when we interact with a product. Working at Just Eat revealed so much complexity around the takeaway industry that I had not even considered before. I hadn’t even thought about the technology that the restaurants use. We can order food from restaurants without even seeing the physical location — would we make the same choices and hold the same attitudes if we could see the physical location? It’s easy to sit there behind an app and constantly check the little map to see the rider and get increasingly annoyed, but if you could see all that was going on in this restaurant, perhaps you wouldn’t feel this way. The Just Eat insights team has bridged the gap between themselves and the customers and vendors using an internal video series. It goes from watching the customers order a takeaway at home to speaking with the restaurant where they ordered. Having this journey played out to the company creates a deeper level of empathy for the different players in the food delivery process. Design at this level can actually make a real impact to people’s lives All these small changes and features that we work on have a very real effect in the running of the restaurant (take the example above of scheduled orders). When speaking to restaurant managers in the field, it’s evident that they have worked so hard to grow their business from scratch, and that their lives revolve around their business. For a lot of our restaurant partners, the business is family run and passed through generations. Our restaurant partners are relentlessly hard working, passionate and ambitious. The use case for restaurants is day in, day out. Each little feature we work on culminates to enable them to better run their business and, in turn, makes a difference to their lives. The most important thing I have learned since changing jobs, is that you can’t improve if you’re not prepared to push yourself outside your comfort zone. Sure, the comfort zone might seem…comfy…but you won’t get far if you never get out of that bean bag. Your learning should never stop, no matter how many years experience you have under your belt or what your level may be.", "date": "2019-07-16"},
{"website": "JustTakeAway", "title": "A trusting and supportive engineering culture at Just Eat", "author": ["Written by Sacha Zvetelman"], "link": "https://tech.justeattakeaway.com/2019/07/05/a-trusting-and-supporting-engineering-culture-at-just-eat/", "abstract": "After spending a few months as a Technology Manager at Just Eat, I have to admit I’m impressed with the culture I’ve encountered, especially with regards to the support and trust in the engineering teams. That’s why I thought it would be a good idea to write a brief article with first-hand observations about the people and culture at Just Eat that you won’t find in a job spec, unlike company benefits or tech initiatives. Hopefully, the takeaway of this article (no pun intended), will be why Just Eat has a fantastic engineering culture, making it a great place to work. Disclaimer: this is not intended to be a sales speech or an attempt to suggest Just Eat is perfect, because it’s not. We have problems and several things we have to get better at, like every other company. Interview process I’ll start with the interview process, since, naturally, this was my first contact with Just Eat, and I believe it’s the starting point of the items I’ll cover next. Given I was interviewing for an engineering position, there were a lot of thorough and varied technical stages, to verify I could do my job from a technical point of view. However, I was happy to quickly find out how much emphasis the interviewers were putting on the cultural, learning and knowledge sharing aspects. I was interviewed by six different people with very heterogeneous roles and positions at Just Eat: Senior Managers, Developers, other Technology Managers and Human Resources. They were all interested in finding out how they would work with me and if I was a person that could learn from others and vice versa. The mentality of the engineering teams I’ve found at Just Eat is that you’re never too experienced to stop learning, and you’re never too inexperienced to be teaching or helping others. Had I been a strong technical candidate, but not the right person for the team and Just Eat, I wouldn’t have been offered a position. And that would have been the right decision: it takes just one person to pollute or even destroy not just a team, but an entire department. Probation period as a backup Based on my own experience and conversations I’ve had with colleagues that have been working for longer at Just Eat, and as a positive consequence of the thorough and detailed interview process, I’d say the probation period feels more like a ‘backup period’, just in case one of the two parties changes their mind, which can happen, of course. I highlight this point because in many companies the probation period can be a stressful time in which you feel closely observed and tested to see if you fail. Starting off on the right foot is important. Thus, feeling supported and trusted from the very first day in your new role helps you to confidently and smoothly sail through your probation period, to the point you don’t even realise you are in it. People are friendly In my eight months here, I haven’t found a single person who isn’t nice, friendly and respectful to me. Yes, Just Eat is a huge public company with tens of engineering teams in various locations, but you can still feel the startup-ish vibe and mentality. I haven’t met everyone, but I can say, once again, that as a consequence of the interviewing process, the large majority of my colleagues are collaborative and laid back. Approachable people On a similar note, at Just Eat, you can talk to anyone. It doesn’t matter what your or the other person’s role is, you can get a hold of them and they will be glad to help you or openly listen to your suggestions and ideas. Your voice is not louder because of your role, but because of your ideas and your behaviour. You, from your position, whatever that may be, can set the direction of projects, practices, and more. Using some Agile buzzwords, you can be more of a player than a pawn. Successes are recognised With all the things going on in a fast-paced engineering department, in my experience, many times successes are not recognised, or at least not in a way they should. I have to admit that, as a manager, I have fallen in that trap a few times as well. Since joining Just Eat, I’ve seen how my colleagues clearly understand how important it is to communicate not just when things need improvement, but also when things are going well. I’ve received emails from other teams, including people in high positions, praising the team I work with for stuff we’ve done. Nobody had asked them for feedback, but they were able to identify improvements in the way we work and realised how important it is to inform the team they are aware of their hard work and improvements. Failures and mistakes don’t involve pointing fingers We work in teams, therefore we succeed and we fail as teams. When we screw up — and it happens, since we are humans —we all try to learn from the mistakes and take relevant actions to reduce the chances of making the same mistake again. It doesn’t matter what kind of error it was, from a minor problem to accidentally introducing a serious bug taking the platform down, nobody will point fingers at individuals here. The reason behind this is the following: let’s say a developer accidentally introduced a bug that prevented customers from placing orders. Is it the developer’s responsibility? What about our testers? What about the people who reviewed and approved the pull-requests? What about the Tech Manager or someone from Delivery or Product? We failed as a team, so we own the problem as a team and deal with it as a team. Empathic people at all levels I want to work for a company that has my back if I have a delicate situation to deal with in my personal life. Throughout my professional career, I’ve witnessed a few occasions in which employees, unfortunately, had to deal with some difficult personal situations. I’ve also witnessed how some of these people faced an alarming lack of empathy from their managers and employers, which resulted in very negative internal publicity for the company. However, at Just Eat, when we had a few people with a situation to deal with at home, managers —some of them quite senior —were completely understanding and accommodating to help the person solve their issues. The unanimous opinion was that family and health are the most important things in life. Yes, more than work. Therefore, if you have an emergency, you should be able to focus on that. This makes employees feel safe and protected. Nobody wants to have personal emergencies, but if you have one, you want to be able to fully focus on it to solve it. Conclusion Is Just Eat culture perfect? No, of course not, as I pointed out in my disclaimer. But I do feel my colleagues and I are making a big effort to continuously improve the place in which we work.", "date": "2019-07-05"},
{"website": "JustTakeAway", "title": "Effective iOS app UI testing with AutomationTools", "author": ["Written by Sneha Narayanaswamy"], "link": "https://tech.justeattakeaway.com/2019/08/19/effective-ios-app-ui-testing-with-automationtools/", "abstract": "Introduction UI tests for iOS apps using XCUITest have received increasing attention in recent years as the platform has matured and uptake has increased. UI tests are an integral part of the testing pyramid and should be considered in the test strategy for all apps/products. So, we created AutomationTools , a framework with a set of UI tests best practices and common functions that can be used across projects. Why AutomationTools? The JustEat iOS app is highly modularised for better code architecture. One advantage this gives us is that all modules can have their own unit and UI tests. However, this means each module duplicates a lot of setup steps for tests. Also, we often run experiments or A/B tests in our apps when we introduce new features. A/B tests can be run for a simple UI change on a screen, or change the user journey entirely. It can be challenging to run UI tests while A/B tests are active as the tests do not know which variant the app is running.  When using the XCUITest framework, the way you inject state into the app under test is via the use of launch arguments and environment variables. The launch arguments are passed in to XCUIApplication’s launch method as an array of strings and the environment variables are passed in as a dictionary. While this can be simple, it can quickly become complicated when using the same mechanism to configure experiment variants, state and flags to facilitate the UI test (e.g. simulating that the user is already logged in). AutomationTools is a framework that allows the easy setup of state configuration for our UI test suites, and also provides base classes and extensions to remove the need for duplicated setup code across test suites. The Code AutomationTools is divided into two sections, Core and HostApp. Core consists of the test case side of state configuration, useful base classes and utilities. HostApp provides functionality that acts as a bridge between the test case and the client app by unwrapping the flags that were injected by the testcase. Core JustEatTestCase This is a base class that inherits from XCTestCase. Test cases should inherit from it, rather than directly from XCTestCase. It holds a handy reference to an instance of XCUIApplication which should be used either directly or injected into page objects.  It contains the most common overrides of the setUp() and tearDown() methods, which add in the condition to not continue after failure and to terminate the app during teardown. These overrides can be overridden again by any test cases that require them to be different, but for us this has proved to be a very small number of tests. PageObject For easy maintenance, avoiding code duplication and to speed up development of test cases, we use the Page Object Model. PageObject.swift is a base class that should be inherited from by all page object classes. It holds an injected instance of  XCUIApplication to be used when defining the elements in the page object classes (etc.) and also holds an injected instance of the current XCTestCase to allow the use of predicates when waiting for elements or other conditions in page object methods. Extensions ExtensionXCTestCase This contains basic waiting utility methods to be used in tests. We have tried to keep this to a sensible minimum. ExtensionXCUIApplication This extends XCUIApplication and provides a launchApp method which takes in arrays of Flags and makes use of the LaunchArgumentBuilder discussed above. It takes in arguments for featureFlags, automationFlags, envVariables and otherArgs. launchApp(featureFlags: [Flag] = [],\n                   automationFlags: [Flag] = [],\n                   envVariables: [String: String] = [:],\n                   otherArgs: [String] = []) FeatureFlags and automationFlags have been discussed already. Environment variables are exactly what they sound like. The OtherArgs parameter can be used to pass in any other test setup or state, e.g. configuration for mock data. It should be noted that this is all a nice way of configuring an array of strings to pass into XCUIApplication’s launch app method and that you will need to handle all of these flags and implement them in your host app. Please see the next section. LaunchArgumentsBuilder As mentioned above, we run test cases by flagging features or experiment variants on or off as required. Passing this information into the launchApp method as launch arguments gets  complicated when handling multiple features, variants and options at the same time. LaunchArgumentsBuilder allows you to define an array of flags to configure this state which you want your test to run with. It takes in separate arrays for featureFlags and for automationFlags, and prepends each flag to distinguish them easily. AutomationBridge.swift discussed later contains the reverse functionality for the host app to use to unmarshall the flags for use in the appropriate state configuration. Flags are structs defined as: public struct Flag {\n    public let key: String\n    public let value: Any\n} It is important to distinguish between the feature and automation flags: Feature flags should be used to drive decision making paths in code to allow easier testing (for our products at Just Eat we use JustTweak , a framework for feature flagging and A/B testing for iOS apps) with an ephemeral configuration. Automation flags should be used by the app to set up the environment (e.g. directly deeplink to a specific screen, simulate the user to be logged in at startup, etc.) HostApp AutomationBridge This class contains the functionality to unmarshall the flags that were passed into the launchApp method by the testcase. The client should make use of it to obtain these flags and then implement their state configuration in the client app. AutomationBridge contains a method called isRunningAutomationTests which will return a boolean if the runningAutomationTests flag was set in the launchArguments. This should be referenced when performing UI test specific setup etc. in the client app code. It also publicly exposes the ephemeral configuration that is used to set up feature flags in the client code. Again, we suggest using JustTweak to set up a TweaksConfigurationsCoordinator with an ephemeral configuration that should be given the highest priority. Conclusion AutomationTools has allowed us to easily configure complex state in our UI tests. It has ensured consistency in our test suites and the included base classes have made it quicker than ever to get started with new projects. While each project has its own unique requirements, using AutomationTools in your test suite should make it easy for all of your future projects to have a consistent approach. You will be ready to configure more complex state in your app when tests require it! For more specific information and for code examples, please see the project README .", "date": "2019-08-19"},
{"website": "JustTakeAway", "title": "Out with the old, in with the new! Part 2 — Replacing legacy components without down time", "author": ["Written by Michael Kerby"], "link": "https://tech.justeattakeaway.com/2019/06/14/out-with-the-old-in-with-the-new-part-2-replacing-legacy-components-without-down-time/", "abstract": "Replacing a legacy component with a new component in a production environment without downtime can be difficult. Within this post I will describe how we did just that! By running, in production, both components side by side, verifying that our new implementation would handle both load and business logic before slowly switching traffic from the old to the new component . In part 1 , I described the thought process and high level results that came from the work we did in replacing rather than refactoring existing APIs. In this part, we will delve deeper into the implementation of our new Search Architecture at Just Eat and the tools and techniques we used to achieve this without any down time . Identifying the correct boundaries Before we started work, we need to make sure that have had outlined the correct boundary for our components. We had a component which was doing too many things and had multiple responsibilities. To start building something new we needed to understand these responsibilities clearly and define the purpose of the new API. Restaurant Discovery So, we already had implemented an orchestration layer within our domain, pulling sources of data together and aggregating these to expose externally. What we had yet to break out was the core functionality required to support searching for restaurants — Restaurant Discovery. This would focus mainly on receiving the input of a location, for example a postcode or geospatial information (latitude/longitude), and return a list of restaurants which were available in those locations. This was functionality that was already supported by our legacy component but was hard to decouple. It would be high risk to create a whole new component and then switch over all traffic to this. If something went wrong this would cause massive problems for us and would likely have multiple components that would need to be rolled back and fixed in a production environment. So, we worked out the following plan which we will go through step by step. MVP and Deploy Darkly Test, Test, Test Dual Call and Circuit Break Compare and Switch Deprecate! MVP and Deploy Darkly So, we took the minimal viable product (MVP) and the feature set that we thought would be required to support our new component. All the knowledge we had gathered within the team about our domain, we had a pretty good picture of what we needed to implement to get something to work. Over a period of 3 days, as part of a hackathon, we implemented the new component. MVP Architecture for supporting search results. This was not a lift and shift. The legacy code we had, did everything we needed to. But it was such a tangled web that it was difficult to see through it to dissect just the pieces we needed. In fact, our scope was so small that we knew it would be quicker just to rewrite from scratch. By doing this we also got the benefit of being able to jump to the latest and greatest frameworks, notably .net core, and take advantage of the benefits we got with this without having to worry about backward compatibility. One piece of technical debt we did take on at this stage, was to connect to the same data store as the legacy component. We were not at this point going to change the schema as it did still fit quite nicely and the performance of this data store was not an issue. There were some aspects which were not going to be related to what we needed, but those could be tidied up later. Although this meant that we technically broke a rule of microservices in sharing a data store (the horror!), it was a pragmatic approach that meant we were able to move a lot quicker and focus on solving one problem at a time. Dependency Graph for the legacy component – very bloated! Dependency Graph for new MVP solution – Nice! Once we had this MVP, we were able to deploy darkly into our production environment. Not hooked up to anything and not receiving any live traffic (yet). With this we were able to quickly iterate and begin testing without impacting other production components. Test, Test, Test We had our component in production. Let’s start using it then, right? Hold your horses for a second there cowboy! We are talking about a critical code path that would have a massive impact on the site if something went wrong. We need to test it first. Yes we had unit tests. Yes we had integration tests. We could prove that functionality was roughly the same. But we had yet to test to see whether this component was going to handle the load we would expect at peak time and would actually end up performing better than what we currently had. From a business perspective, we needed to prove what these changes were going to  improve customer conversion rate, reduce costs and reduce coupling. We knew this would reduce coupling of domains but, on its own, with other competing work, would be unlikely to get the approval for this to progress fully. We needed to fully load test to give us an initial indication that we would see other benefits. What we did was to run a copy of current peak load traffic through the new component. By doing so, we were able to get a picture of how it would perform. The results were very promising. We managed to achieve the full production load traffic on 10% of the hardware of the previous component. Our load testing of our new component – 1 server is performing adequately compared to 12 (!) under the legacy code base. Once we had done this we pushed the system to destruction and found that we could go to 5x the load at the given configuration without having any significant performance degradation! Upper response times between legacy and new components at different load levels whilst running load tests. With production traffic (shown later) we did not replicate the lower bound but were successful in keeping below the upper. This meant that we could proceed and get buy in from the wider business to kick off a project of work to deliver this fully into production. Dual Call and Circuit Break — Take 1 As part of the plan presented, we decided that the lowest risk approach was to implement a refactoring approach which would dual call the 2 systems at our orchestration layer and then compare the results from the calls. This was fairly straightforward to do as we leveraged the help of a library called Scientist.NET , a refactoring tool. This tool provided a good framework for how we could have a control vs candidate comparisons made which would give us an indication of where we had any differences in our logic. This would also be living in the real world, in production, against live traffic. We could in real time see when and where we had any issues. What we also implemented was circuit breaking of the calls to the new API, to shield it further from any potential failure due to slow response times. We implemented this using Polly , a circuit breaking library which allow for us to programmatically stop calling any failing services/dependencies. Dual Calling the 2 components together. Production traffic is served by SearchAPI and a comparison of the responses back are done within the Orchestration layer. Logs and metrics are then taken. With these 2 tools in place we were able to record statistics and logs over time which gave us a picture of when and where discrepancies were occurring and how we should fix them. However, we made a mistake. We attempted to start doing this for 100% of traffic. Because of a bug that we later found in our comparison logic, our orchestration service started using too much memory and CPU and crashed. Luckily with the monitoring and alerting we had in place this was caught and reverted. The positive: We failed fast. The negative: We impacted consumers. We said that couldn’t happen! Dual Call and Circuit Break — Take 2 With the fixes in place, we attempted to roll out these changes again, this time in a staggered approach. We released behind a feature toggle with 1%, 10%, 25%, 50%, 100% increments to traffic which was included in our comparison. Compare and Switch With 100% of traffic being compared, we were able to then look back through logs at our leisure for any discrepancies. We did find some; we had missed some fields or had rounding issues here and there. We put in fixes and managed to get this down to 0 discrepancies! Comparisons were logged over time. The gap shown here is when we disabled comparisons for a period of time. The next step was to switch the traffic we were sending. This would simply be switching the candidate and source data within our Scientist code. This would mean that we could continue doing comparisons but would prefer to return the results from our new API. Just for some extra safety, we included an additional feature toggle to allow for us to switch back if needed. Response times were better as well whilst dual calling. Above shows the difference between the legacy(SearchAPI) vs the new (RestaurantDiscovery) With this, we had successfully (with a few hiccups!) managed to get a new API ready for full release with a relatively low risk approach — All responses were still coming from the system, we had confidence that we had implemented everything correctly and we could still disable its use at any time if something happened. We let this run for a week in this configuration before we decided we had enough data to be 100% confident. In fact, once we released the new code, we saw an immediate improvement in response times from our dependent APIs. On Release, we saw a significant improvement in the overall response times at our domain boundary Deprecate! We were able to remove the feature toggles, the comparison code, the code paths calling the old APIs within our domain and scale down the old services. This was definitely satisfying to do, but it did take longer than anticipated. Due to the nature of the legacy component, its deprecation had a long sting in the tail caused by other domains still using it. We were able to fully derisk our domain, however. This was in fact highlighted by an outage which was caused by the legacy component and the search domain was unaffected … to the surprise of management and operations, but not to us! Final Thoughts It is never easy to make large scale changes to a code base without disruption. This in itself can mean changes are not made due to the risks of disruption. By taking advantage of techniques such as dual calling, circuit breaking and feature toggling, you can reduce the risk significantly and make these types of changes in a safe fashion", "date": "2019-06-14"},
{"website": "JustTakeAway", "title": "Deep Linking at Scale on iOS", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2019/04/16/deep-linking-at-scale-on-ios/", "abstract": "In this article, we propose an architecture to implement a scalable solution to Deep Linking on iOS using an underlying Flow Controller-based architecture, all powered by a state machine and the Futures & Promises paradigm to keep the code more readable. At Just Eat, we use a dedicated component named NavigationEngine that is domain-specific to the Just Eat apps and their use cases. A demo project named NavigationEngineDemo that includes the NavigationEngine architecture (stripped of many details not necessary to showcase the solution) is available on GitHub . Overview Deep linking is one of the most underestimated problems to solve on mobile. A naïve explanation would say that given some sort of input, mobile apps can load a specific screen but it only has practical meaning when combined with Universal Links on iOS and App Links on Android. In such cases, the input is a URL that would load a web page on the companion website. Let’s use an example from Just Eat: opening the URL https://www.just-eat.co.uk/area/ec4m-london on a web browser would load the list of restaurants in the UK London area for the postcode EC4M. Deep linking to the mobile apps using the same URL should give a similar experience to the user. In reality, the problem is more complex than what it seems at first glance; non-tech people – and sometimes even developers – find it hard to grasp. Loading a web page in a browser is fundamentally different from implementing dedicated logic on mobile to show a UIViewController (iOS) or Activity (Android) to the user and populate it with information that will most likely be gathered from an API call. The logic to perform deep linking starts with parsing the URL, understanding the intent, constructing the user journey, performing the navigation to the target screen passing the info all the way down, and ultimately loading any required data asynchronously from a remote API. On top of all this, it also has to consider the state of the app: the user might have previously left the app in a particular state and dedicated logic would be needed to deep link from the existing to the target screen. A scenario to consider is when the user is not logged in and therefore some sections of the app may not be available. Deep linking can actually be triggered from a variety of sources: Safari web browser any app that allows tapping on a link (iMessage, Notes, etc.) any app that explicitly tries to open the app using custom URL schemes the app itself (to perform jumps between sections) TodayExtension shortcut items ( Home Screen Quick Actions ) It should be evident that implementing a comprehensive and scalable solution that fully addresses deep linking is far from being trivial. It shouldn’t be an after-thought but rather be baked into the app architecture from the initial app design. It should also be quite glaring what the main problem that needs to be solved first is: the app Navigation . Navigation itself is not a problem with a single solution (if it was, the solution would have been provided by Apple/Google and developers would have simply stuck to it). A number of solutions were proposed over the years trying to make it simpler and generic to some degree – Router , Compass , XCoordinator to name just a few open-source components. I proposed the concept of Flow Controllers in my article Flow Controllers on iOS for a better navigation control back in 2014 when the community had already (I believe) started shifting towards similar approaches. Articles such as Improve your iOS Architecture with FlowControllers (by Krzysztof Zabłocki ), A Better MVC, Part 2: Fixing Encapsulation (by Dave DeLong ), Flow Coordinators in iOS (by Dennis Walsh ) got good traction, and even as recently as 2019, Navigation with Flow Controllers (by Majid Jabrayilov ) was published. To me, all the proposals share one main common denominator: flow controllers/coordinator and their API are necessarily domain-specific . Consider the following methods taken from one of the articles mentioned above referring to specific use cases: func showLoginViewController() { ... }\nfunc showSignupViewController() { ... }\nfunc showPasswordViewController() { ... } With the support of colleagues and friends, I tried proposing a generic and abstract solution but ultimately hit a wall. Attempts were proposed using enums to list the supported transitions (as XCoordinator shows in its README for instance) or relying on meta-programming dark magic in Objective-C (which is definitely the sign of a terrible design), neither of which satisfied me in terms of reusability and abstraction. I ultimately realized that it’s perfectly normal for such problem to be domain-specific and that we don’t necessarily have to find abstract solutions to all problems. Terminology For clarity on some of the terminology used in this article. Deep Linking : the ability to reach specific screens (via a flow) in the app either via a Deep Link or a Universal Link. Deep Link : URI with custom scheme (e.g. just-eat://just-eat.co.uk/login , just-eat-dk://just-eat.co.uk/settings ) containing the information to perform deep linking in the app. When it comes to deep links, the host is irrelevant but it’s good to keep it as part of the URL since it makes it easier to construct the URL using URLComponents and it keeps things more ‘standard’. Universal Link : URI with http/https scheme (e.g. https://just-eat.co.uk/login ) containing the information to perform deep linking in the app. Intent : the abstract intent of reaching a specific area of the app. E.g. goToOrderDetails(OrderId) . State machine transition : transitions in the state machine allow navigating to a specific area in the app (state) from another one. If the app is in a state where the deep linking to a specific screen should not be allowed, the underlying state machine should not have the corresponding transition. Solution NavigationEngine is the iOS module (pod) used by the teams at Just Eat, that holds the isolated logic for navigation and deep linking. As mentioned above, the magic sauce includes the usage of: FlowControllers to handle the transitions between ViewControllers in a clear and pre-defined way. Stateful state machines to allow transitions according to the current application state. More information on FSM (Finite State Machine) here and on the library at The easiest State Machine in Swift . Promis to keep the code readable using Futures & Promises to help avoiding the Pyramid of doom . Sticking to such a paradigm is also a key aspect for the whole design since every API in the stack is async. More info on the library at The easiest Promises in Swift . a pretty heavy amount of 🧠 NavigationEngine maintains separation of concerns between URL Parsing , Navigation , and Deep Linking . Readers can inspect the code in the NavigationEngineDemo project that also includes unit tests with virtually 100% code coverage. Following is an overview of the class diagram of the entire architecture stack. While the navigation is powered by a FlowController-based architecture, the deep linking logic is powered by NavigationIntentHandler and NavigationTransitioner (on top of the navigation stack). Note the single entry point named DeepLinkingFacade exposes the following API to cover the various input/sources we mentioned earlier: public func handleURL(_ url: URL) -> Future public func openDeepLink(_ deepLink: DeepLink) -> Future public func openShortcutItem(_ item: UIApplicationShortcutItem) -> Future public func openSpotlightItem(_ userActivity: NSUserActivityProtocol) -> Future Here are the sequence diagrams for each one. Refer to the demo project to inspect the code. Navigation As mentioned earlier, the important concept to grasp is that there is simply no single solution to Navigation. I’ve noticed that such a topic quickly raises discussions and each engineer has different, sometimes strong opinions. It’s more important to agree on a working solution that satisfies the given requirements rather than forcing personal preferences. Our NavigationEngine relies on the following navigation rules (based on Flow Controllers): FlowControllers wire up the domain-specific logic for the navigation ViewControllers don’t allocate FlowControllers Only FlowControllers , AppDelegate and similar top-level objects can allocate ViewControllers FlowControllers are owned (retained) by the creators FlowControllers can have children FlowControllers and create a parent-child chain and can, therefore, be in a 1-to-many relationship FlowControllers in parent-child relationships communicate via delegation ViewControllers have weak references to FlowControllers ViewControllers are in a 1-to-1 relationship with FlowControllers All the FlowController domain-specific API must be future-based with Future as return type Deep linking navigation should occur with no more than one animation (i.e. for long journeys, only the last step should be animated) Deep linking navigation that pops a stack should occur without animation In the demo project, there are a number of *FlowControllerProtocols , each corresponding to a different section/domain of the hosting app. Examples such as RestaurantsFlowControllerProtocol amd OrdersFlowControllerProtocol are taken from the Just Eat app and each one has domain specific APIs, e.g: func goToSearchAnimated(postcode: Postcode?, cuisine: Cuisine?, animated: Bool) -> Future func goToOrder(orderId: OrderId, animated: Bool) -> Future func goToRestaurant(restaurantId: RestaurantId) -> Future func goToCheckout(animated: Bool) -> Future Note that each one: accepts the animated parameter returns Future so that flow sequence can be combined Flow controllers should be combined sensibly to represent the app UI structure. In the case of Just Eat we have a RootFlowController as the root-level flow controller orchestrating the children. A FlowControllerProvider , used by the NavigationTransitioner , is instead the single entry point to access the entire tree of flow controllers. NavigationTransitioner provides an API such as: func goToLogin(animated: Bool) -> Future func goFromHomeToSearch(postcode: Postcode?, cuisine: Cuisine?, animated: Bool) -> Future This is responsible to keep the underlying state machine and what the app actually shows in sync. Note the goFromHomeToSearch method being verbose on purpose; it takes care of the specific transition from a given state (home). One level up in the stack, NavigationIntentHandler is responsible for combining the actions available from the NavigationTransitioner starting from a given NavigationIntent and creating a complete deep linking journey. It also takes into account the current state of the app. For example, showing the history of the orders should be allowed only if the user is logged in, but it would also be advisable to prompt the user to log in in case he/she is not, and then resume the original action. Allowing so provides a superior user experience rather than simply aborting the flow (it’s what websites achieve by using the referring URL). Here is the implementation of the .goToOrderHistory intent in the NavigationIntentHandler : case .goToOrderHistory:\n    switch userStatusProvider.userStatus {\n    case .loggedIn:\n        return navigationTransitioner.goToRoot(animated: false).thenWithResult { _ -> Future in\n            self.navigationTransitioner.goToOrderHistory(animated: true)\n        }\n    case .loggedOut:\n        return navigationTransitioner.requestUserToLogin().then { future in\n            switch future.state {\n            case .result:\n                return self.handleIntent(intent) // go recursive\n            default:\n                return Future .futureWithResolution(of: future)\n            }\n        }\n} Since in the design we make the entire API future-based, we can potentially interrupt the deep linking flow to prompt the user for details or simply gather missing information from a remote API. This is crucial and allows us to construct complex flows. By design, all journeys start by resetting the state of the app by calling goToRoot . This vastly reduces the number of possible transitions to take care of as we will describe in more detail in the next section dedicated to the underlying state machine. State Machine As you might have realized by now, the proposed architecture makes use of an underlying Finite State Machine to keep track of the state of the app during a deep linking journey. Here is a simplified version of the state machine configurations used in the Just Eat iOS apps. In the picture, the red arrows are transitions that are available for logged in users only, the blue ones are for logged out users only, while the black ones can always be performed. Note that every state should allow going back to the .allPoppedToRoot state so that, regardless of what the current state of the app is, we can always reset the state and perform a deep linking action starting afresh. This drastically simplifies the graph, avoiding unnecessary transitions such as the one shown in the next picture. Notice that intents ( NavigationIntent ) are different from transitions ( NavigationEngine.StateMachine.EventType ). An intent contains the information to perform a deep linking journey, while the event type is the transition from one FSM state to another (or the same). NavigationTransitioner is the class that performs the transitions and applies the companion navigation changes. A navigation step is performed only if the corresponding transition is allowed and completed successfully. If a transition is not allowed, the flow is interrupted, reporting an error in the future. You can showcase a failure in the demo app by trying to follow the Login Universal Link ( https://just-eat.co.uk/login ) after having faked the login when following the Order History Universal Link ( https://just-eat.co.uk/orders ). Usage NavigationEngineDemo includes the whole stack that readers can use in client projects. Here are the steps for a generic integration of the code. Add the NavigationEngine stack ( NavigationEngineDemo/NavigationEngine folder) to the client project. This can be done by either creating a dedicated pod as we do at Just Eat or by directly including the code. Include Promis and Stateful as dependencies in your Podfile (assuming the usage of Cocoapods). Modify according to your needs, implement classes for all the *FlowControllerProtocols , and connect them to the ViewControllers of the client. This step can be quite tedious depending on the status of your app and we suggest trying to mimic what has been done in the demo app. Add CFBundleTypeRole and CFBundleURLSchemes to the main target Info.plist file to support Deep Links. E.g. CFBundleURLTypes CFBundleTypeRole Editor CFBundleURLSchemes je-internal justeat just-eat just-eat-uk Add the applinks (in the Capabilities -> Associated Domains section of the main target) you’d like to support. This will allow iOS to register the app for Universal Links on the given domains looking for the apple-app-site-association file at the root of those domains once the app is installed. E.g. Implement concrete classes for DeepLinkingSettingsProtocol and UserStatusProviding according to your needs. Again, see the examples in the demo project. The internalDeepLinkSchemes property in DeepLinkSettingsProtocol should contain the same values previously added to CFBundleURLSchemes , while the universalLinkHosts should contain the same applinks: values defined in Capabilities -> Associated Domains. Setup the NavigationEngine stack in the AppDelegate’s applicationDidFinishLaunching . To some degree, it should be something similar to the following: var window: UIWindow?\nvar rootFlowController: RootFlowController!\nvar deepLinkingFacade: DeepLinkingFacade!\nvar userStatusProvider = UserStatusProvider()\nlet deepLinkingSettings = DeepLinkingSettings()\nfunc applicationDidFinishLaunching(_ application: UIApplication) {\n  // Init UI Stack\n  let window = UIWindow(frame: UIScreen.main.bounds)\n  let tabBarController = TabBarController.instantiate()\n  // Root Flow Controller\n  rootFlowController = RootFlowController(with: tabBarController)\n  tabBarController.flowController = rootFlowController\n  // Deep Linking core\n  let flowControllerProvider = FlowControllerProvider(rootFlowController: rootFlowController)\n  deepLinkingFacade = DeepLinkingFacade(flowControllerProvider: flowControllerProvider,\n                                        navigationTransitionerDataSource: self,\n                                        settings: deepLinkingSettings,\n                                        userStatusProvider: userStatusProvider)\n  // Complete UI Stack\n  window.rootViewController = tabBarController\n  window.makeKeyAndVisible()\n  self.window = window\n} Modify NavigationTransitionerDataSource according to your needs and implement its methods. You might want to have a separate component and not using the AppDelegate. extension AppDelegate: NavigationTransitionerDataSource {\n  func navigationTransitionerDidRequestUserToLogin() -> Future {\n    \n  }\n  ...\n} Implement the entry points for handling incoming URLs/inputs in the AppDelegate : func application(_ app: UIApplication, open url: URL, options: [UIApplication.OpenURLOptionsKey : Any] = [:]) -> Bool {\n  // from internal deep links & TodayExtension\n  deepLinkingFacade.openDeeplink(url).finally { future in\n    \n  }\n  return true\n}\nfunc application(_ application: UIApplication, continue userActivity: NSUserActivity, restorationHandler: @escaping ([UIUserActivityRestoring]?) -> Void) -> Bool {\n  switch userActivity.activityType {\n  // from Safari\n  case NSUserActivityTypeBrowsingWeb:\n    if let webpageURL = userActivity.webpageURL {\n      self.deepLinkingFacade.handleURL(webpageURL).finally { future in\n        \n      }\n      return true\n    }\n    return false\n  // from Spotlight\n  case CSSearchableItemActionType:\n    self.deepLinkingFacade.openSpotlightItem(userActivity).finally { future in\n      let originalInput = userActivity.userInfo![CSSearchableItemActivityIdentifier] as! String\n        \n      }\n    return true\n  default:\n    return false\n  }\n}\nfunc application(_ application: UIApplication, performActionFor shortcutItem: UIApplicationShortcutItem, completionHandler: @escaping (Bool) -> Void) {\n  // from shortcut items (Home Screen Quick Actions)\n  deepLinkingFacade.openShortcutItem(shortcutItem).finally { future in\n    let originalInput = shortcutItem.type\n    \n    completionHandler(future.hasResult())\n  }\n} N.B. Since a number of tasks are usually performed at startup (both from cold and warm starts), it’s suggested to schedule them using operation queues. The deep linking task should be one of the last tasks in the queue to make sure that dependencies are previously set up. Here is the great Advanced NSOperations talk by Dave DeLong from WWDC15. The UniversalLinkConverter class should be modified to match the paths in the apple-app-site-association , which should be reachable at the root of the website (the associated domain). It should be noted that if the app is opened instead of the browser, it would be because the Universal Link can be handled; and redirecting the user back to the web would be a fundamental mistake that should be solved by correctly defining the supported paths in the apple-app-site-association file. To perform internal app navigation via deep linking, the DeeplinkFactory class should be used to create DeepLink objects that can be fed into either handleURL(_ url: URL) or openDeepLink(_ deepLink: DeepLink) . In-app testing The module exposes a DeepLinkingTesterViewController that can be used to easily test deep linking within an app. Simply define a JSON file containing the Universal Links and Deep Links to test: {\n  \"universal_links\": [\n    \"https://just-eat.co.uk/\",\n    \"https://just-eat.co.uk/home\",\n    \"https://just-eat.co.uk/login\",\n    ...\n  ],\n  \"deep_links\": [\n    \"JUSTEAT://irrelev.ant/home\",\n    \"justeat://irrelev.ant/login\",\n    \"just-eat://irrelev.ant/resetPassword?resetToken=xyz\",\n    ...\n  ]\n} Then feed it to the view controller as shown below. Alternatively, use a storyboard reference as shown in the demo app. let deepLinkingTesterViewController = DeepLinkingTesterViewController.instantiate()\ndeepLinkingTesterViewController.delegate = self\nlet path = Bundle.main.path(forResource: \"deeplinking_test_list\", ofType: \"json\")!\ndeepLinkingTesterViewController.loadTestLinks(atPath: path) 1 2 3 4 and implement the DeepLinkingTesterViewControllerDelegate extension AppDelegate: DeepLinkingTesterViewControllerDelegate {\n    func deepLinkingTesterViewController(_ deepLinkingTesterViewController: DeepLinkingTesterViewController, didSelect url: URL) {\n        self.deepLinkingFacade.handleURL(universalLink).finally { future in\n            self.handleFuture(future, originalInput: universalLink.absoluteString)\n        }\n    }\n} Conclusion The solution proposed in this article has proven to be highly scalable and customizable. We shipped it in the Just Eat iOS apps in March 2019 and our teams are gradually increasing the number of Universal Links supported as you can see from our apple-app-site-association . Before implementing and adopting NavigationEngine , supporting new kinds of links was a real hassle. Thanks to this architecture, it is now easy for each team in the company to support new deep link journeys. The declarative approach in defining the API, states, transitions, and intents forces a single way to extend the code which enables a coherent approach throughout the codebase.", "date": "2019-04-16"},
{"website": "JustTakeAway", "title": "Data Ingestion At Just Eat – Part 1: The Concept", "author": ["Written by Martin Grayson"], "link": "https://tech.justeattakeaway.com/2019/03/29/data-ingestion-at-just-eat-part-1-the-concept/", "abstract": "At Just Eat, we’re constantly asking more of our data platform. Frequent requests for information mean new data has to be added to our Data Lake on Google Cloud Platform (GCP). In this post we’ll discuss the various processes that we carry out to ingest data into our data lake. We’ll move on to document a high level view of our architecture and wrap up by looking at the tenets that are carried through into the implementation. Much of the ingestion process is repetitive; the same high-level steps are generally carried out for each ingestion job regardless of source system nuances. These steps are: Data Retrieval: Typically, the first step in any ingestion process is to extract the data from the source system. We ingest data from over 100 heterogeneous systems, these systems may be internal or external to Just Eat. Data is transferred using a variety of mechanisms (S3, GCS, database extract, HTTP, FTP, etc.), or streamed into our real time ingestion API. Schema Inference & Evolution : Due to the development cycle of any product or data feed, changes are inevitable. To be able to handle additional data points and removal or change of existing ones, we need to have a way to track and handle schema changes. PII Identification & Obfuscation : We take data privacy seriously here at Just Eat, ensuring we handle all of the data that we consume with care. As part of our ingestion, we need to identify any potential PII data, anonymise it and store it correctly. This also ties in with our Subject Access Request and Right To Be Forgotten processing, enabling us to comply with GDPR. Data Loading : After retrieving the data, inferring its schema and ensuring it’s been through the relevant anonymisation process, it must finally be loaded into our Data Lake. Introducing Airflow Ingest The data engineering community has helped curate a number of tools to orchestrate the above processes. There are a various different platforms that support such orchestration. We’re big fans of Apache Airflow. It scales extremely well and offers us the ability to enhance and extend it. In addition to this, Airflow has great community, helped by the fact that its been taken into the Apache Software Foundation. We’ve been running Airflow since 2016. After some experimentation, we eventually settled on a CQRS architecture (separated ingestion from consumption). This paradigm saw the formation of three new tools, each with a razor-sharp, focused purpose. Airflow Ingests (as it’s aptly named), raison d’etre is to provide a highly flexible and configurable environment for extracting data from an arbitrary source and landing it in its raw form in GCS. No transformation or outbound data feeds, just ingestion. The five pillars of our data engineering toolkit, Ingest, Optimus, CAS, Egress and Orchestrator. Laying The Foundations Before we dive into building a DAG (Directed Acyclic Graph to the uninitiated) on our new ingestion platform, let’s lay some ground rules which prevent us from writing lots of repetitive code that’s difficult to maintain, reuse or extend: System Agnostic : An ingestion DAG’s sole purpose is to move data from source to target (our data lake). We tend to create an abstraction around source file systems (Google Cloud Storage, S3, SFTP or else) rather than the business logic. This allows us to code a limited number of generic, configurable ingestions that implement the processing steps mentioned in the introduction (extract, schema inference, PII handling and loading). The process of adding a new ingestion is as simple as adding a few lines of config as we don’t need to treat a particular source system differently. Organisation Is Key: Data Lakes aren’t highly curated, standardised stores of data. They store data in varying formats, with schemas inferred at read-time. This doesn’t mean they need be to organised like a teenager’s bedroom. We ingest over 100gb of data per day; to keep this data manageable we’ve chosen to enforce a naming convention throughout our ingestion processes. Data is organised using a number of sub divisions; data on both GCS and BigQuery is logically structured by organisation, system and topic. Further separation is created by versioning data and introducing date partitioning. Some conformity of the data into a standard file format (in our case Gzipped JSONL files) also eased processing downstream. Driver assignment data sourced from our internal message bus, justsaying. This data is versioned and stored in daily partitioned files on GCS. Don’t Reinvent the Wheel : Airflow provides some great inbuilt features (sensors, hooks and operators etc.) to encapsulate common operations. We made use of these features throughout our jobs but soon realised most of our DAGs looked similar. Actions like moving data from GCS to our Data Lake (BigQuery), would appear in most of our ingestions.  We were instantiating the same operators with the same parameters in a lot of our DAGs. We rapidly came to the conclusion that a Factory Method would allow us to facilitate reuse, extension and testing. As a side effect, this also made our DAGs much more readable and compact. In a similar vein, Airflow’s architecture allowed us to encapsulate common operations into our own Hooks and Operators (extending BaseHook and BaseOperator ). We earlier spoke of metadata management being one of our key tasks within ingestion – Schema Inference & Evolution. Calling out to our Schema Registry service became a matter of adding a new custom Operator to your DAG, naturally this too fell into our Task Factory. Think Big : We’re hungry for data – data powers all of our decisions at Just Eat – the data lake even contains output from JIRA to help us analyse our workflow. Our ingestion process has to be able to cope with future demands without being rebuilt from the ground up. Airflow isn’t built to do the heavy lifting, but rather orchestrate all the work. With that being said, when you’re running 10,000+ tasks per day, distributing your workload makes life easier. Resilience and scalability can be delivered by scaling worker deployments using Kubernetes, having a number of pods available to execute Airflow tasks. This also allows us to cope with varying requirements and resources, dependent applications can also be varied per worker class. For example, we have some tasks that are memory intensive, to handle this we have a high-memory-worker that work can be distributed to. Summary In this post we’ve introduced Data Engineering at Just Eat, focusing on one of the key functions of a data team — Ingestion. We talked about the main sub-tasks executed during ingestion, touching on how they actually operate. This paved the way to discuss our Airflow-centric tooling, introducing our ingestion platform and the surrounding tools that facilitate transformation and egress. We finished up by drilling down into the key paradigms that are followed within implementation. In subsequent posts, we’ll delve into some implementation specifics and get our hands dirty with some code samples.", "date": "2019-03-29"},
{"website": "JustTakeAway", "title": "Shock – Better Automation Testing for iOS", "author": ["Written by Jack and Sneha"], "link": "https://tech.justeattakeaway.com/2019/03/05/shock-better-automation-testing-for-ios/", "abstract": "UI testing is a relatively young addition to Xcode, and while Apple provides a good bedrock of APIs from which to build in XCUITest, it does not provide a complete picture in terms of writing end-to-end or comprehensive feature tests. In particular, using only the tools provided in XCUITest, Xcode provides no tools for dealing with staging the environment in which your app is running – this includes, but is not limited to: external APIs, 3rd party SDKs and mock configuration. In particular, external API dependencies can be difficult to work with. Usually developers are faced with one of two choices: point to an unstable and/or rapidly changing QA or Staging environment, or point at production APIs. Neither of these options are suitable if you aren’t in control of the data in the environments in question, and there are several other issues with going down this road: It is bad for the health of a production environment to pollute it with test data Even production APIs rarely have 100% uptime across all endpoints You have to coordinate testing with other teams consuming these environments. These are just a few of the many issues with using live APIs in UI tests. We built Shock to avoid these issues. Alternative Solutions Before diving further into what Shock provides to solve the aforementioned problems, it’s useful to examine the benefits and drawbacks of existing solutions in this space. NSURLSession Stubbing (OHHTTPStubs) A common solution is to use OHHTTPStubs to stub the networking layer. OHHTTPStubs uses the Objective-C runtime to swizzle methods on NSURLSession and related classes to allow a locally defined response to be returned. In practice, this is accomplished by setting up rules for testing an NSURLRequest (e.g. host, resource, headers, query string) and providing the name of a JSON file in the local bundle. There are a few advantages to this approach: It’s very easy to integrate and use. There are lots of existing blog posts and tutorials. However when it comes to using this for UI Tests, there are some drawbacks: It’s reliant on the Objective-C runtime which comes with it’s own set of drawbacks and considerations – this is particularly undesirable if you are working purely in Swift. JSON responses and code must be bundled with and executed in your app. No actual network connection is made / attempted. Lack of an actual system-level network request means some classes of network errors and issues cannot be tested (e.g. latency issues). Relatedly, the Github repo has been largely inactive for over a year. Using a QA Environment Another solution is to use a real – but staged – environment that’s available. This is a great method for testing as it’s as close to calling APIs in a real environment as is possible without the lack of predictability that can result from production data. Other benefits include: Latest changes are (usually) available meaning new tests. Tests can be written in parallel to API changes being made. Minimal mocking or stubbing is required other than changing the target host name. Drawbacks: QA environments are subject to instability. This means a lot of time can be wasted trying to track down test failures caused by data inconsistencies, API outages, and so on. QA environments are sometimes only available during business hours / days – this is inconvenient if you have nightly test runs. QA environments are usually in use by all clients and, as such, automated testing usually needs to be scheduled during a period where the data in the environment isn’t being used or altered. Why Shock? Shock aims to provide a middle-ground between the above solutions, while also eliminating some of the drawbacks. Real network connections Shock runs a real HTTP server so that real requests can be made. This opens up many options in terms of how you want to form or constrain requests, and how you want to generate and return mock data. Templatable Mock Data Shock includes tools with which to generate or modify your mock data based on the incoming request. The Mustache templating engine allows you to structure your dynamic responses clearly and declaratively. Atomicity Shock runs entirely within the test process. This means that you don’t have to clutter your app with excessive test code or test data, and that you have full control over the server and data. Simple readable syntax Setting up Shock is very simple. You can have a test running using a mocked endpoint in a matter of minutes. Getting Started Preparing your App To connect to the Shock server, your app will need to use the host address of the machine running the tests. In your code change the base URL you are using to the following: let baseUrl = \"localhost:9000\" It’s advisable to make sure this is easy to switch between when running UI tests versus running a debug build. This can be accomplished by writing code similar to the following: let isRunningUITests = ProcessInfo.processInfo.arguments.contains(\"UITests\")\nif isRunningUITests {\n    apiConfiguration.setHostname(\"http://localhost:6789/\")\n} If you have multiple base URLs (i.e. distinct APIs), don’t worry, you can modify them all to point at the Shock server – alternatively you can run multiple versions of the server on different ports and point at them individually. let baseUrlA = \"localhost:9000\" let baseUrlB = \"localhost:9001\" (Note that these server instances will be separate and not share any mock data.) Creating a Test Create a new XCUITestCase subclass in your test target and add the following test method: func testHappyPath() {\n   mockServer = MockServer(port: 6789, bundle: Bundle(for: HappyPathTests.self))\n   let route: MockHTTPRoute = .simple(\n    method: .GET,\n    urlPath: \"/my/api/endpoint\",\n    code: 200,\n    filename: \"my-test-data.json\"\n)\n   mockServer.setup(route: route)\n/* ... Your UI test code ... */\n} Let’s take a look at this line by line. First we create an instance of our mock server: mockServer = MockServer(port: 6789, bundle: Bundle(for: HappyPathTests.self)) Here we specify the port we want the server to listen on; this should be the same port that your app is going to specify for it’s base API URL. Next we create a route: let route: MockHTTPRoute = .simple(\n    method: .GET,\n    urlPath: \"/my/api/endpoint\",\n    code: 200,\n    filename: \"my-test-data.json\"\n) This specifies: the type of request (GET/POST/PUT/etc.) the path we expect the app to call (/my/api/endpoint, which will become the absolute path http://localhost:6789/my/api/endpoint) the status code we want to return to the app (200) (and finally) a filename containing the response that should be included in the body of the request. Lastly we pass the route to the mock server. mockServer.setup(route: route) After this, run your test code as normal and you should see your UI being populated with the data found in my-test-data.json Route Collections Sometimes we want to create collections of routes that work nicely together for different parts of our application flow. To accomplish this we can use the “collection” route type, which takes an array of other routes as it’s only associated value let route1: MockHTTPRoute = .simple(\n    method: .GET,\n    urlPath: \"/my/api/endpoint\",\n    code: 200,\n    filename: \"my-test-data.json\"\n)\nlet route2: MockHTTPRoute = .simple(\n    method: .GET,\n    urlPath: \"/my/api/endpoint\",\n    code: 200,\n    filename: \"my-test-data.json\"\n)\nlet routeCollection: MockHTTPRoute = .collection([ route1, route2 ])\nmockServer.setup(route: routeCollection) You can even make collections of collections and these will be handled and applied recursively by the mock server. Templated Routes Templates allow you to add a level of dynamism to your mocks. The problem with simple mocks is that they are static. For example, if you call the same endpoint with different query items – say /search?q=term vs /search?q=term2 – you’ll get the same response. Templates provide a simple solution to this: .template(\n    method: .POST,\n    urlPath: \"/my/api/endpoint\",\n    code: 200,\n    filename: \"template-route.json\",\n    data: [ \"templateKey\": \"A templated value\" ]\n) The above will replace all instances of the templateKey token in your JSON – the templated JSON “template-route.json” looks like this: {\n    \"Text\": \"{{templateKey}}\"\n} When your app calls this endpoint it will get the following response: {\n    \"Text\": \"A templated value\"\n} Future Development We use Shock extensively in our UI testing suites here at Just Eat, and so we are committed to keeping this repo up-to-date and bug free. Suggestions for features are always appreciated and you’re free to submit a PR for any improvements you’d like to see – and it would be great to here what they are in advance: raise an issue in the repo and label it “Feature Request” if you have something cool in mind. And of course, it goes without saying that bug reports – and fixes – are welcome too!", "date": "2019-03-05"},
{"website": "JustTakeAway", "title": "Out with the old, in with the new! Part 1  -  Replacing legacy components without down time", "author": ["Written by Michael Kerby"], "link": "https://tech.justeattakeaway.com/2019/03/04/out-with-the-old-in-with-the-new-part-1-%e2%80%8a-%e2%80%8areplacing-legacy-components-without-down-time/", "abstract": "All we wanted to do was to make things simpler. Remove that bloated code. Deprecate API endpoints. It sounds so simple. Yet, when you are dealing with components that are built up over time, it can be difficult to see the wood for the trees. Trimming back the technical debt which has been left unmanaged; you just don’t know where to start. Sometimes it can be better just to cut your losses and replace the whole tree! We broke down our API into its key functional areas and then began moving this functionality to brand new components. This allowed us to avoid breaking production code paths and instead reduce the risk whilst we introduced these over time. This post will detail the reasons behind this and the process we followed to achieve our end goal. Background JustEat’s platform has grown very rapidly from humble beginnings. Humble beginnings in this case means a monolithic architecture. This served Just Eat well for a long period of time but as more load was put on the system, the engineering team grew and the goals of the business grew, the system needed to be split up. This was done over time, over many different organisational restructures and what we were left with in relation to Search was something like this. Search data is used by other domains to answer provide responses about restaurants availability. We had an API (aptly named Search API) listening to a single datastore to answer the question “what restaurants are available in an area”. Great! We then had multiple different components and business domains which used this API to answer this question. For example, to know whether a restaurant is open for delivery, a Basket API would call Search API. To know whether a restaurant had available menus, a Menu API would call Search API. This works, but is starting to sound complex … We had multiple different teams that wanted to shape the result from this API to allow them to experiment with new ideas and features – adding in additional databases and sources in the process. Hmm, things are getting a little bit too big now! We had a single point of failure. Oh no. This is not going to end well. And it didn’t. We had reliability issues which caused outages, all caused from a single component not working properly. A failure of a core API would have knock-on impacts across the whole platform Fast forward A lot of work was done to improve the reliability of the services we had, doing all the tweaks we needed to get the systems back up and working as expected. A great deal of work went into improving the reliability of our services, making any changes required to provide a stable platform for our users. We were successful, but we knew we couldn’t stop there. More fundamental changes were needed to ensure we didn’t find ourselves in the same position again in the future. We started by decoupling our domains across the platform, Search being the first to attempt this journey… We then had a drive to push for decoupling our domains from each other, Search being one of the first to take on this journey. In doing so, Search API had to be dealt with but we also needed to provide the same backward compatible functionality. So, we began trying to understand what the best approach would be for doing this. We started by outlining the functionality that this API was actually doing and started dividing it up so it could be better managed, developed upon and kept reliable. Search API was broken down into 5 key areas: Orchestration, Validation, Restaurant Discovery, Ranking & Personalisation and Experimentation We identified that this one API actually had 5 key pieces of functionality, all being used in different ways Orchestration – Taking all of the data sources, combine into a single contract which can be consumed by client services (apps, websites etc). Validation – Based on the address entered, make sure that restaurants can deliver to them. Restaurant Discovery – Answering the basic question from above,”what restaurants are available in an area”. Personalisation and Ranking – Based on key information (consumer preferences, distance, restaurant performance and others) filter or modify the restaurants returned for the given request. Experimentation – Based on a given request, change/modify behaviour or data so that we can test changes to designs easily. This is a lot for a single API to be doing. So we began splitting things up along the boundaries that have been defined above and the teams that own them. Taking each piece of functionality, we added in new components in and around our existing infrastructure (so not to break things) and slowly began to rewire our domain. Over the course of 12 months we went from: to; By doing this we have been able to reduce the bottleneck caused by our single point of failure. This also allowed for us to continue supporting other business requirements whilst reducing the impact this has on our domain. For example, we are now able to release changings with more confidence and speed, given that the potential impact has been limited to only the components that we own. The older Search API has been put into a maintenance mode – where no new features are being implemented and only bug fixes are being made – and other parts of the business can prioritise removing the dependencies. This had fitted in nicely to allow us to progress with modifying and tidying up our codebase without being restricted by the demands of stakeholders elsewhere in the business who might have other priorities. We could have put effort into decoupling the existing APIs from other places in the business, however this would have taken time and effort and require our teams to build up knowledge of other domains first. Instead, the approach we have taken means we unblock ourselves and are less reliant on large scale organisational alignment. Obviously this doesn’t come without risk and complexity that goes along with many more, smaller components. I will leave how we actually progressed from before to after to another post where we will focus more on how one of these smaller services was rolled out to production successfully.", "date": "2019-03-04"},
{"website": "JustTakeAway", "title": "Building Stable Systems with Load-Test Driven Development", "author": ["Written by Matthew Kempa"], "link": "https://tech.justeattakeaway.com/2019/01/28/ltdd/", "abstract": "The concept and why we do it Features are important to a business, but equally, so is performant code. At Just Eat, we’re introducing the concept of Load-test driven development to produce not only functional code but performant code. Load-test driven development (LTDD) is the concept of using load-testing to test and iterate through software versions. Rather than solely prioritising the functional requirements, we place the non-functional requirements as an equal priority from the start. At Just Eat, we are used to working with large volumes of traffic on a global scale so getting this right is crucial. Imagine this scenario: You’ve just been working on a medium-long term project creating a new API or Microsite. You’re trying to progress the tech stack of the business and implemented a new architecture. It meets the functional needs of the business well. As you start to integrate the new API, it struggles to cope with the large volumes of traffic. Using LTDD, we aim to mitigate this issue early in development. Reasons we’ve used LTDD: It gets us thinking of scale from the start. To test architectural decisions as a proof of concept before committing and finding problems later in production. To influence the decision to use an entirely new framework & server-side rendering approach. To ensure our endpoints are scalable and reliable. To help inform decisions around the correct instance size and scaling policies for a component. Our first experience of this methodology was used to find the performance difference between .NET core and Node to make network calls to a CDN. We soon learnt that the LTDD approach was greatly influential on our decision, with early attempts causing our AWS instances to run out of memory under load (fig 1). This was due to a memory leak that would not have been noticed through solely local/functional testing. As we tested the different configurations and continuously optimised our code, the load testing helped prove the application was stable, whilst identifying some serious bottlenecks early on. Figure 1: An early .Net/Node proof of concept struggling under load Figure 1 shows the available memory remaining on our EC2 instances for a component. The amber and red lines show our warning and critical states retrospectively. Each of the remaining coloured lines each indicates a single EC2 instance, each of which experiencing serious memory issues and hitting a critical state after less than 10 minutes of load. How to implement LTDD using JMeter For LTDD, we need to define a test plan using JMeter. A test plan is simply a series of steps that the load testing server will take. For demonstration purposes, we’ll be creating a test plan that starts a user-defined number of threads, each thread will be reading a route and an ID from a CSV file and making a GET request to our endpoint using these values. This will be on a loop, cycling through the CSV, such that it can hit many endpoints. Each request will be handled by a response assertion to ensure that there is a 200 response. What you’ll need Fakeload – When set up on a server, this will run your plan. JMeter – This is the tool we will be using to generate a plan. Using JMeter to create a test plan When you first open JMeter, you should be greeted with this page: Change your plan name and description using the name and comments boxes, then click ‘File’->’Save as’ Create a new folder for this, as we will be storing our endpoint/IDs CSV here later. Now we can proceed to make a thread group, simply right-click on the test plan and follow the menus. This is just a set of threads that are executing the same scenario. We want to configure this thread group to use a user-defined number of threads and ramp-up period to allow for any server auto-scaling. Notice we’re using user-defined variables in the thread properties. These are defined in our root plan file. __P(PropertyName, DefaultValue) is the method to access properties. Properties are different to variables in that they’re shared across all of the executing threads. To store our properties, we need a plan.yaml file. Using the same file name as your JMX file, create a file in your plan directory called [jmx file name].plan.yaml and add: plans:      \n  - params:\n      threads: 5\n      host: myexamplehost.com\n      rampUpMinutes: 0 We now have the foundations of our test plan ready. Now we need to start looking at how the threads will execute and interact with the endpoint. Firstly, we’ll think about the timing between each iteration, let’s add a Step Delay to the thread group using a constant timer. We’ve altered the name and comments/description, but the thread delay defaults to 300 milliseconds. We’ll keep this at 300ms for now, but we could make it into a variable or random later. To make this random, you can simply use ‘$(__Random(low, high))’ Next, we need to think about the request itself. We’ll start by setting the request headers. We can do this using an HTTP Header Manager. We’ve added a series of defaults to this, you may not need all these. It will depend on your endpoint. Next, we need to parse any query parameters or routes into our request URL. We’re going to build a CSV with realistic data later in this tutorial, but for now, we will just create a CSV file and read basic data from it. To do this, we need to add a CSV Data Set Config. Set the filename to be relative to your JMX file. For simplicity, we will keep them in the same directory. Next, list the variable names of your values, as they will be represented in your CSV, this will allow you to use them in the request. Next, we need to create the HTTP request. For our basic GET request, we only need to fill in the basic details. Set the protocol to http/https then set the server name to be the host defined in your user-defined variables. Set the request method to GET and set your port and path. The path can be built using the variables gained from your CSV file. This, for example, would hit the following endpoint: ourUserDefinedHost.com/staticroute/csvDefinedRoute?id=csvDefinedId When we have a populated CSV, this should successfully send requests. We want to check that the response from these requests is valid and not failing. In this instance, we’ll be checking if the HTTP response is 200 OK. To do this, we need to create a response assertion that tests the ‘Response Code’ and checks it to include ‘200’. Finally, we will need a listener to listen to the response and reveal any issues/errors whilst we’re developing our load tests. It’s worth disabling it during load tests as they consume a large amount of memory. The listeners require little/no configuration and the plan can now be run in JMeter and any errors can be flagged for debugging by watching the listener components logs. Defining a Great CSV test file If you want to simulate real traffic, this can be done by having a realistic test data set stored in your CSV. It’s important to consider traffic patterns and which endpoints get the most traffic. If you have an existing application or services running, use metrics from these to predict how your endpoint will be used. Let’s assume we have an endpoint that returns product information. We could simply rotate through a product list CSV and request each item an equal number of times in turn, but this is highly unrepresentative of real traffic. Let’s assume that: 25% of pages, get 50% of total traffic, such as the home page, landing pages and most popular products (A pages). 50% of pages receive another 40% of the total traffic, such as individual products (B pages) The final 25% of pages only get 10% of the remaining traffic, such as info pages, reviews, policies and such (C pages). From this, let’s create a representative CSV file that will hit a range of endpoints, for simplicity, we’ll assume there are only 100 possible endpoints and 1000 lines in this CSV and it loops. We will also assume every page is hit at least once. 500 of these lines will be for our 25 A pages, 400 will be our 50 B pages, and 100 will be our 25 C pages. If we randomise the order of these pages, we can evaluate the effectiveness of our system, and in particular, see the performance gains through caching implementations and optimisations. Evaluating the results to become more performant Once you’ve run your load tests against your endpoint it is important to evaluate the performance. Implementing a monitoring stack and using tools like Grafana can help you to visually see the performance of your endpoints and make quick comparisons to acceptable performance thresholds. Comparison between metrics such as average response time, requests per second/minute and Apdex (Application Performance Index) are key to measuring performance gains or loses. Use these metrics to ask yourself the following questions: How many requests can our endpoint handle now compared to what it will need to handle at peak? How is the business evolving? If you experience a period of growth, does the server have remaining headroom be able to match that demand? If your endpoint is not as performant as it needs to be, set targets to improve and make the endpoint as performant as possible to exceed your demand at the minimum cost to the business. Further to this, load tests provide a benchmark to compare future changes. When you make an implementation or configuration change, re-run the load tests. If the performance significantly degrades, the change will need rolling back until the performance loss is isolated and resolved. From using this methodology in early development, we modified our system architecture to receive these results:", "date": "2019-01-28"},
{"website": "JustTakeAway", "title": "Fast growing architectures with serverless framework and .NET Core", "author": ["Written by Samuele Resca"], "link": "https://tech.justeattakeaway.com/2018/12/20/fast-growing-architectures-with-serverless-framework-and-net-core/", "abstract": "Serverless technologies provide a fast and independent way for developers to get implementations into production. This technique is becoming every day more popular in the enterprises’ stack, and it has been featured as a trial technique in the ThoughtWorks technology radar since 2017. The first part of the following article covers some general notions about serverless computing. The second one shows how to build a .NET Core Lambda on AWS using the serverless framework. Benefits of serverless computing Serverless technology is part of FaaS (function­-as­-a-service) technologies family. These kind of techniques are becoming popular with the adoption of the cloud systems. Nowadays, serverless implementations are raised as the preferred technology to use solutions that involve the cloud providers, both private and public. Furthermore, the typical software services and system perform operations by keeping a massive amount of data in­memory and by writing batches of data in complex data sources. Serverless, and in general FaaS technologies are designed to keep our system quick and reactive, by serving a lot of small requests and events as fast as possible. Serverless components are usually strongly­ coupled with the events provided by the cloud provider where they are running: a notification, an event dispatched by a queue or an incoming request from an API gateway is considered as a trigger for a small unit of computation contained in a serverless component. Therefore, that’s the main reason why the cloud providers pricing systems are based on the number of requests , and not on the computation time. Moreover, serverless components usually have some limitations on the execution time. Just like every technology, serverless are not suitable for every solution and system. Indeed, it simplifies the life of software engineers. Indeed the lambda deployment cycle is regularly fast and, as a developer, we can quickly get new features into production by doing a small amount of work. Furthermore, building components using serverless techniques means that the developer doesn’t need to care about scaling problems or failure since the cloud provider cares about those problems. Finally, we should also consider that serverless functions are stateless . Consequently, each system built on top of this technique is more modular and loosely­-coupled . Serverless pain points Nothing of this power and agility come for free. First of all, serverless functions are executed on the cloud, and they are usually triggered by events that are strongly ­coupled with your cloud provider, as a consequence, debugging them is not easy. That’s a valid reason to keep their scope as small as possible, and always separate the core logic of your function with the external components and events. Moreover, it very important to cover serverless code with unit tests and integration tests. Secondly, just like microservices architectures, which has a lot of services with a small focus, also serverless components are hard to monitor, and specific problems are quite challenging to detect. Finally, it is hard to have a comprehensive view of the architecture and the dependencies between the different serverless components. For that reason, both cloud providers and third-­party companies are investing a lot on all-­in­-one tools which provide both monitoring and systems analysis features. Experiment using serverless computing Nowadays more than ever, it is essential to quickly evolve services and evolve architectures basing on the incoming business requests. Data-­driven experiments are part of this process. Furthermore, before releasing a new feature, we should implement an MVP and test it on a restricted group of the customer base. If the results of the experiment are positive, it is worth to invest in the MVP to transform it into a feature of our product. Well, serverless computing provides a way to evolve our architecture evolving without caring about the infrastructure quickly. Serverless light­-overhead delivers a way to implement disposable MVP for experimenting with new features and functionalities. Furthermore, they can be plugged and un­plugged easily. Implementing AWS Lambda using .NET Core The following section covers a simple implementation of some AWS Lambda using .NET Core. The example involves three key technologies: AWS is the cloud provider which hosts our serverless feature; The serverless framework , which is a very useful tool to get our lambdas into AWS. As a general purpose framework, it is compatible with all the main cloud providers; .NET Core which is the open­source, cross­platform framework powered by Microsoft; The example we are going to discuss it is also present in the serverless GitHub repository at the following URL: serverless/examples/aws­dotnet­rest­api­withdynamodb . The example is part of some template projects provided by the serverless framework. The AWS Lambda project follows this feature schema: In summary, the feature implements some reading/writing operations on data. An HTTP request comes from the client through the API Gateway, the lambda project defines three functions: GetItem , InsertItem and UpdateItem Each of them performs an operation to a DynamoDB table. Project structure The solution we are going to implement has the following project structure: src/DotNetServerless.Application project contains the core logic executed by the serverless logic; src/DotNetServerless.Lambda project contains the entry points of the serverless  functions and all the components tightly­coupled with the AWS; tests/DotNetServerless.Tests project contains the unit tests and integrations tests of the serverless feature; Domain project Let’s start by analyzing the application layer. The core entity of the project is the Item class which represents the stored entity in the dynamo database table: using Amazon.DynamoDBv2.DataModel;\nnamespace DotNetServerless.Application.Entity\n{\n  public class Item\n  {\n    [DynamoDBHashKey]\n    public string Id { get; set; }\n    [DynamoDBRangeKey]\n    public string Code { get; set; }\n    [DynamoDBProperty]\n    public string Description { get; set; }\n    [DynamoDBProperty]\n    public bool IsChecked { get; set; }\n  }\n} The entity fields are decorated with some attribute in order to map them with the DynamoDb store model. The Item entity is referred by the IItemsRepository interface which defines the operations for storing data: using System.Collections.Generic;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Amazon.DynamoDBv2;\nusing Amazon.DynamoDBv2.DataModel;\nusing Amazon.DynamoDBv2.DocumentModel;\nusing DotNetServerless.Application.Entities;\nusing DotNetServerless.Application.Infrastructure.Configs;\nnamespace DotNetServerless.Application.Infrastructure.Repositories\n{\n  public interface IItemRepository\n  {\n    Task<IEnumerable<T>> GetById<T>(string id, CancellationToken cancellationToken);\n    Task Save(Item item, CancellationToken cancellationToken);\n  }\n  public class ItemDynamoRepository : IItemRepository\n  {\n    private readonly AmazonDynamoDBClient _client;\n    private readonly DynamoDBOperationConfig _configuration;\n    public ItemDynamoRepository(DynamoDbConfiguration configuration,\n      IAwsClientFactory<AmazonDynamoDBClient> clientFactory)\n    {\n      _client = clientFactory.GetAwsClient();\n      _configuration = new DynamoDBOperationConfig\n      {\n        OverrideTableName = configuration.TableName,\n        SkipVersionCheck = true\n      };\n    }\n    public async Task Save(Item item, CancellationToken cancellationToken)\n    {\n      using (var context = new DynamoDBContext(_client))\n      {\n        await context.SaveAsync(item, _configuration, cancellationToken);\n      }\n    }\n    public async Task<IEnumerable<T>> GetById<T>(string id, CancellationToken cancellationToken)\n    {\n      var resultList = new List<T>();\n      using (var context = new DynamoDBContext(_client))\n      {\n        var scanCondition = new ScanCondition(nameof(Item.Id), ScanOperator.Equal, id);\n        var search = context.ScanAsync<T>(new[] {scanCondition}, _configuration);\n        while (!search.IsDone)\n        {\n          var entities = await search.GetNextSetAsync(cancellationToken);\n          resultList.AddRange(entities);\n        }\n      }\n      return resultList;\n    }\n  }\n}] The implementation of the IItemRepository defines two essential operations: Save , which allows the consumer to insert and update the entity on dynamo; GetById which returns an object using the id of the dynamo record; Finally, the front layer of the DotNetServerless.Application project is the handler part. Furthermore, the whole application project is based on the mediator pattern to guarantee the loosely­coupling between the AWS functions and the core logic. Let’s take as an example the definition of the CreateItemHandler : using System;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing DotNetServerless.Application.Entities;\nusing DotNetServerless.Application.Infrastructure.Repositories;\nusing DotNetServerless.Application.Requests;\nusing MediatR;\nnamespace DotNetServerless.Application.Handlers\n{\n  public class CreateItemHandler : IRequestHandler<CreateItemRequest, Item>\n  {\n    private readonly IItemRepository _itemRepository;\n    public CreateItemHandler(IItemRepository itemRepository)\n    {\n      _itemRepository = itemRepository;\n    }\n    public async Task<Item> Handle(CreateItemRequest request, CancellationToken cancellationToken)\n    {\n      var item = request.Map();\n      item.Id = Guid.NewGuid().ToString();\n      await _itemRepository.Save(item, cancellationToken);\n      return item;\n    }\n  }\n} As you can see the code is totally easy. The CreateItemHandler implements, IRequestHandler and it uses built­in dependency injection to resolve the IItemRepository interface. The Handler method of the handler simply map the incoming request with the Item entity and it calls the Save method provided by the IItemRepository interface. Functions project The function project contains the entry points of the lambda feature. It defines three function classes, which represent the AWS lambda: CreateItemFunction , GetItemFunction and UpdateItemFunction ; As we will see later, each function will be mapped with a specific route of the API Gateway . Let’s dig a little bit into the function definitions by taking as an example the CreateItemFunction : using System;\nusing System.Threading.Tasks;\nusing Amazon.Lambda.APIGatewayEvents;\nusing Amazon.Lambda.Core;\nusing DotNetServerless.Application.Requests;\nusing MediatR;\nusing Microsoft.Extensions.DependencyInjection;\nusing Newtonsoft.Json;\nnamespace DotNetServerless.Lambda.Functions\n{\n  public class CreateItemFunction\n  {\n    private readonly IServiceProvider _serviceProvider;\n    public CreateItemFunction() : this(Startup\n      .BuildContainer()\n      .BuildServiceProvider())\n    {\n    }\n    public CreateItemFunction(IServiceProvider serviceProvider)\n    {\n      _serviceProvider = serviceProvider;\n    }\n    [LambdaSerializer(typeof(Amazon.Lambda.Serialization.Json.JsonSerializer))]\n    public async Task<APIGatewayProxyResponse> Run(APIGatewayProxyRequest request)\n    {\n      var requestModel = JsonConvert.DeserializeObject<CreateItemRequest>(request.Body);\n      var mediator = _serviceProvider.GetService<IMediator>();\n      var result = await mediator.Send(requestModel);\n      return new APIGatewayProxyResponse { StatusCode =  201,  Body = JsonConvert.SerializeObject(result)};\n    }\n  }\n} The code mentioned above defines the entry point of the function. First of all, it declares a constructor and it uses the BuildContainer and BuildServiceProvider methods exposed by the Startup class. As we will see later, these methods are provided in order to initialize the dependency injection container. The Run method of the CreateItemFunction is decorated with the LambdaSerializer attribute, which means that it is the entry point of the function. Furthermore, the Run function uses the APIGatewayProxyRequest and the APIGatewayProxyReposne as input and output of the computation of the lambda. Dependency injection The project uses the built­in dependency injection provided by the out­-of­-box of .NET Core. The Startup class defines the BuildContainer static method which returns a new ServiceCollection which contains the dependency mapping between entities: using System.IO;\nusing DotNetServerless.Application.Infrastructure;\nusing DotNetServerless.Application.Infrastructure.Configs;\nusing DotNetServerless.Application.Infrastructure.Repositories;\nusing DotNetServerless.Lambda.Extensions;\nusing MediatR;\nusing Microsoft.Extensions.Configuration;\nusing Microsoft.Extensions.DependencyInjection;\nnamespace DotNetServerless.Lambda\n{\n  public class Startup\n  {\n    public static IServiceCollection BuildContainer()\n    {\n      var configuration = new ConfigurationBuilder()\n        .SetBasePath(Directory.GetCurrentDirectory())\n        .AddEnvironmentVariables()\n        .Build();\n      return ConfigureServices(configuration);\n    }\n    private static IServiceCollection ConfigureServices(IConfigurationRoot configurationRoot)\n    {\n      var services = new ServiceCollection();\n      services\n        .AddMediatR()\n        .AddTransient(typeof(IAwsClientFactory<>), typeof(AwsClientFactory<>))\n        .AddTransient<IItemRepository, ItemDynamoRepository>()\n        .BindAndConfigure(configurationRoot.GetSection(\"DynamoDbConfiguration\"), new DynamoDbConfiguration())\n        .BindAndConfigure(configurationRoot.GetSection(\"AwsBasicConfiguration\"), new AwsBasicConfiguration());\n      return services;\n    }\n  }\n} The Startup uses the ConfigureServices to initialize a new ServiceCollection and resolve the dependencies with it. Furthermore, it also uses the BindAndConfigure method to create some configurations objects. The BuildContainer method will be called by the functions to resolve the dependencies. Testing functions As said before, testing our code, especially in a lambda project it is essential regarding continuous integration and delivery. In that case, the tests are covering the integration between the IMediator interface and the handlers. Moreover, they also cover the dependency injection part. Let’s see the CreateItemFunctionTests implementation: using System.Threading;\nusing System.Threading.Tasks;\nusing Amazon.Lambda.APIGatewayEvents;\nusing DotNetServerless.Application.Entities;\nusing DotNetServerless.Application.Infrastructure.Repositories;\nusing DotNetServerless.Application.Requests;\nusing DotNetServerless.Lambda;\nusing DotNetServerless.Lambda.Functions;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.DependencyInjection.Extensions;\nusing Moq;\nusing Newtonsoft.Json;\nusing Xunit;\nnamespace DotNetServerless.Tests.Functions\n{\n  public class CreateItemFunctionTests\n  {\n    public CreateItemFunctionTests()\n    {\n      _mockRepository = new Mock<IItemRepository>();\n      _mockRepository.Setup(_ => _.Save(It.IsAny<Item>(), It.IsAny<CancellationToken>())).Returns(Task.CompletedTask);\n      var serviceCollection = Startup.BuildContainer();\n      serviceCollection.Replace(new ServiceDescriptor(typeof(IItemRepository), _ => _mockRepository.Object,\n        ServiceLifetime.Transient));\n      _sut = new CreateItemFunction(serviceCollection.BuildServiceProvider());\n    }\n    private readonly CreateItemFunction _sut;\n    private readonly Mock<IItemRepository> _mockRepository;\n    [Fact]\n    public async Task run_should_trigger_mediator_handler_and_repository()\n    {\n      await _sut.Run(new APIGatewayProxyRequest {Body = JsonConvert.SerializeObject(new CreateItemRequest())});\n      _mockRepository.Verify(_ => _.Save(It.IsAny<Item>(), It.IsAny<CancellationToken>()), Times.Once);\n    }\n    [Theory]\n    [InlineData(201)]\n    public async Task run_should_return_201_created(int statusCode)\n    {\n      var result = await _sut.Run(new APIGatewayProxyRequest {Body = JsonConvert.SerializeObject(new CreateItemRequest())});\n      Assert.Equal(result.StatusCode, statusCode);\n    }\n  }\n} As you can see, the code mentioned above executes our functions and it performs some verification on the resolved dependencies and it verifies that the Save method exposed by the IItemRepository is called. For obvious reasons, the test class doesn’t cover the DynamoDb feature. Furthermore, when we combine complex entities and operations, it is possible to use a Docker container to cover the database part with some integrations tests. Speaking about what’s next about .NET Core and AWS topics, The .NET AWS team has relased a nice tool to improve lambda testing: LambdaTestTool. Deploy the project Let’s discover how to get the project into AWS. For this purpose, we are going to use the serverless framework. The framework is defined as: The Serverless framework is a CLI tool that allows users to build & deploy auto­scaling, pay­per­execution, event­driven functions. In order to get serverless into our project we should execute the following command inside our main project: npm install serverless --save-dev Define the infrastructure By default, the definition of the infrastructure will be placed in the serverless.yml file. In that case the file, looks like this one: service: ${file(env.configs.yml):feature}\nframeworkVersion: \">=1.6.0 <2.1.0\"\nprovider:\n  name: aws\n  stackName: ${file(env.configs.yml):feature}-${file(env.configs.yml):environment}\n  runtime: dotnetcore2.1\n  region: ${file(env.configs.yml):region}\n  accountId: ${file(env.configs.yml):accountId}\n  environment:\n    DynamoDbConfiguration__TableName: ${file(env.configs.yml):dynamoTable}\n  iamRoleStatements:\n    - Effect: Allow\n      Action:\n        - dynamodb:*\n      Resource: \"arn:aws:dynamodb:${self:provider.region}:*:table/${self:provider.environment.DynamoDbConfiguration__TableName}\"\npackage:\n  artifact: bin/release/netcoreapp2.1/deploy-package.zip\nfunctions:\n  create:\n    handler: DotNetServerless.Lambda::DotNetServerless.Lambda.Functions.CreateItemFunction::Run\n    events:\n      - http:\n          path: items\n          method: post\n          cors: true\n  get:\n    handler: DotNetServerless.Lambda::DotNetServerless.Lambda.Functions.GetItemFunction::Run\n    events:\n      - http:\n          path: items/{id}\n          method: get\n          cors: true\n  update:\n    handler: DotNetServerless.Lambda::DotNetServerless.Lambda.Functions.UpdateItemFunction::Run\n    events:\n      - http:\n          path: items\n          method: put\n          cors: true\nresources:\n  Resources:\n    ItemsDynamoDbTable:\n      Type: 'AWS::DynamoDB::Table'\n      DeletionPolicy: Retain\n      Properties:\n        AttributeDefinitions:\n          - AttributeName: Id\n            AttributeType: S\n          - AttributeName: Code\n            AttributeType: S\n        KeySchema:\n          - AttributeName: Id\n            KeyType: HASH\n          - AttributeName: Code\n            KeyType: RANGE\n        ProvisionedThroughput:\n          ReadCapacityUnits: 1\n          WriteCapacityUnits: 1\n        TableName: ${self:provider.environment.DynamoDbConfiguration__TableName} The code mentioned above performs some operations on the infrastructure using cloud formation. The provider node defines some of the information about our lambda, such as the stack name, the runtime and some information about the AWS account. Furthermore, it also describes the roles, and the authorizations of the lambda, e.g., the lambda should be allowed to perform operations on the DynamoDb table. The functions node defines the different lambda functions, and it maps them with a specific HTTP path. Finally, the resource node is used to set the DynamoDB table schema. Configuration file The serverless.yml definition is usually combined with another YAML file which just defines the configurations related to the environment. For example, this is the case of the DynamoDbConfiguration__TableName node which uses the following syntax to get the information from another YAML file: ${file(env.configs.yml):dynamoTable} . The following snippet shows an example of the env.configs.yml file: feature: <feature_name>\nversion: 1.0.0.0\nregion: <aws_region>\nenvironment: <environment>\naccountId: <aws_account_id>\ndynamoTable: <dynamo_table_name> Final thoughts The post covers some theory topics about serverless computing as well as a practical example of .NET Core lambda implementation. It is important to underline how serverless computing can be used to push fast forward our architecture. Moreover, experimenting is a key aspect of an evolving product, it is important to evolve adapt fast to the incoming changes of the business. In conclusion, you can find some example of lambda with serverless in the following repository serverless/examples/aws­dotnet­rest­api­with­dynamodb . @samueleresca https://samueleresca.net", "date": "2018-12-20"},
{"website": "JustTakeAway", "title": "Jailbreaking ServiceStack v3", "author": ["Written by Dmytro Liubarskyi"], "link": "https://tech.justeattakeaway.com/2018/12/10/jailbreaking-servicestack-v3/", "abstract": "Introduction Why ServiceStack? “ ServiceStack is a simple, fast, versatile and highly-productive full-featured Web and Web Services Framework that’s thoughtfully-architected to reduce artificial complexity and promote remote services best-practices with a message-based design that allows for maximum re-use that can leverage an integrated Service Gateway for the creation of loosely-coupled Modularized Service Architectures.” This article is a detective story about performance issues we encountered with the free version of ServiceStack framework and how we managed to eliminate them. Background Since August 2013 ServiceStack has moved to a self-sustaining commercial model for commercial usage of ServiceStack from v4+ onwards. From v3.9.62 Release Notes : “ v3.9.62 marks the point where v3 goes into feature-freeze to make room for the future v4 commercial version of ServiceStack . ” While the vast majority of our APIs at Just Eat have been migrated to or initially spun up with other frameworks, there are still a couple of elephants in the room that keep on giving. AppDynamics About a year ago from the time of writing (2018) Just Eat embraced AppDynamics – a powerful microservices monitoring tool that keeps an eye on every line of code and instantly sends performance counters providing comprehensive telemetry on the health of our services. Once we baked a new Amazon machine image (AMI) with AppDynamics built-in and rolled this out to our production environments, we encountered sporadic off-peak and quite painful at-peak CPU spikes on EC2 boxes running ServiceStack services on them. (figure 1.1) In short, whenever we combined AppDynamics & ServiceStack we could see a very strong correlation between CPU spiking and said combination of ServiceStack and AppDynamics. This was escalated to AppDynamics for investigation and many iterations and attempts at uncovering the issue ensued. Eventual AWS based deadlines forced us to upgrade our underlying AMI without making use of AppDynamics and to our surprise, the issue was again present, this time without AppDynamics. Further extensive experimentation followed and every possible permutation of including and excluding AppDynamics, Codedeploy upgrades and Spectre meltdown patches … finally pointed towards a Windows Update that got rolled into a newly backed AWS AMI. Sleeves rolled up! Having this finding in mind, we tried to look at things from a different angle. While trying to match CPU spike timestamps with log entries, we noticed that a specific error log message (“ Error while logging request ”) showed correlation with the relevant CPU spike metrics. (figure 2.1) Having proved that this error had always been present in our logs, the question became: Why did this not cause any performance issues before the specific Windows update got installed and applied to the server instances? Looking closer at the error that have not caused any serious implications before, we can see that the underlying cause of the error is a NullReferenceException : (figure 2.2) Looking at the corresponding error as produced by the “Bad” server instance – leading to high CPU spiking – we can see a less trivial issue as the underlying exception: (figure 2.3) One of the key parts of the puzzle was the discovery that the ThreadAbortException was being thrown exactly 120 seconds after the request came into the service. In turn, the CPU spikes also lasted for exactly the same amount of time. (figure 2.4) This implied that the request was being processed way too long and eventually got terminated by IIS after the configured timeout had expired (by default, the connection timeout for IIS is 120 seconds). (figure 2.5) The latter got us thinking that it could be a deadlock or more likely a spin-wait lock – which is a while-loop causing CPU heat up when a lock does not release for a relatively long time – since the spikes lasted for exactly 2 minutes long then cooling down immediately after thread termination. Surgery Finding deadlock causing code by sifting through source code is not a trivial task. However, we did have the stack trace of exceptions thrown from ServiceStack’s ObjectPool.Release() method (shown in figure 2.3), which was called from the ConcurrentQueue.TryDequeue() method. If you look at the ObjectPool.Release() method, you find a home-made spin-wait lock implementation – which is in fact not fully thread-safe – that used to cause the lesser impactful NullReferenceException (shown in figure 2.2). The fact that this method threw different exceptions whilst running on machines with different Windows updates gave us reason to suspect that the produced MSIL was being JIT-compiled differently. To try and prove that this is the case, with the help of BenchmarkDotNet , we implemented and ran a small tool. The tool takes the mentioned ServiceStack “spin-wait” code, and produces some corresponding assembler code against the differing Windows updated machines. Here are the results: When running on the machine with the suspicious Windows updates, we see that the addIndex field load is hoisted out of the loop. This effectively made the loop infinite when the condition in the while statement is met at least once and usually happens when two concurrent threads are trying to acquire the lock simultaneously: As long as the addIndex field is not marked as volatile , this JIT optimization behaviour is completely legitimate. The BenchmarkDotNet utility also helped to identify what changed in the environment after the suspected Windows update installation. In spite of the target runtime framework being kept at version 4.6 , the next-generation RyuJIT normally shipped along with .NET Framework version 4.7 was used regardless, and some new runtime optimizations kicked in. Another BenchmarkDotNet test helped to prove that marking the field as volatile suppresses the mentioned optimization causing this side effect: Solution! By the time this article was published an appropriate pull-request had been raised and merged into the ServiceStack v3 branch and we are waiting for the NuGet package to appear with the fix. But for the sake of a dramatic ending, this article will tell you what we did to eliminate the problem while the fix was on the way to the ServiceStack Github repo. As Microsoft has been receiving loads of reports about application behaviour affected by the RyuJIT compiler, they provided a list of recommendations to mitigate such problems. One of the options was to disable RyuJIT and fallback to the legacy JIT64 compiler. None of the disabling mechanisms were suitable for us – the per-application config does not apply to ASP.NET applications and the other two options seemed risky and tricky in terms of deployment. The rest of the recommended work basically revolves around suppression of various optimizations and would be considered only as a last resort. So instead of mitigating performance issues by general degradation of service performance, we injected an alternative implementation of ServiceStack’s RequestLogger . The safest way was to replicate this implementation with the one-line change replacing ServiceStack’s ConcurrentQueue (which depends on the dodgy ObjectPool ) with a reliable “native” .NET ConcurrentQueue implementation: One has to be aware that to inject a custom implementation of an interface to substitute ServiceStack’s own stuff, you have to tell its IoC container to give preference to your registrations: In the end, we pushed this out to Production on the newest AMI with AppDynamics monitoring the performance of our services with the issue finally resolved. Conclusion In a multithreading environment, you should be extra careful with fields accessed concurrently. E.g., as demonstrated above, while a thread is expecting (by design) another thread to change the state – runtime optimizations may play tricks on you. Protect yourself with thread-safe constructions provided by the chosen language. The .NET Framework is not perfect, and everything has its own pitfalls. No matter how experienced you are, you have to find a very sound justification to implement something that has already been done and well tested ( ConcurrentQueue and SpinWait being a good example here – assuming I’m not missing anything obviously bad about them). Otherwise, chances are that your homemade handcraft will turn out as another weakness in your software application. Microsoft has been doing a great job with performance improvements in their software environment, and it’s rare, but it does happen that these improvements may lead to a stable implementation behaving differently within the context of a newer environment (causing race conditions, etc). When choosing an off-the-shelf solution, you have to embrace the fact that this in itself is no silver bullet. An example here would be .NET Core, which is a good stuff, but still is being continually enhanced with bugs fixes and so on. Just Eat engineers also contribute to this by raising issues and making feature requests . About the Author This blog post was written by Dmytro Liubarskyi , an Engineering Lead and Acting Technology Manager of the Payments team at Just Eat.", "date": "2018-12-10"},
{"website": "JustTakeAway", "title": "Tracking AWS support tickets across accounts with JustSupport", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2018/08/01/tracking-aws-support-tickets-across-accounts-with-justsupport/", "abstract": "Just Eat Service Operations Centre announces the release of Just Support under the Apache 2.0 License. JustSupport is a tool that helps you keep track of AWS support cases across multiple accounts. It integrates with Jira to enable tracking of internal tickets by updating Jira tickets when AWS support tickets have been replied to. It also provides and easy way to reply to AWS cases straight from your own Jira issue. JustSupport works with all levels of AWS support including Basic, Developer, Business and Enterprise. 2 way comms across multiple accounts Features: Pulls latest updates from AWS Support tickets and pushes them into Jira tickets as comments Allows a jira user to respond to the AWS ticket from within Jira by starting a comment with #DearAWS When chat already exists the chat history is pulled from AWS and added to the Jira Issue Supports more than one AWS ticket per Jira Issue Supports multiple AWS accounts 1 line Serverless Deployment Why we wrote it: At Just Eat we operate all of our production infrastructure in AWS across multiple accounts and regions. Keeping track of support requests raised and responding to them through the AWS console is time consuming and inefficient. Just Support lets us see the latest updates, in a central location, to issues that have been raised in our daily production meeting where we review open issues from the last 24 hours. This appears as comments in Jira Setup and Deployment We’ve leveraged the serverless framework to make setup and deployment as easy as possible. The serverless deployment includes the cloudformation file to create the AWS resources the tool needs. Further details on configuration are in the readme here Summary We’ve found JustSupport incredibly useful in saving us time managing AWS tickets. Hopefully it will help others with similar problems. We’re happy to accept contributions.", "date": "2018-08-01"},
{"website": "JustTakeAway", "title": "ASP.NET Core 2.1 – Supercharging Our Applications 🚀", "author": ["Written by Martin-Costello"], "link": "https://tech.justeattakeaway.com/2018/06/14/aspnet-core-21-supercharging-our-applications/", "abstract": "Introduction ASP.NET Core 2.1 was released by Microsoft at the end of May, and last week we deployed two consumer-facing applications upgraded to use ASP.NET Core 2.1 to production for the first time. These applications have now been run in production for an entire weekend of peak traffic, and we’ve seen some great performance improvements – in some cases improving average response times by over 40% . Background .NET Core 1.0 was released almost 2 years ago now, in June 2016 , and we’ve been using it in some form or another at Just Eat ever since. Early adoption was rather limited amongst our teams, mainly due to the fact that .NET Standard support had not yet filtered through to many of the third-party dependencies we use to build our applications, such as the AWS SDK and NUnit, coupled with some APIs that are available in the full .NET Framework not being available in the first release. .NET Core 2.0 was subsequently released in August last year , adding many more APIs and features, plus applying feedback Microsoft received since the first release from user feedback for real-world application use-cases. Since the release of .NET Core 2.0, more and more teams here at Just Eat have been adopting .NET Core for new projects, plus a small number of refactors and/or rewrites of existing applications. At the time of writing, we have over 40 different applications (we call these features here at Just Eat) running in our production environments. In fact, our internal Just Eat technology radar has recently moved .NET Core from a Trial technology to an Adopt technology. Upgrades Since the first preview of .NET Core 2.1 was made available in February, the team I work in had been maintaining a branch of two of our ASP.NET Core 2.0 features . One is a consumer-facing MVC web application that serves HTML pages using Razor, and the other is an MVC API application that acts as an orchestration layer between that web application and a number of our lower-level internal APIs, all of which run on .NET Framework 4.6.x. A simple illustration of how these applications fit together in our architecture is shown below. From Previews 1 and 2 to Release Candidate 1, we experimented with various new features and capabilities, as well as finding a few bugs , feature gaps and other changes along the way. While upgrading from 2.0 to 2.1 is relatively simple if you just follow the migration guide , some of the more interesting additions in .NET Core 2.1 require additional development effort to leverage in your existing application. One new feature we found particularly interesting was HttpClientFactory . Given that both applications perform many HTTP requests as part of their daily operations, particularly the orchestration API, we felt that using the factory and the underlying HttpClient changes to use a managed Sockets implementation would give us some good performance improvements. Another was the new WebApplicationFactory<T> class, which allowed us to delete lots of boilerplate code in our test suite and make our integration tests faster and easier to set up. Overall the Git patches for the two applications to go from 2.0 to 2.1 were +199/−194 lines for the web application, and +246/−297 lines for the API. Once the final ASP.NET 2.1 release was available (it was even released a few days early with some caveats) and we updated our Amazon Machine Image to have the ASP.NET Core 2.1 Runtime pre-installed, we were ready to deploy the upgrade. First we deployed the 2.1 build of the API to production as a canary (serving a small percentage of traffic) on Monday 4th June and eagerly awaited the outcome. The morning after showed no errors or degraded performance, so we promoted the canary to 100% of traffic for that feature on the Tuesday morning. Once that was done, we deployed the 2.1 build of the web application to production for another over overnight canary. Again, no detriment was seen in our logs and monitoring, so we promoted that to 100% of user traffic on the morning of the 6th June. At Just Eat our busiest times for traffic are typically in the evenings, particularly at the weekend, so the real litmus test for the new versions of both applications targeting ASP.NET Core 2.1 would be their first weekend serving requests to hungry users of our UK consumer website. On Monday 11th June I came into the office eager to pore over the results of the weekends’ traffic, and I was not disappointed with the results at all! Show Me The Numbers! The comparisons below are for both applications running ASP.NET Core 2.1 on Sunday 10/06/18 compared to the previous Sunday (03/06/18) where it was running ASP.NET Core 2.0. Sunday was chosen as the basis for comparison (rather than Friday or Saturday) as traffic levels from users was the most similar week-to-week. The response statistics were collected using statsd from either a middleware or HttpMessageHandler depending on the context, which are pushed to Graphite and then visualised with Grafana. CPU statistics are collected separately on our AWS EC2 instances and also pushed to Graphite. API The API averaged 107ms per HTTP GET compared to 141ms ( approximately a 24% improvement ), with much less deviation. The overall request rate also is shown to give an indication of the relative load on the API. CPU usage is also illustrated, which again shows a slight overall improvement. The spike in the graph is where auto-scaling increased the number of EC2 instances in service based on average CPU load. Web The web app averaged 130ms per page render compared to 230ms ( approximately a 43% improvement ) which again shows much less deviation. The overall request rate also is shown to give an indication of the relative load on the web application. CPU usage is also illustrated, which again shows a slight overall improvement. The spikes in the graph is where auto-scaling increased the number of EC2 instances in service based on average CPU load. Scaling occurs later than the previous week as the application instances are now able to handle more load per-box for the same average CPU usage. This gives an additional benefit beyond performance, as it can reduce the overall cost of running the application as the total provisioned CPU required for like-for-like load means that less EC2 instances are required. The second spike on the CPU graph is caused by raised CPU as the application is installed onto the new EC2 instances coming into service, which skews the CPU average temporarily. Conclusion The performance improvements made by Microsoft and the Open Source community in ASP.NET Core and the Core CLR are non-trivial. With minimal code changes you can super-charge your existing ASP.NET Core 2.0 applications by upgrading to ASP.NET Core 2.1. This can reduce your response times to make your users’ experience better, and reduce your CPU utilisation to help make your hosting bills lower too! About the Author This blog post was written by Martin Costello , a Senior Engineer at Just Eat, who works on consumer-facing websites and the back-end services that power them. If you like solving problems at scale, why not check out open positions in Technology at Just Eat ?", "date": "2018-06-14"},
{"website": "JustTakeAway", "title": "Designing For The Google Assistant", "author": ["Written by George Powell"], "link": "https://tech.justeattakeaway.com/2018/04/05/designing-for-google-assistant/", "abstract": "As we move towards the age of conversational computing, basic daily tasks, whims and curiosities should become seamless to fulfill with natural language. The promise of conversational UI is that life can be simplified and friction removed from the interaction with technology. In this post, we’ll take a look at what designing for the Google Assistant (one of the most exciting conversational UIs on the market today) entails and the opportunities it brings. The State of the Art Hi, I’m Craig Pugsley – Creative Lead in the Product Research team here at Just Eat. As well as being a massive fan of everything beautiful, I’m also a huge tech freak and gadget fan. I love the innovation that’s been happening in the voice tech space in the last few years, so let’s start with a tiny introduction to the state of the art. Voice-based smart speakers have been around for years. Google brought the Google Home to UK shores back in April 2017 and its capabilities and hardware siblings have grown the Google Home family ever since. At CES 2018, Google also announced their new ‘smart display’ platform, powered by the Google Assistant, with hardware from a range of OEM manufacturers. Google was users to experience the value of the Google Assistant, that powers devices such as Google Home, so that a quick ‘Hey Google’ is all you need to say to get something done Google call their platform ‘the Google Assistant’ and its showing up across almost every type of digital surface you can image – Android, iPhones, iPads, Nest Cams, Android Wear smart watches, Android Auto-powered in-cars systems, wireless headphones, etc… Creating an ‘Action’ or app for the Google Assistant may unlock the door for some of these surfaces – with varying degrees of visual UI – but with a strong core capability set you can build a compelling experience on. Design for Differences Designing for Conversational UIs can seem daunting at first, but many of the same principles and processes you’re used to using are still very appropriate. One of the biggest differences to consider is the UI bandwidth you have to engage with your user is considerably smaller. If you’re used to the luxury of a sea of smartphone pixels, relying on copy (and maybe a set of pre-defined visual widgets) may seem like a frustratingly limited toolset to craft a user experience around. This is, however, entirely the wrong way to look at designing for CUIs. Instead, what you have is a perfect opportunity to strip back superfluousness and focus on delivering real user value in a way that gets your users’ jobs done potentially more quickly and efficiently than any other platform. Considering conversation UI bandwidth – and designing explicitly for it – is crucial. Voice-only surfaces like Google Home present a very different UI bandwidth channel than, for example, talking to the Google Assistant via the smartphone app. With the Google Assistant you can augment context and decisions related to the conversation with elements shown on-screen. This makes conveying more complex information to your users so much easier, and you can introduce more sophisticated interactions when it comes to asking your users to make choices from a range of options. Chatbots – and specifically apps for the Google Assistant – are the focus of this article. Importance of Lean Experimentation With the proliferation of new Assistant-connected surfaces and UI options, where do you begin to ensure a seamless user experience? The answer’s the same as for any platform! Design it user-centred, design it lean. Specifically, find where the user value lives. Here in the Just Eat Product Research team, we fundamentally believe in this lean, iterative, user-centred approach to product creation. For us, that means experimentation and a frank and honest approach to failure. Culturally, many of us struggle with experimentation, as it means we have to point at things we tried that didn’t work and justify the time we spent on them. How our managers, business owners, colleagues and peers react to our failures is deep-seated and cultural and will take years to adjust. But it’s only by having a progressive approach to failure and recognising the value of knowing where not to go that we can unlock true product innovation and discover real user value. The days of the HIPPO (Highest Paid Person in the Office) making the decisions have to end – building products based on our prejudice and assumptions isn’t great for anyone: it’s incoherent, wasteful and of low-value to users, and it’s expensive for businesses. Sailing a massive tanker in the wrong direction, then trying to turn it when you realise you’re off-course is slow, arduous, deeply unmotivating and unproductive. The key to embracing failure and using experimentation to unlock innovation is asking the right questions early on: find the jobs that people are really trying to get done, then try as many solutions to these jobs as quickly as you can. And by ‘try’ I mean putting something in front of your real users that’s as close to the real thing as possible, in a setting that’s most natural to them and how they’re going to use your experience. When you find something that resonates with your users, that’s when you double-down. Find The Key Use-Cases Now, the reason I’m ranting on the art of product creation is because the ‘designing the right thing’ part of the classic Design Council ‘double diamond’ is absolutely crucial to building a great conversational UI. Finding the one or two core use cases from your native app that suit a conversation UI will mean your users are happier, served better and mean you can focus on designing a few things really well. Voice and chat interfaces force you to re-think your flows. No longer can you display 20 pages of SERP results and ask your users to move through your 4-step funnel to goal completion. While you’re defining product scope, you need to continually ask yourself “why would they use this, rather than the native app?”. It’s only when you (and, more importantly, your users) can answer that question definitively that you know you’ve struck chatbot gold. Getting Started With Design OK, so you’ve used Design Thinking or some other lean method to identify the core use-cases you’re going to support. What next? Script-writing and rapid prototyping, that’s what! Designing a conversational flow should be done in three phases: initial script & roleplaying to figure out the structure, 2) prototyping & fitting to interaction methods to make sure it works as a chatbot and, 3) final tweaking to add polish & delight. Phase 1: Rough Script & Roleplaying To get started with phase one, you need to quickly write the conversation out, like you were writing a screenplay with your conversational app and user as the actors. Don’t get too hung up on the exact words or options you’re giving to your users just yet. You won’t get it right first time, and you’ll be doing lots of iteration. Just make a start. Here are some tips for writing your first conversation script: Become familiar with the seven core building blocks of the Google Assistant conversation. You can present simple unformatted text, cards with formatted text and images, animated gifs, buttons with onward action suggestions, lists, etc… get to know these components, what they offer and their limitations. Then you’d know the right one to use for each response. Don’t worry too much about this while writing your first script, just focus on the words, But just be aware that copy isn’t the only way to communicate in the Google Assistant. Maybe jot down some notes in the margin as you’re writing the script. Keep your responses short and conversational. Use contractions where possible (“won’t” rather than “will not”) and use slang and colloquialisms if it fits your brand. Find your organisation’s tone of voice guidelines and read them. Keep their guideline do’s and don’t in mind while you’re choosing your words. In your copy, try to speak in the voice of your Brand. Make sure you end each response with a question. This gives the user a clear understanding of what they need to do next, and also ensures the user understands precisely when the microphone is listening. Now you need to roleplay your script with a real human being! Find one, then get them to be your user, while you be your Google Assistant app. This will be weird. Don’t worry. Swallow that embarrassment, brace yourself and become your app! Speak the script as you’ve written it to hear what it sounds like. With this first pass, you’ll notice loads of words or phrases that could be tweaked, sound weird and robotic when you say them or just plain don’t work. Make adjustments and notes about what works as you test each turn of the scripted conversation. If you’re making lots of structural changes, find another human victim to test on so you get a fresh pair of ears 😉 Phase 2: Prototyping & User Testing With your tweaked script you now need to move straight to phase two and create a prototype to allow you to test with real users . Creating a conversation in the Google Assistant is super-easy using Google’s own web-based WYSIWYG (what you say is what you get!) tooling called DialogFlow . I won’t explain how to use that here, as there are plenty of really great tutorials on the web. Basically, each state your app is in is called an ‘intent’. These intents are usually triggered by your user saying something that matches what your intent is listening for. When an intent is triggered, your app will respond with the copy you’ve written, you’ll ask a question, and the user will say something that triggers another intent. This goes on until they hit a ‘final’ intent and the chat closes. When you move to production and build your Google Assistant Action for real, Dialogflow will hand off to some code stored in the cloud that your engineers will write. The code will decide how to reply, and replace the copy you’ve put in your Dialogflow intent with whatever the real code says it should do. Once you’ve got your prototype working, run through the flow using both touch, type and voice. Switch between modes (start with voice, move to touch, finish with typing) to test how that feels. Are you asking the right question at the end of a response? If the user taps to make a selection and interrupts your spoken response, is it still obvious what they should do next? You can edit your written copy and spoken copy separately. So you could, for example, show more detail on screen than you actually say. You don’t know how your users will be interacting with your Action, but you need to design for voice-only as much as voice, touch and typing. Hide the screen and try your flow using only your voice. Does it make sense? Now test with real users! This should be done on the real Google Assistant platform. As you’re user testing, try to spend time probing your users’ mental models. User expectations of what your bot can do and the realities of your use-case might be very different. Pay close attention to how you’ve onboarded your users and set the scene, probe their mental model of what your app can do. Make notes about the bits of your script that make people smile or frown – don’t forget, the bandwidth of this channel is way more limited than your apps or websites. You need to work really hard to squeeze a positive emotional response out of your users, but that’s what you should be aiming for! Find areas that feel emotionally flat as you’re testing, make notes of them and make sure you focus on those in the final phase. Phase 3: Dialing It In You made it! Your final phase now consists of fit & finish, final polish and amplifying delight. This means really spending time tweaking those words: does the response really need to be as long? Can I say the same thing in half the words? What’s a more exciting way of phrasing that? Could I use an emoji instead? Would an animated gif get the message across? It’s this phase where you can really exploit this new medium’s opportunities to create something delightful! To finish, here are some top tips to remember when designing for the Google Assistant : Be your brand. Express it in as few words possible. The Google Assistant supports animated gifs, sound effects, SSML (style markup for your copy). Exploit these features! Know your Google Assistant widgets. Especially which ones can be used together. Always try to use suggestion chips & carousels, in case the user wants tap rather than typing or speaking. You can only respond with max 2 items. You can do a lot in DialogFlow, but it quickly becomes limited when you need logic / external data. However, this does create a nice division of labour between you and your engineers. Make sure you ending every response with a question People move between interaction modes (start with voice, may move to touch for selection). It’s very import you design for all modes at all points of your flow. Be very clear about the use-cases that suit conversation UIs. Have honest conversations with your Product people. Keep responses short – remember why people are using your chatbot over your app or website. Try to user test in context (not in the lab, if you can help it). Even if you take the participant out for a walk! Be sensitive to how the Google Assistant works on all platforms (Mobile, Home, Android Wear, Android Auto, etc…). You can specify the first response in Dialogflow, then hand off to your engineer’s code for the next response. You can provide a load of first responses, and Dialogflow will pick one at random. This is great for making your bot feel more natural. Where your responses are generated in code, try to get some variation into your responses. This will be difficult to argue for when you’re agreeing project scope, but it’ll be worth it for how much more natural your conversational app feels. Have a go at designing for the Google Assistant and let us know how you get on in the comments below!", "date": "2018-04-05"},
{"website": "JustTakeAway", "title": "How we use activity-oriented metrics to drive Engineering Excellence", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2018/03/07/how-we-use-activity-oriented-metrics-to-drive-engineering-excellence/", "abstract": "At Just Eat we have had for a while a tool called “RAG report” to measure the level of compliance for every service (~450) we have in production. We recently introduced something called Scorecard, to provide teams with better informations and to introduce some fun and gamification to this process. Scorecard use the RAG report data and provide an extra level of insights on a team by team basis. As you can see below, Scorecard allows you to see data by feature (service), team, department and to see a full blown matrix of everything Scorecard Matrix Below you can see a sample of a matrix view. It lists all the services ( web sites, apis, databases, workers, lamdas), the team owning the service, the tiering of the service (Tier-1: order taking, Tier-2 order dispatching, etc…) and all the activities we want teams to complete and comply with. There basic activities we track are: Pipeline & Infrastructure: Service is in the pipeline and not in a stale status (unreleased code of infrastructure changes), Service is on the latest version of infrastructure as code (a migration we have in progress) Security: Service is on the latest AMI (continuous activity), is using the latest standard to define Security Groups (a migration we have in progress), etc.. Monitoring and Alerting: Service has all the basic Seyren checks configured etc… As we become more sophisticated, we introduce new checks to make sure we keep the bar high across all of our services Scorecard by Service The last view (below) I would like to show is the team view. This view is particularly useful in the Engineering / Product conversation of how much “Engineering Excellence” work we should have in each team backlog. Every team should use this view to make sure there’s a clear balance between work that provide direct business value vs work that has to be done to keep the lights on. Points = calculated based on #of services * # of things a service should comply with In the Scorecard by Service, 10 points for green, 5 points for amber, 0 points for red ( or something similar, depending on the activity) We have the ability to weight activities differently to calculate the final score Features = Services ( we’ve historically called our services features ) The total amount of points per team is also a very significant indicator for how many complex services a team is supporting and helps to make decision on team size and team formation Scorecard by Teams Scorecard by Team Big shout to Zac Charles (https://twitter.com/zaccharles0) for building the tool in his spare time 🙂", "date": "2018-03-07"},
{"website": "JustTakeAway", "title": "Effective iOS error management", "author": ["Written by Chris Newton"], "link": "https://tech.justeattakeaway.com/2018/01/26/effective-ios-error-management/", "abstract": "How to manage errors across iOS apps and dependencies Let’s begin with a story. Once upon a time, there was a wonderful Princess App in the kingdom. The app was very complicated and full of very obscure parts, but it brought great joy to all who used it. One day, however, an evil bug appeared and showed an ugly and unintelligible error code to the App users of the kingdom, as below. A brave prince developer was called to kill the dragon fix the bug, and the ticket he was given went more or less like this: “The user can’t proceed with the purchase – sometimes a strange error appears, sometimes nothing happens.” The brave developer picked up his keyboard and… ignored the error, deleted the entire codebase and rewrote everything in Swift 5. No error code was ever seen again and they all lived happily ever after. The end. Unfortunately, there are no such fairy tales in real life; errors exist and as a developer your task is to understand the problem and fix it. Now let’s take a look at the most common problems and deficiencies of how errors are generally handled in iOS apps. Lost errors When the app fails and nothing happens, this could mean that one or more errors have been lost at some point. For example: func getDataFromUrl(url: URL, completion:(_ data: Data?, _  response: URLResponse?, _ error: NSError?) -> Void) {\n\t...\n\tif (data) {\n\t\t// Do something\n\t}\n\t...\n} What has happened to the error? Errors from Hell to Heaven Imagine the software being designed in layers; we can think of the bottom layer as “hell” and the top layer, the closest to the user, as “heaven”. The error shown in the screenshot is quite clearly not meant to be shown to the user. It is too technical and in most cases not localised. This happened because a low-level error has been propagated through the various app layers up to the UI. For example: func getDataFromUrl(url: URL, completion:(_ data: Data?, _ error: NSError?) -> Void) {\n\t...\n\tif (error) {\n\t\t// Show error in UIAlertcontroller\n\t\t//or\n\t\t//send it up with a delegate/completion block to the higher level of the app.\n\t}\n\telse if (data) {\n\t\t// Do something\n\t}\n\t...\n} What has happened here can be illustrated as below: While some of the negative outcomes of this approach might appear evident, some others are more ambiguous. The obvious: Ugly and unintelligible error messages appearing on the user’s screen These error messages are clearly not meant for users. The only consequence of these being visible to users is a load of bad reviews in the App Store. The ambiguous: Loss of crucial information The same low-level (network/parsing/Core data) error can impact your app in many different ways. If only the original error is propagated, all the information in between is lost, not logged or tracked, and ultimately debugging the error becomes much more difficult. These undesirable outcomes for the user and the integrity of the code can – and should be – avoided. Effective error handling Or, How to propagate errors in a standard and consistent way. Error management is always an important part of any app, even more so if your app is fragmented into Internal Pods, Development Pods, Open Source Pods, Libraries, Utilities and Internal Frameworks. We’ll refer to these dependencies as Modules throughout the article. At Just Eat we explored several options for our Swift/Objective-C codebase. In the end, we chose to stick with the classic evergreen NSError and NSUnderlyingErrorKey universe, an approach used especially in macOS and suggested by Apple since the very first years of the platform. Before introducing the proposed solution, let’s go through few key concepts. Key concepts Anatomy of an NSError As explained in the official documentation and in this article by NSHipster, an NSError object is composed of: An error domain – A string representing the “Error context”, or where the error came from. An error code – An Int value representing the error. It is the responsibility of the error creator to make it meaningful. Different domains can have the same error codes with completely different meanings. A user info dictionary – A dictionary containing all the additional useful information about the error. The Error Chain An error chain is a linked list of NSErrors. Each error embeds the previous error in the user info dictionary using the standard key NSUnderlyingErrorKey . You can think of it as a standard linked list . This is illustrated below. The Just Eat approach Rationale We have put together a set of guidelines to unify our approach to error management across all the app’s parts, modules and libraries. The final result is a codebase that consistently Propagates Displays Logs any error generated across the app. That means that the final error structure received from the app UI is something like: {\n    Domain: com.yourapp.UI\n    Code: 3XXX\n    ...\n    UserInfo: {\n        Domain: com.yourapp.managerX\n        Code: 2XXX\n        …\n        UserInfo: {\n            Domain: com.yourapp.apiLib\n            Code: 1XXX\n            ...\n        \tUserInfo: {\n            \tDomain: NSERLErrorDomain\n            \tCode: 403\n            \t...\n        \t}\n        }\n    }\n} This approach preserves all the information of the error chain and allows every app layer to make decisions and enrich the error information. Error handling concepts In order to achieve this result we decided to enforce some basic concepts: Error builder Each app and module has its own error builder, where the error domain is specified and the errors are created, using the pre-assigned error code range. import ErrorUtilities\npublic let ModuleNameErrorDomain = \"com.yourapp.modulename\"\n@objc public enum ModuleNameErrorCode: ErrorCode {\n    case anErrorCase = X000\n    ...\n}\n@objc public class ModuleNameErrorBuilder: NSObject, ErrorBuilder {\n    public class func error(forCode code: ErrorCode) -> NSError {\n        var userInfo = [String: Any]()\n        userInfo[NSLocalizedDescriptionKey] = localizedDescriptionForCode(code: code)\n        userInfo[NSLocalizedFailureReasonErrorKey] = failureReasonForCode(code: code)\n        return NSError(domain: ApplePayErrorDomain, code: code, userInfo: userInfo)\n    }\n    private class func localizedDescriptionForCode(code: ErrorCode) -> String {\n        switch code {\n        case ModuleNameErrorCode.anErrorCase.rawValue:\n            return NSLocalizedString(\"your localised error description\", comment: \"ModuleName\")\n        ...\n    }\n    private class func failureReasonForCode(code: ErrorCode) -> String {\n        switch code {\n        case ModuleNameErrorCode.anErrorCase.rawValue:\n            return NSLocalizedString(\"your localised error failure reason\", comment: \"ModuleName\")\n        ...\n    }\n} Mandatory error propagation All the module interface methods that could report an error should be able to propagate it using any of the available techniques (for example using delegation, completion blocks, futures, Swift exceptions and so on). When a module-A receives an error from module-B, the error needs to be preserved and module-A should create a new error encapsulating the underlying error, incrementing the error chain. Errors and UI Only the errors created in the UI level error builder (com.yourapp.ui domain) can be presented to the user. Utility This approach introduces some unorthodox structures and ideas and needs a few utilities that we have collected in a tiny pod: https://github.com/justeat/iOS.ErrorUtilities ErrorBuilder – The protocol implemented by every Error builder in the app. NSError+Chaining – Collection of utilities to create and query error chains, insert and extract errors. NSError+HTTP – Query functions on error chains that identifies common HTTP errors. NSError+Readability – Utility functions to print a readable NSError. Conclusion Another part of our work that hugely benefits from this approach is error logging and the post-event debugging. Digging into the app remote or local logs is now much easier. Instead of having a spread list of errors, we now have a nice chain of errors that helps us rebuild the user journey and debug the app more efficiently using the console or our remote logging system (Kibana). Expanded: About the author Federico Cappelli – Senior iOS Engineer at Just Eat. ( Github | Github-JE )", "date": "2018-01-26"},
{"website": "JustTakeAway", "title": "Just Eat's Rapid Innovation Framework", "author": ["Written by George Powell"], "link": "https://tech.justeattakeaway.com/2018/01/03/just-eats-rapid-innovation-framework/", "abstract": "How does a large organisation rapidly innovate their existing product or break into a new opportunity space? We’ve produced a paper that shares how Just Eat retain a technological lead in a highly competitive space. Just Eat are the world’s leading global food marketplace, operating in a space we have lead for over fifteen years. The food tech space is highly competitive with a large number of start-ups appearing and disappearing each year. Over the last few years we have developed a finely tuned innovation framework that ensures our position as the industry leader. This paper is the first in a two part series, download the Just Eat Rapid Innovation Framework I .", "date": "2018-01-03"},
{"website": "JustTakeAway", "title": "Top 10 Voice Design Tips for the Amazon Echo Show", "author": ["Written by George Powell"], "link": "https://tech.justeattakeaway.com/2017/10/02/top-10-voice-design-tips-for-the-amazon-echo-show/", "abstract": "When we started work on the Amazon Echo Show design, our first feeling was of recognisable comfort. We’ve been designing voice interactions for over a year and half, but this new device brings a touch screen into the mix and, with it, a whole new set of design challenges and opportunities. In this article I’ll take you through some of the lessons we learnt adapting our voice-first Alexa experience to voice-first-with-screen, and give you the head-start you need to make the most out of your own voice-enabled apps. I’m Craig Pugsley, Principle Designer in Just Eat’s Product Research team. I’ve been designing touch-screen experiences for 10 years. Last year, we made a little journey into the world of voice-based apps with our original Amazon Echo skill, and it mangled my mind. Having just about got my head around that paradigm shift, Amazon came along with their new Echo Show device, with its 1024px x 600px touch screen, and everything changed again. I started getting flashes of adapting our iOS or android apps to a landscape aspect screen. Designing nice big Fitts-law-observing buttons that could be mashed from across the room. But it very soon became apparent that Amazon have been making some carefully orchestrated decisions about how experiences should be designed for their new ‘voice-first’ devices, and trying to adapt an existing visual experience just wouldn’t cut the mustard. A Bit of Background But I’m getting ahead of myself here. Let’s jump back to 2014, when Amazon brought to the US market the world’s first voice-enabled speaker. You could play music, manage calendars, order from the Amazon store, set kitchen timers, check the weather, etc… all with your voice, naturally, as though they were having a conversation with another human. Fast forward to 2017 and you can now pick from hundreds of third-party apps to extend the speaker’s functionality. Many of the big tech names have ‘skills’ for the Echo, including Uber, Sky, The Trainline, Jamie Oliver, Philips Hue and Just Eat. Since 2014, Amazon have brought a range of Alexa-enabled devices to market, at a multitude of wallet-friendly prices – starting with the £50 Echo Dot (like it’s big brother, but without the nice speaker) up to the new Echo Show at £199 (essentially a standard Echo, but with a touch screen and camera), with screens of all shapes and sizes in-between. Why did we get into voice? Our job is to hedge the company’s bets. Just Eat’s mission is to create the world’s greatest food community, and that community is incredibly diverse – from the individual who orders their weekly treat, all the way through to repeat customers using our upwards of thirty thousand restaurants to try something new every night. To be this inclusive, and let our restaurant partners reach the widest possible audience, we need to be available on every platform, everywhere our users are. Just Eat’s core teams are hard at work on the traditional platforms of iOS, Android and Web, so we take longer-shot calculated risks with new technologies, methodologies, business models and platforms. Being a small, rapidly-iterative, user-centred team, our goal is to fail more often than we succeed – and scout a route to interesting new platforms and interactions, without needing to send the whole army off in a new direction. So, we made a bet on voice. To be honest, it was a fairly low-risk gamble: the smartphone market has stagnated for years, become ripe for new innovation to make the next evolutionary step, and we’ve reached peak iPhone. We have projects looking at VR, AR, big screens, one buttons, distributed ordering, (so many, in fact, that we had to showcase them all in a swanky Shoreditch event last year). It was only natural that voice (or, more specifically, conversational user interfaces) would be in that mix. When we were handed an Amazon Echo device under a table in a cafe in London (sometime in early 2016 – several months before the Echo’s UK release) that gave us the route to market we were looking for. The Next Frontier From a Design perspective, conversational UIs are clearly the next interaction frontier. They’re the perfect fit for busy people, they don’t suffer from the cognitive load and friction of moving between inconsistently-designed apps’ walled gardens (something I’ve called Beautiful Room Syndrome ), and they have a slew of tangential benefits that might not be obvious at first thought. For example, our data seems to suggest that more users interacting with our skill seem to skew older. I find this fascinating! And entirely obvious, when you think about it. There’s a whole generation of people for whom technology is alien and removed from the kinds of interactions they’re used to. Now, almost out of nowhere, the technologies of deep learning, natural language processing, neural networks, speech recognition and cloud computing power have matured to enable a kind of interaction at once both startlingly new and compelling, whilst being so obvious, inevitable and natural. At last, these people who would have been forced to learn the complexities and vagaries of touchscreen interfaces to engage in the digital world, will be given access using an interface they’ve have been using since childhood. Amazon clearly recognised the new market they were unlocking. After the Amazon Echo speaker (around £150), they quickly followed up with a range of new devices and price points. Possibly most compelling is the £50 Echo Dot – a device barely larger than a paperback on it’s side, but packing all the same far-field microphone technology allowing it to hear you across the room and all the same Alexa-enabled smarts as it’s more expensive cousins. With the launch of the Echo Show, Amazon have addressed one of the more significant constraints of a voice-only interface: we live in an information age, and sometimes it’s just better to show what the user’s asked for, rather than describe it. Designing For Alexa Amazon’s design guidance on their screen-based devices is strong, and shows their obvious strategic push towards voice experiences that are augmented by simple information displays. Designing for the Show will give you all you need to translate your skill to Alexa on Fire tablets and Fire TVs, if and when Amazon enable these devices. It’s an inevitable natural progression of the voice interface, and Amazon have made some strategic design decisions to help make your skill as portable as possible. For example, you don’t have control over all of those 1024×600 pixels. Instead, you have (at the moment) 6 customisable templates that you can insert content into. Ostensibly, there are two types: lists and blocks of text. Into that, you have four font sizes and a range of basic markup you can specify (bold, italic, etc.). You can also insert inline images (although not animated GIFs – we tried!) and ‘action buttons’ which are controls that will fire the same action as if they user said the command. Each template also contains a logo in the top right, page title and a background image. It’s fair to say the slots you get to fill are fairly limited, but this is deliberate and positive step for the Alexa user experience. [For a more detailed breakdown of how to build an app for Echo Show, take a look at my colleague Andy May’s in-depth article] One key element is the background image you can display on each screen. You can make your background work really hard, so definitely spend some time exploring concepts. Amazon’s guidance is to use a photo, with a 70% black fill, but I find that too muddy and it felt too dark for our brand. Instead, we used our brand’s signature colours for the background to denote each key stage of our flow. I like how this subliminally suggests where the user is in the flow (e.g. while you’re editing your basket, the background remains blue) and gives a sense of progression. Top 10 Tips for Designing Voice Interactions Be Voice First You have to remember you’re designing an experience that is augmented with a visual display. This one’s probably the hardest to train yourself to think about – we’ve been designing UI-first visual interfaces for so long, that thinking in this voice-first way is going to feel really unnatural for a while. Start by nailing your voice-only flows first, then tactically augment that flow with information using the screen. The 7ft Test Amazon provide four font sizes for you to use: small, medium, large and extra large. You have to make sure crucial information is large enough to be read from 7ft away. Remember: users will almost certainly be interacting only with their voice, probably from across the room. Be Context Aware Your users have chosen to use your Alexa skill over your iOS app. Be mindful of that reason and context. Maybe their hands are busy making something? Maybe they’re dashing through the kitchen on their way out, and just remembered something? Maybe they’re multi-tasking? Maybe they’re an older user who is engaging with your brand for the first time? Use research to figure out how and why your users use your voice skill, and use that insight to design to that context. Don’t Just Show What’s Said An obvious one, but worth mentioning in this new world. Your engineers will need to build a view to be shown on the screen for each state of your flow – the Show platform will not show a ‘default’ screen automatically (which, we admit, is kinda weird) and you’ll end up in a situation where you’re showing old content while talking about something at an entirely different stage of the flow. Super confusing to the user. So, we found it was useful to start by building screens that displayed roughly what was being spoken about first, for every state. This will let you, the designer, make sure you’ve nailed your voice experience first, before then cherry-picking what you want to display at each state. You can use the display to show more than you’re saying, and even give additional actions to the user. Remember, like all good UX, less is most definitely more. Use the screen only when you think it would significantly add to the experience. If not, just display a shortened version of what you’re asking to the user. Typically, this could be one or two verb-based words, displayed in large font size. Be careful with lists In fact, be careful with how much information you’re saying, period. It’s a good design tip to chunk lists when reading them out (e.g. ‘this, this, this and this. Want to hear five more?’), but when you’ve got a screen, you can subtly adjust what you say to cue the user to look at the screen. You could, for example say ‘this, this, this and these five more’ while showing all eight on the screen. Consistency If you’re building a VUI with multiple steps in the flow, make sure you’re consistent in what you’re showing on screen. This is one of the few tips you can carry over from the world of visual UI design. Make sure you have consistent page titles, your background images follow some kind of semantically-relevant pattern (images related to the current task, colours that change based on state, etc…) and that you refer to objects in your system (verbs, nouns) repeatedly in the same way. You can (and should) vary what you say to users – humans expect questions to be asked and information to be presented in slightly different ways each time, so it feels more natural to be asked if they want to continue using synonymous verbs (‘continue’, ‘carry on’, ‘move on’, etc…). This is more engineering and voice design work, but it will make your experience feel incredibly endearing and natural. Be Wary of Sessions Remember what your user was doing, and decide whether you want to pick up that flow again next time they interact. If you’re building an e-comm flow, maybe you persist the basket between sessions. If you’re getting directions, remember where the user said they wanted to go from. This advice applies equally to non-screen Alexa devices, but it’s critical on the Show due to the way skills timeout if not interacted with. Users can tap the screen at any time in your flow. Alexa will stop speaking and the user has to say “Alexa” to re-start the conversation. If they don’t, your skill will remain on screen for 30 seconds, before returning to the Show’s home screen. When your user interacts with your skill again, you should handle picking up that state from where they were, in whatever way make sense to your skill. You could ask if they want to resume where they were, or you could figure out how long it was since they last interacted and decide that it’s been a couple of days, so they probably want to start again. Show the prompt to continue on screen This one is super-critical on the Echo Show. Best practise suggests that you should have your prompt question (the thing that Alexa will be listening to the answer for) at the end of her speech. But, if the user starts interacting with the screen, Alexa will immediately stop talking, and the user won’t hear the question and won’t know what to say to proceed. You need to decide what’s best for your skill, but we found that putting the prompt question in the page title (and doing it consistently on every page) meant users could safely interrupt to interact with the screen, while still having a clear indication of how to proceed. Worship your copywriter Another tip relevant to non-screen voice interfaces, but it really takes the nuanced skills of a professional wordsmith to target the same message to be both spoken, written in the companion app card, and displayed on the limited real estate of the Echo Show screen. Make sure you’re good friends with your team’s copywriter. By them beer regularly and keep them close to the development of your voice interface. Encourage them to develop personality and tone of voice style guides specifically for VUIs. They’re as much a core part of your design team as UX or User Researchers. Treat them well. In terms of user testing, we weren’t able to work with actual customers to test and iterate the designs for the Echo Show, as we routinely do with all our other products, due to the commercial sensitivity around the Echo Show UK release. So, we had to make the best judgements we could, based on the analytics we had and some expert reviewing within the team 😉 That said, we did plenty of internal testing with unsuspecting reception staff and people from other teams – Neisen’s guidance still stands: 5 users can get you 80% of usability issues , and we definitely found UX improvement, even testing with internals. Aside from the Show, we test future concepts in a wizard-of-oz style with one of us dialing in to the test lab and pretending to be Alexa. We get a huge amount of insight without writing a single line of code using this method, but that’s a whole other blog post for another day 😉 So there we go. Armed with these words of wisdom, and your existing voice-first skill, you should be fully equipped to create the next big app for the next big platform. Remember: think differently, this market is very new, look for users outside your traditional demographics and be prepared to keep your skills updated regularly as tech and consumer adoption changes. Good luck! Craig Pugsley Bristol, UK – Sept 2017 To find out more about the Just Eat Alexa Skill visit: https://www.just-eat.co.uk/alexa For more information visit on designing for Alexa visit: https://developer.amazon.com/designing-for-voice/", "date": "2017-10-02"},
{"website": "JustTakeAway", "title": "Top 5 Tips for Building Just Eat on Amazon’s Echo Show", "author": ["Written by George Powell"], "link": "https://tech.justeattakeaway.com/2017/10/02/top-5-tips-for-building-just-eat-on-amazons-echo-show/", "abstract": "Hi, I’m Andy May – Senior Engineer in Just Eat’s Product Research team. I’m going to take you through some top tips for porting your existing Alexa voice-only skill to Amazon’s new Echo Show device, pointing out some of the main challenges we encountered and solved. Since we started work on the Just Eat Alexa skill back in 2016, we’ve seen the adoption to voice interfaces explode in popularity. Amazon’s relentless release schedule for Alexa-based devices has fueled this, but the improvements in the foundational tech (AI, deep learning, speech models, cloud computing) coupled with the vibrant third-party skill community look set to establish Alexa as arguably the leader in voice apps. From an engineering perspective adapting our existing code base to support the new Echo Show was incredibly easy. But, as with any new platform, simply porting an existing experience across doesn’t do the capabilities of the new platform justice. I worked incredibly closely with my partner-in-crime Principle Designer Craig Pugsley to take advantage of what now became possible with a screen and touch input. In fact, Craig’s written some top tips about exactly that just over here … In order to add a Show screen to your voice response you simply extend the JSON response to include markup that describes the template you want to render on the device. The new template object (Display.RenderTemplate) is added to a directives Array in the response. {\n…\n“directives”: [\n  {\n  “type”: “Display.RenderTemplates”,\n  …\n  }\n]\n} For more details on the Alexa response object visit https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interface-reference#response-body-syntax Sounds simple, doesn’t it? Well, it’s not rocket science, but it does have a few significant challenges that I wished someone had told me about before I started on this adventure. Here are five tips to help you successfully port your voice skill to voice-and-screen. 1. You need to handle device-targeting logic The first and main gotcha we found was that you cannot send a response including a template to a standard Echo or Dot device. We incorrectly assumed a device that does not support screens would simply ignore the additional objects in the response. Our own Conversation Class that all Alexa requests and responses go though is built on top of the Alea Node SDK. The SDK did not exist when we first launched our Skill. We added a quick helper method from the Alexa Cook Book ( https://github.com/alexa/alexa-cookbook/blob/master/display-directive/listTemplate/index.js#L589 ) to check if we are dealing with an Echo Show or voice only device. get supportsDisplay(): boolean {\n   const hasDisplay =\n     this.alexa.event.context &&\n     this.alexa.event.context.System &&\n     this.alexa.event.context.System.device &&\n     this.alexa.event.context.System.device.supportedInterfaces &&\n     this.alexa.event.context.System.device.supportedInterfaces.Display;\n   return hasDisplay;\n } This method is called before we return our response to ensure we only send RenderTemplates to devices that support them. Finally we extended our Response Class to accept the new template objects and include them in the response sent to Alexa. The result visual screens are displayed on the Echo Show alongside the spoken voice response. 2. Don’t fight the display templates There are currently 6 templates provided to display information on the Echo Show. We decided to create one file this means the markup and structure is only declared once. We then pass the data we need to populate the template. Object destructuring, string literals alongside array.map and array.reduce make generating templates easy. We use Crypto to generic a unique token for every template we return. Image of list – mapping basket to template listItems. Image of basket list  – reducing basket to single string. justEatCuisineList(data) {\nconst { cuisineData, title } = data;\nconst cuisineItems = cuisineData.map(x => ({\n  Token: “...”\n  Image: {...}\n  textContent: {...}\n }) )\n// template object\nreturn {\n…\nlistItems: cuisineItems\n...\n}\n} Markup is limited to basic HTML tags including line breaks, bold, italic, font size, inline images, and action links. Action Links are really interesting but the default blue styling meant we have so far had to avoid using them. Many of the templates that support images take an array of image objects however just the first image object is used. We experimented providing more than one image to provide a fallback image or randomise the image displayed. The lack of fallback images means that we need to make a request to our S3 bucket to validate the image exists before including in the template. Don’t try to hack these templates to get them to do things that weren’t designed for. Each template’s capabilities have been consciously limited by Amazon to give users a consistent user experience. Spend your time gently stroking your friendly designer and telling them they’re in a new world now. Set their expectations around the layouts, markup and list objects that are available. Encourage them to read Craig’s post. 3. Take advantage of touch input alongside voice The Echo Show offers some great new functionality to improve user experience and make some interactions easier. Users can now make selections and trigger intents but touching the screen or saying the list item number “select number 2”. It is your job to implement capture touch and voice selection. When a user selects a list item you code will receive a new request object of type Display.ElementSelected. The token attribute you specify when creating the list is passed back in this new request object: \"request\": {\n    \"type\": \"Display.ElementSelected\",\n    \"requestId\": \"amzn1.echo-api.request.xxx\",\n    \"timestamp\": \"2017-06-06T20:05:04Z\",\n    \"locale\": \"en-UK\",\n    \"token\": \"Indian\"\n  } In the above example we receive the value ‘Indian’ and can treat this in the same way we would the cuisine slot value. Our state management code knows to wait for the cuisine intent with slot value or Display.ElementSelected request. Finally we create a new Intent, utterances an a slot to handle number selection. If our new Intent is triggered with a valid number we simply match the cuisine value from the cuisine array in state with a index offset. Find out more about touch and voice selection – https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/display-interface-reference#touch-selection-events 4. Adapt your response based on device The Echo Show provides lots of opportunities and features. In one part of our Skill we decided to change the flow and responses based on the device type. When we offer users the opportunity to add popular dishes it made sense for us to shorten the flow as we can add use the screen in addition to the voice response. We use the same supportsDisplay method to change the flow of our skill. if(this.converstaion.supportsDisplay) {\n // Echo Show - skip to listing popular dishes\n} else {\n // Standard device - prompt user to see if they want to add a popular dish\n} We use the same logic when displaying the list of popular dishes. Based on Amazon recommendations if the device supports display we don’t read out all the dishes. You can find out more about our thoughts designing user experience for the Echo Show here. 5. The back button doesn’t work The back button caused us some problems. When a user touches the back button the Echo Show will display the previous template. Unfortunately no callback is sent back to your code. This creates huge state management problem for us. For example a user can get the checkout stage at this point our state engine expects only a 2 intents Pay Now or Change Something  (exc back, cancel and stop). If a Echo Show user touched back the template would now show our Allergy prompt. The state engine does not know this change has taken place so we could  not process the users Yes/No intents to move on from allergy as think the user is still on the checkout stage. Just to add to this problem the user can actually click back through multiple templates. Thankfully you can disable the back button in the template response object: backButton: 'HIDDEN' To find out more about the Just Eat Alexa Skill visit https://www.just-eat.co.uk/alexa For more information visit on developing Alexa Display Interface visit https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/display-interface-reference", "date": "2017-10-02"},
{"website": "JustTakeAway", "title": "Reliably Testing HTTP Integrations in a .NET Application", "author": ["Written by Martin-Costello"], "link": "https://tech.justeattakeaway.com/2017/10/02/reliably-testing-http-integrations-in-a-dotnet-application/", "abstract": "Introduction Testing HTTP dependencies in modern web applications is a common problem, but it’s also something that can create difficulty for authoring reliable tests. Today, we’re open-sourcing a library to help reduce the friction many developers have with this common requirement: JustEat.HttpClientInterception. You can find the repository in our GitHub organisation and can find the package available to download at JustEat.HttpClientInterception on NuGet.org . The Problem Many modern software applications integrate with external Application Programming Interfaces (APIs) to provide solutions for problems within their domain of responsibility, whether it be delivering your night in, booking a flight, trading financial instruments, or monitoring transport infrastructure in real-time. These APIs are very often HTTP-based, with RESTful APIs that consume and produce JSON often being the implementation of choice. Of course integrations might not be with full-blown APIs, but just external resources that are available over HTTP, such as a website exposing HTML, or a file download portal. In .NET applications, whether these are console-based, rich GUIs, background services, or ASP.NET web apps, a common go-to way of consuming such HTTP-based services from code is using the `HttpClient` class. `HttpClient` provides a simple API surface for `GET`-ing and `POST`-ing resources over HTTP(S) to external services in many different data formats, as well as functionality for reading and writing HTTP headers and more advanced extensibility capabilities. It is also commonly exposed as a dependency that can be injected into other services, such as third-party dependencies for tasks such as implementing OAuth-based authentication. Overall this makes `HttpClient` a common and appropriate choice for writing HTTP-based integrations for .NET applications. An important part of software development is not just implementing a solution to a given problem, but also writing tests for your applications. A good test suite helps ensure that delivered software is of a high quality, is functionally correct, is resilient in the face of failure, and provides a safety net against regression for future work. When your application depends on external resources though, then testing becomes a bit more involved. You don’t want to have the code under test making network calls to these external services for a myriad of reasons. They make your tests brittle and hard to maintain, require a network connection to be able to run successfully, might cost you money, and slow down your test suite, to name but a few examples. These issues lead to approaches using things like mocks and stubs. `HttpClient`, and its more low-level counterpart `HttpMessageHandler` , are not simple to mock however. While not difficult to do, their lack of interface and design lead to a requirement to implement classes that derive from `HttpMessageHandler` in order to override `protected` members to drive test scenarios, and to build non-primitive types by hand, such as `HttpResponseMessage` . Another approach that can be used to simplify the ability to use mocks is to create your own custom `IHttpClient` interface and wrap your usage of `HttpClient` within an implementation of this interface. This creates its own problems in non-trivial integrations though, with the interface often swelling to the point of being a one-to-one representation of `HttpClient` itself to expose enough functionality for your use-cases. While this mocking and wrapping is feasible, once your application does more than one or two simple interactions with an HTTP-based service, the amount of test code required to drive your test scenarios can balloon quite quickly and become a burden to maintain. It is also an approach that only works in a typical unit test approach. As usage of `HttpClient` is typically fairly low down in your application’s stack, this does not make it a viable solution for other test types, such as functional and integration tests. A Solution Today we’re publishing a way of solving some of these problems by releasing our JustEat.HttpClientInterception .NET library as open-source to our organisation in GitHub.com under the Apache 2.0 licence. A compiled version of the .NET assembly is also available from JustEat.HttpClientInterception on NuGet.org that supports .NET Standard 1.3 (and later) and .NET Framework 4.6.1 (and later). `JustEat.HttpClientInterception` provides a number of types that allow HTTP requests and their corresponding responses to be declared using the builder pattern to register interceptions for HTTP requests in your code to bypass the network and return responses that drive your test scenarios. Below is a simple example that shows registering an interception for an HTTP GET request to the Just Eat Public API : // Install-Package JustEat.HttpClientInterception\n// using JustEat.HttpClientInterception;\nvar builder = new HttpRequestInterceptionBuilder()\n.ForHost(\"public.je-apis.com\")\n.ForPath(\"terms\")\n.WithJsonContent(new { Id = 1, Link = \"https://www.just-eat.co.uk/privacy-policy\" });\nvar options = new HttpClientInterceptorOptions()\n.Register(builder);\n// Create an instance of HttpClient to make requests with\nvar client = options.CreateHttpClient();\n// The value of the json variable will be \"{\\\"Id\\\":1,\\\"Link\\\":\\\"https://www.just-eat.co.uk/privacy-policy\\\"}\"\nvar json = await client.GetStringAsync(\"http://public.je-apis.com/terms\"); The library provides a strongly-typed API that supports easily setting up interceptions for arbitrary HTTP requests to any URL and for any HTTP verb, returning responses that consist of either raw bytes, strings or objects that are serialized into a response as-and-when they are required. Fault injection is also supported by allowing arbitrary HTTP codes to be set for intercepted responses, as well as latency injection via an ability to specify a custom asynchronous call-back that is invoked before the intercepted response is made available to the code under test. With ASP.NET Core adding Dependency Injection as a first-class feature and being easy to self-host for use within test projects , a small number of changes to your production code allows `HttpClientInterceptorOptions` to be injected into your application’s dependencies for use with integration tests without your application needing to take a dependency on `JustEat.HttpClientInterception` for itself. With the library injected into the application, HTTP requests using `HttpClient` and/or `HttpMessageHandler` that are resolved by your IoC container of choice can be inspected and intercepted as-required before any network connections are made. You can also opt-in to behaviour that throws an exception for any un-intercepted requests, allowing you to flush out all HTTP requests made by your application from your tests. Further examples of using the library can be found at these links: In the project’s README file A sample application Some example tests The Benefits We’ve used this library successfully with two internal applications we’re developing with ASP.NET Core (one an API, the other an MVC website) to really simplify our tests, and provide good code coverage, by using a test approach that is primarily a black-box approach . The applications’ test suites self-host the application using Kestrel , with the service registration set-up to create a chain of `DelegatingHandler` implementations when resolving instances of `HttpClient` and `HttpMessageHandler`. With `HttpClientInterceptorOptions` registered to provide instances of `DelegatingHandler` by the test start-up code when the application is self-hosted, this allows all HTTP calls within the self-hosted application in the tests to be intercepted to drive the tests. The tests themselves then either initiate HTTP calls to the public surface of the self-hosted server with a vanilla `HttpClient` in the case of the API, or use Selenium to test the rendered pages using browser automation in the case of the website. This approach provides many benefits, such as: Simple setup for testing positive and negative code paths for HTTP responses, such as for error handling. Exercises serialization and deserialization code for HTTP request and response bodies. Testing behaviour in degraded scenarios, such as network latency, for handling of timeouts. Removes dependencies on external services for the tests to pass and the need to have access to an active network connection for services that may only be resolvable on a internal/private network. No administrative permissions required to set-up port bindings. Speeds up test execution by removing IO-bound network operations. Allows you to skip set-up steps to create test data for CRUD operations, such as having to create resources to test their deletion. Can be integrated in a way that other delegating handlers your application may use are still exercised and tested implicitly. Allows us to intercept calls to IdentityServer for our user authentication and issue valid self-signed JSON Web Tokens (JWTs) in the tests to authenticate browser calls in Selenium tests. In the case of the ASP.NET Core API using this test approach, at the time of writing, we’ve been able to achieve over 90% statement coverage of a several thousand line application with just over 200 unit, integration and end-to-end tests. Using our TeamCity server, the build installs the .NET Core runtime, restores its dependencies from NuGet, compiles all the code and runs all the tests in just over three-and-a-half minutes. Some Caveats Of course such a solution is not a silver bullet. Intercepting all of your HTTP dependencies isolates you from interface changes in your dependencies. If an external service changes its interfaces, such as by adding a new API version or deprecating the one you use, adds new fields to the responses, or changes to require all traffic to support HTTPS instead of HTTP, your integration tests will not find such changes. It also does not validate that your application integrates correctly with APIs that require authentication or apply rate-limits. Similarly, the black-box approach is relatively heavyweight compared to a simple unit test, so may not be suited to testing all of the edge cases in your code and low-level assertions on your responses. Finally, your intercepted responses will only cater for the behaviour you’ve seen and catered-for within your tests. A real external dependency may change its behaviour over time in ways that your static simulated behaviours will not necessarily emulate. A good mixture of unit, interception-based integration tests, and end-to-end tests against your real dependencies are needed to give you a good robust test suite that runs quickly and also gives you confidence in your changes as you develop your application over time. Shipping little and often is a key tenet of Continuous Delivery. In Conclusion We hope that you’ve found this blog post interesting and that you find `JustEat.HttpClientInterception` useful in your own test suites for simplifying things and making your applications even more awesome. You can find the project in our organisation on GitHub and you can download the library to use in your .NET projects from the JustEat.HttpClientInterception package page on NuGet.org . Contributions to the library are welcome – check out the contributing guide if you’d like to get involved! If you like solving problems at scale and don’t think testing is just for QAs, why not check out open positions in Technology at Just Eat ? About the Author This blog post was written by Martin Costello , a Senior Engineer at Just Eat, who works on consumer-facing websites and the back-end services that power them.", "date": "2017-10-02"},
{"website": "JustTakeAway", "title": "OWASP meetup in Just Eat", "author": ["Written by Dave Williams"], "link": "https://tech.justeattakeaway.com/2017/07/27/owasp-meetup-in-just-eat/", "abstract": "We are looking forward to tonight’s OWASP London Chapter event at Just Eat offices in London. We will start broadcasting live at 6:30 PM UK time today. watch live now on youtube", "date": "2017-07-27"},
{"website": "JustTakeAway", "title": "How to abstract your persistence layer and migrate to another one on iOS with JustPersist", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2017/03/02/how-to-abstract-your-persistence-layer-and-migrate-to-another-one-on-ios-with-justpersist/", "abstract": "In this blog post we introduce a solution to deal with data persistence. We developed it for the Just Eat iOS app and we call it JustPersist. It’s available open source on Github at github.com/justeat/JustPersist . JustPersist aims to be the easiest and safest way to do persistence on iOS with Core Data support out of the box. It also allows you to migrate to any new persistence framework with minimal effort. The main author behind JustPersist and its design is Keith Moon . Major kudos to Keith for the excellent execution in Swift! Overview At Just Eat, we persist a variety of data in the iOS app. In 2014 we decided to use MagicalRecord as a wrapper on top of Core Data but over time the numerous problems and fundamental thread-safety issues, arose. In 2017, MagicalRecord is not supported anymore and new solutions look more appealing. We decided to adopt Skopelos : a much younger and lightweight Core Data stack, with a simpler design, developed by Alberto De Bortoli , one of our engineers. The design of the persistence layer interface gets inspiration from Skopelos as well, and we invite the reader to take a look at its documentation . The main problem in adopting a new persistence solution is migrating to it. It is rarely easy, especially if the legacy codebase doesn’t hide the adopted framework (in our case MagicalRecord) but rather spread it around in view controllers, managers, helper classes, categories and sometimes views. Ultimately, in the case of Core Data, there is a single persistent store and this is enough to make impossible to move access across “one at a time”. There can only be one active persistence solution at a time. We believe this is a very common problem, especially in the mobile world. We created JustPersist for this precise reason and to ease the migration process. At the end of the day, JustPersist is two things: a persistence layer with a clear and simple interface for transactional readings and writings (Skopelos-style) a solution to migrate from one persistence layer to another with (we believe) the minimum possible effort JustPersist aims to be the easiest and safest way to do persistence on iOS. It supports Core Data out of the box and can be extended to transparently support other frameworks. Since moving from MagicalRecord to Skopelos, we provide available wrappers for these two frameworks. The tone of JustPersist is very much Core Data-oriented but it enables you to migrate to any other persistence framework if a custom data store (wrapper) is implemented (in-memory, key-value store, even Realm if you are brave enough). JustPersist is available through CocoaPods . To install it, simply add the following line to your Podfile: pod \"JustPersist/Skopelos\"\n# or\npod \"JustPersist/MagicalRecord\" Using only pod JustPersist will add the core pod with no subspecs and you’ll have to implement your own wrapper to use the it. If you intend to extend JustPersist to support other frameworks, we suggest creating a subspec. Usage of the persistence layer To perform operation you need a data store, which you can setup like this (or see “common way of setting up a data store”): let dataStore = SkopelosDataStore(sqliteStack: <modelURL>)\n// or\nlet dataStore = MagicalRecordDataStore() Before using the data store for the first time, you must call `setup()` on it, and possibly `tearDown()` when you are completely done with it. We suggest setting up the stack at app startup time, in the `applicationDidFinishLaunchingWithOptions` method in the AppDelegate and to tear it down at the end of the life cycle of your entire app, when resetting the state of the app (if you provide support to do so) or in the `tearDown` method of your unit tests suite. To hide the underlying persistence framework used, JustPersist provides things that conform to `DataStoreItem` and `MutableDataStoreItem`, rather than the CoreData specific `NSManagedObject`. These protocols provide access to properties using `objectForKey` and `setObject:forKey:` methods. In the case of Core Data, JustPersist provides an extension to `NSManagedObject` to make it conforming to `MutableDataStoreItem`. Readings and writings The separation between readings and writings is the foundation of JustPersist. Reading are always synchronous by design: dataStore.read { (accessor) in\n  ...\n} While writings can be both synchronous or asynchronous: dataStore.writeSync { (accessor) in\n  ...\n}\ndataStore.writeAsync { (accessor) in\n  ...\n} The accessor provided by the blocks can be a read one (`DataStoreReadAccessor`) or a read/write one (`DataStoreReadWriteAccessor`). Read accessors allow you to do read operations such as: func items(forRequest request: DataStoreRequest) -> [DataStoreItem]\nfunc firstItem(forRequest request: DataStoreRequest) -> DataStoreItem?\nfunc countItems(forRequest request: DataStoreRequest) -> Int While the read/write ones allow you to perform a complete set of CRUD operations: func mutableItems(forRequest request: DataStoreRequest) -> [MutableDataStoreItem]\nfunc firstMutableItem(forRequest request: DataStoreRequest) -> MutableDataStoreItem?\nfunc createItem(ofMutableType itemType: MutableDataStoreItem.Type) -> MutableDataStoreItem?\nfunc insert(_ item: MutableDataStoreItem) -> Bool\nfunc delete(item: MutableDataStoreItem) -> Bool\nfunc deleteAllItems(ofMutableType itemType: MutableDataStoreItem.Type) -> Bool\nfunc mutableVersion(ofItem item: DataStoreItem) -> MutableDataStoreItem? To perform an operation you might need a `DataStoreRequest` which can be customized with itemType, an NSPredicate, an array of NSSortDescriptor, offset and limit. Think of it as the corresponding Core Data’s `NSFetchRequest`. Here are some complete examples: dataStore.read { (accessor) in\n  let request = DataStoreRequest(itemType: Restaurant.self)\n  let count = accessor.countItems(forRequest: request)\n}\ndataStore.read { (accessor) in\n  let request = DataStoreRequest(itemType: Restaurant.self)\n  request.setFilter(whereAttribute: \"name\", equalsValue: <some_name>)\n  guard let restaurant = accessor.firstItem(forRequest: request) as? Restaurant else { return }\n  ...\n}\ndataStore.writeSync { (accessor) in\n  let restaurant = accessor.createItem(ofMutableType: Restaurant.self) as! Restaurant\n  restaurant.name = <some_name>\n  ...\n  let wasDeleted = accessor.delete(item: restaurant)\n} In write blocks there is no need to make any call to a save method. Since it would be the obvious thing to do at the end of a transactional block, JustPersist does it for you. Read blocks are not meant to modify the store and you wouldn’t even have the API available to do so (unless `DataStoreItem` objects are casted to `NSManagedObject` in the case of CoreData to allow the setting of properties), therefore a save will not be performed under the hood. Common way of setting up a data store We recommend to use dependency injection to pass the data store around but sometimes it might be hard. If you wish to access your data store via a singleton, here is how your app could create a shared instance for the DataStoreClient (e.g. `DataStoreClient.swift`) using Skopelos. @objc\nclass DataStoreClient: NSObject {\nstatic let shared: DataStore = {\n  return DataStoreClient.sqliteStack()\n}()\nstatic let inMemoryShared: DataStore = {\n  return DataStoreClient.inMemoryStack()\n}()\nclass func sqliteStack() -> DataStore {\n  let modelURL = Bundle.main.url(forResource: \"\", withExtension: \"momd\")! // want to crash if schema is missing\n  return SkopelosDataStore(sqliteStack: modelURL, securityApplicationGroupIdentifier: ) { error in\n    print(\"Core Data error reported via SkopelosDataStore (sqliteStack): \\(error.localizedDescription)\")\n  }\n}\nclass func inMemoryStack() -> DataStore {\n  let modelURL = Bundle.main.url(forResource: \"\", withExtension: \"momd\")! // want to crash if schema is missing\n  return SkopelosDataStore(inMemoryStack: modelURL) { error in\n    print(\"Core Data error reported via SkopelosDataStore (inMemoryStack): \\(error.localizedDescription)\"\")\n  }\n} For unit tests, you might want to use the `inMemoryShared` for better performance. Child data store A child data store is useful in situations where you might have the need to rollback all the changes performed in a specific section of the app or in a part of the user journey. Think of it as a scratch/disposable context in the Core Data stack by Marcus Zarra. At Just Eat we use a child data store for the addition of complex products to the basket. The user might make many updates to the product and it is easier to perform the final save operation when the user confirms the addition rather than dealing with multiple CRUD operations on the main data store. A child data store behaves just like a normal data store, with the only exception that, to save the changes back to the main data store, developers must explicitly merge the data stores. Here is a complete example: let childDataStore = dataStore.makeChildDataStore()\nchildDataStore.setup()\n...\ndataStore.merge(childDataStore)\nchildDataStore.tearDown() Thread-safety notes Read and sync write blocks are always performed on the main thread, no matter which thread calls them. Async write blocks are always performed on a background thread. Sync writings return only when the changes are persisted (in the case of Core Data, usually to the `NSManagedObjectContext` with main concurrency type). Async writings return immediately and leave the job of saving to the source of truth to JustPersist (whether it be the context or a persistent store). They are eventual consistent, meaning that the next reading could potentially not have the data available. Forcing a transactional programming model for readings and writings helps developers to avoid thread-safety issues which in Core Data can be caught setting the `-com.apple.CoreData.ConcurrencyDebug 1` flag in your scheme (which we recommend enabling). How to migrate to a different persistence layer Examples in this sections are in Objective-C as 1. they deal with the legacy code for the nature of the example and 2. to show that JustPersist works just fine with Objective-C too. Here we’ll outline the steps we made to migrate away from MagicalRecord to Skopelos using JustPersist. We believe that a lot of apps still use MagicalRecord, so this may apply to your case too. If your need is to move from and to other 2 frameworks, you need to implement the corresponding data stores to wrap them. You should start by implementing your `DataStoreClient` (you could follow the steps in “common way of setting up a data store” and allocating the data store for the current persistence layer used by your app in the `sqliteStack` method and possibly in the `inMemoryStack` one too. In our case, since we want to move away from MagicalRecord, the data store used would be `MagicalRecordDataStore`. Standard CRUD interactions with MagicalRecord are like so: NSManagedObjectContext *mainContext = [NSManagedObjectContext MR_defaultContext];\nNSManagedObjectContext *childContext = [NSManagedObjectContext MR_contextWithParent:mainContext];\n// writing (Create)\n[childContext performBlockAndWait:^{\n  Restaurant *restaurant = [Restaurant MR_createEntityInContext:localContext];\n  [childContext MR_saveToPersistentStoreAndWait];\n}];\n// reading (Read)\n[childContext performBlockAndWait:^{\n  Restaurant *restaurant = [Restaurant MR_findFirstInContext:childContext];\n}];\n// writing (Update)\n[childContext performBlockAndWait:^{\n  Restaurant *restaurant = [Restaurant MR_findFirstInContext:childContext];\n  restaurant.name = <some_name>\n  [childContext MR_saveToPersistentStoreAndWait];\n}];\n// writing (Delete)\n[childContext performBlockAndWait:^{\n  [Restaurant MR_truncateAllInContext:localContext];\n  [childContext MR_saveToPersistentStoreAndWait];\n}]; All of them should be converted one by one to JustPersist: DataStore *dataStore = [DataStoreClient shared];\n// writing (Create)\n[dataStore writeSync:^(id accessor) {\n  Restaurant *restaurant = (Restaurant *)[accessor createItemOfMutableType:Restaurant.class];\n}];\n// reading (Read)\n[dataStore read:^(id accessor) {\n  JEDataStoreRequest *request = [[JEDataStoreRequest alloc] initWithItemType:Restaurant.class];\n  Restaurant *restaurant = (Restaurant *)[accessor firstItemForRequest:request];\n}];\n// writing (Update)\n[dataStore writeSync:^(id accessor) {\n  JEDataStoreRequest *request = [[JEDataStoreRequest alloc] initWithItemType:Restaurant.class];\n  Restaurant *restaurant = (Restaurant *)[accessor firstItemForRequest:request];\n  restaurant.name = <some_name>\n}];\n// writing (Delete)\n[dataStore writeSync:^(id accessor) {\n  [accessor deleteAllItemsOfMutableType:Restaurant.class];\n}]; You should make sure you don’t perform any UI work within the blocks even if the `read` and `writeSync` ones are executed on the main thread. Actually, you should aim for doing only the necessary work related to interact with the persistence layer, which often might be copying values out of objects to have them accessible outside the block (in Objective-C via the `__block` keyword). Developers should not hold references to model objects to pass them around threads (transactional blocks help ensure such rule). By having moved all the direct interactions from MagicalRecord to JustPersist, you should be now able to remove all the various `@import MagicalRecord` and `#import <MagicalRecord/MagicalRecord.h>` from the entire codebase. Once At this point, your `DataStoreClient` can be modified to allocate the target data store in the `sqliteStack` and `inMemoryStack` methods. In our case, the `SkopelosDataStore`. Conclusion JustPersist aims to be the easiest and safest way to do persistence on iOS. It supports Core Data out of the box and can be extended to transparently support other frameworks. You can use JustPersist to migrate from one persistence layer to another with minimal effort. Since we moved from MagicalRecord to Skopelos, we provide available wrappers for these two frameworks. At its core, JustPersist is a persistence layer with a clear and simple interface to do transactional readings and writings, taking inspirations from Skopelos where readings and writings are separated by design. We hope this library will ease the process of setting up a persistence stack, avoiding the common headache of Core Data and potential threading pitfalls. About the authors Alberto De Bortoli is the Principal iOS Engineer at Just Eat. Keith Moon is a Senior iOS Engineer in the Payments team at Just Eat.", "date": "2017-03-02"},
{"website": "JustTakeAway", "title": "Xamarin 101, S01E01 – UI Tests", "author": ["Written by Chris Newton"], "link": "https://tech.justeattakeaway.com/2017/03/23/xamarin-101-s01e01-ui-tests/", "abstract": "Xamarin 101 is a new series that we hope will prepare you and your team to use Xamarin in production. Each episode will focus on one particular topic on Xamarin development. Subscribe to our meetup page to receive notifications about all our future events including the next Xamarin 101 episodes, we’ll also give you free pizza and drinks whilst you learn, win-win. https://www.meetup.com/London-Mobile-Dev . Our first episode was about UI Tests, the full presentation is now available on our Youtube channel! Two of our JUST EAT Xamarin Engineers also spoke at the event. Xamarin UITest and Xamarin Test Cloud by Gavin Bryan This talk covered Xamarin’s Automation Test library, which allows you to create, deploy and run automation tests on mobile devices, simulators and emulators. The library is based on Calabash and allows you to write automation tests in C# using NUnit in a cross-platform manner so that tests can be shared across different platforms if required. The library is very rich in functionality, allowing quite involved and complex automation tests to be written. The talk gave an overview of some basic test automation and tools available for creating and running tests. The automation tests can be run both locally, in CI environments and in Xamarin Test Cloud (XTC). XTC is an on-demand cloud based service with over 2000 real mobile devices available to run your automation tests on. We showed the options available for running automation tests on a variety of devices in XTC and showed the analysis and reporting that was available in XTC. Presentation Assets Slides – goo.gl/TXTVzI Demo – goo.gl/QiKprk ____ BDD in Xamarin with Specflow and Xamarin UITest by Emanuel Amiguinho Following Gavin’s presentation, it was time to bring BDD to Xamarin development using Specflow to fill in the gap between Gherkin Feature/Steps definition and Xamarin.UITest framework to have the best UI test coverage possible and good documentation that everyone inside of your team can understand (technical and non-technical personnel). Presentation Assets Slides – goo.gl/ITWen8 Demo – goo.gl/7BDpfp ____ Our next topic is databases and we are currently looking for speakers that have had experience with any type of local database in their development (SQLite, DocumentDB, Realm database, etc). If you are interested, please send an email outlining which database you want to talk about and your availability to: emanuel.amiguinho@just-eat.com or nathan.lecoanet@just-eat.com", "date": "2017-03-23"},
{"website": "JustTakeAway", "title": "iOS Event tracking with JustTrack", "author": ["Written by Chris Newton"], "link": "https://tech.justeattakeaway.com/2017/02/01/ios-event-tracking-with-justtrack/", "abstract": "Overview At Just Eat , tracking events is a fundamental part of our business analysis and the information we collect informs our technical and strategic decisions. To collect the information required we needed a flexible, future-proof and easy to use tracking system that enables us to add, remove and swap the underlying integrations with analytical systems and services with minimal impact on our applications’ code. We also wanted to solve the problem of keeping the required event metadata up-to-date whenever the requirements change. JustTrack is the event tracking solution we built for that and it’s available open source on Github at https://github.com/justeat/JustTrack . Examples and documentation are available in the Github repository Readme . Main features Events are declared in a .plist file and Swift 3 code is automatically generated at build time from it. Events can be sent to multiple destinations (called Trackers ) at the same time . Custom Trackers are easy to create and use. Events Definition One of the problems we found with existing solutions is that the events are declared in code and therefore can only be maintained by developers. Similarly, existing solutions offer very generic tracking facilities for developers. Because of that, whenever the required metadata associated with an event changes for any reason, the developer has to search the code base and update all instances of the event with the correct implementation. This, of course, is a very fragile process and is prone to errors. JustTrack solves these problems by declaring events in a plist file that is used to automatically generate equivalent definitions of the events in Swift that can be used in the app. This provides several benefits: Each event is uniquely identified. The metadata associated with each event is type checked. When the requirements for an event change, the developers can see it through build errors and warnings that will naturally occur. Plists can be edited as XML, which means anybody in the business can edit them. It’s easy to search for events that are no longer used and deleted events won’t compile. An Event is made of: Name : the unique identifier Registered Trackers : List of event destinations (e.g. Google Analytics) Payload : The metadata associated with the event (at this time only String key-value pairs are supported) Trackers Another problem we found with existing solutions is that, generally speaking, all the events need to be tracked with the same Trackers. The developer doesn’t have any freedom to decide which event goes to which Tracker, and several solutions only support specific tracking technologies (such as GA, Firebase, and so on) JustTrack solves this problem by allowing the developer to specify the “registered” trackers for each event and to create custom trackers. A Tracker is an object implementing the JETracker protocol and is loaded using: tracker.loadCustomTracker( ... ) function. You can implement any tracker you want and JustTrack provides a few default trackers: JETrackerConsole – print events to the system’s console JEFacebookTraker (not yet implemented) JEGoogleAnalyticsTraker (not yet implemented, Google’s pods can’t be used as a dependency in a pod) JETrakerFirebase (not yet implemented, Google’s pods can’t be used as a dependency in a pod) Conclusions Using JustTrack you can easily track your app’s events to multiple destinations, having all the necessaire flexibility, intrinsic events documentation and all the expandability you will ever need in future, all of that writing the minimum amount of code possible. About the author Federico Cappelli is a Senior iOS Engineer and iOS COG Leader at Just Eat.", "date": "2017-02-01"},
{"website": "JustTakeAway", "title": "Troy Hunt – Hack Yourself First workshop at Just Eat", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2017/05/03/security-training-with-troy-hunt/", "abstract": "2016 was a year full of internet security issues from the Yahoo breach, to TalkTalk hack to US Election rigging and the massive Tesco Bank breach to an Internet crippling DDoS attack . Today, internet security is now no longer just the domain of techies and security experts, but the responsibility of all of us. A close-up of Troy Hunt’s demo site with hacked videos inserted I remember my first computer. It was a ZX Spectrum , running with 48K of RAM on a Z80 processor running at 3.5 MHz. It was on this rubber-keyed machine that I learnt about for loops , if clauses and how much fun it was getting a computer to do your bidding, even if it was only to print “HELLO” all the way down the screen. Today, many years later, I spend most days getting Just Eat’s computers to do what I want them to do. And it’s still as satisfying as it always was. A few weeks ago, Troy Hunt came and visited Just Eat for the second year running, to lead a fresh group of our engineers through his two-day ‘Hack Yourself First’ security workshop. And I learned something new and interesting – how to get other people’s computers to do what I wanted them to… (For those of you who don’t know, Troy is one of the world’s best known web security experts.) Troy Hunt discovers his test site has been hacked by Rick Astley Twenty Just Eat engineers participated in the workshop, which consisted of a mixture of an overview of some of the most common security flaws out there in the wild, taking us gently (and sometimes not-so-gently) through (among other things) SQL injection attacks, badly configured applications and poorly thought-out password policies. Not only did he show us what the implications were when these things happened, but showed us how to get our hands dirty and hack a demonstration website that he had made specifically to be hacked. Now my interest was piqued. Of course, as a seasoned developer, I’d heard about most of the security flaws that Troy was talking about, but actually being able to hack a site and see what information gets compromised: getting someone else’s computer to do what I wanted it to do was even more satisfying than getting my own one to behave: having my trusty laptop break a website (albeit one written purposely to have these security holes), spam its reviews, and enumerate through all the registered users’ details in less than ten minutes was an eye-opener. Troy Hunt discovers his test site has been hacked Troy’s workshop helped all of us to understand, through our own practical application & experience, that security is something we must all take responsibility for, and how to do this in a practical way. Troy continues to be instrumental in highlighting security issues, and showing how to prevent or combat them (through his blog , his database of data leaks and his online courses ). Our thanks to Troy for spending a couple of days giving us a fairly broad yet deep dive into some of these issues. I for one was inspired to look deeper into this fascinating part of our industry, and the feedback suggests it wasn’t just me!", "date": "2017-05-03"},
{"website": "JustTakeAway", "title": "A better local and remote logging on iOS with JustLog", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2017/01/18/a-better-local-and-remote-logging-on-ios-with-justlog/", "abstract": "In this blog post we introduce the solution for local and remote logging we developed for the Just Eat iOS app. It’s named JustLog and it’s available open source on Github at github.com/justeat/JustLog Overview At Just Eat, logging and monitoring are fundamental parts of our job as engineers. Whether you are a back-end engineer or a front-end one, you’ll often find yourself in the situation where understanding how your software behaves in production is important, if not critical. The ELK stack for real-time logging has gained great adoption over recent years, mainly in the back-end world where multiple microservices often interact with each other. In the mobile world, the common approach to investigating issues is gathering logs from devices or trying to reproduce the issue by following a sequence of reported steps. Mobile developers are mostly familiar with tools such as Google Analytics or Fabric.io but they are tracking systems, not fully fledged logging solutions. We believe tracking is different in nature from logging and that mobile apps should take advantage of ELK too in order to take their monitoring and analysis to another level. Remote logging the right set of information could provide valuable information that would be difficult to gather otherwise, unveil unexpected behaviors and bugs, and even if the data was properly anonymized, identify the sequences of actions of singular users. JustLog takes logging on iOS to the next level. It supports console, file and remote Logstash logging via TCP socket out of the box. You can also setup JustLog to use logz.io with no effort. JustLog relies on CocoaAsyncSocket and SwiftyBeaver , exposes a simple swifty API but it also plays just fine with Objective-C. JustLog sets the focus on remote logging, but fully covers the basic needs of local console and file logging. Usage JustLog, is available through CocoaPods . To install it, simply add the following line to your Podfile: pod \"JustLog\" Import it into your files like so: // swift\nimport JustLog\n// Objective-C\n@import JustLog; This logging system strongly relies on SwiftyBeaver . We decided to adopt SwiftyBeaver due to the following reasons: good and extensible design ability to upload logs to the cloud macOS app to analyze logs A log can be of one of 5 different types, to be used according to the specific need. A reasonable adopted convention on mobile could be the following: 📣 verbose : Use to trace the code, trying to find one part of a function specifically, sort of debuggin with extensive information. 📝 debug : Information that is diagnostically helpful to developers to diagnose an issue. ℹ️ info : Generally useful information to log (service start/stop, configuration assumptions, etc). Info to always have available but usually don’t care about under normal circumstances. Out-of-the-box config level. ⚠️ warning : Anything that can potentially cause application oddities but an automatic recovery is possible (such as retrying an operation, missing data, etc.) ☠️ error : Any error which is fatal to the operation, but not the service or application (can’t open a required file, missing data, etc.). These errors will force user intervention. These are usually reserved for failed API calls, missing services, etc. When using JustLog, the only object to interact with is the shared instance of the Logger class, which supports 3 destinations: sync writing to Console (custom destination) sync writing to File (custom destination) async sending logs to Logstash (usually part of an ELK stack) Following is a code sample to configure and setup the Logger. It should be done at app startup time, in the applicationDidFinishLaunchingWithOptions method in the AppDelegate. let logger = Logger.shared\n// file destination\nlogger.logFilename = \"justeat-demo.log\"\n// logstash destination\nlogger.logstashHost = \"my.logstash.endpoint.com\"\nlogger.logstashPort = 3515\nlogger.logstashTimeout = 5\nlogger.logLogstashSocketActivity = true\n// default info\nlogger.defaultUserInfo = [\"app\": \"my iOS App\",\n\"environment\": \"production\",\n\"tenant\": \"UK\",\n\"sessionID\": someSessionID]\nlogger.setup() The defaultUserInfo dictionary contains a set of basic information to add to every log. The Logger class exposes 5 functions for the different types of logs. The only required parameter is the message, optional error and userInfo can be provided. Here are some examples of sending logs to JustLog: Logger.shared.verbose(\"not so important\")\nLogger.shared.debug(\"something to debug\")\nLogger.shared.info(\"a nice information\", userInfo: [\"some key\": \"some extra info\"])\nLogger.shared.warning(\"oh no, that won’t be good\", userInfo: [\"some key\": \"some extra info\"])\nLogger.shared.error(\"ouch, an error did occur!\", error: someError, userInfo: [\"some key\": \"some extra info\"]) It plays nicely with Objective-C too: [Logger.shared debug_objc:@\"some message\"];\n[Logger.shared info_objc:@\"some message\" userInfo:someUserInfo];\n[Logger.shared error_objc:@\"some message\" error:someError];\n[Logger.shared error_objc:@\"some message\" error:someError userInfo:someUserInfo]; The message is the only required argument for each log type, while userInfo and error are optional. The Logger unifies the information from message , error , error.userInfo , userInfo , defaultUserInfo and call-site info/metadata in a single dictionary with the following schema form of type [String : Any] (we call this ‘aggregated form’). E.g. in JSON representation: {\n  \"message\": ...,\n  \"userInfo\": {\n    \"NSLocalizedDescription\": ...,\n    \"error_domain\": ...,\n    \"some key\": ...,\n    ...\n  },\n  \"metadata\": {\n    \"file\": ...,\n    \"function\": ...,\n    \"line\": ...,\n    ...\n  }\n} All destinations (console, file, logstash) are enabled by default but they can be disabled at configuration time like so: logger.enableConsoleLogging = false\nlogger.enableFileLogging = false\nlogger.enableLogstashLogging = false The above 5 logs are treated and showed differently on the each destination: Console The console prints only the message. File On file we store all the log info in the ‘aggregated form’. 2016-12-24 12:31:02.734 📣 VERBOSE: {\"metadata\":{\"file\":\"ViewController.swift\",\"app_version\":\"1.0 (1)\",\"version\":\"10.1\",\"function\":\"verbose()\",\"device\":\"x86_64\",\"line\":\"15\"},\"userInfo\":{\"environment\":\"production\",\"app\":\"my iOS App\",\"log_type\":\"verbose\",\"tenant\":\"UK\"},\"message\":\"not so important\"}\n2016-12-24 12:31:36.777 📝 DEBUG: {\"metadata\":{\"file\":\"ViewController.swift\",\"app_version\":\"1.0 (1)\",\"version\":\"10.1\",\"function\":\"debug()\",\"device\":\"x86_64\",\"line\":\"19\"},\"userInfo\":{\"environment\":\"production\",\"app\":\"my iOS App\",\"log_type\":\"debug\",\"tenant\":\"UK\"},\"message\":\"something to debug\"}\n2016-12-24 12:31:37.368 ℹ️ INFO: {\"metadata\":{\"file\":\"ViewController.swift\",\"app_version\":\"1.0 (1)\",\"version\":\"10.1\",\"function\":\"info()\",\"device\":\"x86_64\",\"line\":\"23\"},\"userInfo\":{\"environment\":\"production\",\"app\":\"my iOS App\",\"log_type\":\"info\",\"tenant\":\"UK\",\"some key\":\"some extra info\"},\"message\":\"a nice information\"}\n2016-12-24 12:31:37.884 ⚠️ WARNING: {\"metadata\":{\"file\":\"ViewController.swift\",\"app_version\":\"1.0 (1)\",\"version\":\"10.1\",\"function\":\"warning()\",\"device\":\"x86_64\",\"line\":\"27\"},\"userInfo\":{\"environment\":\"production\",\"app\":\"my iOS App\",\"log_type\":\"warning\",\"tenant\":\"UK\",\"some key\":\"some extra info\"},\"message\":\"oh no, that won’t be good\"}\n2016-12-24 12:31:38.475 ☠️ ERROR: {\"metadata\":{\"file\":\"ViewController.swift\",\"app_version\":\"1.0 (1)\",\"version\":\"10.1\",\"function\":\"error()\",\"device\":\"x86_64\",\"line\":\"47\"},\"userInfo\":{\"error_code\":1234,\"environment\":\"production\",\"error_domain\":\"com.just-eat.test\",\"log_type\":\"error\",\"some key\":\"some extra info\",\"NSLocalizedDescription\":\"description\",\"NSLocalizedRecoverySuggestion\":\"recovery suggestion\",\"app\":\"my iOS App\",\"tenant\":\"UK\",\"NSLocalizedFailureReason\":\"error value\"},\"message\":\"ouch, an error did occur!\"}\n﻿ Logstash Before sending a log to Logstash, the ‘aggregated form’ is flattened to a simpler [String : Any] dictionary, easily understood by Logstash and handy to be displayed on Kibana. E.g. in JSON representation: {\n  \"message\": \"ouch, an error did occur!\",\n  \"environment\": \"production\",\n  \"log_type\": \"error\",\n  \"version\": \"10.1\",\n  \"app\": \"iOS UK app\",\n  \"tenant\": \"UK\",\n  \"app_version\": \"1.0 (1)\",\n  \"device\": \"x86_64\",\n  \"file\": \"ViewController.swift\",\n  \"function\": \"error()\",\n  \"line\": \"47\",\n  \"error_domain\": \"com.just-eat.test\",\n  \"error_code\": \"1234\",\n  \"NSLocalizedDescription\": \"description\",\n  \"NSLocalizedFailureReason\": \"error value\",\n  \"NSLocalizedRecoverySuggestion\": \"recovery suggestion\"\n} Which would be shown in Kibana as follows: A note on Logstash destination The logstash destination is configured via properties exposed by the Logger. E.g.: let logger = Logger.shared\nlogger.logstashHost = \"my.logstash.endpoint.com\"\nlogger.logstashPort = 3515\nlogger.logstashTimeout = 5\nlogger.logLogstashSocketActivity = true When the logLogstashSocketActivity is set to true, socket activity is printed to the console: This destination is the only asynchronous destination that comes with JustLog. This means that logs to Logstash are batched and sent at some point in future when the timer fires. The logstashTimeout property can be set to the number of seconds for the dispatch. In some cases, it might be important to dispatch the logs immediately after an event occurs like so: Logger.shared.forceSend() or, more generally, in the applicationDidEnterBackground and applicationWillTerminate methods in the AppDelegate like so: func applicationDidEnterBackground(_ application: UIApplication) {\n  forceSendLogs(application)\n}\n\nfunc applicationWillTerminate(_ application: UIApplication) {\n  forceSendLogs(application)\n}\nprivate func forceSendLogs(_ application: UIApplication) {\n  var identifier: UIBackgroundTaskIdentifier = 0\n  identifier = application.beginBackgroundTask(expirationHandler: {\n    application.endBackgroundTask(identifier)\n    identifier = UIBackgroundTaskInvalid\n  })\n  Logger.shared.forceSend { completionHandler in\n    application.endBackgroundTask(identifier)\n    identifier = UIBackgroundTaskInvalid\n  }\n} Sending logs to logz.io JustLog supports sending logs to logz.io . At the time of writing, logz.io uses the following host and port (please refer to the official documentation ): logger.logstashHost = \"listener.logz.io\"\nlogger.logstashPort = 5052 When configuring the Logger (before calling setup() ), simply set the token like so: logger.logzioToken = <logzio_token> Conclusion JustLog aims to be an easy-to-use working solution with minimal setup. It covers the most basic logging needs (console and file logging) via the great foundations given by SwiftBeaver, but also provides an advanced remote logging solution for Logstash (which is usually paired with Elasticsearch and Kibana in an ELK stack). JustLog integrates with logz.io , one of the most widely used ELK SaaS, placing itself as the only solution in the market (at the time of writing) to leverage such stack on iOS. We hope this library will ease the process of setting up the logging for your team and help you find solutions to the issues you didn’t know you had. About the author Alberto De Bortoli is the Principal iOS Engineer at Just Eat.", "date": "2017-01-18"},
{"website": "JustTakeAway", "title": "Introducing JustPeek", "author": ["Written by Dave Williams"], "link": "https://tech.justeattakeaway.com/2016/12/01/introducing-justpeek/", "abstract": "An iOS framework that ports Force-Touch Peek/Pop-like interactions to devices that aren’t force-touch enabled At Just Eat , teams tend to build software with the community in mind. Over time, Just Eat has published several Open Source projects and has been an important pillar to the C# community. More recently, the Company has grown its iOS team in London to focus on improving the Just Eat app for the UK market. With this growth phase, I had the opportunity to join the team for some time. As part of the effort to improve our app, launch the new Just Eat brand and build modules to help us in the long run, we’re building and open sourcing more iOS projects than ever before. JustPeek, is the first of them to get published. JustPeek is an iOS library that adds support for Force Touch-like Peek / Pop interactions on devices that don’t natively support it due to lack of force recognition capability in the screen. Under the hood it uses the native implementation if available, otherwise a custom implementation based on UILongPressGestureRecognizer . It fallbacks to the latter also on the iOS Simulator if needed. Unlike similar libraries already available on the web, JustPeek tries to mimic entirely the original implementation. It doesn’t simply use screenshots to display a preview, but previews the UIViewController itself. As of version 0.2.0, it also supports committing to a preview in the fallback implementation. As there’s no way to measure pressure on old devices, committing happens when a user keeps the preview open for more than a certain amount of time – 3 seconds at the time of writing. Here’s how you use JustPeek: // In a UITableViewController\nimport JustPeek\n...\nvar peekController: PeekController?\n// MARK: View Lifecycle\noverride func viewDidLoad() {\n    super.viewDidLoad()\n    peekController = PeekController()\n    peekController?.register(viewController: self, forPeekingWithDelegate: self, sourceView: tableView)\n}\n// MARK: PeekingDelegate\nfunc peekContext(_ context: PeekContext, viewControllerForPeekingAt location: CGPoint) -> UIViewController? {\n    let viewController = storyboard?.instantiateViewController(withIdentifier: \"ViewController\")\n    if let viewController = viewController, let indexPath = tableView.indexPathForRow(at: location) {\n        configureViewController(viewController, withItemAtIndexPath: indexPath)\n        if let cell = tableView.cellForRow(at: indexPath) {\n            context.sourceRect = cell.frame\n        }\n        return viewController\n    }\n    return nil\n}\nfunc peekContext(_ context: PeekContext, commit viewController: UIViewController) {\n    show(viewController, sender: self)\n} Here’s what it looks like in the demo application we ship with the code, as run on the iOS Simulator: We use JustPeek in the Just Eat UK app, in the list of results you get when searching for restaurants that serve your area, to show a preview of popular dishes a restaurant has to offer. JustPeek is now publicly available on GitHub and can be added to your projects using CocoaPods . We really hope you’ll like it! About the author Gianluca Tranchedone is a Senior iOS Engineer and, at the time of writing, the main contributor of JustPeek.", "date": "2016-12-01"},
{"website": "JustTakeAway", "title": "Unit testing front-end JavaScript with AVA and jsdom", "author": ["Written by Damian Mullins"], "link": "https://tech.justeattakeaway.com/2016/11/10/unit-testing-front-end-javascript-with-ava-and-jsdom/", "abstract": "Writing tests for JavaScript code that interacts with the DOM can be tricky. Luckily, using a combination of AVA and jsdom , writing those tests becomes a lot easier. This article will walk you through how to set everything up so you can get started writing your tests today. What is AVA? AVA is described as a “ Futuristic JavaScript test runner “. Sounds fancy, huh?! So, what is it exactly that makes it “ futuristic “?! Tests run quickly AVA runs test files in parallel, each in its own separate process, with the tests inside those files running concurrently. This offers better performance than other test runners that run tests serially, such as Mocha. This also means that each test file is run in an isolated environment — great for writing atomic tests. Simple API AVA’s API is very small because, in AVA’s own words, it is “ highly opinionated “. You won’t find any assertion aliases here! This reduces the cognitive load required when writing tests. Write tests in ES2015 You don’t need to do anything to be able to write tests in ES2015, AVA supports this out of the box! Under the covers it’s using Babel to transpile with the es2015 and stage-2 presets. No implicit globals AVA has no implicit globals, simply import it into your test file and you have everything you need. Other benefits There are a whole host of other benefits which AVA offers such as: Promise support Generator function support Async function support Observable support Enhanced assertion messages Clean stack traces All of this combined sounds very “ futuristic ” to me! Getting off the launchpad with AVA Now that we know more about AVA, let’s create a new project and start writing some tests. Start by running npm init inside a new project folder. This will create a package.json file, which will contain various pieces of information about the project such as its name, authors, and dependencies, among others. Hitting enter for each question will fill in a default value. Installing AVA Add AVA to the project by typing npm install ava --save-dev , then update the scripts section in package.json : \"scripts\": {\n  \"test\": \"ava --verbose\"\n} The --verbose flag enables the verbose reporter, which means more information is displayed when the tests are run. When using npm scripts, the path to AVA in the node_modules folder will be resolved for us, so all we need to do is type npm test on the command line. Doing so at the moment this will give us an exception: ✖ Couldn't find any files to test Let’s fix that by adding a test. Writing a test Create a test directory, with a  file named demo.test.js inside, then add a test: import test from 'ava';\ntest('can add numbers', t => {\n    t.is(1 + 1, 2);\n}); First, AVA is imported into the module, then the test function is called, passing a string as the first parameter which describes what the test is doing. The second parameter is the test implementation function which contains the body of the test, this provides us with an object, t , from which we can call the assertion functions. The is assertion is used here, which takes two values and checks that they are both equal (using === so there is no type conversion). Note : You can choose any name you like for the t parameter, such as assert . However, using the t convention in AVA will wrap the assertions with power-assert which provides more descriptive messages. Run npm test and the test result will be printed out √ can add numbers\n1 test passed Success! Our test passed as expected. To see an example of what a failing test would look like change the test assertion to t.is(1 + 1, 1) . Run the test now and you’ll see an error × demo » can add numbers\n t.is(1 + 1, 1)\n 1 test failed\n 1. can add numbers\n AssertionError:\n   t.is(1 + 1, 1)\n       Test.fn (demo.test.js:4:7) As you can see, there is a lot of useful information provided in order to help us track down the issue. Testing modules To demonstrate how to test a module, create a new folder called src in the root of the project with a file inside called demo-module.js with the contents: export function demo () {\n    return 'Hello, from demo module.';\n} Update demo.test.js by first importing the module, then adding a new test: import test from 'ava';\nimport { demo } from '../src/demo-module';\n...\ntest('can import from demo module', t => {\n    const expected = 'Hello, from demo module.';\n    const result = demo();\n    t.is(result, expected);\n}); Running npm test now will give you the following exception export function demo () {\n^^^^^^\nSyntaxError: Unexpected token export Uh oh, what happened? AVA will transpile ES2015 code in your tests; however, it won’t transpile code in modules imported from outside those tests. This is so that AVA has zero impact on your production environment. If our source modules are written in ES2015, how do we tell AVA that we’d like them to be transpiled too? Transpiling source files To transpile source files, the quick and dirty option is to tell AVA to load babel-register which will automatically transpile the source files on the fly. This is ok if you have a small number of test files, but there is a performance cost which comes with loading babel-register in every forked process. The other option is to transpile your sources before running the tests in order to improve performance. The next two sections look at how each technique can be achieved. Transpile with babel-register Add babel-register by running npm install babel-register --save-dev , then add a \"babel\" config to package.json \"babel\": {\n  \"presets\": [\"es2015\"]\n} Next, add \"babel-register\" to the AVA \"require\" section \"ava\": {\n  \"require\": [\"babel-register\"]\n} Run npm test and the tests will once again pass, great! √ demo » can add numbers\n√ demo-module » can import from demo module\n2 tests passed The recommendation from the AVA team is to use babel-register “ until the performance penalty becomes too great “. As your test base grows you’ll need to look into setting up a precompilation step. Setting up a precompilation step A precompilation step will transpile your source modules before the tests are run in order to improve performance. Let’s look at one way to set this up. Note : If you were following along with the last section you’ll need to remove the references to babel-register . First run npm uninstall babel-register --save-dev , then remove \"babel-register\" from the AVA \"require\" section in package.json . Start by adding the babel-cli and babel-preset-es2015 packages to the project: npm install babel-cli babel-preset-es2015 --save-dev . Next, add a \"babel\" config to package.json \"babel\": {\n  \"presets\": [\"es2015\"]\n} In order to run the tests, we need to update the npm scripts. Add a new npm script called precompile \"scripts\": {\n    \"precompile\": \"babel src --out-dir=dist\",\n    ...\n } The precompile npm script will tell Babel to take the files in the src directory, transpile them, then output the results to the dist directory. Next, the test npm script needs to be updated so that it runs the precompile step before running the tests \"test\": \"npm run precompile && ava --verbose\" The double ampersand ( && ) tells npm to first run the precompile script and then the AVA tests. The final task is to update the reference to demo-module inside demo.test.js to point at the compiled code, we do this by replacing ../src with ../dist : import { demo } from '../dist/demo-module'; Run npm test and we’re presented with all green tests! √ demo » can add numbers\n√ demo-module » can import from demo module\n2 tests passed Testing the DOM using Node So far we have the ability to test JavaScript code, but what if we’d like to test a function which makes use of the DOM? Node doesn’t have a DOM tree, so how do we get around this? One option is to use a combination of a test runner and a browser — a popular combination is Karma and PhantomJS. These offer a lot of benefits like being able to test against real browsers, run UI tests, take screenshots, and the ability to be run as part of a CI process. However, they typically come with a fairly large overhead, so running lots of small tests can take minutes at a time. Wouldn’t it be great if there was a JavaScript implementation of the DOM? Welcome to the stage; jsdom! jsdom jsdom is described as “ A JavaScript implementation of the WHATWG DOM and HTML standards, for use with Node.js “. It supports the DOM, HTML, canvas, and many other web platform APIs, making it ideal for our requirements. Because it’s purely JavaScript, jsdom has very little overhead when creating a new document instance which means that tests run quickly. There is a downside to using a JavaScript implementation over an actual browser – you are putting your trust in the standards being implemented and tested correctly, and any inconsistencies between browsers will not be detected. This is a deal breaker for some, but for the purposes of unit testing I think it is a reasonable risk to take; jsdom has been around since early 2010, is actively maintained, and thoroughly tested. If you are looking to write UI tests then a combination of something like Karma and PhantomJS may be a better fit for you. Integrating jsdom Setting up jsdom can be a daunting task, the documentation is great, but very lengthy and goes into a lot of detail (you should still read it!). Luckily a package called browser-env can help us out. Add browser-env to the project npm install browser-env --save-dev . Create a helpers directory (which is ignored by convention when using AVA) inside test , then add setup-browser-env.js with the contents require('browser-env')(); We need to tell AVA to require this module before any of the tests are run so that browser-env can create the full browser environment before any DOM references are encountered. Inside your package.json add \"ava\": {\n  \"require\": [\"./test/helpers/setup-browser-env.js\"]\n} Note : You may have noticed that this file is written in ES5. This is because AVA will transpile ES2015 code in the tests, yet it won’t transpile any modules imported or, in this case, required from outside the tests — see the transpiling source files section. Testing the DOM Let’s write a test which makes use of the document global which has been provided thanks to jsdom. Add a new test to the end of demo.test.js : ...\ntest('can query for DOM elements', t => {\n    document.body.innerHTML = 'Hello, world';\n    const para = document.querySelector('p');\n    t.is(para.innerHTML, 'Hello, world');\n}); First, we add a paragraph element with some text to the document body, then query for that element using document.querySelector , and finally, we verify that the selected paragraph tag has an innerHTML value equal to 'Hello, world' . Run the tests with npm test √ can add numbers\n√ can query for DOM elements\n2 tests passed Congratulations, you’ve just unit-tested the (virtual) DOM! Test coverage with nyc As a bonus let’s quickly set up some test coverage. Because AVA runs each test file in a separate Node.js process, we need a code coverage tool which supports this. nyc ticks the box — it’s basically istanbul with support for subprocesses. Add it to the project with npm install nyc --save-dev , then update the test npm script by adding nyc before the call to ava : \"scripts\": {\n  \"test\": \"nyc ava --verbose\"\n} You’ll also need to update the Babel config to tell it to include source maps when developing so that the reporter can output the correct lines for the transpiled code: \"babel\": {\n  ...\n  \"env\": {\n    \"development\": {\n      \"sourceMaps\": \"inline\"\n    }\n  }\n} Run the tests and witness the awesome code coverage table! √ demo-module » can import from demo module\n  √ demo » can add numbers\n  √ demo » can query for DOM elements\n  3 tests passed\n----------------|----------|----------|----------|----------|----------------|\nFile            |  % Stmts | % Branch |  % Funcs |  % Lines |Uncovered Lines |\n----------------|----------|----------|----------|----------|----------------|\nAll files       |      100 |      100 |      100 |      100 |                |\n demo-module.js |      100 |      100 |      100 |      100 |                |\n----------------|----------|----------|----------|----------|----------------| What next? If you’re interested in what else you can do with AVA, have a look through the AVA readme , check out the AVA recipe docs , read about common pitfalls , and listen to this JavaScript Air podcast episode . I’d also recommend looking into setting up linting for your code. You can browse the source code for this blog post on GitHub . So, now you have no excuse for not testing your front-end JavaScript! Damian Mullins is a UI Engineer at Just Eat. Progressive enhancement advocate, web standards supporter, JavaScript enthusiast.", "date": "2016-11-10"},
{"website": "JustTakeAway", "title": "Angular in Paris – ng-europe 2016", "author": ["Written by Daniel Richardson"], "link": "https://tech.justeattakeaway.com/2016/11/01/angular-in-paris-ng-europe-2016/", "abstract": "In Product Research at Just Eat we aim to research, design and prototype rapidly. The more prototypes we can put in users’ hands, the more chance we’ll have of discovering a good idea. Hopefully we’ll do just that and start to iterate, but in reality most prototypes are binned. So the more we build, the more likely we are to find something that people want. Angular could be a great fit for us, and the recent excitement around the Angular2 release meant it was high time for us to immerse ourselves in the community for a few days at ng-europe . Here’s a quick summary of the bits that jumped out for me along with some notes on things I plan to research. The first day I woke up quite early on my first morning in Paris and headed down to breakfast. Happily, Miško Hevery , who created Angular, and his Google colleague Alex Eagle were also awake, (I think they had jet lag). Over a serendipitous breakfast we talked about Angular but also about sugar, autonomous cars and the number of atoms in the universe. This was a good start! Keynote In his keynote to a packed hall Miško’s talked about how Angular has grown, both the community and the platform itself in its ambition and potential. It’s now everywhere: we can build Angular web apps, iOS and Android apps and desktop apps. Performance is hugely improved and experiences are native. Things to research : Benchpress , Augury . RxJS Rob Wormald gave a standout talk on RxJS and observables, here are his slides . I really like his style, making a complex subject accessible. Observables have had a massive impact on Angular’s architecture, and would feature in many of the later talks. Things to research : RxJS ; using the switchMap operator in requests and in the router ; avoiding the elvis operator in templates with smart and dumb components . ng-bootstrap The ng-bootstrap project powers Bootstrap 4 components natively with Angular2. Pawel Kozlowski described how, because they were especially written for Angular2, these components get the most from the framework. A good example is only rendering expensive content when it’s needed. Here are his slides . (He talked about how well tested the components are, but he didn’t mention accessibility, a special interest — I looked at the repo and don’t worry, it’s well in hand!) Things to research : components with observable children ; components with a programmatic API ; alternatives like Material . Data science Ari Lerner first plugged his company’s book (looks good, I’ve a sample to read, I’ll let you know) and then really caught my attention in his talk on data science. At Just Eat we have access to huge amounts of rich data which we analyse intelligently to drive and validate change. Ari described pages that randomly reorder themselves until an optimal variation — one that best suited conversion goals — is found with machine learning. I couldn’t track Ari down to ask him more about this, I have a load of questions, I’ll try and find him tomorrow. Things to research : this work (it’s open source, I’ll update with a link once I get one). TypeScript Live coding while public speaking goes badly wrong for some, but not for Daniel Rosenwasser . His talk on TypeScript was excellent, lots of smiles in the crowd. I already liked the idea of TypeScript (a superset of JavaScript) but had no idea of how awesome it has become, especially in combination with good tooling. Daniel renamed a .js file to a .ts file then quickly worked through the compiler warnings and refactored into way more robust idiomatic TypeScript. He showed off some new features of which improved type inference really impressed, clever compiler. Things to research : VS Code , the latest TypeScript . The rest of day one Other talks, all really interesting, covered BatScanner, unit testing, hybrid apps, security, migration and AngularFire2; all were recorded if you feel you’ve missed out. Looking forward to tomorrow! The second day Angular CLI The day began with Hans Larsen reminding everyone that setting up tooling is really painful and robs way too much time from app development. Great to hear this isn’t just me. The Angular team are taking this seriously. Angular CLI aims to stay out of your way, to understand your app and evolve with it by understanding your intent to generate, serve, test and build. You can generate classes, directives, enums, modules, pipes, services — and generation code adapts to its context. Here’s what it looks like: ng new prototype\ncd prototype\nng serve Out of the box, Angular CLI can compile Less or Sass, transpile TypeScript, optimise assets, understand environments and serve a live reloading dev server. The compiler tree-shakes and compiles ahead of time, so the bundle is small and no renderer is needed on the client — users experience apps that load and render quickly. Hans talked about the future, the CLI is getting smaller and more interoperable. Progressive web apps and server-side rendering will use ‘add-ons’ to change CLI behaviour. Add-ons and scripting will open the CLI to the community. I really like the fact that the CLI is part of Angular, it will always be in sync as Angular evolves. Learning just one tool is going to seriously speed up web app development. Things to research : Angular CLI Auguary Vanessa Yuen showed us Augury , an open source Angular2 Chrome Extension built by Rangle.io and Google. Augury uses the debug API so that you can inspect and debug an Angular2 app in DevTools. We watched as Vanessa used Augury to navigate an app’s component hierarchy, inspect state, change properties, invoke events, understand dependencies and view a router tree. One especially cool feature was using $a to reference a selected component in the console. Augury looks pretty essential, especially when apps grow in complexity. Reassuring to hear how committed Rangle.io are to keep it in sync as Angular changes. Things to research : Vanessa’s slides , Augury on GitHub . Angular Universal Next up, Wassim Chegham discussed Angular Universal : server-side rendering for Angular2. Not so relevant in product research, but still really impressive to hear about the latest. SEO and link previews are the obvious problems that Universal solves for Angular apps. But more than that, it improves the user experience by removing the ‘web app gap’, the several seconds of blank screen a user experiences while an app bootstraps and renders. By rendering on the server the user sees the page immediately, before the app bootstraps. If they interact with the page before the app’s ready, those events are captured by Preboot until they may be safely replayed. Things to research : Wassim’s slides , universal-starter seed . The new router Nir Kaufman gave us the lowdown on Angular2’s powerful root matching. This looks way easier to use with a consistent API and patterns. Lazy loading is new and as easy as replacing children with loadChildren. Params and query params now use observables which you pluck inside the component. It’s easier to secure states and prevent children activating or loading with ‘guards’. Especially cool, multiple views may now be serialised in the URL. I couldn’t quite get some of this talk, luckily Nir pointed us at some excellent guidance. Things to research : Victor Savkin’s Articles and book . Generative music As a musician and someone who’s played around with music technology quite a bit, the talk by Tero Parviainen was by far the most entertaining and inspiring. Tero took us through his project In C , an Angular2 WebAudio app inspired by Terry Riley’s In C in which the user starts ‘players’ and advances them through patterns to produce a divergent generative composition. Tero explained beautifully and with great clarity how Redux inspired the architecture of the app’s four decoupled component parts: a metronomic pulse, an immutable observable store, audio generation, and the UI. This architecture let him time travel through historic state and hot load code changes as he developed — awesome. So no, this probably isn’t going to be relevant at work but just fantastic to see such a talent, much cheering and applause from everyone for Tero. Things to research : Redux , ngrx/store . Animation Matias Niemelä discussed how animation in Angular2 uses a declarative domain specific language (DSL) inside the component. Testable, ahead-of-time compilable, independent from the DOM and web worker friendly. This was the point in the conference when it really started to sink in for me what it means to for Angular to be platform agnostic. That was a nice moment, seriously impressive engineering. The DSL composes states and transitions into triggers that are triggered from the component or from a property change in the template. For the web the implementation uses the Web Animations API — there’s a polyfill for lagging browsers . Matias described a slew of upcoming features, a couple that jumped out were pulling animations from CSS and animating layout. I’m excited enough about playing around with what’s here today. Things to research : Angular’s animation system , Matias’s slides . Mobile apps Adam Bradley from Ionic talked passionately about web standards and pointed out how fast and responsive the mobile web now is. Later, Christian Weyer and Thorsten Hans compiled the same codebase with Cordova and Electron yielding the same app across Mac OS, Windows, iOS, Android and web. Great talks but my issue with all these techniques is that apps don’t quite feel native enough, that you’re always dealing with web view’s shortcomings or chasing platform conventions and evolution. So I was more interested in Marc Laval ’s talk on custom renderers. In Angular2 the renderer is separated from Angular core and your app (in the browser they’re separated in the UI thread and a web worker). That renderer may be swapped so that Angular2 can work where there is no DOM, for example on Node, where Universal uses a server renderer, or on a mobile phone. Both React Native and NativeScript work with Angular and render UIs natively with no web view. That’s a truly native experience and much more appealing to me. Neither need Angular to work, but how awesome would it be to master one framework for all three platforms? Things to research : Marc’s slides , Angular2 &React Native , Angular2 & NativeScript . Building RxJS By now I was getting my head around observables, so André Staltz ’s talk was well timed. Starting with an explanation of callback types, he flawlessly hacked away from simple JavaScript to a basic implementation of RxJS. He changed the code step-by-step, for example giveMeSomeData() became subscribe(). Really funny, and did the trick, definitely some lights going on all around. Fast loading apps Alex Eagle ’s pointed at stats that give us about two seconds to make our apps responsive before users leave. At 2G or 250 kb/s that’s 500 k for JS and assets. That’s not very much so we need static analysis techniques, lazy loading, tree-shaking, bundling and minification to help us spend the budget. As he dug into the complexity of these things I felt massively relieved that people as smart as Alex are figuring out how to do all this – even more relieved to learn that Angular CLI will take care of everything and I don’t need to worry. Wrapping up And that was it! It’s all recorded if you any of that piqued your interest. The Q&A at the end was really interesting. What most stuck with me was that AngularJS to Angular2 has been painful for everyone, but that this kind of upgrade won’t happen again. Angular2 is now a platform that will just grow in its power and reach as it shields us from turbulent and rapidly changing platforms. Farewell Paris Thanks ng-europe for a fascinating couple of days, it was great to spend time with such a friendly, smart and supportive community. Ben Glynn is a Principal UI Engineer for Product Research at Just Eat. He’s passionate about product discovery, the user interface and user experience.", "date": "2016-11-01"},
{"website": "JustTakeAway", "title": "Bringing Apple Pay to the web", "author": ["Written by Martin-Costello"], "link": "https://tech.justeattakeaway.com/2016/10/10/bringing-apple-pay-to-the-web/", "abstract": "Using Apple Pay on the web from just-eat.co.uk. Introduction Back in June at WWDC , Apple announced that Apple Pay was expanding its reach. No longer just for apps and Wallet on TouchID compatible iOS devices and the Apple Watch, it would also be coming to Safari in iOS 10 and macOS Sierra in September 2016. Just Eat was a launch partner when Apple Pay was released in the UK in our iOS app in 2015. We wanted to again be one of the first websites to support Apple Pay on the web by making it available within just-eat.co.uk . Our mission is to make food discovery exciting for everyone – and supporting Apple Pay for payment will make your experience even more dynamic and friction-free. Alberto from our iOS team wrote a post about how we introduced Apple Pay into our iOS app last year, and this post follows on from that journey with a write-up of how we went about making Apple Pay available on our website to iOS and macOS users with the new Apple Pay JS SDK . Getting set up In the iOS world, due to the App Store review process and signed entitlements, once your app is in users’ hands you just use PassKit to get coding to accept payments. For the web things are a little different. Due to the more loosely-coupled nature of the integration, instead trust between the merchant (Just Eat in our case) and Apple is provided through some additional means: A valid SSL/TLS certificate A validated domain name to prove a merchant owns a given domain A Merchant Identify Certificate As we already use Apple Pay here at Just Eat, the first few steps for getting up and running have already been achieved. We already have an Apple developer account and a merchant identifier via our iOS app development, and the Just Eat website is already served over HTTPS, so we have a valid SSL/TLS certificate. We also do not need to worry about decrypting Apple Pay payment tokens ourselves. We use a third-party payment provider to offload our payment processing, and internal APIs for passing an Apple Pay token for processing via our payment provider already exists for handling iOS payments, so the website can integrate with those as well. To get up and running and coding end-to-end, we need just need a Merchant Identity Certificate. This is used to perform two-way TLS authentication between our servers and the Apple Pay servers to validate the merchant session when the Apple Pay sheet is first displayed on a device. The first step in getting a Merchant Identify Certificate is to validate a domain. This involves entering a domain name into the Apple Pay Developer Portal for the merchant identifier you want to set up Apple Pay on the web for – where you then get a file to download. This is just a text file that verifies the association between your domain and your merchant ID. You just need to deploy this file to the web server(s) hosting your domain so Apple can perform a one-time request to verify that the file can be found at your domain. You need to do this for all domains you wish to use Apple Pay for, including internal ones for testing, so you may have to white list the Apple IP addresses so that the validation succeeds. Once you have validated at least one domain, you can generate your Merchant Identify Certificate for your Merchant Identifier. This requires providing a Certificate Signing Request (CSR). Uploading the CSR file in the Apple Developer Portal will generate a certificate file ( merchant_id.cer ) for you to download. This acts as the public key for your Merchant Identify Certificate. The private key is the CSR you provided. In order to create a valid TLS connection to the Apple Pay merchant validation server, you will need to create a public-private key pair using the CSR and the CER files, such as using a tool like OpenSSL. In our case we generated a .pfx file for use with .NET. Make sure you keep this file secure on your server and don’t expose it to your client-side code. Separating concerns So now we’ve got a validated domain and a Merchant Identify Certificate, we can start thinking about implementing the JavaScript SDK. At a high-level the components needed to create a working Apple Pay implementation in Safari are: JavaScript to test for the presence of Apple Pay, display the Apple Pay sheet and to respond to user interactions and receive the payment token CSS to render the Apple Pay button on a page An HTTPS resource to perform merchant validation From the user’s point of view though, it’s just a button. So rather than add all the code for handling Apple Pay transactions directly into the codebase of our website, we decided instead to contain as much of the implementation as possible in a separate service. This service presents its own API surface to our website, abstracting the detail of the Apple Pay JavaScript SDK itself away. The high-level implementation from the website’s point of view is therefore like this: Render a hidden div on the appropriate page in the checkout flow to represent the Apple Pay button as well as some meta and link tags to drive our JavaScript API Reference a JavaScript file from the Apple Pay service via a script tag Provide some minimal CSS to make the Apple Pay button size and colour appropriate to the current page Call a function on our JavaScript API to test for whether Apple Pay is available If it is, call a second function passing in some parameters related to the current checkout page, such as the user’s basket, the DOM element for the div representing the Apple Pay button and some callback functions for when the payment is authorised, fails or an error occurs. The rest of the Apple Pay implementation is handled by our JavaScript abstraction so that the Just Eat website itself never directly calls the Apple Pay JavaScript functions. Our new Apple Pay service itself should have the following responsibilities: Serve the JavaScript file for the abstraction for the website Serve a file containing the base CSS for styling the Apple Pay button Provide HTTP resources that support Cross Origin Resource Sharing ( CORS ) to: Provide the payment request properties to set up an Apple Pay sheet Validate merchant sessions Verify that a restaurant partner delivers to the selected delivery address Receive the Apple Pay payment token to capture funds from the user and place their order Separating the CSS, JavaScript and back-end implementation allows us to decouple the implementation from our website itself allowing for more discrete changes. For example, the current Apple Pay version is 1 . By abstracting things away we could make changes to support a future version 2 transparently from the website’s point-of-view. Delving into the implementation As mentioned in the high-level design above, integrating Apple Pay into a website requires a mix of client-side and server-side implementation. We need to implement some JavaScript, make some CSS available and provide some server-side HTTP resources to handle merchant validation of payment processing. There’s also some HTTP meta and link tags you can add to enhance your integration. Let’s delve into the different layers and things we need to add… HTML Well first we need an Apple Pay button. You can add one with some HTML like this: <div class=\"hide apple-pay-button apple-pay-button-black\" /> Ignore the apple-pay-* CSS classes for now as I’ll come back to them, but the hide class (or some other similar approach) ensures that the div for the button is not visible when the page first loads. This allows us to display it as appropriate once we have detected that Apple Pay is available in the browser using JavaScript. HTML metadata Apple Pay supports a number of different HTML meta and link tags that you can use to improve the user experience for your integration. First, there’s some link tags you can add to provide an icon for use on an iPhone or iPad when a confirmation message is shown to the user initiating a payment from macOS: <link rel=\"apple-touch-icon\" href=\"https://dy3erx8o0a6nh.cloudfront.net/images/touch-icon-120.png\" sizes=\"120x120\" />\n<link rel=\"apple-touch-icon\" href=\"https://dy3erx8o0a6nh.cloudfront.net/images/touch-icon-152.png\" sizes=\"152x152\" />\n<link rel=\"apple-touch-icon\" href=\"https://dy3erx8o0a6nh.cloudfront.net/images/touch-icon-180.png\" sizes=\"180x180\" /> These link elements can even be added dynamically by scripts when you detect the Apple Pay is available, provided that they are in the DOM before you create an ApplePaySession object. There’s also some meta tags you can add so that crawlers (such as Googlebot can identify your website as supporting payment through Apple Pay: <meta property=\"product:payment_method\" content=\"ApplePay\" />\n<meta name=\"payment-country-code\" content=\"GB\" />\n<meta name=\"payment-currency-code\" content=\"GBP\" /> Integrating the Apple Pay JavaScript SDK So now we’ve got the HTML for the Apple Pay button and some metadata tags, we need some JavaScript to drive the integration. In our case we have placed all of our Apple Pay-related JavaScript into a single file. This allows us to use server-side feature flags to decide to render the script tag for it (or not), so that the relevant file is only fetched when the feature is enabled. Within this JavaScript file, there are functions for dealing with the Apple Pay workflow and calling the Safari functions in the browser. The psuedo-code for an implementation within a consuming website would be: // Determine if Apple Pay is supported by the current device\nif (je.applePay.isSupportedForCheckout() === true) {\n  // Create a configuration object from the page using\n  // conventions to detect controls like the button.\n  var config = je.applePay.Controller.createConfig();\n  // Register a callback function to invoke when the\n  // payment is successfully authorised. In our case\n  // the response contains the user's order Id.\n  config.onPaymentAuthorized = function (response) {\n    if (response.orderId) {\n      // Do something with the order Id\n    }\n  };\n  // Create a controller to handle the Apple Pay workflow such as\n  // the event handler for when the Apple Pay button is clicked.\n  var controller = new je.applePay.Controller(config);\n  // Call the function that determines if Apple Pay is available\n  // for use in the current window location (e.g. https://www.just-eat.co.uk)\n  // and displays the Apple Pay button to the user if it is.\n  controller.displayIfAvailable();\n} First we have functions in je.applePay that contain simple functions for feature detection. For example, the isSupportedByDevice() function tests if the current browser supports Apple Pay at all, where as the isSupportedForCheckout() function additionally tests if the Just Eat specific information (such as the ID of the basket to pay for) is available to the current page. The controller is the top-level object in our abstraction that the containing page uses to handle the Apple Pay payment flow. This handles things so that when the user clicks the Apple Pay button, we create an Apple Pay session with the appropriate payment information, do callbacks to the server to validate the merchant session and capture payment – and invoke the website-supplied callback functions when the payment process ends. Within our abstraction, we use the ApplePaySession object to drive our integration. For example, to test for Apple Pay support, we use code similar to this (logging removed for brevity): /**\n * Returns whether Apple Pay is supported on the current device.\n * @returns {Boolean} Whether Apple Pay is supported on the current device.\n */\nje.applePay.isSupportedByDevice = function () {\n    var isSupported = false;\n    var isApplePaySessionInWindow = \"ApplePaySession\" in window && ApplePaySession;\n    if (isApplePaySessionInWindow) {\n        var canMakePayments = ApplePaySession.canMakePayments() === true;\n        var supportsVersion = ApplePaySession.supportsVersion(1) === true;\n        isSupported = canMakePayments && supportsVersion;\n    }\n    return isSupported;\n}; Assuming that the device supports Apple Pay then we’ll want to display the Apple Pay button. However before we do that we’ll need to wire-up an onclick event handler to invoke the JavaScript to handle the payment process itself when it is clicked or pressed. For example with jQuery: // Get the button\nvar button = $(\".apple-pay-button\");\n// Register the event handler\nbutton.on(\"click\", function (e) {\n  // Apple Pay implementation\n});\n// Show the button now it is ready for use\nbutton.removeClass(\"hide\"); Now the Apple Pay button will be displayed. The rendering of the button itself is handled by the CSS provided by Apple. There are four possible variants. First there’s a choice between a black or a white button, then there’s the choice of either an Apple Pay logo only, or the logo prefixed by “By with” ( CSS ). The logo itself is provided by resources built into Safari, such as shown in this snippet: .apple-pay-button-black {\n  background-image: -webkit-named-image(apple-pay-logo-white);\n  background-color: black;\n} The CSS file for this is loaded dynamically by our JavaScript abstraction so users with devices that do not support Apple Pay do not pay the penalty of a network request to get the CSS file. This also removes the need for the consuming website to explicitly load the CSS itself with a link tag and allows the location of the CSS file itself to be modified at any time in our Apple Pay service. So when the user either taps or clicks the button, that’s when the work to start the Apple Pay session begins. First you need to create a properly set up payment request object to create an instance of ApplePaySession along with the Apple Pay version (currently 1 ). Be careful here – Apple Pay only allows an ApplePaySession object to be created when invoked as part of a user gesture. So, if you want to do any interaction with your server-side implementation here, ensure you do not make use of asynchronous code such as with a Promise object. Otherwise creating the ApplePaySession may occur outside the scope of the gesture handler, which will cause a JavaScript exception to be thrown and the session creation to fail. We haven’t done enough to show the sheet yet though. Next we need to register the callback functions for the events we want to receive callbacks for. At a minimum you will need two of these: onvalidatemerchant onpaymentauthorized onvalidatemerchant is called after the sheet is displayed to the user. It provides you with a URL to pass to the server-side of your implementation to validate the merchant session. An example of how you could do this in jQuery is shown in the snippet below: session.onvalidatemerchant = function (event) {\n  var data = {\n    validationUrl: event.validationURL\n  };\n  $.post(\"/your-validation-resource-url\", data).then(function (merchantSession) {\n    session.completeMerchantValidation(merchantSession);\n  });\n}; onpaymentauthorized is called after payment is authorised by the user either with a fingerprint from an iPhone or iPad or by pressing a button on their Apple Watch. This provides the payment token for capturing the funds from the user. An example of how you could do this in jQuery is shown in the snippet below: session.onpaymentauthorized = function (event) {\n  var data = {\n    billingContact: event.payment.billingContact,\n    shippingContact: event.payment.shippingContact,\n    token: event.payment.token.paymentData\n  };\n  $.post(\"/your-payment-processing-url\", data).then(function (response) {\n    session.completePayment(\n      response.successful ?\n      ApplePaySession.STATUS_SUCCESS :\n      ApplePaySession.STATUS_FAILURE);\n  }); The functionality to actually capture funds from the user is outside the scope of this blog post – information about decrypting Apple Pay payment tokens can be found here . There’s also events for payment selection, shipping method selection, shipping contact selection and cancellation. This allows you to do things such as: Dynamically adjust pricing based on payment method or shipping address Validate that the shipping address is valid, for example whether a restaurant delivers to the specified shipping address Note that before the payment is authorised by the user, not all of the shipping contact and billing contact information is yet available to you via the parameters passed to the event handlers. For example, the country, locality (eg a city or town), administrative area (eg a county or state) and the first part of the postal code (eg outward code in the UK, such as EC4M 7RF ). This is for privacy reasons as before the user authorises the payment it is still a request for payment, and as such the full information is only revealed to use you the integrator by the onpaymentauthorized event. Once you’ve registered all your event handlers, you just need to call the begin function to display the Apple Pay sheet. HTTP resources Our server-side implementation has 4 main resources that we consume from our JavaScript code for all flows: GET /applepay/metadata GET /applepay/basket/{id} POST /applepay/validate POST /applepay/payment The metadata resource is used to test whether Apple Pay is available on the current domain (for example www.just-eat.co.uk ). The JSON response returned indicates whether the Apple Pay feature is enabled for the referring domain, the merchant capabilities, the supported payment networks, the country and currency code and the available Apple Pay touch icons and their URIs. This allows our JavaScript example to build up the link tags for the touch icons dynamically, deferring the need for them until necessary. The basket resource is used to fetch details about the user’s current basket so that we can render the Apple Pay sheet to show the items for their order, the total, the shipping method and the required shipping contact fields. For example, we require the user’s postal address for delivery orders but that isn’t required for collection orders. This removes the need for the JavaScript to determine any of this information itself, as it can just copy the fields into the payment request object for the ApplePaySession constructor directly from the JSON response. The validate resource is used to implement the merchant session validation with the Apple Pay servers. This posts the Apple validation URL to our back-end which then calls the specified URL using the Merchant Identify Certificate associated with the requesting domain to validate the merchant session. The JSON response then returns a MerchantSession dictionary for consumption by the JavaScript to pass to the completeMerchantValidation function. The payment resource is used to POST the encrypted payment token, as well as the basket ID and billing and shipping contact details to our server to place the order. This resource then returns either an order ID (and optionally a token if a guest user account was created) if the payment was authorised successful or an error code otherwise. For delivery orders we also have a POST /applepay/basket/{id}/validatepostalcode resource to check that the user’s chosen shipping address can be delivered to. Merchant Validation Initiating the POST to Apple’s servers to validate the session is relatively simple in ASP.NET Core (more about that later), provided you’ve already performed the steps to create a .pfx file for your Merchant Identify Certificate. First we need to load the certificate, whether that’s from the certificate store or from a file on disk. In our service we store the certificate as an embedded resource as we have multiple certificates for different environments, but the simplest form is loading from disk. var certificate = new X509Certificate2(\n    \"merchant_id.pfx\",\n    \"MySuperSecretPa$$w0rd\"); This was the approach I was using in some local initial testing, but when I deployed the code to a Microsoft Azure App Service to leverage the free SSL certificate, this stopped working. After some digging around I found that this was because on Windows you need to be able to load the user profile to access private keys in certificates, and this isn’t possible by default in IIS as it isn’t loaded. This is easy enough to fix when you have full control of the infrastructure (such as our Amazon Web Services (AWS) Elastic Cloud Compute (EC2) instances), but there’s no option available to enable this in Azure. Luckily there is a way around this. First, you upload the certificate that has a private key that you wish to use to the App Service using the “SSL certificates” tab in the Azure Portal . Next, you add the WEBSITE_LOAD_CERTIFICATES App setting to the “Application settings” tab and set its value to the thumbprint of the certificate you want to use. This causes the App Service to make the specified certificate available in the “My” store in the “Current User” location so it can be read by the identity associated with the IIS App Pool. Note that the validOnly parameter value is set to false ; if it is not the Merchant Identifier Certificate will not be loaded as it is not considered valid for use by Windows, even though it is valid from Apple’s perspective. using (var store = new X509Store(StoreName.My, StoreLocation.CurrentUser))\n{\n    store.Open(OpenFlags.ReadOnly);\n    var certificates = store.Certificates.Find(\n        X509FindType.FindByThumbprint,\n        \"MyCertificateThumbprint\",\n        validOnly: false);\n    var certificate = certificates[0];\n} The next step in the merchant validation process is to construct the payload to POST to the Apple server. For this we need our domain name, the store display name (in our case “Just Eat”) and the merchant identifier. While we could configure the merchant identifier to use per domain, we can be smart about it and read it from the Merchant Identifier Certificate instead. Thanks to Tom Dale’s node.js example implementation, we discovered that this can be found from the 1.2.840.113635.100.6.32 X.509 extension field, so we can read it out of our X509Certificate2 like so: var extension = certificate.Extensions[\"1.2.840.113635.100.6.32\"];\nvar merchantId = System.Text.Encoding.ASCII.GetString(extension.RawData).Substring(2); Now we can POST to the validation URL we received from the JavaScript. As mentioned previously we need to provide the Merchant Identifier Certificate with the request for two-way TLS authentication. This is achieved by using the HttpClientHandler class which provides a ClientCertificates property where we can use our certificate, and then pass it into the constructor of HttpClient to handle authentication for use when we POST the data: var payload = new\n{\n    merchantIdentifier = merchantId,\n    domainName = \"www.just-eat.co.uk\"\n    displayName = \"Just Eat\"\n};\nvar handler = new HttpClientHandler();\nhandler.ClientCertificates.Add(certificate);\nvar httpClient = new HttpClient(handler, disposeHandler: true);\nvar jsonPayload = JsonConvert.SerializeObject(payload);\nvar content = new StringContent(jsonPayload, Encoding.UTF8, \"application/json\");\nvar response = await httpClient.PostAsync(requestUri, content);\nresponse.EnsureSuccessStatusCode(); Assuming we get a valid response from the Apple server, then we just need to deserialise the JSON containing the merchant session and return it to the client from our API controller method: var merchantSessionJson = await response.Content.ReadAsStringAsync();\nvar merchantSession = JObject.Parse(merchantSessionJson);\nreturn Json(merchantSession); Now our JavaScript needs to consume the response body as mentioned earlier in the JavaScript implementation to pass it to the ApplePaySession.completeMerchantValidation function to allow the user to authorise the payment. New Tricks with ASP.NET Core When we started implementing Apple Pay for our website, ASP.NET Core 1.0.0 had just been released, and as such we were running all our C#-based code on the full .NET Framework. We decided that given the relatively small size and self-contained nature of the service for Apple Pay (plus there being no legacy code to worry about) that we’d dip our toes into the new world of ASP.NET Core for implementing the service for Apple Pay. There are a number of capabilities and enhancements of ASP.NET Core that made it attractive for the implementation, but the main one was the improved integration with client-side focused technologies, such as Bower , Gulp and npm . Given that a bulk of the implementation is in JavaScript, this made it easier to use best-practice tools for JavaScript (and CSS) that provide features such as concatenation, minification, linting and testing. This made implementing the JavaScript part of the integration much easier to implement that the equivalent workflow in an ASP.NET MVC project in Visual Studio. Getting cut at the bleeding edge Of course, going with a new version of a well-established technology isn’t all plain-sailing. There’s been a few trade-offs moving to ASP.NET Core that have made us go back a few steps in some areas. These are gaps we hope to address in the near future to obtain feature parity with our existing ASP.NET applications. Some of these trade-offs are detailed below. Dependencies Here at Just Eat we have a variety of shared libraries that we add as dependencies into our .NET applications to share a common best-practice and allow services to focus on their primary purpose, rather than also have to worry about boiler-plate code, such as for logging , monitoring and communicating with other Just Eat services over HTTP. Unfortunately a number of these dependencies are not quite in the position to support consumption from .NET Core-based applications. In most cases this is due to dependencies we consume ourselves not supporting .NET Core (such as Autofixture used in tests), or using .NET APIs that are not present in .NET Core’s surface area (such as changes to the UdpClient class). We’re planning to move such libraries over to support .NET Core in due course ( example ), but the structure of the dependencies makes this a non-trivial task. The plan is to move our Apple Pay service over to versions of our libraries supporting .NET Core as they become available, for now it uses its own .NET Core forks of these libraries. Monitoring At Just Eat we have a very mature monitoring and logging solutions using Kibana and Grafana , amongst other tools. Part of our monitoring solution involves a custom service that is installed on our AWS EC2 Amazon Machine Images (AMIs) which collects performance counter data to publish to StatsD . Unfortunately ASP.NET Core does not currently implement performance counters on Windows. In ASP.NET, there are various performance counters available that we collect as part of our monitoring, such as the number of current IIS connections, request execution times, etc. Even though ASP.NET Core can be hosted via IIS, because the .NET Framework is not used, these performance counters are of no use when it comes to monitoring an ASP.NET Core application. Testing the implementation So once we’ve gotten our server-side implementation to get details for rendering the Apple Pay sheet, validating merchant sessions and processing payment in place, as well as our JavaScript abstraction and base CSS, we can start going about testing it out. But how do we test Apple Pay without using our own personal credit/debit card? Luckily with iOS 10, watchOS 3 and macOS Sierra, Apple have provided us with a way to do this. It’s called the Apple Pay Sandbox . This provides us with a way to set up users with “real” payment cards that allow us to test transaction processing (at least up to the point of trying to capture funds). You can find more details on the website , but the main steps are: Setup a sandbox tester account in iTunes Connect Sign into iCloud on your test device(s) using your sandbox tester Add one or more test card(s) to Wallet on you test device(s) Using the Apple Pay sandbox then allows you to test as many transactions as you like on your test devices without worrying about spending a fortune or misplacing your personal payment card details. Stubbing Out the SDK With the majority of Just Eat’s back-end services (and our website) being written in ASP.NET, this posed a bit of a challenge for testing. Of course the interactions with the sheet and the rendering need to be tested on a real Apple Pay-supporting device, but how could we run the full-back end stack on our local Windows 10 machines and use Apple Pay for local testing of changes without setting up lots of proxying to macOS and iOS test devices? Well luckily in JavaScript it’s quite simple to add a polyfill to a browser to provide a native API where there would otherwise not be one available. So that’s what we did. You can find it in a here on GitHub. Effectively the polyfill provides the ApplePaySession object if it does not already exist, and functions in a way that makes the functions behave as if Apple Pay is available on the current device and chains the events and their handlers together to make it appear that a user is interacting with the Apple Pay sheet. Of course it is no substitute for testing with a real device, but the polyfill provides enough of an implementation to test feature detection (i.e. only adding the button if Apple Pay is supported) and the server side implementation for fetching and rendering the basket, performing merchant validation, and passing on a valid sandbox payment token. You can get a valid payment token for a sandbox transaction that you can embed within your own copy of the Polyfill by adding some JavaScript logging to print out the text representation of the object passed as the event parameter to the onpaymentauthorized function, as well as populating it with some appropriate billing and payment contact details. We use the polyfill for testing in our QA environments by loading it into the browser via a script tag in our checkout-related pages where the Apple Pay button would appear. Deployment So we’ve got our new service, and we’ve integrated it into our website and it’s all working locally. Now it just needs deploying to our QA environments for testing, and then eventually onto our production environment. We have our own deployment pipeline here at Just Eat that sets up deploying IIS applications from ZIP packages and we also build our own custom AWS AMIs to deploy our services onto, so that’s all taken care of by our Platform Engineering team. Our AMIs do not yet have .NET Core installed on them though, so if we tried to use the deployed in IIS it would return an HTTP 502. That’s easy enough to resolve though, we just need to make a new AMI with .NET Core on it. This is nice and easy as Chocolatey provides packages for both the .NET Core runtime and the Windows Server Hosting installer for IIS hosting. Now there’s just a few more things we need to do to get our feature ready to run: We need to set the ASPNETCORE_ENVIRONMENT environment variable so that the application runs with the right configuration We need to set up the registry hives required for the ASP.NET Core data protection system (used for things like antiforgery tokens) We need to adjust the App Pool configuration Our deployment process already provides us with hooks to run PowerShell scripts post-deployment, so we just need to write some small scripts to do the steps. Setting the environment name We can set the environment name machine-wide because we deploy each service on its own EC2 instance. There are other approaches available, like setting environment variables in the ASP.NET Core Module , but this was simpler: [Environment]::SetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\", $environmentName, [System.EnvironmentVariableTarget]::Machine) Configuring the App Pool We also need to amend the IIS App Pool for the website to disable the .NET Framework (because we don’t need it) and to load the user profile so we can load the private keys in our Merchant Identifier Certificates. $appCmd = [IO.Path]::Combine($env:WinDir, \"System32\", \"inetsrv\", \"appcmd\")\n# Set the site to run no managed code\n& $appCmd set apppool \"/apppool.name:$siteName\" /managedRuntimeVersion:\n# Load the user profile so X509 certificate private keys can be loaded\n& $appCmd set config -section:applicationPools \"/[name='$siteName'].processModel.loadUserProfile:true\" Setting Up Data Protection The process for setting up Data Protection for IIS, which in turn provides a link to a PowerShell script, can be found here . After these three steps are done, then IIS just needs to be restarted (such as with iisreset ) to pick up the configuration changes. The (Apple) pay off So now with Apple Pay integrated into our website, it’s possible for the user to pay using the cards loaded into Wallet on either their iPhone running iOS 10 or their Apple Watch running watchOS 3 when paired with a MacBook running macOS Sierra. iPhone payment flow At the start of the checkout flow the user is prompted to select what time they would like their food delivered for (or be ready for collection) and an optional note for the restaurant. At first the user is shown the Apple Pay button in additional to the usual button to continue through checkout to provide their delivery and payment details. The user taps the Apple Pay button and the Apple sheet is displayed. Then the user selects their payment card as well as their delivery address. While this happens we asynchronously validate the merchant session to enable TouchID to authorize payment as well as validate that the restaurant selected delivers to the postcode provided by the user in the case of a delivery order. Once the user authorizes payment with their finger or thumb, the sheet is dismissed, they are logged in to a guest account if not already logged in, and redirected to the order confirmation page. The Apple Pay button displayed during checkout in Safari on iOS 10. The Apple Pay payment sheet in iOS. macOS payment flow At the start of the checkout flow the user is prompted to select what time they would like their food delivered for (or be ready for collection) and an optional note for the restaurant. Here the user is shown the Apple Pay button in additional to the usual button to continue through checkout to provide their delivery and payment details. The Apple Pay button displayed during checkout in Safari on macOS Sierra. The user clicks the Apple Pay button and the Apple sheet is displayed. The user selects their payment card as well as their delivery address. While this happens we asynchronously validate the merchant session to enable the ability to authorize payment using an iPhone, iPad or Apple Watch paired with the signed in iCloud account, as well as validate that the restaurant selected delivers to the postcode provided by the user in the case of a delivery order. The Apple Pay payment sheet in macOS Sierra. Once the merchant session is validated, the user is then prompted to authorize the payment on their paired device, for example using either an iPhone with TouchID or an Apple Watch. Payment confirmation for a purchase from macOS using Touch ID on an iPhone. Payment confirmation for a purchase from macOS using Apple Watch. Once the user authorizes payment with their finger or thumb with TouchID or by pressing a button on their Apple Watch, the sheet is dismissed, they are logged in to a guest account if not already logged in, and redirected to the order confirmation page. Now the user just needs to wait for their food with their inner food mood to be prepared. Example integration An example integration of Apple Pay JS adapted from our own implementation is available on GitHub . You should be able to use it as a guide to implementing Apple Pay into your website by viewing the JavaScript for creating an ApplePaySession and the C# for validating a merchant session. Also, provided you have an Apple Developer account so that you can generate your own merchant identifier and the associated certificates, you should also be able to run it yourself and see Apple Pay in action. Conclusion We hope you’ve found this post about how we brought Apple Pay to the Just Eat website informative and interesting, and that the example integration is a useful resource if you’re thinking about implementing Apple Pay into your own e-commerce solution yourself. It’s been an interesting SDK to integrate with a number of challenges along the way, but we’ve also learned a lot in the process, particularly about Apple Pay itself, as well as the differences between ASP.NET and ASP.NET Core (the good and the not so good). Just Eat is here to help you find your flavour, and with Apple Pay as a payment option in our website now, we hope you’ll now be able to find it even easier!", "date": "2016-10-10"},
{"website": "JustTakeAway", "title": "Why fewer End-to-End Tests?", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/10/04/why-fewer-end-to-end-tests/", "abstract": "At Just Eat, we undertake a lot of end to end (e2e) tests that continuously run every time we change something on our website. It goes without saying that tests give you more confidence about the product’s quality since it covers more use cases from the user’s perspective. However time is crucial in the Software Development Life Cycle – the more you wait to get feedback, the slower the development process will be. The issues we face having more e2e website tests are… They require lot of knowledge, skills and time to write quality e2e tests. They can take a long time to execute, which means feedback is slow. They can be very brittle, due to relying on UI which changes often. Testing negative paths can be complex if integrating with APIs or databases. They can be fragile since they normally rely on external dependencies and environment. And when e2e test fails… it can be like finding a needle in a haystack. So how do we overcome these issues? Well we need to make sure we are writing the right amount of tests at the right level… and by level I mean unit, acceptance and end to end. If we consider unit tests, they test a small unit/component in isolation and are fast, often very reliable and normally make easy to find the bug when they fail. The main disadvantage of a unit test is that even if the unit/component works well in isolation, we do not know if it works well with the rest of the system. In other words we don’t test the communications between each component. For that we need to have an integration test and these tests should focus on the contracts between each component and how they act when this contract is met and not met. But despite the above issues with e2e tests we know that they are the only tests which simulate real user scenarios, ie when I place an order or when I leave a review, etc – so it’s important to have a right balance of all these test types. The best visual indication of this is the Agile Testing Pyramid (see below). According to the pyramid, the best combination will be 70% unit tests, 20% integration/acceptance tests and only 10% end to end tests. In-memory acceptance tests We allow each developer to run all of the integration/acceptance level test locally. To achieve this we’ve developed a framework to minimise the issues normally encountered. The framework supports real tests via browser by hosting the website in-memory and mocks out all of the endpoint calls so it’s not dependent on the actual QA environment. FiddlerCore – Is the base component used by Fiddler and allows you to capture and modify HTTP and HTTPS traffic and we use it injecting a proxy. Coypu/Selenium – This is a wrapper for browser automation tools on .Net with Selenium WebDriver (same as Capybara in the ruby framework). IIS Express – Visual Studio .NET built in IIS server which we use here to host the Website. When you trigger an in-memory test the Selenium driver; with the Coypu wrapper, communicates with the browser whilst Fiddlercore will inject the proxy to the request via a header. The browser accesses the Website; hosted in IIS Express, exercising the server side code which attempts to communicate with various endpoints (APIs). Through FiddlerCore, we can listen to the API call being made, inject a proxy and mock the API response, so we can test the presentation layer in isolation. You can mock scenarios where the API can fail or return unexpected data and handle how this will affect the user journey – in most cases, you can just show a minimal set to a user instead of an error page. In some cases eg Authentication, you can inject an in-memory implementation of the identity server that can hijack authentication requests and issue tokens that your application trusts. Ideally, a developer should be able to run most of the tests, including all unit tests and a huge subset of the acceptance tests suite without being connected to a network. The benefits of this framework are… Faster and reliable than e2e tests. Can also be used to write tests to simulate real user scenarios. Can also be used to write integration/api tests. They provide the ability to write scenarios where you can gracefully degrade in terms of functionality. No dependency on the environment or QA. Earlier feedback in the development cycle. They can be run locally. Works well with continuous integration. Motivates the developer to write their own e2e and Integration tests. Also it helps developers to think about and simplify the architecture with simpler dependencies. Summary : Having more e2e tests don’t automatically reflect on faster delivery but it is important to have a right balance of tests and a good test automation strategy. So it goes; without saying, less e2e tests will save you time plus you can now also spend more time on exploratory testing. Also “right” set of tests allows you to evolve your architecture, refactor your code in order to continuously improve. As a footnote I would like to mention ‘Rajpal Wilkhu’ who architectured and helped us to develop this amazing framework. Thanks for reading … Deepthi Lansakara", "date": "2016-10-04"},
{"website": "JustTakeAway", "title": "Beautiful Rooms & Why Smartphones Are Too Dumb", "author": ["Written by Daniel Richardson"], "link": "https://tech.justeattakeaway.com/2016/05/19/beautiful-rooms-why-smartphones-are-too-dumb/", "abstract": "Some time in the future, the age of the smartphone will draw to a close and experiences will become more in-tune with the way humans actually live. We need to be thinking about this new wave of interactions at a time when our customer’s attention is a premium. We need to be augmenting their worlds, not trying to replace them… I’m Craig Pugsley – a Principal UX Designer in Product Research. Our team’s job is to bring JUST EAT’s world-leading food ordering experience to the places our consumers will be spending their future, using technology that won’t be mainstream for twelve to eighteen months. It’s a great job – I get to scratch my tech-geek itch every day. Exploring this future-facing tech makes me realise how old the systems and platforms we’re using right now actually are. Sometimes it feels like we’ve become their slaves, contorting the way we want to get something done to match the limitations of their platforms and the narrow worldview of the experiences we’ve designed for them. I think it’s time for change. I think smartphones are dumb… I feel like we’ve been led to believe that ever more capable cameras or better-than-the-eye-can-tell displays make our phones more useful. For the most part, this is marketing nonsense. For the last few years, major smartphone hardware has stagnated – the occasional speed bump here, the odd fingerprint sensor there… But nothing that genuinely makes our phones any smarter. It’s probably fair to say that we’ve reached peak phone hardware. What we need is a sea-change. Something that gives us real value. Something that recognises we’re probably done with pushing hardware towards ever-more incremental improvements and focuses on something else. Now is the time to get radical with the software. I was watching some old Steve Jobs presentation videos recently (best not to ask) and came across the seminal launch of the first iPhone. At tech presentation school, this Keynote will be shown in class 101. Apart from general ambient levels of epicness, the one thing that struck me was how Steve referred to the iPhone’s screen as being infinitely malleable to the need – we’re entirely oblivious to it now, but at that time phones came with hardware keyboards. Rows of little buttons with fixed locations and fixed functions. If you shipped the phone but thought of an amazing idea six months down the line, you were screwed. In his unveiling of the second generation of iPhone, Jobs sells it as being the most malleable phone ever made. “Look!” (he says), “We’ve got all the room on this screen to put whatever buttons you want! Every app can show the buttons that make sense to what you want to do!”. Steve describes a world where we can essentially morph the functionality of a device purely through software. But we’ve not been doing that. Our software platforms have stagnated like our hardware has. Arguably, Android has basic usability issues that it’s still struggling with; only recently have the worse Bloatware offenders stopped totally crippling devices out-the-box. iOS’s icon-based interface hasn’t changed since it came out. Sure, more stuff has been added, but we’re tinkering with the edges – just like we’ve been doing with the hardware. We need something radically different. One of the biggest problems I find with our current mobile operating systems is that they’re ignorant of the ecosystem they live within. With our apps, we’ve created these odd little spaces, completely oblivious to each other. We force you to come out of one and go in the front door of the next. We force you to think first not about what you want to do, but about the tool you want to use to do it. We’ve created beautiful rooms. Turning on a smartphone forces you to confront the rows and rows of shiny front doors. “ Isn’t our little room lovely” (they cry!) “ Look, we’ve decorated everything to look like our brand. Our tables and chairs are lovely and soft. Please come this way, take a seat and press these buttons. Behold our content! I think you’ll find you can’t get this anywhere else… Hey! Don’t leave! Come back!” “Hello madame. It’s great to see you, come right this way. Banking, you say? You’re in safe hands with us. Please take a seat and use this little pen on a string…” With a recent iOS update, you’re now allowed you to take a piece of content from one room and push it through a little tube into the room next door. Crippled by the paralysis of not alienating their existing customers, Android and iOS have stagnated. Interestingly, other vendors have made tantalizing movements away from this beautiful-room paradigm into something far more interesting. One of my favorite operating systems of all time, WebOS, was shipped with the first Palm Pre. There was so much to love about both the hardware and software for this phone. It’s one of the tragedies of modern mobile computing that Palm weren’t able to make more of this platform. At the core, the operating system did one central thing really, really well – your services were integrated at a system level. Email, Facebook, Twitter, Flickr, Skype, contacts – all managed by the system in one place. This meant you could use Facebook photos in an email. Make a phone call using Skype to one of your contacts on Yahoo. You still had to think about what beautiful room you needed to go into to find the tools you needed, but now the rooms were more like department stores – clusters of functionality that essentially lived in the same space. Microsoft took this idea even further with Windows Phone. The start screen on a Windows Phone is a thing of beauty – entirely personal to you, surfacing relevant information, aware of both context and utility. Email not as important to you as Snapchat? No worries, just make the email tile smaller and it’ll report just the number of emails you haven’t seen. Live and die by Twitter? Make the tile huge and it’ll surface messages or retweets directly in the tile itself. Ambient. Aware. Useful. Sadly, both these operating systems have tiny market shares. But the one concept they both share is a unification of content. A deliberate, systematic and well executed breaking down of the beautiful room syndrome. They didn’t, however, go quite far enough. For example, in the case of Windows Phone, if I want to contact someone I still need to think about how I’m going to do it. Going into the ‘People Hub’ shows me people (rather than the tools to contact them), but is integrated only with the phone, SMS and email. What happens when the next trendy new communication app comes along and the People Hub isn’t updated to support the new app? Tantalizingly close, but still no cigar. What we need is a truly open platform. Agnostic of vendors and representing services by their fundamentally useful components. We need a way to easily swap out service providers at any time. In fact, the user shouldn’t know or care. Expose them to the things they want to do (be reminded of an event, send a picture to mum, look up a country’s flag, order tonight’s dinner) and figure out how that’s done automatically. That’s the way around it should be. That’s the way we should be thinking when designing the experiences of the future. Consider Microsoft’s Hololens, which was recently released to developers outside of Microsoft. We can anticipate an explosion of inventiveness in the experiences created – the Hololens being a unique device leapfrogging the problem of beautiful rooms to augment your existing real-world beautiful rooms with the virtual. Holographic interface creators will be forced to take into account the ergonomics of your physical world and work harmoniously, contextually, thoughtfully and sparingly within it. Many digital experience designers working today should admit to the fact that they rarely take into account what their users were doing just before or just after their app. This forces users to break their flow and adapt their behavior to match the expectations of the app. As users, we’ve become pretty good at rapid task switching, but doing so takes attention and energy away from what’s really important – the real world and the problems we want to solve. Microsoft may be one of the first to market with Hololens, but VR and AR hardware is coming fast from the likes of HTC, Steam, Facebook and Sony. Two-dimensional interfaces are on the path to extinction, a singular event that can’t come quick enough.", "date": "2016-05-19"},
{"website": "JustTakeAway", "title": "Tech-talk: David Clarke of Wonga on Scaling Agile Planning", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/04/29/tech-talk-david-clarke-of-wonga-on-scaling-agile-planning/", "abstract": "Yesterday, David Clarke of Wonga came and talked to us about how they plan the work that they take on in engineering. Regularly, across a unit of 150 people, in a company of 600. Speaker: David Clarke, Head of Tech Delivery. Abstract: Planning @ Wonga Every six weeks all our Scrum teams (approx. 8), together with Tech Ops and Commercial guys go off site to plan.  We have done this at Wonga for a long time.  The evolution to what and how we do it reflects Wonga’s evolution as an organisation from early start up days to being a formally regulated body.  To people coming along for the first time it can seem like organised chaos. Why we do it (and what happens when we don’t) Who is involved (and what happens when they are not) How we prepare for planning How we do it (and lots of ways we don’t do it anymore, epic failures included) What metrics we collect Biscuit awards (and other ways to keep it fun) The talk will be based on real planning event artefacts, data and plenty of photos from the events. Recording", "date": "2016-04-29"},
{"website": "JustTakeAway", "title": "Solving Italian address input with Google Places", "author": ["Written by Nick Thompson"], "link": "https://tech.justeattakeaway.com/2016/05/04/solving-italian-address-input-with-google-places/", "abstract": "Solving Italian address input with Google Places Postcodes in Italy JUST EAT’s UK website uses postcodes to determine whether or not a takeaway restaurant delivers to an address. This is the case for a lot of our international websites and it often proves to be an effective way of accurately specifying a location. When JUST EAT started operating in Italy we observed that postcodes are not a popular with our customers as a way of defining delivery address. One possible reason for this is that postcodes in Italy are not as accurate as we are used to in the UK, even in built up areas. This was issue as our systems use postcodes. There were already projects in place to move from postcodes to latitude and longitude which would allow us to define our own custom delivery areas. This would remove our dependency on postcodes but from a customer’s point of view would not help them define their location anymore easily. We needed a user interface that would allow the customer to enter their delivery address in a way that suited them. What did we try? We produced three experiments that were A/B tested in parallel. The experiments were made available to a limited percentage of customers over the course of a month to ensure the results were statistically significant. The three experiments are described below. Postcodes Anywhere Postcodes Anywhere is now known as PCAPredict. They provide an address lookup service called Capture+. This service autocompletes the user’s input and forces them to make a selection from the address options given. A trial version was implemented using PCAPredict’s prototyping tool. This allowed us to insert an instance of the Capture+ interface that would capture the address and pass the appropriate data to our server upon search. This was the easiest of the three experiments to implement. Google Places Google Places is a google service for retrieving location data for residential areas, business areas and tourist attractions. An autocomplete service is provided as with PCAPredict with a slight difference. Google Places suggests locations at different levels of accuracy instead of forcing residential address level accuracy. The autocomplete widget provided by google allows you to attach to an existing html input element and react to selection events. When experimenting with Google Places, we needed to specify two options; ‘componentRestrictions’ and ‘types’. //Configuration options for the Google Places widget\nvar options = {\n    types: ['address'],\n    componentRestrictions: {\n        country: 'IT’\n    }\n}; ComponentRestrictions Types The ComponentRestrictions allow us to filter by country using a two-character, ISO 3166-1 Alpha-2 compatible country code so this was set to ‘IT’. The ‘address’ type instructs the Places service to return only geocoding results with a precise address. Google data is always improving, however does not always suggest street level accuracy. This was an issue we needed to rectify in order to use the widget with our existing system. This will be discussed in a later section. Once the widget was configured, in much the same way as the PCAPredict Capture+ tool, the data needed to be processed and passed to our servers. Google Geocoder Google Geocoder allows a string description of an address to be submitted to the service with the closest matching location data sent back to the client. This service is not designed for autocomplete suggestions but during initial investigations, the suggestions were consistent with behaviour expected from google map searches compared to the suggestions given from Google Places. We also found that the quality of the data seemed to be more mature than that of Google Places but during the course of development this doesn’t seem to be as apparent anymore. This was the reason we decided that it was useful to test a solution based on Google Geocoder in addition to Google Places. We constructed a widget that was similar to the Google Places widget but serving suggestions from the Google Geocoder. What was the outcome of our A/B testing? The experiments were being carried out against the existing home page search. This search was made up of four input boxes that allowed the user to specify the street, street number, city and postcode. This was used as a control that the experiments could be compared against. The experiments were run for approximately four weeks with 10% of Italian users being sent to them. The metric we were using to determine success was conversion which is defined as the percentage of visitors to the website that complete an order with JUST EAT. Poscodes Anywhere The PCAPredict Capture+ experiment didn’t see an increase in conversion during A/B testing. Google Geocoder The Google Geocoder showed small improvement although the interface was awkward and not designed for this purpose. The overall increase in conversion was minor. Google Places Showed an substantial increase in conversion. This was a stand out winner from our testing but there were still areas we thought we could improve. The suggestions could not be filtered to the ones that provided the accuracy we required. This would mean users would have to keep trying options until one met the criteria for a successful search. How did we resolve the issues? Based on the A/B testing results, we made the decision to develop the Google Places experiment further. The accuracy of suggestions was still an issue and after testing it had been revealed that this issue was mostly focused on getting from street to street number accuracy. The solution we decided upon was that we would ask the user for the street number explicitly when this situation occurred. This would take the form of an additional input that would be revealed to the user that prompted them for this information. To achieve this, we took the interface that had been built for the Google Geocoder and replaced the Google Geocoder service with the Google Places autocomplete service. As we had complete control of the logic within the widget, it was trivial to detect the missing data events and react to them by displaying the additional input. The second issue encountered with Google Places was that sometimes addresses could not be found that users were requesting. This was not an issue we encountered with the Google Geocoder service. For this reason we built a fall back into the Google Places custom widget that would Geocode the given address if the street number was provided but the address was not found. //Supplies suggestions based on an address string\nnew google.maps.places.AutocompleteService();\n//Returns location data based on an address string\nnew google.maps.Geocoder(); What was the final outcome? The final outcome of implementing the custom Google Places search on the the Italian homepage was a significant increase in conversion. This implementation is now being used for 100% of users in Italy. What next? There are still many ways we can improve on this feature. Google Places allows us to alter the type of suggestions made to the user. It also returns more data than we currently make use of and allows us to upload our own data that can be returned with selected suggestions. Google Places also integrates seamlessly with Google Maps which opens up more possibilities for specifying location and returning location based results. For these reasons, JUST EAT will be continuing to experiment with Google Places during the second quarter of 2016 with an aim to roll this feature out internationally. Stay tuned for more updates.", "date": "2016-05-04"},
{"website": "JustTakeAway", "title": "Customising Salesforce Marketing Cloud", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/04/04/customising-salesforce-marketing-cloud/", "abstract": "Personalised marketing has evolved quickly in recent years and customising digital communications based on customer behaviour has become commonplace. With the widespread consumer adoption of mobile devices and social media, the number of channels across which marketing operations must be carried out has only increased – meaning data plays a crucial role in creating tailored, cross-channel customer interactions. Fortunately, a number of marketing management CRM solutions are available to greatly streamline this process. At JUST EAT, we have chosen Salesforce Marketing Cloud . Marketing Cloud offers a comprehensive suite of services, including data and analytics, email editing, management of social media advertising, interactive website creation, and cross-channel marketing automation. We currently only use a subset of Marketing Cloud’s functionality but it is already proving to be a powerful enabling platform for the marketing team – helping them to build automated campaigns without the need to write code. However, we have found that there are many business requirements that still can not be fulfilled by the vanilla Marketing Cloud product. Fortunately, we can customise the experience and provide the marketing team with even more automation tools. A (very) brief Marketing Cloud 101 We needed to make unique voucher codes available to campaigns from our main e-commerce platform. A worked example will best illustrate the problem we encountered when introducing this custom behaviour. We have built a Windows service that sits inside our Virtual Private Cloud (VPC) in AWS and subscribes to a number of important messages published to our internal message bus. In turn, these messages are mapped to a structure that enables each one to be sent in a POST request to Marketing Cloud’s REST API – the information will be written to a new row in a Marketing Cloud Data Extension . Just think of Data Extensions as tables in a relational database. The following image shows a simple Data Extension with some test customer entries pushed to Marketing Cloud by our service. Marketing Cloud uses a contact model to provide a single view of a customer’s information across Data Extensions, so let’s assume that this Data Extension is correctly set up with our contact model, otherwise we couldn’t be able use this data in our Marketing Cloud campaigns. The first step to building a campaign will be to build a simple automation in Marketing Cloud’s Automation Studio . Automations can be used for a number of purposes but we’ve found them particularly useful for running a series of activities to firstly query the data in our Data Extensions, in order to establish an audience for a campaign based on some criteria, and then trigger the running of the campaign for this audience. For example, we may want to run a campaign that sends out vouchers to an audience which only contains customers who haven’t recently placed an order. The image shows a simple automation with just two activities – a query and a trigger. The query activity will write the audience to another Data Extension which we define and the trigger will fire an event which will run our campaign for any contacts written to this Data Extension. The campaign will be defined as a customer journey in Marketing Cloud. We can use Marketing Cloud’s Journey Builder to drag and drop the different activities that make up a customer journey from a palette onto a canvas. Example activities include sending an email or SMS, updating rows in Data Extensions, waiting for a period of time, or making decisions to send our contacts on different paths through the journey. We can define a simple journey that just sends an email. Note that Journey Builder also requires a wait activity before a contact exits a journey. Our entry event shows the event data source as our Data Extension that contains our audience. Each contact in this Data Extension will pass through this journey and should eventually receive an email based on a template that we define for the email activity. Now we want to add an additional activity before sending the email that requests a voucher from our internal Voucher API to include in the email. This is the exact problem that we encountered in our recent work and, by default, there’s no way to do that from a customer journey. However, we can create a custom activity that will be available from the activity palette and allow us to do just that. Building custom behaviour A custom activity is simply a web application that is hosted on a web server. The structure that these applications must follow in order to be used as Journey Builder activities is well defined but there is still a great deal of flexibility with regards to the technology chosen to build the application. All of the basic examples provided by Salesforce are built using the Express web framework for Node.js so we decided to do the same as it seemed the path of least resistance. However, knowing what we know now, we could have just as easily built it using other web frameworks or technologies. When a contact reaches our custom activity in a customer journey we want the following chain of events to occur… A voucher request is made from the journey to our web application back-end and the contact moves to a wait activity in the journey. The web application makes a request to our internal Voucher API and receives a voucher code in the response. The web application sends the voucher code to the Marketing Cloud REST API so that it can be written to a column in our campaign audience Data Extension against the contact record. The contact moves to the email activity in the journey where some server-side JavaScript inside the email template fetches the voucher code for that contact from the Data Extension and writes it to the email. We need to write the voucher codes to a Data Extension in order to make them accessible to a Marketing Cloud email template. The back-end for the web application is a fairly standard Express REST API that includes a number of endpoints required by Journey Builder. During a running journey, the voucher request is sent to an endpoint in order to execute the functionality required to complete steps two and three, listed previously. There are a few other endpoints that are only required by Journey Builder when the journey is being edited. During the editing process, both Standard and custom activities in Journey Builder have a configuration wizard that displays in an HTML iframe in order to configure the activity after it is placed on the canvas. For example, for our voucher custom activity it makes sense for us to be able define voucher amount, validity period and other related parameters for that particular campaign. We also need to choose the Data Extension and column to which the voucher codes will be written. This wizard is provided by the front-end code of our web application. Salesforce even provides FuelUX , a front-end framework which extends Bootstrap and provides some additional JavaScript controls. This enabled us to match the look and feel of the Marketing Cloud UI and include a picker for choosing the Data Extension and column for the voucher codes. There are a couple of requirements for the front-end code to function correctly in Journey Builder. Firstly, Postmonger must be used in our code. It is a lightweight JavaScript utility for cross-domain messaging and is required as a mediator between our configuration wizard and Journey Builder. Secondly, the root of the front-end code must include a configuration file that contains, amongst other things, the URLs for our back-end endpoints, the inputs and outputs of the custom activity, and a unique application key. We define the unique application key when we create a new application in the Salesforce App Center as an Application Extension and add our custom activity to this. We also need to provide the endpoint of our custom activity at this point. This step is required for connecting applications to the Marketing Cloud platform and will provide us with a generated Client ID and Client Secret to authenticate with Marketing Cloud and allow our custom activity to interact with the Marketing Cloud API. Salesforce recommend to use Heroku for hosting custom activities. Heroku is a great option for this type of lightweight Node.js application but wasn’t ideal for us as we needed to interact with our Voucher API which sits inside our VPC. As a result, our custom activity is also hosted inside our VPC so communication with any internal resources is not an issue. This means that we only have to manage the security between our custom activity and Marketing Cloud without publicly exposing the endpoint to our Voucher API. Hosting within the VPC also allows us to take advantage of our internal stacks setup for logging and recording stats. Following these steps we are now able to drag and drop our custom activity from the activity palette onto the canvas for use in the customer journey. Conclusion Not only did we deliver a critical component that will be used across a number of our marketing campaigns, but we also opened up the possibilities of what can be done within the confines of a customer journey. Marketing Cloud offers some great automation tools for marketers but pairing it with the flexibility of our own platform in AWS should open up some interesting opportunities for coordination between the two. We will surely be exploring what other custom activities we can add to the marketing team’s toolset in order to further enable them to react quickly without the need to make amendments to our codebase.", "date": "2016-04-04"},
{"website": "JustTakeAway", "title": "The minimal form (part one – the explanation)", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/03/28/the-minimal-form-part-one-the-explanation/", "abstract": "Before we start, you’ll notice I’ve named this blog part one – I plan to deliver a series of posts over the coming weeks, and this is primarily so you can see my progression and how my learning amplifies as I dig deeper into the topic of the minimal form. But, at this point you’re probably wondering what the minimal form is. So without further ado… The minimal form (or single input field) is a new way to display and interact with form fields. It is essentially a single form field that switches and changes to suit the desired input once a user submits their information. It’s primary purpose is to simplify the form filling process, whilst keeping it engaging and less tiresome. Here is a great example I’ve grabbed from the web, which outlines the concept in full. These types of forms aren’t limited to capturing basic information like name, address or telephone number. They can be implemented to enhance the form filling process for far more complex information, such as credit card or payment information . Furthermore, there are a few companies who are currently taking full-advantage of the minimal form to create engaging and easy form filling questionnaires, such as Typeform . What’s the benefit for your users? Sure, the majority of these forms dotted around the web are simply seen as nice to haves or delightful elements. However, there are certainly some initial wins from a user experience perspective when it comes to the minimal form, especially beyond first glances. For example, the most tangible benefit of a minimal form would be for mobile devices. As you can see, I’ve included a screen that outlines the skeleton of a common mobile web form. You can see the canvas space that designers have to play with, when they’re specifically considering form fields in the design process. In 2016 it is considered common practice to simply collapse information to fit onto mobile. However, when you do this, spatial problems arise, and to maximise product objectives designers and engineers can minimise extensive clutter in multiple ways. For example, you can stack content below the fold, hide it amongst tabs, accordions or drawers. Ultimately, these solutions are seen as functional in most cases, which is great for mobile users across the web. However, at JUST EAT, the UX team don’t just aspire to build experiences which are simply functional. As a collective team, we aim to build, explore and innovate these kind of experiences to help us empower our users to love their takeaway experience. Ultimately, this means it’s our job to ensure we’re making experiences which are functional, reliable, usable, as well as pleasurable. Subsequently, we’ve identified that minimal form is certainly an interesting area which could push the usable and pleasurable aspect of our product, whilst retaining that functional and reliable foundation. There’s no denying that minimal form (for mobile, at least) has multiple benefits from a user perspective… It allows users to concentrate their effort on one form field at a time. Ensures the user is not overwhelmed with the sheer amount of information he/she may or may not be required to submit. No/minimal scrolling required. Less tapping/frustration. We assume it will be a quicker experience for our users – although, it would be interesting to see if this is the case via user testing. Perhaps it’s actually a longer process, but is perceived to feel quicker?) An assumption might be, if you’re only focusing at one form at a time, would the information submitted be more accurate from your user base? We’ll dive deeper into the kind of metrics a minimal form could improve in part two… Some disadvantages… There are certainly some disadvantages to the minimal form concept. For example, how would you display an error state? Would a user have to go back if their password confirmation was wrong? Surely that would go against the progressive motion of the minimal form? Furthermore, it’s great not having to see all of the forms you have to fill, but how would you check to ensure all of the information was correct before submitting? Also, what place does the minimal form have on desktop? What tangible benefits does implementing a minimal form have for desktop users? And finally, how many people using this form for the first time will know how it works? I’m confident that these are problems that can be solved when implemented in a user flow with a bit of intuitive design thinking. Ok, so where would you start? Well, you’ll have to stay tuned for part two where we look at implementing something similar to the minimal form as we continue to enhance our product, and help implement a more usable and pleasurable experience for all things mobile.", "date": "2016-03-28"},
{"website": "JustTakeAway", "title": "Product management in country improvements", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2016/03/14/product-management-in-country-improvements/", "abstract": "Product management can be a difficult role to define. Most companies have a slightly different view of what a product manager should do, and so the definition (and even job title) of a product manager will often vary between organisations. It is also the case that within the same organisation different areas of the business can evolve their product management positions in different ways, as is the way at JUST EAT. And this is fine, as product managers are typically people who are willing and able to adapt themselves to the needs of the team(s) they work with and the products they manage. Technology at JUST EAT is split into eight core areas, one of which is International Product Development (IPD), which I work in. Each area has their own focus and each have evolved independently in terms of their requirements of product management. However, we’re all working towards a common goal – to build a JUST EAT experience for consumers and restaurant partners that delivers against our mission to “Empower consumers to love their takeaway experience” and supports our restaurants to become as successful as possible. So what’s working as a product manager in JUST EAT like? As product manager of the International Advanced Market (IAM) team it’s my responsibility to ensure the team is supporting the group of advanced market countries (Denmark, Norway, Ireland, Belgium, Netherlands), together with my team’s technology manager. The support my team provides is underpinned by our mission statement: All this sounds very nice, but what does it mean in practical terms? On a day-to-day basis my job as a product manager can be broken down into the following areas: Supporting the team Partnering with the countries Looking to the future Personal development Supporting the team This is what I would call fundamental product management. It’s the core task of requirement definition (in whatever form it takes) and prioritisation. In the IAM team we use the Scrum framework for development and this covers user story creation, backlog grooming, sprint planning and representing the user. It’s a privileged position, working between my colleagues in Tech (engineering, UX, QA), Business Intelligence and the teams in the countries, bringing together lots of different skills to create an awesome product. With such a fast-paced environment at JUST EAT this aspect of the job is very demanding, but with such highly skilled people across all of Tech this is also extremely rewarding, and continually delivering tangible improvements gives me the drive to keep on pushing the bar even higher. Partnering with the countries Each of our businesses around the world are run independently, managed by local teams who understand their market and drive success. With all product development resource based in the UK and Kiev it’s critical that we stay closely aligned to their businesses and understand what’s happening in their local markets. We have regular calls with the country teams and collaborate on requirements and prioritisation (see my previous blog post – Tools for International Communication to Manage Technical Change ), share market research and data analysis as well as experimenting with changes to the site/apps to see what works for users in different countries. The remit of the team I work for requires additional responsibility of understanding and representing the countries I support. With so many teams across Tech delivering changes at breakneck speeds, I get called upon to ensure country specific requirements and nuances are not lost in the continual improvement happening across the platform. It’s an immensely satisfying part my job and comes with added benefits of regular travel to visit the country teams. Looking to the future It’s easy to fall into a trap of only focusing on what you have in front of you, fire-fighting today’s issues while forgetting you need to be planning what comes next. There are times when this might be the correct move but you need to be conscious of it and take action.  JUST EAT plan on a quarterly basis using the OKR methodology (we’ve even built our own tool for managing them – Managing OKRs at JUST EAT tech with our own web app ) and so it is paramount that the product managers maintain that roadmap and vision for what comes next – not just the following quarter but for the next year or more. Of course your plans change, that’s the great thing about working in such an agile company, but having that draft plan to start with is key to figuring out what’s changing in the marketplace and how you should react. We’ve adopted some of the templates created by Roman Pichler and adapted them for our own requirements – for anyone looking to instill some structure into their planning for agile teams this is a great place to start. We’re continually developing our templates and have established a great framework within IPD for planning and communicating our vision for the future. Personal development Personal development is a big deal at JUST EAT, and all employees are supported to create and maintain a personal development plan to achieve their career aspirations. The document leads with the question “Where do you want to get to?” and helps you create a plan to get there. This is backed up with personal OKRs and a training budget of £1,000 each year to spend on the resources you need to support your development plan. It’s great to work somewhere that values people development so highly, and backs that up by empowering you to take true control of it. In summary JUST EAT is a fantastic place for product managers – there is so much energy and enthusiasm, a ton of exciting work happening across the company, and it’s immensely satisfying to be working for a company that truly understands what product management is all about.", "date": "2016-03-14"},
{"website": "JustTakeAway", "title": "Tech-talk: Ant Stanley and from zero to MVP in 20 days with no servers", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/03/09/tech-talk-ant-stanley-and-from-zero-to-mvp-in-20-days-with-no-servers/", "abstract": "Yesterday, Ant Stanley of acloud.guru regaled us with a talk about how his startup was able to go from zero to a working MVP in just 20 working days, using AWS Lambda and API Gateway plus a set of other *aaS products, and no servers . Abstract acloud.guru are a training course provider that host their service with AWS and don’t use any servers. This is how they did that. Ant Stanley Ant has successfully helped organisations from the UK, Europe, and South Africa transform their IT platforms to deliver business value. With over 16 years IT experience, Ant understands the rapidly changing IT landscape enterprises have to deal with today. Ant has worked for organisations such as Sungard Availability Services, Level 3 Communications, Hostway and Business Connexion. Recording Tech talks at JUST EAT This is one of the reciprocal tech talks that we arrange at JUST EAT. See full details here .", "date": "2016-03-09"},
{"website": "JustTakeAway", "title": "Calabash Page Objects", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/03/21/calabash-page-objects/", "abstract": "Faster development of Calabash tests While creating the page object classes in our Calabash mobile test suites at JUST EAT, we found ourselves repeating a lot of actions when waiting for, scrolling to and interacting with elements on the screen. We abstracted these actions into a library to avoid this unnecessary duplication of code and made these actions agnostic to screen size. This library has now been published as a ruby gem called calabash-page-objects . Why use this? Dealing with small screens Sometimes you have to scroll to elements on small screens but not on larger screens. We initially used if-statements dealing with an environment variable for ‘small screen’ inside your test code – not good! We wrote a method to scroll to an element if it wasn’t immediately there. This method was then included in many of the methods available to our elements; touching, inputting text, asserting presence etc. Multiple scrollable views When attempting to use Calabash’s default scroll method, we noticed that sometimes it didn’t appear to scroll the view we wanted if there were multiple scrollable views on the screen. After looking into the Calabash methods, we noticed that you could perform scroll actions on locators. We wrapped up this method in the gem too so that we could pass both the element we’re searching for and the view it belongs in into all the helper methods. This became the ‘parent’ parameter that the gem methods can optionally take. How to use? The calabash-page-objects gem exposes two element classes, one for iOS the other for Android. They are implemented in the same way regardless of the platform under test. These element classes have methods for, waiting for them to be visible, waiting for them to disappear, touching them, etc. These methods all take variables in a consistent format. require 'calabash-page-objects'\nclass HomeScreen\n def initialize\n   @my_element = IElement.new(\"* id:'something'\")\n   @form_scrollview = IElement.new(\"* id:'something_else'\")\n end\n def some_method\n   @calabash_query = @my_element.screen_query\n   @my_element.when_visible(30)\n   @my_element.prod\n   @my_element.when_not_visible(30)\n   @element_present? = @my_element.present?(timeout: 2, scroll: true)\n   @my_element.input('email@email.com', parent: @form_scrollview)\n   @my_element.check\n   @my_checkbox.uncheck\n   @ischecked? = @my_checkbox.checked?\n   @element_text = @my_element.text\n end\nend More information See the project on Github for more information, and a more detailed description of the methods and parameters. Feel free to fork it and contribute too.", "date": "2016-03-21"},
{"website": "JustTakeAway", "title": "Google’s material design", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2016/02/04/googles-material-design/", "abstract": "Google announced material design in June 2014 and it’s essentially a set of design guidelines Google have created to enhance the user interface and user experience of android apps, responsive websites and even software. It’s a well rounded and maintained document (found here: http://www.google.com/design) including principle guidelines for things such as grid based layouts, image preferences, typography, transitions, animations, static and animated icons, plus a whole lot more. In basic terms, material design was created due to extensive design inconsistencies across Google products, and Larry Page (CEO of Google’s parent company, Alphabet Inc) took a top down approach to encourage Google to re-design all of their products, which created a mobile-centric shift in the company’s approach to design. At JUST EAT, the international product development team are in the process of updating the android apps to include elements of material design. Design improvements: Cleaner and clearer Simplification of typography Removal of skeuomorphic elements Clear information hierarchy Bold and clear CTAs (call to actions) Introduction of white throughout the app You can also see our open source Android sticker sheets via: ux.just-eat.com Material design encompasses the idea of layers. Every component sits on a particular layer; depending on your design, each layer holds a significant value in the hierarchy of information within the design, and within the development regarding transitions and animations. (Image reference: google.com/design) Adopting these guidelines isn’t just an excuse for designers to create ‘cool’ or ‘nice’ design: The principles, meaning and significance which has been crafted into these guidelines hold a lot of weight for designers who want to create a lasting user experience which engages users in a consistent form across devices. I think you’d agree, as the web now sits across an array of different devices such as: desktop, tablet, mobile and smartwatches, it’s becoming imperative to achieve a consistent user experience and retain a sense of familiarity through-and-through. The card: Material design helps with this by coming up with concepts which strategically consider the possibility of varying screen sizes whilst keeping the user at the forefront of the thinking. For example, the introduction of ‘cards’; although not directly invented by material design, they certainly play a strong role within the design patterns. A card visually betrays itself as a physical object (via styling) which helps segregate content into digestible pieces as well as making them responsive across devices. (You can learn more about material design cards, here ). (Image reference: google.com/design) The FAB: Floating action buttons (FABs) are a great example too. The FAB concept, albeit one of the most controversial components to come out of material design, is primarily intended to make the most prominent actions the easiest to reach and the loudest on the page. For desktop they’re typically on the top right (depending on the design) and for mobile they’re usually bottom right to enable easy access for users as it’s closest to the user’s thumb. (Image reference: http://www.androidcentral.com/sites/androidcentral.com/files/styles/larger_wm_brw/public/article_images/2014/10/google-inbox-4.jpg?itok=uYsAr_mz) You can learn more about material design’s floating action buttons, here. There is a whole set of design components typically used throughout material design, so I won’t go through all of them, although, you can craft an idea about how much thought has gone into considering these components. Transitions and animations: The guidelines also give you all the principles to implement animations and transitions in a meaningful way. By that statement, I’m not referring to those endless parallax websites with 15 thousand lines of jQuery and JavaScript code holding it all together with 3D explosions which only work on Chrome. I’m referring to the guidelines which make it incredibly easy for designers to replicate the extremely lightweight, flat and meaningful transitions exposed within the design language to help integrate within your designs whilst contributing to an immersive user experience. Again, you can learn more about material designs animation section, here. One of the best features of material design is that it can be adopted onto any app or website. Regardless of the brand, this design language is flexible enough to be subtly implemented in ways which can optimise product objectives. Sure, Google have shown off adverts of material design with web/apps displaying an array of exasperating bright colours, over-the-top transitions  and of course, gimmicky music. This, of course helps for marketing purposes, which convinces designers like myself to initially take notice of their launch. However, once you’ve stripped this concept of all of its ‘trendy’ gimmicks and looking at its core principles for an effective and clean UX  you’ll be surprised of its value not just for now, today  but for user experience principles which will stick for a very long time. Here are a list of useful and cool links which might spark further interest in material design: https://design.google.com/ https://design.google.com/articles/evolving-the-google-identity/ https://design.google.com/articles/the-art-behind-android-marshmallows-new-wallpapers/ https://design.google.com/articles/expressing-brand-in-material/ http://www.fastcodesign.com/3046512/how-google-finally-got-design", "date": "2016-02-04"},
{"website": "JustTakeAway", "title": "Aligning your Front End Process", "author": ["Written by Ashley Nolan"], "link": "https://tech.justeattakeaway.com/2016/01/11/aligning-your-front-end-process/", "abstract": "Developing with front-end code is highly deceptive. From a distance it seems straightforward; HTML, CSS and JavaScript are easy languages right? Not so much. As a project scales, the structure of your front-end code becomes essential. How you choose to do things when your website has just a few pages and one developer working on it can quickly come back to bite you if you neglect to think about the future. At JUST EAT we have a number of developers working on the front-end of any one of our products. As with any project, ensuring that those developers are working together, and not against each other, is of massive importance. I want to take you through some of the things that I’ve found help to keep our projects and developers aligned and our codebase more structured. Although these solutions have come about because of working with larger scale projects, they are equally useful on smaller projects. In essence, they are all about encouraging good habits and practices. Consistent Workflow Front-end workflow tools such as Gulp and Grunt have become extremely popular over the last couple of years. They put in place a set of tasks that can then be carried out by any developer working with the project. Each task is comprised of a set of operations to run; this can include things like code linting, CSS preprocessing, compressing code and images and running JavaScript tests you have written. The biggest benefit of using any workflow tool is that it gives all of the developers working on the project a consistent way of carrying out these tasks. The configuration files are a part of your base project and so, once the tasks are setup, a newcomer to the project will be able to run these almost immediately. These tasks can also be run as part of a projects build process to ensure that uncompressed code and images are never deployed. Doing this means you aren’t reliant on developers having to remember to carry out tasks required to make your website production ready – they are done automatically in your release build. If you take away only one thing from this article, it should be the importance of spending some effort on your projects workflow. The benefits vastly outweigh any potential time you will spend initially setting it up. Modularise One of the hardest things to do when developing the front-end of a website or application is to step back from thinking about the context you are developing for and build code for any scenario. Taking CSS as an example, it’s easy to name things with the page we are building for, such as .searchResults-item or .homepageCarousel . These classnames are useless in terms of reusing those styles; what happens if I later want to use the same styles that the .searchResults-item element has, but to a list of user comments instead? It would make much more sense to use classnames that can be used in any context. Try to build all of your code in components. Across teams, this means that you will save time as you reuse classes and styles when building new pages; Some components may be able to be used with very little modification, others may be able to be adapted and modified slightly. The same goes for JavaScript. Ensure modules that you write can be potentially reused in any part of your site, or on any future projects. Well written JavaScript modules can be used on any project, not just the one you are working on right now. Thinking in components isn’t an easy habit to get into, but as you get better at doing so, your front-end code will become much easier to work with, while having the additional benefit of being much more future-proof. Stay on the same page It’s a widely held saying that a good codebase should look like it has been written by one developer, irrespective of how many people are working on it. This can be a challenge; every developer likes to write code in slightly different ways and so this involves encouraging change for the benefit of the team and project. In my experience, good developers will embrace code consistency within a team over any personal preferences they may hold. Ultimately, there’s no point arguing over the use of spaces vs tabs. If you have a preference for a certain type of formatting, then try to make sure you can give reasons beyond ‘because I like doing it that way’. The most important thing is to make these choices as a team and stick to them. Using a linter – such as Stylelint for CSS or ESLint for JavaScript – will ensure that the formatting you decide on is adhered to. Using linters for code formatting can seem over the top and un-needed to some. I find them to be quite the opposite. They act as your first line of defence in code quality, stopping any badly formatted code hitting the codebase. They also help to ensure code reviews focus on the way code has been written rather than getting bogged down trying to spot formatting inconsistencies. This is tiresome both for those doing the reviewing and those whose code is being reviewed. CSS Naming Schemes are also very valuable in terms of project consistency. BEM and Kickoff are naming schemes that help promote modularity, but also put in place rules over how classnames are defined. When a naming scheme is being used, classnames become more self documenting and it becomes easier to see how classnames relate to one another. Ultimately, the more you can do to promote consistency across your team, the easier your codebase will be to maintain and work with. Code Commenting and Documentation As with anything important, you truly realise how good something is when it’s not there and this is especially true of code comments and documentation. First up, if you aren’t commenting your code, you need to start. This doesn’t just apply to those working in a team; code you write today you will inevitably return to in several weeks or months. It’s always good to have an insight into what you were thinking when you first wrote that code. Commenting doesn’t need to be some elaborate process – it’s just about explaining the basics. If you’re writing a function in JavaScript, say what it does, even if you think it’s obvious. Writing a utility class or an element modifier in CSS? Then explain how it will affect an element it is applied to. Code commenting becomes even more important when working in a team. For example, at JUST EAT, it’s impossible for me to see every piece of code that goes into the project I work on. So when I come to modifying an area of the site I’ve never used, it is a massive help if the person who wrote that code commented it. At the very least, I try to write a comment for every CSS component that I write, or for every function in JavaScript. Documentation is more involved but is equally useful. Spending time to document how things have been done in your project is useful not just to other developers working on the project but also to you. At JUST EAT, we maintain a Component Library which contains every CSS component class available in our project. If another developer comes to work on a new feature, they can take a look through this documentation to see if something can be reused first. Without this, every developer would need to know every front-end component in the codebase – an unrealistic goal for any mid-size project that changes over time. If you have a number of developers in your team, try to spend some time building up documentation relevant to your project – or projects – that you work on. They can be valuable as a reference point for all the members of a team, and especially so for new hires. In summary Trying to keep a consistent codebase which is being worked on by a number of developers isn’t easy, especially as new members join and your team grows. The key is to put the right processes in place that will gain you and your team the most value over time. Look at what causes you the most headaches – is there confusion over how to do simple things like compiling preprocessor code or image compressing? If so, spend time on your workflow tooling. Is your CSS hard to manage because the structure isn’t clear or there is too much inconsistency? Look at how you can modularise your CSS and tools that can help ensure your team writes CSS in a more consistent manner. Putting the effort in to review your workflow and team processes will pay back handsomely over time.", "date": "2016-01-11"},
{"website": "JustTakeAway", "title": "Designing at JUST EAT", "author": ["Written by Simon Poole"], "link": "https://tech.justeattakeaway.com/2016/01/07/designing-at-just-eat/", "abstract": "Find out more about how the UX team work at Just Eat on our Just Eat design blog . A while back I joined the JUST EAT UX design department to work in the Strategic Projects team. Working at the world’s leading digital marketplace for takeaway, with 11 million active users, and generates around £1 billion for the restaurant industry, is definitely a huge responsibility and it comes with a lot of challenges. We design experiences to shape the future of food and technology. Our main focus is empowering consumers to love their takeaway experience, and everything we do is revolved around this idea. Working here means that you have to be frank, innovative, hard-working, and most of all, passionate. Now let me give you some more insights. The design process Here at JUST EAT we don’t do design just for the sake of design. Our job is to understand the user’s problem, propose a hypothesis, research and validate, establish a testing approach and deliver a solution. All these can fit into three simple questions — what, who and how? What problem are we solving?— Being a designer means that we have to solve user problems while generating real business value. In terms of JUST EAT, that means improving the experience, building new features or identifying new opportunities that are easy to scale. Who are we addressing? — Creating personas based on qualitative and quantitative research. This is helping us figure out how our ideas are gonna fit into the real world, and will make it easier to validate those, or prioritise features. (Such as helping time sensitive customers get their food faster.) How are we solving it? — The third step is figuring out a way to solve the problem. This means doing research, validating ideas, prototype, design, test, and deliver. We’re not just sitting in front of a computer, we’re ‘getting our hands dirty’ by analysing data, having focus groups, doing usability testing, and so on. Most of the times we have to take small steps — like releasing a quick fix for a much bigger problem, until we can actually gather enough data to solve it properly. We’re not alone on this though. Some of the best product managers, researchers, developers and other UX colleagues are gonna have your back. After all, one of our four values is ‘team’. Teams JUST EAT is growing fast. In 2015 we hired actively on all fronts, with 156 new employees. The UX team is now formed of 11 designers working between London and Bristol. Before showing how we nailed this collaboration, let me give you some insights on how we work: Cross-platform — Every designer that’s part of our team knows how to work for web, iOS and Android. We try to embrace the platform, using the native behaviour as a starting point (based on the human interface/material design guidelines). Once in awhile we get overly excited and get involved in different small projects, like designing for Xbox or tvOS. Vertical teams — We’re split between autonomous teams, focusing on different objectives (Customer Acquisition, Retention, Strategic Projects, and so on). Most of the teams are formed of a Technology Manager, Product Manager, Business Intelligence, UX and Development — all working together to achieve the team’s OKRs. Agile — The same as most big companies, we’re guided by agile methodologies in order to allow incremental, iterative work between self organising, cross-functional teams. Collaboration Focusing on various objectives between two different cities makes it tricky to collaborate. There is no proper recipe on ‘how it should be done’ but along the way we realised it’s a mix of transparency, communication, and honest feedback. UX sync — Every week we have a general sync with all the design teams at JUST EAT. Everyone has to share ‘what they worked on’, ‘what they learned from it’ and ‘what’s the next step’ . This way we make sure there are no overlapping projects and that we can get feedback before sending things to production. HipChat Room — We have a dedicated room for design conversations where we ask for feedback, share opinions, or post random gifs. Short catch-ups — Whenever we feel like we need to get a better understanding of something, we have a short hang-out. These are regular things at JUST EAT and it helps smoother transition between two projects built by different teams. UX guidelines — A short while ago we created our first JUST EAT guidelines (still a work in progress). Working across 15 countries on different features, and in separate teams, makes it hard to be consistent. These guidelines are helping us achieve that consistency — leading to a unified product. All work and all play JUST EAT has a reputation for being an fun and exciting place to work. Having a proper work-life balance is what helps us create a healthy environment, so finishing work at 4.30pm on Fridays, playing ping-pong, FIFA, or fussball are just regular activities here. And apart from the odd free takeaway, everyone brings cookies when they come back from holiday. I hope this article gave you a bit of an understanding on what happens under the design curtain here at JUST EAT. And just so you know — we’re still on the look for great UX designers .", "date": "2016-01-07"},
{"website": "JustTakeAway", "title": "Hack your commit history for fun and profit", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2016/01/06/hack-your-commit-history-for-fun-and-profit/", "abstract": "We hold quarterly hackathons at JUST EAT tech, and for the last hackathon of 2015 my team (Ahmed Malik and Raffi Tamizian ) and I decided to take a leaf out of Adam Tornhill’s book to see if we could gain some insights into the way we work by analysing our commit histories. At the same time, we wanted to combine other analysis to get a better understanding of the quality of our codebase and how they have changed over time. Oh, and it was also a chance for us to flex our F# muscles. Getting started First, we need to be able to talk to Github easily and thankfully Octokit makes this relatively painless. So let’s go ahead and create a function to instantiate a new GitHubClient… Next, let’s add a couple of functions to download all the commits for a given repo… A couple of things to note from the above: Both functions use F#’s async workflow where any value bound to let! or return! is performed asynchronously . It’s similar to C#’s async-await feature, whose design was heavily influenced by F#’s async workflow , but there are some notable differences and a few gotchas in C#’s async-await ). client.Repository.Commits.GetAll(..) maps to the ‘ List commits in a repository ‘ action on the Github API, which doesn’t include some crucial information for our analysis – such as the files changed. Which is why for each commit we also need to call the commit function to fetch all the details for that commit. This is done asynchronously and in parallel. That client.Repository.Commits.GetAll(…) returns a collection of GitHubCommit instances, but Files and Stats properties are always null by design is an API design flaw – the return type does not accurately describe the contract of the operation and what expectations API consumer should have. Analysing commits Now that we have retrieved this high fidelity information about all the commits in a repo, running an analysis becomes a simple matter of writing a function that processes an array of GitHubCommit objects. For instance, here’s a function that analyses the commits and provides a high level summary of our repo… this function returns a Summary type as defined below… Visualising the results We can then use XPlot , FSharp.Charting , or even the R Type Provider to quickly plot the results to visualise them. For instance, we can get a break down on the contents of the repo by file type, or see the rate of changes in each commit… Code noise Moving beyond the basic analysis we just saw, we can do plenty of interesting things. For example, we were inspired by Simon Cousin’s work and interested to see… How much noise (null checks, braces, Etc.) we have in this codebase How each kind of noise has changed over time How the signal-to-noise ratio (% of useful code) has changed over time Fortunately, we were again able to borrow heavily from Simon’s work by making use of his open source code . Running the analysis against the head of the repo and visualising the results using XPlot , we can see that the Signal-to-Noise Ratio (SNR) of this repo is 57.8% at the moment. If we sample a few commits over the last 12 months and perform the same analysis, then we can see how they have changed over time (where the oldest commit is on the left). From this graph, we can deduce that some time between commit ref 3cfc7 and ee4ce there was a big refactoring which removed a lot of code. We can also see from the following graph that, over the last 12 months the SNR has made small but steady improvements. Temporal coupling By tracking files that are changed in each commit, we can identify files that are often changed together as a form of temporal coupling. These temporal couplings could be result of… Logical dependency between the files Copy-and-pasted code that needs to be updated in sync Or maybe something else entirely We can use the following function to analyse all the commits in a repo and return the top N pair of files that are changed together the most often. Ok, this is not the most trivial bit of F# code you’ll ever see, so let’s take a moment to explain what’s happening here. First, there are some things you need to know about F# here. We’re using F#’s pipe (|>) operator to chain a sequence of functions calls together, this works the same way as Unix pipes, or pipes in Powershell. The use of pipes is a very common idiom amongst F# programmers and allows you to make nested function calls easier to follow The Seq module allows you to create lazy collections, and is analogous to C#’s IEnumerable<T> Seq.map is analogous to LINQ’s Enumerable.Select Seq.collect is analogous to LINQ’s Enumerable.SelectMany Given the array of Github commits… We first map each commit into an array of source files (ie .cs and .fs files) that were changed in that commit Then for each array, we turn the files into pairs by mapping each file into an array of string-string tuples with other changed files. Eg for the array [| “A”; “B”; “C”; “D” |], we’ll generate the following arrays (one for each element): [| (“A”, “B”); (“A”, “C”); (“A”, “D”) |] [| (“B”, “C”); (“B”, “D”) |] [| (“C”, “D”) |] [| |] We’ll collect all the elements from these subarrays into one sequence of string-string tuples using Seq.collect (which takes a mapping function, so in this case we supplied it with the identity function , id ) By now, we have transformed the files for each commit into pairs of files that were changed together in that commit. Let’s collect all such pairs from all the commits into one unified sequence, which is what the outer Seq.collect does. Since some files are changed together often, we expect some of these tuples to be duplicated and the number of duplicates will tell us how often those files have changed together. To do that, let’s group the tuples by themselves. (But wait, ‘what if “A” appears before “B” in some commits but not others? Would you not miss some duplicates because of ordering of the pair?’ Great question, fortunately the GitHubCommit.Files list is already sorted, so “A” will always appear before “B” in this case). Now we can further massage the grouped data into the shape we want. Sort by count, and return only the top N results as an array. (notice in this last code snippet, we’re using pattern matching to extract the elements of the string-string tuple out as file1 and file2 . In C#, you’d have to write something along the lines of… __.Select((tuple, gr) => Tuple.Create(tuple.Item1, tuple.Item2, gr.Length())); I think it’s fair to say the F# version with pattern matching is much more expressive ) We can then take the results of this function and visualize them in a Sankey diagram. Some interesting things popped out, for instance, OrderService.cs and OrderServiceTests.cs are changed together an awful lot: So perhaps that’s an indication that the tests are too tightly coupled to the implementation ? The same theme is repeated a few times throughout this data set, eg with OrderTransmissionService.cs and OrderTransmissionServiceTests.cs When Order.cs is changed, it tends to be changed along with OrderService.cs , OrderServiceTests.cs and OrderContainerExtension.cs . This sounds like a case of logical dependency – OrderService processes Order , and OrderServiceTests uses Order to test OrderService , etc. Finally, we can also see that OrderProcessor.cs (the green bar in the middle) is an important component in this repo since it depends on/is depended upon by many other components. So if you’re new to this repo and want to quickly figure out how the various parts fit together, OrderProcessor.cs might be a good starting point. Hotspots We can find hotspots in the repo by ranking files by how often it is changed. We can also find hotspots by committer count, ie the number of programmers that has changed that file. This is useful to know because: The number of programmers that works on a file is an effective indicator of the likelihood of defects Many committers on the same file is an indication that file (or corresponding class) has too many responsibilities and therefore reason for many programmers to change it If you compare the two graphs side by side, then you can easily identify files that are changed often, and by many programmers. These are good candidates for refactoring. Commit patterns By analysing the timestamp of the commits, some patterns about the way people work start to surface. For instance, some developers like to commit at the start and end of the day. In general, the programmers working on this repo tend to commit more often in the morning. So perhaps we could introduce a quiet-hour policy (i.e. no meetings) between 9 and 11am so that they can better focus during the most productive part of their day? We can also see how often programmers commit by the day of the week. For instance, we might find that programmers commit more often from Monday to Wednesday because it’s safer to deploy changes. Unsurprisingly we found the same drop-off in commit frequency across all the top committers in this repo. Clearly nobody wants to deploy a buggy build on a Friday afternoon and then deal with the fallouts over the weekend. Once we have identified these patterns, we can investigate further and ask people if that’s how they prefer to work. If not, what can we do as an organisation to help shape their work pattern to better fit the way they ideally like to work? So despite not appearing very useful (though slightly amusing) at first, understanding your programmers’ commit patterns can help you ask interesting questions about your organisation and act as a catalyst for change. Knowledge owners Your analysis doesn’t have to be limited to just one repo. For example, you can find your top repos by commit count. And for each of the repos, click through to see who are the top committers. These top committers – especially those in green – are the ‘ knowledge owners ‘ of these components as they have the most knowledge about them. As you repeat the exercise across all of your repos, you can identify the key players in your organisation – people who are knowledge owners of many components. These key players are very important to your organisation, so treat them accordingly. At the same time, they also represent risk of potential knowledge loss if they were to leave, especially if they are singlehandedly responsible for most of the commits in many repos. So having identified the risk, you can be more proactive about knowledge sharing and start spreading knowledge and responsibility around the team more evenly. Single point of failures (SPOFs) are such an important topic in software architecture, and where you have software that is designed and written by people you also have dependency on people. Institutional SPOFs are just as damaging and its effects can be even longer lasting. Loss of knowledge is often cited as the reason to rewrite or abandon entire subsystems, and yet many might have been prevented had we identified the risks earlier. Hopefully, with the ability to easily identify institutional SPOFs we can mitigate them before more drastic actions are required after the fact. Integrate with other systems You’re not limited to Github commit history either. Within your organisation, you can also integrate with your CI solution, bug tracking system, or logging/monitoring systems too. For example, we use TeamCity for most of our automation tasks at JUST EAT. With thousands of build configurations, it can be overwhelming and sometimes hard to even find the right build configuration for the project you’re working on. So, what if you can integrate your analysis tool with TeamCity, and for each repo, report the build configurations that are connected to it? Wouldn’t that make life easier for you? Find brittle tests You can also detect builds that fail from time to time due to flakey tests – perhaps your tests have dependency on time and the performance characteristics of the build server is different from your local machine and throwing your timing off? Find buggy code You can also integrate with bug tracking systems such as JIRA and find code that are most buggy. If you correlate that with results from other analysis (eg hotspot, LOC, cyclomatic complexity, etc.) then you might find out other interesting things about your codebase. Or, maybe you can correlate error logs from your logging system in production (eg Elasticsearch is a popular choice and used heavily here at JUST EAT) back to particular commits/releases? Other ideas Besides the more serious ideas that we have mentioned, you can also do quirky things, for example… ‘Angry Coder’ – perform sentiment analysis on commit messages and find your most angry programmer. ( Evelina Gabasova has a nice example on how to do sentiment analysis using the Stanford NLP Parser library) ‘Bad code terminator’ – aggregate deletion stats by committer and see who deletes the most code over the history of this repo, or even across all your repos. ‘Hear/See your commits’ – generate random music/image from the stats/changes of your commits. The possibility is really endless, and I’m interested to hear what use cases you have in mind too (both serious and fun ones), so be sure to comment and let us know what you come up with!", "date": "2016-01-06"},
{"website": "JustTakeAway", "title": "In Memory DynamoDb", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/12/09/making-development-with-dynamodb-faster/", "abstract": "Problem Here at JUST EAT we use DynamoDb in a lot of our components. The DynamoDb API can be awkward and slow to work with at times and this has sometimes lead to a decision between having complicated tests or sacrificing coverage. Usually our integration tests are run against a QA AWS account that mirrors production. This gives us confidence that our changes are good to push live, but can be slow to iterate on when designing new features. To speed up our cycle time, we wanted a way to decouple our integration tests from AWS without losing confidence in our code. Solution Amazon provide a java app that can be used to deploy a local version of DynamoDb. We used this to develop a new feature, and it was the fastest we’ve ever worked with DynamoDb. We decided to wrap it up in a nuget package for use in other projects and when that succeeded just as well we decided to open source it. The nuget contains a wrapper for this jar file to start and stop it, and provides a DynamoDb client that is configured to use the local instance of dynamo. This eliminated the need to mock out the complicated DynamoDb api in our tests without sacrificing speed or coverage. It also removed a lot of lines from our tests. Caveats There are a couple of potential pitfalls with our approach. AWS have licensed the DynamoDb jar file in such a way that we couldn’t include it in the nuget. This means that as part of setup, you need to download the file and its dependencies from Amazon and include them in your solution. This has the downside that the version you are testing against has the potential to get out of sync with the version that is running in AWS. Our QA environments have a mirror of our production security groups, therefore any tests passing in QA give us confidence that our feature will work as intended in production. The local version of DynamoDb provided by Amazon has no concept of security groups, meaning that this aspect of our testing would no longer be covered. To ensure security groups continued to be tested adequately, we implemented a dependency check endpoint in our API that executes a read and write against dynamo and returns an http response giving success or failure information. This check is run post-deploy and has to pass before a deployment is considered successful. The project is available on nuget: nuget.org/packages/LocalDynamoDb and on github at: github.com/justeat/LocalDynamoDb Alan Nichols & Steve Brazier", "date": "2015-12-09"},
{"website": "JustTakeAway", "title": "The road to responsive web", "author": ["Written by Brendan Fuller"], "link": "https://tech.justeattakeaway.com/2015/12/07/the-road-to-responsive-web/", "abstract": "The benefits of moving JUST EAT International sites to a responsive web design are pretty clear. In a nutshell, responsive web design means that if a user switches between viewing your website on a Mobile device to viewing on a Desktop, the website should automatically adapt to cater for the different resolution.  This is all achieved through things like CSS media queries and fluid grid layouts . Responsive Web – a ‘fluid’ design means your site should adapt for different resolutions and devices. But, as well as the customer facing benefits, responsive web also provides efficiencies of scale from an engineering point of view – by moving to one common code base for all our international sites means we can develop features and enhancements more efficiently. That said, rolling out a completely new version of a JUST EAT website is bound to cause some sort of nervousness and its down to the International Product teams to manage this to ensure that the countries feel confident about the change and supported at every stage. As the Product Manager in the team that has rolled out responsive web in Denmark and is in the process of getting the Canadian responsive website live, I thought it was worthwhile to share the experiences I’ve had and the steps taken to make sure the roll out of responsive web was a success. Gap analysis The starting point is to identify the features or requirements for the country. Whether it be sales tax in Canada or integration with a specific delivery service in Denmark, the earlier you identify the unique requirements up front, the earlier you can consider them as part of your scope. Story development and prioritisation With our scope defined, we followed the principles of grooming the stories with the team and prioritising the stories based on what was the minimum required in order to get a beta version of the site ready and in front of staff to place real orders. Automating the testing as much as possible We created our acceptance criteria in the ‘given, when, then’ format, so the acceptance criteria could help form the basis of our Gherkin based feature files. Together these become our Executable Specification . We implement the feature files in .NET using SpecFlow and Selenium WebDriver to automate functional testing and identify regression issues more efficiently. Having experienced working in development teams where testing wasn’t automated, it can’t be underestimated how much time and effort can be lost without automation. Stakeholder engagement Once we’ve developed the responsive web version of the site that meets our MVP definition, we will demo the site to the country. These sessions are really positive as the countries can start to see the benefits that the new responsive website will give them. After the demo, we will then get the ‘green light’ to launch the site to a group of internal staff members, eg beta and running through the process of collecting feedback Establishing a phased roll-out approach Our approach typically takes four stages, beta, 10% of customers, then 50% and 100%. Beta, involves launching the responsive website to a set group of internal staff from the country team, where we set up a feedback form for the country to feedback issues they’ve faced, or general comments. Moving beyond beta through to 100% of customers, is largely based on how quickly we resolve defects identified during beta. Daily triage and defect prioritisation calls When it comes to the point of trying to move from 10% to 50% to 100%, I’ve found it particularly useful to run daily triage calls with the country to help establish a priortised view of the defects and, most importantly, an agreed accepted level of outstanding defects before going to 100%.  When you have this agreed target, it’s far easier to motivate everyone to reach that goal! Analytics Once we’ve gone live with beta, we verify that conversion events are being successfully recorded, but we are also make sure that we set up our reporting dashboards in advance so we can report back to the country about how the responsive website is performing versus the legacy website in each step of the funnel. Being proactive with this type of analysis also helps when pushing from 10% to 50% to 100%. Efficient defect resolution To ensure clarity and expected resolution for defects, I would discuss the detail of the issue with the country and replicate myself. Then, by detailing the current behaviour and expected behaviour of the issue, the dev team team were able to rattle through the prioritised defects and in the majority of cases resolve them first time. This quality output also helped to give the country team confidence that we would deliver against our promises and suggested ETAs. Based on our experience with Denmark, from the start of the development process through to going live to 100% of all customers has taken 6 to 8 weeks. When you sit back and think about it, 6 to 8 weeks to roll out a completely new version of a fully functioning website across desktop, mobile and tablet, that’s pretty cool huh?! #minifistpump", "date": "2015-12-07"},
{"website": "JustTakeAway", "title": "Tools for International Communication to Manage Technical Change", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/12/01/tools-for-international-communication-to-manage-technical-change/", "abstract": "In 2015 we began a project to migrate countries running the legacy JUST EAT frontend application to a new platform – a change that delivers enhanced features such as responsive design, backed by a component based microservices architecture, opening up new doors to innovation for myself and other Product Managers to explore. Migrating a country to the new platform gives users in that country a better JUST EAT experience and provides us with new tools to support the business of our restaurant partners. The flipside of such a change is that there is a new list of product enhancements for the team of software engineers I work with to deliver. Old development plans are thrown out the window, and feature requests are reconsidered based on changes in functionality and capability of the new platform. The initial migration is handled by our Platform Improvement team, whose core remit is to ensure feature and performance parity between the new and old platforms. By the time the site lands in my team post-migration, all urgent problems will have been addressed and the key performance indicators of the site will be at, or exceeding, pre-migration levels. It then becomes my responsibility to work with my Technology Manager and software engineers to prioritise and resolve any outstanding issues, and build a roadmap for the future with the country teams to take advantage of the new platform. Screenshots showing the improvement in the user interface delivered to JUST EAT customers through migration of a country from a legacy platform to a new responsive platform. Following the successful migration we assess the processes that are in place, and look to shift them from a short-term tactical solution to build a longer term strategic partnership between country and technology team. This involves taking an active part in the relationship towards the end of the migration and building a relationship with all key stakeholders to ensure a smooth handover. Effective visibility of progress Typically during high pressure activities such as platform migrations, quick and easy tools such as shared spreadsheets are used to manage the project.  These are familiar to people and simple to edit, allowing everyone involved to send updates and responses easily and quickly.  However, over time spreadsheets and any documents not dynamically linked to the source information get out of date and become a chore for everyone involved to keep in sync. This eventually leads to people working from different versions of the “truth”. To improve upon the use of shared documents,we replaced this with the use of a wiki page.  The JUST EAT wiki tool is Confluence, which is from the same suite as our development issue tracking tool, JIRA.  Based on saved filters in JIRA we pull in charts and tables with the information we need to discuss priorities and progress with country management.  Managing the filters in JIRA means that we can reuse them for different countries, and if an improvement is needed then this can be made across the board without having to update each country page individually. The wiki provides the country with a more user-friendly experience, showing the up-to-date status of platform improvements. This means that the country staff know the latest information about the progress of each ticket in one single place. No more need to copy and paste information to update spreadsheets, no more working from an old version of the truth and happier stakeholders due to increased visibility of platform improvements. Staying in touch During the migration, the team handling the work had daily calls with the country to work on defects and signing off changes. This is not a long term communication strategy and we have now transitioned to weekly calls (well, Google Hangouts actually) with an an agenda alternating between strategic and tactical each week. These have allowed us to keep on top of defect priority for the country and shift our focus on short term to longer term strategic development work. In addition to weekly calls we’ve flown out to each country, visiting the JUST EAT offices and building relationships with the country management teams. The value of face-to-face meetings cannot be underestimated, and even in today’s global instant-messaging world, nothing beats physically meeting someone in person in order to build a strong relationship. And, to maintain these good relationships we aim to visit each of our countries at least once a quarter, to check-in and carry out quarterly reviews and perform high-level planning – covering those topics that aren’t quite as easy while talking with slightly raised voices (as you’re never quite sure if they can hear you clearly) into a “Chromebox for meetings”. Looking to the future We’re never quite sure what the future will bring; we can try to predict what will happen and sometimes we’re right, sometimes wrong. But whatever it brings, it’s crucial for us in IPD to maintain strong relationships with our countries. We will continue to look for tools and process improvements to make international collaboration easier and to make our colleagues feel more involved in product development wherever they are. Ultimately, with all of this, our aim is to make sure that our colleagues around the world are kept well informed and most importantly confident that we’re supporting them and their business.", "date": "2015-12-01"},
{"website": "JustTakeAway", "title": "Offline UI testing on iOS with stubs", "author": ["Written by Alberto De Bortoli"], "link": "https://tech.justeattakeaway.com/2015/11/23/offline-ui-testing-on-ios-with-stubs/", "abstract": "Here at JUST EAT, while we have always used stubs in Unit Tests, we tested against production public APIs for our functional and UI Testing. This always caused us problems with APIs returning different data depending on external factors, such as time of day. We have recently adopted the UI testing framework that Apple introduced at the WWDC 2015 to run functional/automation tests on the iOS UK app and stubs for our APIs along with it. This has enabled us to solve the test failures caused by network requests gone wrong or returning unexpected results. Problem For out UI Testing we used to rely on KIF but we have never been completely satisfied, for reasons such as: The difficulty of reading KIF output because it was mixed in the app logs The cumbersome process of taking screenshots of the app upon a test failure General issues also reported by the community on the GitHub page We believe that Apple is providing developers with a full set of development tools and even though some of them are far from being reliable in their initial releases, we trust they will become more and more stable over time. Another pitfall for us is that our APIs return different values, based on the time of the day, because restaurants might be closed and/or their menu might change. As a consequence, the execution of automation tests against our public APIs was causing some tests not to pass. Proposed Solution Rethinking our functional tests from scratch allowed us to raise the bar and solve outstanding issues with a fresh mind. We realised we could use the same technology used in our Unit test to add support for offline testing in the automation tests, and therefore we designed around OHHTTPStubs to stub the API calls from the app. Doing this was not as trivial as it might seem at first. OHHTTPStubs works nicely when writing unit tests as stubs can be created and removed during the test, but when it comes to automation tests it simply doesn’t work. The tests and application run as different instances, meaning that there is no way to inject data directly from the test code. The solution here is to launch the application instance with some launch arguments for enabling a “testing mode” and therefore generating a different data flow. We pass parameters to the app either in the setup method (per test suite): override func setUp() {\n    super.setUp()\n    continueAfterFailure = false\n    let app = XCUIApplication()\n    app.launchArguments = [\"STUB_API_CALLS_stubsTemplate_addresses\", \"RUNNING_AUTOMATION_TESTS\"]\n    app.launch()\n} or per single test: func test_ApplePayAvailable_UserLoggedIn_ServiceTypeDelivery() {\n    let app = XCUIApplication()\n    app.launchArguments = [\"STUB_API_CALLS_stubsTemplate_addresses\", \"RUNNING_AUTOMATION_TESTS\"]\n    app.launch()\n    // test code\n} In our example we pass two parameters to signal to the app that the automation tests are running. The first parameter is used to stub a particular set of API calls (we’ll come back to the naming later) while the second one is particularly useful to fake the reachability check or the network layer to avoid any kind of outgoing connections. This helps to make sure that the app is fully stubbed, because if not, tests could break in the future due to missing connectivity on the CI machine, API issues or time sensitive events (restaurants are closed etc). We enable the global stubbing at the end of the application:didFinishLaunchingWithOptions: method: #ifndef APP_STORE_BUILD\n    [self _stubAPICallsIfNeeded];\n#endif\n//...\n- (void)_stubAPICallsIfNeeded\n{\n    // e.g. if 'STUB_API_CALLS_stubsTemplate_addresses' is received as argument\n    // we globally stub the app using the 'stubsTemplate_addresses.bundle'\n    NSString *stubPrefix = @\"STUB_API_CALLS_\";\n    NSString *bundleName = [[[[NSProcessInfo processInfo].arguments filterUsingBlock:^BOOL(NSString *arg) {\n        return [arg hasPrefix:stubPrefix];\n    }] firstObject] stringByReplacingOccurrencesOfString:stubPrefix withString:@\"\"];\n    if (bundleName)\n    {\n        [JEHTTPStubManager applyStubsInBundleWithName:bundleName];\n    }\n} The launch arguments are retrieved from the application thanks to the NSProcessInfo class. It should now be clearer why we used the STUB_API_CALLS_stubsTemplate_addresses argument: the suffix stubsTemplate_addresses is used to identify a special bundle folder in the app containing the necessary information to stub the API calls involved in the test. This way the Test Automation Engineers can prepare the bundle and drop it into the project without the hassle of writing code to stub the calls. In our design, each bundle folder contains a stubsRules.plist file with the relevant information to stub an API call with a given status code, HTTP method and, of course, the response body (provided in a file in the bundle). This is how the stubs rules are structured: At this point, there’s nothing more left than showing some code responsible for doing the hard work of stubbing. Here is the JEHTTPStubManager class previously mentioned in the AppDelegate. #import <Foundation/Foundation.h>\n@interface JEHTTPStubManager : NSObject\n+ (void)applyStubsInBundleWithName:(NSString *)bundleName;\n@end #import \"JEHTTPStubManager.h\"\n#import \"OHHTTPStubs+JEAdditions.h\"\nstatic NSString *je_mappingFilename = @\"stubsMapping\";\nstatic NSString const *je_matchingURL = @\"matching_url\";\nstatic NSString const *je_jsonFile = @\"json_file\";\nstatic NSString const *je_statusCode = @\"status_code\";\nstatic NSString const *je_httpMethod = @\"http_method\";\n@implementation JEHTTPStubManager\n+ (void)applyStubsInBundleWithName:(NSString *)bundleName\n{\n    NSParameterAssert(bundleName);\n    NSString *bundlePath = [[NSBundle mainBundle] pathForResource:bundleName ofType:@\"bundle\"];\n    NSBundle *bundle = [NSBundle bundleWithPath:bundlePath];\n    NSString *mappingFilePath = [bundle pathForResource:je_mappingFilename ofType:@\"plist\"];\n    NSArray *mapping = [NSArray arrayWithContentsOfFile:mappingFilePath];\n    [mapping enumerateObjectsUsingBlock:^(NSDictionary * _Nonnull stubInfo, NSUInteger idx, BOOL * _Nonnull stop) {\n        NSString *matchingURL = stubInfo[je_matchingURL];\n        NSString *jsonFile = stubInfo[je_jsonFile];\n        NSNumber *statusCode = stubInfo[je_statusCode];\n        NSString *httpMethod = stubInfo[je_httpMethod];\n        NSString *inlineResponse = stubInfo[je_inlineResponse];\n        id stub = [OHHTTPStubs stubURLThatMatchesPattern:matchingURL\n                                        withJSONFileName:jsonFile\n                                              statusCode:[statusCode integerValue]\n                                              HTTPMethod:httpMethod\n                                                  bundle:bundle];\n    }];\n}\n@end We created an utility category around OHHTTPStubs: #import \"OHHTTPStubs.h\"\n@interface OHHTTPStubs (JEAdditions)\n+ (id)stubURLThatMatchesPattern:(NSString *)regexPattern\n               withJSONFileName:(NSString *)jsonFileName\n                     statusCode:(NSInteger)statusCode\n                     HTTPMethod:(NSString *)HTTPMethod\n                         bundle:(NSBundle *)bundle;\n// ...\n@end #import \"OHHTTPStubs+JEAdditions.h\"\n@implementation OHHTTPStubs (JEAdditions)\n#pragma mark - Public\n+ (id)stubURLThatMatchesPattern:(NSString *)regexPattern\n               withJSONFileName:(NSString *)jsonFileName\n                     statusCode:(NSInteger)statusCode\n                     HTTPMethod:(NSString *)HTTPMethod\n                         bundle:(NSBundle *)bundle\n{\n    NSBundle *targetBundle = bundle ?: [NSBundle bundleForClass:[self class]];\n    NSString *path = [targetBundle pathForResource:jsonFileName ofType:@\"json\"];\n    NSString *responseString = [NSString stringWithContentsOfFile:path\n                                                         encoding:NSUTF8StringEncoding\n                                                            error:nil];\n    return [self _stubURLThatMatchesPattern:regexPattern withResponseString:responseString statusCode:statusCode HTTPMethod:HTTPMethod];\n}\n// ...\n#pragma mark - Private\n+ (id)_stubURLThatMatchesPattern:(NSString *)regexPattern\n              withResponseString:(NSString *)responseString\n                      statusCode:(NSInteger)statusCode\n                      HTTPMethod:(NSString *)HTTPMethod\n{\n    NSRegularExpression *regex = [NSRegularExpression regularExpressionWithPattern:regexPattern options:0 error:nil];\n    return [OHHTTPStubs stubRequestsPassingTest:^BOOL(NSURLRequest *request) {\n        if (HTTPMethod && ![request.HTTPMethod isEqualToString:HTTPMethod])\n        {\n            return NO;\n        }\n        NSString *requestURLString = [request.URL absoluteString];\n        if ([regex firstMatchInString:requestURLString options:kNilOptions range:NSMakeRange(0, [requestURLString length])]) {\n            return YES;\n        }\n        return NO;\n    } withStubResponse:^OHHTTPStubsResponse*(NSURLRequest *request) {\n        NSData *response = [responseString dataUsingEncoding:NSUTF8StringEncoding];\n        return [OHHTTPStubsResponse responseWithData:response\n                                          statusCode:(int)statusCode\n                                             headers:@{@\"Content-Type\":@\"application/json; charset=utf-8\"}];\n    }];\n}\n@end Having our automation tests running offline reduced the majority of red test reports we were seeing with our previous setup. For every non-trivial application, running all the test suites takes several minutes and the last thing you want to see is a red mark in C.I. due to a network request gone wrong. The combination of OHHTTPStubs and Apple’s test framework has enabled us to run the automation tests at anytime during the day and to completely remove the possibility of errors that arise as a result of network requests going wrong.", "date": "2015-11-23"},
{"website": "JustTakeAway", "title": "Dependency Injection on Android", "author": ["Written by Neil Davies"], "link": "https://tech.justeattakeaway.com/2015/10/26/dependency-injection-on-android/", "abstract": "Back when I started writing Android apps in 2009 things were a little different. Apps were a whole new world of software development and everything was evolving, no one took apps too seriously and they were just a bit of fun. Fast forward to the present day and the mobile app landscape has totally changed. Apps are now big business and are becoming cornerstones of companies strategies. At JUST EAT this is very much the case. We know our customers love using our apps and so we see it as really important that we create apps that are robust and give a fantastic user experience. So as you may be able to tell, we’re serious about our apps, and serious developers use serious software tools and techniques. One of those software techniques we use in JUST EAT is Dependency Injection and the associated frameworks that go along with it. It’s just a design pattern Dependency Injection (DI) is a design pattern which has been around for while, but recently it has become more commonly used in the development of Android applications, due mainly to the implementation of some rather nifty DI frameworks. DI allows developers to write code that has low coupling and which can therefore be easily tested. The more complex and longer lived your Android software the more important it becomes to be able to test it effectively. At JUST EAT we see DI as key to allowing our code to be configurable and therefore testable, so creating a codebase we can have confidence in.  Even though we have a fairly large and complicated codebase, we can make releases regularly and quickly because we have robust testing which is achieved in part by using DI.  With that in mind hopefully I’ve convinced you that DI is worth a look, but first let’s have a quick recap of what DI is. Basics of Dependency Injection When we write code we will often find that our classes will have dependencies on other classes. So class A might need to have a reference, or dependency to class B. To make things a little clearer let’s look at the case where we have a Car class that needs to use an Engine class. public class Car {\n    private Engine engine;\n    public Car() {\n        engine = new PetrolEngine();\n    }\n} This code works fine, but the downside is that the coupling between the Car and the Engine is high. The Car class creates the new Engine object itself and so it has to know exactly what Engine it needs, in this case a PetrolEngine. Maybe we could do a little better and reduce the coupling, so let’s look at a different way of creating this Car class. public class Car {\n    private Engine engine;\n    public Car(Engine engine) {\n        this.engine = engine;\n    }\n} Here we have passed the Engine into the Car via the car’s constructor method. This means that the coupling between the two objects is now lower. The car doesn’t need to know what concrete class the Engine is, it could be any type of Engine as long as it extends the original Engine class. In this example since we have passed, or injected, the dependency via the Car classes constructor we have performed a type of injection known as constructor injection, we can also perform injection via methods and with the use of DI frameworks directly into fields. So that’s really all there is to DI. At its most basic it is just passing dependencies into a class rather than instantiating them directly in the class. If DI is simple, why do we need Frameworks? Now that we understand what DI is, it’s quite straightforward to start using it in our code. We simply look at what dependencies are needed and pass them via a constructor or a method  call. This is fine for simple dependencies, but you’ll soon find that for more complex dependencies things can start getting a little messy. Let’s return to our example of a Car that has a dependency on an Engine. Now imagine that the engine also has it’s own set of dependencies. Let’s say it needs a crank shaft, pistons, block and head. If we follow DI principles we will pass these dependencies into the Engine class, that’s not so bad, we just need to create these objects first and pass them into the Engine object when we create that. Finally we pass the Engine to the Car. Next let’s make our example a little more complicated. If we imagine trying to create classes for each part of an engine we can see that we would soon end up with possibly hundreds of classes with a complicated tree (more accurately it is a graph) structure of dependencies. A simplified graph of dependencies for our example. Here the leaf dependencies have to be created first then passed to the objects that depends on them. All objects have to be created in the correct order. To create our dependencies we would then have to carefully create all our objects in the correct order, starting with the leaf node dependencies and passing those in turn to each of their parent dependencies and so on until we reach the top most or root dependency. Things are starting to get quite complicated, if we also used factories and builders to create our classes we can soon see that we have to start creating quite a lot of complicated code just to create and pass our dependencies, this type of code is commonly know as boilerplate code and generally it’s something we want to avoid writing and maintaining. From our example we can see that implementing DI on our own can lead to creating a lot of  boilerplate code and the more complex your dependencies the more boilerplate you will have to write. DI has been around for a while and so has this problem, so to solve it Frameworks for using DI have been create. These frameworks make it simple to configure dependencies and in some cases generate factory and builder classes for creating objects, making it very straightforward to create complex dependencies that are easily managed. Which DI framework should I use for Android? Since DI has been around for a while there are unsurprisingly quite a few DI frameworks that we can choose from. In the Java world we have Spring, Guice and more recently Dagger. So which framework should we use and why? Spring is a DI framework that’s been around for sometime. It’s aim was to solve the problem of declaring dependencies and instantiating objects. It’s approach was to use XML to do this. The downside to this was that the XML was almost as verbose as writing the code by hand and also validation of it was done at runtime. Spring introduced a number of problems while trying to solve the initial problems of using DI. In the history of Java DI frameworks, Guice was really the next evolution after Spring. Guice got rid of the XML configuration files and did all of its configuration in Java using annotations such as @Inject and @Provides. Things were starting to look a whole lot better, but there were still some problems. Debugging and tracking down errors with applications built using Guice could be somewhat difficult. Additionally it still did runtime validation of the dependency graphs and also made heavy use of reflection, both of which are fine for server side applications, but can be quite expensive for mobile applications that are launched much more often on devices with much lower performance. While Guice was a big step forward it really didn’t solve all the problems and its design was also not ideally suited for use on mobile devices. With that in mind a team of developers at a company called Square developed Dagger. Dagger takes its name from our tree structure of dependencies. Remember more accurately it is a graph of dependencies and in this case the graph is actually a Directed Acyclic Graph or DAG hence the name DAGger. Dagger’s aim was to address some of the concerns of using Guice and especially using Guice on mobile devices. Dagger took the approach of moving a lot of its workload to compile time rather than runtime and also tried to remove as much reflection from the process as possible, both of which really helped performance when running on mobile applications.  This was done at the slight expense of reducing the feature set offered by the likes of Guice, but for Android apps Dagger was still a step in the right direction. With Dagger we are nearly at a good solution for a DI framework that is suitable for mobile devices, but a team at Google decided that things could still be done a little better and so they created Dagger 2. Dagger 2 does even more of its work at compile time and also does a better job of removing reflection, finally it also generates code that is even easier to debug than the original version of Dagger. In my opinion there really isn’t a better solution for DI on Android, so if you’re going to use a DI framework, I believe that Dagger 2 really is the easiest to use and debug with, while also having the best performance. Getting started with DI on Android. So where do you go from here? Luckily Dagger and Dagger 2 already have a strong following so there are plenty of tutorials and presentation to help you get up to speed. The main Dagger 2 website can be found here and to get a good overview of Dagger 2 and it features there’s a great presentation by Jake Wharton that you can find here . It covers the basics and then goes on to discuss how Modules and Components work, while also covering the subject of scopes in Dagger 2. Finally to get you firmly on the road to using DI in Android here’s a list of handy tutorials: Good overview of Dagger 2: http://fernandocejas.com/2015/04/11/tasting-dagger-2-on-android What is Dagger 2 and how to use it: http://konmik.github.io/snorkeling-with-dagger-2.html Scopes in Dagger 2 : http://frogermcs.github.io/dependency-injection-with-dagger-2-custom-scopes/ Using Dagger 2 with Espresso and Mockito for testing http://blog.sqisland.com/2015/04/dagger-2-espresso-2-mockito.html", "date": "2015-10-26"},
{"website": "JustTakeAway", "title": "Tech talk: Towards a Docker and containerised future", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/10/14/tech-talk-towards-a-docker-and-containerised-future/", "abstract": "Last week, Ben Hall came and talked to us about how Docker can be used, even within a Windows-hosted platform. This was really interesting, and has opened up a few lines of experimentation; thanks Ben! Abstract Container based deployments are rapidly becoming the de-facto standard for system deployments ranging from small wordpress sites to how Google deployment their clusters. During this talk, Ben will discuss how you can architecture your applications for use with Docker and a container based deployment approach. Ben will introduce the current Container Patterns and approaches that are moving us towards a containerised future. At the end, developers, testers and system administrators will understand the issues associated with this new way of thinking, how production environments need to change to support containers and the advantages they bring for maintainability across multiple environments and clusters. Ben Hall Ben is the founder of Ocelot Uproar, a company focused on building products loved by users. Ben’s worked as systems administrator, tester, software developer and launched several companies. Still finds the time to publish a book and speak at conferences. Ben enjoys looking for the next challenges to solve, usually over an occasional beer. Ben recently launched Scrapbook ( joinscrapbook.com ), a hosted online environment for developers. Scrapbook helps break down the barriers to learning new technologies such as Docker & containers. Recording Tech talks at JUST EAT This is one of the reciprocal tech talks that we arrange at JUST EAT. See full details here .", "date": "2015-10-14"},
{"website": "JustTakeAway", "title": "Tech talk: TheTrainLine & AntiFragile Systems", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/10/12/tech-talk-thetrainline-antifragile-systems/", "abstract": "On 22nd September 2015, we were delighted to host Stefano Germani from TheTrainLine , who gave us a talk about building Anti Fragile systems. The abstract is: When building a software system the biggest problem we face is change. Requirements change, technologies change, traffic on our web application changes, everything changes and our software that was perfect just a moment ago, now is broken and no longer suitable for our purpose. The usual solution to this problem is to ignore it or, even worse, to resist changes. So we end up with a requirement change that requires the system to be completely rewritten or with a spike of traffic that will collapse it. Antifragile systems instead of fighting the change, embrace it. This is the story of the Trainline journey into the antifragile world. This was the third in a series of reciprocal tech talks that we’re organising with companies that also have interesting things to say about shipping software. We’re always on the lookout for more speakers!", "date": "2015-10-12"},
{"website": "JustTakeAway", "title": "Adaptive Testing @The Lead Dev", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/10/05/adaptive-testing-the-lead-dev/", "abstract": "The Conference Recently, my colleague Preet Sandhu and I gave a talk at The Lead Developer conference, held at the QE II Conference Centre on 11th September 2015. The whole day was absolutely amazing – with brilliant talks – and we were honoured to have a speaking slot. Our talk was titled “Adaptive Testing”. You can see the video and the slides that accompanied this talk. Overview of the Talk One of the issues that we have come across during our careers, is that everyone has a different opinion of what “perfect” looks like for a team that wants to incorporate testing into their process. None of those opinions are wrong necessarily, it’s just that testing can be arranged in so many different ways and one approach that works perfectly for one team, can completely fail for another. When we were preparing this talk, we sat down and had a think about all the different teams we’ve worked in and how we approached testing for those teams. We realised that for each team we were in, we displayed very different behaviours. For some teams we were hands on: creating test frameworks and writing automated tests; with other teams we were guiding the team towards testing solutions and best practices. We settled on the idea that we were adapting our testing approach to suit the team we were working with. We found that we adapted based on some different factors: What kind of people were in the team What stage of maturity was the team’s process at What skills we had at that point in time Summary We believe that Adaptive testers need to have a few different skills: We need to be able to communicate, persuade and negotiate. We need to be able to communicate effectively weather is something simple as pairing with developers on writing tests, it could be updating management on the status of a build, it could be convincing a team of people who have been burned by bad testing to give you a chance. We need to be able to lead without authority. If we need to implement lasting change in a team, then we need to win trust and confidence of the team, so that they listen to us and are receptive towards the changes we suggest and see the value we add. We need to be able to coach, mentor and enable. Whether it’s training people on how to write tests, helping teams to stand alone, working with individuals to help them see the value of tests. Technical understanding is very important . You need to understand the impact of changes made to a code base,you need to understand how the releases work for a given product, you need to know how environments work, and how components interact with each other. One of the key differences with testing as opposed to other disciplines is the fact that it’s highly likely you will be a minority within a team, and even if you are not that senior in your own field, you may still need to implement change in your team without always having the support or authority needed to make those changes. That’s why from the start of a testers career, it’s important to help them develop not just the ability to test, but also how to understand the holistic view of where a team is in terms of their journey towards testing. We need to avoid shortcutting our teams to their end-goal, and instead should take the time to help the team get there themselves in a sustainable way. Thanks Massive thanks to Ruth at White October Events , who did a great job of arranging the whole conference, to Meri Williams (@Geek_Manager) for chairing the conference and finally to Russ Miles (@russmiles) who made all of our nerves disappear by playing electric guitar on stage, giving his talk and THEN proposing to his partner on stage! 🙂", "date": "2015-10-05"},
{"website": "JustTakeAway", "title": "Process Managers", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/05/26/process-managers/", "abstract": "Background Domain Domain Driven Design(DDD) is an approach to building software that tries to tackle the complexity of the business by focusing on the core domain, building an evolvable model of the domain, and involving domain experts in the evolution of the model to develop and ensure a consistent understanding and language. You can think of a domain as a problem space. For example, allowing people to order food online from their local takeaway, and giving restaurants an online presence and the ability to process orders with no upfront investment in infrastructure is the problem space of JUST EAT. Subdomains A domain is broken down into smaller subdomains to simplify the understanding of the problem space. A few examples of subdomains in JUST EAT would be restaurants, ratings, order processing, payments, menus, business intelligence, finance, marketing, etc. Some of the domains are more important than others and some domains are there just to support others. Accounting and finance are supporting subdomains for example, but the order processing and customer communication are core subdomains. The subdomain that differentiates a business from competitors – or that makes most of the money for the company – is called the core domain. Usually the core domain gets the most investments in terms of development time, care and attention. Bounded context In an ideal world, once the subdomains and the core domain are identified, teams of developers and domain experts are formed around them to develop the model, the implementation and the language specific to each subdomain. The team, the model and the language they create to solve the domain problem belongs to the solution space, and they form what is known in DDD as a bounded context. It’s ideal for a bounded context to map exactly to a subdomain to avoid confusing language in the team. A team working on both the finance and restaurants subdomains would always have to qualify the term ‘account’ in their discussions, for example, because both subdomains are using the same term but they view it differently. In reality though, this is difficult to achieve and most of the time bounded contexts cut across multiple subdomains. Implementing Domain Driven Design Core domain It’s very rare that the core differentiator of a business comes from a single bounded context inside the business. More often than not the core domain comes from an integration of multiple parts of the business. Imagine you place an order for a product and the supplier doesn’t have it in stock. The supplier could just delay the order until the stock is replenished or simply reject the order. Neither of these outcomes are ideal for the customer. There is an opportunity for improvement though. The supplier could just upgrade the customer to a better spec of the product that is already in stock and tell the customer that if they still want the original product to send the upgrade back. This is way better than the previous scenario and it becomes the core domain as it keeps everyone happy. To be able to do this more than one part of the business needs to be involved to discover what the possible upgrades are available, what their prices are and how quick can be delivered. Dealing with time and thinking in loops A long-running process introduces complexity with regards to timing. Say, the order is placed but the payments context never confirmed that the actual monetary transaction happened. Without the payment confirmation the process can’t go further. An indefinite wait could be fine in some cases but most of the times there needs to be a definite answer as to whether the product can be delivered or not. So we need a way of notifying the process in the future to cancel the order, if the payment has not been confirmed in, say, two minutes. In order to deal with these timing issues, we need to introduce an alarm or timeout service that can delay-send messages. When an order is placed we tell the alarm service to send us a cancellation message in the future. So if no payment is made in the next couple of minutes we’ll receive the order cancellation message from the alarm. If however the payment was made and the order was advanced through the process, when the cancellation is received it is ignored right away. Thinking in time loops when designing a process is very powerful because if something unexpected happens, the process can enter a state where manual intervention is required so people can pick up the work and fix the unexpected problem. The ultimate goal of process automation is not to totally replace manual interventions but to automate as much work as possible. The order process manager In the next section I’m going to introduce the process manager pattern and give some code examples in C#. While there are many ways of implementing the process manager pattern I’ll keep it simple and try to minimize the dependencies on other frameworks or platforms. The process manager pattern How do we route a message through multiple processing steps when the required steps may not be known at design time and may not be sequential? Enterprise Integration Patterns The main purpose of a process manager is to encapsulate the process specific logic and maintain a central point of control. It’s initiated by a trigger message which could be an event coming out of a bounded context. The process manager decides what to execute next once a process is completed becoming a hub for start and finish types of messages. It can also become a performance bottleneck when parallel processes try to communicate back their state. Here’s the simplest implementation of process manager. The When methods will be called when something interesting happens. public class OrderProcessManager\n{\n    public enum OrderProcessState\n    {\n        NotStarted,\n        OrderPlaced,\n        PaymentCompleted,\n        OrderDispatched,\n        OrderDelivered\n    }\n    public OrderProcessManager()\n    {\n        State = OrderProcessState.NotStarted;\n    }\n    public Guid Id { get; private set; }\n    public OrderProcessState State { get; set; }\n    public void When(OrderPlaced @event)\n    {\n    }\n    public void When(PaymentCompleted @event)\n    {\n    }\n    public void When(OrderDispatched @event)\n    {\n    }\n    public void When(OrderDelivered @event)\n    {\n    }\n} The trigger message is the OrderPlaced event. public class OrderPlaced\n{\n    public Guid OrderId { get; set; }\n    public Guid RestaurantId { get; set; }\n    public Guid UserId { get; set; }\n    public List<OrderItem> Items { get; set; }\n    public decimal Price { get; set; }\n    public Address DeliveryAddress { get; set; }\n}\npublic class Address\n{\n    public string Street { get; set; }\n    public string PostalCode { get; set; }\n}\npublic class OrderItem\n{\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n} All this is plain old c# code which is great but there is some infrastructure missing here. How does the OrderPlaced event get delivered and who creates a new instance of the OrderProcessManager class? I’m not going to go into the infrastructure details here but you can use a Service Bus implementation to deliver the messages for you. In my example I’ve used JustSaying which is an open source library that runs on top of Amazon Web Services and delivers messages for me using Simple Queue Service and Simple Notification Service . State management One other responsibility of the process manager is to maintain state between the message loops. It does this by using some sort of persistent store like a SQL database or a NoSQL/Document database. All messages come to a message handler which in this case plays the role of a Process Manager factory and message router. Here is a simple implementation of the OrderPlaced handler. I used the name router as that is its main purpose, to find the right instance of the Process Manager and forward the message to it. public class OrderProcessRouter:\n        IHandler<OrderPlaced>\n{\n    private readonly IRepository<OrderProcessManager> _repository;\n    public OrderProcessRouter(IRepository<OrderProcessManager> repository)\n    {\n        _repository = repository;\n    }\n    public bool Handle(OrderPlaced message)\n    {\n        var pm = _repository.Load(message.OrderId);\n        if (pm == null)\n        {\n            pm = new OrderProcessManager();\n        }\n        pm.When(message);\n        _repository.Save(pm);\n        return true;\n    }\n} A few things are happening here. Firstly, correlation. Every process manager instance needs to be identified and re-loaded when a message is received. To be able to identify it, every message needs to share a correlation ID with the process manager instance. In this case I am using the OrderId as the correlation ID. Secondly, idempotence. Even though the OrderPlaced event is the trigger for a new instance of process manager, the router is first trying to find the process manager by ID. If the process for that order has already been started it means that the message we received is a duplicate message. In distributed messaging systems like AWS SQS you will occasionally get a duplicate message and you have to deal with it. Most distributed messaging system guarantee at-least-once delivery for messages. Thirdly, message forwarding/routing. The router simply hands over the message to the process manager here. Lastly, persistence. The newly created instance or the process manager is persisted into the repository. You could have a race condition here, where another router on another machine is persisting the same instance so you need to implement some versioning in the process manager and enforce concurrency checks in the repository. This is critical. Event processing Let’s have a look at what happens when the process manager receives the OrderPlaced event. public class OrderProcessManager\n{\n  public Guid Id { get; set; }\n  public OrderProcessState State { get; set; }\n  public int Version { get; set; }\n  public List<Command> CommandsToSend { get; private set; }\n  public Guid RestaurantId { get; set; }\n  public Guid UserId { get; set; }\n  public List<OrderItem> Items { get; private set; }\n  public Address DeliveryAddress { get; set; }\n  public decimal Amount { get; set; }\n  public void When(OrderPlaced @event)\n  {\n      switch (State)\n      {\n          case OrderProcessState.NotStarted:\n              State = OrderProcessState.OrderPlaced;\n              Id = @event.OrderId;\n              Items = @event.Items;\n              RestaurantId = @event.RestaurantId;\n              UserId = @event.UserId;\n              DeliveryAddress = @event.DeliveryAddress;\n              Amount = @event.Amount;\n              SendCommand(new ProcessPayment\n              {\n                  OrderId = @event.OrderId,\n                  RestaurantId = @event.RestaurantId,\n                  Amount = @event.Amount\n              });\n              break;\n          // idempotence - same message sent twice\n          case OrderProcessState.OrderPlaced:\n              break;\n          default:\n              throw new InvalidOperationException(\"Invalid state for this message\");\n      }\n  }\n} The process manager stores all the information and changes its internal state to OrderPlaced. It also ‘sends’ a command to payments to process the order passing a few important details about the order. The SendCommand function below is just adding the command to a list of commands to be sent so the name is misleading but I could not come up with a better one. When the process manager instance is saved the repository will use a bit of infrastructure to send the commands. I am not going to show that here though. private void SendCommand(Command command)\n{\n    CommandsToSend.Add(command);\n} At this point the ‘payment loop’ is started. In the payments bounded context many things will happen. For simplicity sake I didn’t involve an alarm service here but ideally you want the process manager instance to be notified if nothing happened with the payment for two minutes. One way to do it would be to add another command to be sent to the Alarm that contains the necessary details. Once the payment is completed successfully a PaymentCompleted event comes out of the Payments bounded context. The Process Manager now moves to the next step which is to dispatch the order to the restaurant for delivery. public void When(PaymentCompleted @event)\n{\n    switch (State)\n    {\n        case OrderProcessState.OrderPlaced:\n            State = OrderProcessState.PaymentCompleted;\n            SendCommand(new DispatchOrder\n            {\n                OrderId = Id,\n                RestaurantId = RestaurantId,\n                Items = Items.ToList(),\n                Amount = Amount,\n                DeliveryAddress = DeliveryAddress\n            });\n            break;\n        // idempotence - same message sent twice\n        case OrderProcessState.PaymentCompleted:\n            break;\n        default:\n            throw new InvalidOperationException(\"Invalid state for this message\");\n    }\n} Same pattern here. The state is changed and a new processing loop is started, the order dispatching loop. You can find more details about the implementation in this article here https://github.com/justeat/ProcessManager", "date": "2015-05-26"},
{"website": "JustTakeAway", "title": "Introducing MickeyDB", "author": ["Written by Ian Warwick"], "link": "https://tech.justeattakeaway.com/2015/04/13/introducing-mickeydb/", "abstract": "History MickeyDB is essentially a fork of an open-source project called Mechanoid. Mechanoid is a productivity tool which utilises several Domain Specific Languages (DSL) to automatically generate source-code for architectural layers covering Web API integration, Sqlite database access and more for our Android consumer application. The issue with Mechanoid is that it is an Eclipse plugin, and since the Android development community have migrated to Android Studio, Mechanoid became less feasible since it does not have the necessary language integration that it has with Eclipse, which means no syntax highlighting, no code completion and no grammar validation. We decided to create the MickeyDB project so we can focus our efforts on a single project with a specific purpose and provide the necessary support for the Android Studio IDE experience. MickeyDB In a nutshell, MickeyDB allows you to define your Android sqlite database as DDL statements in *.mickey files, and the Mickey code generator will generate a SqliteOpenHelper and ContentProvider implementation with lots of useful ways to access your data as we will learn in the following sections. Example Project This article will build on the example project on github https://github.com/justeat/mickeydb/tree/master/examples/android-studio The example project is a project you can import into Android Studio. I won’t go over the details here on the gradle build as the example project demonstrates that and will reserve the project and build setup for a later article. Initialize Mickey The first thing you need to do is create an android application class implementation like the ExampleApplication Class implementation and initialize Mickey. This makes sure that Mickey gets the application context at the earliest opportunity and will make coding with the useful generated Mickey code and Mickey library later. public class ExampleApplication extends Application {\n    @Override\n    public void onCreate() {\n        Mickey.initMickey(this);\n    }\n} Next we need a Mickey init file, takeaways.init.mickey which is effectively an entry point for the generator. The contents of the file at minimum needs to contain the databases fully qualified name (FQN) which defines the java package the generated code goes into. database com.justeat.example.db.TakeawaysDB Finally we need to declare the content provider that will be generated for this database in the AndroidManifest.xml . >provider\n            android:authorities=\"com.justeat.example.db.takeawaysdb\"\n            android:name=\".db.TakeawaysDBContentProvider\" /> Notice in the example above that our provider authority is the lowercased database FQN that we defined in the takeaways.init.mickey file. Migrations One of the main benefits of Mickey is database schema migrations. With mickey you must specify changes to your database in migrate blocks, as in the example file takeaways.001.mickey (and below). database com.justeat.example.db.TakeawaysDB\nmigrate initial {\n    create table takeaways (\n        _id integer primary key autoincrement,\n        name text\n    );\n} Breaking this down, we define the database FQN again at the top of the file. This is what ties together DDL migrations to a specific database. Then we define one migrate block which we give the name initial . The name can be anything, by uniquely naming migrations allows us to refer to them when migrating from one to another. Inside the migrate block we define one table takeaways with two columns id and name . If we wish to add more columns to the takeaways we can define them either in the same file, or in a new *.mickey file. The naming convention of files is arbitrary but by using numbers, we can sequence them in the file system so they list in order, for example, create a new file takeaways.002.mickey database com.justeat.example.db.TakeawaysDB\nmigrate takeaway_description from initial {\n    alter table takeaways add column description text;\n} Rebuilding the project will go through the code generation once more adding this new column to our generated code and creating a new version in the SqliteOpenHelper with the necessary code to migrate from one version to another. Our new database version will become version 2 since the sum of migrations we have so far is 2. It’s important to specify what we are migrating from, which is why we define migrate takeaway_description from initial . We are saying that the takeaway description migration is applied after initial migration. We can define as many migrations as we like to shape our database across releases using migrations, we can add migrations to the same file, or across different files for easier management. Working with data Mickey generates implementations of ContentProvider, SqliteOpenHelper and Contract class for your database. The names of these generated classes are derived from the database name. For our takeaway database example this will be TakeawaysDBContentProvider, TakeawaysDBSqliteOpenHelper, TakeawaysDBContract. We can use standard content provider queries to access and change our data, however Mickey provides several generated classes that make this easier. Record Builders The first useful generated code are record builders. For each generated table and view, in the case of our takeaways table, we can insert records as follows… Takeaways.newBuilder()\n    .setName(\"Awesome Pizza\")\n    .setDescription(\"Best pizza place in london\")\n    .insert(); Updating data is just as easy. Takeaways.newBuilder()\n    .setDescription(\"Best pizza place in the world\")\n    .update(123); // unique id of this restaurant (_id) We can also provide a query object (more on querying later) to specify what to update. Takeaways.newBuilder()\n    .setDescription(\"Best pizza place in the world\")\n    .update(Mickey.query().eq(Takaways.NAME, \"Awesome Pizza\")); // unique id of this restaurant (_id) In the example we say to update takeaways where the takeaway’s name is equal to Awesome Pizza , the constant Takeaways.NAME is generated. Active Record Sometimes working with builders is not desirable. For each Mickey table and view, a corresponding active record class is generated to make it easier to work with record. To insert data using the generated active record we can do the following… TakeawaysRecord record = new TakeawaysRecord();\nrecord.setName(\"Awesome Pizza\");\nrecord.save(); Similarly we can fetch and update record. TakeawaysRecord record = new TakeawaysRecord.get(123);\nrecord.setName(\"Awesome Pizza\");\nrecord.save(); Query data Mickey provides a useful Query fluent interface. A simple query to fetch a cursor would look like this… Cursor cursor = Mickey.query()\n        .eq(Takeaways.NAME, \"Awesome Pizza\")\n        .select(Takeaways.CONTENT_URI,\n        new String[] {\n            Takeaways._ID,\n            Takeaways.NAME,\n            Takeaways.DESCRIPTION\n        }); The Query API provides a convenient way to build queries which would otherwise have to be constructed using the content provider manually with string builders. Querying using the fluent query API supports most expressions such as ==, !=, >, < and more. The methods for each expression follow the naming eq, neq, gt, lt and so on. After our expressions we can use of the many terminators such as select, delete, selectFirst, firstInt, firstString and so on. In the example above we also provide the projection as the last parameter to specify which columns we want back. new String[] {\n    Takeaways._ID,\n    Takeaways.NAME,\n    Takeaways.DESCRIPTION\n} Those familiar with the content provider API will recognise this. The following example selects Active Records using a like query. List records = Mickey.query()\n    .like(Takeaways.NAME, \"%Awesome%\")\n    .select(Takeaways.CONTENT_URI); In this case we do not need to specify a projection since the record already has a projection of all the columns in the associated takeaways table. Also the Query interface provides asynchronous database queries as follows. Mickey.query()\n    .like(Takeaways.NAME, \"%Awesome%\")\n    .selectAsync(new AsyncQueryCallback() {\n        @Override\n        public void onQueryComplete(Cursor cursor) {\n            // TODO: do something with cursor\n        }\n    },\n    Takeaways.CONTENT_URI,\n    new String[] {\n        Takeaways._ID,\n        Takeaways.NAME,\n        Takeaways.DESCRIPTION\n    }); When the query is complete the onQueryComplete callback is invoked with the loaded cursor data. At this point we can do what we want with the cursor. Conclusion MickeyDB is an early preview of a new open-source project we are using at JUST EAT to manage our database. It is effectively a rewrite of Mechanoid db, so those familiar with Mechanoid db should understand the concepts presented in this article. We are waiting for xtext Android Studio support which should be available late Q2 of 2015 which will allow us to add syntax highlighting, code completion, cross-referencing support and grammar validation. We welcome contributions to this project. We aim to provide a consistent way to perform android database migrations, and a rich generated API to access your database. MickeyDB can be found on our github repository here https://github.com/justeat/mickeydb .", "date": "2015-04-13"},
{"website": "JustTakeAway", "title": "Decouple your UI logic from backend logic using AngularJS & Web API", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/04/07/decouple-your-ui-logic-from-backend-logic-using-angularjs-web-api/", "abstract": "If we step back in history about five years we’d realise that the best web technologies available for APS.NET developers were Web Forms and MVC frameworks. Those technologies have offered a great deal of rapid development and better user experience to consumers. Therefore, many businesses have adopted them to build their business tools and web applications. But to cope with the market demand for supporting web users on various devices such as desktops, mobiles, tablets etc… the web technology has to adapt to those demands and it has to be flexible enough to accommodate the change quickly when needed. In technology we have learnt that system decoupling is the best mechanism to allow change and apply system maintenance without having to rewrite the entire software. As a web developer, it was a dream for me to completely decouple the UI logic from the backend using a structural language. So if I need to use a different javascript framework for UI or change my backend framework or even the programming language for my current application, I have the flexibility to do so without having to rewrite the entire application. I struggled to achieve this with ASP.NET web forms or MVC but when I came across AngularJS it was like a dream come true. In this article I will show you how you can simply build pure “HTML” pages that use AngularJS for data binding and Restangular to consume data served by a RESTful ASP.NET Web API . The AngularJS & Web API Implementation We are going to build a single page application that can add, edit, read and delete user details. Create MVC5 project with a Web API controller called “user” that handles CRUD operations for user details. The code for this controller is like this… namespace UserManagement.Site.Controllers\n{\n    [RoutePrefix(\"api/users\")]\n    public class UserController : ApiController\n    {\n        // GET: /api/users\n        [HttpGet]\n        [Route(\"\")]\n        public IHttpActionResult Get()\n        {\n            var result = GetUsers();\n            return Ok(result);\n        }\n        // GET: /api/users/{id}\n        [HttpGet]\n        [Route(\"{id}\", Name = \"GetUserById\")]\n        public IHttpActionResult GetById(int id)\n        {\n            var result = new User();\n            return Ok(result);\n        }\n        // POST: /api/users\n        [HttpPost]\n        [Route(\"\")]\n        public IHttpActionResult Create(User model)\n        {\n            return CreatedAtRoute(\"GetUSerById\", new {id = model.Id}, model);\n        }\n        // PUT: /api/users/{id}\n        [HttpPut]\n        [Route(\"{id}\")]\n        public IHttpActionResult Update(string id, User model)\n        {\n            return Ok(model);\n        }\n        // DELETE: /api/users/{id}\n        [HttpDelete]\n        [Route(\"{id}\")]\n        public IHttpActionResult Delete(string id)\n        {\n            return Ok();\n        }\n        private List GetUsers()\n        {\n            return new List\n            {\n                new User { Id = 1 , DisplayName = \"Ahmed\"},\n                new User { Id = 2 , DisplayName = \"John\"},\n                new User { Id = 3 , DisplayName = \"Tom\"},\n                new User { Id = 4 , DisplayName = \"Mark\"},\n            };\n        }\n    }\n} At this point we can use Fiddler to execute a GET request to /api/users/ we will have a JSON result of the users list. Adding AngularJS In master page or like in my case Index.cshtml (as I’m using Razor only to bundle my scripts and css files) you will have to make two changes: Add script entries for AnguralJS, Restangular and other required scripts, as follows @Scripts.Render(\"~/Scripts/app\") In HTML tag add ng-app attribute which indicates to the AngularJS framework that this is an AngularJS application Using AngularJS In the main content part of the HTML, add data-ng-view attribute (similar to in Razor) by this directive angular know that, this is the place where template will be injected. In my application route I have set my home template and controller as follows: $routeProvider.when('/',\n{\n   controller: 'HomeController',\n   templateUrl: '/scripts/app/user/home.html'\n}); Routing, application configs and modules registration are defined inside Scripts/app/app.js file. HomeController is the main controller that handles all the logic for the home template when the application loads, so lets have a look at it: angular.module(\"app.user\", ['app.user.factory'])\n    .controller(\"HomeController\", ['$scope', '$modal', '$location', 'GetUsersFactory', 'UserFactory',\n         function ($scope, $modal, $location, getUsersFactory, userFactory) {\n             $scope.users = [];\n             $scope.data = {\n                 showError: false\n             };\n             getUsersFactory.get().then(function (data) {\n                 $scope.users = data;\n             }, function (error) {\n                 if (error.status == 404) {\n                     return $location.path('/error/not-found-restaurant');\n                 }\n                 if (error.status != 401) {\n                     return $location.path('/error');\n                 }\n             });\n             $scope.deleteUser = function (user) {\n                 user.remove().then(function () {\n                         var index = $scope.users.indexOf(user);\n                         $scope.users.splice(index, 1);\n                     }, function (error) {\n                         $scope.data.showError = true;\n                     });\n             };\n             $scope.addUser = function () {\n                 $scope.data.showError = false;\n                 $modal.open({\n                     templateUrl: '/scripts/app/user/addUserModal.html',\n                     controller: \"addUserModalController\",\n                     resolve: {\n                         model: function () {\n                             return userFactory.create();\n                         },\n                         users: function () {\n                             return $scope.users;\n                         }\n                     }\n                 });\n             };\n             $scope.editUser = function (user) {\n                 $scope.data.showUsernameUniqueError = false;\n                 $scope.data.showError = false;\n                 var modalInstance = $modal.open({\n                     templateUrl: '/scripts/app/user/editUserModal.html',\n                     controller: \"editUserModalController\",\n                     size: \"small\",\n                     resolve: {\n                         model: function () {\n                             $scope.original = user;\n                             return user;\n                         },\n                         users: function () {\n                             return $scope.users;\n                         }\n                     }\n                 });\n                 modalInstance.result.then(function (userItem) {\n                     userItem.put().then(function (editedUser) {\n                         angular.copy(editedUser, $scope.original);\n                     }, function (error) {\n                         $scope.data.showError = true;\n                     });\n                 });\n             };\n         }]); The controller code is very easy and self explanatory, it’s just a function. The name of the function is the name of the controller that is used in the application route, the function takes a number of parameters. The first one is $scope. This is the object that is getting bound to the DOM for example if in this function I set $scope.foo = “bar”; in markup I can use this binding expression {{ foo }} so I will have the string “bar” in my span after binding. Another thing to note here is that we are injecting app.user.factory module into the controller module, this module holds the initialisation for Restangular object. “Restangular is an AngularJS service that simplifies common GET, POST, DELETE, and UPDATE requests with a minimum of client code. It’s a perfect fit for any WebApp that consumes data from a RESTful API.” So let’s look at app.user.factory in the following script: angular.module(\"app.user.factory\", [])\n .factory('UserFactory', ['Restangular',\n        function (restangular) {\n            var service = {\n                create: function () {\n                    var user = {\n                        id: null,\n                        displayName: \"\",\n                        mobileContact: \"\",\n                        post: function () {\n                            return restangular.all(\"users\").post(this);\n                        }\n                    };\n                    return user;\n                }\n            };\n            return service;\n        }])\n.factory('GetUsersFactory', ['Restangular',\n        function (restangular) {\n            var service = {\n                get: function () {\n                    return restangular.all('users').getList().then(function (data) {\n                        return data;\n                    });\n                }\n            };\n            return service;\n        }]); In the code above I’m defining my user resource and creating main Restangular object. There are three ways of creating a main Restangular object. The first one and most common one is by stating the main route of all requests. The second one is by stating the main route and object of all requests. // Only stating main route\nRestangular.all(users)\n// Stating main object\nRestangular.one('users', userData)\n// Gets a list of all of those accounts\nRestangular.several('users', userData1, userData2, userData3); In my case I’m using restangular.all(“users”).post(this) which is performing a post request on api/users endpoint to create the user. Similar to restangular.all(‘users’).getList() which is a get request on api/users returning all list of users. Running the application When running the application you will be presented with a list of users displayed in an HTML table using ngRepeat . When the application loads you will be able to perform basic CRUD operations on the user resource by calling the Web API endpoints defined in the HomeController. By doing this, we have no dependencies between UI and backend logics, no shared models nor UI controls. The user interface layer is purely HTML, javascript, css and backend layer is purely c#. Pretty neat – enjoy it! The full source code of the application is available on GitHub here: https://github.com/justeat/AngularJSWebAPI.Experiment.git", "date": "2015-04-07"},
{"website": "JustTakeAway", "title": "Tests that rely on Data", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/04/20/tests-that-rely-on-data/", "abstract": "With our day-to-day test automation, we try to avoid dependencies as much as possible. The majority of tests are executed on a set of data that exists in the database in the QA environment. However, there are situations when we need to add or edit data in order to carry out end to end testing. When we do this, we need to make sure we don’t influence existing data which is used by different tests. Cucumber with ActiveRecord Since we have a limited number of tests that are dependent on a unique set of data, we use a standalone ruby script to deal with the database without creating a model. We create a connection to the QA database using ActiveRecord. We use this connection to perform queries and validate data. Here is an example test scenario where we altered data values via ActiveRecord… Scenario: Previously ordered item is unavailable when reordering\n    Given I have placed an order including a new item\n    And I am on the home page\n    And one of the items is unavailable\n    When I select the option to reorder\n    Then I am taken to the menu page\n    And I should see a message that says some items are not available\n    And I should see the basket summary as:\n      | item         | Americana |\n      | price        | 10.00     |\n      | delivery_fee | 2.00      |\n      | total        | £12.00    | Here is how we make the DB connection with ActiveRecord in the helper file. Also it notifies us after the DB has connected successfully and vice-versa. require 'active_record'\nclass DbDataHelper\n def initialize\n   db_connect('#{tenant}', 'qa')\n  end\n def db_connect(tenant, environment)\n   dsn_string = \"Driver={SQL Server};Server=-server_name;Database=database_name;Uid=user_id;Pwd=password;\"\n   ActiveRecord::Base.establish_connection(:adapter => 'sqlserver', :dsn => dsn_string, :mode => 'odbc')\n   begin\n     ActiveRecord::Base.connection.active?\n     puts \"Info - Connected to the [#{DB_name}] Database\"\n   rescue => e\n     puts \"Error - Exception db connection : #{e.message}\"\n     raise \"Failed to connect to the [#{DB_name}] Database\"\n   end\n   ActiveRecord::Base.connection\n end\nend The scenario validates certain error messages if the product price has changed or a product has been removed from the menu when the user orders something they have ordered before. Since we need to delete the item or edit the item price for a particular menu, we had to add a new item to the DB every time we ran this test. This was to ensure we were not affecting existing data which is used by other tests. The following example describes creating a new product and adding it to certain menus. The first task is to a create the new item. We had to make sure every new item was created with a unique name. When the item is created, it is given a generated product ID, which we then needed to extract in order to add it to a menu. @new_item_name = generate_new_item_name\n  def generate_new_item_name\n    'Test Item' + generate_random_text(5)\n  end\n  def create_new_item(restaurant_id, menu_id)\n  @new_item_name = generate_new_item_name\n    add_item_into_product = \"\n        INSERT INTO dbo.Item\n                ( XX1,\n                  XX2,\n                  XX3\n                )\n        VALUES  ( 'YY1' , -- XX1- varchar(50)\n                  '#{@new_item_name}', -- XX2- varchar(50)\n                  YY3, -- XX3- int\n                                 )\" Initially we assign the SQL insertion query which needs to be executed into a variable. Afterwards, we pass the assigned variable along with the DB connection. In this scenario, we have used the execute method since we don’t expect anything to return. ActiveRecord::Base.connection.execute(add_item_into_product) Then we have used the select_value method to return a single value (i.e. product ID) from a record, in order to add the new item into the respective menu. get_the_product_id = \"SELECT ProductID FROM dbo.Item\n                WHERE ResturanteID = restaurant_id}AND Name = '#{@new_item_name}'\"\n product_id = ActiveRecord::Base.connection.select_value(get_the_product_id)\nadd_item_into_menucardproduct = \"INSERT INTO dbo.Mc_product\n            ( XX1,\n                  XX2,\n                  XX3\n                )\n        VALUES  ( 'YY1' , -- XX1- varchar(50)\n                  YY2, -- XX2- varchar(50)\n                  #{product_id}, -- XX3- int\n                                 )\"\n    ActiveRecord::Base.connection.execute(add_item_into_menucardproduct)\n    kill_connections\n    { product: product_id, item: @new_item_name }\n  end When updating a certain value related to a product (e.g. price of the product), we pass the update SQL query to the execute method, like we did in the insertion step. In each method we clear the DB connection which we created at the beginning, because we don’t want to leave the DB connection open. def kill_connections\n   ActiveRecord::Base.clear_active_connections!\n end The changes that we are doing within the test need to be reset at the end of test. Here’s an example where we delete the item from all the references it has… def delete_the_item(product_id, menu_id)\n    delete_item_from_menucard_product = \"DELETE FROM dbo.Mc_product\n    WHERE ProductID = #{product_id} AND MenuCardID = #{menu_id}\"\n    delete_item_from_product = \"DELETE FROM dbo.Item\n                WHERE ProductID = #{product_id} AND Name = '#{@new_item_name}'\"\n    ActiveRecord::Base.connection.execute(delete_item_from_menucard_product)\n    ActiveRecord::Base.connection.execute(delete_item_from_product)\n    kill_connections\n  end We add an exit tag for the tests that require data to be reset at the end of execution. This tag ensures that the item is deleted from the DB even if the test failed in the middle of execution. This is how we defined the exit tag in the env.rb file. After('@reset_product') do |scenario|\n  delete_item_from_menu '#{menu_id}' if scenario.failed?\nend DB changes via API Since the components of the JUST EAT site communicate with various APIs, any data updated in the DB needs an API refresh in order to be visible in the front-end. For example, when we are updating an existing item price via the DB, we need to clear the cache of the menu API in order to see the price change in the menu (in the front-end) during the test execution. def edit_item_price(menu_id)\n  access_db = DbDataHelper.new\n  access_db.edit_the_item_price(@product_id, menu_id)\n  clear_cacheing_menu_api menu_id\nend\ndef clear_cacheing_menu_api(menu_id)\n  request_url = URI.escape(\"#{ menu_api_url }/menu/#{ menu_id }\")\n  post(request_url, '{}')\nend Summary We can see how data intensive it can be to test an application. In this instance particularly, the tests are completely data dependent, and getting the data right at first is an absolute must. As an example we can see not only the data model approach but also the direct sql query execution fits the purpose of retrieving or altering data. Thanks for reading. ~ Deepthi", "date": "2015-04-20"},
{"website": "JustTakeAway", "title": "How to fix flaky tests", "author": ["Written by Amarpreet Sandhu"], "link": "https://tech.justeattakeaway.com/2015/03/30/how-to-fix-flaky-tests/", "abstract": "How to debug flaky tests After working in the Test Automation industry for several years, I have learned that it’s easy to write hundreds of tests, but it’s often difficult to maintain those tests and keep them useful. Flaky tests are one of the biggest hurdles in maintaining reliable automation frameworks. Often we hear testers complaining that tests are failing on CI but pass locally, or some tests fail intermittently. These statements result in teams losing interest in automation and testers losing faith in their own tests. So it’s really important to get into the root cause of those failing tests. Here are a couple of steps which help me to debug flaky tests: The first thing to keep in mind is that when a test fails, there is definitely something wrong – flakiness is not magic. Trust your tests and start debugging! It is easy to ignore failing tests How many times have you found a failing test and just blindly rerun it until it passes? It’s important that as good test automators, we don’t ignore these random failures, but instead quarantine and then systematically fix them. Similarly – how many times have you seen a failing test and automatically assumed the problem is in your test code? Don’t forget to first of all verify that there isn’t a bug in the application causing the failure. Run Locally One of the first things I do upon finding a test failing on my CI system is to run the failing test locally. If a test is failing on CI, but not locally, then that can indicate some differences between how the tests are being run. I usually start by checking that the build machine has the same environment configuration as my local machine (e.g. browser, device, os etc). If there is no difference in environment then I look for network issues. Sometimes a CI box takes longer to perform some actions than the local machine. In this case custom wait methods could be helpful. Isolate the failing tests Group the failing tests and try to find out a common theme. Is it a particular browser, device or functionality where the tests are failing? Often I find that there is a common theme between failing tests. Being an iOS automation engineer, I often find tests failing either in iOS 8.1 or 7.1 or failing only in iPads or iPhones irrespective of the operating system. This gives me a good base to start debugging. Understand the functionality When tests fail randomly, look for a pattern in the failures. Often I have found that tests fail around a certain functionality. For example, once I experienced that most of the time tests were failing around a particular functionality in my app, I had a chat with a developer and told them that this particular area of app does not look very stable. After doing some investigations we found that there were some issues in the app around that functionality which were causing the failures. The developer fixed the issues and tests became stable. So, next time when your tests are flaky, do check if it’s a particular functionality around which the tests are flaky. Run tests in combination Sometimes a test passes when it’s run individually but fails while run in combination with other tests. If that is the case, I check the test/tests which ran previously and run both together. Sometimes previous tests put the device or browser into a state where the next test cannot continue, and if you are not resetting the device or browser between tests, it can result in failures. For example, I had some tests which were using stubs and I was adding a stub in at the beginning of each test and deleting it at the end. I was not doing this before and after every single test because not all scenarios were using stubs. Initially the tests were fine, but after some days they started failing and in addition some other tests also started failing which were otherwise stable and running successfully when run individually. After some debugging I found that whenever a test with stub failed before reaching its end, the stub was never removed and all following tests ran with the stubbed data instead of real data. The solution we followed for this was to separate out the stubbed scenarios and add/delete stubs before/after each scenario so we do not end up in this hanging state. It’s not always tests Recently I found a test which was crashing the app only in iOS 8.2 but the tests were passing on iOS 7.1. It was quite easy to debug this problem after I looked at the stack trace and did not find anything which could be caused by the tests, so I paired up with a developer and they figured out that the reason for the crash was that the api we were using was not working for iOS 8.2. So next time when you see a flaky test do not just jump onto fixing the tests, they could be just doing their job of telling you that something is wrong in your app. Do not get overwhelmed by failing tests It is quite easy to get frustrated while trying to find the root cause of these random failing tests. Often it is not straightforward and you may not find anything, even after spending a good couple of hours, this is part of the joy of being a tester. It can help to think of a strategy. If different tests are failing on different devices, then stick to one device first, fix all the tests, and then move to the next one. Pair up with another tester or developer. I always find solutions quickly when I pair with someone, and it’s always good to have another pair of eyes. Set time limits. For example – if you do not find the issue within a certain time limit, then seek someone’s help or put things on hold for some time, freshen up, and then start again. The key thing here is not to get overwhelmed and give up. You wrote them in the first place, so you can definitely fix them! Thanks for reading. ~ Preet", "date": "2015-03-30"},
{"website": "JustTakeAway", "title": "Using Natural Language Processing to improve our service…", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/03/24/using-natural-language-processing-to-improve-our-service/", "abstract": "Our project We’d noticed that customers and restaurants sometimes text back to automated text messages we send to them. These texts were going unanswered, so we wanted to change that. We get a few hundred of these messages a month, so it could have been possible to have a human text back. However, that wouldn’t really have been a scalable approach. Using Natural Language Processing stood out as a solution as firstly, it’s something we could apply in other areas of the business and, secondly, it’s really interesting. We knew Stanford University were a front runner in the field of Natural Language Processing. They’ve got some really excellent free courses on the topic, as well as software libraries that we could use for the ‘brain’ of our application. From here we got started designing our application. Natural Language Processing in a nutshell Natural Language Processing refers to human-machine interaction and the ability of software to process and understand human language. The Stanford course mentioned above is highly recommended for those wanting a bit more of an in depth understanding. Here’s how it works… Word Selection Regular expressions are used to pick out dates, phone numbers and other patterned expressions. Tokens, which roughly translate to words, are extracted. In some instances, such as for the word pairing ‘New York’ a token might be a collection of words. Reducing complexity The dialogue is normalised, removing capitalisation and some apostrophes in order to reduce the complexity and deviation in the language input. Stemming might be applied. This is where we effectively chop off the end of a word to further normalise them. Something like ‘ponies’ would become ‘poni’, as would ‘pony’. The process of stemming is a fairly crude approach and so some language processing tools opt not to use it. Lemmatisation is used, which is the process of reduction verb inflections into a base term. For example, ‘am’, ‘are’, and ‘is’ become ‘be’. Dealing with ambiguity Where there’s a level of ambiguity, caused by a typo or a synonym, we can work out the probability of each word appearing between the two words either side of it using probability algorithms. Parsing sentences Sentences can be picked out using punctuation, as well as the probability of various words ending a sentence or new line spacing. A parse tree is used to break out the specific elements of a sentence such a nouns, verbs, adjectives and noun and verb phrases. From this we can understand the purpose of specific words in a sentence. For example, if someone says ‘Find me restaurants in London’ the language processor can parse the sentence, pick ‘London’ and ‘restaurants’ as nouns. It can understand the direction ‘find me’ and attribute that to a specific action. It might then pop up a maps application and locate restaurants for the user. Analysis of sentiment (the attitude of a speaker towards their topic), colloquialism, and ambiguous sentences such as ‘I’ve unplugged my mouse’ (computer or rodent?) can be more complicated for an application to interpret. Often, it must rely on context and the probability of the meaning of a given word. How we build our application We wrote a service to listen out for inbound text messages and a service to parse the messages and learn new words. We used the Stanford Natural Language Processor plugin within our application to parse the content of texts and drop them into a category based on the message content. We also built a micro-api to get details about the context of a texter, such as their name, their order, if they were a customer or a restaurant, and previous messages. We trained our application using a selection of 100 texts. We wrote some unit tests to assert that it correctly categorised each of the texts and had it run through the process until it got all of the right. It improved each time because the brain of the application was able to recognise words what were used in place of other words. We deployed the application into our production environment to build up a vocabulary by listening to live messages and telling us what it would say in response. We checked it now and then and told it when it had learnt words correctly and when it was wrong. What our robot friend learned It got pretty good at responding to messages about refunds, or questions about why the order could have been cancelled (because a lot of people text this kind of message) and so it had a large training set. It struggled with some of the more left-field messages. For example, because it was used to recognising the word ‘free’ in relation to requests for free meals, when a texter replied by asking us to message them via a free instant messenger app, it thought they wanted free food. It learnt that ‘shut’ and ‘offline’ meant ‘closed’ because it saw them used by restaurants in the same kinds of sentences. It was 25% sure that ‘take’ was like ‘refund’ because it saw those two words used in conjunction frequently. It thought that ‘oh’ meant that someone was satisfied with their level of service, because when it text people to say their order was on its way people often text back ‘oh’. They also text back saying just ‘oh’ when their order was cancelled. In the second example, it missed some of the sentiment of that kind of response.. It also learnt a lot of bad language. Ahem. What we learned Our training set for the application was quite small because we didn’t receive large amounts of messages, so the application had limited scope to learn. Given more time though we hope that it will build up a better idea of the language used in messages and what that language means. The application also became a bit of a pessimist. Currently we mostly text customers if there’s an issue with their order and often, when they text back, it’s to tell us they’re not too happy. The application started to associate words like ‘I’m’ with words like ‘unhappy’ and flagged them both as words that meant someone was upset. We could change the app’s configuration so that it doesn’t bother to learn words that are less than three letters. However, this point does highlight how context dependent the language it has learnt is. It would perhaps need to start from scratch if we were to respond to texts in a different context. It’s been a very interesting project and we’ve learned a lot on the way. Hopefully, with a bit more work, we’ll be able to have the app ready to respond to customer and restaurant texts in the near future. Even if it’s just those texts asking about refunds.", "date": "2015-03-24"},
{"website": "JustTakeAway", "title": "Testing Ratings & Reviews", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/03/17/testing-ratings-reviews/", "abstract": "Teams at Just Eat Here at JUST EAT, we are constantly making improvements to the user experience of our products in order to create the best environment for our consumers to love their takeaway experience. Recently, in the Web team, we have been spending time moving the functionality in our web app into a new responsive codebase. We are prioritising areas of the code to do this with, based on where we see business need. Currently we know that one of the things our consumers are looking for from us is high quality reviews of our restaurants, so we focused on the Ratings and Reviews functionality most recently. As well as moving the code, we had requirements for a redesign of the architecture and the user interface as well. In order to achieve this objective, we created a virtual team, with representatives from Web, Automation, UX, Consumer-order API team and Rating API team – I was the test automation engineer working on this team. Ratings & Reviews Requirements The task at hand was to improve our customers experience of the review functionality – with the hope of getting more users to leave reviews for restaurants. From the web point of view this included moving the review code into our new responsive codebase and applying a visual redesign. My role was to provide automated testing for the web changes. This change was important because it’s the primary way that a customer gets the chance to provide their honest feedback of the meal they have ordered. This is useful from the  customer point of view since when they are placing a new order it enables them to  get a feel for their meal and sometimes help to make certain adjustments beforehand (e.g. make it more spicy) based on the review comments. Here are some example scenarios from this requirement: Background:\n   * New rate my meal\n Scenario: Access rate my meal page from home page\n    Given I am a user who has ordered before\n    When I choose to rate my order from home page\n    Then I am taken to the rate_my_meal page\n Scenario: Review submission with comment\n   Given I am on the rate my meal page\n   And I rate my order as:\n     | quality  | 3 |\n     | service  | 4 |\n     | delivery | 4 |\n   And I enter my feedback\n   When I submit the review\n   Then I see non-editable version of the review page\n Scenario: Order paid by cash\n    Given I am a user who has placed an order with cash\n    When I choose to rate my order from home page\n    Then I am taken to the rate_my_meal page\n    And I see the order summary:\n      | Order Summary           | $order_number       |\n      | Americana               | 10.00               |\n      | Subtotal                | 10.00               |\n      | Delivery fee            | 2.00                |\n      | Total                   | £12.00              |\n      | Total paid by Cash      | £12.00              |\n      | Requested delivery time | $selected_time      | Team Processes Once the feature was broken down into a set of requirements, the Product Manager and Test Automation Engineer worked together to define the acceptance criteria as a set of feature files. I shared these feature files with the engineering team so that everyone was on the same page. In an agile environment, testing is not just an isolated activity and is performed in parallel with development. Having the tests ready is handy when it comes to rapid development, the developers unit test their code, but having functional tests ready by the time the developer commits means that less iterations are necessary. Testing The challenge in this task was that in order to access the rate my meal page – you needed an existing order that could be rated and reviewed. I made the test do some ground work to get to the stage where the user visited the rate my meal page. This was a prerequisite for all the tests that were checking the rating functionality. The following flow of events were needed for this prerequisite task. Place an order (Make sure user has placed an order in order to review the meal) Order is accepted and ready for rating (Make sure the order is in a state where the user can review it. The order status is mimicked to act as a real order via an SQL update) Generate the rating code for the given new order id (The QA environment that I used for testing does not generate the email that gets generated once an order is ready to be reviewed. Due to this reason, I had to programmatically generate a rating code from API function calls. I used the Net::HTTP library to make GET and POST requests for order and rating API function calls) First I had to find the new order id for the given legacy order id via the Order API call with a GET request. There I returned the new order id from the JSON response. def find_new_order_id (order_id)\nrequest_url = URI.escape(\"#{order_api_url}/order/#{order_id}\")\nreq = get(request_url)\nJSON.parse(req.body)['id']\nend As the next step I had to generate the rating code for the given new order id via the Rating API call. It is a POST request which generates the rating code for the given order. def generate_the_rating_code\n  new_order_id = find_new_order_id @order_number\n  user_id = find_the_user_id @username\n  @restaurant_id = find_the_restaurant_id @order_number\n  rating_api_body = {\n    'customercity' => 'Testing',\n    'CustomerName' => 'Test user',\n    'orderid'      => new_order_id,\n    'RestaurantId' => @restaurant_id,\n    'userid'       => user_id\n  }\n  request_url = URI.escape(\"#{rating_api_url}/rating\")\n  req = post(request_url, JSON.generate(rating_api_body))\n  @rating_code = JSON.parse(req.body)['RatingCode']\nend Make sure rating code is not empty (The idea behind the rating code is to provide the ability for the user to access rate my meal page without logging into the just-eat web site. By doing so there is a security validation that would check whether the customer will only be able to see the rate my meal page for the order that they genuinely placed once the rating code is generated. So I had to include an assertion step to verify rating code is not empty in the URL when it directs to rate my meal page from different sources (from email, from home page or from order overview) and user see 403 page if it is empty) Learnings Helping to identify issues related to the requirements at an early stage where the acceptance criteria is defined is very useful. Getting the early involvement from the perspective of a tester not only helps to validate the business logic, it also highlights any impact that this feature would have for the rest of the application. Doing the testing in parallel to the development, so that bugs are identified quickly can make the whole development process go quicker. If we start working on step definitions before we have the application, we can make guesses at a sensible code structure based on experience, and if it differs to the end result – that conversation can be really valuable. We can also prep the data scenarios required ahead of time, and understand if we need to do something extraordinary in order to be able to test the functionality. On several occasions, I have had the chance to pair with developers to discuss the rate my meal page code design and implementation. The conversations have helped us to identify some issues at an early stage from both the development and testing perspective and also that knowledge changed the course and direction of my automation tests and the application code in some situations. In some situations we agreed  on implementing UI elements where it helps to write automation tests easily. Thanks for reading! ~ Deepthi", "date": "2015-03-17"},
{"website": "JustTakeAway", "title": "Using Android Emulators on CI", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/02/26/using-android-emulators-on-ci/", "abstract": "Introduction In the JUST EAT Android team, we use a continuous integration system called TeamCity, which compiles and packages the app, installs it on our test devices, runs the tests on each one and then reports the result back to TeamCity. The team uses Git for version control of the code, and our build server is linked to activity on the repository and will automatically run jobs when certain events occur. The main problem I found myself solving with this setup was that the emulators would eventually crash if they were kept running. The build server’s functional tests job The team’s TeamCity build agent kicks off functional tests on a variety of devices each time there is a merge into the develop branch of the Android repository. We have a separate build job for each device to give us visibility of test successes/failures on a per-device basis. Some of the devices are real ones plugged into the build machine, while some are emulated using an Android emulator called Genymotion . We decided to test more on emulated devices than real ones due to problems with the physical devices losing wifi intermittently, running out of battery due to only being trickle-charged by the machine, and occasionally just losing connection to the machine (cables just add another point at which to fail!) Genymotion Emulator running VB underneath The first problem Unfortunately, all Android emulators are sometimes prone to crashing if left running for a while. However,  Genymotion is still viewed by the Android community (and us!) as the best emulator programs for Android, especially in terms of speed, so giving up Genymotion wouldn’t have been the correct solution here. The emulators were left constantly for days, reinstalling the app and running test suite after test suite, and would always inevitably crash and require some manual rebooting. I decided to find a way to launch each device every time a suite was due to run on it, and close it again when the tests were complete. Genymotion comes with its own shell as a separate program, which executes commands with the emulators including starting devices (but at first glance I couldn’t find a command to shut them down). You can start an emulator with the ‘player’ command: /Applications/Genymotion.app/Contents/MacOS/player --vm-name <VM name / VM id> I shut the emulator down with a ruby script just using the build machine’s process list. This means I can also kill the emulator task if it has frozen: player_grep = `ps aux | grep player.app/Contents/MacOS/player`\n`pkill player` if player_grep.lines.count > 1 (This last number is 1, not 0, because the act of searching with grep creates a new process, and that process contains the string I’m grepping for! Grepception.) Genymotion uses VirtualBox behind the scenes. When specifying the device parameter, you can either use the device’s name as displayed in Genymotion, or you can use its associated VirtualBox ID. I used the IDs because they would always be constant for the installation of that emulator, while one could easily change the title of the device in Genymotion’s main window at any time. So I needed to find out the Virtual Machine IDs of each of my Genymotion devices. I did this with VirtualBox’s own VboxManage executable, which is in the VirtualBox installation directory: /Applications/VirtualBox.app/Contents/MacOS/VBoxManage list vms Output: \"Google Nexus 5 - 5.0.0 - API 21 - 1080x1920\" {8703de0c-763d-41d3-9ec9-fac79f912a9a}\n\"Google Nexus 9 - 5.0.0 - API 21 - 2048x1536\" {935ec14b-7e22-4177-9386-1118e01231d8}\n\"Samsung Galaxy Note 3 - 4.4.4 - API 19 - 1080x1920\" {9303b388-246e-4eb4-b426-1414d3ac3a22}\n\"Samsung Galaxy S4 - 4.4.4 - API 19 - 1080x1920\" {a20cf63e-9b75-41f9-936f-7887b002de62}\n\"Samsung Galaxy S5 - 4.4.4 - API 19 - 1080x1920\" {9e7b825c-e70a-4fca-af77-fc744c066791} So now I can launch the Galaxy S4 emulator with one command: /Applications/Genymotion.app/Contents/MacOS/player –-vm-name a20cf63e-9b75-41f9-936f-7887b002de62 I can now execute the launching of each emulator as a build step inside their respective build jobs. The second problem The Android SDK has a program called ‘ Android Debug Bridge ‘, which is used for interaction between a system and a connected Android device. Each android device has its own serial number, which can be viewed by the command ‘adb devices’, with an optional ‘-l’ parameter also printing out extra useful information such as the model. Unfortunately, the device serials for all the emulators were dynamically-generated IP addresses and would be different every time an emulator was booted up. I haven’t found a way to set static device serials on emulators. I couldn’t set this in the VM settings either; you can alter the network configuration of a device, but not the serial ID of a device as it appears in adb. The output for ‘adb devices -l’ looks like this: List of devices attached\n3004526803f69100 device product:goldenve3gxx model:GT_I8200N device:goldenve3g\n192.168.56.101:5555 device product:vbox86p model:Galaxy_Note_3_4_4_4 device:vbox86p The number on the left is the serial and there are several bits of information on the right of the line. I collaborated with Beccy writing a script which runs after an emulator is launched. As it boots up, the script loops round once a second for 60 seconds, parsing the output from an ‘adb devices’ command. It reads each line of ‘adb devices -l’, splits up the chunks of information and maps them together in an array. Then the script takes a device_name parameter, sent in by TeamCity, and searches for this inside the array. If found, it returns the matching serial for that map. If not, it throws an error of ‘Device not found’. tries = 60\ndevice_found = false\nputs 'Searching for device...'\ntries.times do\n  # rubocop:disable Metrics/LineLength\n  adb_devices = `adb devices -l`.gsub('List of devices attached', '').strip.split(\"\\n\")\n  devices = {}\n  sleep 1\n  next unless adb_devices\n  adb_devices.each do |device_line|\n    device_serial = device_line.split[0].to_s\n    device_name = device_line.split('model:')[1].split(' ')[0].to_s\n    devices[device_name] = device_serial\n  end\n  device_name = ARGV[0]\n  if devices.include? device_name\n    device_found = true\n    file = File.open('device_serial.txt', 'w')\n    file << devices[device_name]\n    file.close\n  end\n  break if device_found\nend\nadb_devices = `adb devices -l`\nputs 'adb devices = ' + adb_devices unless device_found\nfail 'Device not found' unless device_found\nputs 'Device found' If the device was found, the script will have written the device serial to a file, which I can then read in a later build step and use to tell adb to launch the tests only on that device. You can specify a single device when using adb by using its serial a ‘-s’ parameter: adb -s 192.168.56.101:5555 <command> The 3rd problem Once a Genymotion emulator has opened, it appears in ‘adb devices’ while it is still booting up. This means the next build steps would fail to run the tests because the device wasn’t ready to receive commands like installing apps. I got round this by using ADB again. With it, you can access the device’s own shell and therefore get extra info from and send more commands to the device. I used the following useful command to check if the device had finished its boot cycle or not: adb -s <deviceID> shell getprop init.svc.bootanim This returns ‘running’ if the device is still booting and ‘stopped’ if it has booted. Now all I had to do was write a script that ran this command in a loop for up to 60 seconds and wait until the output of this shell command equalled ‘stopped’: for i in $(seq 1 60);\ndo\nboot_state=$(adb -s `cat device_serial.txt` shell getprop init.svc.bootanim)\nif [[ ! -z $(echo $boot_state | grep 'stopped') ]]; then\n    echo 'DEVICE BOOTED'; exit 0\nfi\nsleep 1\ndone\necho \"Device didn't boot within the time limit\"; exit 1 If the bootanim query returned ‘stopped’ within 60 seconds, the script would exit with success code 0, otherwise after the 60 seconds is up and the command hasn’t returned ‘stopped’, the script would exit with failure code 1 The 4th problem When you start a Genymotion emulator using the ‘player’ command, the terminal you executed the task in would be stuck running that command until the emulator was closed again. This was a problem for our each of our build jobs, which would run in one shell from start to finish. For this reason, I put the emulator launch command (the one that uses ‘player’) in a ‘.sh’ script for each device, and executed them in the specific job’s terminal with the ‘open’ command. This spawned a new terminal, freeing the main one up immediately. However, this meant that when the tests had run and the job had finished, this left a tower of dead terminals on the screen. You can change Terminal’s preferences to exit once a command is complete. But don’t worry, this only affects my terminals which are spawned with the ‘open’ command; it doesn’t exit the terminal every time you do something normally. Thanks for reading! =D -Andy Barnett, Test Automation Engineer", "date": "2015-02-26"},
{"website": "JustTakeAway", "title": "Pairing with Developers", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/03/06/pairing-with-developers/", "abstract": "Working on the same branch When I first started out in testing, I used to work in teams where we would work for months on features, and then have them held up by weeks of testing and bug finding. These were the bad old days. I don’t know if anyone else has noticed, but I’m suddenly in an awesome world, where I’m now playing catch up with teams of developers who are able to move forward so fast with new features, that it’s almost a blur to me! I’ve been looking for a way to keep the tests and code in sync with each other, and to ensure that we don’t end up doing releases to production with untested code. The best example I have of this is recently working with one of our developers – ladies and gents – may I introduce Adam (Right)! Adam is one of our amazing front end developers, and recently he embarked on some work to add a cookie banner onto one of our newly redesigned responsive pages. The brief was as follows: Branching The first thing we did was set up a branch that we could both work on. The reason being – we wanted the tests and the implementation to go in as one Pull Request into our repository. So we made our branch cwa_2354 (the Jira number!) – and I committed the feature file we had discussed beforehand into the branch: @serp @cookie_banner @selenium\nFeature: Cookie Banner\n  Scenario: Adding a cookie banner\n    Given I land directly on the search page\n    Then I can see the cookie banner\n    When I select the close option\n    Then the cookie banner is closed\n  Scenario: User has already seen banner\n    Given I land directly on the search page\n    And I land directly on the search page again\n    Then the cookie banner is closed My Guesses For the step definitions, I took some educated guesses as to how Adam would structure the HTML. I took a guess that the cookie banner would be developed with a class of ‘cookieBanner’, and that when the page first loaded, the node would exist, and that on selecting ‘Hide’ the node with that class would not be there. Adam started out by checking in his code to solve the above problem. He then took a look at my best guesses for the code, and unfortunately they were not 100% right :(. This was how I originally wrote the step definitions: Then(/^I can see the cookie banner$/) do\n  expect(page).to have_css('.cookieWarning')\nend\nWhen(/^I select the close option$/) do\n  find('.cookieWarning .hideButton').click\nend\nThen(/^the cookie banner is closed$/) do\n  expect(page).not_to have_css '.cookieWarning'\nend When Adam was ready we took a look at my guessed step definitions together, and Adam immediately saw that they wouldn’t work. He explained to me that the first time the search page was loaded, there would be a node with an id rather than a class of cookieBanner, and that when the ‘Hide’ button was selected, the node would still be present, but it would have a class of ‘hide’ applied to it. However, on subsequent page loads (i.e. once the user has seen the warning) the node would not be present at all. Adam and I had a conversation about this meaning we effectively had two different ‘hidden’ states: The case when the node is not present The case when the node is present and hidden We discussed whose code should deal with this – either change Adam’s code to only have one hidden state, or change my test code to deal with the complexity – we didn’t see a problem either way, so we changed the test code. The final step definitions ended up like this: Then(/^I can see the cookie banner$/) do\n  expect(page).to have_css('#cookieWarning div:not(.hide)', :visible => true)\nend\nWhen(/^I select the close option$/) do\n  @closed = true\n  find('#cookieWarning .hideButton').click\nend\nThen(/^the cookie banner is closed$/) do\n  if @closed\n    expect(page).to have_css('#cookieWarning div.hide', :visible => false)\n  else\n    expect(page).not_to have_css '#cookieWarning'\n  end\nend Our Pull Request Our joint pull request looked like this: Thanks for reading! ~ Beccy", "date": "2015-03-06"},
{"website": "JustTakeAway", "title": "Everyone hates slow native application tests and so do we", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/03/18/everyone-hates-slow-native-application-tests-and-so-do-we/", "abstract": "Parallelising tests on real devices with calabash-android When testing one of our products, we have to run our automation suite on multiple types of device, sometimes with added permutations when trialling components. We looked into using a third party cloud service with real devices, like Xamarin Test Cloud, but some of the devices weren’t available. Simulated devices weren’t an option as we are often testing push notifications. With the added advantages of being able to watch tests running and interact with them manually if necessary, we decided to look into strategies on site. Running tests on one device at a time would be very time consuming, slowing feedback loops and release cycles so we looked into a way to run our tests in parallel. This would speed things up, be more scalable and as a bonus, help to highlight any concurrency issues. The approach we chose was to set to run all of our tests on each device simultaneously using a rake multitask . multitask :run_parallel => @task_list This multitask would invoke other tasks that would run against each device. The information about the devices to run on would be passed in via an environment variable, which would be used to generate the subtasks and put into a task list that was passed into the multitask. ENV['USERS'] ||= 'User_1, User_2, User_3'\nusers = ENV[‘USERS’].split(',')\n@task_list = []\nusers.each do |user_name|\n  user_name.strip!\n  task_name = user_name.gsub(/\\s+/, \"\").to_sym\n  report_path = \"Results/\" + get_device_info(user_name)['device_id'] + '/features_report.html'\n  Cucumber::Rake::Task.new(task_name, 'All completed scenarios') do |t|\n    t.cucumber_opts = \"--tags @finished --format html --out=#{report_path}\"\n  end\n  @task_list << task_name.to_s\nend The next step was to make all of the variables in our code thread-safe to prevent conflicts. This meant replacing environment variables with thread variables. Thread.current['NAME_OF_THE_VARIABLE'] One place this was particularly inconvenient was when passing the id of the target device into the cucumber task, as using the standard method of passing in environment variables was no longer an option. Using a thread variable wouldn’t work either, as the cucumber task runs on a separate thread to the one where it was created. Our solution was to use the id from the report title for that thread and pull it out as part of the setup for each test run. The final step was to make our tests run against multiple user accounts, so that we could avoid clashes between all of the threads. We started by making our step definitions agnostic to the account it was using, and then created an account per device that was selected from a mapping yml file when initialising. If these kind of challenges interest you, we are hiring . Thanks for reading! Alan & Adrian", "date": "2015-03-18"},
{"website": "JustTakeAway", "title": "Automating Release Tests", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/02/23/automating-release-tests/", "abstract": "Removing pain from a process In the Consumer Web Apps team, before every production release goes out we run a set of “Release Tests” manually. We introduced this process because we found that when our responsive website was being released, while it looked great on desktop browsers, we had a lot of visual bugs on mobile devices. Release testing involves putting a delivery, collection, cash and card order through on an iPhone, iPad, Android Phone and Android Tablet. The process takes about 30 minutes, and is run as soon as our release candidate reaches our QA environment. We share the responsibility for this testing between every single team member, including product, developers, test automation engineers and UX. I have recently tried to utilise our existing functional test suite to automate this process to create a set of screenshots that can be viewed by the person on release testing duty, to try and remove some of the pain for this process. For context, the functional test suite is written using Ruby, Cucumber and Capybara. In order to extend this to be able to run on devices, I have been using BrowserStack. Set up The first step was to create a new feature file to run through all the pages in our app. I tagged the scenario with @ipad, which is used later in the environment setup code. I already have methods to do these actions, so I was able to reuse those with some modifications as some of them didn’t translate onto mobile devices too well. Feature: Visit pages on iOS devices\n  @ipad\n  Scenario: Load pages on iPad\n    * I am on the homepage\n    * I am on the search_page\n    * I am on the menu\n    * I add items to the basket\n    * I navigate to the basket page\n    * I navigate to the login page\n    * I navigate to the order details page\n    * I navigate to the time and note page\n    * I navigate to the payment options page\n    * I navigate to the order confirm page Connecting to BrowserStack I added the following code to our env.rb file: Capybara.register_driver :ipad do |app|\n  caps = Selenium::WebDriver::Remote::Capabilities.new\n  caps['browserstack.debug'] = 'true'\n  caps[:name] = 'JUST EAT Tests'\n  caps[:browserName] = 'iPad'\n  caps[:platform] = 'MAC'\n  caps[:device] = 'iPad 4th'\n  caps[:emulator] = 'true'\n  caps[:acceptSslCerts] = 'true'\n  start_browserstack(app, caps)\nend Before('@ipad') do\n  @browser = 'ipad'\n  Capybara.current_driver = :ipad\nend Making the test useful I hooked the test into TeamCity, and put a trigger on it to run on every deploy to our QA environment. This means that with every release candidate that gets built, this test will run and take screenshots across the four devices. It also published a test results html report, with the screenshots embedded. So now, the job for the person on Release Testing duty is just to go to TeamCity (where they’d go to look at the other test results) and access the html report, which looks something like this:", "date": "2015-02-23"},
{"website": "JustTakeAway", "title": "Entry Level Developers Wanted", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/12/22/entry-level-developers-wanted/", "abstract": "Now taking applications With the growing success of JUST EAT and the growth of our engineering team this year it seemed only fitting that the company looked at what it could offer to the wider tech community. So as well as our open source contributions,  JUST EAT started up various entry-level schemes earlier this year in order to discover talented individuals who had not yet gained the experience required for our more senior developer roles. An internship and three entry level positions were created, providing a stepping stone for developers entering the professional market. Due to the success of these roles we have decided to open up three more entry level roles in our Bristol office . While we are accepting applications as of today, we do not expect to have these roles start until the end of the current academic year – July 2015. For more information on these roles and how to apply please see the full job description here . Here are a couple of quotes to give a taste of the experiences of this year’s successful applicants… “I love working alongside programmers who love what they do and are passionate about tech.” – Charlie Mills (Junior Developer) Read Charlie’s story here. “My time at JUST EAT showed me how working with a shared code-base in a professional environment is totally different to working with a code-base shared between a group of students!” – Niklas Begley (Intern) Read Niklas’s story here.", "date": "2014-12-22"},
{"website": "JustTakeAway", "title": "Useful Automated Tests", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2015/01/06/useful-automated-tests/", "abstract": "How do we make automated tests useful to a team? I’ve found that as a Test Automation Engineer, it’s incredibly easy to write many, many automated functional tests for an application. Throughout my career, I myself have written thousands of different front end tests, each one being very robust when run in isolation, and (for the most part!) each one being well thought out and necessary. One of the things I’ve been working on over the past couple of years is what I can do to then make those tests as useful as possible for the teams I work with. For me, a useful automated test runs quickly, gives consistent output and tells you whether or not the application under test is suitable for release. When writing automated tests, I try to ensure that automation is a sustainable and embedded part of a team’s daily working practises. The tests must be visible and well understood by the team I’m working with. At any point in time, my team must be able to ask one question of the automated tests: Can we go live? And they should expect a timely and reliable response from those tests. Easy to navigate One of the things that I ask of people I’m working with is to run tests locally before checking in their changes. This usually means that a team member needs to do two things. Firstly, they must understand the impact of the changes they have made, in order to identify a suitable subset of the tests to be run. Secondly, they must be able to find and then run those tests easily and quickly. There’s a lot to be said for arranging your tests sensibly, in well structured directories, and to have sensible naming conventions, however this is a small part of solving this issue. It’s usually a good idea to have a wide range of people involved in defining the feature files from the beginning of your process. If developers, product managers and testers work together on defining the feature files then you have more chance that these people will describe the features using the same language. This makes it easier for people who are less familiar with the tests to find the ones they need to run for a particular change. In my current project, for example, we have this modal appearing as part of our user flow: Initially there were various names being bandied around for this modal. Some referred to it as the ‘Meal picker modal’, some called it the ‘Double deal light box’, and some of us just called it ‘That modal that appears, you know, when you select a complex item, you know, the thing with the red header’. Not ideal. Luckily, by working together to define this piece of functionality, we all now refer to it as the ‘Meal deal modal’, and our conversations about it (we do spend a worrying amount of time talking about it) are much more straight forward! I also like to keep the barrier to entry for my automation tests as low as possible. If someone on your team finds it difficult to locate, set up and then run the tests, they most likely are just not going to bother. And that means the tests are not going to be useful for them. Making the setup and running of the tests as easy as possible is going to make running the tests less of a pain, and it will give the people on your team one less reason to not bother with the functional tests. In the JUST EAT functional tests, we have a rake file, that looks something like this: task :install do\nsystem('bundle install --no-cache --binstubs --path vendor/bundle')\nend\ntask :test do\nsystem 'bundle exec cucumber --tags @mechanize --tags ~@manual --tags ~@not_local'\nend This gives the team members one command to set up the framework (which will only need to be run once) and then one command to run the tests. Easy! Zero tolerance to flaky tests I have a zero tolerance approach to flaky tests. If a test fails and there isn’t a bug in the application? Delete it. My reasoning for this is that flaky tests tend to result in people (including myself) losing faith in the tests – and that for me is the most damaging thing that can happen for a suite of automated tests. I’ve definitely written and owned test frameworks before that when a test failed, I instantly started looking into the test code to figure out what I’d done wrong, instead of checking the application. After a few hours of digging around in the test code, when I actually bothered to check the application and realised that it was a bug that was causing the test to fail, I was pleasantly surprised. If you are ever pleasantly surprised that your tests are failing for a real bug, then you may already have an issue with the usefulness of your tests. Fast execution time If your tests take hours to run, then you may also have issues with how useful these tests are. I like to use a measurement called the Developer Boredom Threshold (DBT) to measure the execution time of my test. This super accurate metric is a bit of unmath that I like to use to indicate how long a developer on a team will wait for tests to run before getting bored and deploying to production anyway. From in depth testing and years of research I have estimated the average DBT is around ten minutes, and for that reason, I try to not have tests that take longer to run than that. When I joined JUST EAT we had a suite of tests for the web app that would take around two hours to run, plus a further hour in which to ascertain if the failures were ‘genuine failures’ or ‘just another issue with the tests’. The first thing I did with this test suite was to quarantine all the failing tests and investigate them. I also cleaned up a lot of the scenarios, removing duplicates and invalid ones where necessary. I then introduced Mechanize, as a way to headlessly test some of the scenarios that didn’t require javascript – Mechanize tests can run very fast, and very reliably. I also invested in some additional AWS agents, and rearranged my tests into logical functional areas so that they could easily be run in parallel. This has been a long journey, involving lots of time from my team, but we are now in a much better place. The tests run robustly across 12 agents, taking no longer than seven minutes. This is what a good day for the web app looks like: High quality test code I used to be of the opinion that test code quality was completely useless and not necessary. However, about a year ago, I was curious about how good my ruby code was so I ran a tool called Rubocop over my code. Rubocop checks Ruby code for adherence to the Ruby community standards and guidelines, and can be found here: https://github.com/bbatsov/rubocop. I was pretty confident that my ruby was pretty amazing anyway, so I ran Rubocop. It came back with over 5,000 ways in which I was offending the ruby community! Since that day, I have always used Rubocop – and surprisingly, when you write Ruby in a more efficient and higher quality way, your tests run faster! In summary then, vast swathes of automated tests that randomly pass; are only known about by your QA team; and surprise you when they find a bug are not going to be useful to your team. The first step towards useful tests is to get one robust, easy to read and easy to run test and then get your team in the habit of looking at it before releasing, and running it before committing. If they can learn this behaviour, then introduce a second test and then a third. Before you know it, your automation will be catching bugs left, right and centre. #minifistpump", "date": "2015-01-06"},
{"website": "JustTakeAway", "title": "JUST EAT and region failure – presenting JustSaying v2.1", "author": ["Written by Brian Murphy"], "link": "https://tech.justeattakeaway.com/2015/01/21/just-eat-and-region-failure-presenting-justsaying-v2-1/", "abstract": "The JUST EAT platform JUST EAT is the world’s leading online takeaway ordering service; we operate a web portal that allows people to order takeaway from their local restaurants. That portal is hosted on our platform and that’s what we build here in the Technology team – the JUST EAT platform. We run on Amazon Web Services and make use of many of the AWS services, such as EC2, CloudFormation, S3, etc. What we’re building is a highly available, resilient system. We have lots of traffic from lots of hungry customers (particularly on weekends) and, sometimes, things break. Network connections fail, databases die, queries time out, APIs throw exceptions. Failures happen in all software systems, but the greater scale of cloud systems forces us to consider these issues in our designs early. Availability So, we’re building a highly available system and that means that we need to make sure that our building blocks, i.e. the AWS services that we use, are highly available. It’s useful to consider that there are two types of services in regions. Those where you are expected to do some work to make the service highly available. With EC2 , for example, you can choose to run instances in Auto Scaling Groups spanning multiple Availability Zones. Those where AWS have built greater availability into the service itself, but where you may be expected to do some extra work to be resilient to the failure of that service within a region. Some examples of these services are SNS , SQS or SES . At JUST EAT, for the services that we use, we’ve already worked to increase the resiliency of that first type of service and now we’re working through the latter types of services. We use SES: we can now cope with temporary outages there. With our platform becoming increasingly dependent on messaging, via SQS and SNS, it’s important for us that we look to increase the availability of those services next. Being fans and users of much open-source software, we decided that we’d document our development of this strategy in the open, here on our blog. Region failure (a.k.a. what happened to eu-west-1?) Amazon services exist inside geographical regions. You can see a matrix of where each service is offered from on this Amazon web page . When I, for example, create a queue with SQS, I choose to create that queue in a particular region. Now, as mentioned above, things break sometimes. Outages occur. It’s very rare for an AWS service to become unavailable for an entire region, but  it can and does happen. Netflix, for one, have blogged about some of their experiences with cloud outages . For a system like the JUST EAT platform, which has become heavily reliant on messaging via SQS and SNS such an outage could have a significant impact on our business. Broadly, if we had the ability to switch from using SQS/Ireland to SQS/Frankfurt, then we could minimize that impact. But how? Decisions, decisions… Our open source messaging component JustSaying is built on top of SQS and SNS and is how our teams do AWS messaging at JUST EAT. Teams are pretty independent internally, free to choose the best tools and solutions for their domain, but we do recommend using JustSaying if you’re doing messaging on AWS. Building region failover support into that component seems like the best way to build it into the platform. So, what’s the solution? What are our options? We spent some time researching and thinking about how we could build an AWS system resilient to these failures and came up with three possible architectures. Option 1: Active-Active ‘Publish messages to all regions and subscribe to messages from all regions.’ In the case of two regions, this solution results in subscribers receiving two copies of each message and therefore needing logic to ensure each message is only processed once. On the other hand, we wouldn’t need logic to detect a failure or switch the ‘current’ region. Option 2: Active-Passive Subscriber ‘Publish messages to all regions. Subscribe to messages from the primary region, switching to the secondary region when an outage is detected.’ Unlike the Active-Active option, this solution requires that we can detect the outage and to to respond by changing the subscriber to listen to messages from the secondary region. Also, during the failover, it’s possible that the subscriber could receive a particular message once from each region, necessitating the same deduplication logic as the previous solution. Option 3: Active-Passive Publisher ‘Subscribe to messages from all regions. Publish to the primary region, switching to the secondary region when an outage is detected.’ Again, this option requires us to be able to detect the outage. Unlike this previous option, however, when we detect a problem we would switch the publisher to direct messages to the secondary region. For our needs, we decided that option 3 above works best. Active-Active would require us to ‘de-dupe’ messages and Active-Passive Subscriber needs us to tell every subscriber to update and listen to the new region. Since we have more subscribers than publishers, that makes Active-Passive Publisher a better solution for us. The result The result? JustSaying 2.1 supports region failover for messaging using the Active/Passive Publisher model described above. You can grab the NuGet package from the usual place and check out the code on GitHub . The new functionality is straightforward to use; there’s been a non-breaking change to the fluent syntax such that you can, optionally, specify which additional region is used for failover: ...\nCreateMeABus\n    .InRegion(\"eu-west-1\")\n    .WithFailoverRegion(\"eu-central-1\")\n    .WithActiveRegion(() => \"eu-west-1\")\n... This syntax is telling JustSaying to subscribe to messages from both the eu-west-1 and eu-central-1 regions and to publish messages to eu-west-1; the lambda supplied to WithActiveRegion() lets JustSaying know which region to publish messages to.  For example, you might want something like this: WithActiveRegion(() => _activeRegionProvider.CurrentActiveRegion) In this case ‘_activeRegionProvider’ would be a component responsible for providing the currently active region – perhaps it could read from a config file, a key/value configuration store or an in-memory cache. What works best for you will depend on your environment/scale. That’s all there is to it; no changes are required to your messages or to your message handlers. If you want to change the active region, just update your preferred config file/store/cache. Live and kicking Our ambition was to make this functionality straightforward to use so that our teams could upgrade upgrade as painlessly as possible. From that point of view, we’ve been pretty successful: several teams are already using JustSaying 2.1 in production at JUST EAT. Over the next few weeks, we’ll continue this transition so that the platform as a whole becomes resilient to SNS/SQS outages. Interested in seeing how we go about proving a feature in our DevOps environment? Have a look of this previous post and read about performance, graphs and deployments.", "date": "2015-01-21"},
{"website": "JustTakeAway", "title": "JustSaying and Reliability", "author": ["Written by dan rowlands"], "link": "https://tech.justeattakeaway.com/2015/01/29/justsaying-and-reliability/", "abstract": "We’ve recently adopted a message-based architecture for a big chunk of our platform here at JUST EAT. In particular, we use one-way messaging to communicate between our Autonomous Components (ACs). One of the main promises of this style of architecture is ‘Reliability’. Imagine a scenario where a customer is ordering their takeaway on JUST EAT website, and as soon as we charge their credit card, we encounter a deadlock in the database which results in a fatal error. Whoops. So what happens to the customer’s dinner then? In this blog post, I’ll cover what measures we have in place to sort out scenarios like these. We use a custom-built, light-weight, and now open source message bus called JustSaying which uses Amazon Simple Queue Service ( SQS ) as transport and Amazon Simple Notification Service ( SNS ) as publishing service. Like all credible message buses, JustSaying promises reliable messaging. Here’s a definition of what I mean by reliable messaging… ‘Given an accurate registration of publishers and consumers, every published message is guaranteed to be delivered to all intended consumers.’ This is possible since we use SQS as transport which is a reliable transport . While SQS guarantees a reliable transport, you’re not protected against unreliability in your consumer’s logic and your application code. JustSaying ensures reliability against both transient errors eg database deadlocks and permanent errors eg. NullReferenceException due to missing data in consumers’ logic in different ways. If the nature of error is transient then the correct course of action is to retry the operation hoping the issue is resolved. Retry Policy JustSaying takes care of retrying your messages for you out of the box and by default. If your consumer throws an exception for any reason, the message will be redelivered to your consumer upto five times. Of course the number of time your messages will be retried is configurable at the time of registration of your consumers. CreateMeABus\n.InRegion(RegionEndpoint.EUWest1.SystemName)\n.WithSqsTopicSubscriber()\n.IntoQueue(\"CustomerOrders\")\n.ConfigureSubscriptionWith(\ncnf => cnf.RetryCountBeforeSendingToErrorQueue = 3)\n.WithMessageHandler(new OrderNotifier())\n.StartListening(); Error queue For those errors that are not transient eg. application bugs resulting in NullReferenceException , no amount of retries is going to solve the problem. Instead JustSaying moves the unhandled messages to an error queue (dead queue). Once the problem is resolved, you can move messages from error queue back into the incoming queue and they will be processed by your consumers. JustSaying uses the underlying Redrive Policy provided by SQS to implement error queues. Error queues are generated at the time of declaration of the consumer and the convention for their name is <queue_name>_error. Yo can move messages from error queue into your incoming queue from the command line using JustSaying powertool which is available on Nuget from http://www.nuget.org/packages/JustSaying.Tools/ cd RetryPolicy\\bin\\Debug\nJustSaying.Tools.exe move -from \"customerorders_error\" -to \"customerorders\" -count \"2\" If you decide not to have an error queue you can opt out explicitly while registering consumers. CreateMeABus\n.InRegion(RegionEndpoint.EUWest1.SystemName)\n.WithSqsTopicSubscriber()\n.IntoQueue(\"CustomerOrders\")\n.ConfigureSubscriptionWith(cnf =>\n{\n   cnf.VisibilityTimeoutSeconds = 0;\n   cnf.ErrorQueueOptOut = true;\n}) Downloads Source code demonstrating how to configure retry policy in JustSaying is available on GitHub here: https://github.com/payman81/JustSaying.Samples", "date": "2015-01-29"},
{"website": "JustTakeAway", "title": "OpenSourced: JustFakeIt", "author": ["Written by Jaimal Chohan"], "link": "https://tech.justeattakeaway.com/2015/01/28/opensourced-justfakeit/", "abstract": "Introducing JustFakeIt. A Http Mocking Library JustFakeIt is designed to help you write better tests by mocking away any 3rd party HTTP services your application might depend on.  JustFakeIt does this by acting as a HTTP server which is hosted within your unit test process and which you can specify mock endpoints and responses against. Because it’s hosted In Process, it’s really fast and takes very little effort to use. Why would I want to use this? We’re used to writing tests that mock HTTP clients, like the really basic test below which mocks a CustomerApiClient public void OrderService_WhenFetchingAValidOrder_CustomerDetailsAreReturned()\n{\n   OrderService orderService = new OrderService(GetMockFor<ICustomerApiClient>());\n   GetMockFor<ICustomerApiClient>().Setup(c => c.GetCustomerById(1))\n                                   .Returns(new Customer { Id = 1, Name = \"Jo Bloggs\" });\n   Order order = orderService.GetOrder(123);\n   GetMockFor<ICustomerApiClient>().Verify(c => c.GetCustomerById(1));\n   Assert.That(order.Customer.Id.Equals(1));\n} The problem with tests like these are that by mocking away the CustomerApiClient, we don’t actually test it.  What about the logic that goes on inside the client? Typically we might write a suite of integration tests against the CustomerApiClient, but integration tests take a fair amount of time and effort to write compared to a unit test, and we might not cover our core usage scenarios (integration tests against external dependencies are usually very isolated) JustFakeIt helps in this situation by allowing us to write a unit test that covers the whole of the API client. private FakeServer _fakeServer;\n[SetUp]\npublic void SetUp()\n{\n   _fakeServer = new FakeServer(8081);\n}\n[Test]\npublic void OrderService_WhenFetchingAValidOrder_CustomerDetailsAreReturned()\n{\n   _fakeServer.Expect.Get(\"customer/1\").Returns(new { Id = 1, Name = \"Joe Bloggs\"});\n   _fakeServer.Start();\n   orderService = new OrderService(new CustomerApiClient(\"http://localhost:8081\")); //concrete class with endpoint injected\n   Order order = orderService.GetOrder(123);\n   Assert.That(order.Customer.Id.Equals(1));\n} Tests like this are still expensive to write, however we do get 2 big benefits our tests covers the whole of the API client, including any logic and dependencies within the client itself we can easily adapt the test to run against the actual API, by injecting a real URL into the client, and thus reuse it as an integration test Teams at JUST EAT don’t write all of their tests using JustFakeIt, that’d be overkill! Most teams use JustFakeIt when writing bigger outside-in tests, or when writing SDKs for their own internal APIs, and that’s where we’ve JustFakeIt to have the biggest benefit. Downloads Source code is available on GitHub here: https://github.com/justeat/JustFakeIt NuGet packages available here: https://www.nuget.org/packages/justfakeit", "date": "2015-01-28"},
{"website": "JustTakeAway", "title": "Outsourcing troublesome tasks to an EC2 Worker", "author": ["Written by Anton Jefcoate"], "link": "https://tech.justeattakeaway.com/2014/11/26/outsourcing-troublesome-tasks-to-an-ec2-worker/", "abstract": "Setting the scene… Our team owns a collection of APIs which are responsible for managing restaurant information – data like their name, contact details, whether they’re currently open, etc. The clients of these APIs are mostly internal to JUST EAT. We’d recently seen that one of our write operations was behaving erratically when called in high load scenarios; causing high CPU usage on the EC2 instance hosting it, unacceptably long response times and generally being a bad citizen in our AWS ecosystem. A knock-on effect was other requests being received by the same instance were being queued with response times being negatively impacted. The operation looked like this: Original API architecture From profiling, it was obvious that this was a database-constrained task, so we first had a look at optimising the underlying SQL. Some simple changes allowed us to reduce the workload here; batching multiple updates into a single ‘UPDATE WHERE IN’, for example. This bought us a little bit of headroom, but didn’t fix the underlying problem, which is that the operation is SQL-heavy and can receive large amounts of traffic in a very short time under load. On top of this, we had a new requirement to send an SMS notification as part of the workflow (hey, Twilio !). No matter how much we tuned the SQL/data side of things, there was no way to add that into the mix and still keep the response times of the operation in the handfuls-of-milliseconds that we like. Ultimately, it was clear that this current state of things wasn’t going to meet our needs going forward. What to do with a troublesome, long-running request like this? Our solution A notable aspect of this particular API operation is that it doesn’t guarantee that the work is completed immediately; the consumers of the operation are happy that it completes within ‘a few seconds’. This gave us flexibility to process the work asynchronously, away from the API and to notify the consumer of our progress, but how? We decided to create a new component that would be solely responsible for processing these long-running tasks, a Worker. The API could outsource the task to the Worker, freeing it up to service other requests. We’re big fans of messaging and event architectures here at JUST EAT, so this sounded like a job for a lightweight command/message bus and, luckily, that’s exactly what we have in the form for our own open source solution JustSaying . Using JustSaying, the API publishes a command which describes the task for consumption by the Worker. We’ve made some changes to JustSaying so that it can publish this command directly to Amazon SQS with the Worker subscribing directly to that same queue. So, here’s what our API looks like now: New API architecture As you can see the API itself no longer does any processing. Instead, it has two simple (and fast-running ) calls: Add a command to a queue. Add an item to an Amazon DynamoDB table to record the state of that command (initially ‘Queued’). Thanks to JustSaying, publishing the command is as simple as: var command = new UpdateRestaurantStatusCommand\n{\n...\n}\ncommandBus.Publish(command); The response ( 202 – Accepted ) includes an ID and a resource URL for tracking the task state (‘Queued’, ‘Succeeded’, ‘Failed’, etc), using that DynamoDB table. At the other end of the queue, we have our new Worker component, responsible for processing those commands. We’ve implemented this as a Windows service hosted on an Amazon EC2 instance. Subscribing to the command, thanks to JustSaying, is as simple as: commandBus\n    .WithSqsPointToPointSubscriber()\n    .IntoQueue(\"UpdateRestaurantStatusCommand\")\n    .ConfigureSubscriptionWith(c =>\n    {\n    ...\n    })\n    .WithMessageHandler(_updateRestaurantStatusCommandHandler)\n    .StartListening(); And here’s what the Worker looks like internally: Worker architecture This may look slightly more complex than our original all-in-one API solution, but actually offers a host of additional benefits. The queue stores the commands until the Worker is free to process them; if the Service stops working no work is lost. We have automatic retries thanks to JustSaying . Unprocessed commands are added to a Dead Letter Queue (thanks again, JustSaying). We can scale the Service independent of the API. It now doesn’t matter how long the work itself takes as the consumer of the API gets a sub 20ms response. We can add extra functionality to the operation without impacting the API. Show Me the Graphs Being heavy users of monitoring at JUST EAT, we had plenty of historical data showing how the previous architecture was behaving under load, so we next ran some performance testing in one of our QA environments to see how the new system was behaving. We used JMeter for this, keeping an eye on our graphs. Immediately, we saw a dramatic decrease in the API response time, greater consistency in response times and a huge reduction in CPU load when sending very large numbers of requests. You can see below how consistently the API was now responding (the spikes you can see were due to DynamoDB calls taking longer than usual). API 202 response times For the Worker itself, we added monitoring hooks to give us some insight into how the service was behaving. You can see in the next graph how the components of the Worker are running as part of the overall ‘handle a command’ operation. As we suspected, most clock time is still being spent in that same section of SQL as before – we’ve moved this from the API to the Worker but it’s still ultimately running the same piece of SQL. Worker command processing times With this benchmarking completed, it was clear that the new architecture was a significant improvement, so we felt eager to deploy to production! Deployment We validated the new the system in production by deploying a new instance of the Worker and a single instance of the new version of the API, allowing us to compare the two systems side-by-side under identical load. Comparing the response times from the original API call (old average) to the new one (new average) , the response times for the API call are now around 20ms and no longer as erratic, just as we’d seen in our testing. Again, we do see an occasional spike as a result of the DynamoDB call, but increasing the write capacity will hopefully fix this. API 200 vs. 202 response times Similarly, the new Worker also behaves just as we expected from our testing. Success! Operations Since we believe in DevOps here, we know that to support our systems in production, we need comprehensive monitoring and alerting in place. Interestingly, what we found was that the monitoring that we’d set up as part of the performance testing we went through before deploying was pretty much exactly what we needed to monitor the health of the live system. How cool is that? This is why we love DevOps: because it means writing solid code with great instrumentation and monitoring up-front , reducing the chances of incidents in production (and being paged/woken up). In addition to the alerts around the response times of the API and the Worker, we have ones for the length of the command queue and the CPU utilisation of the Worker. Conclusion We have managed to decrease the response time for the operation and removed the bottleneck that this operation sometimes caused in IIS. We have moved the long running process to a service hosted on a different set of EC2 instances, which gives us greater control over scalability. By throttling how many messages each instance can process the queue gives us predictable performance, we can lower this amount if our instances are being strained (quicker than scaling more instances) or increase if we have a large backlog of commands we need to process. This approach will not work for all scenarios, such as when an API response has to return something in real time, but for cases where the consumer doesn’t need an immediate response there are plenty of benefits to moving the processing to an external service.", "date": "2014-11-26"},
{"website": "JustTakeAway", "title": "OpenSourced: OpenRasta Middleware", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/10/07/opensourced-openrasta-middleware/", "abstract": "OpenRasta meet OWIN We are at it again. Previously we brought you OpenRastaSwagger allow you to have shiny documentation for your APIs. Now we wanted to contribute something to the community that would get you excited again as an OpenRasta developer or someone looking for an alternative to the WebAPI world. Enter OpenRasta.Owin – a Middleware package for the OpenRasta framework allowing you to host your APIs in Katana on the OWIN Specification. Why have we created this? Today we’re working in the OpenRasta community which has served us well – after all, we’ve accomplished great things. But as always many minds are always better and, by joining forces with the OWIN Middleware community, we open a whole new world of options to our solution and yours. The Middleware community has exploded recently and will continue to do so with the introduction of vNext, which is hosted on Owin/Katana by default. nuget.org search: ‘Search for OWIN returned 478 packages’ ‘How do I move’ We hear you say. The move can be as easy as three steps… Remove your current OpenRasta hosting. Add OpenRasta.Owin hosting Middleware. Create the magic Startup.cs with the following code. The key feature that the OpenRasta.Owin package brings you is the UseOpenRasta Middleware extension. public void Configuration(IAppBuilder appBuilder)\n{\n   IConfigurationSource configSources = new OpenRastaConfig();\n   appBuilder.UseOpenRasta(configSources);\n} If your project is a little more complicated and you need to specify your own dependency resolver that’s also fine. You can specify the dependency as the second parameter. public void Configuration(IAppBuilder appBuilder)\n{\n   IConfigurationSource configSources = new OpenRastaConfig();\n   IDependencyResolverAccessor customDependencyResolver = new CustomDependencyResolver()\n   appBuilder.UseOpenRasta(configSources, customDependencyResolver);\n} I need more from this Package!! No one is perfect and, despite our best efforts, this package might not meet everyone’s expectations. But don’t worry – there is light at the end of the tunnel. The light is you…. Yes, you can contribute or even fork it and make it your own. Take a look at the licence on GitHub for peace of mind. Downloads Source code is available on GitHub here: https://github.com/justeat/openrasta-hosting-owin NuGet packages available here: https://www.nuget.org/packages/OpenRasta.Owin", "date": "2014-10-07"},
{"website": "JustTakeAway", "title": "OpenSourced: The ZendeskApiClient", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/09/19/opensourced-the-zendeskapiclient/", "abstract": "Introducing the JUSTEAT ZendeskApiClient A few months ago we started using Zendesk to manage our engagements with our customer and with our partners. Zendesk provides an API which is a really important tool for us as we need to be able to hook our internal systems into the data we have saved in Zendesk. We wanted a lightweight tool to enable us to RESTfully connect with the Zendesk Api with a minimal code. We wanted to only be returned data for the objects we were working with. We needed to make sure our solution was well tested. Finally, we wanted to be able to reuse the code within a number of our internal systems. The result was the ZendeskApiClient So how does this thing work? The ZendeskApiClient comes as a NuGet package. Install it, then create yourself a client: IZendeskClient client = new ZendeskClient(\n    new Uri(\"https://my-zendesk-api-host-endpoint\"),\n    \"my-zendesk-username\",\n    \"my-zendesk-token\"\n); Once you’ve got your client, you’ll have access to a number of resources: client.Tickets...\nclient.Organizations...\nclient.Users...\nclient.Groups... These resources can be called RESTfully like this: IResponse response = client.Tickets.Get((long)1234);\nIListResponse response = client.Tickets.GetAll(new List { 1234, 4321 });\nIResponse response = client.Tickets.Put(new TicketRequest{ Item = ticket });\nIResponse response = client.Tickets.Post(new TicketRequest { Item = ticket });\nclient.Tickets.Delete((long)1234)); Paging and filtering is also pretty useful when you’re working with organisations and tickets in Zendesk, so we added that into the client. IZendeskQuery query = new ZendeskQuery();\nquery.WithPaging(pageNumber:2, pageSize:10);\nquery.WithCustomFilter(field:\"name\", value:\"Coffee Express\");\nquery.WithOrdering(orderBy:OrderBy.created_at, order:Order.Relevance);\nIResponse response = client.Search.Find(query); We hate reading documentation as much as the next developer, so we went for a discoverable approach: IResponse response = client.Search.Find(\n    new ZendeskQuery()\n    .WithCustomFilter(\"email\", \"jazzy.b@just-eat.com\")\n);\nIResponse response = client.Search.Find(\n    new ZendeskQuery()\n    .WithCustomFilter(\"name\", \"Cupcake Cafe\")\n); So is this an all signing, all dancing client? Well… no. We work in a pretty Agile way so we only implemented the methods that we needed. Having said that, we are making good use of Zendesk so the client is likely to provide you with most of the resources you need to get started. We like our code clean and we tried to make the client easily extensible and well tested. Hopefully this will make it easy to extend and work with if you do need to add more functionality and collaborate with us on this project. Can I steal it? Yes. Take it. Put it into your application. Sell it on Ebay. Give it to your mum as a birthday present. We like to be pretty liberal about the stuff we open source. Check out the license on our GitHub repo . If you do take it and you’re feeling community spirited, contribute back. Downloads Source code is available on GitHub here: https://github.com/justeat/ZendeskApiClient NuGet packages available here: http://www.nuget.org/packages/ZendeskApiClient", "date": "2014-09-19"},
{"website": "JustTakeAway", "title": "Apple WorldWide Developers Conference 2014", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/08/08/apple-worldwide-developers-conference-2014/", "abstract": "WWDC 2014 took place at the beginning of June in San Francisco. It spinned out lots of announcements and introducing the next major release of iOS 8 and OS X Yosemite. The keynote began with the adoption statistics – 89% iOS 7 adoption and 9 millions developers in total with highest 47% annual increase. I was quite surprised with such a high iOS 7 adoption rate as I thought there were be still a lot of iOS 6 style fans. OS X 10.10 Yosemite I won’t be spending too much time with interface changes and functionality updates as we are focusing on mobile platform, but there are several highlights I want to point out. Concept of “ continuity ” between iPhone, iPad and Mac. For example if you receive a call, you will be notified on your Mac and you will be given the option to answer from your Mac or iPhone. The same applies for texts and messages. “ Handoff ” technology which allows you to begin a task either on iOS or OS X and immediately pick it up and continue on another device iOS 8 From a user perspective there are not so many UI changes, but Apple has added nice features, such as interactive notifications, latest contacted people, third party keyboards etc. From a developer perspective, iOS 8 SDK is the biggest release yet with over 4,000 new API calls . This will give us access and the option of using iOS frameworks we never had before. iOS 8 offers new frameworks including: HealthKit – makes it possible for health and fitness apps to communicate with each other and use specific information to manage your health Metal – new graphics technology with 10-fold improvement in draw call speed. It allows developer to bring console class games into mobile devices Swift – new programming language Touch ID – provides authentication with a successful fingerprint match and keeps the fingerprint data safe CloudKit, PhotoKit , new Camera API, new App Store features, TestFlight , iTunes Connect with free analytics , SceneKit HomeKit , Apps Extensions and others I am going to describe some of the features and the frameworks a little bit more, because we will use and integrate them at JUST EAT in the near future. Size Classes Apple introduced with iOS 6 and iPhone 5 release Auto Layout, relationship between UI elements. Now with iOS8 it announced Adaptive UI, of which main feature is the ability to specify layout rules depending on Size Classes , which are an enumeration that represent compact and regular sizes. We will be able to use same ViewController with various layout rules applied across Size Classes to adapt devices of all sizes. This is sending out a message that Apple is preparing new devices without specific resolutions or screen sizes. The other indication that something new is on the horizon is an iOS Simulator with resizable screen. Size classes chart Swift One of the biggest and most surprising announcements of the keynote was the new programming language – Swift . It is meant to be faster , safer , more modern and easier to use. Swift is also designed to be concise and interactive as the code is interpreted and rendered in the Playgrounds . Safe refers to the fact that the language is type safe, but thanks to type inference, the type declarations are less onerous which make Swift more concise than Objective-C. Along with type inference, Swift also introduces very concise closures (lambdas). On the compilation and runtime side of things, Swift targets the same Cocoa (OS X) and Cocoa Touch (iOS) APIs, and uses the same LLVM as Objective-C. Swift code can co-exist with Objective-C code in the same project, encouraging adoption. Swift is still in evolution and it will be nice to see how it will grow into a modern programming language that conforms to the mother methods of app development, and eventually replaces 30-year-old Objective-C . On the other hand iOS and Android are now sharing C support and it will get more difficult to port the Swift app to Android now. Playground – interactive side bar Analytics iTunes Connect Analytics will provide the ways to measure engagement by being able to track installations, active devices, sessions and stickiness. Users will be able to see the source of download and retentions, but we need to remember there’s lots of information available on the other platforms for a while. New App Store View will give you information about metrics such as how many times people viewed your app page and what is your current conversion rate . See if you are getting lots of traffic, but no downloads. I don’t think Apple’s analytics will displace other analytics solutions like Google Analytics, but it will provide more insight into what is actually happening on the App Store. Debugging Apple gave to the developers another update for Xcode. Maybe you had a situation where you had to quickly test some piece of code and you ended up creating lots of “test projects”. I have probably seven of them and the new Xcode 6 actually gives you a solution for it. It’s called Playgrounds where you can try out your code without creating a new project. With interactive sidebar you can instantly see output of your code there. It displays variables in a graph, inspects each step of your drawing and watches SpriteKit or SceneKit animations. Xcode 6 includes UI inspector – live debugging tool to resolve difficult UI bugs , as can be seen on the picture. Apple is following the example set by third party tools such as Reveal and Spark Inspector . The fact that Apple included the feature in the Xcode 6 gives us the comfort of getting it for free with no need to buy any additional tools. Conclusion Apple shows again that it can still surprise everyone beyond all expectations by releasing new programming language Swift and a huge iOS SDK update. On the other hand we have seen some of those ideas before in other third party tools, and we can probably expect this to continue. WWDC Announcements will affect not only design but also the architecture of our iOS applications. Our UX/UI is creating new layout to support the “Adaptive UI” and we are  supporting them from a technical perspective to get it ready by September, when the release of the iOS 8 and probably the new device is expected. WWDC was for me a great experience where I could present our company, speak to Apple’s engineers, and meet other developers and friends. I believe this opportunity will help everyone in our iOS team to improve and I hope all of us will have a chance to attend the great conference in San Francisco. Apple has such complete control over both the hardware and software ends of the market, which allows its devices to work so well together. Nowhere has this been more apparent than in the product integration we could have seen on the WWDC stages for the whole week. That’s why I believe all the announcements will allow Apple overtake the competition with regards to the innovations, because they are so significant. Pavol Polak", "date": "2014-08-08"},
{"website": "JustTakeAway", "title": "OpenSourced: JustSaying (AWS MessageBus)", "author": ["Written by Anton Jefcoate"], "link": "https://tech.justeattakeaway.com/2014/07/24/opensourced-justsaying-aws-messagebus/", "abstract": "JUST EAT, AWS and a Message Bus We’ve recently embraced Event Driven Architecture as a cornerstone for a decent chunk of our platform. This post introduces our latest open source library which is a super simple, easy to use Message Bus built on top of AWS. In later posts I’ll cover the motivations and benefits of our decision and some more in-depth first hand experience of our journey. So, for now… Introducing “ JustSaying “… What is it? JustSaying is a c # library giving very easy to use Message Bus type functionality on top of Amazon’s SNS and SQS services. We’ve taken inspiration from the likes of Greg Young , MassTransit , NServiceBus et al. Ultimately we’ve come up with a simplistic, robust AWS centric way of getting messages published and consumed across component boundaries without the need to host any additional infrastructure. We’ve put a lot of focus on the following: Fluent, readable, extensible configuration syntax Developer friendliness Minimal configuration Un-intrusive extension of existing/legacy codebases Yeah Yeah…whatever – just show me the snippets… Publishing a message: public class OrderAccepted : Message\n{\n    public int OrderId { get; set; }\n}\nvar publisher = JustSaying\n    .CreateMeABus.InRegion(RegionEndpoint.EUWest1.SystemName)\n    .WithSnsMessagePublisher();\npublisher.Publish(new OrderAccepted { OrderId = 123456 }); Consuming a message public class OrderNotifier : IHandler\n{\n    public bool Handle(OrderAccepted message)\n    {\n        // Some logic here ...\n        return true;\n    }\n}\nJustSaying.CreateMeABus\n    .InRegion(RegionEndpoint.EUWest1.SystemName)\n    .WithSqsTopicSubscriber()\n    .IntoQueue(\"CustomerOrders\")\n    .WithMessageHandler(new OrderNotifier())\n    .StartListening(); More advanced? Hit the read-me on GitHub: https://github.com/justeat/JustSaying How does it work? SNS as an exchange. SQS as a delivery mechanism. Enough said? Why we built it One of the main things we were missing in JUST EAT’s platform until recently was a structured way to represent State Change across our services, causing us some difficulties in API design and working with our legacy components. We wanted a more robust system in place for dealing with outages, which resulted in less service interruption and data loss… Hey, we’re in The Cloud – instances go missing! It isn’t an easy job to wire together the Amazon services; discovery can be an issue. A tool to make this easy was the only option in a multi (independent) team environment. Due to the above, we have been running it internally in an OpenSource fashion for some time. Using SNS and SQS together is extremely cost-effective and reliable. A voyage of discovery We’ve taken a highly agile approach of only build what you need, and so the current state of the library is a direct reflection of our problem space. Here’s a (by no means exclusive) list of the features we needed and have built in: Throttling consumers Delayed message delivery Configurable publish and consume retry attempts Monitoring of key metrics Extensible subscription and publishing configuration (for example delivery one per instance for load balanced components) Guaranteed only once delivery Strongly typed message delivery Configurable serialisation strategy Dead letter queues (for failed message handling) Blog posts on some of these detailed topics to come. Downloads Source code is available on GitHub here: https://github.com/justeat/JustSaying NuGet packages available here: http://www.nuget.org/packages/JustSaying", "date": "2014-07-24"},
{"website": "JustTakeAway", "title": "JUST EAT @ Altconf", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/07/16/just-eat-altconf/", "abstract": "JUST EAT @ Altconf For those who don’t know, Altconf is the free alternative to Apple’s WWDC conference in San Francisco. With 5 days of speakers and labs running at the same time you could not ask for much more. The world’s top iOS developers and solution providers come together to talk tech and answer all of your questions. There is no alternative to Altconf, not even WWDC. What makes Altconf special and particularly important to me, is that it’s my time to sharpen my saw. It’s a great time for reflection and personal improvement. As a developer it can be hard to understand the necessity to do this. You might think “Can I show this will improve my output?”, or “What will I learn?” It’s not always that simple. The Talks “Being better” Mike Lee This talk felt like a performance, one of those moments that you never forget. Like the first time you hear “ The Planets ”, your hairs will stand up on the back of your neck and you will never feel the same again. It was beautifully crafted and polished, yet felt like it could have been delivered straight from memory from the heart. One of the key messages that stuck with me was: “When good people let themselves be lazy they can do evil.” It’s a great reminder to us all that on a day to day basis it’s important to be careful and bring it back to programming. Take time to review that pull request. If you’re tired leave it, don’t just merge it in. But obviously this is meant for larger ethical questions that we face in tech. “Nine ways to stop hurting and start helping women in tech” Brianna Wu This is obviously a very important topic and something we have talked about on the blog previously. It’s something we feel very strongly about at JUST EAT. To reference the talk, it’s not about making a big deal about it, it’s about working towards a solution. Realise it’s a large issue, but don’t hold it as other controversies and just class it as this thing that “doesn’t happen to us”. We need to continue to be sensitive and inclusive, it’s for everyone to be aware and take action when required. It should not be a second consideration for anyone. JUST EAT welcomes and understands the benefits that a diverse team of any nature can bring. That’s why we have engineering teams across the world and are an equal opportunity company. We see no boundaries or stereotypes, just great engineers and different approaches to problems. Both of which any company would be foolish to miss out on. “Mindful Design & Marketing” Carla White Dealing with depression and a failed business idea, Carla White shows how you can come back from the very brink and be stronger for doing it. It’s so important for us to be grateful for the little things. Clara explains that it only takes 30 days of repeated work to reshape your brain and build the muscle memory to keep you interested in doing a regular task. After those 30 days it should no longer feel like a chore and just be part of what you do. For Clara this was keeping a gratefulness diary every day that became the inspiration for an app. My favorite part of the talk was the discussion of the “happiness advantage”. When you are not stressed you should find yourself being more creative, obviously more reasonable, and in general, better at problem solving. I personally find this one hard because I work well under pressure but it doesn’t make me a happy person. The balance there is important. We should be mindful of the toll that being stressed can take. “Networking on the Mac” Aaron Hillegass Well we had to have a little bit of tech in this post, and for me this shines above all the others. Aaron Hillegass explains what will be on all of our whiteboards in the next few years. Many clients communicating via websockets with a CSP based server that communicates with an Authorization and Authentication service as well as some version of “the truth” (i.e. a DB). He says it’s a billion dollar idea for a company to solve, they would be the “next parse”. I completely agree. Great watching and a simple message. Powerful. “Stables and Volatiles” Michael Lopp A brief history of Silicon Valley and the tech culture that we are all accustomed to today. “Are you wearing flipflops?” – Think about how many other industries are cool with that – it’s kind of cool. The concept of “stables” and “volatiles” is a way to conceptually understand two different types of developers. Now, do not worry you are not being boxed here. Like anything in life this is actually many shades of grey. You fall on a line somewhere between them. See, the stables are people who enjoy process and delivering results. You want to be told what to do and you think about the future intensely. You might work at a large corporation like IBM and you build huge complicated systems. At the other end of the spectrum you might be a “volatile”, you don’t want to be told what to do. You want only the ember of an idea and to be given space to run away with it. You ship code and get it done fast. LIKE, REAL FAST. It won’t scale but damn, it will get done fast. Where do you fall ? Watch the video and take the toaster test, it will make sense soon… This video had too much to talk about, really just go watch it. What now? Our profession needs Altconf, we need to take time to understand the non tech side of our world. I hope you agree that being a developer/engineer is hard, it drives us into a world of introversion and black and white decisions. Programming is less than one hundred years old, our minds and bodies as well as the world is still adapting. It presents so many new challenges for us as humans that we are do not know how to cope with. Professions like farming have existed for thousands of years, we know the pressure and lifestyle that it creates. How do we deal with these challenges ? If you watched the videos this wont come as a surprise but in short: be grateful, don’t be lazy, challenge your expectations. Think about the future fondly, consider the past, good ideas might come round a few times. People are different, find the best way to work with everyone, be mindful of that and figure out a way to work with them. Take time to breathe , take care of yourself, build awesome things and never lose your passion. But most important, never stop improving, keep that saw sharp. Ben Chester", "date": "2014-07-16"},
{"website": "JustTakeAway", "title": "OpenSourced: Topshelf.Nancy", "author": ["Written by Jaimal Chohan"], "link": "https://tech.justeattakeaway.com/2014/07/14/opensourced-topshelf-nancy/", "abstract": "Topshelf and JUST EAT We’re proponents of Event Driven Architecture and for a few components of our system this means we have messages which are pushed into queues and then processed by multi-threaded Windows Service applications.  This results in state changes (for example changing the state of an order) followed by further events being published to notify other components of the state change. Windows Service installation has always been a bit of an arduous task, requiring the creation of installers, usage of installutil.exe for installation configuration and resulting in a difficult to debug application. Topshelf is a great framework for hosting Windows Services and we’ve been using it for a while now.  It’s removed the friction around debugging, installation and configuration of our Window Service applications. Once a Windows Service is installed and running there’s not much interaction to be had with it. It just runs. But sometimes that’s just not good enough.  We wanted to know more about the runtime health of our services and be able to use that information to make a decision as to whether the service was healthy.  For example: are threads completing in a reasonable time? do we have good connectivity to external services (SNS, SQS, Dynamo, S3, SQL Server)? is working memory usage too high? In a web application you could have a health check page displaying this information, and this health check page would return a 200 or 500 response so that your load balancers would be able to take them in and out of service.  We decided to do the same for our Windows Services and used the Nancy Self Host to host a Nancy endpoint in our Windows Services. We wrapped this up in a Topshelf extension to make it reusable and we used it to expose the health of the Windows Service, so we know when it’s not performing as expected. Downloads Source code is available on GitHub here: https://github.com/justeat/topshelf.nancy NuGet packages available here: http://www.nuget.org/packages/topshelf.nancy", "date": "2014-07-14"},
{"website": "JustTakeAway", "title": "OpenSourced: OpenRastaSwagger", "author": ["Written by Jaimal Chohan"], "link": "https://tech.justeattakeaway.com/2014/05/19/opensourced-openrastaswagger/", "abstract": "OpenRasta and JUST EAT We’ve previously talked about how we build on a lot of open source at JUST EAT – so today we want to give a little back and publish some of our extensions to a C# project called OpenRasta . OpenRasta is an open source, C#, Resource-Oriented framework that you can use to build MVC-style frameworks and RESTful APIs .  For the past couple of years, we’ve used it to power many of our internal web-APIs . We have a handful of different C# web frameworks in production (OpenRasta, ServiceStack , NancyFx and ASP.NET MVC ), all serving different purposes. Regardless of the framework we’ve chosen for a particular task, there’s one constant when we’re building web-APIs – the need for accurate, human-readable documentation. But first… a brief history of documenting RESTful APIs Documenting RESTful APIs is a challenge – predictable API structures were lost as the industry transitioned away from SOAP web-services , with the closest attempt at a verifiable spec for REST emerging as WADL . WADL was submitted for to the W3C in 2009 , but there are currently no active plans to standardise it and it has never reached widespread adoption. WADL was widely criticised during its gestation in 2007-2008 for a variety of reasons. It’s focus on client generation drew ire from the RESTful community who drew parallels to the negative points of WSDLs in WS-* APIs , they considered the specification too broad to be useful, and the belief that RESTful APIs should be self describing using HTTP’s semantics undermined adoption. With WADL effectively dead, in early 2010 a company called wordnik started developing the spec that became known as swagger to document their APIs in a language agnostic way. Instead of designing a spec to support compile-time code-generation like WADL, swagger put the focus on generating usable and human readable documentation. Swagger managed to hit the sweet spot in the middle – it provided useful tooling in the swagger-ui that wasn’t taxing to implement, without forcing SOAP-like designs onto RESTful APIs. It doesn’t support everything – versioning is notably absent – but it’s good enough to be useful. Documenting our OpenRasta APIs We haven’t been flying blind – over the past few years, we’d put together our own “json spec” for documenting RESTful APIs that worked well for us – it was the way we dealt with communicating API requirements between teams – but we’d never invested the time in building tooling as sophisticated as the swagger-ui. In late 2013, we started building an API using ServiceStack and we found that the out-of-the-box swagger-ui plugin proved really productive, and gave us a great way to demo and interact with a headless API. Because there’s no “one framework to rule them all” we wanted to get the same awesome UI and manual feedback cycle from some of the other frameworks we use. The first target was OpenRasta, so in our first hackathon of 2014 a couple of our devs started digging through the OpenRasta source code and worked out how to plug in some OpenRasta Handlers (similar to the Controllers in other MVC frameworks). These handlers use a combination of the OpenRasta meta-model and reflection to emit swagger 1.2 formatted json that described the other handlers in the application. We’ve added a couple of things – extra attributes to let you drop notes into the generated swagger documentation, and attributes to help you hint at types that we can’t discover by reflection (notably, handler methods that return OperationResult(xyz) must be annotated for return types to be correctly identified). We’ve started to roll these things out across our internal APIs and we’re pleased with the results. Downloads The OpenRasta project is a fairly quiet at the moment, so we’re releasing this as a stand-alone assembly and NuGet package that targets the last stable available version of OpenRasta (2.1). Source code is available on GitHub here: https://github.com/justeat/OpenRastaSwagger NuGet packages available here: https://www.nuget.org/packages/OpenRastaSwagger/", "date": "2014-05-19"},
{"website": "JustTakeAway", "title": "Conference: JECON 2014", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/05/23/conference-jecon-2014/", "abstract": "Over 50 technology minded people attended JUST EAT’s first Engineering Conference (JECON) at the start of this month. The highest rated sessions from the day were a mix of Engineering Concepts, Technical Work and Conceptual Thinking, all presented by members of the Engineering team at JUST EAT. Some particularly popular sessions were The Forgotten Process (Principles of TDD) , Building a distributed system using EDA and Future technology for restaurants . While there were some lessons to be learnt around timing of events and ensuring there is more coffee (never underestimate how much caffeine you need!), the event was declared a success and we’re looking forward to putting on another one in 6 months time. Here’s the full timetable of over 15 sessions spread over 3 tracks – please do come and join us at an upcoming event if you’d like to find out more about any of them.", "date": "2014-05-23"},
{"website": "JustTakeAway", "title": "OpenSourced: JustBehave", "author": ["Written by Anton Jefcoate"], "link": "https://tech.justeattakeaway.com/2014/04/22/opensourced-justbehave/", "abstract": "Open Source and JUST EAT Like most of the technology world, we use a lot of open source software at JUST EAT.  Open source is important to us and reflects how we like to work. It’s collaborative, it’s open, it’s pragmatic – all qualities that are essential if you want to build great software and services. As we grow and expand ( we’re hiring, get in touch! ) we’ve made an internal commitment to engaging more with the open source products communities that have helped us become successful. We’re committing to contributing back our modifications to open source projects, we’re going to be sending more pull requests to try and make the world a little bit better. But we want to do more – we’re going to start releasing some of our own proprietary libraries (initially C# and Ruby code) as open source under the permissive Apache license, and start developing them in the open. We’ll happily take pull requests, and help you use our code – we believe that as a software community, we’re better working together. We try and use the best of both open and closed source software to give deliver the best experience we possibly can to our users and partners, and we’re thrilled to be able to share some of our work with you. Announcing JustBehave – our BDD extensions for .NET testing On the back of our commitment to open source, we’d like to announce the publication of our first open source project – JustBehave.  We use C# to build a lot of our internal services at JUST EAT and meaningful test suites are vital to shipping reliable code. Some of our teams are proponents of a BDD approach to testing – organising our test suites around the concepts of contexts and scenarios. This helps us understand the functionality of our application, and lets us organise our test suites into meaningful, feature-focused namespaces, and helps us extend our tests with minimal friction. What we didn’t necessarily agree with when looking at the frameworks available, was the focus on the Gherkin language popularised by Cucumber, which was supported in the leading BDD framework for .NET SpecFlow . Gherkin’s focus on acting as a communication format between “the business” and programmers – didn’t fit our needs (in a unit testing context). We weren’t looking for business people to write our behaviour tests and preferred to have tests written in an internal rather than an external DSL . We use Gherkin for our acceptance test suite, but we wanted something more language native for our C# tests. We found a sweet spot in the middle – a way of organising tests into contexts and scenarios without straying too far from the path of nUnit and xUnit in the way that frameworks like xBehave do, and developed a convention of test classes with well defined “Given”, “When” and “Then” steps. This evolved into JustBehave, a small C# package that enforces these conventions. A typical JustBehave test will look like this: public class TestClassNamedAfterABddContext : BehaviourTest<TheClassThatIWantToTest> { protected override void Given(){ /*…*/ } protected override void When(){ /*…*/ } [Then] public void Assertion1(){ /*…*/ } [Then] public void Assertion2(){ /*…*/ } } and will happily execute using nUnit, xUnit, TestDriven.NET, Resharper and NCrunch. We evolved a pattern of using inheritance and giving our test classes meaningful names to build scenarios out of test contexts. Using file structures like: /Tests/Payments/WhenIMakeAPayment.cs /Tests/Payments/AndMyPaymentIsRejected.cs (inherits from WhenIMakeAPayment.cs) /Tests/Payments/AndMyPaymentIsAccepted.cs (inherits from WhenIMakeAPayment.cs) to represent the behaviour of our applications. These simple patterns, along with enforcing the convention of “Given, When, Then” in our test classes, helped us build a meaningful and useful set of behavior driven tests that render well in test runners. JustBehave has evolved a little over the last couple of years – it now supports AutoMocking, an opinionated way to test for exceptions and works with both nUnit and xUnit and it helps us get our job done. We’re releasing it on GitHub and NuGet under the Apache license – check it out. https://github.com/justeat/JustBehave http://www.nuget.org/packages/JustBehave/ Keep an eye out for the imminent release of our extension packages giving you immediate NSubstitue, Rhino and MOQ driven tests too…", "date": "2014-04-22"},
{"website": "JustTakeAway", "title": "Being a female @ JUST EAT Technology", "author": ["Written by Sylvia Cichocka"], "link": "https://tech.justeattakeaway.com/2014/05/14/being-a-female-just-eat-technology/", "abstract": "Being a female in a male dominated industry can often be challenging, with many companies creating a less than pleasant working environment for their female employees compared to that of their male counterparts. Having worked in technology for the last eight years, I have experienced both the pros and cons of being one of a few females in a team. In previous workplaces, being a female in a male-dominated work place has been a huge disadvantage. No senior management would consider my ideas on a professional level, nor would I be included in social activities like the Friday trip to the pub. I didn’t feel I could grow as a person or that I would ever be given the equal opportunities for development as my male colleagues. It became really frustrating and there were times when I became demoralized because it appeared as if these problems would follow me throughout my career. However, when I joined JUST EAT it came as a pleasant surprise that things could be very different. The Technology Team at JUST EAT and its dynamics are completely different to what I have experienced before! I now have the confidence to bring ideas to meetings and be creative within my job, knowing that my colleagues will be happy to discuss them openly and on their own merits. I am also being given opportunities to learn new skills and receive the proper support to apply these into my role. JUST EAT Technology is a workplace where hard work and commitment are appreciated and recognised and, no matter what your gender, when you prove you are good at what you do then you can really progress in your career. Trust me on this one 🙂 I have grown a lot since joining the company two and a half years ago. And every morning I go to work with a smile on my face. The people are friendly, welcoming and simply a pleasure to be around. I finally feel part of a real team. And this is a great feeling!", "date": "2014-05-14"},
{"website": "JustTakeAway", "title": "Presentation: AWS Summit 2014 London", "author": ["Written by Daniel Richardson"], "link": "https://tech.justeattakeaway.com/2014/05/06/presentation-aws-summit-2014-london/", "abstract": "Nearly 1,000 takeaways ordered a minute from hungry consumers, with near real time confirmation from restaurants and delivery of their food just 45 minutes later is a hard technical challenge. At the AWS Summit 2014 in London I presented on how AWS allows the many small engineering teams at JUST EAT to take responsibility to meet that challenge, as we build and operate a platform that empowers consumers to love their takeaway experience. Here are the slides from the session, but please do get in touch if you’d like to know more: AWS Summit London 2014 – JUST EAT – High Availability and Rapid Change from daniel-richardson", "date": "2014-05-06"},
{"website": "JustTakeAway", "title": "Heartbleed", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/04/15/heartbleed/", "abstract": "So the big news last week was the “Heartbleed” bug disclosed by Codenomicon and Google engineers. I won’t repeat details of the vulnerability here, you can see the original post and references at heartbleed.com . Here are a few high level details of the way that JUST EAT handled the situation. Due to time differences, JUST EAT staff in Canada were first to receive notification about Heartbleed in the early hours of Tuesday 8th April (UK time), and investigations into what needed to happen were well underway by the time the main security and engineering teams came online in London. Because JUST EATs main platform is not served from Linux servers the first reaction may be that we were not affected, but we do have Linux based services as part of our communications path, so it was important we took all necessary steps to be absolutely sure our customers and partners data was protected. Followers of JUST EAT will know we utilise partners such as Google and Amazon for many internal and public facing services. Most of the JUST EAT services affected were offloading SSL/TLS to those services and we were in touch with their technical teams throughout the day to track the progress of their remediation. Obviously, being one of the reporters of the vulnerability Google were already well underway, and Amazon were not far behind. We could see the speed at which Amazon were rolling out the patch to their Elastic Load Balancers, which front the JUST EAT public facing sites, using a script to repeatedly test our sites. As soon as we received confirmation that each service had been remediated we proceeded with replacing our certificates/keys. As of Thursday 10th April all core JUST EAT services have been fully remediated and we recommend that customers and partners change their passwords.", "date": "2014-04-15"},
{"website": "JustTakeAway", "title": "Presentation: Embracing DevOps to operate in the cloud", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/04/08/presentation-embracing-devops-to-operate-in-the-cloud/", "abstract": "Yesterday, I presented at the April meetup for London DevOps , a usergroup focussing on DevOps . A whole set of us from JUST EAT Tech came along and supplied beer and pizza to the 50 or so people that turned up to hear me talk about how we get our developers to operate our platform. As promised, here are the slides!", "date": "2014-04-08"},
{"website": "JustTakeAway", "title": "Acceptance Testing with Specflow", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/03/21/acceptance-testing-with-specflow/", "abstract": "What is Acceptance Testing? If you are an avid unit tester, you will know the value of breaking down your production code, component by component, and running modular tests. Acceptance testing takes a high level approach, testing the streams of functionality that run through your system and attempts to recreate the actions of a user or consumer of your application. The idea behind acceptance testing is that your Product Owner, or the representative of your users or customers, should be able to write the tests in plain English and the developer should be able to implement the test script in code. Frameworks such as Specflow and Cucumber provide the infrastructure to join up the plain text specifications with the supporting code. Why Acceptance Test? Acceptance tests add value to a code base in several ways: They help you to verify the code that you’ve written fulfils any functional requirements you had for writing that code and they help you to pick up on defects. A suite of acceptance tests can help you to regression test your system. When a developer makes a change he or she is able to run the acceptance tests locally before checking-in to make sure his or her changes haven’t broken existing code. Many teams also set their test suite to run on the build server and fail the build if any tests fail. Well written tests provide a form of living documentation for your system. They set out functionality supported by the system and they provide detail of how your software should handle both the happy path and other common routes through your application. Acceptance testing at JUST EAT At JUST EAT we use acceptance tests to quality assure our code. We run our tests on the Team City build server every time someone checks in some new code to protect against regression. We leave the specifics of acceptance testing up to each team but the general approach is to write acceptance tests every time a new piece of functionality is introduced. The responsibility for writing tests sits with the developer or developers who wrote the functionality as we believe in taking ownership of testing our own code and making sure it’s of a high quality standard. Writing Tests If you work with user stories, the acceptance criteria for these can be a good place to start. If not, it still makes sense to break down your tests so that each is only responsible for a discrete area or feature of your system. This way, if you get a failing test you will have a better idea which area of your system is at fault and you’ll be able to keep your testing code more light weight. Acceptance tests are commonly written in the “Given…When…Then” format. This annotation sets out your test’s starting point (Given), some form of interaction with your system (When) and the outcome that you expect (Then). For further detail on the “Given…When…Then” syntax a good place to look would be Martin Fowler’s blog Getting Started Specflow is an acceptance testing tool that can be added as an extension into Visual Studio. Specflow uses Gherkin which is a language designed to be easily readable so that business users can write test definitions which developers can then implement as an acceptance testing suit. A specflow scenario looks something like this: Under the hood, Specflow binds to steps in “Step Definition” files using attributes. Here, the “When I login” step from the above example is bound to the method WhenILogin(). The method also has an attribute tying it to a “Given I login” step. You can add as many “[Given()]”, “[When()]” or “[Then()]” attributes to your steps as you like. Passing Parameters and Variables Often, it’s useful to pass different variables through to a step as in the “Given” step from the example above. To provide an implementation for the above, in your step file you would simply add a method which accepts two strings and uses the “‘(.*)’” expression to identify to Specflow where to expect the variables to be passed through in the string. The same implementation would work with integers and enums. If you want to pass through lists of variables you can use a table. Specflow provides a Table type which allows you to do this using a table format in your scenario which would look something like this: The “|” annotation is used to define individual cells and the header names are used as keys for each column when accessing the values in your code. In your step implementation you can then loop through each row in your table to access the values passed in. Specflow also allows you to pass blank values; simply leave the cell blank in your scenario. Passing or Failing? In order to pass or fail a test, you need to add an assertion within your “Then” step. A unit testing tool such as NUnit works well for this and allows you to make assertions based on the outcome of your scenario. Specflow works really well alongside a tool such as Selenium which provides a framework for interacting with a browser in the same way that a user might. Selenium would allow you to provide an implementation for your scenario which spun up a browser, logged in and then checked the title of the page to make such that the user had landed on the welcome page, as expected. Selenium is a topic to be covered in a later post, so for now this implementation has been abstracted away into the “IAmOnTheWelcomePage()” method call. Scoping Scenarios Specflow will bind scenario steps from any feature file to any matching step definitions it finds. If you have more than one matching binding your test will fail. If you want to reuse the same binding text but you want to provide different implementations you can use the scope attribute. You can scope steps or whole step classes to to features, scenarios or to a given tag. Here’s an example of two different implementations for the same “When I click save” scenario step. In the left hand example, the implementation relates to creating orders, whereas on the right the example relates to editing details. In order to distinguish between the two I can use the scope attribute in conjunction a tag. For my scenario, in order to distinguish that I want to use the “CreateOrderSteps” version of my “When I click save” step, I can simply add the “@Order” tag and Specflow will bind to the scoped step. Organising your Code In Specflow you will create your scenarios in “feature” files. A sensible approach is to create many feature files with relating step definitions for sets of user-centric functionality within your system. Building your features on tasks which users would undertake will help you to maintain focus on how a user would use your system and make sure that’s what you’re testing. Structuring your tests in this manner will also allow you to run suites of tests more easily, either locally through visual studio or within your Continuous Integration environment on your build server. This can be very useful if you build up a larger suite of tests and one to focus on subsections in order to reduce the feedback loop.", "date": "2014-03-21"},
{"website": "JustTakeAway", "title": "Monitoring Hadoop Clusters with Ganglia", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/07/17/monitoring-hadoop-clusters-with-ganglia/", "abstract": "When starting out using Hadoop with Amazon’s EMR I would normally go through a process of trial and error, deciding what size and type of machines to have in my cluster to run the job as efficiently as possible. That was until I discovered a monitoring tool to help me decide, Ganglia Ganglia is a website that gets installed on the cluster at the bootstrap stage that provides insight into how each box in the cluster is performing. If you noticed that the CPU was running high it might be wise to choose EC2 instances that had larger cores, or if the memory was over utilised then maybe choose EC2 instances with a larger memory capacity. Ganglia offers many features such as showing each machine in the cluster on its own graph with a different colour background which is dictated by the stress on the machine, from blue meaning minimal load to red indicating that the machine is under a great amount of stress. This is a quick and useful way of telling you to use larger or smaller boxes. As Hadoop utilises the JVM it provides a vast array of statistics that the JVM can output, so you can check specific areas of the job throughout the process. Not only can you check the cluster overall, you can also filter down the results to an individual machine, which can be a good to way to find a bad or redundant machine in your cluster. When using Ganglia with Hadoop’s built-in Job Tracker you can see how the cluster performs at each stage of the Hadoop job; the Mapper stage and the Reducer stage, which sometimes leaves you with a dilemma as you may see the cluster under great stress during one of the stages but not in the other which normally means deciding to use larger machines to ease the most demanding stage as opposed to altering the cluster size in flight. To install Ganglia on your Hadoop cluster using EMR just add the following bootstrap (no arguments required): Action-Name : ganglia-monitoring Path : s3://elasticmapreduce/bootstrap-actions/install-ganglia To access the Ganglia website running on your Hadoop cluster you need to open up the Security Group ports. For simplicity if you open up port 80 on the ElasticMapReduce security groups; master and slave, then using the Master Public DNS Name entry, which is found on the description tab of the running EMR console view. You can then access the website using the url: http://<Master Public DNS Name>/ganglia e.g. http://ec2-54-220-183-114.eu-west-1.compute.amazonaws.com/ganglia/ Ganglia isn’t the ‘be all and end all’ for Hadoop monitoring but it is extremely useful and has a vast array of reports that do compliment the Hadoop process very nicely. Author : Billy Oliver References: Ganglia Monitoring System – http://ganglia.sourceforge.net/", "date": "2014-07-17"},
{"website": "JustTakeAway", "title": "Tips for using EC2 Spot Instances", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2014/02/13/tips-for-using-ec2-spot-instances/", "abstract": "When I first started using AWS EC2 instances regularly it was to run Amazon EMR jobs. During this phase I would mix and match EC2 instance types to get the most cost-effective solution for crunching our data. With the cluster size needed to process all of our data cost was always going to be a factor, so  I ventured into the using spot instances over on-demand instances . What are spot instances? One of the main benefits of using AWS  is that you only pay for what you use and EC2 instances are a great example for that. So as the demand for EC2 instances fluctuates Amazon attempts to sell the surplus resources off in the form of spot instances. The approach Amazon have adopted for users to get this surplus computing power is a bidding process, so the highest bid gets the surplus computing power for the time they are the highest bidder or until they no longer need it. For example; if there is surplus availability of m1.small type instances then Amazon may start the bidding at 2 cents per hours.  So if someone bids 6 cents then they will get that resource at 2 cents and the current bid stays at 2 cents until all the surplus computing is used by that bidder; or by any other bidders. If someone requires more than is currently free and available at the time; due to it having already been allocated, then at this point the current bid raises to whatever the person is willing to bid, if this is less than your maximum bid then you keep your instances otherwise you will have your instances; unceremoniously, terminated and these will be re-allocated to the highest bidder.  The new current bid will now be set above your old maximum bid of 6 cents and someone will have to outbid this to gain control of the surplus resource. How bidding works Example of Spot Price Bidding The only upside to losing out whilst in flight is that you don’t pay for the current hour you are using, so if you are 50 mins into your allocated time and you lose the instance due to a higher bidder then you don’t pay for that hour, so in fact you get 50 mins free. Bidding is per instance type, in a particular Availability Zone in a particular region and Amazon implies that bidding for an m1.small will not affect the bidding for an m1.large in the same Availability Zone nor will bidding for an m1.small in Availability Zone A affect the bidding for an m1.small in Availability Zone B. When you submit your spot request you give the details of what instance type you need, Availability Zone, Region, etc. along with your maximum bid.  Now the maximum bid price is sometimes misunderstood in that people think this is the price you will actually pay for the instance; which is not the case, it is the maximum you are willing to pay per hour for the instance. If the instance bid price remains at 2 cents and your maximum bid price is 10 cents you still only pay 2 cents per hour.  If another bidder came in and bid 8 cents then you will still keep the instances but now you will be paying 8 cents per hour and not 2 cents but still not 10 cents. Choosing when to use spot instances Losing out on instances whilst in flight can obviously be a major issue in certain situations so you must carefully decide whether using spot instances is acceptable for you.  In my case this was fine because I could merely re-run the job again manually or waiting until the next hourly scheduled job kicked in again but if you are supporting critical services then spot instances may not be suitable. Beware the m1.small spike factor From my experience I have noticed that the activity on the spot market for m1.small instances is more volatile than the other instance types.   The result of this is that the spot pricing for this instance type can vary dramatically thus leaving you either unable to get a spot instance or losing it in flight. I have a few theories on why its so volatile, the first one being that when a company starts to use AWS; and EC2 instances, they normally start this venture with a spike or two and with little knowledge of the pricing structure and with the cost implications at the back of their minds they normally choose the cheap (and default) option; that being m1.small spot instances.  Also with the knowledge that they pay for what they use they often terminate their instance early thinking that this will cost them less not knowing that they still pay for the hour regardless.  So if all companies start via this route then this will cause the spot price to fluctuate and increase the activity on the spot market. To compound this issue; whilst the aforementioned is going on another company performs the same actions but sold on the fact that spot instances are cheaper they raise their bid price to make sure they get their spot instance, not realising that their bid price is greater than the cost of the on-demand instance (you can often see bid prices of $10 for a $0.20 instance).  This; again, causes the spot price to fluctuate. Another reasons for high activity on the spot market for m1.small instances is that companies often create throw away environments such as QA, Staging, R&D, etc. which require less Ec2 power than the Production environment so they opted for the smaller cheaper instances; such as the m1.small spot instance, which obviously drains the resources, raises the bid price and, causes high activity on the spot market. To overcome the m1.small spike factor you might want to try using m1.medium spot instances instead.  These are hopefully less popular and could be cheaper than the m1.small spot instances; due to their demand being less, and hopefully a little more reliable. Author : Billy Oliver", "date": "2014-02-13"},
{"website": "JustTakeAway", "title": "Creating a heat-map with D3", "author": ["Written by Rob Lascelles"], "link": "https://tech.justeattakeaway.com/2014/03/11/creating-a-heat-map-with-d3/", "abstract": "Here at JUST EAT we have regular 3-day hackathons , where we get the chance to work on whatever we believe will help empower consumers to love their takeaway experience. I’ve found they are a great opportunity to work in an area outside that of my current day-to-day work, and also to familiarise myself with new technologies. My first hackathon at JUST EAT was a great example of this. Along with others I spent the 3 days putting together a reporting dashboard using the following stack; most of which were new to me at the time: Redshift – AWS’s data warehouse service ServiceStack – framework for the API that queries our data in Redshift Backbone – providing the framework for our client-side application which consumes the API Rickshaw – graphing toolkit for displaying time-series (built on top of D3) D3.js (Data-Driven-Documents) – JavaScript library for manipulating documents based on data Seeing as we already had D3 in the mix, and having postcode dimensions in our data warehouse, I thought it’d be fairly simple to add a D3 choropleth (or heat-map) to our prototype to add a bit of visual flair. After reading the excellent Let’s Make a Map tutorial by Mike Bostock , I knew that the hardest part was going to be sourcing the postcode shape-files. Sourcing the data Shapefiles are the standard data format for describing geospatial information – shapefiles of UK postcodes are available to buy from the O/S – unfortunately outside of the budget for the prototype we were creating. After much experimentation, and learning some new tools, I managed to cobble together some UK postcode area data. I found that there is an amazing array of open source geospatial tools – www.maptools.org is an excellent resource for these; the most useful of which was QGIS for visualising and editing geospatial data. Once I had the shapefile, it was an easy process to simplify the geometries (hence reducing the file-size), and export to the TopoJSON format that D3 maps consume. I’ve posted a full description of what was involved to create the TopoJSON at github.com/roblascelles/uk-postcode-map/wiki/Cobbling-together-UK-postcode-area-data Displaying the data Drawing the postcode map was simply a case of replacing the boundary data in the tutorial . After that, it was almost trivial to add a fill value for each postcode-shape based on external data. The following snippets are from github.com/roblascelles/uk-postcode-map/wiki/Displaying-the-data , which describe all the steps of how to display an example choropleth. Assuming that we’ve already obtained a JavaScript associative array called “areadata” that holds the values we need to represent for each postcode – we can use D3 to create a function that will map each value to a colour like this: var color = d3.scale.quantize().range([\n        \"rgb(198,219,239)\",\n        \"rgb(158,202,225)\",\n        \"rgb(107,174,214)\",\n        \"rgb(66,146,198)\",\n        \"rgb(33,113,181)\",\n        \"rgb(8,81,156)\",\n        \"rgb(8,48,107)\"]);\n    color.domain(d3.extent(_.toArray(areadata))); Note, the RGB values are a nice range of blues from learning-d3-part-7-choropleth-maps . Also, we’re using the excellent underscore.js library here to flatten-out our data into an array so D3 can use it. Now we just need to pass the value in the color function for each postcode (the .style(“fill”.. section below) svg.selectAll(\".postcode_area\")\n        .data(topojson.feature(uk, uk.objects['uk-postcode-area']).features)\n        .enter().append(\"path\")\n        .attr(\"class\", \"postcode_area\")\n        .attr(\"d\", path)\n        .style(\"fill\", function(d) {\n            var value = areadata[d.id];\n            return (value)? color(value) : \"#AAA\";\n        }); As an example, let’s generate some test data based purely on the first character of the postcode: var areas=[\"AB\", \"AL\", \"B\", \"BA\", \"BB\", \"BD\", \"BH\", \"BL\", \"BN\", \"BR\", \"BS\", \"BT\", \"CA\", \"CB\", \"CF\", \"CH\", \"CM\", \"CO\", \"CR\", \"CT\", \"CV\", \"CW\", \"DA\", \"DD\", \"DE\", \"DG\",\"DH\", \"DL\", \"DN\", \"DT\", \"DY\", \"E\", \"EC\", \"EH\", \"EN\", \"EX\", \"FK\", \"FY\", \"G\", \"GL\", \"GU\", \"HA\", \"HD\", \"HG\", \"HP\", \"HR\", \"HS\", \"HU\", \"HX\", \"IG\", \"IP\", \"IV\", \"KA\", \"KT\", \"KW\", \"KY\", \"L\", \"LA\", \"LD\", \"LE\", \"LL\", \"LN\", \"LS\", \"LU\", \"M\", \"ME\", \"MK\", \"ML\", \"N\", \"NE\", \"NG\", \"NN\", \"NP\", \"NR\", \"NW\", \"OL\", \"OX\", \"PA\", \"PE\", \"PH\", \"PL\", \"PO\", \"PR\", \"RG\", \"RH\", \"RM\", \"S\", \"SA\", \"SE\", \"SG\", \"SK\", \"SL\", \"SM\", \"SN\", \"SO\", \"SP\", \"SR\", \"SS\", \"ST\", \"SW\", \"SY\", \"TA\", \"TD\", \"TF\", \"TN\", \"TQ\", \"TR\", \"TS\", \"TW\", \"UB\", \"W\", \"WA\", \"WC\", \"WD\", \"WF\", \"WN\", \"WR\", \"WS\", \"WV\", \"YO\", \"ZE\"];\n    var areadata={};\n    _.each(areas, function(a) {\n        areadata[a]=a.charCodeAt(0);\n    }); We can now see those character values, represented as colours on our choropleth: We’re not going to win an information is beautiful award , but it’s much better than a table. The actual map I finished with during the hackathon was obviously loading actual data from the service; I also added a zoom function from this click-to-zoom via transform example (was just a few extra lines of code).  My next step is to display more details on the map as you zoom in – but that’s for a future hackathon.", "date": "2014-03-11"},
{"website": "JustTakeAway", "title": "Using DynamoDB Global Secondary Indexes", "author": ["Written by Anton Jefcoate"], "link": "https://tech.justeattakeaway.com/2014/03/19/using-dynamodb-global-secondary-indexes/", "abstract": "DynamoDB DynamoDB is a schema-less NOSQL storage hosted by AWS, it has auto-scaling, read/write capacity management and alerting controlled using CloudWatch. When creating a Dynamo table you assign a Key consisting of either a Hash or a Hash and Range, and the read/write capacity (amount of consistent reads and writes that can be performed per second) .  When Dynamo was first released there were three different ways of accessing an item, by Key (if a Key has both a Hash and Range then both must be set) , Query and Scan.  Accessing an item using the Key requires you to know the exact value of the Hash or the Hash and Range.  Query allows you to perform comparisons against the Range part of the Key, but you must still specify which Hash you want to search.  Scan performs a search across all columns without having to specify the Hash/Range, but in doing so it reads every item in your table, so can be an expensive operation if your table has 100s/1000s of items. Global Secondary Indexes to the rescue Recently Dynamo released Global Secondary Indexes , which are additional indexes (max 5 at present) that can be set against any column in your table, the only limitation being they must to be created up front.  Each index always returns the Key from the main table and an optional list of projected attributes (equivalent to a column in SQL). When designing your index there are two considerations to make, cost versus performance.  Each index is a copy of the data from the main table and will contain at least an index Key and the table’s Key, plus any projected attributes.  Including all attributes in an index will double the storage cost as there will be duplicate copies of the data, but will not require any additional reads to the table.  An index that only has a Key will be slower as you will need to perform additional reads to the table using the table’s Key from the Query result.  You can of course project a subset of the attributes, but as an index has to be created when the table is created any schema changes i.e. additional attributes, can not be added to an index at a later date. To Query an index you set the IndexName and define a list of KeyConditions , these are a list of operations that you want to perform against the Key in the specified index, but be warned, you can only perform an equality (EQ) query against a Hash Key, whereas you can perform any operation against a Range Key. var query = new QueryRequest\n{\n\tTableName = \"Ratings\",\n\tIndexName = \"RestId-Index\",\n\tKeyConditions = new Dictionary<String, Condition>\n\t{\n\t\t{\n\t\t\"RestaurantId\", new Condition\n\t\t\t{\n\t\t\t\tComparisonOperator = \"EQ\",\n\t\t\t\tAttributeValueList = {new AttributeValue {N = \"1\"}}\n\t\t\t}\n\t\t}\n\t},\n\tLimit = 10,\n\tScanIndexForward = true\n}; When you Query an index the data is returned as a List of Dictionary<string, AttributeValue> , the string is the name of the attribute and the AttributeValue the value.  Attributes can be three different base types: string (S), number (N), binary (B) or it can be a set of types (SS, NS and BS) .  The AttributeValue has a property for each type which makes mapping the data to an entity more difficult and not very DRY. Query Result Mapping Here are the steps to create your own generic entity mapper. 1) Get list of public properties from T. var type = typeof(T);\nvar ret = new T();\nvar entityProperties = type.GetProperties(BindingFlags.Instance | BindingFlags.Public); 2) Loop each property in T . 3) Get the attribute name. private static string GetAttributeName(PropertyInfo property)\n{\n\tvar attributeName = property.Name;\n\tvar dynamoDbProperty = property.GetCustomAttributes(typeof (DynamoDBPropertyAttribute), true).SingleOrDefault();\n\tif (dynamoDbProperty != null)\n\t{\n\t\tvar dynamoDbPropertyValue = (DynamoDBPropertyAttribute) dynamoDbProperty;\n\t\tif (!string.IsNullOrWhiteSpace(dynamoDbPropertyValue.AttributeName))\n\t\t{\n\t\t\tattributeName = dynamoDbPropertyValue.AttributeName;\n\t\t}\n\t}\n\treturn attributeName;\n} 4) Find the attribute in the Dictionary . private static AttributeValue GetAttribute(IDictionary<string, AttributeValue> item, string attributeName)\n{\n\tif (!item.ContainsKey(attributeName))\n\t{\n\t\treturn null;\n\t}\n\tAttributeValue value;\n\treturn !item.TryGetValue(attributeName, out value) ? null : value;\n} 5) If the attribute is not found, no mapping required, so the property’s default will be returned. 6) If the attribute is found cast it to the property’s type. var propertyType = property.PropertyType;\nswitch (Type.GetTypeCode(propertyType))\n{\n\tcase TypeCode.String:\n\t\tproperty.SetValue(ret, attribute.S, null);\n\t\tbreak;\n\tcase TypeCode.Int32:\n\t\tvar intValue = ParseInt(attribute, attributeName);\n\t\tproperty.SetValue(ret, intValue, null);\n\t\tbreak;\n\tcase TypeCode.Boolean:\n\t\tvar booleanValue = ParseBoolean(attribute);\n\t\tproperty.SetValue(ret, booleanValue, null);\n\t\tbreak;\n\tcase TypeCode.DateTime:\n\t\tvar dateTimeValue = ParseDateTime(attribute, attributeName);\n\t\tproperty.SetValue(ret, dateTimeValue, null);\n\t\tbreak;\n\tdefault:\n\t\tvar attributeValue = GetNonPrimitiveTypeValue(attribute);\n\t\tif (attributeValue != null)\n\t\t{\n\t\t\tvar serializedItem = attributeValue is string\n\t\t\t\t? attributeValue.ToString()\n\t\t\t\t: _serializer.Serialize(attributeValue);\n\t\t\tvar deserializedItem = _serializer.Deserialize(serializedItem, propertyType);\n\t\t\tif (deserializedItem != null)\n\t\t\t{\n\t\t\t\tproperty.SetValue(ret, deserializedItem, null);\n\t\t\t}\n\t\t}\n\t\tbreak;\n} To get a non-primitive type value you must check each type in the AttributeValue . private static object GetNonPrimitiveTypeValue(AttributeValue value)\n{\n\tif (!string.IsNullOrWhiteSpace(value.S))\n\t{\n\t\treturn value.S;\n\t}\n\tif (!string.IsNullOrWhiteSpace(value.N))\n\t{\n\t\treturn value.N;\n\t}\n\tif (value.SS != null && value.SS.Any())\n\t{\n\t\treturn value.SS;\n\t}\n\tif (value.NS != null && value.NS.Any())\n\t{\n\t\treturn value.NS;\n\t}\n\treturn null;\n} IJsonSerializer interface is a wrapper around the Serialization logic. public interface IJsonSerializer\n{\n\tstring Serialize(T item);\n\tobject Deserialize(string value, Type type);\n} This has been tested against String, Integer, Float, DateTime, Boolean, Collection and Non-Primitive Types so it should work against most custom entity classes. Full class implementation Conclusion Dynamo is schema-less so the attributes can be of any type, when you Query an index you will not always get back all of the attributes of an item, only those defined in the projected fields.  In my opinion, it is best to create a different entity/view to map against each index, this will help to avoid attempting to cast properties that do not exist in the index. Pros: Different read capacity per index so can be tailored depending on how often each index will be accessed Return only a subset of attributes to help performance Cons: Cannot create new indexes or edit existing ones after table creation Limited to only 5 indexes Can only perform equality on Hash Key Custom mapping required, although the code above fixes this limitation Dynamo has come a long way since it was first launched and Global Secondary Indexes are a nice addition that adds some much needed functionality.", "date": "2014-03-19"},
{"website": "JustTakeAway", "title": "AWS for Dev and Test – part 3 Performance Testing", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2013/12/17/aws-for-dev-and-test-part-3-performance-testing/", "abstract": "Performance Testing Automated performance testing was the next logical step from automated development and continuous deployment . As you can imagine with the architecture to support Just Eat, which is not only a high-transaction e-commerce solution, for our customers, but also involves a distributed messaging system, for the restaurants. There are a lot of inter-connected pieces, so when we make an API change we need to ensure that this does not have an adverse affect, on either the front end site, the mobile devices, the communications with the restaurants, or the back end administrative tools. We have a VPC setup for each team to performance test in and with a simple change of the parameters passed into the CloudFormations, we are able to build an as live version of any or all of our features. As you can imagine, having this as live setup, for all applications, can get quite expensive. So rather than having this environment up all day we have implemented the following design which enables us to run automated nightly performance tests. Automated performance testing in AWS On a set schedule all stacks required for the desire application to function are created in our VPC of a production size and configuration. Once these stacks are online, a performance stack is created. This stack consists of a Test Runner to co-ordinate the test, and an auto scaling group of Test Agents to execute distributed load across the application being tested (see bees with machine guns or wapt ) . After the test has run the results from the test runner are uploaded to S3. These are the collated along with all the relevant metrics from the monitoring server. This data is then analysed and the build will pass or fail depending on how these performance metrics compared to previous runs. At this stage artifacts are published with the relevant graphs onto the build server, this allows developers to easily see why a build has failed and compare the metrics to previous runs. As this performance environment is built from scratch every night we can be confident that what is set up and deployed on this environment, is identical to what will be deployed on production. I hope this has given you a brief insight into how we at Just Eat are using AWS to help us develop and test in a reliable, repeatable and scale-able fashion.", "date": "2013-12-17"},
{"website": "JustTakeAway", "title": "AWS for Dev and Test – part 2 Continuous Deployment", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2013/12/10/aws-for-dev-and-test-part-2-continuous-deployment/", "abstract": "Continuous Deployment What I described in Section 1 is fine for having an environment available for you to use all day, but what about keeping it up to date during the day? When we create our stacks in our development VPC we pass in another parameter, this specifies if we want the stack to auto update or not. When this is set to true, a scheduled task is set up on the EC2 instance that polls S3 for the latest version of the application. If a newer version is found, the package is downloaded and installed on the instance. Continuous Deployment in QA Now we realise this means the EC2 instance cannot be considered a truly immutable server , as we perform in place updates on them, however this is something we do only on QA, and even then these servers are re-built from scratch daily. We have chosen to do this rather than rebuild a new EC2 instance each time as we already had the in place update available on our installation packages, and this results in the least amount of downtime for our development environment. Next I will share with you how Just Eat have implemented on demand performance testing in AWS.", "date": "2013-12-10"},
{"website": "JustTakeAway", "title": "AWS for Dev and Test – part 1 QA Environments", "author": ["Written by Bennie Johnston"], "link": "https://tech.justeattakeaway.com/2013/12/03/aws-for-dev-and-test-part-1-qa-environments/", "abstract": "I recently presented at an webinar on using  AWS for development and testing, I then figured the work I put into the presentation should be published here in case you’re interested in how we are using AWS for dev and test at JUST EAT. Creating a QA Environment When we first started with AWS we created a VPC for our team so as not to interfere with the rest of the company. This meant that the components we deployed could easily talk to each other without accidentally talking to other teams environments, or production. When it came to launching our application, it quickly became apparent that CloudFormation was the way to go for us, as this gave us the ability to have full control over the required stack for our application. Using the Cloudformation to create a QA environment Through integration with the AWS APIs and Team City we are able to automate tearing down and creating our stacks from scratch. This mean 100% of our server elements in development are reset to a known state on a daily basis. One thing we do do in our CloudFormations is to use the mappings section to configure the environment size you are building. When the stack is created in AWS we pass in parameters that allow us to configure many aspects of the stack, from the number of instances, to the instance type, to the Zones it is deployed in. This means that for development and testing we run single instances on as smaller types as possible, yet for performance testing we can easily spin up production sized environments. This helps reduce our AWS costs, while still allowing us to have the environment we want when we need it. \"EnvironmentSizes\" : {\n    \"production\" : {\n        \"DesiredCapacity\" : 9,\n        \"MaxCapacity\" : \"12\",\n        \"MinCapacity\" : \"9\",\n        \"Ec2InstanceType\" : \"m1.medium\",\n        \"AvailabilityZones\" : [\"eu-west-1a\",\"eu-west-1b\",\"eu-west-1c\"]\n    },\n    \"qa\" : {\n        \"DesiredCapacity\" : 1,\n        \"MaxCapacity\" : \"1\",\n        \"MinCapacity\" : \"1\",\n        \"Ec2InstanceType\" : \"m1.small\",\n        \"AvailabilityZones\" : [\"eu-west-1b\"]\n    }\n} [environment size section of the Mappings in the CloudFormation] \"InstanceType\" : {\n    \"Fn::FindInMap\" : [\n        \"EnvironmentSizes\",\n            {\n                \"Ref\" : \"EnvironmentSize\"\n            },\n        \"Ec2InstanceType\"\n    ]\n} [referenced in the properties of the  AWS::AutoScaling::LaunchConfiguration] I hope this helps make it clear how we a utilising CloudFormation for our development and  QA environments. Next I will let you know how we are implementing Continuous Deployment with the above setup.", "date": "2013-12-03"}
]