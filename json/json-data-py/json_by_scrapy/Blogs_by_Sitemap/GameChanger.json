[
{"website": "GameChanger", "title": "Improving User Experience by Doing the Big Refactor", "author": ["Adam Tecle"], "link": "http://tech.gc.com/how-we-improved-user-experience-by-doing-the-big-refactor/", "abstract": "In a long lived and complex application, there often comes a time when some early decisions end up becoming constraints that are difficult or impossible to overcome. At some point in the lifespan of the software, you may be faced with a choice ‚Äî do the Big Refactor, or find ways to work around the problems. More often than not, the latter route is taken. When you have a lot of immediate priorities, it‚Äôs difficult to justify spending a lot of time on a big and potentially risky project. Sometimes however, the benefits of doing the big refactor greatly outweigh the drawbacks. It takes a cost-benefit analysis specific to your app to determine if that is the case. And recently at GameChanger, the big refactor was worth it, so we did it! This blog post is about how we at GameChanger successfully executed a huge overhaul to our data model. We removed a foundational database entity, person , upon which almost every part of our system relied in some way, and replaced it with new entities which enabled the user experience that we knew our customers expected. We‚Äôll take a look at what user-facing problem we were addressing and the high level adjustment to our data model. We‚Äôll also talk about how we approached such a large project tactically, some of the interesting technical problems we solved along the way, and the benefits we‚Äôre reaping a couple months out from shipping. The Problem üß© For a long time, one of our most common complaints from users of Team Manager was the complexity around who can edit players and what information they could edit. The root of the complexity was this: a player could be created without an email address, but, once a player was attached to an email address, a user could not edit that player‚Äôs information, like first name or last name, independently of another team. A player‚Äôs name and their relationships to other users would carry over from team to team. And that meant in order to avoid situations where a coach on one team could make edits to a player which would be disruptive to another team, we pretty much disabled editing of player information entirely after the player was attached to an email address. From the launch of Team Manager in 2017 to summer in 2020 when we first started seriously considering doing this project, we received thousands of CX cases that had something to do with this issue. In addition to these CX cases, we had evidence from conducting user tests that there was confusion around how editing players worked. Clearly, this user experience was unbecoming of a best-in-class team management app! The technical reason for this user experience limitation was relatively simple - a single record in a person table was the source of truth around player information across all teams. In essence, players were in the ‚Äúglobal‚Äù scope. So the problem at hand was to adjust our data model so that player information could be scoped to individual teams, allowing coaches to freely edit players without worrying about affecting that player‚Äôs information on another team. The original motivation for building the data model in this way was to support building career stats for players. The idea was that a player‚Äôs info could be carried over from team to team, allowing us to easily create a view of a player‚Äôs youth sports journey. However, we found that the old model wasn‚Äôt actually cleanly supporting this use case. Notably, we had issues with duplicate players. So, we were paying the cost for the complexity of this model, but not getting the benefits we wanted without doing some extra work on our previous data model. One of our values at GameChanger is to put the customer first, and in this context, we thought putting the customer first meant fixing this UX problem now, despite the potential downside for career stats being tricker to support in the future. So, the big technical lift was to fix our data model. One major challenge with that was that the concept of a person was deeply embedded into all three of our clients (iOS, Android, and web) as well as the API powering them. Thousands upon thousands of lines of code dealing with person . Not to mention, a busy baseball season soon approaching. How will we update our client apps with the new data model, and cleanly deprecate app versions on the old model? Will there have to be downtime? How will we safely and accurately migrate person data to new tables that we may create? Is it worth doing this project so close to a busy season? The answer to the latter question we determined was yes ‚Äì we wanted as many users on the new data model as soon as possible. Why? We expected that Spring 2021 would bring an influx of new Team Manager users, as well as users migrating from the legacy GameChanger app ‚Äì and the less users that had to have their data migrated from the old model to the new model the better. Whatever solution we came up with, it was clear the level of effort required to implement it would be immense. But, this was no deterrence for us on the Athlete Experience team at GameChanger. One of the great things about working here is that teams are empowered to solve problems that are important within our mission - and we understood that fixing this user experience problem would offer long term benefits to our users and our product. And so, we got to work! Research üî¨ We‚Äôd identified at a high level the problem, the ideal solution, and some alternative solutions. The next thing to do was to enumerate the work in detail, write a proposal, and disseminate it for feedback. Our proposal had to cover a lot of ground. Here are some of the things our proposal discussed: What is the dependency graph between the person table and the rest of our data model? What high level concepts in our API and in our clients are involved in making our person model work? What new tables will be created? Which existing tables will require migrations? What new high level concepts will we have and how will the old concepts map to them? Exactly how will iOS, Android, and web apps be migrated to use the new data model? We use Realm on our mobile apps - so what Realm tables will need migrations? How will each high level feature that we support be affected by this data model change? How will it be migrated to the new data model? How will we migrate data from the person table to other tables (we‚Äôll explain this in a bit more detail later) What are the alternative solutions? Here‚Äôs a brief summary of the data model that we had, and the new one we proposed: Previously: A person stored someone‚Äôs name, phone number, and linked them to their user account if they had one. A player linked a person and a team Associations between a user and a player were modeled as links between person IDs This should illustrate the ‚Äúglobal‚Äù player issue described earlier. If a player is on multiple teams and has a user account, that player‚Äôs name and relationship information comes from the record in the person table. There was no straightforward way in our previous model to scope that to a single team. Currently: A player stores a link to a team, and optionally a user. It also stores a name and a phone number. Associations between users and players are stored as links between user IDs and player IDs. This change allows players to have different relationships and different names per team. We completely remove the concept of a person . We should note, above are the high level changes to show how we adjusted our data model to accommodate the user experience we desired. But, omitted are many auxiliary changes and other important details that were needed to transition our data model away from person . A Detour - Data Mirroring üëØ‚Äç‚ôÇÔ∏è One interesting problem outlined in our proposal was how to populate our new tables with data. For existing data, we could run a script that would migrate all of it to the new data model. However, for data currently being written into the system, we needed a way to migrate that data immediately. Blocking writes in order to migrate data was definitely not an option ‚Äì we wanted the system to keep humming along and have data from the old models migrated to the new models in as close to real time as possible. We referred to the solution that we came up with as ‚Äúdata mirroring‚Äù, and it would help ensure that we lost no data while migrating over to the new data model. Its function was essentially to update our new tables every time the old tables changed. Architecturally, this was implemented as a service in our Node.js API. We proposed two ways in which data mirroring could occur: Inline mirroring : update the new tables within the same transaction that the old tables are updated. Queued mirroring : update the new tables after the transaction where the old tables are updated. We put the work of updating the new tables on a queue. You may be wondering, when each of these would be used and why. The purpose of these two ways of mirroring data was to ensure data consistency. Imagine we need to update a person‚Äôs name. In the old data model, this is an update to the person table. In the new data model, this might be an update to a player, or an update to a user. If we only used inline data mirroring, then one possible scenario would be a person‚Äôs name changed at the same time that the person was added as a player. If the transactions run in parallel, then the new player might be created with the old name, since each transaction works on a view of the database after the last transaction. We could use postgres‚Äô SERIALIZABLE mode, but that is a non-starter for us due to various unrelated reasons (though let us know if you want to hear more about our experience working with Postgres). We address this scenario by queuing the task to update a person‚Äôs name, and then queuing the task to add that person as a player. The order in which these execute does not matter, both will see the database after the relevant changes are applied, and the data is consistent. Most data mirroring operations would occur as transactional operations, and queued mirroring primarily served the purpose of fixing issues that would arise from transactions happening in parallel. Execution üîß After we identified what had to be done, we broke up our work into roughly 7 phases, with a point of no return that we referred to as ‚Äúcutover‚Äù. Phase 1: Create new tables, write new API endpoints, implement data mirroring. Phase 2: Update our sync system to work with our new data model. Sync is our mechanism for keeping client devices up to date with the server - read more about our implementation here . Phase 3: Build new UI on iOS and Android to accommodate the new data model. Phase 4: Migrate various features on the server and mobile clients to work with our new data model. Phase 5: Update the web client to work with the new data model Phase 6: Data & Analytics changes ‚Äî Cutover ‚Äî Phase 7: Cleanup, take on various work we punted on. Cutover ‚ö†Ô∏è Cutover was the point at which we would stop writing to the old models, and begin reading and writing to the new models. This was a point of no return because data mirroring was essentially a one-way operation ‚Äì once they diverged, it would be tricky, though not impossible, to get back to a place where they were 1 to 1. For all intents and purposes though, we treated it as a point of no return, and so we treated the execution of cutover as a particularly sensitive part in the process of migrating to our new data model. We did a number of things to make sure the execution of cutover would be successful: We created a runbook for the day of cutover. We did it at a low traffic time. We assigned roles for each person involved in executing cutover. A scribe - someone to capture what was going on during the video call and write it into Slack A monitor - someone to monitor system behavior to make sure the apps and the API were performing properly. We had 2 people in the monitor role, one person for the API, and one person for the mobile apps. A console - someone to perform various development tasks. We had two consoles, someone to run scripts, and someone to flip feature tags. We did cutover on staging a week prior to doing it on production as a practice run. All the planning and preparation paid off, as we were able to execute cutover with no major issues and no data loss. Looking Back üåá At the time of writing, we‚Äôre a few months out from cutting over our API and our clients to use the new data model exclusively. The project has been a resounding success ‚Äì our app now works the way our users want it to, which was the aim of this project. In addition to eliminating the CX cases in our queue that deal with the complexity around editing players, we‚Äôve also managed to reduce the amount of requests that our sync system has to send when players are updated, since we can send syncs only to the team affected by a change to a player, instead of to all teams where the player has an association. This work would not have been possible if it were not for the amazing people that we have at GameChanger. Every team at the company played some role in making this project a success, but a very special shout to Eliran Ben-Ezra, Peter Gelsomino, Abhinav Suraiya, Ami Kumar, Adam Tecle, Israel Flores, Dane Menten-Weil, Janell Eichelberger, Wai-Jee Ho, and Leah Giacalone. Share on Twitter Share on Facebook", "date": "2021-05-13"},
{"website": "GameChanger", "title": "Let me automate that for you II, Electric Bugaloo", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/let-me-automate-that-for-you-ii-electric-bugaloo/", "abstract": "Improving our original, embedded SQL generator and some related scripts by converting them to a better, long term, stand alone SQL producer that‚Äôs faster, more reliable, and more obvious. About seventeen years ago, in 2019, I published my blog post ‚ÄúLet me automate that for you‚Äù about a design for automating creating warehouse tables based on schemas for new event data. The idea was when our ETL system couldn‚Äôt load waiting data into a warehouse table (as there was no table to be found), it would look up the schema for that data, convert the schema to a SQL statement, then issue a PR to the repo where SQL migrations for such needs are kept. Eventually creating tables made a friend, updating tables when there was a mismatch between the schema of the data we were loading and the schema of the table in the warehouse, and a third buddy joined the part, optimizing a table to improve its performance. The system had some absolutely great qualities: it automated acting on errors it saw, it generated great documentation in the PR and the SQL statement (with comments for discussions and places to review more closely), and it posted to Slack to let engineers know that there was something for them to do a final review on. However‚Ä¶¬†it wasn‚Äôt perfect. Reading is going toward something that is about to be, and no one yet knows what it will be. [1] Let me take you through the evolution of our embedded SQL generator to stand-alone SQL producer. Limitations of previous implementation Opportunities to build it better Building blocks of a stand-alone SQL producer Joining the human needs with the computer‚Äôs logic Detailed breakdown of the services available Troubleshooting live Final thoughts Appendix A: Redshift optimization queries Appendix B: select Github logic Limitations of previous implementation While the SQL generator eased so much work for so many different people in the company, it had some‚Ä¶¬†strange caveats, shall we say. Some were more noticable than others but all were, in their own way, just that little bit too grating to live with long term. Systems program building is an entropy-decreasing process, hence inherently metastable. Program maintenance is an entropy-increasing process, and even its most skillful execution only delays the subsidence of the system into unfixable obsolescence. [6] The most obvious was that the SQL generator was reactive . It might takes hours for the loader to hit a problem that causes it to try generating a SQL migration. This long turnaround was painful for the data team, painful for the engineers making the upstream changes ‚Äî¬†it was just too unpredictable and drawn out for us to ever feel comfortable. Nothing like a PR showing up at 2am on Saturday because your robot engineer doesn‚Äôt have a sense of boundaries and working hours! On top of that, the SQL generator wasn‚Äôt always invoked when it should be, especially around updates to schemas. We require, with help from the Schema Registry, that all of our schemas be backwards compatible, which has this odd quirk that means the loader can still load the new schema‚Äôs data‚Ä¶¬†into the old schema ‚Äî¬†good for sending data, not for warehouse table design! The issue here was we had the code to generate updates, but we didn‚Äôt have the code to trigger updates every time. Paired with the long turn around time for ‚Äúwill it/won‚Äôt it,‚Äù the system still required a lot of hands on attention, from triggering the update manually to finding gaps that went unnoticed. We were also pushing the boundaries of Python; it just was no longer the right implementation language for this system. The most distrubing yet hilarious example was being able to tell the difference between ‚Äúthe default value is null ‚Äù and ‚Äúthere is no default value,‚Äù both of which ‚Äî¬†in Python ‚Äî¬†are None . We ended up using a magic string of \"‚àÖ‚¶∞‚¶±‚¶≤‚¶≥‚¶¥\" to try to indicate these differentiate between these types of emptiness but we all knew, this indicated we had gone too far with this set of tools. We needed something better, something designed to work together instead of three related but separate mini systems that needed constant supervision. Opportunities to build it better With our new data pipeline out the door, we had an opportunity. You see, the Schema Registry writes all its schemas to Kafka. This actually means you can subscribe to schema changes from a Kafka consumer without a lot of fuss. Get updates within a few seconds or minutes of a new or updated schema instead of hours or days? Uh, yes please! That is a much more reasonable turnaround time and removed the problem of not updating for every changed schema. With the valve‚Äôs Kafka consumer setup in Scala, that presented a companion opportunity to switch implementation languages to one that could better represent the strong typing of the two systems we were converting (Avro and SQL), including different forms of emptiness! :tada: It‚Äôs the simple wins in life sometimes that give you hope and being able to delete \"‚àÖ‚¶∞‚¶±‚¶≤‚¶≥‚¶¥\" as a mission critical part of a system was truly a win. Thus we had a new plan: move the embedded SQL generator to a stand-alone SQL producer in Scala that consumed from Kafka, opening up the chance for faster turnaround, better representation of the data , easy access to the official Avro library (so we wouldn‚Äôt have to reimplement their logic), and a better setup for testing nitty gritty, hard-to-spot edge cases in both the short and long term. I saw and heard, and knew at last The How and Why of all things, past, and present, and forevermore. [7] It just made sense. Building blocks of a stand-alone SQL producer To start (re)implementing a system like this required tackling it both from the foundation as well as from the high level, ‚Äúwhat will be the final output?‚Äù view, to ensure the two met somewhere reasonable; the previous system had grown organically but we really needed to replace it all at once, mostly for our own sanity but also to not have the two competing against each other. We scraped together implentations from the valve and other one-off scripts we had to form the basis of starting code that wasn‚Äôt unique to the SQL producer: things like producing to and consuming from Kafka, connecting to Consul or the Schema Registry, and talking with Redshift especially about the metadata of our warehouse tables. Then we looked at what did our Python implementation hint at the existence of but not fully explore as a data structure or stand alone function taking care of a specific task without outside help? What could we do to better leverage this new implementation language to make the code as obvious as possible? Don‚Äôt tell me what I‚Äôm doing; I don‚Äôt want to know. [2] A good place for us to start was, since we‚Äôd be combining multiple services within this one system to do specialized work, how could we talk about all of their output collectively? They each produce one or more migrations, after all, so‚Ä¶¬†could we start with that? trait Migration { def schema : String def table : String def hasChange : Boolean def migrations : Array [ SqlStatement ] def branchName : String = s \"bot-${prType.toString}-$schema-$table\" def prType : PullRequestType def prTitle : String = s \"Migration to ${prType.toString} $schema.$table\" def prDescription : Markdown } Internally, each data structure that extends the Migration type does a lot of logic to produce their unique array of one or more migrations and the detailed PR writeups, but hiding that complex code away allows them to be self contained. Ultimately, when we‚Äôre ‚Äúdone‚Äù with a service processing its request, we just need to be able to publish the migration to Github and ping Slack about it. The above exposes for us just what we need and nothing more. Which, of course, meant that another foundational building block would be publishing migrations: object Publish { def apply ( migrations : Set [ Migration ]) : Unit = migrations . foreach ( migration => { if (! migration . hasChange ) Log . info ( s \"No changes were found for `${migration.schema}.${migration.table}`\" ) else if ( recentMigrationAlready ( migration )) Log . info ( s \"There's already a recent migration for `${migration.schema}.${migration.table}` so not going to publish\" ) else { Log . info ( s \"Migration for `${migration.schema}.${migration.table}` has changes which going to publish\" ) val branchName = github . commit ( migration ) val prUrl = issuePr ( branchName , migration ) val channelsPostTo = channelsToNotify ( migration ) notifyHumans ( channelsPostTo , migration , prUrl ) if ( shouldUpdateDeduplication ( migration )) updateDeduplication ( branchName , migration ) Log . info ( \"Migration has been created, issued, and shared\" ) } }) } Here I‚Äôve included only the main block of orchestration logic but you can already see how we can build complex flows from such a simple input as a Migration . For any set of migrations, so long as there are changes and we haven‚Äôt already recently issued a migration for it, we‚Äôll commit it to Github (more in Appendix B), issue the PR, get the Slack channels to notify, let the humans know, and maybe even go back in to update other files like our JSON of deduplication rules for our loader. Configuration files have never been treated so well! Another grouping of foundational items we needed were converters, translating from one language to another, for example from Avro types to Redshift types: object ColumnDefinition { val defaultStringColumn = \"CHARACTER VARYING(24)\" def avroToWarehouse ( schemaField : ProcessedField ) : String = schemaField . `type` match { case Schema . Type . STRING if schemaField . name . endsWith ( \"id\" ) => \"CHARACTER(36)\" case Schema . Type . STRING if schemaField . name . endsWith ( \"ts\" ) => \"TIMESTAMP\" case Schema . Type . STRING if schemaField . name . contains ( \"email\" ) => \"CHARACTER VARYING(256)\" case _ if schemaField . name . contains ( \"latitude\" ) => \"DECIMAL(8,6)\" case _ if schemaField . name . contains ( \"longitude\" ) => \"DECIMAL(9,6)\" case Schema . Type . STRING => defaultStringColumn case Schema . Type . BOOLEAN => \"BOOLEAN\" case Schema . Type . INT => \"INTEGER\" case Schema . Type . LONG => \"BIGINT\" case Schema . Type . FLOAT => \"NUMERIC\" case Schema . Type . UNION => avroToWarehouse ( schemaField . copy ( `type` = schemaField . unionTypeValues . get . filterNot ( _ . getName == \"null\" ) . head . getType ) ) case _ => defaultStringColumn } } This has a combination of simple translation using the Avro library‚Äôs built in types along with business logic, for example that every perceived identity field will be a UUID and thus exactly 36 characters in length. I also default string fields to a small number of characters, so that humans have to review it and consider what‚Äôs a more appropriate length. Emails, though, we let those get wild at 256 characters. These sorts of conversions existed in our Python implementation but were nowhere near as easy to reason about nor readable. While the above switch case statement is massive, it‚Äôs super obvious what it‚Äôs doing and super easy to add to it if we, say, had a new specialized type like phone number that we wanted to handle. It‚Äôs a great example of could a human do this? Yes. Would a human do anything different than a machine in doing this? Not really, we‚Äôd just go look up the translation and go through a similar flow to find the right one. The system doesn‚Äôt get every case right every time but the ones it misses require human judgement anyway and are a great opportunity for someone new to say, ‚ÄúI think I have a rule for how to automate this.‚Äù Dans la nature rien ne se cr√©e, rien ne se perd, tout change. In nature nothing is created, nothing is lost, everything changes. [5] The last grouping of foundational items fell into a sort of ‚Äúexpert decision making‚Äù category. These functions don‚Äôt replace the average engineer looking at an Avro schema and saying an INT becomes an INTEGER in Redshift; they replaced a data engineer saying, ‚ÄúSort keys should follow this pattern, distribution keys should follow this pattern, here‚Äôs what optimized types look like.‚Äù This is where the difficult decisions and need for deep knowledge become embedded in the system, which both helps make the attention of data engineers less scare (if they‚Äôre in a meeting, you can always look up what they have the expert system do for an idea of what they would tell you) while also ensuring humans don‚Äôt accidentally forget something minor along the way (which is 100% what I would do all the time when I tried to optimize tables by hand, omg the amount of small things to check became wild and you litter typos everywhere). So long as the experts have implemented and tested the rules, then all the cases they would know how to handle are handled, and other ones can be added as they‚Äôre discovered. case class ColumnEncoding ( column : String , recommendedEncoding : String , savings : Double ) { lazy val reason : String = s \"Switch to recommended encoding with savings of $savings%\" } object FixEncoding { def apply ( definition : Set [ ColumnDefinition ], encodings : Set [ ColumnEncoding ]) : Option [ ProposedChange ] = { var notes : Map [ String , String ] = Map () val changes = encodings . map ( recommendation => { definition . filter ( column => column . name == recommendation . column ). head match { case column if column . sortKey && column . encoding != \"RAW\" => notes += ( column . name -> \"Sort key should have a `raw` encoding\" ) column . copy ( encoding = \"RAW\" ) case column if ! column . sortKey && column . encoding != recommendation . recommendedEncoding && recommendation . savings > 1.0 => notes += ( column . name -> recommendation . reason ) column . copy ( encoding = recommendation . recommendedEncoding ) case column => column } }) if ( definition != changes ) Some ( ProposedChange ( definition , changes , \"Corrected encodings that were mismatched, such as using tightest compression or not encoding the sort key.\" , Some ( notes ) ) ) else None } } The encoding example is probably the easiest to read (though I realize it‚Äôs still a touch wild) but has one of the most nuanced caveats in the system: we want to use the recommended encoding for all columns except the sort key . Why? Well, the tighter the compression, the less reading from disk Redshift has to do, which is one of the slowest acts it has to perform. However if you encode the sort key (which Redshift will make recommendations for), then you actually cause Redshift to need to perform more reads from disk to find the blocks of data it‚Äôs looking for. I would have no expectation that a randomly selected engineer in the office would remember that ‚Äî it‚Äôs a deep bit of knowledge for data engineers, as the specialists in this area, to know and care about. But hey, if I‚Äôm on vacation, you can come look at the code and see that \"Sort key should have a raw encoding\" . Sometimes, that‚Äôs enough. Joining the human needs with the computer‚Äôs logic Instead of showing what I built with these foundational pieces next, let me show you the entry point for the SQL producer: I think this will give you a better idea of how the bridge from high level entry point to small, dedicated blocks of foundatal code were built and, even better, how they can be changed, extended, or added to over time, depending on what we need. We are what we repeatedly do. Excellence, therefore, is not an act, but a habit. [4] Our driver is incredibly simple: object Driver { val topics = Set ( schemaTopic , optimizeTableTopic , optimizeSchemaTopic ) def process ( topic : String , messages : List [ Message ]) : Unit = topic match { case _ if topic == schemaTopic => schemaChanges ( messages ) case _ if topic == optimizeTableTopic => optimizeTables ( messages ) case _ if topic == optimizeSchemaTopic => optimizeSchema case _ => Log . warn ( s \"Topic $topic does not have any supported actions.\" ) } def main ( args : Array [ String ]) : Unit = PipelineConsumer ( serviceName , topics , process ) } Similar to my writeup of our valve system , we make use of a Kafka consumer that we can pass in a function to execute against for each batch of messages it receives. This consumer, however, actually acts on multiple topics: one for Schema Registry changes (either new or updated schemas), one for optimizing a specific table, and one for optimizing a specific schema. The function we pass in to the Kafka consumer, then, is essentially just an orchestrator that immediately moves each batch of messages to the processor that‚Äôs designed for its topic. So, what does that processor look like? object Processor { private def warehouseSchema = Consul . valueFor ( \"data-warehouse\" , \"schema\" ) private def start ( action : String , about : String , metric : String ) : Unit = { Log . info ( s \"Going to $action `$warehouseSchema`\" ) Metric . attempting ( metric , warehouseSchema ) } private def finish ( action : String , about : String , metric : String , results : Try [ Unit ]) : Unit = results match { case Success ( _ ) => Log . info ( s \"Able to $action `$about`\" ) Metric . succeeded ( metric , s \"$about\" ) case Failure ( exception ) => Log . error ( s \"Unable to $action `$about`\" , exception ) Metric . failed ( metric , s \"$about\" ) Slack ( Publish . channelsDefault , s \":x: Unable to $action `$about` because of `${exception.getMessage}`\" , \":dna:\" , s \"SQL producer ($environment)\" ) } val schemaTopic : Topic = \"__schema\" private [ service ] def createTable ( schema : SchemaMessage ) : Unit = { val action = \"create table\" val metric = \"create.table\" start ( action , s \"$warehouseSchema.${schema.topic}\" , metric ) val migrations = CreateTable ( warehouseSchema , schema . topic ) val results = Try ( Publish ( migrations )) finish ( action , s \"$warehouseSchema.${schema.topic}\" , metric , results ) } private [ service ] def updateTable ( schema : SchemaMessage ) : Unit = { val action = \"update table\" val metric = \"update.table\" start ( action , s \"$warehouseSchema.${schema.topic}\" , metric ) val migrations = UpdateTable ( warehouseSchema , schema . topic , schema . version ) val results = Try ( Publish ( migrations )) finish ( action , s \"$warehouseSchema.${schema.topic}\" , metric , results ) } def schemaChanges ( messages : List [ Message ]) : Unit = { val ( newSchemas , updatedSchemas ) = messages . map ( _ . asInstanceOf [ SchemaMessage ]) . partition ( _ . isNew ) // if schema version == 1 newSchemas . foreach ( schema => createTable ( schema )) updatedSchemas . foreach ( schema => updateTable ( schema )) } val optimizeTableTopic : Topic = \"_optimize_table\" private [ service ] def optimizeTable ( table : Message ) : Unit = { val action = \"optimize table\" val metric = \"optimize.table\" start ( action , s \"$warehouseSchema.${table.topic}\" , metric ) val migrations = OptimizeTable ( warehouseSchema , table . topic ) val results = Try ( Publish ( migrations )) finish ( action , s \"$warehouseSchema.$table\" , metric , results ) } def optimizeTables ( messages : List [ Message ]) : Unit = messages . foreach ( message => optimizeTable ( message )) val optimizeSchemaTopic : Topic = \"_optimize_schema\" def optimizeSchema : Unit = { val action = \"optimize schema\" val metric = \"optimize.schema\" start ( action , s \"$warehouseSchema\" , metric ) val results = Try ( OptimizeSchema ( warehouseSchema )) finish ( action , s \"$warehouseSchema\" , metric , results ) } } There‚Äôs essentially five main groups of code within the processor: There‚Äôs getting the main warehouse schema our SQL producer is in charge of, which comes from Consul. The reason it‚Äôs a function that keeps getting the value is in case we change the schema; the long-lived SQL producer instance handles staying up to date so no one has to think to refresh it. There‚Äôs starting and finishing processing: log what doing, log what happened, incremement the correct metric, and possibly reach out on Slack to let humans know that there was a problem, this way we can act on bugs as soon as possible. There‚Äôs the processing block for schema changes , which includes figuring out if the schema is new or updated then acting on each accordingly before publishing any changes found. There‚Äôs the processing block for table optimizations , which checks the warehouse for any improvements to be made for the specified table and publishes what it finds. There‚Äôs the processing block for schema optimizations , which walks all the tables available in the schema to find any that can be improved before putting such requests into the pipeline to be consumed by the SQL producer later on to optimize each table. As you can imagine, this high level orchestration hides a lot of nitty-gritty complexity, but that is by design. The complex logic of what each input maps to as output is handled in either the small, foundational items or in the middle level of dedicated logic, both of which are heavily tested for every edge case we can think of or have encountered in the wild. Thus the orchestration is simple to read, simple to test (both automatically and manually, as live has its own set of problems ), and easy to drill into if there‚Äôs a bug to be tackled. Want to add a new service? It‚Äôs very clear how to do it. (I should state this code was recently refactored so its tidiness is due to that: if you build your own custom SQL producer and it looks much more messy, believe me ours was a mess too, thus the refactor. It just hasn‚Äôt had time to grow organically again quite yet.) What you‚Äôll notice is that each function essentially starts the action, hands off processing to a dedicated bit of logic that generates migrations, then publishes the migrations and finishes its work. The reason it ended up like this is that while the input and output for each service is nearly identical, the way the input is used to generate outputs varies wildly. Maintenance wise, this is a nice win, as we can choose to focus on either what all the services share or one specific service at a time in keeping the system up to date. But that does rather leave, ya know, the complex marriage of the input to its output left to implement. Detailed breakdown of the services available Walking you through each service in detail would be not just worthy of a blog post for each one, but possibly multiple blog posts for each! Instead I‚Äôll run you through the logic for each service, which is pretty unique to each technical landscape a SQL producer would be needed in. You might have different rules or opinions about, for example, a standard sort key than we do, and that‚Äôs fine: the point is just to get those rules or opinions into the code, so the system handles them for you. Creating a table The simplest service is, truly, the most foundational. Figure out if the table already exists. If it does, you‚Äôre done. Translate the Avro schema, in particular each field, to a Redshift table, in particular the columns. A good rule of thumb for encodings in a new table is set everything except the sort key and booleans to ZSTD ; leave the two exceptions as RAW . Later on you can optimize the encodings, once there‚Äôs data in the system, but until then this will work well enough. Updating a table In my opinion this is the most complex service; it is difficult for both humans and the system to get this sort of update right, which is why having the system helps: it might take a while to implement but then humans don‚Äôt have to worry about doing it themselves. Figure out if the table already exists. If it doesn‚Äôt, you‚Äôre done or create it, your choice. Get a copy of what it currently looks like in Redshift, what its previous Avro schema looked like , and what its current Avro schema looks like. Find the difference between the Redshift table and previous Avro schema compared to the current Avro schema. For each change, translate the difference into a block of SQL statements. You may want to issue multiple migrations for each block, depending on how you run migrations and what you feel comfortable with. By not just comparing the two schemas but also looking at the Redshift table, you find a lot of edge cases that are super easy to miss. There‚Äôs also certain changes in Avro that aren‚Äôt really as dramatic in Redshift, so you might be able to discard certain changes as not actually having any impact on Redshift. Optimizing a table Honestly, this is the most fun service, both in terms of writing it and, most importantly, in terms of benefiting from it. When you create or update a table, you‚Äôre making an educated guess on what to set the columns, sort, and distribution to be, but being able to go back and review those guesses when you have more information is fantastic. This is especially helpful if you have an existing warehouse with tables in a variety of states from a little out of whack to what the hell is happening here. Grab yourself a whole lot of metadata about the table in Redshift: what its column definitions look like, what its recommended encodings are, what it‚Äôs skew is, just about everything. (Appendix A contains more details about how to do this.) Using each bit of metadata, find each change you want to make. For each change, translate the update into a block of SQL statements. As with updating a table, you might want to issue multiple migrations. You can also recreate the table from scratch, moving the data from the old table to the new one, if you find it easier. (We do!) Obviously this service, unlike the Schema Registry centric ones, can be triggered by a human wanting to see if a table can be made better, for example a data analyst who is working with a table that‚Äôs super slow. We hooked our workflow system up to produce a message for this service whenever a human has a particular interest; otherwise, it tends to be requested by its companion‚Ä¶ Optimizing a schema This was the next step up from optimizing a table. Sure, an out of whack table should be optimized, but what is an out of whack table to optimize? Another flaw in the human character is that everybody wants to build and nobody wants to do maintenance. [8] Our workflow system, every week, triggers checking our main schema and picking up to so many tables to optimize for us. At first this produced the max number of migrations every time but now we‚Äôll go weeks without any optimizations, because the tables are kept so up to date and pristine. (The reason for limiting how many tables are optimized is purely so that humans aren‚Äôt flooded with too many pull requests, especially when we knew our warehouse had a lot of old tables that needed a lot of work.) Get yourself a list of tables to focus on. The way we do that is: Get all tables in the schema. Cross reference all of these tables with the Schema Registry, to verify they‚Äôre of interest to us and not a table in the wrong schema. Do some light metadata checking for if they‚Äôre poorly optimized (see Appendix A for detailed instructions on this). A deeper check will come later. If we have enough poorly optimized tables, focus on those; otherwise, take the list of all tables to do a more random dive. Shuffle the possible tables to focus on (so you don‚Äôt have a bias towards those early in the alphabet) and take twice the max number of tables you want to end up with. This limit is purely to speed up the system so you can change it as you‚Äôd like. Only keep tables that have at least 90 days worth of data. This is to ensure we don‚Äôt prematurely optimize a table. Check each table‚Äôs metadata more deeply, such as for incorrect sort keys, missing encodings, or skew. Only keep those that have some deep issue we think we can correct. Of all remaining tables, take the max number. For each table, produce a message to Kafka for the optimize table service about the table. While there is overlap in the metadata that the optimize table and optimize schema service review, breaking them down is both mentally easier to reason about and keeps the optimize schema request (which might issue some long running queries) moving along without timing out or making Kafka think it failed to consume a message. Like it did that one time where it spent all night issuing like a hundred PRs for the same table‚Ä¶¬†yeah don‚Äôt do that, make sure it can complete within the amount of time Kafka is giving it to say it‚Äôs done. Troubleshooting live Sixty years ago I knew everything; now I know nothing; education is a progressive discovery of our own ignorance. [4] As I alluded to above, no matter how much you test automatically, live has its own problems. Sometimes a new case for evolving a schema shows up, so you have to add in support to capture that in the future. Sometimes migrations make sense at each individual statement level but ultimately don‚Äôt add anything to the system, like making an already nullable column nullable, so you find ways to remove that code when the system sees such a migration since it has no actual ‚Äúchange‚Äù suggested. Sometimes Avro default types show up really heckin funny compared to what you thought they‚Äôd be, so you need to change the comparison logic to convert Avro‚Äôs NULL constant to a JVM null value. There will always be gaps ‚Äî¬†that‚Äôs fine. Because the Schema Registry only sees new or updated schema so often, it‚Äôs not as easy to test live as say walking a schema to find some tables to optimize, which we could hammer in both our lower and upper environments to see what happened. What I‚Äôd recommend for those schema-dependent services is: take every change that does happen and every little ‚Äúhmm‚Äù the migration or PR puts in you, and really ask yourself, ‚ÄúShould I do something here?‚Äù Even if it‚Äôs just a ticket you throw at the bottom of your backlog, having the example of here‚Äôs what happened, here‚Äôs what I‚Äôd expected to happen, here‚Äôs how this can be fixed ‚Äî¬†you‚Äôll probably see this problem again, so you‚Äôll be grateful you captured it. Those sorts of bugs might also be a great onboarding item for people new to the system who want to play around and get exposure to it. Optimizing a schema or tables, though, you can get wild! Since it has human triggers, and for us at least only posts to Slack for our team, we can run it whenever we want and then discuss very particular cases we either set up or found to figure out, ‚ÄúWhat is better here? How do we keep this data useful?‚Äù Invite feedback from others as well! We had an optimization for one of our largest tables, with its very thorough writeup in the PR, when fellow GCer Matt C pointed out that, if we had notes from the PR writeup in the SQL migration, we could comment on them specifically to have a deeper discussion. Brilliant! We have that now, just as a little comment at the end of each line for, if there was a change, what was the reason. The PR presents the full writeup, the SQL comments give you a place to drill in and figure out if this was the right decision. @gamechanger/data\n\n:wave: I've automatically created this migration for `public.sample_table` because I noticed it could be improved. :tada:\n\nHowever I can't do everything a human can, so I've noted the parts that need verification and possibly updating below along with what I did.\n\nI hope I did a good job! :blush: ## Table schema ### Description >This is a sample table for testing. ### Before * Sort: `test_column` * Distribution: `test_column` column | type  | nullable | encoding | comment |\n ------ | ----- | :------: | -------- | ------- | `row_id` | `BIGINT` |  | `ZSTD` | &empty; | `event_ts` | `TIMESTAMP` |  | `RAW` | &empty; | `test_column` | `CHARACTER VARYING(36)` |  | `LZO` | This is a test of the emergency broadcasting system. | `empty_column` | `CHARACTER VARYING(256)` | &check; | `ZSTD` | &empty; | ### After * Sort: `event_ts` * Distribution: `test_column` column | type  | nullable | encoding | comment |\n ------ | ----- | :------: | -------- | ------- | `test_column` | `CHARACTER(36)` |  | `ZSTD` | This is a test of the emergency broadcasting system. | `row_id` | `BIGINT` |  | `AZ64` | &empty; | `event_ts` | `TIMESTAMP` |  | `RAW` | &empty; | ## What changed? ### Type and length column | before | after | rationale |\n ------ | ------ | ----- | --------- | `test_column` | `CHARACTER VARYING(36)` | `CHARACTER(36)` | Max and min are same length | ### Dropped columns column | type | rationale |\n ------ | ---- | --------- | `empty_column` | `CHARACTER VARYING(256)` | Column contains no data | ### Encoding column | before | after | savings |\n ----------- | ------ | ----- | ------ | `row_id` | `ZSTD` | `AZ64` | Switch to recommended encoding with savings of 10.0% | `test_column` | `LZO` | `ZSTD` | Switch to recommended encoding with savings of 27.0% | ### Sort * Before: `test_column` * After: `event_ts` * Reason: Made `event_ts` the only sort key. ### Distribution * Before: `test_column` * After: `test_column` * Reason: Currently there is no logic to automatically change the distribution if required. ## When reviewing, please focus on: * Types and lengths changed, impacting * `test_column` * Columns were dropped, impacting * `empty_column` * Encodings changed, impacting * `row_id` * `test_column` * Distribution key, **human intervention is required** * Sort key changed, impacting * `event_ts` * `test_column` Sample PR writeup CREATE TABLE public . sample_table_temp ( test_column CHARACTER ( 36 ) NOT NULL ENCODE ZSTD , -- Corrected encodings that were mismatched, such as using tightest compression or not encoding the sort key. Optimize character column so it's just the size it needs to be. row_id BIGINT identity ( 0 , 1 ) PRIMARY KEY NOT NULL ENCODE AZ64 , -- Corrected encodings that were mismatched, such as using tightest compression or not encoding the sort key. event_ts TIMESTAMP NOT NULL ENCODE RAW ) DISTSTYLE KEY DISTKEY ( test_column ) -- Currently there is no logic to automatically change the distribution if required. COMPOUND SORTKEY ( event_ts ); -- Made `event_ts` the only sort key. INSERT INTO public . sample_table_temp ( test_column , event_ts ) ( SELECT test_column , event_ts FROM public . sample_table ); DROP TABLE public . sample_table ; ALTER TABLE public . sample_table_temp RENAME TO sample_table ; DELETE FROM metadata . comments WHERE schema_name = 'public' AND table_name = 'sample_table' AND column_name = 'empty_column' ; GRANT ALL ON public . sample_table TO GROUP human_users ; GRANT SELECT ON public . sample_table TO GROUP system_users ; ANALYZE public . sample_table ; Sample SQL migration And as always, do be sure to include a wide variety of emojis in your PRs. The PR might be from some code but that code is still, in this instance, a teammate doing their best. Final thoughts Life can only be understood backwards; but it must be lived forwards. [3] Converting the embedded SQL generator to a stand alone SQL producer probably struck outside people as a weird thing to give attention to: after all, the current thing works fine enough, so like‚Ä¶¬†who cares? Well, ‚Äúworks fine enough‚Äù isn‚Äôt the same as ‚Äúworks.‚Äù We were relying on it more and more as a company, all while it became harder to maintain and missed more edge cases. The long turn around was causing ongoing confusion. The Hack Day project in Python that the SQL generator had started out as needed to, finally, become a true production-ready system. It‚Äôs a big system, bigger than the valve; its Python implementations hid how complex it was. I like to say that while the valve is complicated to explain, it‚Äôs got a simple implementation ‚Äî the SQL producer is the reverse. You really become aware of how much you know and how many heurestic rules you use to do this sort of work once you start getting it down into code with numerous tests to verify everything. Even within the team, there were differences in what we looked for and how we decided what to do with the same information. But it‚Äôs a great system: it‚Äôs a second example of Scala and Kafka consumers, it reacts quickly (great for inspiring more streaming ideas), and it allows humans to not even have to think about it or the problems it addresses. If you‚Äôre needed, a PR will tag you and Slack will have a message; otherwise, you keep doing your thing. Truthfully, it‚Äôs been one of my favorite systems to work on, even when it aggrevates me to no end. It combines so many different pieces (Kafka, Avro, Schema Registry, Redshift, SQL) in a way that makes sense and relieves the burden of work on me. I used to spend a lot of time creating, updating, and optimizing tables, which led to lots of mistakes no one caught or lots of tradeoffs because I didn‚Äôt have the time ‚Äî¬†no more! :tada: And it shows how the implementation language can impact the implementation you produce: you might start off picking what everyone is most comfortable with but ultimately you‚Äôll need to use what‚Äôs the right language or framework or set of tools for the problem at hand, otherwise you‚Äôll have friends for \"‚àÖ‚¶∞‚¶±‚¶≤‚¶≥‚¶¥\" . You don‚Äôt want friends for \"‚àÖ‚¶∞‚¶±‚¶≤‚¶≥‚¶¥\" . You do, however, want automated PRs with emojis. Trust me, it‚Äôll make you smile every time. Appendix A: Redshift optimization queries Please read my crash course to Redshift for a more dedicated walkthrough of Redshift basics and early optimizations to focus on . A lot of the queries included below are described there in more detail for newer Redshift users. Our code uses the AWS Redshift JDBC driver for Java without the AWS SDK but any Postgres connector should work. I‚Äôm providing the queries as Scala strings with basic interpolation, so it‚Äôs obvious what values need to be passed in from the service running these queries. You parameterize your queries as you like though for production systems. Also, because the JDBC returns truly the funkiest data structure anyone has seen, here‚Äôs the StackOverflow you‚Äôd probably search the Internet for about turning the JDBC results into a Scala map along with the realization of it we use, you‚Äôre welcome . (Yes, we do link to the answer directly in our code, you should too.) Assuming your JDBC results are stored in a results variable: // https://codereview.stackexchange.com/questions/64662/realizing-a-sql-resultset-into-a-map-in-scala Iterator . continually ( results . next ) . takeWhile ( identity ) . map ( _ => ( for ( column <- columns ) yield column -> results . getObject ( column ) ). toMap ) . toSet This returns a set of maps, where each element in the set is a row and each map is the column to value of that row. Highly recommending setting type WarehouseRecord = Map[String, Any] and type WarehouseResults = Set[WarehouseRecord] to make it just that bit more obvious, even if Scala doesn‚Äôt yet have opaque type aliasing . Schema of a table s \"\"\"SELECT * \nFROM pg_table_def \nWHERE \n\tschemaname = '$schema' \n\tAND tablename = '$table'\"\"\" While this query is great to get an overview of what the table currently looks like, we‚Äôve also found it helpful in seeing if a human already updated a table ahead of the system or if the ‚Äúrevised‚Äù table the system will suggest a migration for is actually that different from the table right now. Metadata about a table on disk First execute s \"ANALYZE $schema.$table;\" to refresh Redshift‚Äôs metadata, then execute s \"\"\"SELECT\n    results.rows AS numRows,\n    tableInfo.unsorted AS percentUnsorted,\n    tableInfo.size AS sizeOnDiskInMB,\n    tableInfo.max_varchar AS maxVarCharColumn,\n    tableInfo.encoded AS encodingDefinedAtLeastOnce,\n    tableInfo.diststyle AS distStyle,\n    tableInfo.sortkey_num AS numSortKeys,\n    tableInfo.sortkey1 AS sortKeyFirstColumn\nFROM SVV_TABLE_INFO AS tableInfo\nLEFT JOIN STL_ANALYZE AS results\nON results.table_id = tableInfo.table_id\nWHERE\n    tableInfo.schema = '$schema'\n    AND tableInfo.table = '$table'\nORDER BY results.endtime DESC\nLIMIT 1;\"\"\" to get the latest metadata for yourself. The results tell you things like if you‚Äôre missing encodings (bad), the size on disk (to determine how much of an impact tweaking this table might have), and what your sort and distribution currently look like. Great for both ‚Äúwhat do we fix?‚Äù and ‚Äúwhat is the benefit of doing the fix?‚Äù Skew of a table across the cluster s \"\"\"SELECT skew_rows\nFROM svv_table_info\nWHERE\n\tschema = '$schema'\n\tAND \"table\" = '$table';\"\"\" This is a handy one I learned while looking for ways to automate distribution suggestions. Skew can be particularly hard to spot as the table needs time to accumulate data before a bad distribution style or key becomes evident. Ideal skew is 1.0 ; we choose to recommend distribution optimization on any table with skew of 3.0 or higher. Like golf, lower is better here. Recommended encodings of an established table s \"ANALYZE COMPRESSION $schema.$table\" I have seen Redshift recommend we bounce a particular column between two encoding types, over and over, so we tend to only use a recommendation if there‚Äôs other changes we‚Äôre making or the change will save us a minimum amount of space on disk. You can combine this with metadata about the table‚Äôs size on disk to figure out if there‚Äôs enough savings to make it worth it: def encodingsIndiciateOptimize ( schema : String , table : String , eventKey : String , diskSavingsMinimum : Double ) : Boolean = { Redshift . recommendedEncodingsFor ( schema , table ) match { case Success ( results ) => val sizeOnDisk = Redshift . execute ( s \"\"\"SELECT tableInfo.size AS sizeOnDiskInMB\nFROM SVV_TABLE_INFO AS tableInfo\nLEFT JOIN STL_ANALYZE AS results\nON results.table_id = tableInfo.table_id\nWHERE\n    tableInfo.schema = '$schema'\n    AND tableInfo.table = '$table'\nORDER BY results.endtime DESC\nLIMIT 1;\"\"\" ) . get . head ( \"sizeOnDiskInMB\" . toLowerCase ) . asInstanceOf [ Long ] val savings = results . filter ( _ . column != \"row_id\" ) // our surrogate primary key . filter ( _ . column != eventKey ) // our standard sort key . map ( result => result . savings / 100.0 * sizeOnDisk ) . fold ( 0.0 )( _ + _ ) savings >= diskSavingsMinimum case Failure ( _ ) => false } } We look for at least 25 GB of savings typically, to ensure doing the work is worth it, but we might drop the amount soon as all of our really poorly encoded tables have already been found. (For a really thrilling/terrifying warehouse, you might want to start higher to focus on the biggest wins possible with encodings, especially if you‚Äôre trying to build an argument for spending time optimizing tables by hand or building out your own automation. Tweaking two tables for us one time saved us terabytes of data and sped just about every query in the warehouse up.) Max and min length of varying character columns s \"SELECT \n\tMAX(OCTET_LENGTH(${column.name})) AS max, \n\tMIN(OCTET_LENGTH(${column.name})) AS min \nFROM $schema.$table;\" This query actually let‚Äôs us do a couple of things: if both lengths are 0 , the column is empty so can possibly be dropped if both lengths are the same, we can convert a VARYING CHARACTER column to a CHARACTER column if the max is under where the schema indicates we set the limit, we can lower it to something more realisitic We use powers of 2 to make a recommendation, such as a column with a max value length of 92 characters being set to allow a max of 128 characters instead of 256 or 1024 characters. This is less for performance and more for, when a human looks at a column, having a vague idea of how much shtuff each value contains. A field called ‚Äúname‚Äù that‚Äôs 1024 characters wide is a weird thing to find in the wild; a field called ‚Äúname‚Äù that‚Äôs 64 characters wide makes more sense mentally. (If you‚Äôre wondering with we use OCTET_LENGTH in this query: emojis.) Tables that truly need your love and attention I‚Äôm not going to pretend to fully understand the following query; the Redshift Advisor suggested it for finding what they considered poorly optimized tables. What is helpful about this query (which I‚Äôm sure AWS has an explanation for somewhere though I‚Äôve tweaked it a bit) is that it surfaces tables that truly need your love and attention as soon as you can give it to them. Even if you‚Äôre not going to have your SQL producer optimize tables, this is helpful for a human to use to find where to look in Redshift and put attention. s \"\"\"SELECT DISTINCT ti.\"table\" AS \"table\" \nFROM svv_table_info AS ti\nLEFT JOIN(\n    SELECT\n        tbl AS table_id,\n        COUNT(*) AS size\n    FROM stv_blocklist\n    WHERE (tbl, col) IN (\n        SELECT\n            attrelid,\n            attnum - 1\n        FROM pg_attribute\n        WHERE\n             attencodingtype IN (0,128)\n             AND attnum > 0\n             AND attsortkeyord != 1\n    )\n    GROUP BY  tbl\n) AS raw_size USING (table_id)\nWHERE\n    raw_size.size IS NOT NULL\n    AND (\n      raw_size.size > 100\n      OR skew_rows > 3.0\n    )\n    AND ti.schema = '$schema'\n;\"\"\" Appendix B: select Github logic We use this Github Java driver for interacting with the Github API but others are available, both natively in Scala and Java. The Github API has a lot of power but can be hard for a new person to wrap their head around, thus why I am providing our code essentially as-is. (Also shoutout to GC alumni Hesham, now at Github, who helped me debug my problems and make my ideas a reality!) With this base, you should be able to tweak anything to match your needs while also finding other functionality to add following a similar pattern. Our setup involves connecting to a specific repo using an access token but you can make it more generic if necessary. We also use some established values like defaultBaseBranch and pathToMigrations (since this system explicitly puts out migrations) which can be easily swapped out for your specific needs or, again, made more generic. class Github ( accessToken : String , repoName : String ) { private val repo = new GitHubBuilder () . withOAuthToken ( accessToken , organization ) . build . getRepository ( s \"$repoName\" ) def getBranch ( branchName : String ) : GHRef = repo . getRef ( s \"heads/$branchName\" ) private def makeBranch ( branchName : String ) : GHRef = { val base = repo . getRef ( s \"heads/$defaultBaseBranch\" ) val baseSha = base . getObject . getSha repo . createRef ( s \"refs/heads/$branchName\" , baseSha ) } /** If the branch does not yet exist, create it. If it does exist, it can be created again or returned as is. */ def createBranch ( branchName : String , deleteIfExists : Boolean = false ) : GHRef = { if ( deleteIfExists ) deleteBranch ( branchName ) Try ( getBranch ( branchName )) match { case Success ( branch ) => branch case Failure ( _ ) => makeBranch ( branchName ) } } def deleteBranch ( branchName : String ) : Unit = Try ( getBranch ( branchName )). map ( branch => branch . delete ) private def getMaxMigrationNumber : Int = repo . getDirectoryContent ( pathToMigrations ) . asScala . map ( content => content . getName ) . filter ( _ . startsWith ( \"V\" )) . map ( _ . stripPrefix ( \"V\" )) . map ( _ . split ( \"__\" )( 0 )) . map ( _ . toInt ) . max def commit ( migration : Migration , deleteBranchIfExists : Boolean = true ) : String = { val branch = createBranch ( migration . branchName , deleteBranchIfExists ) var nextMigrationNumber = getMaxMigrationNumber + 1 migration . migrations . foreach ( sql => { repo . createContent . branch ( migration . branchName ) . path ( s \"${pathToMigrations}V${nextMigrationNumber}__${migration.prType.toString}_${migration.schema}_${migration.table}.sql\" ) . content ( sql ) . message ( s \"Creating migration to ${migration.prType.toString} ${migration.schema}.${migration.table} in warehouse\" ) . commit nextMigrationNumber += 1 }) migration . branchName } /** If a branch has had commits since a given datetime. */ def hasCommitsSince ( branchName : String , since : Date ) : Boolean = repo . queryCommits . from ( branchName ) . since ( since ) . list . toList . size > 0 def makePullRequest ( branchName : String , migration : Migration ) : URL = new URL ( repo . createPullRequest ( s \"${migration.prTitle} ($environment)\" , branchName , defaultBaseBranch , migration . prDescription , true ) . getIssueUrl . toString . replace ( \"api.\" , \"\" ) . replace ( \"repos/\" , \"\" ) . replace ( \"issues\" , \"pull\" ) ) def getContentsOf ( branchName : String , path : String ) : String = repo . getFileContent ( path , s \"heads/$branchName\" ) . getContent def updateContentsOf ( branchName : String , path : String , newContent : String , commitMessage : String ) : Unit = repo . getFileContent ( path , s \"heads/$branchName\" ) . update ( newContent , commitMessage , branchName ) } I felt bad there were no images in this post so here‚Äôs a kitten, thank you for making it this far. :bowing_woman: Footnotes Italo Calvino, If on a Winter‚Äôs Night a Traveler . Federico Fellini. S√∏ren Kierkegaard‚Äôs journals. Will Durant. Antoine-Laurent de Lavoisier, Elements of Chemistry . Frederick P. Brooks Jr., Mythical Man Month . Edna St. Vincent Millay, Renascence and Other Poems . Kurt Vonnegut, Hocus Pocus . Share on Twitter Share on Facebook", "date": "2021-05-07"},
{"website": "GameChanger", "title": "From pipeline to beyond", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/from-pipeline-to-beyond/", "abstract": "An overview of off-the-shelf solutions for moving data out of Kafka, problems we had making those systems work, and how we wrote our own solution and stood it up for those in a similar situation. You cannot know everything a system will be used for when you start: it is only at the end of its life you can have such certainty. [1] Many moons ago I wrote about our design for upgrading our data pipeline , which lightly touched on how we‚Äôd move data out of our pipeline ( Kafka ) to downstream systems, namely our data warehouse and data archive. At that time we hadn‚Äôt really been able to dive into focusing on getting the data out of Kafka, because getting data in to Kafka is often much more custom and complex, and we thought we‚Äôd be able to use an off the shelf solution like Kafka Connect to move data out, don‚Äôt even worry about it. We were, uh ‚Äî¬†we were wrong. Let me take you on our journey, in case you‚Äôre on this journey too. The problem space Solution 1: Kafka Connect Solution 2: Secor or Gobblin Solution 3: we‚Äôll do this ourselves Tangent: naming things How you can do this yourselves, code edition How you can do this yourselves, infrastructure and metrics edition Takeaways The problem space Programmers are not to be measured by their ingenuity and their logic but by the completeness of their case analysis. [4] At a high level, the problem we needed a solution for was as follows: Data enters the data pipeline from numerous backend systems. This crossover point is producers into the pipeline , which we‚Äôd already implemented. Data from the data pipeline needs to move into the data warehouse and the data archive . This crossover point would be a consumer on the pipeline . Ideally we‚Äôd like the same consumer for both needs that we can simply configure differently. We want to preserve our data‚Äôs Avro format along side its schemas. This would allow every system that interacts with the data to use the same language. We want to write our data to S3 . data warehouse : This will be our holding tank where data waits between extraction and loading. ( More on this name later. ) data archive : This will be our archive where data waits until we maybe, one day, thaw it, should it be needed again. We‚Äôd like to store data based on a timestamp within the event. data warehouse : We want to use the processing time. data archive : We want to use the event time. That last item is a really subtle, nuanced one so let me give an example of what I‚Äôm talking about: say there was some sort of outage on April 2nd, 2019. Maybe an upstream system hit two unrelated bugs that caused about fifeteen minutes of downtime when they joined forces, you know the kind I mean. The user impact was resolved quick as can be, all of the systems seemed to go back to normal, we all went along with our lives. Then, in March of 2021, one of our data scientists realizes we‚Äôre missing this block of data and it‚Äôd be great if we could grab it for them, to fill in this gap for their work. Would that be possible? They‚Äôre doing some sort of deep dive so having all the data they can would be :kissing_heart::ok_hand: but no worries if that‚Äôs not possible; data will always be imperfect. Sure thing, we can grab that data! We go to the upstream system and replay the fifteen minutes of missing data, on March 28th, 2021. (I know that implies someone was working on a Sunday but it was my birthday so let me have it.) The data goes into the data pipeline again, ready to move to its downstream systems like nothing ever happened. Where does that data land? Well for our data warehouse, the data lands in the holding tank to wait to be moved into the warehouse. Because it was processed recently, we put it with other March 28th, 2021 data. For our data archive though, it‚Äôs a bit different, as this data was processed recently but is about April 2nd, 2019: you can see that in its event time. If we ever wanted to go back and look at all April 2019 archived data ‚Äî¬†say for a legal or security request ‚Äî¬†but the backfilled data wasn‚Äôt there with its friends, we‚Äôd miss that when we pull data from the archive. And what‚Äôs the point of a data archive if you can‚Äôt trust that the data is neatly organized to pull from? Our data archive passed 50 TB a long time ago, we can‚Äôt sift through it easily by hand, nor can we pull all data from April 2019 until now to compensate for bad sorting in fear that we‚Äôd miss something important. That‚Äôs no way to live your life. And that is why we need to store data in the two systems based on different timestamps within the events. We know both fields will always be available (it‚Äôs a requirement for our producers to put data into the pipeline) so want to take advantage of that to make sure we keep the data archive tidy and usable. When you shove things in a freezer without labels, you often don‚Äôt know what food it is ‚Äî¬†same problem here when we‚Äôre trying to figure out what data we need to thaw from the archive. So‚Ä¶¬†how do we tackle this problem? Solution 1: Kafka Connect Good judgement comes from experience, and experience comes from bad judgement. [2] If you Google for how to solve this problem (which is possibly how you came across this writeup), the Internet will tell you to use Kafka Connect . And that makes sense: I love being able to use the native, built in solutions a system is meant to work with. It makes your life easier, it‚Äôs less maintenance, you‚Äôll probably have much more community support ‚Äî¬†it‚Äôs a win all around! And Kafka Connect can both write data into Kafka and write data out from Kafka , so you can use it in multiple places. We stan a solution that just makes sense. #!/bin/bash ENVIRONMENT = \"production\" KEY_WANT = \"archive_bucket\" BUCKET_NAME = $( curl https://consul/v1/kv/data-pipeline/ ${ ENVIRONMENT } / ${ KEY_WANT } | jq '.[0].Value' | sed -e 's/^\"//' -e 's/\"$//' | base64 --decode ) curl -X PUT -H \"Content-Type: application/json\" --data '{\n   \"tasks.max\": 15,\n   \"errors.tolerance\": \"all\",\n   \"errors.log.enable\": true,\n   \"errors.deadletterqueue.context.headers.enable\": true,\n   \"errors.deadletterqueue.topic.name\": \"__deadletter_archiver\",\n   \"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",\n   \"storage.class\": \"io.confluent.connect.s3.storage.S3Storage\",\n   \"s3.region\": \"us-east-1\",\n   \"s3.bucket.name\": \"' ${ BUCKET_NAME } '\",\n   \"s3.part.size\": \"5242880\",\n   \"topics.dir\": \"\",\n   \"format.class\": \"io.confluent.connect.s3.format.avro.AvroFormat\",\n   \"schema.compatibility\": \"BACKWARD\",\n   \"partitioner.class\": \"io.confluent.connect.storage.partitioner.TimeBasedPartitioner\",\n   \"timestamp.extractor\": \"RecordField\",\n   \"timestamp.field\": \"event_ts\",\n   \"partition.duration.ms\": \"7200000\",\n   \"rotate.schedule.interval.ms\": \"7200000\",\n   \"flush.size\": \"1000\",\n   \"path.format\": \"' \\' 'year' \\' '=YYYY/' \\' 'month' \\' '=MM/' \\' 'day' \\' '=dd/' \\' 'hour' \\' '=HH\",\n   \"file.delim\": \".\",\n   \"topics.regex\": \"^[^_].*\",\n   \"locale\": \"en\",\n   \"timezone\": \"UTC\"\n}' http://localhost:8083/connectors/archiver/config A Kafka Connect job assignment we used, here for our production archiver Except‚Ä¶¬†people have issues with Kafka Connect writing data to S3. There‚Äôs just something about that combination that causes it to have issues. I‚Äôll leave you to decide whose accounts online you‚Äôd like to read, as it seems we all encountered different though related problems, but what we found over the nearly year and a half we had Kafka Connect running was it would run out of memory no matter what we did, how we changed the configurations, how much memory we gave it, and how we limited the data it was processing. From people who run the system in production to the main authors of the system, no one could figure out or explain why . Real talk: I‚Äôm not hear for that sort of mystery in my systems, especially ones as critical as this one would be. You shouldn‚Äôt be either, you deserve better. When you work with bounded data ‚Äî a word here which means a finite set of data ‚Äî you can do things like batch the data for processing and design your solution around the specifics of the data set. But when you‚Äôre processing unbounded data ‚Äî¬†infinite data that just keeps coming, at higher or lower volumes, depending on factors outside your control ‚Äî¬†you have to be able to handle those ups and downs gracefully. To understand the limitation of things, desire them. [8] We wanted a streaming solution ‚Äî one designed for unbounded data sets ‚Äî to ensure both that we were moving data as quickly as we could, to deliver insights as quickly as we could, but also to ease the maintenance. It might sound strange, as you‚Äôd think a streaming solution would be hella more work to maintain in the long term than a batch system, but in this case it was quite the opposite. The system we‚Äôd used in our previous data pipeline was batch based but it was hard to onboard new people to, required a lot of deep knowledge of areas like MapReduce to reason about, and if it got a little bit behind ‚Äî oof, forget about it, once it hadn‚Äôt run for four hours you were about to be in for punishment. One time it was down over the weekend and took a whole week (maybe longer?) to catch up, with single batch runs taking 36+ hours before failing out. Our team took turns having to watch it and keep it running at all hours of the day, and we still had data loss because we weren‚Äôt fast enough despite throwing all the hardware we could at it. There will be large volumes of data, and the data will keep coming. A streaming solution is the right way forward, but Kafka Connect wasn‚Äôt ready for the needs we had. So we started to look elsewhere for what other solutions there were. The Internet warned us to write our own solution, but the hope persisted that in this one area, a system already existed that we could use. Solution 2: Secor or Gobblin My gracious data engineering partner-in-crime Emma took the lead on looking into two other solutions we‚Äôd come across that weren‚Äôt as popular as Kafka Connect but could maybe, still, be made to work. I‚Äôd like to thank her for her dedication in the great Battle To Make These Systems Work; she was a brave soldier who fought a good fight, which I would not have been able to make it through. That‚Äôs what I consider true generosity: you give your all, and yet you always feel as if it costs you nothing. [7] Secor was our first contender. It‚Äôs from Pinterest, who has some hecka big Kafka clusters, so I trust that their Kafka experts understand what it‚Äôs like to run these systems in production in the long term. It was easy enough to get started with but we couldn‚Äôt get it to behave as we expected, similar to Kafka Connect. We were seeing strange issues related to codecs, especially around Avro, so someone moving around data without Avro might not face the same challenges. But this for us was a very important requirement so forced us off of Secor to our next item. Gobblin is the successor to the previous system we‚Äôd used for this need, Camus , and yes that does mean we were running Camus years after it was sunsetted, yes it was a scary time I‚Äôd like to not think about again. We had thought that Gobblin, while more a batch design, being the evolution of the system we already used, perhaps this would be easy enough to set up and we could tweak it over time. We were still hopeful. Uh, yeah, so ‚Äî¬†no. I‚Äôm sorry Emma! We spent a week or two intensively working on just seeing if we could make this work ‚Äî¬†not meet all our needs, purely work . We had trouble with the Avro time partitioning, which had been so easy with Camus. We had constant issues with Gobblin‚Äôs Maven dependencies, which was not expected. We couldn‚Äôt set the S3 path where the files would be, when it did occasionally put out three files before crashing. An architect‚Äôs first work is apt to be spare and clean. He knows he doesn‚Äôt know what he‚Äôs doing, so he does it carefully and with great restraint. As he designs the first work, frill after frill and embellishment after embellishment occur to him. These get stored away to be used ‚Äúnext time.‚Äù Sooner or later the first system is finished, and the architect, with firm confidence and a demonstrated mastery of that class of systems, is ready to build a second system. This second is the most dangerous system a man ever designs. [2] Yeah, so, the Internet was right all along that we‚Äôd need to write our own solution. Would it be something we‚Äôd need to maintain? Yes. But would it at least work? We had to hope so! Solution 3: we‚Äôll do this ourselves At this point, we had a data pipeline being held up by the need to do two simple, nearly identical things: move data from the pipeline to the archive, and move data from the pipeline to the holding tank. That was it, these were S3 locations, we wanted Avro files, let us specify the timestamps ‚Äî¬†nothing crazy or out of the ordinary. Simple needs, all configuration based, but none of the off the shelf solutions would work for us and we needed to deliver . So we wrote our own solution. I will certainly not contend that only the architects will have good architectural ideas. Often the fresh concept does come from an implementer or from a user. However, all my own experience convinces me, and I have tried to show, that the conceptual integrity of a system determines its ease of use. Good features and ideas that do not integrate with a system‚Äôs basic concepts are best left out. [2] Before I walk you through the solution in detail, let me explain how it came about. See, when Emma and I were discussing what we‚Äôd do knowing we needed to write our own implementation, we decided we‚Äôd write a Scala consumer using the Kafka native consumer library. Avro and the Schema Registry were also easy enough to throw into the mix, grabbing from Hack Day projects and other one-off scripts we‚Äôd used over the years. We decided we‚Äôd pair program the implementation and gave ourselves a few days, as I was more familiar with Scala and had a stronger sense of what we‚Äôd want to achieve, but we both agreed we needed more than one person to understand the system and be able to maintain it. We cleared our calendars, we made sure people knew we‚Äôd be doing Big Things, and then we settled in for several days of intense work. I‚Äôd say about, oh, 40 minutes in, we had 90% of the logic down. :woman_facepalming: A setback has often cleared the way for greater prosperity. Many things have fallen only to rise to more exalted heights. [6] Is our solution going to win any awards? No, but that‚Äôs fine. We‚Äôre not going for a clever solution; in fact, I explicitly didn‚Äôt want a clever solution. A clever solution isn‚Äôt obvious, a clever solution uses slight of hand and is hard to maintain. What I wanted was a smart solution, a solution that when you look at it you say, ‚ÄúWell of course,‚Äù as if there was no other way to logically do this, duh. A smart solution should be so obvious it‚Äôs almost painful that you had other iterations before it, but it can take a lot of deep knowledge and work to arrive at such a solution. A smart solution hides the sweat and toil that went into it, but it is the right solution in the long run. We can replace our solution at any time . We can add to our solution at any time. Anyone on the team can poke about the solution. The solution doesn‚Äôt need constant attention . It just is . And that‚Äôs what we‚Äôd been after this whole time. Tangent: naming things But of course, we all know the two hardest problems in computer science are naming things, cache invalidation, and off by one counting errors. However I‚Äôve found a nifty way of naming things that helps to attack this NP-hard problem: poll your coworkers! Not just your team but the whole company! Let everyone brainstorm, especially people who aren‚Äôt deeply in the thick of the implementation so can think more freely about what the system is, so it‚Äôs name makes sense to them. Ultimately, the most important part of naming things is just that: meaningful names. As with clever vs smart solutions, there‚Äôs a difference in a clever vs smart name, and once you see the smart name, people will assume that was the obvious name from the start. I don‚Äôt care if you know what a functor is so long as you can see one in the wild and go, ‚ÄúYeah I get how that works.‚Äù There shouldn‚Äôt be barriers to entry making in groups for who understands a system and who doesn‚Äôt. What‚Äôs the point? You‚Äôve made people feel stupid and excluded, you‚Äôve limited who can and will help you, and for what? A smart name is meaningful to everyone who wants to come along and go, ‚ÄúWhat‚Äôs this? Tell me more.‚Äù I‚Äôve polled people in the past to great results: when we needed a name for the data store where we kept extracted data from the pipeline before loading it into the warehouse, a teammate suggested ‚Äúholding tank.‚Äù Beautiful! Love it! Never been done before! It‚Äôs the tank of data that holds stuff waiting to move. It‚Äôs specific, it‚Äôs meaningful, it‚Äôs smart naming. The finest of pleasures are always the unexpected ones. [9] So off went the poll for this system: we‚Äôre creating a system to sit between the pipeline and the holding tank/archive. It‚Äôll regulate moving data from the one to the other. What should we call it? Shoutout to TJ for suggesting ‚Äúvalve,‚Äù a beautiful, succinct name that just fits ‚Äî¬†so obviously, it‚Äôs like it was always meant to be. We had a name, we had a plan, we had a language in hand. Time to implement! How you can do this yourselves, code edition A language that doesn‚Äôt affect the way you think about programming is not worth knowing. [4] Our solution being in Scala had its up and down sides. The upside is, Kafka is very much JVM first and written in a mixture of Java and Scala, so there‚Äôs lots of examples online and helpful libraries you can use. Scala also allows you to easily use higher order functions and, being heavily typed, allows you to stub out functions you‚Äôll come back to later. I often find functional Scala works beautifully with test-driven development because you can set up these basic pieces and then, truly, treat each implementation like a black box that just needs to meet your test‚Äôs expectations. Having these tests then act as documentation as well as specifications, especially with a given/when/then format which neatly breaks down the test into preconditions, the action being tested, and the postconditions. test ( \"get value that exists\" ) { Given ( \"a Consul directory\" ) val directory = \"data-consul-test-directory\" assume ( directory . nonEmpty ) assume (! directory . endsWith ( \"/\" )) And ( \"a Consul key\" ) val key = \"test_key_1\" assume ( key . nonEmpty ) When ( \"I get the value from Consul\" ) val result = Try ( Consul . valueFor ( directory , key )) Then ( \"I am successful\" ) assert ( result . isSuccess , s \"Result should have been a success but was a failure: $result\" ) And ( \"the value I expect comes back\" ) val expected = \"this is a test key\" assert ( result . get == expected , s \"Result `${result.get}` should have matched expected `$expected` but didn't.\" ) } An example Scala test using given/when/then that was set up before the code it tests was written. The downside is Scala has a hella steep learning curve. Once you‚Äôre in it, it‚Äôs pretty smooth sailing, but getting to that point can be difficult. For that reason, I‚Äôm going to throw a lot of code at you to assist you and give you a starting point to jump off from. If you want to translate it to Java, go for it, do what makes the most sense for you! If you want to do it in another language, again get wild ‚Äî¬†you‚Äôll have to live with the implementation and maintenance, so make the choice that is right for you and your team. It is exciting to discover electrons and figure out the equations that govern their movement; it is boring to use those principles to design electric can openers. From here on out, it‚Äôs all can openers. [3] Domain specific language Our DSL package contains the building block data structures we use throughout the system. For us, this was most importantly the message data structure to contain our input we take in from the pipeline. /** Represents a neat, cleaned up message from the pipeline.\n  * \n  * @param schema The processed schema that represents this event.\n  * @param message The contents of this event.\n  * @param raw The original, raw contents of this event.\n  * @param timestamp The timestamp for when this event was from, if available.\n  * @param partition The partition for where this event was from, if available.\n  * @param offset The offset for where this event was in the pipeline, if available.\n  */ case class Message ( schema : ProcessedSchema , message : Map [ String , Any ], raw : GenericRecord , timestamp : Option [ Long ] = None , partition : Option [ Int ] = None , offset : Option [ Long ] = None ) { /** Returns the timestamp for this message based on the extraction key provided.\n    * \n    * @param key The key in the message to get the value with.\n    * @return Formatted string of the value retrieved.\n    */ def extractionStringOf ( key : String ) : String = { require ( message . keys . toSet . contains ( key ), { s \"Cannot extract string using key `$key` when not in message: ${message.keys}\" }) val extractionTimestampString = message ( key ). toString val extractionTimestampDate = new DateTime ( extractionTimestampString , DateTimeZone . forID ( \"UTC\" )) Message . dateFormatter . print ( extractionTimestampDate ) } } The ProcessedSchema type here is just a wrapper around the raw Avro schema with some key metadata made easily available, like the name of the schema and a list of its field names, mostly for debugging purposes. Avro is a very concise format, which is great for sending and storing large volumes of messages but not so easy for human eyeballs to make sense of! One of the big pieces of the message we‚Äôll need for our system is getting the datetime string from the input that we want to use for where we put the message: if it‚Äôs from April 2nd, 2019 at 14:00, we need to pull out 2019-04-02 14:00:00 from somewhere to know that! Which key we use is configured when the system gets its assignment, but you can always have more logic around determining that if you want. The formatter can also be made specific to your needs; we use year=YYYY/month=MM/day=dd/hour=HH for historical reasons but you can use whatever you like. Another key data structure is the messages we‚Äôll output to our deadletter queue about problems the system encountered. This wraps the input message but also provides additional information for debugging later like the error we saw, when we had the issue, who specifically had the problem (in case you have multiple systems outputting deadletters), whatever it is you‚Äôd want to know to work on the problem. /** Deadletter for the deadletter queue, ie message that could not be processed and instead has an error associated with it that requires further investigation.\n  * \n  * @param message Message that could not be processed.\n  * @param exception Error encountered.\n  * @param processedAt When the message was being processed.\n  * @param processedBy Name of who was processing it.\n  */ case class Deadletter ( val message : Message , val exception : Throwable , val processedAt : DateTime , val processedBy : String ) { lazy val asJson : Map [ String , Any ] = Map [ String , Any ]( \"processed_at\" -> Deadletter . dateFormatter . print ( processedAt ), \"processed_by\" -> processedBy , \"exception\" -> exception . getMessage , \"topic\" -> message . schema . topic , \"message\" -> message . message , \"timestamp\" -> message . timestamp . getOrElse ( null ), \"partition\" -> message . partition . getOrElse ( null ), \"offset\" -> message . offset . getOrElse ( null ) ) } A nice side effect of including the original input is, if we resolve a bug, we can later on send the impacted messages through the pipeline again to its original topic. You can also build out something to track if you keep seeing the same message error out over and over, which might indicate something so broken that it boggles the human brain. I‚Äôve seen some wild data, I bet you have too. Writing the output We know ultimately we‚Äôll need to write our messages out as Avro files. This is relatively straightforward though not super obvious if you‚Äôre not that familiar with the Avro library‚Äôs options. I‚Äôm definitely no expert in this area, but I‚Äôve at least gotten the below to work and its generic enough that I‚Äôve used it in a couple of systems. def write ( messages : Seq [ Message ], fileName : String = \"\" ) : Try [ File ] = Try ({ require ( messages . map ( message => message . schema ). toSet . size == 1 , { \"All messages must be of the same schema.\" }) val fileNameUse = \"tmp/\" + { if ( fileName . isEmpty ) messages . head . processedSchema . topic else fileName }. stripPrefix ( \"tmp/\" ). stripSuffix ( \".avro\" ). replace ( \"/\" , \"-\" ) + \".avro\" val schema = messages . head . processedSchema . rawAvroSchema val writer = new DataFileWriter ( new GenericDatumWriter [ GenericRecord ]( schema )) val output = new File ( fileNameUse ) writer . create ( schema , output ) messages . map ( message => writer . append ( message . raw )) writer . close () output }) This function will write a local tmp/ file of the passed in messages, assuming they‚Äôre all of the same type. Because a message carries metadata about its schema, we can grab the schema from there and make a generic writer that moves our messages from the running system to that local file. This is a very imperative method but I‚Äôve not found a ‚Äúbetter‚Äù way yet, so if someone is more familiar with the Avro options, please let me know! (If you‚Äôre looking to write JSON output like a deadletter, json4s works beautifully with case classes but can be made to work with generic maps of JSON input quite easily as well.) Processing the data Now that we have our handful of building blocks, we can piece them together into our actions we want to take when we have messages. If there is a secret sauce to the system, this is it, so I‚Äôm sharing it nearly as is (removing some internal specific comments). Code that‚Äôs referenced but I‚Äôve not provided is pretty generic, so you can make your own implementations for things like Log and Metric as you‚Äôd like. /** Perform (business) logic for valve consumption of pipeline messages to move them to data stores. */ object Valve { /** Generates a file name for a batch of messages using the provided timestamp value for path and file name.\n    * \n    * @param timestamp Value to use for timestamp to write messages out using.\n    * @param messages Messages from the pipeline consumer.\n    * @return Combined path and file name to use in S3.\n    */ def getFileName ( timestamp : String , messages : List [ Message ]) : String = { require ( timestamp . nonEmpty , { \"Timestamp value must be present\" }) require ( messages . nonEmpty , { \"There must be at least one message in the batch\" }) val topic = messages . head . schema . topic val length = messages . size val uniqueId = java . util . UUID . randomUUID . toString s \"$topic/$timestamp/${topic}_${length}-messages_${uniqueId}.avro\" } /** Writes given messages to S3 using provided timestamp value for generating paths and files.\n    * \n    * @param s3 Connection to S3 bucket for where to write messages.\n    * @param timestamp Value to use for timestamp to write messages out using.\n    * @param messages Messages from the pipeline consumer.\n    * @return File name wrote to S3.\n    */ def writeToS3 ( s3 : S3Connector , timestamp : String , messages : List [ Message ]) : String = { require ( messages . nonEmpty , { \"Cannot write empty messages to a file\" }) val fileName = getFileName ( timestamp , messages ) val file = AvroConverter . write ( messages . toSeq , fileName ). get s3 . write ( fileName , file ). get file . delete fileName } /** Send messages that are associated with an error to the deadletter queue for investigating later.\n    * \n    * @param processor Name of processor that encountered these messages, ie name of valve running.\n    * @param errorTopic Topic to send deadletter messages to.\n    * @param producer Producer for deadletter messages.\n    * @param messages Messages that the error is associated with.\n    * @param exception Error encountered.\n    */ def sendDeadletter ( processor : String , errorTopic : String , producer : PipelineJsonProducer , messages : List [ Message ], exception : Throwable ) : Try [ Unit ] = Try ({ messages . map ( message => Deadletter ( processor , message , exception )) . map ( deadletter => producer . produce ( errorTopic , deadletter . asJson )) . map ( result => result match { case Success ( _ ) => Log . info ( s \"Wrote to deadletter queue $errorTopic for ${messages.size} messages\" ) case Failure ( exception ) => Log . error ( s \"Unable to write to deadletter queue $errorTopic for ${messages.size} messages: ${exception.getMessage}\" ) throw exception }) }) /** Action for a pipeline consumer.\n    * \n    * @param processor Name of processor consuming these messages.\n    * @param s3 Connection to S3 bucket for where to write messages.\n    * @param extractionKey Key to use for timestamp to write messages out using.\n    * @param errorTopic Deadletter queue topic, if errors are encountered.\n    * @param errorProducer Deadletter queue producer, if errors are encountered.\n    * @param messages Messages from the pipeline consumer.\n    */ def process ( processor : String , s3 : S3Connector , extractionKey : String , errorTopic : String , errorProducer : PipelineJsonProducer )( messages : List [ Message ]) : Try [ List [ String ]] = { val result = Try ({ require ( messages . nonEmpty , { \"Cannot consume empty messages\" }) Metric . attempting ( \"process\" , messages . head . schema . topic , messages . size ) messages . map ( message => ( message . extractionStringOf ( extractionKey ), message )) . groupBy ( _ . _1 ) . map ( timestampAndWrappedMessages => { val ( timestamp , wrappedMessages ) = timestampAndWrappedMessages ( timestamp , wrappedMessages . map ( _ . _2 )) }) . flatMap ( timestampAndMessages => { val ( timestamp , messages ) = timestampAndMessages Try ( writeToS3 ( s3 , timestamp , messages )) match { case Success ( fileNames ) => Some ( fileNames ) case Failure ( exception ) => Metric . deadletter ( exception . getCause . toString , messages . head . schema . topic , messages . size ) sendDeadletter ( processor , errorTopic , errorProducer , messages , exception ). get None } }) . toList . sorted }) result match { case Success ( files ) => Log . debug ( s \"Just wrote ${messages.size} messages to: `${files.mkString(\" `, ` \")}`\" ) Metric . succeeded ( \"process\" , messages . head . schema . topic , messages . size ) case Failure ( exception ) => Log . error ( \"Error encountered consuming\" , exception ) if ( messages . nonEmpty ) Metric . failed ( \"process\" , messages . head . schema . topic , messages . size ) } result } } I know there‚Äôs a lot in there but as I said, this is it: this is what makes the system the valve we‚Äôre after. You can tweak how you generate file names for your needs, or maybe send more or less metrics than we do. The process function though will probably remain pretty similar to what we have: For each batch of messages, For each message, get the timestamp we want to use. Bucket each group of messages with those who have the same timestamp, for example all of the 2019-04-02 14:00:00 messages. Clean up our list of (timestamp, messages) to (timestamp, list of messages) . For each timestamp with its messages, Try to write the messages to S3. If successful, collect the file names. Otherwise, send the messages to the deadletter queue. Grab the list of files we wrote and sort them (purely for humans to have an easier time reading). Everything else is metrics and logs! The data structures provide the necessary extraction functions, and the helper functions provide the rest so each piece is as small as possible: easy to update, easy to test, easy to understand. Running it With all of that, all that‚Äôs left is to run the system. object Driver { def main ( args : Array [ String ]) : Unit = { val configs = Configuration () val processor = configs . pipelineGroupName val s3 = S3Connector ( configs . bucket ) val topics = SchemaRegistry . getTopicsToConsume ( configs . topicToConsume ) . get val deadLetterQueueTopic = s \"_dead_letter_queue_$processor\" val deadLetterQueueProducer = PipelineProducer . json ( processor ) PipelineConsumer ( processor , topics , Valve . process ( processor , s3 , configs . groupByTimestamp , deadLetterQueueTopic , deadLetterQueueProducer ), configs . maxBatch ) deadLetterQueueProducer . close } } You might remember from my blog post about only having to specify configurations once that we use Consul to share values. This allows each piece of our system (like pipeline consumers and producers) to grab the configurations they need, so our Configuration object only needs to grab the environment variables specific to this valve, like which bucket will it write the data to or which timestamp are we grouping by. Then, once the pieces are set up, we pass the pipeline consumer our processing function and the rest takes care of itself. (I have choosen to omit the pipeline consumers and producers as they are very basic implementations you can make using the default Kakfa drivers. My only suggestion is do allow the consumer to take in a function it applies to each batch of message ‚Äî¬†this allows you to use the same consumer code in multiple systems with minimal changes in between. Here we also use the Schema Registry to figure out which topics are of interest to us, but you might have another way of specifying that.) How you can do this yourselves, infrastructure and metrics edition Alright, we have a valve, it works well enough to throw it into the wild and see how it behaves. We even have some metrics around it, so like: now we launch it, yeah? How do we launch it though? We like to use AWS ECS on Fargate ( read more about our running Apache Airflow on it here ) for our data systems so I‚Äôll let that description suffice for this blog post as well for how we run containers. The major difference between that setup of Terraform to run containers and what the valve has is scaling: the valve is constantly running, trying to keep pace with data, so we want to make sure we have enough valves for the data we‚Äôre seeing. Since Kafka works with consumers to distribute messages, all we need to do is scale the consumers in the group and Kafka will organize distribution of messages. Thanks, Kafka! The programmer, like the poet, works only slightly removed from pure thought-stuff. He builds his castles in the air, from air, creating by exertion of the imagination. [2] We‚Äôve chosen to scale on three metrics: offset lag (if we‚Äôre more than so many messages behind), time lag (if we‚Äôre more than so many hours behind), and memory usage (as the final backup though we‚Äôve yet to see this really be necessary). The offset lag and time lag we calculate in our consumer and report to AWS, looking at the difference between messages we‚Äôre just consuming and where the most recent data is; for example, if we‚Äôre processing a message from 06:00 today with an offset 973 and right now it‚Äôs 17:32 with the latest offset for this topic/partition being 10452 , we‚Äôre over eleven hours behind and nearly a thousand messages behind. Time to scale on time lag! Within our Terraform module , we use the following generic scaling policies that we configure as necessary with min/max instances, offset lag (for us, 25 batches behind), time lag (two hours behind), and memory usage (60%). This way each version of the valve (one for extracting, one for archiving, always room for more!) can scale on its own but we can also have them configured similarly, since they‚Äôll all be flooded at the same time with the same messages. It‚Äôs easier for us to mentally reason about the system this way while allowing each to do its thing in its own time and way. resource \"aws_appautoscaling_target\" \"valve-scaling-target\" { service_namespace = \"ecs\" resource_id = \"service/ ${ var . pipeline_cluster_name } / ${ aws_ecs_service . valve-service . name } \" scalable_dimension = \"ecs:service:DesiredCount\" max_capacity = var . max_instances min_capacity = var . min_instances } resource \"aws_appautoscaling_policy\" \"valve-scaling-policy-offset-lag\" { name = \" ${ var . environment } ${ var . purpose } scale on message offset lag\" policy_type = \"TargetTrackingScaling\" service_namespace = aws_appautoscaling_target . valve - scaling - target . service_namespace resource_id = aws_appautoscaling_target . valve - scaling - target . resource_id scalable_dimension = aws_appautoscaling_target . valve - scaling - target . scalable_dimension target_tracking_scaling_policy_configuration { target_value = var . target_offset_lag scale_in_cooldown = var . scale_cooldown_up scale_out_cooldown = var . scale_cooldown_down customized_metric_specification { metric_name = \"max_offset_lag\" namespace = var . service statistic = \"Average\" dimensions { name = \"CONSUMER_GROUP\" value = var . purpose } dimensions { name = \"ENVIRONMENT\" value = var . environment } } } } resource \"aws_appautoscaling_policy\" \"valve-scaling-policy-time-lag\" { name = \" ${ var . environment } ${ var . purpose } scale on time lag\" policy_type = \"TargetTrackingScaling\" service_namespace = aws_appautoscaling_target . valve - scaling - target . service_namespace resource_id = aws_appautoscaling_target . valve - scaling - target . resource_id scalable_dimension = aws_appautoscaling_target . valve - scaling - target . scalable_dimension target_tracking_scaling_policy_configuration { target_value = var . target_time_lag scale_in_cooldown = var . scale_cooldown_up scale_out_cooldown = var . scale_cooldown_down customized_metric_specification { metric_name = \"max_time_lag\" namespace = var . service statistic = \"Average\" dimensions { name = \"CONSUMER_GROUP\" value = var . purpose } dimensions { name = \"ENVIRONMENT\" value = var . environment } } } } resource \"aws_appautoscaling_policy\" \"valve-scaling-policy-memory-usage\" { name = \" ${ var . environment } ${ var . purpose } scale on memory usage\" policy_type = \"TargetTrackingScaling\" service_namespace = aws_appautoscaling_target . valve - scaling - target . service_namespace resource_id = aws_appautoscaling_target . valve - scaling - target . resource_id scalable_dimension = aws_appautoscaling_target . valve - scaling - target . scalable_dimension target_tracking_scaling_policy_configuration { target_value = var . target_memory_usage scale_in_cooldown = var . scale_cooldown_up scale_out_cooldown = var . scale_cooldown_down predefined_metric_specification { predefined_metric_type = \"ECSServiceAverageMemoryUtilization\" } } } At this point with the valve up and running, monitoring the system is actually super simple! We like to use the same style boards for most of our systems, so if the below Datadog examples look familiar, it‚Äôs because you‚Äôve seen me use these before . We must trust to nothing but facts: These are presented to us by Nature, and cannot deceive. We ought, in every instance, to submit our reasoning to the test of experiment, and never to search for truth but by the natural road of experiment and observation. [10] Looking at our last month in production, I can see what our valve has been up to: CPU and memory usage have been typically low, excellent, and that makes sense: the system is data intensive but not doing anything complicated. We can also see the pickup in data coming in based on our running or pending tasks; as a sports company, weekends and nice weather are often quite clear in our metrics about what our data systems are up to. We‚Äôre constantly tweaking our max instance limit as well as offset/time lag scaling policies, to make sure we‚Äôre moving the data through our systems as quickly as we can, so this is always exciting to drill into on Monday morning to see if we were aok or need to give the system more room to scale or make it more sensitive to prevent falling behind and then, worse, staying behind. When we looked at processed messages (so messages that hit the Valve.process function and what happened from there), there‚Äôs a lot happening all the time! :fr: Formidable! :fr: We do have a spike in failed processes to look into, but that‚Äôs super rare and ‚Äî¬†given the scale of messages ‚Äî¬†it‚Äôs actually so small that it doesn‚Äôt bring down our success rate. Similar to the processed metrics, the consumed messages (when the Kafka consumer picks up messages and if there‚Äôs an issue in that handoff), there‚Äôs the same amount of messages flowing through but none of them fail to be consumed. We do see our max wait in the pipeline keeps peaking way over our scaling, meaning that while we‚Äôre scaling, we‚Äôre not necessarily doing it fast enough or early enough; certain topics are more impacted by this than others (names removed but I promise they‚Äôre there on the real dashboard). Your scaling patterns might differ from ours so humans can respond more quickly during the week, whereas our load happens while no one is working. Gotta keep Mondays spicy. Takeaways So, uh, hate to say it, but the Internet was right from the start on this one: for moving data from Kafka to other systems, especially S3, writing your own custom solution remains the best way forward. And that can be daunting! Especially if the Kafka native libraries aren‚Äôt that familiar to you, or Kafka itself is a relatively new system. All programmers are optimists. [2] My hope is that, by providing so much detail of our thought processes, our code, our setup and monitoring, I can make taking on this problem and writing your own solution not only less daunting but something you feel confident tackling. A system like the valve might be complicated to describe but its implementation is small, obvious, and can be fully covered in automated tests to ensure both that you know what your code is doing and that others will be able to understand and maintain it as well. Since rolling the valve out, we‚Äôve had no problems that come to my mind outside of purely tweaking the scaling of the system. Again, given that our data is very seasonal and high load tends to come when we‚Äôre not working, getting our scaling just right is a long learning curve that we‚Äôre used to from every system that scales. And the valve has, thankfully, started to pave the way for making use of the newer features our updated Kafka provides us. It shows that now we can not only make huge, data intensive systems that keep pace with the flow of data through the data pipeline, we can do it with just a little, straightforward code that anyone can hop into. If you want to start transforming data in real time, we can do it ‚Äî¬†no more batch jobs that take hours! If you want to start building customer-facing features that don‚Äôt slow down the main RESTful backend server, we can do it ‚Äî¬†and you have all the data available to you! The only limit, really, is your ability to figure out what you want to do with all this power. We can only see a short distance ahead, but we can see plenty there that needs to be done. [5] This journey took a year and a half to come to its conclusion but it was worth it: our new data pipeline was rolled out so smoothly, no one knew we‚Äôd torn down the old one until we announced that we‚Äôd already done it weeks ago. In our ETL process, the extractor is the fastest system with the least number of errors. If anything, the main problem with the valve is that it is so fast, the systems downstream from it look incredibly shabby and slow in comparison. Maybe time to redo those systems, to make them as quick and responsive as the valve is. I‚Äôd like to thank Emma and Eduardo for their help in working with Kafka Connect, Secor, Gobblin, and the valve. I‚Äôd also like to thank the entirety of the data team for their help in running Camus manually during our largest extractor incident, and their faith that if I said we were going to write our own Scala solution, that that was the right way forward and I‚Äôd make the team feel comfortable with it. And finally I‚Äôd like to thank Jason, who never lost faith that Data Engineering would figure it out, even (and especially) when Data Engineering was just me, floundering, doing the best I could. Time to convert another batch system to a streaming solution! Footnotes Siobh√°n Sabino, ‚ÄúWhen You Deserve Better Systems‚Äù , yes I am now quoting myself from a blog post where I quoted myself. Frederick P. Brooks Jr., Mythical Man Month . Neal Stephenson, Cryptonomicon . Alan J. Perlis, ‚ÄúEpigrams on Programming‚Äù. Alan Turing, ‚ÄúComputing Machinery and Intelligence‚Äù. Seneca, Letters from a Stoic . Simone de Beauvoir, All Men Are Mortal . Lao Tzu, Tao Te Ching . Erin Morgenstern, The Night Circus . Antoine-Laurent de Lavoisier, Elements of Chemistry . Share on Twitter Share on Facebook", "date": "2021-05-05"},
{"website": "GameChanger", "title": "Dependency Injection in Typescript with tsyringe", "author": ["Kirt Gittens"], "link": "http://tech.gc.com/dependency-injection/", "abstract": "Why Dependency Injection In any large object oriented codebase, managing dependencies can get difficult. Each class can require any number of third parties or other classes to function, and it can be hard to test the behavior of a single class with mocks if those dependencies aren‚Äôt easy to provide. Fortunately, there‚Äôs a popular design pattern that can be applied to solve this problem, and that is dependency injection. When using dependency injection, classes can be provided their dependencies through a constructor, and those dependencies can be swapped out easily for other implmentations. In tests, mocks can simply be substituted in to test class behavior. While most of the time, this pattern is implemented with a framework, even without one manual dependency injection can give you some of these benefits. Here at Gamechanger, we previously had a form of manual dependency injection in our typescript codebase. Each class would have a constructor that accepted its dependencies, which could be swapped out with mocks or a real implementation. class BusinessLogic { constructor ( dependencyA : DependencyA , dependencyB : DependencyB ) {} private void foo () { this . dependencyA . action (); } } // To instantiate this class, both dependencies must be created const businessLogic = new BusinessLogic ( new DependencyA (), new DependencyB ()); // to test this class, mocks can be passed in const testBusinessLogic = new BusinessLogic ( new MockDependencyA (), new MockDependencyB ()); In this example, if you need to replace a dependency, you can just supply a different class in the constructor for BusinessLogic . This can work great for a small number of classes with a tiny dependency tree, but as your codebase‚Äôs number of dependencies grow, it can become difficult to manage. Once your dependencies have dependencies, its not as straightforward to get an instance of a class. class DependencyC { constructor ( dependencyE , DepedencyE ) { } } class DependencyA { constructor ( dependencyC : DependencyC , dependencyD : DependencyD ) { } } // Now, to instantiate BusinessLogic, we need to create a tree of instances. const dependencyC = new DependencyC ( new DependencyE ()); const businessLogic = new BusinessLogic ( new MockDependencyA ( dependencyC , new DependencyD ()), new MockDependencyB ()); Even in this relatively mild example, it‚Äôs starting to get complicated to manage the dependency tree. If you want to mock dependency C in the business logic dependency chain, you have to create all of the dependencies around it and pass those in. When you need to test a particularly complicated class, setting up all its dependencies can take more time that writing the test itself! If you only need to mock a single subdependency, you need to instantiate everything all the way down until the mock is required, and then pass it in there. Fortunately, there are dependency injection frameworks for typescript that can simplify the work that needs to be done. Using Tsyringe Since we use typescript, we‚Äôve moved to using https://github.com/microsoft/tsyringe Tysringe allows you to tag a particular dependency as injectable with a decorator, and then very easily get an instance of it. At its core, tsyringe provides you a dependency container that keeps track of all your dependencies. When you need to create an instance of a class, you can call resolve on the the container with an injection token and it will return you the right dependency registered under that token. Our previous example becomes much easier to manage with this: import { container , injectable } from ' tsyringe ' ; @ injectable () class DependencyC { constructor ( dependencyE , DepedencyE ) { } } @ injectable () class DependencyA { constructor ( dependencyC : DependencyC , dependencyD : DependencyD ) { } } @ injectable () class BusinessLogic { constructor ( dependencyA : DependencyA , dependencyB : DependencyB ) {} private void foo () { this . dependencyA . action (); } } // Now, all we need to do if we need an instance of business logic, is resolve it const businessLogicInstance = container . resolve ( BusinessLogic ); That‚Äôs it! All you need to do is tag your classes as injectable and tsyringe can take care of instantiating the whole dependency tree. Writing tests also becomes much easier with the framework, when you need to mock a low level dependency, you can just register it with the dependency container, and leave everything else in place. To register a mock, you can call registerInstance on the container, and provide it with the injection token you want to replace, and what you want to replace it with. Once you‚Äôre done with the mock it can be cleared with clearInstances on the container. import { container } from ' tsyringe ' ; describe ( ' BusinessLogic ' , () => { it ( ' should call action on dependencyA when foo is called ' , () => { // We can mock a class at any level in the dependency tree without touching anything else container . registerInstance ( DependencyC , mock ()); // dependency A gets a mock version of dependency C during this resolution. const underTest = container . resolve ( BusinessLogic ); // We can call this now that we're done testing, and the mock will be removed. // When we resolve the instance after this, we get the original dependencies. // In practice, we've found it's easy to just place this in your afterEach block. container . clearInstances () }); }); Here, DependencyC will be replaced with a mock for the duration of this test, and at the end, when clearInstances is called, it will return to its original form. Tsyringe provides some great utilities we‚Äôve been able to leverage to deal with common dependency problems. For instance, its fairly common to have classes that are singletons, and while managing that manually can be a bit difficult, its virtually painless with tsyringe. import { singleton } from ' tysringe ' ; // This class will be a singleton, when container.resolve is called // All calls will return the same instance. @ singleton () class MySingleton { } Tsyringe also contains some great tools for managing the lifecycle of a given dependency. Dependencies can be scoped in a number of different ways. By default, dependencies have the transient scope, which means that every time you resolve this dependency a new instance is created. This can make sense, but also has some performance and memory implications, especially if you have some classes that are large and expensive to construct that aren‚Äôt singletons. For our dependencies, we found that ResolutionScoped worked in reducing our memory usage. Resolution Scoping means that the same dependency will be reused during a resolution chain, so if you have a class that could need a dependency more than once in its dependency tree, it will only ever be instantiated once. Potential Issues There‚Äôs a few quirks we‚Äôve learned to navigate with using tsyringe, mostly related to how to register mocks. There are a few ways to register something with the dependency container, the easiest of which is adding the @injectable() decorator, but you can also manually register something with a call to container.register . This can be useful if you need to register something that‚Äôs not a class. An additional note that‚Äôs useful in this case, is that your injection token can be a string which can also be helpful if you‚Äôre not registering a class. container . register ( ' NonClassDependency ' , { useValue : nonClassObject }); If you need to resolve something from a string token in a contstructor, there‚Äôs an @inject decorator you can use to make sure the dependency is automatically resolved. import { inject } from ' tsyringe ' ; class MyClass { constructor (@ inject ( ' NonClassDependency ' ) nonClassDependency : NonClassDependencyInterface ) { } } One problem with dependencies registered manually can be cleared unintentionally by a call to clearInstances which should be used in-betweeen tests. To register a dependency that is unclearable without the decorator, it needs to be registered with useFactory . The factory should be a function that returns the item you want injected. // Application code container . register ( MyDependency , { useFactory : () => myDependencyObject ); container . register ( OtherDependency , { useValue : otherDependencyObject ); describe ( ' MyDependency ' , () => { it ( ' should not be mocked ' , () => { container . resolve ( MyDependency ); container . resolve ( OtherDependency ); container . clearInstances (); // This will fail! // OtherDependency is no longer registered with the container. container . resolve ( OtherDependency ) // This will be fine, the dependency remains registered after clears container . resolve ( MyDependency ); }); }); This allows you to register whatever classes or objects you want managed by the dependency container in your application code, and then selectively replace them with mocks, in a given test, and then revert back to the original when the test is done. Wrapping Up Adding tsyringe has definitely made managing our application dependencies and testing code much easier, with a dependency injection framework, we now have a much more manageable solution to dealing with our large dependency tree. Share on Twitter Share on Facebook", "date": "2020-11-30"},
{"website": "GameChanger", "title": "Capturing Script Logs", "author": ["Miroslav Shubernetskiy"], "link": "http://tech.gc.com/capturing-script-logs/", "abstract": "At GameChanger, we use scripts in many of our flows such as during deploys\nor running Ansible while booting new instances. Some of these flows are\ncritical to our operations and require good visibility. Traditionally we\ntried to send both metrics and logs to DataDog where we can both monitor what\nscripts are doing and set alerts on various metrics. DataDog integration,\nhowever, is not always available such as during the instance boot. In addition,\nsometimes a DataDog alert does not provide enough context of what failed and most\nimportantly why it failed. It simply states that some threshold was reached. To\ninvestigate the issue requires more manual steps by looking for the appropriate\nlog which is not always intuitive. That is why recently at GameChanger we\nstarted integrating Slack error reporting directly into some of our critical\nscripts. This post describes the exact mechanism of how that is achieved since\nit uses a really cool bash trick. Useful Commands First some background on some of the useful commands. tee Anyone? Tee is a really useful command. It captures an output from a script\nand both echoes it to standard out as well as forwards it to a file.\nThis allows to both see an output as well as capture the same output\nfor later use. For example: $ echo hello there | tee hello.log\nhello there $ cat hello.log\nhello there Process Substitution Some commands only work with files. For example a classic diff : $ echo one > one.txt $ echo two > two.txt $ diff -u one.txt two.txt --- one.txt\n+++ two.txt\n@@ -1 +1 @@ -one +two Sometimes however it is convenient to be able to refer to an output of commands\nas a file without manually creating a temporary file. This is what process\nsubstitution allows to do. Same example as above but with process substitution: $ diff -u < ( echo one ) < ( echo two ) --- /dev/fd/63  2020-11-30 12:33:08.539905663 -0500 +++ /dev/fd/62  2020-11-30 12:33:08.540272967 -0500 @@ -1 +1 @@ -one +two This is the output form of process substitution which uses <(command) syntax.\nIt stores an output of a command into a temporary file for the duration\nof a command. There is also input form of process substitution which uses >(command) syntax. It similarly creates a temporary file to which data can be\nwritten to and process substitution will forward that content to a command. For\nexample: $ echo \"\n> 2\n> 1\" > >( sort ) 1\n2 exec Most common form of exec is to simply execute another command by replacing\nthe process: $ bash -c 'exec echo foo' foo However exec also overtakes process file descriptors which allows exec to\nadjust what a process does with its file descriptors which includes stdout and stderr . For example exec can capture script output into a file: $ cat test.sh exec &> log.txt echo stdout > /dev/stdout echo stderr > /dev/stderr $ bash test.sh $ cat log.txt\nstdout\nstderr Similar results can be achieved by manually sending script output to a file\n(e.g. command &> log.txt ) however exec allows to do that directly within\nthe script. Putting Everything Together Putting all commands together allows to do something like: LOG_PATH = /tmp/deploy_ $( date +%s ) .log exec &> >( tee -a $LOG_PATH ) function error_msg { echo \"Deploy last 50 logs: $( tail -n 50 $LOG_PATH ) \" } function slack_error { msg = ${ 1 :-} slacksend \\ --channel = alerts \\ --color = danger \\ --snippet = \" $( error_msg ) \" \\ --filename = \"deploy.log\" \\ \" $msg \" \\ || true } deploy || ( slack_error \"terminating rolling deployment at $HOSTNAME \" ; exit 1 ) Here is what the above does: Shows script output as normal via tee . This allows normal script log\naggregation to work as normal. In addition it fully captures script stdout and stderr to a log file via exec , process substitution and tee When the deploy command fails, it sends a slack message with the last 50\nlines of the script logs Wrap Up It might not seem like much, but immediately seeing what failed and why by\nincluding the failure logs makes for a much more pleasant debugging experience.\nMost importantly it does not adjust how the script is used so no other changes\nare required to other systems since all the enhancements are baked directly in\ninto the script itself. Hopefully you will find some of these cool bash capabilities useful as well. Share on Twitter Share on Facebook", "date": "2020-12-01"},
{"website": "GameChanger", "title": "Deploys at Gamechanger", "author": ["Kirt Gittens"], "link": "http://tech.gc.com/deploys-at-gamechanger/", "abstract": "At GameChanger, being able to deploy our changes quickly and reliably has always been important. Over the past few months on the platform team, we‚Äôve been working to build a simpler and more reliable deployment pipeline to support our product teams in shipping code with speed and reliability. In this post, I‚Äôll go over the system that we have in place now, and some of our recent improvements. How deploys our work High Level Overview At a high level, the core of our deployment process involves shipping a docker image to some EC2 instances, and then starting a container with that image. Our main code repository contains a Dockerfile with the required configuration for our live app. After each new commit to our main code repository, Our CI process will docker build a new image with a new tag, and push it to our internal docker registry. During a deploy, we pull that image on the relevant EC2 instances, and start a container with it. Once we have a running container, we register that EC2 instance with a load balancer, and start serving traffic. While the high level is straight forward, there‚Äôs a lot of work that goes into making sure our deployments happen safely, and at the right time. Once a new image has been pushed to our registry by our CI process, we need to notify all our currently running instances that there‚Äôs a new version to deploy. Each commit corresponds to a new docker tag that‚Äôs created during the build. Once the tag is created, we notify our internal deployment service of the new version. Once our tag is pushed, and our deployment service is aware of it, we tell our deployment service to start a deploy, which will send a message to all of our running EC2 instances. Serf Messages We use Serf to send that message out without needing to directly contact every instance. The message we send contains a role and environment . The role is the name of a docker image we want to deploy a new version of, and environment will be production or staging depending on where this deployment is headed. Through the Gossip protocol , the Serf message will propagate across our entire cluster in a short period of time, but only the boxes that are responsible for deploying the specified image do anything when they get the message. That‚Äôs because each box runs a serf agent, which listens for our messages and is given Serf tags when it‚Äôs initially provisioned. Serf tags allow us to associate a box with the docker images that should be deployed there. In our infrastructure, each box has a images tag, which is a list of docker images that should be deployed there. We‚Äôll only start a deployment when the serf tags match the details in the deployment message. Serf also allows you to configure event handlers for certain messages. An event handler is just a script that executes when a certain serf message is received by an instance. Each of our boxes has the same event handler for deploy serf messages, which is what starts the execution of our deploy script. Deployment Locking Before we can start creating the new containers, we need to make sure that it‚Äôs safe to start the deployment. During our deploy process, our live containers will stop serving traffic and be removed from our load balancers for a period of time. Since all of our instances receive the deploy message at the same time, they could all start deploying at once, and bring down our application until the deployment is done. In order to make sure that doesn‚Äôt happen, we use a locking system to ensure only 1/3rd of our boxes for any given service can be deploying at once. First, each box uses serf to figure out the current size of the deployment (how many instances are involved) based on the environment and service name. Serf has a built in members command that allows us to see all of the other boxes that are active and running serf agents. We use this and the serf tags to get the count of live boxes we are going to deploy on. We also store this number in a redis key, and decrement it whenever a box is done deploying, so that when it reaches 0, we know a deployment is completely finished. Once we have that, we know how many locks should be available, and each box will simultaneously try to acquire the first deployment lock. Each ‚Äúlock‚Äù is represented by a single redis key with a suffix. We start with 1 as the first lock suffix, and increment from there. We use a SET NX to represent acquiring a lock, which will set a key only if it does not already exist. With only 1 instance of redis, only 1 box can succeed at this operation. The box that successfully sets the redis key will have acquired a deployment lock. The box that succeeds can start deploying, and the rest of the boxes, will try acquiring other deployment locks by if they are available (since we only deploy a 3rd at a time, there are only cluster size / 3 locks available). If there are other locks available, the rest of the boxes will try incrementing the lock key suffix, and attempt to acquire the next lock key. If there aren‚Äôt any locks available, the remaining boxes will poll redis until a lock frees up. Once a box is done deploying, it will release the lock it held by deleting the redis key it created. If there‚Äôs an issue during deployment, or a box freezes or shuts down, we don‚Äôt want to prevent future deployments, so we set a TTL on the lock key of 15 minutes, to prevent holding up any deployments if we have to terminate an instance. Once a box has a deployment lock, it can start running our deployment script. Since our serf message doesn‚Äôt contain which tag we want to deploy, we first reach back out to our deployment service, which tells us which tag to pull from the registry. Once we know the right tag, we can pull it and start some containers. Each of our boxes is provisioned with a docker-compose file, that tells us what containers we need to run on that box, what environment variables they need, and how they‚Äôre connected. The deployment script uses docker-compose to understand how to start up our container and it‚Äôs dependencies. Because each service has it‚Äôs own compose file, starting up an application and it‚Äôs dependencies is as simple as running docker-compose up -d If everything goes well, after our container starts up, the instance will register with our load balancers, and start serving traffic! Blocking bad deploys Before some recent changes, when we started a deployment, we would continue deploying to all of our infrastructure even if there were issues with the image. In cases where we might be deploying a faulty release, there was no way to stop it from rolling out to all of our boxes first, even if we had a fix ready to go. One of the recent improvements we‚Äôve made, has been preventing bad deploying from rolling out to all of the boxes for a service. Since we only deploy a 3rd of the cluster at a time, there‚Äôs no sense in continuing to roll out deploys that we know are broken. To make this work, we take advantage of our locking mechanism. If a deploy is broken (for example: if we can‚Äôt start a container with our new image), we set a special key in redis that all of our currently deploying boxes look for as they‚Äôre waiting for a lock to become available, telling them to cancel the deployment. Once this key is set, the other instances stop deploying, and call out to our deployment service to mark this release as ‚Äúbroken‚Äù. This new feature also allows us to make sure deploys work in lower environments first before automatically promoting them to production. Our pipeline will first, kick off a deploy to a staging environment, and if there are no issues, automatically kick off the production deploy. Wrap Up Our recent improvements have added some reliability and more visibility into our deployment pipeline, but the system is still evolving. Down the line, we‚Äôd like to look at tying in our APM metrics to deployment, and improving our detection of ‚Äúbroken‚Äù releases. We‚Äôre continuing to work on delivering a best in class deployment system to support shipping releases as quickly and reliably as possible! Share on Twitter Share on Facebook", "date": "2020-07-28"},
{"website": "GameChanger", "title": "Apache Airflow on AWS ECS", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/apache-airflow-on-aws-ecs/", "abstract": "An overview of what AWS ECS is, how to run Apache Airflow and tasks on it for eased infrastructure maintenance, and what we‚Äôve encountered so that you have an easier time getting up and running. Our data team recently made the transition in workflow systems from Jenkins to Apache Airflow . Airflow was a completely new system to us that we had no previous experience with but is the current industry standard for the sort of data-centric workflow jobs we were looking to run. While it hasn‚Äôt been a perfect fit, we have been able to get a lot of benefits from it: jobs are defined in code, we‚Äôve the history of each job, it goes through our normal pull request process, and everyone on the team is able to read and write jobs. Since our team is data focused, we wanted our Airflow setup to be as easy to maintain as possible, especially around infrastructure, so we have minimal distractions with high resiliency. This led us to using AWS ECS not only to run Airflow but for our bigger tasks that are already containerized. Not familiar with ECS? Or how to run Airflow or its tasks on it? Don‚Äôt worry, we weren‚Äôt either. This is what we‚Äôve learned. What is ECS? ECS with Terraform Setting up Airflow infrastructure on ECS Setting up Airflow tasks on ECS Learnings from Airflow and ECS tl;dr Running Airflow‚Äôs infrastructure on ECS is super easy but running the ECS operator needs hecka help setting up. What is ECS? ECS is AWS‚Äôs Elastic Container Service , designed to let you run containers without worrying about servers. You do this by creating a cluster for your system, define task definitions for the tasks you want to run, and possibly group your tasks into services. You can also choose if you want to have your containers fully managed (ECS on Fargate), kind of managed but kind of not (ECS on EC2), or if you‚Äôd like to use Kubernetes. If all of that made sense to you, congratulations: you can skip to the next section! If you need a bit more of a breakdown, here‚Äôs what you need to know: A container is like a snapshot of a tiny computer that you can run anywhere. If, for example, you have some Docker containers to run, you can run them on my mac Mac, my father‚Äôs Mac, my mother‚Äôs Windows, my sister‚Äôs Linux, a server in a public cloud, a server in a private data center with an undisclosed location for security reasons ‚Äî¬†anywhere there‚Äôs a computer that supports the container‚Äôs setup can run that container, and you‚Äôll get the same result from the container every time. The main advantage containers give us is that it simplifies making sure that something running locally on an engineer‚Äôs machine, and running perfectly fine in a staging environment QA is testing in, will also run in a production environment where users are making requests, since they‚Äôre all the same container being run the same way. No dependency went missing in the middle, no permission was accidentally changed locally that hid a problem: it‚Äôs the same thing running the same way. You might be saying, that sounds great! I‚Äômma launch containers everywhere! And that is the attitude ECS is meant to capture and bring to fruition, because your other option to running containers is a bit more hands on: after all, a container needs to execute on a system, and that means hardware. That means a server somewhere you define and setup with an operating system of its own, and you might have to install the container‚Äôs setup system on it (for example, installing Docker), and you have keep it all up to date, plus you want to secure it, and you have to have a way to get containers started on that server, and you probably want a way to monitor for if a container goes down with a way to restart it, and there‚Äôs security patches‚Ä¶. This is starting to be a long list, and while it‚Äôs been a pretty standard list for a while, we deserve better systems . We‚Äôre a data team, after all, and none of that is really about working with our data. That‚Äôs where ECS comes in. Instead of running servers ( EC2 ) where you do all the above things to to get your containers running, you can use ECS with Fargate to worry only about the containers: here‚Äôs the CPU and memory it needs, here‚Äôs the start command, here‚Äôs a health check, give it these environment values, and I want two of these running at all times. Boom: your containers are running, and restart as needed, and things are kept up to date for you, and there‚Äôs monitoring built in. There‚Äôs a lot more nuance that goes into picking if you want to run ECS on Fargate or EC2 or Kubernetes, but if you‚Äôre still reading this, you probably want Fargate: with Fargate, you only need to worry about your containers, and the rest is taken care of for you. Now that we‚Äôre all caught up‚Ä¶ ECS with Terraform As discussed in a previous post , we‚Äôre huge fans of using Terraform for our team‚Äôs infrastructure, especially modules to keep systems in line across environments; I‚Äôll let you read that post if you‚Äôd like some background knowledge on how we use Terraform modules and have them interact with Consul for sharing values. The short story though is, as with the above, it keeps the setup easy for our data team so we can continue to focus on our data. For our workflow system, as with our data pipeline, we started by setting up a new Terraform module that contains input values like configurations for the infrastructure or pass through values (we‚Äôll discuss those later) a single output value within Terraform, the workflow security group , so it can be allowed access to other systems like the data warehouse Consul output values like where to find the workflow system or pass through values (again, more on those later) our metadata store for Airflow; we‚Äôre using a Postgres RDS instance a broker store for Celery ; we‚Äôre using a Redis ElasticCache instance secrets management, to go back and forth with Parameter Store which integrates nicely with ECS our ECS cluster to house our workflow system, including its security group and IAM role our main ECS tasks, for the basic Airflow infrastructure (discussed in the next section) That might seem like a big list to you but remember, this is for a fully functional, production ready Airflow setup: you can start much simpler with just the cluster and its tasks, and add on as you go. To start our ECS setup, we first needed a cluster with a capacity provider, ie the management style we want: resource \"aws_ecs_cluster\" \"airflow-cluster\" { name = \"airflow-test\" capacity_providers = [ \"FARGATE\" ] } Our cluster also needed a role, which you can define through Terraform or create manually through the AWS console and then connect in Terraform, so it can have permissions to do things like talk to Redshift : data \"aws_iam_role\" \"airflow-role\" { name = \"test.workflow\" } (If you didn‚Äôt catch it, that‚Äôs a data block instead of a resource block, so it‚Äôs fetching what already exists and making it usable within Terraform. This is especially helpful if you have existing infrastructure that hasn‚Äôt been fully ported over yet but want to set up new infrastructure in Terraform.) The other big thing our clusters needed regardless of its tasks is to control who can talk to it and getting permission to talk to others, since we want to tightly control who can access our data: resource \"aws_security_group\" \"airflow-sg\" { name = \"Airflow\" description = \"Airflow test security group\" vpc_id = var . vpc_id ingress { from_port = 0 to_port = 0 protocol = \"-1\" self = true security_groups = var . security_groups_access_workflow } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [ \"0.0.0.0/0\" ] } } You can see we take in the VPC and security groups from whoever is invoking the module, and then expose elsewhere this Airflow security group for other systems to allow access to. Beautiful, now we have an ECS cluster. We can look at it, we can marvel at it, we can say, ‚ÄúOh yes, I don‚Äôt worry about servers anymore, for I have transcended and run my containers directly.‚Äù But of course, just because we have a cluster doesn‚Äôt mean it does anything: you need to define task definitions for actual work to be done, and possibly services too if you‚Äôd like. (Tasks, as we‚Äôll see later, can be run outside services.) However this does give us the framework now to set up the rest of our Airflow infrastructure within. Setting up Airflow infrastructure on ECS The two main pieces of Airflow infrastructure we needed were dubbed ‚Äúthe controller‚Äù and ‚Äúthe scheduler.‚Äù (Later additions to our setup like Celery workers, nicknamed ‚Äústalks,‚Äù followed the same setup pattern so I won‚Äôt include them here.) Now, you might understand immediatley what the scheduler is doing: it‚Äôs in charge of the Airflow scheduler ( airflow scheduler ). That leaves the controller as a new addition to the Airflow vocabulary. We use the controller to run the UI ( airflow webserver ), make sure the database is all set up ( airflow initdb ), set up our root users ( airflow create_user ‚Ä¶ ), and create pools to throttle access to certain resources ( airflow pool --import throttling_pools.json ). Since it‚Äôs in charge of controlling all these pieces, we have dubbed it the controller, and when more work is needed, it is where we add this work to. (Sidenote: as a team we prefer to use controller/worker language across our systems, with the controller name coming from Kafka where the lead broker is dubbed the controller, since leader refers to a different part of the system and is an easily overloaded term. It works well for nearly all systems we‚Äôve applied it to, and might work well for your systems as well.) Despite these differences between what the controller and scheduler do, they actually have almost identical setups within ECS and use a lot of the same inputs, so I‚Äôll show the scheduler to start with since it has less pieces. The first thing our scheduler needed was a task definition: resource \"aws_ecs_task_definition\" \"scheduler-definition\" { family = \"scheduler-test\" container_definitions = jsonencode ( [ { \"name\" = \"scheduler\" , \"image\" = format ( \"%s/%s\" , var . docker_address , var . controller_container ), \"portMappings\" = [{ \"containerPort\" = var . controller_port }], \"command\" = [ \"sh\" , \"start_scheduler.sh\" ], \"environment\" = [ { \"name\" = \"ENVIRONMENT\" , \"value\" = var . environment }, { \"name\" = \"LOG_LEVEL\" , \"value\" = var . log_level }, { \"name\" = \"CONSUL_ADDRESS\" , \"value\" = var . consul_address }, { \"name\" = \"DOCKER_ADDRESS\" , \"value\" = var . docker_address }, { \"name\" = \"AIRFLOW__CORE__SQL_ALCHEMY_SCHEMA\" , \"value\" = var . database_schema }, { \"name\" = \"AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION\" , \"value\" = \"False\" } ], \"secrets\" = [ { \"name\" = \"AIRFLOW__CORE__SQL_ALCHEMY_CONN\" , \"valueFrom\" = aws_ssm_parameter . metadata_connection . arn }, { \"name\" = \"AWS_ACCESS_KEY_ID\" , \"valueFrom\" = data . aws_ssm_parameter . aws_access_key_id . arn }, { \"name\" = \"AWS_SECRET_ACCESS_KEY\" , \"valueFrom\" = data . aws_ssm_parameter . aws_secret_access_key . arn } ], \"logConfiguration\" = { \"logDriver\" = \"awslogs\" , \"options\" = { \"awslogs-create-group\" = \"true\" , \"awslogs-region\" = var . region , \"awslogs-group\" = \"/ecs/airflow/test/scheduler\" , \"awslogs-stream-prefix\" = \"ecs\" } }, \"healthCheck\" = { \"command\" = var . scheduler_health_command_list , \"startPeriod\" = var . controller_grace_period , \"interval\" = var . controller_healtch_check_interval , \"retries\" = var . controller_healtch_check_retries , \"timeout\" = var . controller_healtch_check_timeout } } ] ) requires_compatibilities = [ \"FARGATE\" ] network_mode = \"awsvpc\" execution_role_arn = data . aws_iam_role . airflow - ecs - role . arn cpu = var . scheduler_cpus * 1024.0 memory = var . scheduler_memory * 1024.0 } The majority of this is the task definition, mainly what container to run, what environment values and secret values to set up, how to log and perform health checks, and what command to run. Then we linked that up with Fargate and our role we created earlier, specify the CPU and memory we want, and we have something that can run the scheduler. (Note that right now, this is not tied to our cluster: task definitions are cluster agnostic, should you have a task you want to run in multiple clusters.) Now, since we always want a scheduler running, we created a service around this task definition to ensure it‚Äôs able to do its job: resource \"aws_ecs_service\" \"scheduler-service\" { name = \"scheduler\" cluster = aws_ecs_cluster . airflow - cluster . arn launch_type = \"FARGATE\" platform_version = \"LATEST\" task_definition = aws_ecs_task_definition . scheduler - definition . arn desired_count = 1 network_configuration { subnets = var . subnets security_groups = [ aws_security_group . airflow - sg . id ] } enable_ecs_managed_tags = true propagate_tags = \"TASK_DEFINITION\" } This service wraps our task definition, pulling it into the cluster which will always make sure one task is running based on it. This takes care of running our Airflow scheduler, nothing else needed, boom we‚Äôre golden. The controller has an almost identical task definition and service setup, sharing nearly all values. What we added to it though was a nice DNS record that can be accessed while on our VPN and the option to run multiple web servers if we wanted to through a load balancer: resource \"aws_ecs_service\" \"controller-service\" { name = \"controller\" cluster = aws_ecs_cluster . airflow - cluster . arn launch_type = \"FARGATE\" platform_version = \"LATEST\" task_definition = aws_ecs_task_definition . controller - definition . arn desired_count = 1 load_balancer { target_group_arn = aws_lb_target_group . controller - target . arn container_name = local . controller_definition_name container_port = var . controller_port } health_check_grace_period_seconds = var . controller_grace_period network_configuration { subnets = var . subnets security_groups = [ aws_security_group . airflow - sg . id ] } enable_ecs_managed_tags = true propagate_tags = \"TASK_DEFINITION\" } resource \"aws_route53_record\" \"controller-dns\" { zone_id = var . dns_zone name = var . controller_address type = \"A\" alias { name = aws_lb . controller - lb . dns_name zone_id = aws_lb . controller - lb . zone_id evaluate_target_health = false } } resource \"aws_lb\" \"controller-lb\" { name = \"controller-test\" subnets = var . subnets load_balancer_type = \"application\" internal = true security_groups = [ aws_security_group . airflow - sg . id ] } resource \"aws_lb_target_group\" \"controller-target\" { name = \"controller-test\" port = var . controller_port protocol = local . controller_protocol vpc_id = var . vpc_id target_type = \"ip\" health_check { path = var . controller_health_endpoint matcher = \"200\" interval = var . controller_grace_period } } resource \"aws_lb_listener\" \"controller-listener\" { load_balancer_arn = aws_lb . controller - lb . arn port = var . controller_port protocol = local . controller_protocol default_action { target_group_arn = aws_lb_target_group . controller - target . arn type = \"forward\" } } resource \"aws_lb_listener_rule\" \"controller-listener-rule\" { listener_arn = aws_lb_listener . controller - listener . arn action { type = \"forward\" target_group_arn = aws_lb_target_group . controller - target . arn } condition { field = \"host-header\" values = [ aws_route53_record . controller - dns . name ] } } (If you‚Äôve never connected a DNS record, load balancer, and auto scaling group in EC2 before, the above might look like a lot of work, but it‚Äôs a pretty standard if verbose setup.) And with that, we now have Airflow up and running: the database can be setup and configured as desired, the scheduler will run, the controller will prep the system if needed before starting the web server, and we‚Äôre good to roll this out for testing. Of course you might choose to pass in your secrets in a different way, or add way more Airflow configurations, but it should be simple no matter what. You might have noticed that I did sneak in a few extra environment variables in those Airflow task definitions: the environment, the log level, the Consul address, and the Docker address. We found that having those always available helped our jobs to run (for example, we know every job can always check the environment it‚Äôs in) and allowed us to build custom utilities, especially around running Airflow tasks on ECS. Setting up Airflow tasks on ECS Airflow has an ECS operator that seems great to start with: run this little bit of code and you‚Äôre done! But‚Ä¶¬†not quite. Unfortunately, Airflow‚Äôs ECS operator assumes you already have your task definitions setup and waiting to be run. If you do, then go ahead and use the operator to run tasks within your Airflow cluster, you are ready to move on. If however you need to define those dynamically with your jobs, like we did, then it‚Äôs time for some Python. Remember how before I said we had pass through values in our Terraform module? That‚Äôs where those come in. Terraform is where we know things like the address for our Docker Registry, or how to connect to our data pipeline and data warehouse. By having Terraform pass those values into Consul, we can then write Python to pull it down and make use of it, same as with our data pipeline setup . See: logic to the madness! We have utility functions in our workflow setup for all of our most common actions: creating a DAG, creating a local (Python) operator, creating a remote (ECS) operator, getting values from Consul, posting to Slack, etc. These grew out of the Airflow provided functionality either not providing everything we needed or requiring additional setup that we wanted to keep standard and sensible for our team, like the remote operator setup. Here, let me even show you our publicly exposed function‚Äôs call signature and documentation for making a remote operator to run something in ECS: def make_remote_operator ( dag , task_id , task_group , task_name , task_container , task_start_command = None , task_memory = CpuValues . M . default_memory (), task_cpus = CpuValues . M , healthcheck_command = None , healthcheck_waittime = None , healthcheck_interval = None , healthcheck_timeout = None , healthcheck_retries = None , environment_values = None , secret_values = None , local_values = None , jit_values = None , throttling_group = None ): '''\n    Create a remote operator. Currently this is an ECS operator. Read more at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html .\n    \n    :param dag: DAG this operator will be part of.\n    :param task_id: Id of this task.\n    :param task_group: Enum value of WorkGroup this task is a type of, for example WorkGroup.ETL for an ETL task.\n    :param task_name: Name of task.\n    :param task_container: Container to run for task.\n    :param task_start_command: Start command to use, if different from default.\n    :param task_memory: How much memory (in MiB) to have; must be valid for the CPU setting. Use the CpuValues enum to verify valid values.\n    :param task_cpus: How much CPU to have, as a CpuValues enum.\n    :param healthcheck_command: Command to run to perform a health check, if any.\n    :param healthcheck_waittime: How long to wait before performing a health check, in seconds. Must be from 0 to 300. Disabled by default.\n    :param healthcheck_interval: How long to wait between performing a health check, in seconds. Must be from 5 to 300. Default is 30 seconds.\n    :param healthcheck_timeout: How long to wait before failing a health check that didn't response, in seconds. Must be from 2 to 60. Default is 5 seconds.\n    :param healthcheck_retries: How many times to retry the health check before failing. Must be from 1 to 10. Default is 3.\n    :param environment_values: List of environment value keys to fetch and populate from Consul.\n    :param secret_values: List of secret environment value keys to fetch and populate from Consul.\n    :param local_values: Map of key/values to populate.\n    :param jit_values: Map of key/values to populate that you cannot know until just before the job runs (just in time).\n    :param throttling_groups: Enum value of ThrottlingGroup this task if in the throttling group of, if any, for example ThrottlingGroup.WAREHOUSE for the warehouse group.\n    :return: ECS operator.\n    ''' Long list, isn‚Äôt it? This is the actual information needed to create a task definition and run it in ECS; you can see why we wrapped it instead of asking everyone on the team to go their own way and hoping for the best. To take you through the steps so you can create a utility that makes sense for your use case, let me walk you through the code as it executes. This actually brought us even deeper into how ECS runs than we needed to know for the controller and scheduler, while also giving the data team a way to easily run their containers remotely without needing engineering help. The first thing we do to create an ECS operator is to get all the values in a specific Consul directory, waiting for a remote task to use. You might find your values are different from ours but what we pop in there is: the ARN for the cluster to run in the execution role to use the launch type ( FARGATE ) the network configurations the AWS region name You can see that most of these were also used in setting up the controller and scheduler; that‚Äôs why passing these from Terraform to Consul is so important in allowing us to use them here. If you decided to move from Fargate to EC2, update Terraform, it‚Äôll tell Consul, and the Python that pulls these values down will automatically update your task definitions. Now with those basics out of the way, we need to generate our task definition, which is what the majority of those parameters are about. Essentially we want to generate the full JSON task definition that the ECS API uses, including settings like: the CPU and memory; we use an enum for them since ECS can be a bit picky the container and, if applicable, the command to start it with; this is where having the Docker Registry address comes in handy any configurations around health checks for the container while it runs environment and secret values, which we can pull from Consul if they‚Äôre present or supply from the job definition directly logging so we can follow what the container is doing This is the part of the code that requires the deepest understanding of ECS but, once implemented, should help new people find quickly what they need to supply and what it does without having to go to the AWS documentation. Alright, at this point we have a task definition but we haven‚Äôt registered it yet. AWS‚Äôs boto3 library provides a way to interact with ECS ; what we do is check for a current version of the task and pull it down. If there‚Äôs no task definition or, and this is the crucial part, if the task definition we just generated is different from what ECS currently knows about , we register a new version; otherwise we let it be. (We founded we needed deepdiff to do this comparison effectively as there‚Äôs no upsert method, so our system‚Ä¶¬†yeah it registered hundreds of thousands of task definitions for no reason. ü§¶üèª‚Äç‚ôÄÔ∏è) So, let‚Äôs say you have a data scientist on your team who changes a job so that a container‚Äôs start command and a few of the input values are different: this will detect that and push up an update to the task definition without the data scientist even having to know what a task definition is. It‚Äôs now and only now that we can finally invoke the ECS operator Airflow provides, putting any last minute overrides like just in time values (maybe input from the user triggering the job) into the operator and adding it to any pools as needed (for example, only letting so many systems interact with the warehouse at once). From there, Airflow handles telling ECS to spin up this task and execute, watching it until it‚Äôs done, and reporting back. That‚Äôs all a lot of work and you might be asking yourself, what‚Äôs the point? Well for our team, the point is that we already have a lot of containers and setup around them: a Docker Compose file , for example, to tell us how to run every task. By running these containers locally and in the workflow system, we know we will have consistent results, and we can extend our utilities to do things like read in the Docker Compose file (it‚Äôs just a YAML, after all) to help us generate our remote operator (what‚Äôs the container name, or startup command, or values needed?) with a bit more ease. For heavy duty, long running operations that might require a lot of CPU or memory, this gives us flexibility. (Currently Fargate doesn‚Äôt support GPUs but that‚Äôs on the roadmap and other ECS capacity providers do support it, which might be of particular interest to data teams.) Learnings from Airflow and ECS Picking up ECS was quite the challenge sometimes, and I‚Äôm not sure we would have made it ourselves without writeups from the community about how they managed it and AWS sending some lovely folks to visit us several months ago and answer our questions. Tasks and services, for example, were very confusing at the start and we couldn‚Äôt get our head around what we were suppose to be doing with them. We‚Äôd discovered ECS at the start of our journey as something people talked about running their Airflow setup on, but hadn‚Äôt found a lot of detail around how to actually do that, especially for working with ECS operators in Airflow. It‚Äôs been our goal from the start to not only get Airflow running and learn how to work with ECS for other systems the data team might have, but to also provide what we learned and did for others to have an easier time getting their system set up and running. If people want to know more than I‚Äôve already written, we can post follow ups; just let us know! Our Airflow setup actually arrived just in time for Jenkins going hard down. We were able to spin up our Airflow production with less than a day‚Äôs notice and start using it. Sure, it still has some quirks the team needs to get used to as Airflow is a hecka quirky system, but for the moment it‚Äôs taken over better than anyone could have anticipated. The rest of the team is quickly getting accustomed to how ECS works, where logs are, how to monitor it ‚Äî¬†everything we‚Äôd hoped for. Ideally in the future we‚Äôll be able to move other systems the team owns from EC2 to ECS, which will both ease what the team needs to know to maintain those services while also easing the burden on other teams to help us keep them up and running. After all, when we do our job well, no one even notices we‚Äôre here doing it, moving and processing terabytes of data like it‚Äôs no big deal, and that‚Äôs how we like it. Also, Airflow contributors? How about building out that ECS operator pretty please? Share on Twitter Share on Facebook", "date": "2020-07-06"},
{"website": "GameChanger", "title": "Navigation Controllers != State Machines", "author": ["Brian Bernberg"], "link": "http://tech.gc.com/blog/2013/9/25/navigation-controllers-state-machines.html", "abstract": "GameChanger supports many different team types e.g. Little League teams, high school teams, travel teams and various others.  Each of these team types require that we gather varying information from the user during the team creation flow.  Consequently, there are various different paths a user can take during this flow.  The app uses a navigation controller and various screens (with corresponding view controllers) to collect the relevant information.  All of this information needs to be stored and finally, once a user has input all required information, the app makes an API call to create the team.  Previously, we relied on these view controllers to each keep track of the team data.  With a re-design we just implemented, the information is now stored in a central controller that also controls the team creation flow. In our old design, team information was passed along from view controller to view controller via properties on each newly instantiated view controller.  This required each view controller to not only know about downstream view controllers but also to be able to determine which view controller to push on the navigation controller ‚Äì essentially, ‚Äúnext state‚Äù logic.  Additionally, information collected early in the flow needed to be passed all the way down the chain to the final ‚ÄúTeam Create‚Äù form.  All of this was a clear violation of the Model-View-Controller paradigm.  It was a brittle design that required a change in one view controller to propagate to all down stream view controllers‚Ä¶assuming whoever made the change realized this! Our old approach in ‚ÄòChoose Team Type‚Äô view controller. Note the next state logic and information passing via properties. Our new approach is to use a central controller that maintains state, stores the collected information and determines which view controller gets pushed next onto the navigation controller. We use a state machine (an instance of YBStateChart ) to organize the central controller, which makes the design concise and easy to read/understand. Create Team Flow state machine With this new approach, each view controller is self contained and does not require knowledge of anything aside from the information it‚Äôs responsible for collecting.  Once the user has input the necessary information on a screen, the view controller relays this information to the central controller, which in turn pushes the appropriate view controller on to the stack. New approach within ‚ÄòChoose Team Type‚Äô view controller Example of event handling in the central controller for the ‚ÄòTeam Type Chosen‚Äô event It is possible that a user needs to back up during the team create flow i.e. if they realize they made a mistake somewhere along the way.  Fortunately, navigation controllers easily support this by popping view controllers from the navigation stack.  Unfortunately, UINavigationController does not have a built-in notification for this situation.  The central controller clearly needs to stay in sync with the navigation controller so we implemented custom code to send notification when the navigation controller is popping.  This notification also allows the central controller to throw out any info that the popped view controller had collected since it‚Äôs no longer valid. Notification for indicating that a view controller is popping from the navigation stack\nSwitching to a central, state-machine based, model-view-controller design has resulted in a cleaner design ‚Äì more reusable, easier to understand and easier to modify as new team types come along.  With self-contained view controllers, changes are localized to just that view controller & the central controller. Share on Twitter Share on Facebook", "date": "2013-09-25"},
{"website": "GameChanger", "title": "Minimizing EC2 Costs", "author": ["Katherine Daniels"], "link": "http://tech.gc.com/blog/2013/9/11/minimizing-ec2-costs.html", "abstract": "As a growing startup, it‚Äôs important for us to keep our costs as low as possible. Recently, we came to the conclusion that our Amazon Web Services costs were higher than we thought they should be. Digging into things a bit further, we noticed that there were some EC2 instances that were up and running (and costing us money) that weren‚Äôt being used. Some of them had our code running on them, but weren‚Äôt configured behind the Elastic Load Balancers that would enable them to actually get traffic and be useful. Some of them didn‚Äôt have our code and weren‚Äôt even in our configuration management system, Chef. Those are servers that should either be stopped or configured properly, so we aren‚Äôt throwing away money on instances that weren‚Äôt providing any value. This could have been done by hand, but that would have been tedious, time-consuming, and not easily repeatable. I‚Äôm a strong proponent of the theory that nothing should be done by humans that could be more effectively done by computers, so I wrote a script that would do this for us. For each of our running EC2 instances, it checks to make sure that it‚Äôs in Chef as well as making sure that it is configured to be behind the correct load balancer. While this was relatively straightforward, it got a bit more complicated by the fact that we have 3 different environments: production, pre-production, and staging. So instead of simply checking ‚Äòis this web server behind the web load balancer‚Äô, it has to be aware of which environment the server is configured in as well. Unfortunately, it‚Äôs not possible to query a server regarding whether or not it‚Äôs behind a load balancer, because the server instances don‚Äôt know that. Instead, the script had to query each of the load balancers for all the the instances that are behind it. And because python, I used some fancy comprehensions. elbs = { env : { name : ebs_conn . get_all_load_balancers ( load_balancer_names = [ ELB_NAMES [ env ][ name ]])[ 0 ] for name in ELB_NAMES [ env ]. keys () } for env in ENVIRONMENTS } elb_instances = { env : { elb : [ instance_info . id for instance_info in elbs [ env ][ elb ]. instances ] for elb in elbs [ env ]. keys () } for env in ENVIRONMENTS } Two lines of code to get an organized set of all the instances behind all the load balancers for all the environments, and now we have an automatic check in place to make sure we‚Äôre not spending money on servers that aren‚Äôt being used. Share on Twitter Share on Facebook", "date": "2013-09-11"},
{"website": "GameChanger", "title": "Crowdsourcing Bug Finding", "author": ["Phil Sarin"], "link": "http://tech.gc.com/blog/2013/8/28/crowdsourcing-bug-finding.html", "abstract": "Ben and I are working on a big enhancement to our core Game Stream feature. It‚Äôs a huge improvement that requires upheaval of a ton of Javascript. We‚Äôve got to do it fast, and we can‚Äôt break our existing Game Stream in the process ‚Äî customers pay for it and they count on it! To keep risk in check while moving quickly, we‚Äôre deploying continuously. Various aspects of our new feature are going live throughout the week, even though the feature itself is still hidden from customers. On Monday, we replaced our stream poller with one that‚Äôs built on Backbone.js. Today, we started populating the models that will drive the new feature. Problem is, we knew with complete certainty that our code was going to break, and we didn‚Äôt know where or why. Finding those points of breakage could take a ton of effort! Baseball games have tons of states and weird data conditions, and our quickly assembled code couldn‚Äôt have accounted for all of them. It would take a day of testing to figure out all of the cases we didn‚Äôt account for, and we‚Äôd still miss something huge. So we‚Äôre getting our customers‚Äô browsers to help us out without disrupting our users‚Äô experience. We surrounded our new code (the fruits of which are still hidden from customers) with a try/catch, and reported errors back to our servers. So, our customers are finding our bugs for us, but, because we‚Äôre catching the errors, they aren‚Äôt feeling any pain from those bugs. try @ set 'fielding' , new FieldingPlayers ( @ makeFieldingPlayers ()) @ set 'offensive' , new OffensivePlayers ( @ makeOffensivePlayers ()) @ on 'change' , => try @ get ( 'fielding' ). reset @ makeFieldingPlayers () @ get ( 'offensive' ). reset @ makeOffensivePlayers () catch innerError logging . error \"GameStream update error #{ printStackTrace ({ e : innerError } )}\" catch error logging . error \"GameStream initialization error #{ printStackTrace ({ e : error } )}\" After a few minutes in production, we discovered a big bug and fixed it. We didn‚Äôt realize how commonly our Game Streams had no batter! Over the next few days, our code will keep running stealthily on customers‚Äô browsers and it‚Äôll report to us the errors that it finds. Our crowdsourced bug-finding won‚Äôt substitute for real traditional testing. Along with Eric, our testing ninja, we will do that too. But our approach does help us find and fix more stuff sooner, and that will help us ship more quickly. Can‚Äôt wait until we unveil our feature! Share on Twitter Share on Facebook", "date": "2013-08-28"},
{"website": "GameChanger", "title": "Summer 2014 Tech Internship Program", "author": ["Ben Yelsey"], "link": "http://tech.gc.com/blog/2013/11/18/summer-2014-tech-internship-program.html", "abstract": "We‚Äôre thrilled to announce the launch of GameChanger‚Äôs Summer 2014 Tech Internship Program! We‚Äôre looking for motivated individuals who code for fun more than school credit and want to cut their teeth on some real-world problems. Running our platform for scoring, stat-keeping, and sharing of amateur sports requires wrangling huge amounts of data (our user base creates more sports data in a day than MLB, the NBA, the NFL, and the NHL do all year combined), scaling infrastructure all the way to the clouds, and a keen sense of what coaches and sports fans want, need, and love.  Interns will gain experience with the technologies and process of building out an application that can handle our exponential growth, all in New York City‚Äôs vibrant startup scene. Interns will spend roughly 8 weeks working directly with our software developers. Every intern will have a specific mentor on the team making sure that their projects are organized, educational, fun, and shippable. Interns will get a competitive stipend and we‚Äôll help them find housing. We are: The scoring, sharing, and stats platform for Amateur Sports A startup in NYC Very friendly polyglottal programmers You are: A currently enrolled student in a college or university, in their Junior year (class of 2015). A prospective major in Computer Science or a related field, and/or with real-world experience developing applications A bona fide hacker We‚Äôve put together a programming challenge for potential interns. Applicants should show us what they‚Äôve got by coding up a game engine that can provide insights for the sport of their choice. More detail here . If you think you‚Äôre what we‚Äôre looking for, or if you know someone who is, check out our programming challenge and apply now! Share on Twitter Share on Facebook", "date": "2013-11-18"},
{"website": "GameChanger", "title": "How to Ship an App (Part 2)", "author": ["Phil Sarin"], "link": "http://tech.gc.com/blog/2013/11/16/how-to-ship-an-app-part-2.html", "abstract": "This is the second part in a series of how we‚Äôve learned to improve the quality of the GameChanger app. In part 1 , I talked about enforcing release criteria that are based primarily on quality and not time. A great way to reduce the risk of a release process is to reduce batch size. Batch size is the size of work that moves through the development pipeline. For instance, if you were to write 10 lines of code, then test it, and then ship it, you‚Äôd have shipped a 10-line batch of code. Though it‚Äôs counterintuitive, it‚Äôs usually more efficient and less risky to ship several small batches than a few large ones. As Fred Wilson writes , ‚ÄúBig changes create big problems. Little changes create little problems.‚Äù Eric Ries has a good post on the topic. You should read every word of that post. How we work in small batches It‚Äôs hard to keep batch sizes small with mobile apps. The App Store review process takes days, and so we have to ship multi-day batches of work to Apple. Even so, we can still operate more efficiently by keeping batches small through the rest of our process. Small code commits Big commits are risky and annoying. It‚Äôs basically impossible to review a huge diff carefully. Testing a big change is hard and time consuming. And big commits have more merge conflicts. It‚Äôs unusual for our 4 person iOS team to have fewer than 10 commits on a given day. Test in small batches We test things as soon as we build them, and we immediately fix the bugs that we find. We don‚Äôt let all the testing work pile up until just before we release. It‚Äôs more efficient for us to fix bugs while the code is still fresh in our minds. We also gain flexibility. Our codebase is always near a shippable state. If our priorities change or if our schedule slips, we can ship what we‚Äôve built, tested, and polished. Then we can replan. Our Trello board shows our testing progress. If we fall behind on testing, then we can bring in reinforcements from the marketing and customer service teams. Frequent regression tests Our customer service team manually regression tests our app three times  a week. As we improve our tools and automation, we expect to be able to do a full set of regression tests at least once daily. Prioritize unit and functional testing We favor unit and functional automated tests over end-to-end automated tests. By emphasizing unit and functional tests, we force ourselves to design software in a way that can be verified in small batches. When we are testing small batches, we can be more thorough and we reduce the risk that a bad bug will get to customers. This year, we‚Äôve increased our unit and functional test count by a factor of 15. As the graph below shows, the test count is steadily increasing. Use pre-release builds internally We do marketing demos and usability tests against pre-release builds. We kick off a pre-release build after each commit that passes unit and functional tests, and we push those builds to HockeyApp. This policy is another way of reducing our testing batch size. Everyone benefits. The marketing team gets to show off features that aren‚Äôt publicly released. Engineers, meanwhile, get early feedback on problems with recently checked in code. An obsession with testing The hardest part of working in small batches has been discovering problems quickly. That‚Äôs why most of this post talks about testing our app. The four approaches here ‚Äî immediate feature testing, regular regression testing, automated unit/functional tests, and regular use of pre-release code ‚Äî have helped us lower batch size. And smaller batch sizes reduce our risk and give us more options when things go wrong. There‚Äôs lots of room for us to improve. Much of our testing process, for instance, is still manual. In the last part of this series, I‚Äôll talk about how we‚Äôre laying the ground work to do a better job in the future. Share on Twitter Share on Facebook", "date": "2013-11-16"},
{"website": "GameChanger", "title": "Monitoring Node.js Using Datadog APM", "author": ["Matt Casey"], "link": "http://tech.gc.com/monitoring-node-js-using-datadog-apm/", "abstract": "Intro GameChanger‚Äôs Team Manager is built on top of a Node.js API server. Although we‚Äôve had a lot of monitoring and logging throughout it‚Äôs lifetime, most of our metrics were at an aggregate level. By alerting on symptoms, it was easy to spot that something was broken, but not necessarily what or why . With the server handling over 100 million requests per day, over 1000 per second, we needed a better way to drill into what the server was doing and figure out causes. Operating at that scale meant that local testing was at best an approximation of production realities. Integrating Datadog APM (Application Performance Monitoring) has solved a lot of our observability issues. It wasn‚Äôt always straightforward to work with‚Äîsome of the tooling isn‚Äôt obvious and the documentation is all over the place‚Äîbut now that things are up and running, it‚Äôd be hard to imagine a world without it. We‚Äôve also open sourced some of the APM utility code we made along the way as an NPM package . Outline What is APM? What does Datadog provide? What challenges did we face? What doesn‚Äôt Datadog provide? A feature wishlist Introducing @gamechanger/datadog-apm Conclusion What is APM? APM Stands for Application Performance Monitoring. It can take different forms from one offering to another, but it generally consists of breaking down what your server is doing while accomplishing a specific unit of work. For example, if your server offers an API, the APM will keep track of when a request starts, what functions are called to handle that request, what database queries are performed, and what the response is. A representative flame chart from Datadog‚Äôs APM for one API request. In Datadog‚Äôs words, APM ‚Äúprovides you with deep insight into your application‚Äôs performance - from automatically generated dashboards for monitoring key metrics, like request volume and latency, to detailed traces of individual requests - side by side with your logs and infrastructure monitoring‚Äù. What does Datadog provide? In addition to APM, Datadog has a host of monitoring options‚Äîlogging, monitoring, analytics, synthetics‚Äîand their web UI gracefully ties them all together. In order to make use of their offerings, Datadog provides a number of client libraries. For the purposes of this post we‚Äôll focus on their dd-trace Node.js library which enables APM. dd-trace has two sources of documentation: there is general Datadog APM documentation along with library specific documentation . Both are adequate to get things up and running, but for our needs we had to turn to a third source of documentation, the source code itself . Fortunately, the code is pretty straightforward, applying the same basic patterns across each of the libraries that it wraps. Although written in vanilla Javascript, dd-trace comes bundles with type definitions for typescript. What challenges did we face? Implementing APM wasn‚Äôt always straightforward; as mentioned earlier, we often had to reference the source code to accomplish what we needed. Getting tracing out of the way This was the hardest part: our philosophy is that monitoring and analytics code should not impact business logic code. Ideally, adding or removing tracing should be a single line change that doesn‚Äôt touch the business code at all. Since our code makes heavy use of classes, decorators were an obvious language feature to accomplish our goal. (Note: although decorators are only a stage 2 proposal for ECMAScript, they are available as a feature in Typescript (with the experimentalDecorators flag turned on) and through babel if you‚Äôre using vanilla JS.) Decorators allow us to augment classes and class methods without modifying the body of either. For example, if we had a decorator to log the name of the method being called, we could add it with a one line PR. Similarly, when we‚Äôre done debugging that function, we can cleanly remove that one line without being afraid of accidentally changing business logic. // Example PR Diff adding logging to MyClass.method class MyClass { +   @logMethodName() method() {} } With that in mind, we wanted our traced code to look like this: // Trace *all* methods of a class @ APM . trace () class GameChanger { public foo () {} private bar () {} } // Trace *individual* methods of a class class GameChanger { @ APM . trace () public foo () {} private bar () {} } This also allows easy modification of how we‚Äôre tracing our code‚Äîconfiguration of the traced code is also separate from the business logic. // The decorator can be configured to override the defaults class EmailQueue { @ APM . trace ({ serviceName : ' queue ' , spanName : ' queue.message ' }) public async pop () {} } The pattern itself isn‚Äôt new, and the Datadog Java client library actually provides this as its interface: import datadog.trace.api.Trace ; public class MyJob { @Trace ( operationName = \"job.exec\" , resourceName = \"MyJob.process\" ) public static void process () { // your method implementation here } } For Node.js, however, you currently have to manually start and stop your spans, as seen in the docs: const tracer = require ( ' dd-trace ' ). init () // sync code const span = tracer . startSpan ( ' web.request ' ) span . setTag ( ' http.url ' , ' /login ' ) span . finish () For synchronous code, like that above, this manual tracing is straightforward. Things get quite cumbersome, though, when dealing with async functions. Javascript has no built in way to detect if a function is a promise, with the best approximation being to check if the returned value is a function itself. And example of this is the dd-trace-js internal code itself: class DatadogTracer extends Tracer { ... trace ( name , options , fn ) { options = Object . assign ({}, { childOf : this . scope (). active () }, options ) const span = this . startSpan ( name , options ) addTags ( span , options ) try { if ( fn . length > 1 ) { return this . scope (). activate ( span , () => fn ( span , err => { addError ( span , err ) span . finish () })) } const result = this . scope (). activate ( span , () => fn ( span )) if ( result && typeof result . then === ' function ' ) { result . then ( () => span . finish (), err => { addError ( span , err ) span . finish () } ) } else { span . finish () } return result } catch ( e ) { addError ( span , e ) span . finish () throw e } } ... } Wrapping each function like that was out of the question, again we wanted to add a single line decorator and be done with it. There were some gotchas, especially in getting typescript to play nicely with a decorator handling both classes and methods, but in the end it worked out great. We made a helper library to hide the complexity behind a decorator interface, which you can read about further down the page . Making tags searchable When we started, you could only search tags that were added to your root span, but there was no documented way to access the root span. There is, however, an accessible property on the Koa context passed to each piece of middleware, context.req._datadog.span . Using that, we were able to add tags to our root span and make them into searchable facets. This is no longer an issue: facets can be created from nested spans as of the writing of this post. That said, there still may be times you want to add tags (but not global tags) to your root span. We included a getRootSpanFromRequestContext utility function in our helper library . Marking non-throwing spans as errors (and dealing with null values) Knowing whether a span was successful is important in almost every part of consuming APM data. Unfortunately it wasn‚Äôt clear straight away how to mark a span as being an error. By default uncaught errors will be flagged, but if your code handles errors you‚Äôre out of luck. For example, in our queue processor, if we have an error we push to an error queue but don‚Äôt throw anything. All that it takes is adding a few of the correct tags to a span for everything to work. The issue is that getting a span can be cumbersome. The tracer.scope().active() function will return the current span, if it exists, or null. That means all of your code needs to handle the null case. If you don‚Äôt mind having if (span) {} littered about your code, perhaps you should be writing golang. In our case, we ended up with a utility function APM.markAsError(error: Error) that handle getting the current span and dealing with a null span for us. Learn more in our helper library . Making spans show up in App Analytics Note: Previously, the documentation on this was less straightforward. It‚Äôs also a less pressing issue now that the new ‚Äútraces‚Äù screen has been rolled out. Getting runtime metrics working This was our fault‚Äîwe were on an old version of both the Datadog agent and Node.js. Another dependency kept us from jumping straight to Node 12, but once we were on Node 11, everything was working as expected. If you have issues getting runtime metrics to work, try upgrading to a newer version of node (we were on 8 when we had the issues). What doesn‚Äôt Datadog provide? A feature wishlist Service page span name selector This is no longer relevant at the time of posting this article. A dropdown selector allows changing which span name is shown on the service page. Previously you had to manually change the URL to the desired span name. Working filter estimates With our number of events, we need some filtering to keep our costs in check. There is a tool to estimate the effect of event filtering on event count, but it was often necessary to simply wait a day to see if the filter had the desired impact. Extrapolated query metrics You can pull in App Analytics queries to any dashboard. However, they will be sampled and there is no good way to get an extrapolated number of the total. If you create a search filter to find the number of POST /games requests, App Analytics will show you an upscaled chart in the top left of the page. If you pull that query into a dashboard, the raw number will be shown in the chart. If APM sampling were a simple percent cut of all events, it‚Äôd be easy to manually extrapolate, but the sampling is dynamic. That makes the dashboard charts utility questionable. Custom measures from tags In App Analytics, the duration tag is special. You get a min/max selector in the facets column, and more importantly it is treated as a ‚Äúmeasure‚Äù in charting. Custom tags, despite being marked as integer or double, are treated as distinct values which limits their value in filtering and charting. Introducing @gamechanger/datadog-apm If you‚Äôre writing a Node.js app with any classes, consider checking out @gamechanger/datadog-apm . In our experience, the decorator syntax has made adding tracing a dead simple process. There are some other utility functions and conveniences built in, so you may even find value if you‚Äôre not using OOP in your code.. Conclusion If you‚Äôre not using an APM product, you probably should be. For a small performance overhead, you get a completely different class of insights than what logs and request metrics can offer. Datadog‚Äôs APM is a fantastic offering that keeps getting better‚Äîthe outline of this blog was put together a couple months ago, and by the time of writing much of the ‚Äúpain point‚Äù section was already out of date. Yes, some of the implementation can be cumbersome, but once set up the web UX/UI is responsive and intuitive. Share on Twitter Share on Facebook", "date": "2020-01-29"},
{"website": "GameChanger", "title": "Crash Course to Redshift", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/crash-course-to-redshift/", "abstract": "Redshift. It can store insane amounts of data. It can also store insane amounts of surprises, considerations, new ideas to learn, skewed tables to fix, distributions to get in line, what‚Äôs a WLM, what am I doing‚ÄΩ This post is meant to give you a crash course into working with Redshift, to get you off and running until you have the time and resources to come back and internalize what it all means. This is by no means a comprehensive review of Redshift, as then it‚Äôd no longer be a crash course, nor does this dive into data warehousing specifics, which I can cover in another post if people want. At a high level what I‚Äôll be covering is: Introduction to Redshift Table design Table analysis Data loading Debugging The vast majority of this post actually comes from our internal documentation, so you can trust that we do use this to help educate those less familiar with Redshift, and get them ramped up and feeling comfortable. Introduction to Redshift On Redshift The Redshift database will behave like other databases you‚Äôve encountered, but under the hood it has some extra considerations to take into account. The main difference between Redshift and most other databases you‚Äôll have encountered is due to scale, with the cluster being important to keep in mind in table design along with standard table design considerations. And since the scale is so much larger, the impact of IO can go up considerably, especially if the cluster needs to move or share data to perform a query. The reasons for this and how to best avoid these inefficiencies are detailed below. More on Redshift database development here. On distributing data Within a Redshift cluster, there is a leader node and many compute nodes. The leader node helps orchestrate the work the compute nodes do. For example, if a query is operating only on data from May of 2017, and all of that data is stored on a single compute node, the leader only needs that node to perform the work. If instead a query is operating on data from the full available timeline, all compute nodes will be needed and they may need to share data across themselves. If a query can be performed in parallel by multiple nodes, then congratulations: your data has been distributed well! ( More on parallel processing here. ) By allowing each compute node to work independently, better performance is achieved. If a query being performed requires multiple nodes to share data across each other constantly, then it will take a lot more effort for the query to be executed and optimization may be needed. While having a query that requires no passing of data ever is highly unlikely, as there is a cost with keeping data pristine at all times, distributing the data in such a manner that we minimize the passing of data will allow the cluster to run efficiently and make best use of Redshift. On IO and performance hits Disks are slow. Reading to them, writing to them ‚Äî while Redshift tries to optimize queries as much as it can ( more on query performance tuning here ), this required work cannot be optimized around at query execution time. Instead it must be considered and finessed when the table is designed, so that data on disk is as optimized as it can be for the query that comes in. For this reason, when designing your table it is advantageous to know what your most important query will be so that you can ensure the design of the table assists the query. ( More on query design here. ) More on why you need to consider disk IO here. Table design I‚Äôm going to assume that you know what column types and sizes you want, and skip constraints as more advanced than this post is meant for, though consider those if you want . More on table design here. Compression Redshift stores data by column, not by row, and by minimizing the size on disk of columns, you end up getting better query performance. The reason is that more data can be pulled into memory, which means less IO needs to be done fetching more data as the query runs, thus better performance: narrow columns (ie tightly compressed columns) thus help work zip by. The exception to this is the columns you leverage as sort keys: if those are highly compressed, it‚Äôs more work to find the data on disk which means more IO. Confused? Amazon has a helpful workflow for deciding if you should or shouldn‚Äôt compress a column. Now that I‚Äôve convinced you, what compression to pick for your columns? ANALYZE COMPRESSION TABLE_NAME_HERE ; The easiest way to determine the optimal compression is to finish designing the basics of your table, load sample data in, then utilize the ANALYZE COMPRESSION command (statement above, more on it here ). Its output will tell you the compression that best works for your sample data for each column, thus doing all the work for you. From there, update your table definition and load the data again. Your disk size should now be smaller (disk size query provided in table metadata section). Still not sure what to pick? Perhaps you don‚Äôt have data yet? Here‚Äôs an easy to remember rule of thumb: If it‚Äôs your sort key or a boolean, use RAW Otherwise, use ZSTD That should get you started until you have enough data to go in and reviews compression choices, as ZSTD gives very strong compression across the majority of data types without a performance hit you‚Äôd notice. More on compression here. References to compression and performance can be found here , here , and here . Distribution and sort It is important to understand the difference between distribution of data and sort of data before moving on to how to use them to your advantage, as they can have the biggest impact on your table‚Äôs performance. Distribution of the data refers to which node it goes to. Sort of the data refers to where on the node it goes to. If your distribution style is even, that means all nodes will get the same amount of data. Or if your distribution style is by key, each node will have data from the same one or more keys. Once the node your data will live on is decided, the sort impacts its ordering there. If you have time sensitive data, you may want each node to store it in order of when it happened. As data comes in, it isn‚Äôt necessarily sorted right away (unsorted data discussed below) but it will be by Redshift as and when necessary or forced (such as during maintenance). Distribution There are two times when data is distributed: When data is first inserted When a query requires data for joins and aggregations The second scenario is more important in terms of the performance impact, as having the data already where it needs to be for a query will have the biggest savings impact by allowing data to only be distributed in the first scenario without a redistribution that slows down the user‚Äôs query. An ideal distribution of data allows each node to handle the same amount of work in parallel with minor amounts of redistribution. This is true both within a table and across tables: two tables constantly joined should have similar distributions so that the data needing joining is already present on the same node. Using the most important and intensive query(ies) allows for the appropriate distribution style to be chosen ( more on using the query plan for distribution decisions here ), of which there are three options, ranked from least likely to be of use to you to most likely: An ALL distribution puts a copy of the entire table on every node. An EVEN distribution splits data up evenly across all nodes without looking at the content of the data. This is helpful if you never join the table with other data or there is no clear way to leverage a KEY distribution (below). A KEY distribution splits data up according to part of the data (the key). Start by seeing if there‚Äôs a particular key that your query is dependent on. If there‚Äôs no obvious one or no joins with other tables, then consider an even distribution. In a staging environment, you can also try setting up the table multiple ways and experimenting with what would happen to get an idea of the impact of the different distribution styles. More on data distribution here. More on best practices here. Sort The sort of data can be leveraged in query execution, especially when there is a range of data being looked at: if the data is already sorted by range, then only that chunk of data needs to be used rather than picking up a larger number of smaller chunks of data. I now regret using the word ‚Äúchunks‚Äù but we‚Äôre sticking with it. There are two options for sorting and which one you pick is highly coupled with the query(ies) you will execute: A COMPOUND sort key uses a prefix of the sort keys‚Äô values and can speed up JOIN , GROUP BY , ORDER BY , and compression. The order of the keys matter. The size of the unsorted region impacts the performance. Use with increasing attributes like identities or datetimes over an INTERLEAVED key. This is the default sort style. An INTERLEAVED sort key gives equal weight to all columns in the sort key and can improve performance when there are multiple queries with different filter criteria or heavily used secondary sort columns. The order of the keys does not matter. Performance of INTERLEAVED over COMPOUND may increase As more sorted columns are filtered in the query. If the single sort column has a long common prefix (think full URLs). For what to put in the sort key, look at the query‚Äôs filter conditions. If you are constantly using a certain set of columns for equality checks or range checks, or you tend to look at data by slice of time, those columns should be leveraged in the sort. If you join your table to another table frequently, put the join column in the sort and distribution key to ensure local work. Using EXPLAIN on a query against a table with sort keys established will show the impact of your sorting on the query‚Äôs execution. More on data sorting here. More on best practices here. Table analysis There are built in commands and tables that can be used to generate and view certain metadata about your table. To ease your burden, the following queries are provided for you premade. More on the ANALYZE command here. More on analyzing tables here. Table schema SELECT * FROM pg_table_def WHERE tablename = 'TABLE_NAME_HERE' ; Table compression ANALYZE COMPRESSION TABLE_NAME_HERE ; The results will tell you, for each column, what encoding is suggested and the size (in percentage) that would be saved by using that encoding over what is currently there. Table metadata ANALYZE VERBOSE SCHEMA_HERE . TABLE_NAME_HERE ; SELECT tableInfo . table AS tableName , results . endtime AS lastRan , results . status AS analysisStatus , results . rows AS numRows , tableInfo . unsorted AS percentUnsorted , tableInfo . size AS sizeOnDiskInMB , tableInfo . max_varchar AS maxVarCharColumn , tableInfo . encoded AS encodingDefinedAtLeastOnce , tableInfo . diststyle AS distStyle , tableInfo . sortkey_num AS numSortKeys , tableInfo . sortkey1 AS sortKeyFirstColumn FROM SVV_TABLE_INFO AS tableInfo LEFT JOIN STL_ANALYZE AS results ON results . table_id = tableInfo . table_id WHERE tableInfo . schema = 'SCHEMA_HERE' AND tableName = 'TABLE_NAME_HERE' ORDER BY lastRan DESC LIMIT 1 ; This query has two parts: the first analyzes the table ( VERBOSE here indicates that it updates you of its status as it runs) and the second outputs metadata from two system tables. The columns have been aliased for easier reading. All columns in the SVV_TABLE_INFO table here. All columns in the STL_ANALYZE table here. Data loading More on data loading here. Unsorted data As data comes into a node, it is not always efficient to sort it in right away. To see how much of a table is unsorted, you can leverage SVV_TABLE_INFO.unsorted from the above table metadata section. A smaller unsorted region means more data is exactly where you told the node it should be. If data tends to come in slowly, regularly running VACUUM will clean up the unsorted region. This can be done as part of regular maintenance at a time when it will have the smallest impact on users. If data tends to come in in large batches, see below for efficient bulk loading. If data tends to be removed from the table wholesale, truncate instead of deleting the rows. TRUNCATE will clean up the disk space whereas DELETE does not. More on managing the unsorted region here. Efficient bulk loads Some tips for all bulk loads: Load your data in sort key order. Do not do loads during maintenance windows. From S3 Use the COPY command. You can even have it choose column compression for you. More on loading from S3 here. From another Redshift table Use a bulk INSERT / SELECT command. In a SQL statement If the data is not yet available on any remote host, use a multirow INSERT command. Debugging What just happened? You can query for queries that have been run in multiple ways: If you want all queries regardless of type, use the statement table . If you want all queries that were DDL, use the DDL table . Eg create table, add column, etc If you want all queries that were DQL, use the query table . Eg select, insert, update, delete, etc If you want all queries that are neither DDL nor DQL, use the utility table . Eg grant, create user, commit, etc InternalError: Load into table 'X' failed. Check 'stl_load_errors' system table for details. It is recommended you to turn on \\x to view this table. SELECT errors . starttime , info . table , errors . colname , errors . err_reason , errors . raw_line FROM stl_load_errors AS errors LEFT JOIN SVV_TABLE_INFO AS info ON errors . tbl = info . table_id ORDER BY 1 DESC LIMIT 5 ; --that‚Äôs just so you don‚Äôt get overwhelmed Show all tables SELECT schemaname , tablename FROM pg_table_def WHERE schemaname != 'pg_catalog' AND NOT tablename LIKE '%_pkey' ORDER BY schemaname , tablename ; Describe a table SELECT * FROM pg_table_def WHERE schemaname = 'public' AND tablename = 'TABLE_NAME' ; What are the largest tables? SELECT tableInfo . schema AS schemaName , tableInfo . table AS tableName , tableInfo . unsorted AS percentUnsorted , tableInfo . size AS sizeOnDiskInMB FROM svv_table_info AS tableInfo ORDER BY sizeOnDiskInMB DESC LIMIT 25 ; Congratulations, you now have the GameChanger Data Engineering seal of approval for Redshift Basics! As you work with Redshift, you‚Äôll start to develop your own rules of thumb and opinions that might add on to what I‚Äôve presented or differ from those rules of thumb we use here. And that‚Äôs ok! Redshift is an evolving system that is designed for many different use cases: there is no right design. And remember, this isn‚Äôt everything there is to know about Redshift nor even all of the features it has for you to make use of. However this does cover the vast majority of basic use cases, and basic use cases are what you want to break your problems into. It‚Äôll make your life easier, and your Redshift work easier too. Share on Twitter Share on Facebook", "date": "2020-03-30"},
{"website": "GameChanger", "title": "I'll say this exactly once", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/ill-say-this-exactly-once/", "abstract": "You‚Äôve finally got everything working in your staging environment: the new systems talk to each other, everything is running smoothly, your dashboards are beautiful and pristine. Now you need to get it into production with zero down time and no interruptions. And then you realize, you‚Äôve no idea what you actually did in your staging environment. Let‚Äôs talk about infrastructure I don‚Äôt consider myself an ‚Äúinfrastructure lady;‚Äù it‚Äôs just not my jam, and that‚Äôs fine. However I have had to learn a lot about our infrastructure for setting up our new data pipeline system ‚Äî¬†and I don‚Äôt just show up to learn the minimum. I come to m a s t e r . IaC , or infrastructure as code, is the idea of setting up your infrastructure using definition files, code, and standard programming practices. In a way, it brings infrastructure to those of us who might otherwise be overwhelmed by what‚Äôs going on and what we‚Äôre suppose to do. Why manually enter configuration values when you can store them in a YAML or JSON file in a git repo? Suddenly you can see its history, you can search it, and it documents itself to a certain extent. This last point is of particular interest to me, as so much of the data pipeline is just how the systems work together, what the configurations are to facilitate this, and and documenting how all this works. If ‚Äî if ‚Äî we could have our data pipeline and all its friends live in some straightforward, self documenting IaC setup, it would make it easy not only to remember what we did but also to onboard someone new to the system and to deploy it to different environments with high confidence that it‚Äôll work. At GameChanger, that meant making three systems work together: Terraform , Consul , and Ansible . Terraform sets up what we want the landscape of our machines to look like. Ansible sets up what we want the landscape on our machines to look like. And Consul is the new kid in the collection, just here to have a good time and be helpful. Leibnizian optimism Alright, so we know what our tools are and we know what we want to do: using Terraform, Ansible, and/or Consul in some way be able to spin up the full data pipeline with a single command configurations should make their way to all the systems that need them automatically and we should be able to use this tool we produce for multiple environments As our Ansible setup will work within the confines of what Terraform sets up for it (can‚Äôt set up a machine that doesn‚Äôt exist yet), Terraform is where we‚Äôll want to start. And this makes sense on a second level too since a lot of what Terraform will output, like the addresses to services it‚Äôs brought up, will be used by Ansible to set configuration values. Therefore, we need those machines set up before we can configure them. Starting with Terraform, there were a few key things I came across that helped me put together a plan of an ideal end state: you can pass in input variables you can pass out output values you can set up a module , which functions kind of like a class Well, I know what I‚Äôd do if I was writing a class to accomplish what I want: take in a few values that specialize the pipeline for the specific environment I want to set it up in, do all the internal secret sauce, then send back the configuration values that are needed for other systems to connect to the pipeline. Running it would thus get me a pipeline ‚Äúobject‚Äù which is, ultimately, one of the few things I want in this world. That and a cat. What we‚Äôve got now is shaping up to be a nice little plan: make a Terraform module; pass in the values that make it unique for an environment; pass out the values that are needed to connect to it. We‚Äôll then need a way to get those values to Ansible, probably using Consul, but one foot in front of the other. Let‚Äôs get ready to M O D U L E A nice thing about Terraform is it figures out the order to run your blocks of instructions in, meaning you can structure your file so that it makes sense to humans. Our Terraform module had a head start in that, before I made the module, I‚Äôd set up bits and pieces of the pipeline in different Terraform files that didn‚Äôt work together but could be refactored into one location. That‚Äôs because Terraform is, despite what it might seem, oddly easy to work with once you get used to reading the documentation and using the examples to make your own version of whatever you need. (Sure, it inevitably needs its own special snowflake version of something vaguely YAML-esque to work, but at this point we all know that‚Äôs how large tech projects assert dominance in the world.) Starting with what the module needed to do helped guide figuring out what needed to go in and come out: bring up a hosted Kafka with its configurations bring up a Schema Registry bring up a Kafka Connect make sure these services can talk to each other make sure other services that should be able to talk to these three pieces can bring up any S3 buckets we want to make use of set up nice DNS records to make it easier for humans to know what they‚Äôre talking to Sure, that‚Äôs a long list, but once the module is set up, it‚Äôll be only one thing that handles all of the interconnectivity, which is thus also documented by the module. That would mean we‚Äôve already covered a huge amount of our ideal end state. Layered like an onion I started at the logical core and worked my way out for the Terraform module, making notes of what I‚Äôd want to have passed in as I went. Everything of interest lived in either main.tf , where I did all the fun Terraform adventuring, or vars.tf , where I documented what a user of the module would need to know. // Must provide variable \"pipeline_name\" { default = \"test\" description = \"The value to use in naming pieces of the data pipeline. This will most likely be the name of the environment but might also be another value instead.\" } variable \"environment\" { } // ‚Ä¶ // Can override variable \"kafka_version\" { default = \"2.2.1\" } // ‚Ä¶ Sample from the vars.tf . Using the description field was particularly helpful in ensuring the ability to make sense of the module without further, separate documentation. I also split the variables into what must be provided at the top and what could be provided at the bottom. Comments not only separated each block of Terraform work in main.tf but also let me put in markdown-style links to where further documentation was, in case someone wanted to read more about, say, the pipeline. I‚Äôd like in the future to go back and break down main.tf into smaller files, one for each chunk of work, but that‚Äôs more advanced than my current Terraform skills so will wait for another day. My favorite thing I‚Äôve learned from Terraform is how many AWS resources can have tags : EC2, MSK, security rules, if you can name it, you can probably tag it! These tags are helpful not just while in AWS, figuring out what is what and searching for something specific, but also can propagate elsewhere as a sort of shared configuration: Ansible can see tags but so can Datadog, for example. Now you can scope your host map, using only tags! Imho the following tags are what I feel best capture what you need to know without going overboard: the environment you‚Äôre in the jurisdiction this piece is part of for example this item could be part of the data-pipeline , or ci-cd , or maybe a collection of related microservices that, together, form one system the project_version , especially if you‚Äôre doing upgrade work the service this actually is under the hood and the purpose of this piece within the grand scheme of things The difference between the last two might be something like service: msk and purpose: pipeline , or service: kafka-connect and purpose: extract . The purpose tag is like a shorthand, then, for what you‚Äôre trying to accomplish without getting bogged down in how you‚Äôre accomplishing it. I could change out MSK for a self-hosted Kafka, but the purpose of that piece would still be to function as the pipeline. Sharing is caring We have our Terraform module now. It‚Äôs beautiful. It‚Äôs orderly. It‚Äôs doing its best to prevent the universe from descending into chaos. We can bring up the whole thing with a single command. Checking our list of hopes and dreams, we now have: using Terraform , Ansible, and/or Consul in some way be able to spin up the full data pipeline with a single command configurations should make their way to all the systems that need them automatically and we should be able to use this tool we produce for multiple environments ( 50% done ) Aight, some progress, but‚Ä¶¬†well, our output goes into our output.tf file but‚Ä¶¬†that‚Äôs not somewhere Ansible can get those values, let alone other services. We need to work on that. But oh! Remember that third service we can use? Consul? Time to shine. If you search for using Consul with Terraform and Ansible, you will get this and this respectively. Those are not the pieces we want. What we want is to use just Consul‚Äôs key-value store functionality, which you‚Äôll find in the same Terraform doc a bit further down but in a distinct part of the Ansible docs because it‚Äôs actually a completely separate part of the system. Go figure. Well, if we can have Terraform produce the configurations into Consul, and Ansible consume the configurations from Consul‚Ä¶¬†that should be what‚Äôs left on our list! Declaring our values Instead of the Consul keys block in Terraform, I actually found the similar sounding but slightly different Consul key prefix block in Terraform to be what I wanted, as it lets me group all my configurations in the most straightforward way possible. // Consul resource \"consul_key_prefix\" \"data-pipeline-configs\" { path_prefix = \" ${ format ( \"data-pipeline/%s/%s/\" , var . environment , var . pipeline_name ) } \" subkeys = { \"pipeline_cluster_name\" = \" ${ aws_msk_cluster . pipeline-cluster . cluster_name } \" \"pipeline_cluster_version\" = \" ${ var . kafka_version } \" // ‚Ä¶ } } Sample of how Terraform outputs are pushed into Consul. Some of this was remembering the inputs originally passed in, but a lot of it was taking the values Terraform had helped create and remembering them for use later. What‚Äôs the address for the Schema Registry? \"schema_registry_servers\" = \"${aws_route53_record.schema-registry-dns.name}\" set the value in Consul. What‚Äôs the name of the bucket I want to use as an archive? \"archive_bucket\" = \"${aws_s3_bucket.archive.id}\" set the value in Consul. You get the idea. Combing through all the configurations I had set in Ansible and in different services, I was able to move all values that would ever change into Consul. This was useful in not just, for example, sharing what is the expected number of partitions a pipeline topic should have ( pipeline_cluster_partitions ) with services that should match that expectation, but also in having a place where a human can go look up all current values. Example of what we‚Äôve stored in Consul. Once it was confirmed that all of the values were making it from Terraform to Consul, it was time to start using them. Configurations for the lazy As Ansible had been used to help determine what Terraform should put into Consul, it then became a matter of replacing the hardcoded values with getting the values from Consul: thus, never again would Ansible need to be updated for a configuration change. Using Ansible‚Äôs Jinja support, all we had to do was change something like KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://boring-and-obtuse-record-name:some-port-you-keep-forgetting out with KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT:// {{ lookup ( 'consul_kv' , 'data-pipeline/staging/staging/pipeline_bootstrap_servers_plaintext' ) . decode ( 'utf-8' ) }} : {{ lookup ( 'consul_kv' , 'data-pipeline/staging/staging/pipeline_bootstrap_servers_plaintext_port' ) . decode ( 'utf-8' ) }} Sure, that takes up hella more space on the line, but it gets the values automatically. If you change what you want your port to be, just have Terraform update Consul! Ansible gets that update for free. The eagle eyed among you will notice the .decode('utf-8') after each lookup. Funny story: everything came back with a b in front of it because of the way the lookup was parsing Consul‚Äôs values (they all came back as byte literals since we run Ansible using Python 3). The short answer to how to fix this is to force encodings. The long answer is‚Ä¶¬†longer, and Ansible isn‚Äôt something I 100% understand so ‚Ä¶ At least you can run Python functions easily in Ansible. Wrapping up our project Let‚Äôs check back in on our list: using Terraform, Ansible, and/or Consul in some way be able to spin up the full data pipeline with a single command configurations should make their way to all the systems that need them automatically and we should be able to use this tool we produce for multiple environments We did it people! We did it! Now any system that needs to talk to the data pipeline can just ask Consul for the values using the very straightforward API, or let Ansible set up the values for them. Terraform can do everything its little heart desires in a sane way that humans can read and understand. Our data pipeline lives. And best of all, our configurations have a single source of truth. In summation a data engineer who doesn‚Äôt know a lot about infrastructure simplified setting up a complex, interconnected infrastructure and got configurations sharing between the different systems all in a self documenting way all in the same project Now that, my friends, is called a victory. Let‚Äôs launch this into production, shall we? Share on Twitter Share on Facebook", "date": "2019-12-18"},
{"website": "GameChanger", "title": "When you deserve better (systems)", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/when-you-deserve-better-systems/", "abstract": "Preface Before we begin, I suggest settling in. Unlike my previous post , this one won‚Äôt be short. It‚Äôll also be more technical, though I will do my best to link to resources in case you get lost along the way. So my suggestion is get a nice cuppa , turn off notifications, and brace yourself: We‚Äôre going on an adventure. In the beginning Back in 2016, when we were all younger and more innocent, Alex Etling wrote a series of blog posts about his learnings in setting up our original data pipeline , which was built around Kafka , which is built upon Zookeeper . I‚Äôll let his blog posts speak for themselves in case you‚Äôre interested in that history, though they‚Äôre not necessary prerequisites for this post if you‚Äôd rather save them for later. Fun With Kafka: Adding a New Box Type Part 1 and Part 2 Experimenting With Kafka Scaling With Kafka Instead to give you a lay of the land, here‚Äôs a high level overview of the data pipeline and data warehouse systems, to function as a map for our adventure: Bringing back this architecture diagram, as it‚Äôs so wonderful. [1] Spiffy, huh? Thus the Kafka and the Zookeeper clusters, and all the host of them, were finished This pipeline setup has allowed us to do things like grow our data warehouse , provide business intelligence to data users, and do deep analysis as a data team to answer questions about how we can better serve our customers. At the same time, a lot has changed around this system: in our infrastructure setup, in Kafka‚Äôs capabilities, and in who uses the pipeline and for what. Back then, the desire was to answer a few basic questions about the customers using a variety of data; now the consumers of our data go in hard to look at the nitty gritty details themselves and really understand the complexity in the data to get their answers. What makes a team successful? Are our features helpful to teams across sports and ages? The people need to know. You cannot know everything a system will be used for when you start: it is only at the end of its life you can have such certainty. [2] Perhaps one of the biggest changes though was hiring a dedicated data engineer ‚Äî¬†me! I had the advantage when I started over Alex in that he had had to learn about Kafka and data pipelines from scratch, whereas I started here specifically because I love both of those and had experience with them. Want to talk about event driven architectures ? Data on the inside versus on the outside ? Logs ? Your favorite part of the Kafka documentation? I think the design section is really an underappreciated gem that is super easy to get into and follow. I learned Scala explicitly to work with Kafka and be able to read the source code, which has led me to a passion for functional programming and test driven development. This is what gets me excited to come to work. But it is important to state that the system I came in to take ownership of was impressive despite its age and dust: there were super detailed dashboards about the metrics, there were tools set up to monitor the clusters, there were libraries for producing into Kafka from outside the JVM when Kafka was still very much JVM only ( take a peak at our original Python client if you‚Äôd like). The problem was that systems, like code, are for people , and naturally over the course of people‚Äôs careers, they will move on to other projects, jobs, and passions. With the original folks gone from the project, a lot of oral knowledge had been lost. What do all the metrics mean? Why is this function of code like this? Also, what does it do ? Let‚Äôs take a step back Today we‚Äôre pushing the system harder than ever and don‚Äôt have a lot of options we like the look of. We‚Äôre two major versions behind so upgrading Kafka is high risk, but that also means we can‚Äôt take advantage of things like streams to relieve pressure elsewhere like on our data warehouse with its batch ETL process. Sometimes we encounter problems where the Internet‚Äôs main solution is to upgrade our Kafka version, but that‚Äôs not exactly something you can do in an incident where you‚Äôre trying to make sure you minimize data loss. What if we have a problem and can‚Äôt fix it? What if the system goes hard down? Is that it? We value keeping the systems our customers rely on operational; everything else can wait. Except for the day you realize that you have internal customers, and suddenly this system that seemed less important than all the rest jumps up in the ranking. Write once, run forever, debug forever Now this all sounds bad but an important question to ask is how bad? Light rain when you don‚Äôt have an umbrella bad? Or suddenly we‚Äôre all using nuclear reactor failure language bad? Can we Apollo 13 our way out of this? Also, small correction from earlier: you know that architecture diagram up there? The one with clean lines? You and I both know it‚Äôs a lie and there are things not captured in it, code or dependencies you come across in the wild and silently to yourself mouth, ‚Äúoh no,‚Äù while contemplating laying face down under your desk to signal to others that, right now, you need alone time. We use a lot of metaphors to describe technology, and I definitely do my part in this, so here‚Äôs another: systems are like gardens, they grow over time. My father loves gardening, and as a child I remember he‚Äôd come home from work and go right into the garden, just walking around and taking it all in. He‚Äôd finally come back with a hand full of sticks and weeds he‚Äôd pruned and collected to compost or recycle before coming inside. He knew what was suppose to be in his garden, what wasn‚Äôt suppose to be there, and how to take care of each of his plants. We couldn‚Äôt necessarily say the same thing about our system, so we performed an audit. Sounds exciting, I know, but it really was valuable. Every single part of the system, from large open source systems like Kafka and Zookeeper down to small helper libraries and even just configuration files were documented. We collected what version it was and when was that released, compared to the latest version and release date. We collected what language and version it was implemented in. We linked to Github repos for open source code and documentation about how to use the systems. We had everything in one giant document. Breakdown of different data pipeline service languages and their versions, and release years for versions we‚Äôre using. You might think the unknown release year bar is terrifying but the unknown Java version is what really upsets me. [2] Then and only then we could look at it and figure out what we had compared to what we‚Äôd thought we‚Äôd had. The State of the Pipeline, which summarized the audit, captured my opinions as the leading expert in the office on what I saw; others gave their feedback, so that finally ‚Äî finally ‚Äî¬†we could start figuring out what we should do next. Second system effect In the wonderful Mythical Man Month , a book I cannot recommend enough, Fred Brooks describes one of the truest aspects of technology that I‚Äôve ever encountered: An architect‚Äôs first work is apt to be spare and clean. He knows he doesn‚Äôt know what he‚Äôs doing, so he does it carefully and with great restraint. As he designs the first work, frill after frill and embellishment after embellishment occur to him. These get stored away to be used ‚Äúnext time.‚Äù Sooner or later the first system is finished, and the architect, with firm confidence and a demonstrated mastery of that class of systems, is ready to build a second system. This second is the most dangerous system a man ever designs. [3] ‚Äú Here be dragons ,‚Äù as medieval cartographers would say. I find keeping in mind Brooks‚Äôs words when starting a system design process to be important in perhaps, maybe, if you‚Äôre lucky, avoiding dragons. How does the architect avoid the second-system effect? Well, obviously he can‚Äôt skip his second system. But he can be conscious of the peculiar hazards of that system, and exert extra self-discipline to avoid functional ornamentation and to avoid extrapolation of functions that are obviated by changes in assumptions and purposes. [3] Knowing there are dragons, and knowing how dangerous our own work can be, the plan to replace our existing pipeline was done cautiously as if we were starting from scratch with nothing in our audit: We evaluated all possible options for our pipeline, though truthfully Kafka was the right choice back then and was still the right choice now for us and our data. The main boxes it ticked were being able to reconsume messages, having support for multiple languages, and wide community support and adoption. We looked into options for running and maintaining the pipeline, including hosted solutions, since (as anyone who‚Äôs sat a software engineering course in college knows) systems will spend most of their life under maintenance, and having engineers keep this system up and running was a pretty big expense in terms of time and knowledge as well as the least savory part of what the audit showed. We took stock of our producers and consumers of the pipeline, their priority in moving over, and our needs as a company now, several years into using them. This was relatively straightforward as our current setup only has two producers and one consumer, forming the most commonly interacted with part of the whole system for engineers, and thus the areas we knew the most about. We then were able to decide on the large epics, determine their ordering, and start filling in the details of implementation. The audit was particularly helpful here in reflecting on how the first system had grown and being able to point at each component to say, ‚ÄúThis needs upgrading, this needs sunsetting, this needs something slightly different.‚Äù The audit, like data in a business decision, couldn‚Äôt replace a human‚Äôs judgement but could help confirm that we were on the right track. Learning from Fred What made the list of todos? set up new Kafka and Zookeeper clusters set up new Schema Registry cluster pick new consumer to act as an extractor and set it up write new producer libraries for two producers and integrate them into existing systems set up metrics and monitoring create documentation about the new system and runbooks for what we will still maintain tests??? That last item was particularly important to me, as a lot of the data team‚Äôs work is so stateful that it is incredibly difficult to test manually and nearly impossible in the current setup to test automatically. Having tests would let us know when we‚Äôd reached the definition of done for each of the items on the list; they would then double as the tests used going forward to ensure nothing is borked as the system evolves over time. Striving to have a stronger emphasis on automated tests also forced us to think about things slightly differently, breaking out chunks of code that could be easily tested so that what is not so easy is as isolated and small as possible, to minimize risk. This led to us thinking of new edge and corner cases as we set up the tests for the purely functional code, and to capture the concerns of other engineers when they work with the system, so that those concerns could be automatically tested as well. Speaking of the other engineers, part of my work in this phase included interviewing them as users of the producer libraries. As you may remember from my previous post , engineers are in charge of writing emitters which take in an event‚Äôs data and pass it to the producer for sending to the pipeline. Since they are often tourists to this area of code, asking them about their challenges and concerns was a wonderful chance to again inform what was actually important for this second pipeline system and what was only important in our minds. I will certainly not contend that only the architects will have good architectural ideas. Often the fresh concept does come from an implementer or from a user. However, all my own experience convinces me, and I have tried to show, that the conceptual integrity of a system determines its ease of use. Good features and ideas that do not integrate with a system‚Äôs basic concepts are best left out. [4] (If there‚Äôs interest for the specific design of the producer code, especially making it easy to work with Avro and the Schema Registry, let me know and I‚Äôll be happy to write a post about that.) Does the above list cover everything required to get into production and deprecate the old system? No, but it‚Äôs a start at the minimum required to at least get into lower environments and make sure we‚Äôre achieving parity. ‚ÄúAchieving parity?‚Äù you might be saying. ‚ÄúHow do you test that bold claim out?‚Äù I‚Äôm so glad you asked. Let‚Äôs do this My background is in computer science, which often shows when I‚Äôm working with people who come from a purely engineering background. If this new second pipeline was a big experiment in ‚Äúcan we learn from the past without committing the second system effect sin?‚Äù, we needed to be able to prove it. We needed to be able to say to people, the data is still making it through at the same rate as the old system, if not better. Emitters are as easy to set up as in the old system, if not better. The ETL process will still work as in the old system, if not better. Better has to be measurable. Better has to be visible. With high risk, something to prove, and a big system, the path forward that made the most sense was a side by side deployment. Simplicity, patience, compassion. These three are your greatest treasures. Simple in actions and thoughts, you return to the source of being. Patient with both friends and enemies, you accord with the way things are. Compassionate toward yourself, you reconcile all beings in the world. [5] A side by side deployment would allow us the time we needed to build out the new system and fix bugs as they cropped up, tuning it as we went along. It would also allow our resource constraint ‚Äî¬†that is to say, only I would be implementing that big ole todo list ‚Äî¬†to not stop the whole project as we delivered things bit by bit. If every week, we could say, ‚ÄúHere‚Äôs how things are getting better already,‚Äù better became measurable and visible. More importantly, if every week, I could say, ‚ÄúHere‚Äôs how things are getting better already,‚Äù I could keep going without feeling overwhelmed by the task ahead of me. Our producers produce into both pipelines. Our consumers each consume from their pipeline. We can compare the data collected and metrics generated. We‚Äôve got a plan. Harvesting Obviously this post doesn‚Äôt cover the details of getting each todo list item checked off (but, again, I can write one that does if people want to hear about it), and we‚Äôre still working on closing out the last bits of what remains, but already we‚Äôre seeing the new pipeline do great things while also getting to enjoy some of the benefits within the old system. For example, a lot of engineers who write emitters complained of the boilerplate that existed, so I condensed what‚Äôs required to just the bare minimum with the system instead then fluffing out the data before sending it to both pipelines. ( That‚Äôs the refactoring project discussed here .) This then also meant we had a lot of pure functions we could test, and a way to automatically test every emitter could do things like encode its data. export class PersonEvent extends BaseEmitter < PersonInterface > { public topicName = ' person ' ; protected doc = ' Represents a person and an associated user if they have one. ' ; protected fields : AvroSchemaField [] = [ { name : ' id ' , type : ' string ' , doc : ' the id of the person ' }, { name : ' user_id ' , type : ' string ' , isNullable : true , doc : ' the id of the user associated with the person, if one exists ' }, ]; } Sample of what an emitter now looks like. That‚Äôs it, everything from producing to the pipeline to filling in things like when the event happened are done automatically within the producer, which is evoked from the base emitter. [6] The best way to both measure and see the benefits is through our metrics dashboards, which I‚Äôve become obsessed with looking at and comparing to the old pipeline‚Äôs figures, as it lets me see in real time how the system is doing as I deploy new pieces. My favorite dashboards in particular are: an overview dashboard that takes your through the flow of the whole system, showing on the left a link to more details and on the right the single most important metric to tell if that piece of the system is healthy or not. It starts with producer errors, goes through the pipeline, and then ends with our ETL process and data warehouse, so you can compare everything quickly and easily as data flows through the system. Example from the overview dashboard for the pipeline. The left links to the more detailed dashboard; the right shows the top metric and colors to indicate if it‚Äôs in a good or bad state. The emojis are just for fun. [7] a data status dashboard that zooms in on if the producers are encountering errors and what data was invalid, dropped, or had some other failure. Again this links to more details but also allows the data team to see a list of events impacted and by how much, should there be an outage or bad deployment. We‚Äôve set this up for the older pipeline as well and it‚Äôs where you can really see how the new system has nearly no errors while the older one, even on a good day, drops data pretty regularly. the new producer dashboards, which again show metrics as a flow through the system, from connecting to emitting to encoding to producing. Coloring once more plays a big part in helping people know, whether in an incident or just curious, if things are fine, questionable, or on fire. We also, where helpful, show a comparison to how we were doing last week, as some days might be more spiky than others but week over week tends to follow the same trend. Example from a producer dashboard. Each section of the dashboard is laid out with the overview and a big number showing times or success rate, then the breakdown of successes in blue and failures (if any) in red. [7] the pipeline dashboard, which is so much tidier now that we no longer host our own Kafka and Zookeeper clusters but instead use a hosted solution. I‚Äôve invested time as the office‚Äôs expert on this system into carefully marking the ranges of good and bad so that if something happens, I‚Äôm not required in the heat of the moment to figure out the state of things. Example from the pipeline dashboard. This one relies the most on color so that no one has to remember how many controllers we should have or if an offline partition is bad or not. [7] What does all this show us? That we‚Äôve begun harvesting what we‚Äôve sewn before the project‚Äôs conclusion, whether in changes we made that are already live or in the side by side comparison. The goal is, once the full pipeline is deployed, to switch between versions for where we‚Äôre pulling the data from and see if we can find differences that require further changes in the new pipeline. If so, we can rollback and make the changes without interrupting the normal flow; if not, then we‚Äôve achieved parity, which is all we‚Äôre currently after. I was going to put in a picture of one of my Minecraft farms for the harvesting metaphor, but they‚Äôre not terribly aesthetic so here‚Äôs a random picture of a sunrise instead. It captures the calm feeling I have when a new part of the pipeline works as expected and its metrics are all smooth. [8] Final thoughts from a data engineer This project has been way too massive and as small as possible and a headache inducer and a load of fun and a great learning opportunity, all rolled into one. A system like this requires a certain amount of investment, because a stable data pipeline is the foundation that allows smaller producers, consumers, or streamers to be worked on, spun up, or sunsetted independently with low effort. One of the main concerns we wanted this system to be able to address was not getting into the same state that brought this system into existence: we shouldn‚Äôt be afraid to upgrade our version nor stuck on an old one, nor should we have code we must maintain but don‚Äôt feel we fully understand. The new pipeline should be able to grow and change with us, rather than having to be worked around. It should be easy for tourists to visit and understand, but also for the rest of the data team, including scientists and analysts, to conceptualize and make use of. A leader is best When people barely know he exists Of a good leader, who talks little, When his work is done, his aim fulfilled, They will say, ‚ÄúWe did this ourselves.‚Äù [5] There‚Äôs still a long road ahead, from getting the new pipeline hardened and into production, to deprecating the old pipeline, to making use of the power we now have at our hands. But the main benefit has been that everyone in the office understands what‚Äôs being worked on, from the data team to the other engineers, to QA and designers and the office manager, to the CTO and CFO. Everyone is able to come in, look around, and say, ‚ÄúI understand and can ask questions if I want to, because I know enough to feel comfortable doing so.‚Äù Along the way, I‚Äôve been capturing as much as I can for posterity, whether its in reference documentation stored as close to the work as possible, to articles and books that helped informed my thinking for whoever might have to retrace my steps. Below I‚Äôve linked some of my favorites for you, if you‚Äôve made it through all of this and want moar . In the end, my main recommendations are: you will forget everything you did, so keep the code simple, your naming verbose, and your tests obvious you will forget what everything means, especially under pressure, so add annotations to your metrics and dashboards test driven development, functional programming concepts, and strong typing make everything infinitely easier when working together you will regret most of the choices you made eventually, so keep in mind you will delete this code to replace it with something else, and that‚Äôs quite alright friends make things easier, so have a pipeline friend or two when implementing your system who are also on the project we‚Äôve learned nothing since OS/360 so read Brooks‚Äôs book, and then read it again if you‚Äôre working with Kafka and have your choice, the JVM makes things so easy, especially Scala, so learn and use it if you‚Äôre working with Kafka, have someone else host your cluster ‚Äî¬†it‚Äôs worth the money People tend to think systems like Kafka are too complicated to be useful, but really there‚Äôs just a need to learn its basics before it becomes easy (and not hosting your own cluster, I cannot recommend that enough). If you then keep your system built around it tidy, observed, and up to date, it‚Äôs smooth sailing thanks to the potential Kafka unlocks. If not, to quote my favorite line in The Bard‚Äôs canon, The fault, dear Brutus, is not in our stars, But in ourselves [9] Watch out for those dragons. Recommended readings Write code that is easy to delete, not easy to extend Write code that‚Äôs easy to delete, and easy to debug too Mythical Man Month by Frederick P. Brooks Jr. The Log: What every software engineer should know about real-time data‚Äôs unifying abstraction Apache Kafka‚Äôs design Kafka Streams in Action Streaming 101: The world beyond batch and 102 Programming in Scala by Martin Odersky, Lex Spoon, and Bill Venners Footnotes Architecture diagram by fellow GameChanger Joshua, first appearing in my previous tech post Siobhan Sabino, yes I am citing myself Frederick P. Brooks Jr., Mythical Man Month , Chapter 5: The Second-System Effect Frederick P. Brooks Jr., Mythical Man Month , Chapter 4: Aristocracy, Democracy, and System Design Lao Tzu, Tao Te Ching Code sample from Typescript service Dashboards in Datadog Screenshot from Minecraft , a game I should not have bought myself William Shakespeare, Julius Caesar Share on Twitter Share on Facebook", "date": "2019-10-25"},
{"website": "GameChanger", "title": "Let me automate that for you", "author": ["Siobhan Sabino"], "link": "http://tech.gc.com/let-me-automate-that-for-you/", "abstract": "As GameChanger‚Äôs data engineer, I oversee the data pipeline and data warehouse. Sounds simple, right? And at a high level, it is! Fig 1.1: high level architecture diagram. Some complexity removed due to it being kinda boring for this post. Producers produce into the pipeline, and our main consumer is the ETL job which moves data to our warehouse, enabling anybody to come get answers to their questions and see what‚Äôs happening across all our systems. Boom: easy. Well, not quite. Who owns what Since data can come from any number of backend systems and teams, engineers are responsible for writing the setup that shepherds their data through the system: a producer that lives near their data, the pipe it travels through in the pipeline, and the warehouse table. This often means new data that I‚Äôm unfamiliar with arrives in our warehouse without me even knowing it‚Äôs been set up, which is actually kind of neat: the system should be so easy to work with that you don‚Äôt need the data engineer. After a recent refactoring project, producers were made as simple as possible with removed boilerplate and plenty of tests to automatically catch the most common bugs engineers encounter. Typically, engineers have no problems with making their producers. Fig 2.1: engineers before and after producer refactor project. Studies have shown that engineers prefer to be happy. The pipe their data travels through is set up by filling in a form and pressing a button. Again, engineers typically have no problems with this. It‚Äôs the warehouse table that becomes a pain point. Follow the readme There are two times non data engineers need to interact with warehouse tables: they‚Äôve created a new producer which needs a table for their data to land in. they‚Äôve updated an existing producer which needs its table updated as well. The second point is trickier and easier to get wrong, but the first point proved just as difficult for many engineers and far more common, especially if the engineers in question had never made a producer before. The warehouse is a different database from the ones they usually interact with, it has slightly different (SQL) syntax than they might be used to, and the code must be written by hand instead of using a library to generate it. It‚Äôs also easy to miss that you‚Äôve forgotten it when you don‚Äôt typically interact with the data warehouse. So how bad is ‚Äúbad‚Äù? Why does it matter that the table exists? Well, if we look at how data crosses the pipeline-to-warehouse boundary, we find a gotcha. Fig 4.1: a gotcha in the ETL system. Unlike data, a gotcha cannot be turned into information. Our warehouse needs a place for data to live, a home to call its own. If there‚Äôs no table for it, our loader isn‚Äôt able to move the data over: it builds up in our holding tank, never getting to the transformer. Previously this caused our loader to become unstable, as it pushed data from the holding tank into the warehouse; our new version pulls data only if there‚Äôs a table, but the table is still key. Fig 4.2: unmoved data. Many studies posit that data, unlike engineers, have no preference towards being happy, but I would disagree. So if engineers often forget or struggle to create the table, the data won‚Äôt move without a table, and I don‚Äôt have the in depth knowledge of what this data is to make every table myself (or even that there‚Äôs new data coming through), what can we do? Thought experiment: an ideal world Let‚Äôs imagine what an ideal world would be for solving this problem: in this world, the table would be created automatically in the warehouse. Well, what keeps us from that? Firstly, we‚Äôd need to know the shape of the data to know the shape of the table. Figure 5.1: the shape of things. Between systems, the same thing should have vaguely the same shape. Actually this is something we already have: all data as it moves through the data pipeline must declare a schema, which is registered with a service that will happily answer the question, ‚Äúwhat is the shape of x ?‚Äù Secondly, we‚Äôd need to be able to convert that schema to a table. Alright, so say we had a way to convert a schema to a table, what else would we need? There‚Äôd be a few gotchas around how to convert the schema fully: an integer is an integer and a boolean a boolean but is a string a VARCHAR(48) or a CHAR(36) or even a DATETIME in hiding? Plus our warehouse is distributed, so what do we distribute it on? Let‚Äôs imagine we had a system that could make most of the table but not all of it: if we asked engineers to then finish the flagged portions of the table, would we have something pretty close to an ideal world? One way to find out. A touch of magic To bring this project from high level to specifics, here are the pieces I needed to make work together: Our loader, which is written in Python. Github, which is where table creation files are reviewed before being automatically run when they hit master . Slack, to let engineers know that there‚Äôs a table in need of a review. Schema Registry , which stores our schemas. Github, Slack, and the Schema Registry all have RESTful APIs which we could easily hit from our Python without much fuss, which meant not only was this doable but it shouldn‚Äôt be too crazy for other engineers to read and understand. data_types = get_data_to_load () tables = get_existing_tables_of ( data_types ) missing_tables = data_types - tables for missing_table in missing_tables : schema = get_schema_for ( missing_table ) create_statement = convert_to_sql ( schema ) branch = create_branch_for ( missing_table , create_statement ) pr_link = create_pr_for ( missing_table , branch ) post_to_slack ( missing_table , pr_link ) Figure 6.1: pseudocode of bringing our ideal world‚Äôs solution to GameChanger. The hardest part of implementing the above ended up being create_branch_for(missing_table, create_statement) as git commands are simple to do on a command line but required introducing a new Python library to achieve in the code. Instead of trying to make that work, I cheated and wrote a shell script ‚Äî¬†ironically, the first time perhaps that a shell script was the simplest implementation for everyone to understand. echo \"Checkout\" git checkout -b ${ NEW_BRANCH } echo \"Add\" git add ${ FILE_PATH } echo \"Commit\" git commit -m \"Create migration for missing ${ TABLE_NAME } table in warehouse.\" echo \"Push\" git push origin ${ NEW_BRANCH } Figure 6.2: pseudocode of the shell script to assist our Python, which only had to add the waiting file and substitute in a few names. As I was implementing and testing the code, other sticky spots arose (the Github API wasn‚Äôt playing nice for some reason but the Python library wrapping it was) as well as opportunities: I realized while writing convert_to_sql(schema) that I could flag the specific lines that needed an engineer‚Äôs attention with a simple -- Please verify this line! when there were strings or where the distribution key was defined. Finally I found a way to add, not just a touch of magic to the system, but also a touch of fun with a little bit of personality in the pull request; this would go a long way in taking the PR from ‚Äúa thing the system demands of you‚Äù to ‚Äúa fellow engineer, albeit not human, asking for your help.‚Äù Figure 6.3: personality in automation was achieved using politeness, random emoji selection, and doing work for other people so they don‚Äôt have to. It also never forgets to add the data team, nor to link to where it found the table missing. What have we learned? So far the only engineer who‚Äôs used the system to create tables has been me, but as I write this three new producers are being reviewed that the system will create tables for. The documentation for creating producers has been massively shortened, with the entire section on creating tables replaced with a reminder that while the system will start the pull request, the engineer needs to finish it. Figure 7.1: how I often feel waiting to see if a thing works in production. As small as it seems, there‚Äôs something to be said for removing that mental burden of switching contexts, writing a SQL CREATE TABLE , thinking through the different options‚Ä¶¬†and instead being asked very specifically, ‚ÄúWhat is x ‚Äôs data type? What does it represent?‚Äù For me this has been especially gratifying as, since the day I started, I‚Äôve known this was a pain point for others that I wanted to address. Being a data engineer at GameChanger means overseeing the data pipeline and data warehouse, sure, but it also means knowing a little bit about every system, listening to what my fellow engineers are saying and struggling with, and coming up with ideas that might seem crazy but are doable with a bit of reading the docs, pushing through, and just the right amount of chutzpah. Of the values we hold as a company , the one I associate with most is, ‚ÄúWe do unglamorous work in service of the team.‚Äù Automating away table creation might have been unglamorous, but it was also satisfying. Wrap up I‚Äôd like to thank my fellow GameChanger Josh for helping to illustrate this blog post and give it, like my automated pull requests, that little bit of personality to make it shine. I‚Äôd also like to thank Eduardo who does the truly unglamorous work of reviewing my pull requests, which are rarely about the same thing two times in a row. We‚Äôre actually looking to hire another data engineer along with a host of other roles in case you‚Äôd like to join us. There‚Äôs so many great things we do at GameChanger but I can guarantee one thing you won‚Äôt ever do, and that‚Äôs have to create warehouse tables for your producers. Share on Twitter Share on Facebook", "date": "2019-09-20"},
{"website": "GameChanger", "title": "Fishing For Correlations", "author": ["Alex Young"], "link": "http://tech.gc.com/application-performance-monitoring/", "abstract": "Have you ever participated in a firefight where the root cause seems unclear, then suddenly the symptoms self-resolve? Sometimes inadvertent action may cause it, other times it appears miraculous. If you find yourself in this situation, or excusing recovery with ‚Äúit was a one-off blip,‚Äù your system likely lacks sufficient observability. With development and operations increasingly converging, application instrumentation continues to concern many teams building new platforms and maintaining legacy ones. The general rule of thumb when introducing observability into a system is to instrument everything. However, this philosophy creates a user experience challenge. How do we deliver the salient system correlations to the people who can best leverage them? The platform team at GameChanger recently had the opportunity to tackle this question when we encountered performance degradation while scaling a new back-end system. We are not yet finished, but we have progressed meaningfully. Diving into the black box Modern Node.js, using async/await with Koa , powers GameChanger‚Äôs nascent Team Manager product . PostgreSQL serves as the primary datastore and PgBouncer manages connection pooling to the database. We already use Datadog to monitor many of our legacy systems. Starting with Datadog for a new system made sense for a number of reasons: Vendor familiarity: engineers already understand the operational model and where to look for diagnostics Breadth of functionality: support for a wide variety of metrics, application performance monitoring (APM) and centralized logging, and mature monitor types, ensure our telemetry can scale alongside the codebase Low adoption cost: integrating new services with our existing Datadog configuration is trivial After building an MVP, our baseline instrumentation included typical system level metrics, such as load average, CPU utilization, and used memory, as well as custom metrics for all requests like duration and status code. Such metrics enable us to monitor attributes of user impact like error rate and response time. We also had the ability to selectively time function calls, but no meaningful way to create correlations such as flame graphs. Lastly, we use Loggly for log aggregation, but constructing a timeline from logs and associating them with behavioral trends we might encounter in Datadog remained challenging. These mechanisms provide robust insight horizontally; we can see behavioral patterns in a cluster from the data we categorize. However, once we identifed problematic requests, we had little transparency vertically, such as the time database or third party network calls consumed during the request. A sample of graphs from one of our dashboards. We categorize similar classes of metrics in order to reduce cognitive overhead during diagnostics. As our active user count increases, we continue to add new functionality to the application. In the initial design, we had clear intent behind some of the architectural patterns, but only hypotheses about whether these patterns would achieve our goals. Additionally, GameChanger‚Äôs engineering culture encourages team members to work across the stack and contribute to codebases and systems in which they might not be experts. These practices stimulate learning and technical growth. However, when intersecting with a new codebase, testing performance and scalability hypotheses becomes problematic. In order to refactor both incrementally and holistically, while maintaining a high velocity of contributions, we wanted to augment how we gather information to inform decisions. On the people side, we formed a decision-making committee from members of many contributing teams. This committee conducts research and effects architectural change in the system. On the technical side, we sought to empower engineers to more easily identify performance bottlenecks using instrumentation. In order to do this, we outlined a few requirements: Distributed correlations: provide a comprehensive view of how all of our components, e.g. Memcached, Redis, AWS Simple Queue Service (SQS), in addition to PgBouncer and Postgres, interact within the lifecycle of a request Granular tracing: expose as many details around timing as possible, from individual function calls to low level framework operations such as the Node.js event loop and garbage collection Extensibility: extending this monitoring with new additions to the codebase, whether it be new endpoints, new providers, etc, should demand minimal effort Stretch: zero-touch installation: avoid requiring changes to application code in order to surface useful metrics Surfacing with clarity Two significant bugs in our system surfaced recently. In hindsight, tools fulfilling such requirements would have revealed the root causes more quickly. The first bug was a mistake in some ORM code, where some requests executed redundant SQL statements orders of magnitude more than they should have. The database handled the load well, yet the I/O-intensive activity caused thread starvation in the Node.js worker pool and we witnessed widespread request timeouts. In this case, we had attempted to correlate Node.js requests to PgBouncer waiting client connections to Postgres transaction throughput. However, without vertical visibility into each request, each component appeared relatively stable in isolation. The second bug was a design flaw in our workers that process SQS queue items. In a loop, workers either perform queue tasks or short poll SQS to determine if items need processing. The polling frequency for empty queues triggered AWS rate limiting. Combined with our exponential backoff response, we witnessed delays in all workers, regardless of queue length. Thus, empty queues cannibalized the throughput of populated queues. In this case, we had no monitoring on our SQS network calls and debugging was an archaeology exercise. To try making some of these bugs easier to diagnose, we turned to APM solutions as a panacea fulfilling many of our requirements. While typically installed on a single application, APMs provide granular tracing and can infer interactions with external services by treating them as a black box. We evaluated both Datadog‚Äôs and New Relic‚Äôs solutions. Each supports Node.js out of the box and is relatively straightforward to install with minimal application code changes. Much to our delight, New Relic provides detailed instrumentation of Node.js internals, exposing metrics on the event loop, memory usage, and garbage collection. Additionally, both services trace request operations, correlating insights from database to external service activity. Yet both solutions had varying support for Koa at the time of adoption. Datadog only recently introduced support for Koa, and New Relic required some manual pattern matching of routes. Aside from those features, we mostly found both services functionally equivalent. We chose Datadog for its superior user interface, familiarity with the platform, and ease of integration (at the cost of vendor lock-in). Datadog provides a more refined web application for browsing traces, and their query DSL is simple and intuitive. Finally, the ability to export trace searches to our existing alerts and dashboards made extending our existing, monitoring coverage trivial. Datadog traces supplement flame graphs with host metrics and provide optional, logging integration. Revisiting the bugs we previously discussed, we now have observability into: SQL statements executing during a request, allowing us to identify excessive database activity Duration of external asynchronous operations, enabling us to monitor outlier slowness While we still lack sufficient observability into Node.js internals with Datadog, we enabled granular, distributed tracing with relatively low overhead . The automatic route matching and support for many common frameworks and libraries in their APM ensures low effort to maintain quality instrumentation as we extend the application with more endpoints and service integrations. The vertical observability of individual requests and API operations supplements the horizontal observability from existing dashboards and graphs. In practice, we still rely heavily on dashboards and graphs to illuminate behavioral trends of dependent components in a system and explain what is happening. APM now brings us closer to potentially answering why such things happen. On the horizon While implementing APM reaped many rewards for us, we still have outstanding concerns. For example, we lack the ability to correlate API HTTP requests to PgBouncer TCP connections to Postgres transactions, which might make it difficult to tune connection pooling strategies. Our logs remain segregated from our monitoring and are impossible to analyze across attributes with high cardinality. Connecting monitoring to the teams best positioned to act upon them continues to challenge us. We briefly considered alternative approaches to instrumentation (not necessarily mutually exclusive). Envoy Proxy appeared on our radar as a solution for monitoring at the network level and also fulfilled our zero-touch installation requirement. Honeycomb.io also intrigued us for its approach to observability primitives and ability to generate many insights from raw data structures. Ultimately, APM provided the most value for effort given the prior state our system, but it would be interesting to explore such options in the future. Share on Twitter Share on Facebook", "date": "2019-02-12"},
{"website": "GameChanger", "title": "90 Days of Kotlin", "author": ["Adetunji Dahunsi"], "link": "http://tech.gc.com/90-days-of-kotlin/", "abstract": "Riding the wave of change Old habits die hard. We‚Äôre creatures of habit, and provided there‚Äôs no stimuli that causes us to change and evolve, we‚Äôll all very likely stay static. After all the more you repeat something, the better you get at it, and there‚Äôs little to be gained from change for change‚Äôs sake save for a new perspective, which can be rather varied in its returns. I‚Äôve been an Android Developer for a little over 5 years now, and I love Java. Quite a bit actually; its APIs for common data structures, its explicitness and the way it makes it easy to read someone else‚Äôs code, its OOO approach and how it lends to easy encapsulation and delegation to class instances, its recent adoption of a pseudo functional paradigm with functional interfaces and single abstract methods‚Ä¶ the list goes on. Not only does it offer all this, but it does so while being backwards compatible with previous versions of the language. It‚Äôs therefore not surprising Java and the JVM form the thriving ecosystem they are today. I‚Äôve spent a considerable amount of time learning about the language and its design decisions and they all seem extremely justifiable to me. However, the programming world moves fast, and with Google embracing Kotlin more and more each year, I needed not only to switch, but to be in an environment that I could use Kotlin in, day in and out. Here at Gamechanger, I‚Äôve found that, and have been learning and using Kotlin for the past 90 days, and there‚Äôs quite a lot to like. Fundamentally Kotlin doesn‚Äôt try to re-invent the wheel, nor is it some great departure from Java that would cause an epic schism or anything of the sort in the JVM ecosystem. If anything, it readily embraces its heritage with 100% interoperability with Java. What Kotlin seeks to do instead, is get out of your way, and let you do as much as you already could with a lot less effort and code. Compiler niceties A lot of what Kotlin does I‚Äôve found, is to take things the compiler could do, and simply make the compiler do them. From inferring generic types, to smart casting and much more. Take for example casting an object: In Java: Object item = container . getItem (); if ( item instanceof Car ) (( Car ) item ). drive (); if ( item instanceof Plane ) (( Plane ) item ). fly (); if ( item instanceof Rocket ) (( Rocket ) item ). launch (); and the Kotlin equivalent: when ( val item = container . item ) { is Car -> item . drive () is Plane -> item . fly () is Rocket -> item . launch () } In Java, the cast is still necessary despite having just performed an instance check right before. In Kotlin, the compiler is smart enough to skip the ceremony and let you start interacting with the Item as what you‚Äôve already ascertained it is, a Car . The standard library In Java, creating a list, or map of items is a bit of a ritual. The tersest form for creating a mutable list of names is: List < String > names = new ArrayList <>( Arrays . asList ( \"Jack\" , \"John\" , \"Mary\" )); And even that is shrouded in nuance, the wrapping ArrayList is only necessary to make the List mutable, as the Arrays.asList method returns an unmodifiable List . Trying to mutate it would cause an exception and you wouldn‚Äôt know unless you read the docs beforehand, or at runtime. Kotlin instead uses the more readable val names = mutableListOf ( \"Jack\" , \"John\" , \"Mary\" ) Which is less verbose, and yet much clearer. Kotlin actually separates mutable collection types from immutable ones, there are no methods available to mutate  a List , they only exist on a MutableList . Whereas prior you couldn‚Äôt communicate that a List was unmodifiable without throwing an Exception , you can now easily convey it in the language itself. Infix Functions Another amazing example of how the language gets out of your way to let you get things done are infix functions. Similar to the above, let‚Äôs create a map of numbers to names in Java. Map < Integer , String > map = new HashMap <>(); map . put ( 1 , \"Jack\" ); map . put ( 2 , \"John\" ); map . put ( 3 , \"Mary\" ); And in Kotlin: val map = mutableMapOf ( 1 to \"Jack\" , 2 to \"John\" , 3 to \"Mary\" ) Again Kotlin is more succint, and more than that, the mapping operator to is not a language keyword, it‚Äôs a regular function that creates a Pair of two elements, i.e: val pair = 1 to \"Jack\" is equivalent to val pair = 1 . to ( \"Jack\" ) is equivalent to val pair = Pair ( 1 , \"Jack\" ) By declaring a function as infix, you can write more readable code with less ceremony, provided you meet the requirements of the infix notation. You can even write your own infix functions if you so desired, and can call them with the same syntax. Extension Functions and Receivers Finally, my favorite Kotlin ability, are extension methods and receivers. Extension methods are statically resolved syntactic sugar that let you define methods on a class, as though you owned it without having to inherit from it. This is very common in Android where framework classes are build for the lowest common denominator and have fairly open APIs for you to make them as robust as you wish. Take for example, trying to get the current displayed View in a ViewPager . In Java, a utility method would need to be created like this: public static View getCurrentView ( ViewPager viewPager ) { ... } While in Kotlin: val ViewPager . currentView : View ? get () { .. . } Calling the method in Java would take the form: View currentView = getCurrentView ( viewPager ) and Kotlin val currentView = viewPager . currentView Of the two, the Kotlin version reads better, as for all intents and purposes, the current view in a ViewPager really is a property of the ViewPager itself despite it not being included with the ViewPager dependency. This is but a tip of the iceberg however. We‚Äôve been able to add a property to a class we have no control over, but what if you could execute arbitrary code in the context of any class at anytime? Think of functional interfaces in Java like a Consumer<T> but instead of the consumer receiving the instance of the object T , it also receives the execution context of T , as though you were writing code inside the body of class T . To illustrate, let‚Äôs bring back the Container class from earlier. Assume the class has since been refactored to contain generic bounds for the item contained within, so a Container of a Car now has the signature Container<Car> . This container is passed around quite often, so in Java I could define a method called onItem of the signature: void onItem ( Consumer < T > consumer ) { consumer . apply ( item ) } And if I were to pass this to a mechanic for example, the mechanic could say, container . onItem ( car -> { int fuelLevel = car . fuelLevel (); if ( fuelLevel < 3 ) car . stopEngine (); }); Which is great and rather succinct. However in Kotlin, rather than having to pass the car, you could pass the context of the car like this. fun onItem ( receiver : T .() -> Unit ) { receiver . invoke ( item ) } Then use it like container . onItem { if ( fuelLevel < 3 ) stopEngine (); }); Within the scope of the lambda expression, I‚Äôm writing code as though it were inside the body of the Car class. All the instance variables and methods in the Car class are available in the lambda context, as well as those available in the execution context of the method block the receiver was called in. This avoids having to create temporary variables for state within the car class that we may need, bloating our code base as it‚Äôs readily accessible within the Car context. The Kotlin Standard library has various permutations of different kinds of methods that use receivers. They take some getting used to, but really are quite lovely, especially when working with Nullable types. Wrap up Kotlin has a lot more to offer than just these perks, but in my 90 days, these are the ones I‚Äôve enjoyed the most as they dealt with the parts of Java I wasn‚Äôt fond of. Do I still love Java? Yes, but more in a legacy way now. Without it, there‚Äôd be no Kotlin, but Kotlin improves on in it so many ways that  it really is night and day between the two. It‚Äôd also be non trivial to bring those same Kotlin features to Java in a backwards compatible way. It only took 90 days, but I wholly prefer Kotlin to Java. Couldn‚Äôt be a second earlier either, Google IO 2019 was a couple months ago‚Ä¶ unsurprisingly, Google is going Kotlin first with Android now, I‚Äôm just glad I caught the train. Share on Twitter Share on Facebook", "date": "2019-07-10"},
{"website": "GameChanger", "title": "Bias Disruption in the Workplace (Part II)", "author": ["Ami Kumar"], "link": "http://tech.gc.com/bias-interruption-workshop-2/", "abstract": "In the first part to this two part article, I talked about the motivation and preparation behind creating and running this activity. Here, I will talk about the setup of the activity itself and the aftermath and feedback we got from it. The Activity We allotted two hours for the activity and broke it down into the following segments: Intro presentation (20 minutes) Round 1 (35 minutes) Round 2 (35 minutes) Conclusion and optional follow up conversations (30 minutes) Each group got one scenario to discuss per round. We were fortunate to have enough survey submissions that no scenario was repeated throughout the activity, and each scenario grouped at least two survey responses. The rounds were broken up as follows: Solo brainstorm with post-it notes (3 minutes) Small group discussion (12 minutes) Presentation to full group (20 minutes total) As with the reading materials and the introductory email sent out before the workshop, the intro presentation was intended to get the group on the same page and give everyone a common foundation on which to have these discussions. I included brief definitions of relevant terminology, most of which was introduced in the pre-reading materials sent out before the activity. The terms included psychological safety, technical privilege, microinequity, and subconscious bias. It would be easiest to have these conversations if everyone was equipped with the vocabulary commonly used around this topic. We also took some time explaining the high level goals of the activity, the code of conduct, and the actual format of the activity. We then briefly showed the group the prompts for the solo generation and small group discussion, though we had these visible during the pertinent parts of the entire activity as a guideline. The prompts were designed to help people either better recognize bias in the future, help guide each individual‚Äôs thought process on the best way to interrupt, or provide a basis for discussions with peers on the best way to interrupt different kinds of biases. There were questions asking how the participant would reassess the situation if one of the people in the situation was a manager, if the victim was an introvert, if they didn‚Äôt know the victim well, and other nuances that could affect what the appropriate response would be. I included questions asking people to identify the different types of biases that were at play in the situations. Participants were encouraged to write down different interruption strategies based on each type of scenario they envisioned. We decided to split the engineering team into groups of three, trying as best we could to make the groups as diverse as possible and include people from different teams. We started with solo generation, which was a strategy to engage everyone equally so that everyone could come into the larger group discussions with ideas to contribute. This eliminated the anxiety of thinking on the spot and stopped any one person from dominating the small group discussions. We used post-it notes during the solo generation. Each discussion point, interruption strategy, and question went on its own post-it note so that they could be easily grouped in similar themes in the discussion that followed. During the solo brainstorm, participants were given prompts but asked to focus on the question: What, if anything, would you say or do? Participants could then easily compare interruption strategies and raise questions or concerns during the small group discussions. The group could easily move around and categorize these individual thoughts during the discussion, in preparation for the larger group presentations. We presented a separate set of prompts during this session, most of which encouraged people to poke holes in the interruption strategies and consider different ways a situation could occur. After the solo generation and small group discussion, we came back to the full group and each team had a couple minutes to present the high level takeaways during their discussions. Groups were encouraged to share their interruption strategies, different ways they envisioned the situation could play out, and the benefits and downfalls of each of their proposed solutions. However, in most large group discussions, there tends to be a few voices that dominate all the discussion, which went against one of my fundamental goals for this activity. Therefore, I limited questions during this segment to just a couple per presentation, and encouraged the group to write down any exploratory questions or comments they had and revisit them in the last thirty minutes of the activity, which was reserved for more free form follow up discussions. This way, we could achieve equal participation during the workshop and limit the unfair advantage large group discussions give to more extroverted individuals. The combination of individual brainstorming, small group discussions, and full team presentations allowed us to have productive conversations with every participant fully engaged, while still being able to share all the discussions with the large group. The brainstorming and discussion prompts for the solo generation and small discussions helped keep the discussions on track and allowed participants to practice how to have productive conversations, which they can hopefully apply in the future as well. The Outcome I was incredibly pleased with the analytical and thoughtful conversations and productive solutions that came out of the workshop. It was clear people thought about the nuances of each situation, and saw multiple ways the situation could play out. Groups came up with many different interruption strategies and advised on the best ways to determine which one to use based on the exact situation that unfolded. They brainstormed interesting questions, and often advised to check in with the victim of the bias, before or after acting, to determine how they could improve upon their interruption strategies. People brainstormed realistic solutions as well, which is crucial if we want to actually implement these solutions. One of our example scenarios was: You are in a meeting and notice a co-worker repeatedly cutting off another colleague and attempting to finish her sentences . The group considered whether the best strategy was to interrupt in the moment, or wait until after the meeting and approach the victim. However, they decided interrupting during the meeting was preferable, regardless of how well they knew the victim or had previously had a conversation with her about this behavior. This is because the negative impact this behavior could have on the victim and the meeting dynamic, especially if they noticed this behavior early in the meeting. The advised bias disruption strategy was to re-interrupt the interrupter and somehow draw attention back to the victim. A simple strategy would be to circle back to the colleague who was interrupted and ask, ‚ÄúIs that what you meant to say?‚Äù They also advised possibly following up with the co-worker doing the interrupting after meeting, especially if this was a repeated pattern. However, they did brainstorm how the victim‚Äôs personality and power dynamics with the interrupter could affect her prefered response strategy, and therefore advised to check in with her before doing so. Finally, they mentioned showing solidarity with a victim in this type of situation was crucial even if there was no opportunity for bias disruption during the meeting. We took notes during the full team presentations and had groups send in any notes they had themselves for these presentations. We compiled these into a document to send out to the company. Even though it is impossible to address every way biases can manifest in the workplace, we were able to address a lot of the common microinequities people at GC have faced in the past. We also sent out a feedback survey immediately after the fact and overall got an extremely positive response. People found it useful, which encouraged us to prepare another iteration of the workshop with the rest of the company who did not participate in the first round. I also heard many success stories in the weeks after the workshop about coworkers following up with one another about whether something they had seen was bias, and whether they interrupted it correctly. I am grateful to the management team for allowing me the opportunity to carry out my vision, to my coworkers who continued the initiative after the first iteration, and most of all to everyone who participated in the workshop for putting their thought and energy into the workshop. My goals were to encourage people to have empathetic and psychologically safe conversations about difficult topics and to speak up and intervene when they see bias. We accomplished both during the session. Share on Twitter Share on Facebook", "date": "2018-07-17"},
{"website": "GameChanger", "title": "Bias Disruption in the Workplace (Part I)", "author": ["Ami Kumar"], "link": "http://tech.gc.com/bias-interruption-workshop-1/", "abstract": "You may have been in a meeting where you see one person constantly get steamrolled, notice one person repeatedly interrupting others, or observe someone feeling uncomfortable to speak up. Although we have, in many ways, grown accustomed to these behaviors and the biases that cause them, they can unfairly favor certain individuals and negatively impact others. Imagine the more productive and fulfilling work life you could have if everyone on your team was fully engaged and felt heard and respected. Your team community would feel stronger and consequently more productive. These types of biases can be common in a professional atmosphere. A subconscious bias is a mental model of the world and by definition is not something people are aware they have. Mental models themselves are not always harmful. In fact, mental models are constructs that usually help people prepare for and process the world around them. However, sometimes people develop mental constructs about other groups of people that can negatively impact their decisions and subconsciously affect how they treat others. The attitude or stereotype behind the action is the subconscious bias; the action it leads to can be called a microinequity. This subconscious bias is different from overt bias, which is bias people know they have and intentionally act upon. However, subconscious bias can be just as harmful. A female coworker once experienced such bias during a breakfast at a tech conference. She was the only woman at the table, and every time someone new came to the table, they shook hands with everyone at the table but her. Even if it was something they did unintentionally, it still had an impact on her morale and left her excluded in the introductions. If these biases go unchecked, they can create uncomfortable and even hostile work environments. Teams where people don‚Äôt feel comfortable or accepted can be unhappy and are actually less productive. Workplace biases can lead to demoralized and disgruntled employees, create friction on teams, and reduce productivity in the workplace, according to a Career Trend article . A recent Harvard study proved that workplace bias has harmful effects on minorities. The study collected data from a French grocery chain and found that the performance of minorities dropped when they were working with biased managers. Workers at the chain worked with different managers daily and the study discovered significant evidence to prove drops in performance correlated with managers who displayed biased tendencies. The best way to address the effects of subconscious bias in the workplace is to accept that everyone has developed subconscious biases in some form and have honest discussions about how to recognize and address them. However, it is often more difficult to recognize or stand up against these biases as the victim in these situations. A very effective solution to biased action is bystander interruption. My intention was to find an actionable and productive way to address subconscious bias at GameChanger. We already had a culture of sharing articles and talks and having open conversations about these topics at GC. I wanted to take this one step farther and somehow address these issues in a more personal and tangible way. I discovered an outline for an activity on the NCWIT website that discussed having a workshop with your team at work. There were nine scenarios listed in the activity, and the document suggested having discussions about actionable ways to deal with the subconscious biases present in each scenario. It recommended acting out the proposed solutions so that people could practice dealing with these scenarios in real life. My idea started forming after looking at this activity. I liked the idea of having open discussions about real scenarios and coming up with actionable solutions to deal with them. I also liked how this activity stressed bystander interruption, and how it made dealing with bias everyone‚Äôs responsibility, not just the victim of these situations. I decided I wanted to develop my own such workshop to try at GC. My Goals I had two goals in mind for this activity: come up with actionable, realistic solutions for dealing with bias in professional scenarios and, on a larger level, learn how to have productive conversations about bias. Come up with realistic bias interrupters for specific scenarios I wanted the solutions we came up with to be as useful in day-to-day life as possible. I came up with two methods to achieve this: Discuss real life scenarios during the workshop Use discussion prompts to keep the solutions specific and realistic I got the scenarios we discussed from the workshop participants‚Äô past experiences in order to keep the discussions relevant and realistic. This would allow people to empathize with the material and keep them engaged in the workshop. They would also be more likely to encounter these scenarios in the future, and hopefully would have strategies to identify and correct them then. Even if we used real life scenarios, the solutions we generated would not work in practice if they were too vague and unrealistic. I made it clear that we were coming up with very specific solutions to these problems, down to the exact words the participants suggested saying. I provided discussion prompts to help people make their solutions more detailed and to encourage people to consider multiple possible ways a scenario could play out. Prompts included asking how the participants would modify the solution if they were dealing with people they knew vs. people they didn‚Äôt know and how they‚Äôd account for the power dynamics of the people involved. This way, everyone in the workshop would be equipped with multiple usable solutions for a specific scenario. They would get practice analyzing different situations and adapting their solutions based on the circumstances. Practice having discussions about bias Bias is nuanced, and there is never an easy solution on how to address it. We need to learn how to talk through scenarios and envision possible solutions. The discussion prompts also helped with this goal. They guided the conversations and gave people examples of questions they should ask during conversations about biases. Guidelines Another key part of having a productive conversation is making sure all parties in the conversation feel comfortable and heard. I had a strict code of conduct to facilitate a safe environment during the workshop and that would hopefully be applied to future such conversations as well. I had four major guidelines for the workshop: Psychological safety Psychological safety is a term coined by organizational behavioral scientist Amy Edmondson, who defined it as ‚Äúa shared belief held by members of a team that the team is safe for interpersonal risk-taking.‚Äù It means feeling heard, safe, and able to take risks without fearing retribution or ridicule. It boosts productivity and personal satisfaction, and is necessary for generating good ideas and having happy and comfortable team members. It is crucial when having tough conversations in order to come up with the most productive solutions. Everyone participates equally I wanted to limit all forms of bias during the actual discussion, including biases that act against introverts and quiet people in most meetings. Equal engagement is crucial in company culture discussions, and I wanted everyone‚Äôs voice to be heard equally. I constructed the actual brainstorming and discussion during the workshop such that everyone could feel comfortable participating and no one person would speak more than the rest. In practice, this included things like enforcing no interruptions and limiting full team discussions, focusing on solo generation of ideas and small group brainstorms. Don‚Äôt vent, and don‚Äôt get defensive While being able to talk and commiserate about issues can be essential in processing the tough things that happen to us, I did not want that to be the purpose of this workshop. I wanted us to be focused on actionable solutions. On the flip side, I did not want anyone to react defensively to the situations addressed. Not only does that stomp on the psychological safety of those around you, but it ends the conversation by taking the focus away from the victims of these situations and onto the perpetrators of the bias. The only way to have productive conversations about reversing and limiting the effects of these biases is to accept that everyone has them. The workshop focused on what to do if you are a bystander who witnesses bias partially so that participants would not feel inclined to vent or get defensive. It is also easier for bystanders to recognize and address bias than it is for the victim in a situation, so this workshop was designed to target what would hopefully be the most effective way in limiting the biases around us. Preparation Once my goals and guidelines were set, I started putting together content for the workshop itself. I got buy-in from our people operations team and upper management early on to make the workshop mandatory for the whole team. The workshop would be most effective and worthwhile if everyone did participate. If only a couple people in each team were present, it would be difficult for those who did to apply the skills we picked up during the workshop with the rest of their teams. We decided the first iteration would be just the engineering team so that the discussions would be as relevant to the participants as possible, and so we could talk about topics as specific as code reviews. If we felt like it was a success, we‚Äôd do another version of it with the rest of the company. I wanted the people in the workshop to generate the content for discussion. The best way to have an engaging and productive discussion was to talk about scenarios participants have experienced, relate to, or find realistic. I collected anecdotes from the group by sending out an anonymous survey to the workshop participants. The survey consisted of one question: ‚ÄúDiscuss a time you experienced or observed subconscious bias in your professional career. What was the situation?‚Äù Participants were encouraged to submit more than one response. Survey participants were informed these responses would be used in the workshop and were advised to keep their responses general and anonymous, especially if the situation happened while they were at GameChanger. Once we got the survey responses, we wanted to take extra steps to anonymize them and wanted to incorporate as many of the responses in the actual activity as possible. We grouped responses into similar themes, including interviewing bias, problematic language used about a woman or minority, and one person taking or receiving credit for another‚Äôs work or idea. We then created composite situations for each group by combining elements from each of the responses, and reframed them so that the subject is an observing third party. For example, if one of the submissions was that the employee‚Äôs manager called her ‚Äúhigh maintenance‚Äù for demanding a peer to meet a deadline, we would reword it to the following: You overheard a manager telling a female coworker she is ‚Äúhigh maintenance.‚Äù We wanted to make sure the scenarios we used were anonymous and could not be traced to any particular situation if they did occur at GameChanger, but still wanted to keep them relatable. We also wanted the scope of the discussion to reach as many of the submitted responses as possible. I wanted to make sure that the actual workshop format and discussion prompts easily facilitated conversations, and wanted to get feedback on the process in general. I put together a small group of three other people and went through one round of dissecting a test scenario. I wanted to test the times allotted for each part of the discussion and get feedback on the format in general. Finally, I wanted to make sure people were in the right frame of mind for the workshop and did not want to spend too much time proving that subconscious bias was harmful to the workplace or that this discussion was necessary. I also wanted participants to come in with a common vocabulary. I sent out a short introduction explaining some common biases women and minorities face in the workplace along with a couple pre-reading articles explaining technical privilege and bias interruption. This small introduction to the activity hopefully got people in the right frame of mind so that we could focus on the actual brainstorming the day of the discussion. We had the resources we needed and had laid out the necessary groundwork for the actual workshop. If you want to read about the activity itself and the aftermath, check out the second part to this blog post. Share on Twitter Share on Facebook", "date": "2018-07-17"},
{"website": "GameChanger", "title": "UX for Multi-Sport Scorekeeping", "author": ["Eduardo Arenas"], "link": "http://tech.gc.com/blog/2014/8/11/ux-for-multisport-scorekeeping.html", "abstract": "The GameChanger mobile app currently provides scorekeeping for Baseball, Softball and Basketball, with more sports coming in the future. This presents an interesting challenge in terms of user experience. Aspects like pace and complexity vary so much from sport to sport, that the challenges a scorekeeper has to face are completely different. Taking these differences into account is essential when designing the app‚Äôs user interface. After years of interacting and getting feedback from coaches and scorekeepers, GameChanger has attempted to create a great product that works well for all the supported sports. This post briefly describes what GameChanger has done so far and some of the main challenges that need to be faced in the future to improve basketball scorekeeping. Baseball and softball are very complex sports. The rules and the way the game is defined allow for a very large combination of events that can happen in a single play. Furthermore, coaches are interested in keeping very detailed and complex stats. What this means is that if a player hits a single, the app has to be able to record if the player hit a ground ball or a line drive, the pitch type, if there was a defensive error, if the ball was hit to the left, right or center field, and what player caught it. That‚Äôs a lot of things to ask for each batter. Fortunately this complexity is mitigated by the pace of the game. Baseball and softball are not very fast sports, which means that there‚Äôs enough time between plays for scorekeepers to record all this information without falling behind in the game. For this reason, the focus when designing baseball and softball scorekeeping for the app was to make all possible flows clear and accessible to the user, even if that meant having to go through several screens to score each play. Baseball scorekeeping Basketball is almost the complete opposite to baseball and softball in terms of complexity and pace. When a player scores, the only information that needs to be recorded is where the shot was made and by whom. Some coaches might also be interested in recording assists. When compared to baseball, the flow that a scorekeeper has to go through is a lot shorter, and yet users often find it difficult to score a game without falling behind. The reason is that basketball is a very fast sport. Many things can happen in a span of seconds and all of them need to be recorded in order to obtain accurate stats. Taking this into account, the GameChanger app attempts to make scoring a basketball play something very quick. Buttons are bigger and options are limited. Furthermore, the app allows the user to check and correct previous plays. What this means is that users know they can continue to pay attention to the game, and when they have a chance they can go back and assign a shot or a foul that was recorded earlier. This helps reducing the potential stress caused by failing to record some of the plays. Basketball scorekeeping Despite these efforts, basketball scorekeeping is still too hard for some users. Scoring plays needs to be even faster. This can be achieved by attempting to predict how users behave when using the app, and optimizing the user interface for that behavior. There are tools that can be used for that purpose such as CogTool, which allows the design and testing of different storyboards by predicting how skilled users would perform on them while doing different tasks. Every change made to the current interface involves some risk, given that it‚Äôs hard to anticipate if users will understand the new interaction models. That is why using a rapid prototyping methodology becomes really important, as it provides early feedback from users. This involves iterating really quickly over some UI concepts, and testing them in office with actual users. After the tests are conducted and analyzed the team is able to decide what works and what doesn‚Äôt, and develop the next prototype for a new round of user testing. As an engineer, this has been a great opportunity for me to get direct contact with users and understand how they perceive and use the app, which I think is really important for being able to make a successful product that actually provides a great value to people who use it. Understanding how the differences between sports affect what users need from the app, is very important for providing a successful experience. This will become harder and even more important in the future as new sports are introduced, because each one of them will bring new challenges that will need to be addressed. To do so successfully, it is very important to maintain a close relationship with coaches and scorekeepers, as GameChanger has done so far, since this is the foundation for understanding what they really need from the product and what the best user experience should be. Share on Twitter Share on Facebook", "date": "2014-08-11"},
{"website": "GameChanger", "title": "Landing in NYC", "author": ["Eduardo Arenas"], "link": "http://tech.gc.com/blog/2014/5/29/landing-in-nyc.html", "abstract": "I arrived to New York a tuesday at 4:00 am, in a flight that departed five hours earlier from Bogota, Colombia. The reason for my travel: taking part in an eighteen month training program at GameChanger. After a few weeks in the city I have found an apartment to live in, bought furniture for said apartment and started the training program. During this time, I have found that there are some things that work very similar in New York and in Bogota, while there are others that definitely require some learning and adjustment. I want to write about two of them that I‚Äôve found particularly interesting. The first one is public transportation. New York and Bogota are both big cities and as a consequence, efficient transportations is a huge challenge for both of them. I was aware that Bogota‚Äôs public transportation system couldn‚Äôt be described as ‚Äògood‚Äô, but after using New York‚Äôs for a couple weeks I can say that the difference is significant. While looking for an apartment I spent whole days going from one place of the city to another, and it amazed me how fast and easy it was to move around. In Bogota, using public transportation can be a very unpleasant experience. There is no subway so the city relies on buses to transport millions of people every day. This buses tend to be overcrowded and slow, and a five mile trip can often take over one hour. Furthermore, there several companies running the transportation system, which means that a lot of people need to pay more than one fare to get from their homes to their destinations. The second one is choosing your healthcare plan when you start working in a new company. In Colombia, the basic medical coverage of all healthcare plans is set by the government, and every healthcare provider is obligated to offer at least the minimal obligatory health care plan. This means that when a person is going to start a new job, the decision comes down to which network of medical institutions you prefer. That‚Äôs it. Furthermore, if you change jobs you can transfer your existing plan to the new company, as every company is obligated to pay the health provider chosen by the employee. On the other hand, choosing my health plan in New York felt like something that required great amounts of knowledge and understanding of the system, which I obviously din‚Äôt have. Deciding wether opening a HSA or a FSA suited me best, how much deductible I should choose in my plans, wether I needed vision and dental care plans and which of the several available plans was the best for my needs. It was an overwhelming experience that required a few hours of reading, researching and the valuable help of a teammate to finally understand and make a decision about my healthcare plans. These are just two of several things I have had to adjust myself to since arriving to New York. The list could also include things like renting an apartment in Manhattan, understanding taxes and driving. And I‚Äôm quite sure I will keep finding more things to add to the list in the future, which is great. It has been an enriching and eye opening experience that I have fully enjoyed so far, and hopefully I‚Äôll continue to learn how things work in the USA. Share on Twitter Share on Facebook", "date": "2014-05-29"},
{"website": "GameChanger", "title": "Speeding Up Provisioning", "author": ["Katherine Daniels"], "link": "http://tech.gc.com/blog/2014/2/6/speeding-up-provisioning.html", "abstract": "One of the things that we often have to do on the GameChanger tech team, especially now that spring baseball season is approaching, is to bring up more server capacity in AWS to respond to higher traffic. We haven‚Äôt yet been able to make use of Amazon‚Äôs autoscaling feature that would handle this for us, so we‚Äôve been bringing this extra capacity up and down largely by hand (albeit with extensive help from a script we‚Äôve written to automate away most of the details). This process has always been rather slow, meaning that we are slower to respond when traffic starts to rapidly increase, so we started looking into how we could speed up the provisioning process. On Chef When we provisioned servers, we started with an essentially blank Amazon Machine Image (AMI, or the template from which EC2 instances are created) - it was a basic Linux installation with no GameChanger code or configuration. All of the configuration and deployment was then done using Chef, starting with this base image. Because it was starting from a blank slate, it had to do everything - from installing the user accounts that get installed everywhere, all the way up through the different services that run on our different types of servers (such as Apache on our web servers). This naturally was fairly time-consuming, taking on average 15-20 minutes to bring up one server. The other problem with doing provisioning entirely with Chef was fragility. If any part of the Chef run failed, the provisioning would stop, leaving the server in a partway-provisioned state that wasn‚Äôt able to handle any production traffic, and had to be fixed by an engineer who could diagnose the issue with Chef. An unfortunate side-effect of this was that external dependencies, such as software packages that get downloaded from third-party repositories, could block our provisioning process if they were unreachable. Chef and AMIs The great thing about Chef is that after it has set something up, the next time it runs it only has to verify that things have stayed in the correct state. If we‚Äôve created a user account, we don‚Äôt have to create it again if it‚Äôs still there. If we‚Äôve downloaded a package, we don‚Äôt have to download it again. This means that subsequent Chef runs complete much more quickly than the initial run. So we decided to use this to our advantage by creating our own AMIs from servers that had already completed this initial Chef run. We started out by creating our base AMI, which contained only things that were common to every single server in our infrastructure. This consisted of things like user accounts, environment variables, system tools, and so on, with no application-specific code or settings. We were then able to use this newly-created GameChanger base AMI to provision new servers, which cut several minutes off the provisioning time. Now, initial Chef runs on those servers could breeze over those common parts and only spend significant time on the application-specific parts. Getting More Specialized We have several main roles that most of our servers fall into. We have groups of servers for the web frontend, the web and mobile APIs, and servers that do various backend data processing. Each server in a group is identical to any other server in the group, so we decided to leverage those commonalities to create even more specific AMIs. Starting with our newly created base image, we extended our AMI-creation script (because of course we aren‚Äôt going to be doing this by hand!) to leverage our existing Chef roles to create a specialized AMI for each type of application server we use. Because the AMI creation process essentially takes a snapshot of anything that is on the server when the image is created, we did have to be a bit careful with what got baked into these images. Specifically, we made sure that Chef-client wasn‚Äôt in the image, to prevent it from getting started a second time on a new server made from this image, and we completely disabled Sensu (our monitoring service) when creating these images, both to prevent re-configuration issues with new servers, and because we don‚Äôt want to monitor anything on servers that only exist to create AMIs for other servers. The Results With the exception of our workers (which have many many queues used to process a variety of tasks), we can use these new customized AMIs to provision any of our servers in under 5 minutes. Overall, this came out to be a 70% improvement across the board. Why? Because almost everything that Chef needs to do has already been done, so it doesn‚Äôt have to do those things again when it brings up a server from these images. And because our AMIs have all the packages we need already installed, they don‚Äôt need to be reinstalled during provisioning of production servers, so we‚Äôre much less dependent on external package repositories than we used to be. Not only will this allow us to be much faster when we need to respond to increased traffic, resulting in a better experience for our customers, we will also be able to leverage these AMIs to make the shift to using Amazon‚Äôs autoscaling at some point in the future. Here‚Äôs looking forward to a busy spring season and a future post on autoscaling here! Share on Twitter Share on Facebook", "date": "2014-02-06"},
{"website": "GameChanger", "title": "The Right Amount of Test Coverage", "author": ["Phil Sarin"], "link": "http://tech.gc.com/blog/2014/2/3/the-right-amount-of-test-coverage.html", "abstract": "Engineers often have poor intuition as to what to unit test, so they fall into one of two camps: unit test everything or unit test nothing. Both of these are unhealthy extremes. Unit tests are important, but it shouldn‚Äôt be all or nothing. My principle for deciding what code should be tested is that the harder it is to detect bugs during manual regression testing, the more necessary to write automated unit tests. I‚Äôve been on both sides of this divide. When I worked at Vontu, automated test coverage was measured, and we sought to hit a high target, such as 80%.For a long time, I accepted this as the right way to work, but after a while I started to get the feeling that much of the time I was spending writing tests was wasted. I also have worked at Amazon.com, which had no institutional policy regarding testing, and at which many teams did no automated testing. Yet Amazon‚Äôs availability is very high ‚Äî for them, not testing everything is working. My objection to some of the attitudes I‚Äôve encountered is that there is often little logic or principle to them. They seem more based in world view than empiricism. Worse, they are often advocated by people who have always used one method. I reject the idea that there is a known amount of test coverage you should always strive for. If you have zero customers, your code coverage has produced exactly zero business value. Knowing what should be unit tested will always rely somewhat on intuition, but we can still discuss principles which should guide your team. My philosophy is that test coverage is not valuable in and of itself. It‚Äôs a proxy‚Äî for achieving quality, customer happiness, and business value, among other things. At GameChanger, we built a sizable customer base with no unit testing. Now, I‚Äôm not recommending this approach. The opposite, in fact. But while shooting for 100% is better than shooting for 0%, it‚Äôs still a huge waste of time. In the past few years, as our UI has become increasingly complex, we have shored up our gaps with automated tests (we use Kiwi to specify behavior), and we write unit and functional tests, in advance, for our new features if they exceed our threshold of needing them. This threshold is something that many engineers inexperienced with testing struggle with. Test writing is not a part of any CS curriculum I know about, and while books and blogs are a decent way to get started, they only get you so far. Without guidance, you can waste a lot of time, and worse still, write tests that miss the point (e.g. you test that some library code is working, rather than the code you‚Äôve written). It‚Äôs mentally easier to have a mandate to cover 80% of your codebase than it is to learn subtle things, but there is a lot of nuance to precisely what needs testing. We have come to believe that to determine when to spend the effort required to make code testable and write tests, it should be evaluated on how hard it is to discover bugs, rather than defaulting to unit testing or not. Here I am making the assumption that manual testing is a non-negotiable component of your release process. Even the most die-hard test-first adherents agree that you have to use your app to make sure it works. Details about our development and release cycle were published in a previous post about how to ship an app . In it, you can read about how much we value writing automated functional and unit tests in advance. However, we write those tests in anticipation of how much manual testing effort they save. To delve into how much automated testing should be done, let‚Äôs break the cost of bugs into two types: the cost of being bitten, and the cost to discover. The cost of being bitten by a bug is what happens to your business value when a bug emerges and affects your customers. A bug which prevents GameChanger users from creating teams is a total disaster, while mis-localizing a field as ‚Äúpostal code‚Äù vs. ‚Äúzip code‚Äù is trivial. The cost to discover is what you had to do to find the bug in testing. Discovering that you cannot create a new team in our app is very easy to discover in manual testing, which has to be done no matter what. On the other hand, making code testable and writing tests represents a cost. Cost to discover * cost of being bitten is roughly the equation which calculates the risk you take on when you code up new stories. So even when the cost of something going wrong is great, if the cost to discover is very low, the risk is low, and testing is a lower priority. Example GameChanger Code In order to make this whole thing concrete, I‚Äôm going to show you what I mean with the play editor from the GameChanger Basketball scoring app. I‚Äôll describe our test coverage as it evolved over several releases of the editor to cover cases of made and missed shots being recorded. Version 3.7 Pictured here is the app immediately after the screen has been tapped on the court area, indicating that a shot was made. On the right is the play editor, showing one play in the play history (made, by player #4), and a new shot, just entered. The model which is built into the UIViews containing the made/missed button and player number button has only two possible states ‚Äî shot.made == true or shot.made == false . Detecting an error in manual testing at this stage is trivial; it would not increase my confidence to write a test verifying that the correct PNG file gets displayed for each state. Version 4.6 In this version, the model still only has shot.made true and false states, but those states lead to more variety in the views. We‚Äôve added a ‚Äúsegment‚Äù (in our lingo, a discrete unit of visual info describing an aspect of a play) for rebounds, which can have either or neither team selected. Making this code testable, and testing it, is still more effort than it‚Äôs worth. Version 4.7 In this version of the app, the play editor moved to the bottom of the screen, and we added a state to the model, so now it has made, missed without rebound, and missed with rebound. It‚Äôs still easy to get the app into these states, and we continued to ship this code without unit testing it. Version 4.8 In v4.8, states explode as we added support for assists and blocks in advanced scoring mode, while simple mode remained the same as 4.7. The play editor would display a segment for adding a block to missed shots and a segment for adding an assist to made shots. Pressing the ‚Äúadd block‚Äù or ‚Äúadd assist‚Äù button would replace that button with yet another segment, which let you specify the player number for the block or assist. The combinatorics of the values a play‚Äôs attributes can have meant lots of states to test. At this point we added testing for this feature; the time to verify its behavior by manual testing would have inflated by ~10x! In retrospect, I‚Äôd have preferred that we added testing for v4.7, but no earlier than that. Version 5.1 (current behavior as of this post) In v5.1 we added support for 3-point and 4-point plays, but fouls can only be added in the editor for the most recent play. At this point there are three states for missed shots (simple, advanced with block, and advanced without block), and made shots have states for current or historical play, simple or advanced with or without blocks, assists, and fouls. Cost to discover bugs without automated testing has skyrocketed, and we have added a robust suite of tests which look like this. context ( @\"for a historical play segment with no timeouts before it\" , ^ { beforeEach ( ^ { [[ manager stubAndReturn : theValue ( NO )] isCurrentPlayManager ]; [[ manager stubAndReturn : theValue ( NO )] areAllSubsequentRowsTimeouts ]; }); it ( @\"has a shot segment and an add assist segment\" , ^ { [ manager updatePlaySegments ]; [[ manager should ] havePlaySegmentTypes : @[ @ ( GCPlaySegmentTypeShot ), @ ( GCPlaySegmentTypeAddAssist )]]; }); context ( @\"when an assist is added\" , ^ { beforeEach ( ^ { manager . event . assistAdded = YES ; }); it ( @\"has a shot segment and an assist segment\" , ^ { [ manager updatePlaySegments ]; [[ manager should ] havePlaySegmentTypes : @[ @ ( GCPlaySegmentTypeShot ), @ ( GCPlaySegmentTypeAssist )]]; }); }); The entire file for our Play Editor specs is just shy of 1k lines, so obviously significant time was invested into writing these tests (and the custom matchers we wrote to increase clarity). I‚Äôm trying to move the testing dialogue away from dogma. I think there are cases where automated testing really pays off, and cases where it doesn‚Äôt. When you write a test, you‚Äôre gambling that the time you take results in saved time and protects you from customer-impacting events, but it‚Äôs silly to act as if every gamble has the same odds. What I‚Äôm trying to do is work out where we should place the threshold, and how we should talk about where to place it. Cost of bug discovery is a major component in my process. I‚Äôd really like to hear what other people think; I suspect that either formally or informally, a lot of people who write unit tests are using thresholds, but probably not talking about them. Share on Twitter Share on Facebook", "date": "2014-02-03"},
{"website": "GameChanger", "title": "Making Downtime Less Painful", "author": ["Katherine Daniels"], "link": "http://tech.gc.com/blog/2014/2/10/making-downtime-less-painful.html", "abstract": "Downtime! It‚Äôs probably every sysadmin‚Äôs least favorite word. But sometimes it‚Äôs necessary, and when we‚Äôre lucky, we can plan for it in advance, during off-hours, to do some much-needed maintenance. Whenever we need to do maintenance or an upgrade on our Mongo database, for example, we put the site into ‚Äúscheduled downtime‚Äù mode. The end result for users is that they see a page saying ‚Äúwe‚Äôre down for scheduled maintenance, please come back in later‚Äù with a link to our status page. If we didn‚Äôt do this, users would instead see a blank page, or get lots of 500 errors, or other undesirable behavior. To accomplish this, we created a tool called Downtime Abbey. Abbey works by changing our Amazon load balancers to send traffic to a different port. This port is set up to respond with a 503 and the maintenance page- the 503 response (instead of the 500 errors that would otherwise results) tells search engines that this is temporary so they won‚Äôt remove our pages from their indexes. The tool uses boto to send all traffic to this downtime port at the beginning of our maintenance windows, and restores the original settings at the end. In addition, it reconfigures the load balancer health checks. Amazon ELBs have configurable health checks that makes sure each node in the load balancer is healthy, with unhealthy nodes being removed so traffic doesn‚Äôt get sent to them. When we‚Äôre returning a 503, this causes the health checks to fail, which would cause all the servers to get removed from the load balancer, which would get rid of the maintenance page that users see and display nothing instead. To work around this, Abbey changes the health checks to check a different target (one that returns a 200 OK during maintenance), again changing them back at the end of maintenance, so the load balancers are happy the whole time. But wait, there‚Äôs more! We use Sensu to monitor all our hosts and services, and naturally during downtime when things are purposely stopped, the checks for these things will fail. We have Sensu configured to send critical alerts to Hipchat, and this used to cause Hipchat to fill up with a lot of red durning downtime. So much red that we almost missed an actual problem in there - the chef-client failure at the bottom indicating that setting up a software RAID array didn‚Äôt work properly. Also, alert fatigue is bad, and training ourselves to ignore alerts is bad, so we needed to come up with a way to make these false (or rather, expected) alerts not happen in the first place. Sensu deals with failures by sending messages to one or more handlers. We have a group of handlers called the ‚Äòdefault‚Äô group that includes Hipchat, email, Datadog, and PagerDuty. Before, this was hard-coded into the Sensu configuration, but to deal with downtime alerting, we made it an attribute in Chef that we could override as needed. We created a Chef recipe called sensu::downtime that overrode the list of default handlers to be empty. Failures will still show up on the Sensu dashboard, but they won‚Äôt go anywhere else. After changing the handlers attribute, the recipe then restarts the sensu-server service so this change takes effect. Adding this recipe to the run list of the Sensu server overrides the default list (all the handlers) with the empty list, and removing it from the run list lets the defaults stay default (also restarting the service so we start getting alerts again). But doing that by hand would have been a pain, and one more thing to potentially forget, so PyChef to the rescue! Now, the buttons in the Abbey tool add and remove this recipe from the Sensu server automatically. This means that Sensu is quiet in Hipchat (and PagerDuty) during our scheduled downtime, so we don‚Äôt get flooded with red messages we don‚Äôt care about. At the press of a button, most of the downtime pain is automatically taken care of‚Ä¶ except for the pesky maintenance itself! Share on Twitter Share on Facebook", "date": "2014-02-10"},
{"website": "GameChanger", "title": "How to Ship an App (Part 1)", "author": ["Phil Sarin"], "link": "http://tech.gc.com/blog/2013/9/3/how-to-ship-an-app-part-1.html", "abstract": "We pride ourselves on getting things right. Our company values, posted on our wall, say, ‚ÄúDetest the unplanned, unpolished and unfinished.‚Äù Yet, several months back, we shipped an unstable version of our app. We didn‚Äôt realize that anything was wrong until the crash reports came in. Because of Apple‚Äôs approval process, we couldn‚Äôt get a fixed app released right away. We scrambled to notify our customers about workarounds. The experience prodded us to adopt new principles for shipping apps. We‚Äôve successfully applied those principles to our recent releases, which included a redesign of global navigation and an overhaul of basketball scoring. Nothing about these principles is specific to apps. I actually could have titled this series, ‚ÄúHow to ship anything.‚Äù In this post, I‚Äôll describe our first principle. Principle 1. Define release criteria based on quality, not time. We used to try to fit every release within a strict three week time box. There‚Äôs nothing inherently bad about time boxes. The problem was that the time box was our most salient release criterion. We‚Äôd feel good about ourselves when we released within our time box. We‚Äôd feel like we‚Äôd failed when our release stretched beyond our time box. Last-minute bugs put us under a lot of pressure. Should we violate our time box, thereby admitting failure? Or should we rush and attempt to meet our deadline? We‚Äôd usually rush. Rushing has a few predictable outcomes (see Seth Godin‚Äôs post ), all of which happened: Heroism. We fix the bug in time to meet our planned release date. Delay. Our rushed fix causes more problems that require more fixes. Disaster. Our rushed fix causes more problems that we don‚Äôt detect. We ship a bad app. Rushing is bad. In truth, deadlines don‚Äôt usually matter that much! We were psyching ourselves out to meet deadlines when taking an extra couple of days would have been better. How did we stop rushing? We redefined our release criteria based on quality and not time. We still try to release every three weeks or so, but we‚Äôll never release until we‚Äôve met our quality criteria: All new functionality must be tested for functional and aesthetic bugs, and all critical bugs must be fixed before release. Our regression tests ‚Äî automated and manual ‚Äî must pass. If we make any fixes, we must retest the changed functionality and rerun the full regression test suite.\nWhen we realize that we need to retest, we allow ourselves the time to do it well. If we slip by a day or two, it isn‚Äôt a big deal. Quality criteria apply to every tier Public releases aren‚Äôt the only ones that have quality criteria. Our release process has always had three tiers: Alpha, Beta, and Release. We defined quality-based entrance criteria and exit criteria for each tier. Alpha: All code that passes unit and functional tests is promoted to Alpha. Beta: Eric Han, our release manager, must approve all changes that are promoted from Alpha to Beta. Only approved Alpha code that passes a small subset of our regression tests is promoted to Beta. Release: Only Beta code that passes our full regression test suite is released to Apple. The success of our ‚Äògcios‚Äô build causes us to automatically release an Alpha build, which is downloadable via HockeyApp. The higher entrance criteria and the higher release criteria complement each other. We only run through our extensive Beta test suite on changes that we deem important enough for retesting. We‚Äôll leave other changes out of our Beta tier and we‚Äôll defer them to the Alpha tier for the next release. It‚Äôs about what you do under pressure Pressure situations make mistakes more likely. A good release process should handle pressure situations well. Under pressure, we make bad decisions and fall back on habits. Our company values don‚Äôt tell us to ‚Äúmeet deadlines at all costs.‚Äù Yet, under pressure, we found ourselves rushing a bad app to customers. Since we‚Äôve introduced quality-based criteria, we‚Äôve reacted the opposite way when under pressure. When we‚Äôve found late-breaking bugs in a recent release, we decided to slow down and re-test. We took an extra couple of days and launched when we were happy with our app. What‚Äôs next? Quality matters more than speed, but speed matters too. This first principle is about ensuring quality. The next two posts will discuss how to do so efficiently. Share on Twitter Share on Facebook", "date": "2013-09-03"},
{"website": "GameChanger", "title": "MVC Recommendations for Cocoa Touch", "author": ["Mike Onorato"], "link": "http://tech.gc.com/blog/2013/9/27/mvc-recommendations-for-cocoa-touch.html", "abstract": "Good MVC design comes with a number of benefits, including improved code readability, testability, and extensibility.  At GameChanger, we‚Äôve seen our crash rates drop significantly after rewriting parts of our app to follow good MVC design principles.  But what exactly is the proper way to implement MVC in Cocoa Touch? Apple‚Äôs recommendations for MVC leave a lot to be desired for iOS developers. The recommendations come with little to no explanation and some of them pertain to concepts specific to OS X (e.g. mediating controllers and Cocoa bindings).  To confuse matters further, the name ‚ÄúUIViewController‚Äù wrongly suggests that view logic and controller logic belong in the same class.  I want to explain how we at GameChanger interpret Apple‚Äôs recommendations and how we‚Äôve applied them to improve the quality of our codebase. Below is a diagram depicting Cocoa‚Äôs version of MVC.  In an ideal world, each box represents a distinct set of classes. Below are Apple‚Äôs recommendations for MVC (in bold), with the parts irrelevant to Cocoa Touch removed, and with some additional translations. 1. ‚ÄúAlthough you can combine MVC roles in an object, the best overall strategy is to keep the separation between roles‚Äù Translation: a) The best strategy is to put only controller logic in a UIViewController and put the view logic in a separate file.  The UIViewController subclass called XYZController can either load a nib from a file called XYZView.xib or initialize a UIView subclass called XYZView. Example: XYZController: - ( void ) loadView { self . view = [[ XYZView alloc ] init ]; } b) Controllers can configure views through properties and methods of view classes or through IBOutlets in nib files.  Views communicate back to controllers through the target-action pattern or delegate pattern.  It‚Äôs best to use target-action for simple views containing just a few UIButtons.  It‚Äôs better to use the delegate pattern when the view needs to interpret more complex interactions, e.g. in a UITableView or a view with custom animations. Target-Action Example: XYZController: - ( void ) loadView { XYZView * xyzView = [[ XYZView alloc ] init ]; [ xyzView . fooButton addTarget : self action : @selector ( fooAction : ) forControlEvents : UIControlEventTouchUpInside ]; self . view = xyzView ; } Delegate Example: XYZController: - ( void ) loadView { XYZView * xyzView = [[ XYZView alloc ] init ]; xyzView . delegate = self ; self . view = xyzView ; } c) Controllers should not be doing elaborate processing of data.  It‚Äôs better to create a model class or a datasource class, or create a separate data processor class and post a notification when data has been updated. 2. ‚ÄúA goal of a well-designed MVC application should be to use as many objects as possible that are (theoretically, at least) reusable‚Äù Translation: a) Use as many views as possible -  Views are generally very reusable so this makes the code base more extensible.  UIViews follow the composition software pattern so it‚Äôs easy to build up a more complex view from simple reusable subviews.  Reusing views also improves UI consistency. b) Use as many model objects as possible - Models are also very reusable.  It‚Äôs also easier to make more controllers when model data isn‚Äôt tied directly to a specific controller. c) Controllers are generally less reusable, but in most cases it‚Äôs still better to have a few simple controllers than one large controller that does lots of different things. The Single Responsibility Principle is key here. 3. ‚ÄúA view class shouldn‚Äôt depend on a model class‚Äù Translation:  This one is pretty apparent from the MVC diagram. 4. ‚ÄúA model class shouldn‚Äôt depend on anything other than other model classes‚Äù Translation:  Model classes shouldn‚Äôt send messages directly to controllers. Controllers can learn about changes to models through NSNotificationCenter or Key-Value Observing. 5. ‚ÄúA [UIViewController] depends on classes of all MVC role types‚Äù Translation:  This one goes without saying. Using good MVC design is important for maintaining a healthy codebase. Cocoa touch is a very flexible framework which is nice in some cases but that means that it‚Äôs even more important to have additional guidelines to keep your code quality in control.  These guidelines don‚Äôt work in every case (UITableView probably warrants its own blog post) but they serve as a useful guideline during the software design process.  A little bit of extra time spent thinking carefully about MVC during the design stage saves a ton of time and headaches later on. Share on Twitter Share on Facebook", "date": "2013-09-27"},
{"website": "GameChanger", "title": "Our Part-Time Testers", "author": ["Emily Yik"], "link": "http://tech.gc.com/outfielders/", "abstract": "Up until the latter half of last year, we, the QA team at GameChanger, dedicated many hours to manually running regression tests on our apps. We found ourselves caught in a time crunch before weekly releases, falling behind in testing new features, and struggling to maintain a high standard of quality for our products. An obvious solution would have been to create a suite of automated tests so that we could forego manual regression testing. Although we are starting to work on this as part of our complete test strategy, we still have a need for manual testing for many reasons. For instance, automated tests may not pick up on certain types of UI bugs like a flashing screen and they certainly cannot provide usability feedback. So we came up with the idea of hiring manual testers for regression testing‚Ä¶ At first, we hired a company that supplied us with on-demand testers who were randomly assigned to us when we needed to run our regression test suites. This meant that tests were always executed by someone who had never seen our apps before. However, without a deep understanding of functionalities within our apps, these testers were unable to effectively test and find bugs. This solution was a swing-and-a-miss. Assembling the Ultimate Team of Part-Time Testers We needed our QA team to focus on creating test plans, testing new features, and strategizing and implementing ways to attain and maintain higher standards of quality. So we set out to hire our own team of part-time testers to run our daily regressions and beta regressions. When we first thought about hiring testers, we were constantly weighing the importance of hiring someone with solid QA experience versus someone who was very familiar with our apps. At the time, we were so impressed with the enthusiasm of GameChanger users applying for the role, we ended up hiring avid GameChanger users with at least some QA experience. Contrary to the outsourced testers, our newly hired testers were real users who were personally invested in our products and able to provide real-world feedback. They were also easy to train, able to run tests faster, quickly find release-blocking bugs, and were more adept at finding complex bugs. The idea of having real users to test products was further solidified by a talk at the STAREAST Conference last year describing how other companies found success in hiring their loyal customers as testers. How our QA Team has Benefitted from our Part-Time Testers After a couple months of tweaking the job post for part-time testers and interviewing candidates, we were very fortunate to find a strong team of GameChanger users with QA experience. At first, we had them run our daily regressions and beta regressions. In time, we found that our testers were so proficient at finding bugs in our apps that we started to give them new features to test as well. If a new feature is ready to test at the end of the day, we are able to have our testers run test cases for the feature that evening. Any bugs that are found are ready for our engineers to work on the next morning, increasing the velocity of our product teams. While it is still our job as QAs to thoroughly test features, having the features tested sooner and having another set of eyes on them increases the speed and quality at which we can release features. We can also get feedback from our testers who are real users before features get shipped. Another huge benefit of having a team of testers is that we gain more coverage on a spectrum of devices and OS versions. In the past, an in-house QA would run an entire regression by him/herself on one device on one OS. We would have needed more QAs to provide the same coverage we have today. Now, we have testers testing our apps on various devices and OSs on a daily basis. The risk of our customers finding bugs related to specific devices and/or OSs has greatly reduced. Getting to Know our Remote Part-Time Testers A few months ago, a group of women at GameChanger, including myself, decided to participate in the Grace Hopper Celebration conference by submitting topics to the conference that focused on women in technology. As we discussed potential topics to submit, I thought about doing some research on our testers on the basis that they were mostly women. Gender aside, I wanted to better understand what drives our testers to put forth incredible amounts of effort every day to ensure that our apps adhere to high standards of quality. Getting a more holistic picture of our testers‚Äô backgrounds and their thoughts could help us understand both the positives and negative aspects of their roles at GameChanger. The hope was to use these findings to help us optimize and enhance how we work with our testers. So I sent out a questionnaire to our team of part-time testers with some open-ended questions and found some common themes in their responses. Some facts about our testers: All but one of our testers are users of our apps All but one of our testers are women All of our female testers have families with children General themes that came up in their responses to the questionnaire: All of those testers were drawn to the role in part because they love our apps and are long-time users of it Only one tester does not have an additional full-time or part-time job Being able to spend more time with family was evident in the women‚Äôs responses Most testers mentioned that lack of physical face-to-face contact was a negative but daily communication via messaging was a plus The women: One of the themes that came up for the women was an increased sense of productiveness, accomplishment, and self-importance Some of the quotes from our testers‚Äô responses that really struck a chord with me: ‚ÄúThis position had nothing to do with the hours/money I really like GameChanger as an app for softball and thought it would be fun to get a job there‚Äù ‚ÄúI love the app and wanted to be part of something new (change it up a little from my normal job)‚Äù ‚ÄúI like being productive, using my brain, actually speaking to adults once in awhile, and earning an income.‚Äù ‚Äú‚Ä¶after 17 years in the same job you get comfortable and you get scared that you couldn‚Äôt leave that job and find something else (not capable enough etc.), but know that I really do have the experience and knowledge and work ethic to gain a new position was pretty powerful (personally). This may sound odd, [‚Ä¶] but it gave me the little extra spark that (if something was to happen with my regular job) I would be ok.‚Äù ‚ÄúI had been a home-based employee the last 8 of those years [‚Ä¶] I have a child with epilepsy and when she was diagnosed 7 years ago we decided that it would be best if I stayed closer to her schools in the event something happened.‚Äù Changes in How We Work with Our Testers Since acquiring the responses to the questionnaire, we have made conscious decisions as to how we work with our testers. Being Sensitive to Personal Schedules The most obvious theme from the responses was that all our testers have family and/or other job obligations. We try to be cognizant of their busy schedules when we communicate with them and when we schedule tests for them to run. We are also flexible as to the time of day they test and if they need time off. To help our testers plan their days, we have set up a calendar with daily assignments that stays quite consistent from week to week. We have found that once we implemented a consistent and transparent schedule that they could follow, it seemed to have lessened anxiety around balancing testing with their other jobs and personal tasks such as picking up their kids from school and going to their kids‚Äô ball games. Meeting with Testers Lack of face-to-face contact was one of the negative aspects of working remotely that we also wanted to address. After being aware of this, we have made a conscious effort to either meet with our testers on video chat or by phone in order to keep our lines of communication more open and personable. In addition, our testers we use Slack to keep us connected at all times. Changes to How We Select our Testers When the time comes to hire additional testers, we will be focusing on candidates who are users of our apps. This approach has already proven to be a huge benefit to our company. Our testers are always excited to test out new features and promote them to their friends and family. They truly care about maintaining high standards for our products as they use them regularly to track teams that their families are involved in. Maintaining Good Relationships with Our Testers Companies should show appreciation to all their employees and we wanted to make sure that our testers felt valued even though they work remotely. We want them to be happy in their roles, continue to care deeply about our products, and be proud to work at GameChanger. Showing appreciation for our colleagues is already part of our company culture for those of us who work in the office. For our remote workers, we try to convey our appreciation on a daily basis through our communication. We try to check in on how they are doing personally and how they feel about our processes and the workload that we give them. We also try to respond to any of their needs and concerns as quickly as possible. In essence, we treat them as part of the GameChanger family. Conclusions While automated test suites are something we are currently looking into building, manual testing is a necessity for our apps due to complex user interfaces and our company‚Äôs extremely agile environment. We are happy to say that we now have a strong group of testers. Our testers have contributed to much lower crash rates in our apps and a lower incidence of hotfixes. We have also seen a significant reduction in customer complaints about bugs. Our QA team is now able to focus on releasing quality features and implementing strategies that help maintain high standards for our apps. Since our testers are real users of our products, they are also great promoters of our products within their communities. User feedback from them is also very valuable to us. Even more rewarding is that not only do our testers benefit our company, but they also find that their roles benefit themselves and their families. We are so grateful to have such a great team of part-time testers and hope that we can continue to work with this amazing group of people. Share on Twitter Share on Facebook", "date": "2018-06-28"},
{"website": "GameChanger", "title": "Learning Kotlin Constructor as a Java Developer", "author": ["Xi Wei"], "link": "http://tech.gc.com/learning-kotlin-constructor-as-a-java-developer/", "abstract": "I have been developing Android apps in Java for years. I recently joined GameChanger and was excited to learn that GameChanger is using Kotlin. I originally thought that moving to Kotlin would be as simple as learning some new syntax, but I discovered that there was more to it. After a brief learning period, I‚Äôm up and running working in Kotlin. In this article, I‚Äôll save you some of the trial-and-error by introducing some important concepts about constructors for those making the jump from Java to Kotlin. Not All Constructors are Created Equal in Kotlin In Java, all constructors are equal in a sense. public class Person { String name ; int age ; public Person ( Person p ) { name = p . name ; age = p . age ; } public Person ( String n , int a ) { name = n ; age = a ; } } Take the above example, Person(Person p) and Person(String name, int age) can be used independently. Of course, you can choose to have one calling another, but it‚Äôs not required by the language. In Kotlin there is always a primary constructor. Any additional constructors are secondary constructors. The primary constructor is always incorporated into the class header. class Person ( n : String , a : Int ) { var name : String = n var age : Int = a } The variables name and age are initialized with n and a . In fact, n and a are available anywhere in the class for variable initialization. class Person ( n : String , a : Int ) { var name : String = n var age : Int = a var nameX2 : String = n + n init { println ( \"The age is: \" + a ) } } Any other constructors would become secondary constructors which are required to call the primary constructor in the very beginning. class Person ( n : String , a : Int ) { var name : String = n var age : Int = a constructor ( p : Person ) : this ( p . name , p . age ) { } } The Order of Creation So, what is the order of processing when calling a secondary constructor? The primary constructor is invoked first, which triggers all the initialization from top to bottom. Then, the body of the secondary constructor is executed. class Person ( n : String , a : Int ) { init { println ( \"1st: initialization block 1 run\" ) } var name : String = n . apply { println ( \"2nd: initialization lines run\" ) } var age : Int init { age = a println ( \"3rd: initialization block 2 run\" ) } constructor ( p : Person ) : this ( p . name , p . age ) { println ( \"4th: secondary constructor run\" ) } } Can I Skip the Primary Constructor? No, a default primary constructor is still there even when you don‚Äôt write it and you are still required to call it in the secondary constructor. Also, you cannot initialize the variables in the secondary constructor because it has already passed the initialization timeframe. (Well, they are actually properties, but they behave just like variables in this case.) class Person () { var name : String // Compile Error: Property must be initialized var age : Int // Compile Error: Property must be initialized constructor ( n : String , a : Int ) : this () { name = n age = a } } OK, if you really want to fake it like a Java constructor, here is the hack. It‚Äôs NOT recommended and I wrote it just for the learning purpose. class Person () { var name : String = \"\" var age : Int = 0 constructor ( n : String , a : Int ) : this () { name = n age = a } } What happened above is that we initialized name and age with a default value and assigned them to a new value in the secondary constructor. However, it doesn‚Äôt work when you replace var with val because Kotlin does not allow variable initialization in a secondary constructor. Overloading Arguments with Default Value Last, but not the least. Kotlin has this elegant way to overload arguments with default values. class Person ( n : String = \"Nameless\" , a : Int = 1 ) { var name : String = n var age : Int = a } fun main ( args : Array < String >) { var p1 = Person () // name: Nameless, age: 1 var p2 = Person ( \"Tom\" ) // name: Tom, age: 1 var p3 = Person ( a = 5 ) // name: Nameless, age: 5 } The primary constructor is actually a sweet requirement in Kotlin. You will always know what is expected to create the object by scanning the the primary constructor without having to look at all the constructors as you would in Java, because constructors are no longer independent of each other. The primary constructor is also easy to spot because it is the first line of the class. If you are starting on learning Kotlin, I think the constructor is a good starting point. I hope you find this post helpful. Share on Twitter Share on Facebook", "date": "2018-03-27"},
{"website": "GameChanger", "title": "Sync for GC Team Manager", "author": ["Alex Etling"], "link": "http://tech.gc.com/sync-post-2/", "abstract": "Introduction In my last article I talked about the sync system for our new GC Team Manager app and the trade-offs we considered in our design process. To reiterate, the high-level structure we settled on was a backend Pub/Sub service, with small granular updates, and we needed to account for the lack of ordering in message delivery. In this article I will cover how we implemented and made this sync system work. I mentioned in the last post that part of the reason we settled on Pub/Sub was because we could use an iterative approach instead of all or nothing. This iteration broke down into two distinct parts, Asynchronous Polling Sync and Pub/Sub Sync. Asynchronous Polling Sync relied on Asynchronous Polling and topic updates. Pub/Sub Sync built device notification on top of Pub/Sub Sync. I will describe each in turn. Asynchronous Polling Sync The goal of Asynchronous Polling Sync was to solve a few distinct problems. First, it focused on building an algorithm that was resilient to out-of-order messages received by the devices. Second, it focused on turning backend database updates into targeted sync topic updates. To simplify the initial build, Asynchronous Polling Sync specifically avoided solving the problem of pushing topic updates to the devices themselves. It did this by allowing the devices to use Asynchronous Polling to get all updates. Switching from Asynchronous Polling to direct topic updates is the provenance of Pub/Sub Sync. I will cover each part of Asynchronous Polling Sync. Asynchronous Polling Sync Algorithm Handling Out of Order Messages When thinking about the out-of-order messaging problem, the goal is to ensure that no matter what order message are received in, all devices will end up in a consistent state. When we were thinking through possible ways to do this in our sync system, we realized that the type of message being sent can help you solve this problem. As a demonstration I will consider three types of updates the backend could send to the app: send the data values of the updates, send the type of data change(update, delete, etc), or just send the id of the thing that changed. Scenario 1: Team 1 is created then updated. All apps receive updates in the correct order Specific Updates: 1. {id: <team_1>, type: 'team', updates: 'created'} 2. {id: <team_1>, type: 'team', updates: {name: 'Cool New Name'}} Type of Updates: 1. {id: <team_1>, type: 'team', change_type: 'created'} 2. {id: <team_1>, type: 'team', change_type: 'updated'} Id and type updates: 1. {id: <team_1>, type: 'team'} 2 {id: <team_1>, type: 'team'} Scenario 2: Team 1 is created then updated. All apps receive updates in reverse order Specific Updates: 1. {id: <team_1>, type: 'team', updates: {name: 'Cool New Name'}} 2. {id: <team_1>, type: 'team', updates: 'created'} Type of Updates: 1. {id: <team_1>, type: 'team', change_type: 'updated'} 2. {id: <team_1>, type: 'team', change_type: 'created'} Id and type updates: 1. {id: <team_1>, type: 'team'} 2 {id: <team_1>, type: 'team'} As you can see, all update types work when messages are received in order. But as soon as messages start to be handled out-of-order, sending the id and the type of update is the only method that cannot lead to incorrect app side data. All the device has to do when it receives an update is to go to the respective API resource endpoint and load the current data. This method will always lead to the app having the most up to date data. (Note that the type of objects sent in our updates map nicely to the resources of in our API.) This simple update then reload algorithm lets our sync system be robust to out-of-order messages. Topics We now have a type of update that we can send that allows the system to survive out-of-order messages. But specific updates are just part of the solution. I also need to discuss the topics themselves and how they are structured. We have a few high level topic types. These were chosen based off of our resource model. Any other resource updates are put in those high level topics. Each topic contains a list of update objects described above Each topic also has three other important values current_offset - This number can be used to help figure out what updates each device has already seen. It is a number indicating the most recent update sent to the topic. Each new update pushed into the topic increments the current_offset . max_number_of_items - This number indicates the max number of updates to store for this topic. We structure our topics to only store a certain number of updates depending on the type of topic. storage_id - If we change the storage database or reset the updates, the storage_id allows us to communicate that effectively to the app. As a simplification, this value indicates the version of the topic you are looking at. Algorithm Given the structures above we use the following algorithm in Asynchronous Polling Sync: When the app initially starts up, it gets the current_offset and storage_id for all topics it cares about. (This same procedure happens when it decides it cares about a new topic). When the app reopens and/or every X minutes, the app asks for all updates on all topics it cares about and sends down the current_offset and storage_id If the storage_id sent does not match that found in the backend, a specific error is returned. If the app gets that error it knows it needs to do a full resync of that topic If the current_offset sent < current_offset in the backend - max_number_of_items a specific error is returned. If the app gets that error it knows to do a full resync of that topic. If both storage_id and current_offset are valid, the backend returns back a list of all updates whose offsets are between the app‚Äôs current_offset and current_offset in the backend. For each update the app receives back it attempts to reload the the resource described in the update from the API. GET /persons/<person_id> , GET /games/<game_id> , etc. When it has successfully reloaded all resources mentioned in the updates, the app updates the current_offset for the topic. This algorithm and the structures described make it very easy for the app to get updates it cares about, every X minutes, without having to worry about out-of-order messages. Database updates -> Sync topic updates Where to store Updates Now that we have a robust app sync algorithm, we still need a way to start pushing updates into the topics when someone alters an API resource. Before we can decide how to transform backend updates to sync topic updates, we first need to figure out where to store our updates. What type of database should we use? The main database for our backend is Postgres. But there were a few reasons we were hesitant to just stick our updates in Postgres: We did not want the large number of sync updates to swamp out our responsiveness to actual API calls Making a dynamic number of topics to hold updates + the three variables each topic needs, ends up being pretty hard to do in Postgres. Contention on the topic updates table could end up very high which could slowdown all Postgres operations. We needed a system which is super fast, has great list support, provides transactions, and is simple to get up and running. For these reasons we decided to store all of our updates in Redis . Transformation So we now have the place to save our updates, but we still need a way to translate the Postgres database update to sync topic updates. We need some sort of transformer like in the picture above. This transformation actually ends up being very easy. We have a 300 line class which takes in query and update objects for each Postgres write and transforms those into a list of topic updates. The topic updates are then saved to Redis and then the main database updates are saved to Postgres. A simplified version of our transformer class can be seen below. After we had built the features and functionality described above, Asynchronous Polling Sync was finished. That left us the time to prioritize and build Pub/Sub Sync when it became appropriate. Pub/Sub Sync The main difference between Asynchronous Polling Sync and Pub/Sub Sync is that Pub/Sub Sync replaces the Asynchronous Polling loop with a Pub/Sub service. All other parts of the system remain the same. This allows the devices to receive updates near instantaneously. Once again there are two pieces to consider when we built out Pub/Sub Sync, the algorithm to use for sending topic updates to the devices and how to actually build the Pub/Sub service. Pub/Sub Algorithm Similar to Asynchronous Polling Sync, there are many different algorithms we could use when pushing updates to the Pub/Sub service. Originally we just planned to send the full sync updates to the Pub/Sub service. This strategy ended up being suboptimal because there are times when Pub/Sub messages are dropped and not sent to the devices they were supposed to. Sending our full topic updates meant our algorithms depended on all devices receiving all sync updates. When this turned out not to be true we would then require a lot of extra complexity to handle dropped messages. Instead, we decided to just send a blank update that indicates something has changed on the topic: {changed} . When a device receives this message, it attempts to load all updates in that topic. For each update it loads, it GETs the proper resource from our API. This algorithm adds an extra step and network call, but keeps the overall algorithm very simple. How to send Pub/Sub system updates We now have an algorithm we want to implement to finish up our sync system. How do we build out this Pub/Sub service to send updates? Lucky for us, after some research and thought, we were able to find a prebuilt system that could serve as our Pub/Sub service: Google‚Äôs Firebase Cloud Messaging . (Specifically FCM‚Äôs topic feature ). This free system allows devices to subscribe to topics and will make sure messages sent to those topics are delivered to those devices. The system also has a bunch of nice features: storing a certain number of messages per device if an app cannot immediately be reached, a very long TTL for messages, a pretty high per topic rate limit, and seemingly no overall rate limit. This system was also very easy to integrate with from both the client and backend side. We pretty much just dropped in FCM as our Pub/Sub Service and with that our sync system was finished. Conclusion We have come to the end of our discussion of our new sync system. We started out with an ideal sync system and some constraints on what our system needed to be able to do. We covered the trade-offs and criteria we considered when designing our system. Finally I covered all the technologies, algorithms, and systems needed to implement a sync system incredibly close to our ideal. The full flow implementation of our system can be seen above. Share on Twitter Share on Facebook", "date": "2018-01-25"},
{"website": "GameChanger", "title": "Sync for GC Team Manager", "author": ["Alex Etling"], "link": "http://tech.gc.com/sync-post-1/", "abstract": "Introduction GameChanger recently released an all new app, GC Team Manager . This wonderful new product helps youth sports communities communicate, coordinate, and organize their team‚Äôs lives. I have been lucky enough to be on the team that gets to build the backend for this new app. Throughout building out this completely new backend, I have gotten to work on a lot of different pieces of the system. The one I want to dive deeply into today is our sync system. Sync is a pretty generic, buzzwordy programming term. Therefore, it is important for me to clarify what sync means in this context. In this article, sync is the processes of ensuring that all devices in our system have a shared, up to date view of the world. If one device sees the state of Team A one way, all other devices should see the exact same state for Team A. If a device makes a change to Team A, that change is reflected instantaneously across all other devices. Note that this is the ideal version of sync. There are many constraints and challenges that make this hard to achieve but this is the goal I set about building towards. The reason sync needed to work this way in our app is: Users expect it - Users have begun to expect near immediate updates to propagate across their and their friends‚Äô devices. Avoids confusion between users - The closer to instantaneously that an app updates itself the less likely it is for two users to find divergences in their data. We want to avoid two coaches on the same team seeing different versions of that team‚Äôs data. Keeps reconciliation simple - The longer period of time where data can diverge, the more work it can be to reconcile changes into one object. We want to try and limit the potential for divergence and to limit the reconciliation complexity. Having settled on what sync means, the rest of this article attempts to describe the design and architectural decisions and trade offs that went into solving the sync problem for our new Team Management app. There will be a follow up article that dives into the nitty gritty implementation of our sync system . But before I can fully describe the trade offs made in designing this system, I must describe the constraints of this system: Each app needs to operate independently - Each app must be able to create, read, update, and delete data without having to coordinate between other apps. Our apps should not solve this distributed system problem. This allows our app to remain quick and responsive and allows our app‚Äôs developers to spend more of their time building features users want to see. That means the solution, complexity, and implementation need to be built in the backend. Apps need to be fast - Sacrificing speed and usability of the app to make this sync system work is a non starter. The solution must handle network outages and off line usage - The parents and coaches that use our apps sometimes find themselves at sports fields with poor or no network coverage. The nearer to instantaneous the update, the better - See the ideal sync system described above. Do not drain the device‚Äôs battery - The fastest path to getting your app uninstalled is to kill the phone‚Äôs battery, so our solution needs to be as battery efficient as possible. Take an iterative approach and use the smallest amount of time possible - Since we are designing a totally new backend for this new app, there are tons of features that need to be built to deliver real value to our users. We cannot drop everything to make this system perfect and anything that allows us to build it iteratively is a big plus. Given the constraints I talked about, I wanted to cover 4 broad alternatives we ran through when designing this system. For each architecture I will discuss the pros and cons of this system. At the end I will describe the architecture solution we went with for our sync system. 1. Just In Time Loading The idea behind just in time load (JITL) is pretty simple. The app stores no data locally, i.e. no local database. When it needs to display a new screen on the app it goes to the backend and loads all data it needs. It then takes that data and displays it as needed on the device. On the next screen, the app simply rinses and repeats. Positives: Simple - This approach is very simple from both a backend and a client perspective. Since we are using http as app‚Äôs sync protocol, it is incredibly lightweight. Data is up to date - Since the app is pulling data from the backend right as is need it, the users will see the most up to date version of the data. This meets the ideal of our sync system. No need to maintain a local database - This is strongly related to the point about simplicity, but not having to build and maintain a local database for the app is complex enough to merit its own data point. Negatives: Slow - Network requests are some of the slowest calls anyone can make in computing. Cellular network requests take even more time due to data bandwidth constraints and latency. In this system the app has now delayed displaying all screens until it makes anywhere between 1 and N network requests. This will slow the app down to a crawl and make it nigh unusable. Only works online - This violates one of our constraints. If the app only display a screen after loading data from the network, it becomes unusable offline. This wastes phone battery - Using a devices network card has a high battery costs. The ideal way to use the network is to make a bunch of requests all at once and then leave the network off for a period of time. In this scenario we would constantly be turning the network card on and off, wasting battery. Data is not always up to date - If a user loads a screen on the app and then sits there for a period of time, the JITL system can lead to divergence. The data displayed can be changed by a different device in the background leading to divergence and confusion. 2. Asynchronous Polling This Asynchronous Polling (AP) system is very different from JITL. First, this system involves a local database. This stores a version of all data the app needs. Every time the app wants to display a screen, it uses the data in its local database. Second, instead of loading data from the backend every time we display a screen, the app loads the data every X minutes. That data is then saved into the app‚Äôs local database. Positives: Fast - The AP is a drag racer compared to JITL‚Äôs bicycle with square wheels. Replacing many network requests with local database reads makes this system fast and user friendly. The system now works offline - Another benefit of the local database is that the system works offline. When the app loses network connection, it just reads data from its local database and stops polling. More battery efficient - In the JITL system every time we loaded a screen we were spending battery life to use our network card. In this new system the app only uses the network card every X minutes. This is much more efficient. Negatives: Introduced divergence and delay - One of the worst things about this solution is that we have moved further away from our ideal sync system. We now have up to an X minute delay between the time one app makes a change to the backend till all other devices have synced that change. Always runs every X minutes - Another downside of this AP solution is that the polling loop runs whether or not any data in the backend has actually changed. This has negative consequences for both the app and the backend .For the app that means that we could be wasting both CPU and battery life every X minutes for no updates. This can lead to the app being slower and more of a battery hog. From the backend‚Äôs perspective the consequences of this system are dramatic. For every device running our app, every X minutes we get a series of request to refresh each app‚Äôs local database. This ends up being a lot of requests and scales linearly with the number of devices. From past experience with our scorekeeping app , these polling sync requests can come to dominate all requests to our backend. Open Questions: How does the app know what to reload? - Even with AP, the app still needs to determine what has changed and what data it should fetch. There is a spectrum of solutions to this problem. At one end, the app can reload all data currently in its local database. On the other end the device can identify itself to the backend and ask what all has changed since its last request. Because the app still does not know what has changed, the pure app solution still involves a lot of unneeded work. The pure backend solution involves a lot of complexity in order to track what each device cares about and what has changed since each device last synced. Ideally a balance is struck, where the backend has a good way to track changes and the app gives it some context on what it actually cares about. Generally though, no matter what our solution here, we will be adding complexity somewhere. 3. Targeted Push Notifications Targeted Push Notifications (TPN) builds on the failures of the AP system. It keeps the app local database which confers the same speed and offline benefits as in the AP system. But now, when an app saves data to the backend, the backend is responsible for notifying all devices who care that something has changed. The backend does this by transforming all database updates into a set of updates per device and sending updates for those devices to a push notification service (something like APNs ). Positives: Near real time data consistency - Because push notifications are sent and received very quickly, we move much closer to the ideal of our sync system. Almost as soon as something changes, our devices should know about it and be able to reload. Much less load on the backend - Another benefit of dropping the polling loop is that every app no longer hits the server every X minutes. Each app only requests data when something has changed. This can drastically cut down on the total number of requests on our backend. More Battery Efficient - Instead of of the apps needing to poll every X minutes they now only use network when saving data or when a push notification informs them that something they care about has changed. This solution begins to reach the upper limit of batter efficiency. Negatives: iOS push notifications are not guaranteed to be delivered - As the saying goes: ‚ÄúIn theory, there is no difference between theory and practice. But, in practice, there is.‚Äù A huge detractor here is that iOS push notifications cannot be relied on to always be delivered. If we build a sync system that relies on devices getting an update, and that update is never received, the divergence in the system quickly becomes very large. There are no ordering guarantees - Because of how distributed systems work, when sending multiple updates to an app, there is no guarantee that the first update will be received first. This is true, even if iOS push notifications were guaranteed to be delivered. That does not mean we cannot use push notifications, but it does mean that our sync system will need to take this into account. Conversion of database save to device push notification is very complex - This solution falls on the all backend side of the reload question above. The backend is now forced to know about what data each device cares about in order to figure out which devices need to be informed on data save. This means that devices now need a way to signal to the backend that they care about a piece of data. This leads to much higher complexity, orchestration, and implementation time. Open Questions: What level should updates be at? - Even if we had the infrastructure already built, could ensure push notification delivery, and ordering guarantees, the TPN system still has an open ended question. What level of updates do I send to the app. If an RSVP for person A for game X on a team 1 changes, do I tell the app that the RSVP changed, the game change, the team changed, or something else? The solution we choose has network and complexity implications for both the backend and the frontend. 4. Pub/Sub The Pub/Sub solution is very similar to the TPN system. Both have a local cache. Instead of transforming database updates into device updates, the Pub/Sub model converts them into a series of topic updates (In our case teams, persons, etc). These topic updates are sent to a Pub/Sub Service. At the same time each device subscribes to all of the topics they care about. Then, when a new topic update is pushed to the Pub/Sub Service, all devices subscribed to that topic get that update. Positives: Near real time data consistency - see TPN positives Much less load on the backend - see TPN positives Much stronger delivery guarantees - Because we are no longer constrained to using push notifications, we have much more solid guarantees around delivery. Converting a backend change to a topic change is relatively simple - The topic updates used in this system are much easier to derive than device updates used in the TPN. All the backend needs to know are business rules mapping backend updates to topic updates. There is no longer a need to coordinate with each device to figure out what it cares about. Negatives: No ordering guarantees - See TPN negatives Worse battery life - Since we are no longer using efficient push notifications we will have to start using more network requests to keep the device up to date with the Pub/Sub service. This might be more efficient then the polling loop described above, depending on the level written at, but is still less efficient then push notifications. Open Questions: What level should updates be at? - see TPN open questions Conclusion So I have discussed four different solutions with different trade offs we could use to build our sync system. Which ones did we go with and why? The final implementation of our system is very much based off of the Pub/Sub system described. On top of that we needed to add a way to handle out of order delivery guarantees and wanted to send targeted updates. The reasons we chose this implementation are: Pub/Sub provides near ideal sync. This allows us to meet users expectations and puts us on a footing to compete with any app out there. We were willing to sacrifice some battery life in order to get stronger delivery guarantees. These stronger guarantees can allow the system to be much more reliable and much simpler from the app‚Äôs perspective. Topic updates are much simpler than device updates and allow this system to be built much more quickly. We were able to map out a simple iterative development path starting with AP and evolving into Pub/Sub that allow this system to be built and tested in pieces. If you want to learn more about the specific implementation  this solution, check out my follow up blog post . Share on Twitter Share on Facebook", "date": "2018-01-24"},
{"website": "GameChanger", "title": "Beyond the Mean", "author": ["Alex Hsu"], "link": "http://tech.gc.com/beyond-the-mean/", "abstract": "I work as a software engineer at a sports tech company and I am a woman. These facts about my life are why I continue to be distressed about the news of the Google Memo, written by software engineer James Damore, and its ramifications for my industry. While the headlines may make the memo seem like it was written by a deranged maniac, the fact that it wasn‚Äôt written as a blatant anti-diversity statement (it begins: ‚ÄúI value diversity and inclusion, am not denying that sexism exists, and don‚Äôt endorse using stereotypes‚Äù) is one of the things that makes it harder for people to be immediately outraged about its contents (and why this discussion is so fascinating and important). As a software engineer, questioning assumptions and raising potentially controversial ideas is part of the job. I agree with the Damore‚Äôs assertion that ‚ÄúIf we can‚Äôt have an honest discussion about this, then we can never truly solve the problem.‚Äù The topic of diversity is critically important and I want to add my voice and insights to the discussion as honestly as possible. I agree with the myriad of responses suggesting that there were many productive ways that Damore could have raised some of his concerns, such as his views that conservative viewpoints aren‚Äôt valued in tech or that the diversity programs at Google should be more inclusive, without perpetuating gender stereotypes to make his point. I am pained to see that people are now afraid to raise controversial or minority opinions or ask questions because of the consequences of this memo. But the issue for me is not the theoretical or hypothetical raising of unpopular opinions. In theory I absolutely agree that is very important. But I want to focus on the reality of this situation and the memo that was actually published. Damore‚Äôs main argument is that ‚ÄúGoogle‚Äôs political bias has equated the freedom from offense with psychological safety, but shaming into silence is the antithesis of psychological safety.‚Äù However, you can‚Äôt ask for psychological safety for one group while stomping on the psychological safety of another group. Content matters. The way these opinions were raised matters. Making an entire gender feel less qualified or question their career choice matters. As a female software engineer, the response to this memo makes me feel less safe. As a coworker, and fellow female engineer, put it, ‚Äú[the memo] weighs on me because it validates my fears that I will have to spend most of my career proving I am competent before I‚Äôm given the space and respect to learn and grow.‚Äù Despite Damore specifically recommending against reducing a population to their average, by even discussing ‚Äúaverage‚Äù traits of women and men, the memo now provides specific stereotypes for people to point to to explain why a woman may be a lesser engineer. When you first start at a job or on a team no one has reasons to assume that your traits are anything but ‚Äúaverage‚Äù until proven otherwise. If people believe that the ‚Äúaverage‚Äù woman is less qualified it puts more onus on women to prove their worth the second they walk in the door. And trust me, even without this memo circulating, women in the tech industry are constantly being forced to prove their competence. Damore selected three particular traits to discuss. He did not include his reasons for selecting those particular traits as his examples of genetic differences between males and females, although he did discuss ways in which, in his opinion , those traits cause problems in the tech industry. In summary, he mentioned that ‚Äúaverage‚Äù female traits include: 1) ‚Äúopenness directed towards feelings and aesthetics, rather than ideas‚Ä¶[including] a stronger interest in people rather than things;‚Äù 2) ‚Äúextraversion expressed as gregariousness rather than assertiveness,‚Äù which included ‚Äúhigher agreeableness;‚Äù and 3) ‚Äúneuroticism (higher anxiety, lower stress tolerance).‚Äù Each of these terms in the memo also hyperlinks to a Wikipedia definition, rather than scientific treatises, and therefore seem to be a random selection of traits and stereotypes that Damore believed to be detrimental to the workplace. However, not only am I skeptical of the representativeness of the chosen traits and the sources cited, I believe that some of these traits are actually crucial to create a balanced, productive workplace. Imagine software built by a team where all members were more interested in ‚Äúthings‚Äù than people and no one wanted to pay attention to aesthetics (assuming the team was able to ship anything if it was comprised of assertive rather than agreeable people). Additionally, while defining these ‚Äúaverage‚Äù traits, Damore‚Äôs stated that ‚ÄúMore men may like coding because it requires systemizing and even within [software engineers], comparatively more women work on front end, which deals with both people and aesthetics.‚Äù This assertion really got under my skin because, for a split second, it made me self conscious of being a front end leaning developer. I do not only enjoy front end work because of aesthetics, that is one of the last pieces that I consider in my day to day work. Working as an engineer in any capacity requires deep systemizing and writing code. Contrasting front end work with coding in this way undermines the amount of critical thinking that front end engineering requires. This also serves to enforce biases that one gender should work on a particular part of the stack. I can only share my own experience, but I believe it is important for more individuals to share their stories. To show that generalizing to the ‚Äúaverage‚Äù actually hurts real people on real journeys. My Journey My entire adult life I have heard things like ‚Äúyou only got into MIT because you‚Äôre a woman‚Äù or ‚Äúyou‚Äôre just a diversity hire.‚Äù I went to a job interview where I asked how many women they had on the engineering team and the interviewer freaked out briefly and said ‚ÄúWe only have one! But we‚Äôre trying to improve that number! That‚Äôs why we‚Äôre interviewing you!‚Äù By citing a diversity statistic as a reason to interview me, I suddenly felt less qualified for the position. Needless to say, I did not accept that job offer. But this constant questioning or suggesting that I am only interesting because I increase diversity numbers leads to feelings of imposter syndrome and wondering if I do belong or if I am actually good enough. When people ask me what I do for a living and I say ‚Äúsoftware engineer‚Äù they often respond with a surprised ‚Äúreally?‚Äù or if I say I work at a tech company they say ‚Äúoh as a designer?‚Äù (while I AM fully qualified to be an engineer, I am in NO WAY qualified to be a designer). I have even had this experience with other attendees at conferences specifically targeted at Software Engineers. As these conversations continue, when we finally do establish that I‚Äôm an Engineer, often times when I say I work on front end/UI work that is when the lightbulb goes off and people say ‚Äúoh that makes sense.‚Äù Because I am finally saying SOMETHING that aligns with their preconceived perception of what I should be doing.\nI would get those reactions no matter where I worked, but I also work at an amazing company that focuses on youth sports. And I happen to work here because I LOVE sports, especially baseball. Plenty of other engineers at my company don‚Äôt care very deeply about sports, I just happen to be one of the ones that does. But even that is something I have to prove whenever I mention it, suddenly being quizzed about minute trivia or the last time my favorite team won the World Series (2013 in case you‚Äôre wondering). This not only occurs when I am discussing my job, but also in contexts such as having internet and cable installed in my new apartment. After an 8.5 hour installation process the cable rep finally turned on the TV and asked me what channel he could check to make sure that it was all installed. I responded ‚ÄúESPN, I got the sports package‚Äù and he just turned around and stared at me like I had grown another head and said ‚ÄúYou like sports?!‚Äù I shouldn‚Äôt have to hesitate when being asked about where I went to school or what I enjoy doing in my free time, but I do. I often answer the simple question ‚Äúwhere did you attend college?‚Äù by responding simply ‚Äúin Boston.‚Äù Because having to explain more is so often a hassle. In addition to enjoying baseball, I also casually enjoy watching basketball, but you will rarely hear me admit it because I often feel like I do not know enough to answer the series of questions that follows. While my day to day interactions with friends and coworkers are positive and supportive, constantly having to explain my job or prove my knowledge to others causes me to second guess myself regularly. Yet I know I am qualified. I worked my ass off in high school, crushed the SATs, volunteered with a bunch of organizations and did the typical over-achiever list of extracurricular activities. I was qualified to get into MIT, just as much as anyone else, but that is not what people see when they see me. It is the same with my job. I am fully qualified and capable of being here. I have both Bachelors of Science and Masters of Engineering degrees in Computer Science from MIT. I had technical internships throughout college and did a 1.5 year research project/thesis that involved building software. Often times I am told I shouldn‚Äôt experience imposter syndrome because of my qualifications, but because of these perceptions and stereotypes I do continue to feel like I don‚Äôt belong. Thoughts & Conclusions If you have never had your career choice or passions questioned or don‚Äôt regularly face people assuming you are unqualified for your job, you probably can‚Äôt comprehend why the fact that someone presented ‚Äúscientific‚Äù ideas suggesting that you‚Äôre unqualified for your job hurts so much. I deal with these reminders of the uphill battle from people outside the tech industry regularly and now the fact that there is a publication within the industry that is causing more women to feel isolated, less qualified, or making them have to prove themselves even more than we already have to is not okay. Additionally, the mere suggestion that if a woman does want to succeed, she has to exhibit more ‚Äúmale‚Äù qualities is (in my opinion) ludicrous. This is the main point of contention that I have with this memo: it put into very public words and thoughts the idea that women may be less genetically qualified to be engineers. While the memo does not say that ALL women cannot be engineers, it emphasizes the opinion that typically ‚Äúfemale‚Äù traits make someone a bad engineer and that therefore the bad engineers are women and men who display more ‚Äúfemale‚Äù traits. The memo also suggests that the diversity programs are lowering the bar for hiring, so even if the author truly believes that some or most of his female colleagues are qualified for their jobs, he has now created a document that allows anyone and everyone to question ALL women and whether they belong in their position or not. I do not stop being a woman when I am doing my job. I don‚Äôt know where I fall on the continuum of traits, but based on the female qualities outlined in the memo, I am fairly solidly on the ‚Äúfemale‚Äù end of the traits spectrum. And I am damn good at my job. What is so concerning is that the memo suggests that the way to succeed in the tech industry is to display traits matching the existing status quo. Women already receive more personality-based feedback and it is worrisome to think that documents like this memo can be used to further encourage women to alter their personalities to succeed. I want women everywhere to know that is unacceptable and untrue. While I am extremely lucky to work at a company that values diversity and works to eliminate biases in the interview process, this is not necessarily true across the tech industry. It is horrible to continually be looking over your shoulder and wondering whether people think of you as a diversity hire, instead of a highly qualified worker. The fact that anyone can say ‚Äúwell maybe he has a good point‚Äù in even implying that ‚Äúfemale‚Äù qualities make someone less qualified to be an engineer confirms my greatest fears and self doubts. The reason I am hurt and angry and upset about this memo is that it means that I am still fighting to prove that I belong here. That I have to continue overcoming these invisible obstacles. For instance, if I have a dissenting opinion in a meeting, I not only have to prove my point, but also prove I‚Äôm not just being ‚Äúneurotic‚Äù AND actually have valid things to say. I would very much like to erase the misconception that women are somehow inherently or biologically less able to be engineers. Knowing that potentially there are people holding those beliefs performing code reviews, filling out peer reviews, giving me feedback, and potentially determining my career path is terrifying. Being told that I may need to alter my personality to be successful or having my errors not necessarily be attributed to the fact that everyone makes mistakes, but the fact that I am a woman makes me feel like I am walking on eggshells. I want all the engineers on my team to be evaluated as engineers, not as men or women. I honestly do believe the author of this memo has a right to question the hiring practices at his company. At GameChanger, I‚Äôve asked questions about our hiring process/interview questions and I‚Äôve spoken to a lot of people about ways we can continue to improve our culture. I do believe that people have the responsibility to ask potentially controversial questions, but I also believe it must be done in a way that does not attack or marginalize or state opinion as fact. This brings me to probably the most upsetting conclusion of the memo, Damore‚Äôs opinion that empathy makes you a bad engineer. The reason there is so much backlash is that the author did not for one second think about how this could impact others. How on earth can you build good software if you‚Äôre not empathetic to your colleagues, company, and most importantly users? You can‚Äôt! But because of this memo people all over the industry are pausing and thinking ‚Äúmaybe that is a good point‚Äù and that is what scares me. I have been in a Software Engineering role full time for 2 years and I am already tired of hearing ‚Äúdon‚Äôt let it bother you.‚Äù I‚Äôve already heard many things that I would have every right to be bothered by and they don‚Äôt bother me at this point, so the fact that this memo has been weighing on me compelled me to speak up. Being a woman in tech you have to pick your battles and this is a battle I am choosing to fight. I feel obligated to stand up for this because I am in a position where I can discuss my experiences candidly and this is a real issue facing the tech industry. I need it to be very clear that being told ‚Äúthis doesn‚Äôt relate to you, just to most women‚Äù or ‚Äújust to the average woman‚Äù still insults me as an individual. Maybe you don‚Äôt believe this about your own coworkers or friends or family. I personally have received an outpouring of support from coworkers and friends and family. But I still know that I am ‚Äúmost women‚Äù to someone out there so if you belittle any of us, you‚Äôve belittled all of us. I am hopeful now that we can shift the conversation beyond stereotypes of what a woman can and cannot do because of her gender. We instead need to focus on what individuals are qualified to do, interested in doing, and eager to take on. To make any meaningful change, we must stop perpetuating generalizing stereotypes and focus on the strengths, passions and abilities of individuals. Note: The views expressed in this post are those of the author and do not necessarily reflect the views of GameChanger. Share on Twitter Share on Facebook", "date": "2017-08-23"},
{"website": "GameChanger", "title": "You Don't Need To Guess Right", "author": ["Nick Schultz"], "link": "http://tech.gc.com/you-dont-need-to-guess-right/", "abstract": "I‚Äôve been working on a new project recently, and when I say new, I mean brand new. This isn‚Äôt just a lateral shift on the team to get my hands dirty in a new area of focus. Nor is it even adding a new product to our existing stack. This is building a brand new backend API for a brand new app. I‚Äôm really excited about being able to develop a new codebase, but the sheer number of unknowns and amount of work ahead is daunting. I wanted to share how my teammate Alex Etling and I embraced modular design to make our decision making process faster and less stressful. Let‚Äôs start this story a few weeks ago, at the inception of the new team. The purpose of the app was clear, but I‚Äôm not sure I needed two hands to count the number of decisions that had been made. In that setting, Alex and I were given the mandate to figure out what the stack looks like and start building. We are both veterans at GameChanger, but neither of us had taken on something with this combination of future scale and current uncertainty. We‚Äôve both architected solutions to a hard problems before, but when looking at a blank text editor and wondering what database should we use? , what language should we write this in? , and most terrifyingly what if we make the wrong choice? , it was hard to figure out where to start. Alex and I were getting started a few weeks ahead of the team that was going to be building the client app that will use this API. That means that we didn‚Äôt have to spin something up immediately, but we knew that in a few weeks we had to have decisions made and a basic API ready for consumption. Everyone else who was working on the project at this point were focused on high level research, not implementation details. Alex and I had to start implementing crucial parts of the stack that could have long lasting consequences if we chose the wrong solution, without the clear definition of future product requirements and direction. It was a bit stressful. So we started. Alex and I split up and did some research for a few days around things like which language, framework and database we would use to build the app. These questions took some time and research to get comfortable with a decision, but we eventually both came back together and presented our research and recommendations to each other. We settled on writing TypeScript, using a Node framework (Koa) and storing our data in Postgres. We felt good about making some early quick but well-researched decisions and we were ready to move on to the next thing. Researching the Unknown The next decision that I looked into was deciding whether an ORM should sit above our database, and if so, which ORM did we want to use? In our earlier decisions, Alex and I were able to pull from some past experience to narrow our search field and get a small set of options to really dig into. When looking into ORMs however, I didn‚Äôt have that past experience, especially not with any of the options available for our Node app. I wasn‚Äôt even able to make the first determination of do we need an ORM? quickly, because I didn‚Äôt know enough about the possible benefits and consequences of the choice. Because of my lack of knowledge, I spent several days reading blogs, books, everything I could find about using ORMs, and never really finding a consensus. Especially because there was so much that was undefined about the future of our project, I couldn‚Äôt even search out the opinions of people who built systems with similar constraints. After a few days of this I realized that I wasn‚Äôt getting anywhere with research, but we had a schedule to stick to.  We had to have something stood up for the client app team to start using sometime soon.  I was just going to have to guess and hope I made the right decision. Even when I finally made a decision, I wasn‚Äôt happy with the ORM I chose, Sequelize. While prototyping with it I realized that it didn‚Äôt quite fit what we needed. There was always a feeling in the back of my head that I was forcing something to work instead of feeling like I was gaining a lot by using Sequelize. It was the best tool out there, although the ideal solution may be building a solution that fits our specific needs. A custom tool wasn‚Äôt an option because we needed to start building a working API, not building out a TypeScript ORM. What if we got it wrong? I was getting pretty frustrated that I was not going to be able to make a well informed decision. I really didn‚Äôt want to set ourselves up for failure by locking in decisions that I had no idea were right. I‚Äôve seen projects that made decisions that seemed perfectly rational given the constraints that have gone off the rails as they evolved. I was worried because I wasn‚Äôt even at rational yet, I felt like I was just guessing. Settling on Sequelize felt like I was selling out my future self, the person who was going to have to deal with the fallout of my bad decision. We came to grips with this frustration by recognizing that we needed to focus not on are we choosing the right technology? , but instead on how can we enable ourselves to change this decision? We started talking about layering the code in our project with strong interfaces between the layers. Then, when it becomes apparent that a better choice is out there, we will be able to replace our earlier choice with the new hotness. Even the language choice is something that we can move away from if need be. Modules can be connected via an HTTP interface to allow us to move sections of responsibility to other microservices, written in other languages. That should also keep our main service small enough that a rewrite in a new language wouldn‚Äôt be untenable. This idea of modularizing the component parts of a system is in no way new. Martin Fowler espouses the exact idea that I am talking about in Is Design Dead . It is a great practice for building maintainable systems, and it doesn‚Äôt just apply to software. Real-life objects like cameras use modularity to enable photographers to use expensive equipment in more ways. The epiphany for me here was how modular design makes the decision making process less stressful. Once we realized that our decision shouldn‚Äôt have permanent consequences, saying I‚Äôm comfortable going with Sequelize becomes much easier. I knew that a future switch to another ORM would be work, but that work would be contained. How is it going? With visions of modularity dancing in our heads, we dove headlong into our next set of decisions and building out the first iteration of the API. The code that we‚Äôre writing has to work, but also has to create barriers between modules that will help us replace a module if need be. Typed languages help a great deal here, we‚Äôve been really happy with TypeScript. If you‚Äôre interested in understanding how we‚Äôve implemented these modules in practice, check out the more technical exploration of modularization (coming soon). The project is still relatively small, so we haven‚Äôt explored the microservices tactic yet, but I think we‚Äôve been relatively successful in writing our code in a way that makes it easy to replace. Overall the project is going great, we‚Äôve successfully provided a basic API to the client app team, and we‚Äôre ready to keep expanding the breadth of resources that we‚Äôre able to handle. We‚Äôre also looking forward to adding depth of responsibility to the API, adding features that turn it from an interface on top of a database to something that provides real customer value. There are lots of interesting projects in the pipeline, and we‚Äôre hiring people to come help us build them. Share on Twitter Share on Facebook", "date": "2017-05-06"},
{"website": "GameChanger", "title": "Demystifying iOS Layout", "author": ["Ami Kumar"], "link": "http://tech.gc.com/demystifying-ios-layout/", "abstract": "Some of the most difficult issues to avoid or debug when you first start building iOS applications are those dealing with view layout and content. Often, these issues happen because of misconceptions about when view updates actually occur. Understanding how and when a view updates requires a deeper understanding of the main run loop of an iOS application and how it relates to some of the methods provided by UIView . This blog post will explain these interactions, hopefully clarifying how to use use UIView ‚Äôs methods to get the behavior you want. Main run loop of an iOS app The main run loop of an iOS application is what handles all user input events and triggers the appropriate responses in your application. Any user interaction with the application is added to an event queue. The application object, shown in the diagram below, takes events off the queue and dispatches them to the other objects in the application. It essentially executes the run loop by interpreting input events from the user and calling the corresponding handlers for that input in the application‚Äôs core objects. These handlers call code written by application developers. Once these method calls return, control returns to the main run loop and the update cycle begins. The update cycle is responsible for laying out and redrawing views (described in the next section). Below is an illustration of how the application communicates with the device and processes user input. https://developer.apple.com/library/content/documentation/General/Conceptual/Devpedia-CocoaApp/MainEventLoop.html Update cycle The update cycle is the point at which control returns to the main run loop after the app finishes running all your event handling code. It‚Äôs at this point that the system begins updating layout, display, and constraints. If you request a change in a view while it is processing event handlers, the system will mark the view as needing a redraw. At the next update cycle, the system will execute all changes on these views. The lag between a user interaction and the layout update should be imperceptible to the user. iOS applications typically animate at 60 fps, meaning that one refresh cycle takes just 1/60 of a second. Because of how quickly this happens, users do not notice a lag in the UI between interacting with applications on their devices and seeing the contents and layout update. However, since there is an interval between when events are processed and when the corresponding views are redrawn, the views may not be updated in the way you want at certain points during the run loop. If you have any computations that depend on the view‚Äôs latest content or layout, you risk operating on stale information about the view. Understanding the run loop, update cycle, and certain UIView methods can help avoid or debug this class of issues. You can see in the diagram below how the update cycle occurs at the end of the run loop. Layout A view‚Äôs layout refers to its size and position on the screen. Every view has a frame that defines where it exists on the superview‚Äôs coordinate system and how large it is. UIView provides methods that let you notify the system that a view‚Äôs layout has changed as well as gives you methods you can override to define actions to take after a view‚Äôs layout has been recalculated. layoutSubviews() This UIView method handles repositioning and resizing a view and all its subviews. It gives the current view and every subview a location and size. This method is expensive because it acts on all subviews of a view and calls their corresponding layoutSubviews methods. The system calls this method whenever it needs to recalculate the frames of views, so you should override it when you want to set frames and specify positioning and sizing. However, you should never call this explicitly when your view hierarchy requires a layout refresh. Instead, there are multiple mechanisms you can use to trigger a layoutSubviews call at different points during the run loop that are much less expensive than calling layoutSubviews itself. When layoutSubviews completes, a call to viewDidLayoutSubviews is triggered in the view controller that owns the view. Since layoutSubviews is the only method that is reliably called after a view‚Äôs layout is updated, you should put any logic that depends on layout and sizing in viewDidLayoutSubviews and not in viewDidLoad or viewDidAppear . This is the only way you will avoid using stale layout and positioning variables for other computations. Automatic refresh triggers There are multiple events that automatically mark a view as having changed its layout, so that layoutSubviews will be called at the next opportunity without the developer doing this manually. Some automatic ways to signal to the system that a view‚Äôs layout has changed are: Resizing a view Adding a subview User scrolling a UIScrollView ( layoutSubviews is called on the UIScrollView and its superview) User rotating their device Updating a view‚Äôs constraints These all communicate to the system that a view‚Äôs position needs to be recalculated and will automatically lead to an eventual layoutSubviews call. However, there are ways to trigger layoutSubviews directly as well. setNeedsLayout() The least expensive way to trigger a layoutSubviews call is calling setNeedsLayout on your view. This will indicate to the system that the view‚Äôs layout needs to be recalculated. setNeedsLayout executes and returns immediately and does not actually update views before returning. Instead, the views will update on the next update cycle, when the system calls layoutSubviews on those views and triggers subsequent layoutSubviews calls on all their subviews. There should be no user impact from the delay because, even though there is an arbitrary time interval between when setNeedsLayout returns and when views are redrawn and laid out, it should never be long enough to cause any lag in the application. layoutIfNeeded() layoutIfNeeded is another method on UIView that will trigger a layoutSubviews call in the future. Instead of queueing layoutSubviews to run on the next update cycle, however, the system will call layoutSubviews immediately if the view needs a layout update. If you call layoutIfNeeded after calling setNeedsLayout or after one of the automatic refresh triggers described above, layoutSubviews will be called on the view. However, if you call layoutIfNeeded and no action has indicated to the system that the view needs to be refreshed, layoutSubviews will not be called. If you call layoutIfNeeded on a view twice during the same run loop without updating its layout in between, the second call will not trigger a layoutSubviews call. Using layoutIfNeeded , laying out and redrawing subviews will happen right away and will have completed before this method returns (except in the case where there are in flight animations), unlike setNeedsLayout . This method is useful if you need to rely on the new layout and cannot wait until views are updated on the next update cycle. However, unless this is the case, you should call setNeedsLayout instead and wait for the next update cycle so that you only update views once per run loop. This method is especially useful when animating changes to constraints. You should call layoutIfNeeded before the start of an animation block to ensure all layout updates are propagated before the start of the animation. Configure your new constraints, then inside the animation block, call layoutIfNeeded again to animate to the new state. Display A view‚Äôs display encompasses properties of the view that do not involve sizing and positioning of the view and its subviews, including color, text, images, and Core Graphics drawing. The display pass includes similar methods as the layout pass for triggering updates, both those called by the system when it has detected a change, and those we can call manually to trigger a refresh. draw(_:) The UIView draw ( drawRect in Objective-C) method acts on the view‚Äôs contents like layoutSubviews does for the view‚Äôs sizing and positioning. However, it does not trigger subsequent draw calls on its subviews. Like layoutSubviews , you should never call draw directly and instead call methods that trigger a draw call at different points during the run loop. setNeedsDisplay() This method is the display equivalent of setNeedsLayout . It sets an internal flag that there has been a content update on a view, but returns before actually redrawing the view. Then, on the next update cycle, the system goes through all views that have been marked with this flag and calls draw on them. If you only want to redraw the contents of part of a view during the next update cycle, you can call setNeedsDisplay and pass the rect within the view that needs updating. Most of the time, updating any UI components on a view will mark the view as ‚Äúdirty,‚Äù by automatically setting the internal ‚Äúcontent updated‚Äù flag, and cause the view‚Äôs contents to be redrawn at the next update cycle without requiring an explicit setNeedsDisplay call. However, if you have any property not directly tied to a UI component but that requires a view redraw on every update, you can define its didSet property observer and call setNeedsDisplay to trigger the appropriate view updates. Sometimes setting a property requires you to perform custom drawing, in which case you should override draw . In the following example, setting numberOfPoints should trigger the system to draw the view as a shape with the specified number of points. In this case, you should do your custom drawing in draw and call setNeedsDisplay in the property observer of numberOfPoints . class MyView : UIView { var numberOfPoints = 0 { didSet { setNeedsDisplay () } } override func draw ( _ rect : CGRect ) { switch numberOfPoints { case 0 : return case 1 : drawPoint ( rect ) case 2 : drawLine ( rect ) case 3 : drawTriangle ( rect ) case 4 : drawRectangle ( rect ) case 5 : drawPentagon ( rect ) default : drawEllipse ( rect ) } } } There is no display method that will trigger an immediate content update on the view, like layoutIfNeeded does with sizing and positioning. It is generally enough to wait until the next update cycle for redrawing views. Constraints There are three steps to laying out and redrawing views in Auto Layout. The first step is updating constraints, where the system calculates and sets all the required constraints on the views. Then comes the layout pass, where the layout engine calculates the frames of views and subviews and lays them out. The display pass completes the cycle and redraws views‚Äô contents if necessary by invoking their draw methods, if they have implemented any. updateConstraints() This method can be used to enable dynamically changing constraints on a view that uses Auto Layout. Like layoutSubviews() for layout and draw for content, updateConstraints() should only be overridden and never explicitly called in your code. In general, you should only implement constraints that are subject to change in updateConstraints . Static constraints should either be specified in interface builder, in the view‚Äôs initializer, or in viewDidLoad() . Generally, activating or deactivating constraints, changing a constraint‚Äôs priority or constant value, or removing a view from the view hierarchy will set an internal flag that will trigger an updateConstraints call on the next update cycle. However, there are ways to set the ‚Äúupdate constraints‚Äù flag explicitly as well, outlined below. setNeedsUpdateConstraints() Calling setNeedsUpdateConstraints() will guarantee a constraint update on the next update cycle. It triggers updateConstraints() by marking that one of the view‚Äôs constraints has been updated. This method works similarly to setNeedsDisplay() and setNeedsLayout() . updateConstraintsIfNeeded() This method is the equivalent of layoutIfNeeded , but for views that use Auto Layout. It will check the ‚Äúconstraint update‚Äù flag (which can be set automatically, by setNeedsUpdateConstraints, or by invalidateInstrinsicContentSize ). If it determines that the constraints need updating, it will trigger updateConstraints() immediately and not wait until the end of the run loop. invalidateIntrinsicContentSize() Some views that use Auto Layout have an intrinsicContentSize property, which is the natural size of the view given its contents. The intrinsicContentSize of a view is typically determined by the constraints on the elements it contains but can also be overriden to provide custom behavior. Calling invalidateIntrinsicContentSize() will set a flag indicating the view‚Äôs intrinsicContentSize is stale and needs to be recalculated at the next layout pass. How it all connects The layout, display, and constraints of views follow very similar patterns in the way they are updated and how to force updates at different points during the run loop. Each component has a method ( layoutSubviews , draw , and updateConstraints ) that actually propagates the updates, which you can override to manually manipulate views but that you should not call explicitly under any circumstance. This method is only called at the end of the run loop if the view has a flag set that tells the system some component of the view needs to be updated. There are certain actions that will automatically set this flag, but there are also methods that allow you to set it explicitly. For layout and constraint related updates, if you cannot wait until the end of the run loop for these updates (i.e. if other actions are dependent upon the view‚Äôs new layout), there are methods you can call to trigger immediate updates, granted the ‚Äúlayout updated‚Äù flag is set. Below is a chart that outlines each of these methods as it relates to each component of the UI that may need an update: Method purposes Layout Display Constraints Implement updates (override, don‚Äôt call explicitly) layoutSubviews draw updateConstraints Explicitly mark view as needing update on next update cycle setNeedsLayout setNeedsDisplay setNeedsUpdateConstraints invalidateIntrinsicContentSize Update immediately if view is marked as ‚Äúdirty‚Äù layoutIfNeeded updateConstraintsIfNeeded Actions that implicitly cause views to be updated addSubview Resizing a view setFrame that changes a view‚Äôs bounds (not just a translation) User scrolls a UIScrollView User rotates device Changes in a view‚Äôs bounds Activate/deactivate constraints Change constraint‚Äôs value or priority Remove view from view hierarchy The following chart summarizes the interaction between the update cycle and the event loop, and indicates where some of the methods explained above fall during the cycle. You can explicitly call layoutIfNeeded or updateConstraintsIfNeeded at any point in the run loop, keeping in mind that this is potentially expensive. At the end of the loop is the update cycle, which updates constraints, layout, and display if specific ‚Äúupdate constraints,‚Äù ‚Äúupdate layout,‚Äù or ‚Äúneeds display‚Äù flags are set. Once these updates are complete, the run loop restarts. https://i.stack.imgur.com/i9YuN.png This summary chart and table, and the more granular method explanations above, hopefully clarify the usage of these methods and how each relates to the main iOS run loop. Understanding these methods and how to efficiently trigger the correct updates in your views will allow you to avoid problems with stale layout or content and other unexpected behavior, and debug any issues that do occur. Share on Twitter Share on Facebook", "date": "2017-04-23"},
{"website": "GameChanger", "title": "Fixing Random Errors Caused by Importing a Swift Bridging Header", "author": ["Moshe Berman"], "link": "http://tech.gc.com/fixing-random-errors-caused-by-importing-a-swift-bridging-header/", "abstract": "We‚Äôve encountered build errors sporadically while importing our module‚Äôs Swift header into Objective-C. These mysterious errors were seemingly unrelated to the code we were working on. SESliderTableViewCell won‚Äôt build because I imported GC-Swift.h into GCAlerts.m . What on earth is going on? To make it even more confusing, we‚Äôve been able to ‚Äúfix‚Äù the error by creating a new Objective-C class in our project and importing GC-Swift.h over there. We used this technique to create shims around new Swift classes that needed to talk to older Objective-C code in our app with some success. Breaking It Down It turns out that the connection between these files and classes, although non-obvious is actually very logical. The build time errors that Xcode gives us tells us what‚Äôs failing to build, and if we look closely,  we see that as part of building GC-Swift.h , Xcode is building AppFriendsUI , which is in turn building SESlideTableViewController . Xcode is building all of these things, because, as part of importing our swift bridging header, it needs to be able to link against anything and everything that goes into the header. Anything and everything in the bridging header, as it turns out, is quite a lot. Something interesting is happening here: When we import the bridging header into non-ARC code, the imported header is treated as if it‚Äôs non-ARC code too. (This makes sense, because header imports are just replaced with code by the pre-processor at compile time.) As a result, those warnings are emitted, causing our project to fail to build. A failure in AppFriendsUI should cause GC-Swift to fail to build, but why did AppFriendsUI not compile? (Spoiler: And why should warnings cause a failure?) Since AppFriendsUI is its own Swift module, it gets its own bridging header. If we look at AppFriendsUI-Swift.h , we‚Äôll see that SESlideTableViewCell is failing to compile too. If we examine SESliderTableViewCell , we see a few errors about property attributes being missing or incorrect. Recall that property attributes are the keywords like nonatomic and strong that go in parenthesis as part of a property decleration. A warning about incorrect property attributes something that I haven‚Äôt seen in a long time, so let‚Äôs look it up online‚Ä¶ As it turns out, prior to automatic reference counting (ARC), the compiler would warn us about missing memory management property attributes. Taking Things Seriously, Sometimes‚Ä¶ You might be wondering, though: If Xcode warns about missing memory management attributes, shouldn‚Äôt our app still build? Of course,  warnings don‚Äôt usually cause the compiler to fail, but as part of our project we‚Äôve told Xcode to treat warnings as errors. So, any pre-ARC code in our project would fail to compile with missing or incorrect memory-management property attributes. So why does our app only fail some of the time after importing GC-Swift.h ? Only some of our source files are set to compile with ARC turned off. We have it enabled by default, but there‚Äôs a flag we set in our target‚Äôs build settings on a per-file basis: -fobjc-no-arc . If any of those files imported the offending code, the error would surface. Building It Back Up By now we‚Äôve seen a random compiler error, figured out what was causing it, and why it only happened sometimes. Let‚Äôs talk about a fix. In diagnosing the problem, we talked about abstracting out the code into another module, and as we discussed earlier, using a wrapper class. We could also try editing the pod generating the errors, but if we need to upgrade it, those fixes would get blown away. These approaches work around the symptoms of the problem by abstracting away build errors to somewhere else, so they don‚Äôt get in our way. It turns out that there‚Äôs a better way to fix the root cause of the problem: Upgrade to ARC as we encounter files suffering from this problem. After changing GCAlerts.m to compile with ARC, we were able to build and run our app with no problems. To summarize: We were encountering random build errors when importing our Swift bridging header into some Objective-C files. Importing a Swift bridging header into Objective-C imports all of the Swift classes and all of their dependencies. Before ARC, the compiler warned about missing memory-management property attributes. We have treat-warnings-as-errors enabled. Any non-ARC class in our project missing memory-management property attributes would fail to compile (Steps 3 & 4) One of our dependencies was missing memory-management property attributes. That dependency is getting pulled into whatever classes import the Swift bridging header. (Step 2) Some of our Objective-C files are still being compiled without ARC. Any of those non-ARC Objective-C classes that import the Swift bridging header would fail to compile. By converting non-ARC files to ARC, those warnings would no longer be emitted, nor promoted to errors. (Inverse of Step 5) The solution is therefore to convert those files to ARC. Addendum: Converting Individual Files to ARC If you search the web for ARC conversion, you‚Äôll see that there‚Äôs a tool built into Xcode to do that. We have a mixed project, though, and we don‚Äôt want to convert everything at once. So if you need to convert just one or two files to ARC in a mixed project, here‚Äôs how you do it: Click on your project in the project explorer. Click on the target that includes the files you want to convert. Click in ‚ÄúBuild Phases‚Äù Phase Click on Compile Sources and find the file you want to convert. Delete the -fno-objc-arc flag As a final step, you need to go into the file and remove any calls to retain , release , and autorelease . Share on Twitter Share on Facebook", "date": "2016-12-05"},
{"website": "GameChanger", "title": "Scaling With Kafka", "author": ["Alex Etling"], "link": "http://tech.gc.com/scaling-with-kafka/", "abstract": "At GameChanger(GC), we recently decided to use Kafka as a core piece in our new data pipeline project.  We chose Kafka for its consistency and availability, ability to provide ordered messages logs, and its impressive throughput. This is the second in a series 2 blog posts I will be doing that describe how we found the best Kafka configuration for GC. My first blog post discussed learning about Kafka through experimentation and the scientific method. In this blog post I am going to address a specific pain point I saw in our Kafka setup: adding and removing boxes from the Kafka cluster. I will start by examining the Kafka system and pain points I faced. Next I will present a series of iterations that made the scaling process easier and easier. Finally, I will show how our current setup allows for no hassle box management. To explain the problems I was facing with scaling Kafka, I need to give some background on how Kafka boxes (brokers) work. When a new box starts up, a broker.id must be specified in the server.properties file for that box. Immediately this presents a set of problems that need to be addressed.  As mentioned in some of my other blog posts , I set up our Kafka boxes using AWS‚Äô auto scaling feature. This means all of the boxes are essentially identical.  How do I specify a different broker.id for each box, if they all run the same launch config? This problem makes just launching Kafka boxes hard. The solution I developed was to create a distinct broker.id based off of the box‚Äôs IP. This guaranteed uniqueness between boxes and allowed each new box to easily get a broker.id . To do this I added a bash script to the Kafka Docker image. The script is below: FOURTHPOWER = ` echo '256^3' | bc ` THIRDPOWER = ` echo '256^2' | bc ` SECONDPOWER = ` echo '256^1' | bc ` ID = ` curl http://169.254.169.254/latest/meta-data/local-ipv4 ` FOURTHIP = ` echo $ID | cut -d '.' -f 1 ` THIRDIP = ` echo $ID | cut -d '.' -f 2 ` SECONDIP = ` echo $ID | cut -d '.' -f 3 ` FIRSTIP = ` echo $ID | cut -d '.' -f 4 ` BROKER_ID = ` expr $FOURTHIP \\* $FOURTHPOWER + $THIRDIP \\* $THIRDPOWER + $SECONDIP \\* $SECONDPOWER + $FIRSTIP ` The script gets the IP of the machine using AWS‚Äô metadata endpoint. It then constructs a 32 bit broker.id from this IP. This was a workable first step, and allowed me to launch as many new boxes as I wanted. But I still had issues. In order to explain these issues, I need describe how Kafka partitions work. To do this I need to define some terms: Broker - box with a unique broker.id Partition - smallest bucket size at which data is managed in Kafka Replication Factor - # of brokers that have a copy of a partitions data Replica Set - set of brokers that a partition is assigned to live on In Sync Replicas - brokers in the replica set whose copy of the partition‚Äôs data is up to date When a partition is created, it is assigned to a replica set of brokers.  This replica set does not change. For example, If I were to create a new partition with replication factor 3 that ended up on brokers 1, 2, and 3, the replica set be brokers 1, 2, and 3.  If broker 1 were to crash, the replica set would stay 1, 2, and 3 but the in sync replicas would shrink to 2 and 3. So why is this implementation detail a pain point for Kafka? Imagine the following scenario: (See the visual below) One of our boxes on AWS was marked for retirement and needed to be replaced with a new box in our cluster. The way we were doing box names meant we were replacing a box with broker.id X with a box with broker.id A. If broker X is replaced with A, the partition whose replica set is X,Y,Z does not change.  Its in sync replicas just goes from X,Y,Z -> Y,Z. The new broker A, does not join the replica set and does not start helping with load. In essence the new box does nothing. In order to enable A to start doing work in the cluster, I had to run a time consuming and confusing script. To combat this issue, I could have manually given the new box the same broker.id as the old box, but that defeats the idea of an autoscaler. What could I do? I needed a better algorithm to define the broker.id .  I switched form using the bash script using an IP -> broker.id mapping to using the following python script: import os from kazoo.client import KazooClient zk = KazooClient ( hosts = os . getenv ( 'ZOOKEEPER_SERVER_1' )) zk . start () zk_broker_ids = zk . get_children ( '/brokers/ids' ) set_broker_ids = set ( map ( int , zk_broker_ids )) possible_broker_ids = set ( range ( 100 )) broker_id = sorted ( possible_broker_ids - set_broker_ids )[ 0 ] print broker_id This algorithm checks with the zookeeper cluster to get all current boxes.  It then gets the lowest broker.id that is not taken by a different box. This simple algorithm change makes all the difference for ease of scaling. Lets go back to a scenario where I needed to replace a box. I had boxes 1,2,3 and killed box 2.  When the autoscaler launched a new box, the broker was assigned the lowest available broker.id (2). After startup, the new broker 2 saw that it should have had the data that the old, removed broker 2 had. It then began to sync itself into the cluster, to get full copies of all the data, and eventually became a leader of some of the partitions.  All of this happens with no manual work from developers! (See the visual below) I was getting to a place where switching out boxes and growing the cluster was becoming very easy. There was still a limiting factor though: time to sync in a node.  Going back to the scenario where there are 3 boxes with broker.id 1,2,3 and say each box had upwards of 1 TB of data on it. Box 3 now dies. It was very slow for new broker 3 to sync into the cluster and  for the cluster to get back to a completely safe state.  Could I shorten this time? The answer was obviously yes, and to do it I needed to use two different technologies: AWS‚Äô Elastic Block Store (EBS) and Docker volume mounts . EBS allows for specification of an independent storage device that can attach to an EC2 box. So if I had a Kafka box running, I could mount an EBS volume at / on that computer. I could then ensure that Kafka was writing all of its data to / using Docker volume mounts. All the data written to / will actually be written to the EBS volume. In a pinch, I could manually take that EBS volume, detach it from the box, and mount it on another machine. Suddenly, I had the ability to take all the data from a box that was shutting down and move it to a new box quickly! Through a series of steps and improvements, it is now fast, simple, and easy to scale our Kafka infrastructure. When there is so much else I have to put my attention on, scaling boxes should be kept out of site and out of mind. I recommend taking similar steps in your own cluster to make scaling Kafka easy. Share on Twitter Share on Facebook", "date": "2016-02-03"},
{"website": "GameChanger", "title": "Building a Baseball Field for Every Screen", "author": ["Eduardo Arenas"], "link": "http://tech.gc.com/building-a-baseball-field-for-every-screen/", "abstract": "The baseball field in GameChanger‚Äôs scorekeeping app is one of the core parts of our user interface. In this post I‚Äôm going to describe how it has evolved over time and how it was rebuilt to support both the new range of iOS devices that Apple has released in recent years and whatever new screen sizes might come in the future. The First Baseball Field When the GameChanger app was released for the iPhone, iOS devices only came with one screen size which made it easy to design and build our baseball field. We used a single png file as a background image and all the buttons and interactive elements sat on top of that. This image had all the complex shapes and textures needed to represent a realistic baseball field. It was the foundation of our scorekeeping UI, and every other element like the runners, defensive players, and bases were designed to match the field on that png file. This approach worked very well for a few years. New generations of iPhones continued to use the same screen size, so we were able to keep using the same image for our baseball field. Some features like spray charts (a graphical representation of the location of every hit in the game) became closely tied to the original png. If a ball was hit to second base then it would get registered as happening on the coordinate where second base was on the original png image. Original field image Adapting for the iPad and iPhone 5 After Apple released the first iPad, GameChanger made the decision to make the app universal, meaning that the same app could be installed and used on both iPhones and iPads. To support the larger screen, a second field png was created. It was larger than the iPhone version and the proportions were a little different to fit the new aspect ratio. By this point we already had hundreds of thousands of game events scored using our iPhone app, which made changing our game data‚Äôs coordinate system impractical. Instead, we used a transformation that allowed us to convert every point from the iPad field to the iPhone‚Äôs coordinate system and back. iPad field image A couple years later, in 2012, Apple introduced the iPhone 5, with a screen a little taller than the previous one. We handled this by replacing the old iPhone field asset with a new version that was a little taller as well. In the iPhone 4, the image would be clipped so that some portion of the bottom and top of the png would be hidden below the navigation and tab bars. In the iPhone 5 the whole asset would be shown, and a small vertical translation would be applied to each play that was scored so that it matched all of the existing spray chart data. 4 inch screen field image All The New Sizes In 2014 Apple announced the iPhones 6 and 6 Plus, which meant that there were two new screen sizes with different aspect ratios coming onto the market. In addition to that, rumors of side by side multitasking coming to the iPad were beginning to gain strength and were later confirmed in 2015. A few months after that, the iPad Pro with a significantly larger screen was announced. All of these announcements in combination meant that the total number of screen sizes in which an app could run grew from three to over seventeen in a very short period of time. Possible app execution sizes in current iOS devices When we started working on fully supporting the new iPhone screen sizes it became clear that our strategy of using a png asset as the background of our field and then transforming coordinates to match the original iPhone wasn‚Äôt going to work anymore. One of the main reasons was that doing so would have meant a large increase in the download size of the app. If we needed up to eighteen high quality assets for each sport (baseball and softball fields look slightly different), and some of these would need to be created in 1x, 2x and 3x sizes, the app bundle size would need to grow significantly. This is a big problem because it makes it harder and more costly for users to download our app on a cellular network. If the app happened to grow over 100 MB, people wouldn‚Äôt have been able to to download it without WiFi. We were not very far from that limit. By this point we had also realized that Apple was not sticking to less than a handful screen sizes any more, and the prospect of having to rework the field and implement a new coordinate transformation each time a new device type was released wasn‚Äôt appealing. In addition to that, the fact that even minor changes to the look and feel of our field meant that our design team would need to create around 30 different assets to support every possible screen size and resolution made it feel like continuing to use static images would incur a high amount of technical debt. We decided that the solution would be to abandon the static image strategy and make a field that could be dynamically drawn on any rectangle, independent of its size and aspect ratio. The Grid We started by defining an abstract coordinate system that would be independent of any specific screen size by dividing our field into a grid that matched our legacy field‚Äôs aspect ratio. Each square in that grid has a height and a width equal to a grid unit. We then assigned a position and a size to every element in the field using this coordinate system and grid units. Coordinates are defined as decimals so elements can be positioned inside grid units. Grid and grid unit With coordinates and locations for every element in the field, we just need to find out what the largest possible grid that we can fit on any given rectangle is. Whatever remaining space we just distribute equally horizontally or vertically depending on each case, and fill with empty space or extra grass later when drawing. Largest grid calculation Then we can use that size to calculate what the size of the grid unit is in pixels for any given rectangle, and transform every coordinate and size from the grid unit coordinate system to the coordinate system of the view that contains the field. The transformed values is what we then pass to the graphics libraries in iOS to draw each of the layers and elements. Coordinate transformation The Layers When we have a transformed value for every point and size in pixels, the actual drawing begins with the help of CAShapeLayer and UIBezierPath . These two classes together allow us to create layers that have any shape that we define. Each layer can have its own background and path, which is very useful to build the complex composition of shapes and textures the field needs. In addition to that, layers can take a CGImage as a background color and use that as a pattern, which was useful for the sand and grass patterns. The bases were drawn on a regular CALayer using images that get sized proportionally to the grid unit. Making the required modifications for the softball field was a very simple task: we just needed to remove the infield grass layer and add a layer with the circular line surrounding the pitcher‚Äôs mound. Layer composition Conclusion This project met our goals of reducing the complexity of having a field that could be properly drawn on any screen size. Having an abstract grid allowed us to define every location and dimension based on it, and then transform them to a rectangle of any aspect and pixel size, while preserving consistency with our historical spray chart data by having a transformation to our original coordinate system. Doing this also had other positive consequences. We were able to reuse the same field view that we used on scorekeeping for displaying spray charts with only a few minor aesthetic changes. Removing both the spray chart background assets and the scorekeeping field assets, and converting some of the images to pdf vectors ended up reducing the downloadable size of our app by about 32 megabytes, which means it‚Äôs less costly for our users to download our app on a cellular network. When this project began there were only rumors about a possible iPad Pro. After it was released some parts of the app needed to be adjusted for the bigger screen size, but the field required no work whatsoever so we‚Äôre already starting to see the benefits of having a baseball field that can adjust to any screen size. Whenever we have to develop new UI components in the future we will try to use the learnings from this project and use similar techniques from the start to make sure they can be easily scaled to any screen size that might come in the future. Share on Twitter Share on Facebook", "date": "2016-06-08"},
{"website": "GameChanger", "title": "Experimenting With Kafka", "author": ["Alex Etling"], "link": "http://tech.gc.com/experimenting-with-kafka/", "abstract": "At GameChanger (GC), we recently decided to use Kafka as a core piece in our new data pipeline project.  We chose Kafka for its consistency and availability, ability to provide ordered messages logs, and its impressive throughput.  This is the first in a series two blog posts I will be doing that describes how we found the best Kafka configuration for GC. In this post I will discuss how I learned as much about Kafka as fast as possible using a set of targeted experiments based off of the scientific method. To learn, I did a large variety of experiments on throughput, box scaling, and what specific configuration values meant. I will discuss three experiments in particular. What happened when I killed a Kafka box while writing messages to it? Experiments around configuration of Kafka‚Äôs topics Experiments around Kafka‚Äôs read and write throughput I will go over the setup for the experiments, why I ran them, the results of the experiments, and how the results are affecting / guiding GC moving forward. At GC we have put a lot of systems and technologies into production. We have found there are often new failure scenarios that present themselves once a system is in production under heavy load. I wanted to expose configuration and failure scenarios for Kafka before they could adversely affect our production environment and our customers. The question I was then faced with: How can I learn as much as possible as fast as possible? I decided to use the scientific method. The basic structure of the scientific method is: Form a hypothesis about an observation Figure out and perform an experiment to test the hypothesis Analyze the data and draw conclusions This method is used in the hard sciences by thousands of people each day to learn new things, but how is it useful when learning a new technology? When learning a new technology, the standard avenue for learning is reading the docs. But language is dynamic and, as I have to explain to my father sometimes, words can mean many things. What I interpret from a written description of a configuration variable might not be what the author meant for me to get out of it. The other most common ways to learn about a system are help forums, like Stack Overflow, and blog posts. Both of these sources give information on how a system or technology worked for someone else in specific scenarios.  But just because it worked one way for them, that does not mean it will work that way for you. The only way to be 100% sure about how things work is to do experiments and test your information and assumptions. That is why the scientific method is the perfect tool here. The scientific method also has some other nice consequences. First, by confirming or disproving information, the scientific method allows people to begin to build a firm foundation of knowledge to move forward with. Second, even if the documentation is written perfectly, experiments still have great value.  They serve to reinforce documentation with specific examples. These examples can then more easily be drawn on later when faced with an unexpected problem in production. So I have given ample reasoning on why I chose to use experiments to learn about Kafka, but I also want to dive into some of the actual experiments I ran. I ran a ton of experiments; everything from attempting to delete a topic when delete is enabled, to changing whether or not topic rebalancing will happen. Each new unknown was approached with the scientific method in mind. Although I ran many experiments, I want to discuss three experiments specifically. (For reference I will be mentioning terms and language used on this page ) Experiment 1: What happened when I killed a Kafka box while writing messages to it? Why is it important? The first and most basic experiment I want to talk about is testing what happened if a box was killed while writing messages to it. There are a few reasons I chose this experiment. First, the experiment is very simple and tests a fundamental part of the system. This makes it easier to apply the information to future scenarios. Second, boxes will fail so we must know how the system will behave when that happens. Experiment setup 3 Kafka Boxes 1 Kafka Topic Kafka Topic had 1 partition with replication factor 3 This means 1 box was the leader or owner of data written Box A wrote to the Kafka cluster Box B read from the Kafka cluster As Box A was writing to the Kafka cluster, I killed the box who owned the topics only partition between rights. Hypothesis I expected that Kafka would handle this killing gracefully and no data would be lost.  That means that data that was accepted as written by Box A, would successfully be read by Box B. Results The results were a little surprising, but generally confirmed the hypothesis. Initially Kafka did not accept any writes to the cluster. It returned an error to Box A: NotLeaderForPartitionException . Quickly thereafter, Kafka started accepting writes again. Throughout the process, B read all acknowledged writes by A and did not read any unacknowledged writes. How we use this knowledge moving forward After this experiment, I had more confidence in Kafka, had learned a new failure case, and had gained a better foundation to approach more complicated experiments. Experiment 2: Testing Kafka‚Äôs Topic characteristics Why is it important? When reading over Kafka‚Äôs documentation, I stumbled over the config variables min.insync.replicas and request.required.acks . These variables enable specification of the number of boxes that must receive and acknowledge a write before that write to the cluster succeeds.  I was hopeful that this would provide a lot of power and flexibility. If there were topics that needed more security/consistency I could require higher numbers of acknowledgments. And if there were topics where I wanted to favor availability over consistency, I could configure them that way. The problem was that I did not have a great feel for how this would work, so I set up some experiments. Experiment setup 3 Kafka Boxes 1 Kafka Topic Kafka Topic had 1 partition with replication factor 3 Box A wrote to the Kafka cluster Box B read from the Kafka cluster I did a series of tests where I tried different write acknowledgment values for the partition As Box A was writing to the Kafka cluster, I killed the box Hypotheses and Results A scaled down version of the tests I performed and their outcomes can be seen in the table below. Box Acknowledgments Hypothesis Results 1 Writes should continue to work Writes continued to work 2 Writes should continue to work Writes continued to work 3 Writes should continue to work Writes did not work. When attempting to write, Box A received an error message: [KafkaEvent,0] failed due to Number of insync replicas for partition [KafkaEvent,0] is [3], below required minimum [2] How we use this knowledge moving forward Using the results of this experiment I learned that both too many and too few required acknowledgments made the system more brittle (a single box dying could cause data or availability loss). This led us to configure all of our current topics to require the majority of boxes to acknowledge writes before accepting ( acknowledgments = (number of boxes / 2) + 1 or written another way N = 2F + 1 ). Experiment 3: Determining our Kafka cluster‚Äôs read and write throughput Why is it important? It has been reported in other blog posts that Kafka is able to handle 50 MB‚Äôs of data per second under heavy load. I wanted to see if I could reproduce these results on our Kafka setup. I wanted to determine whether our boxes had optimal network, disk, and memory configurations. Experiment setup 3 Kafka Boxes 1 Kafka Topic Kafka Topic had 1 partition with replication factor 3 Topic had write acknowledgment of 2, as discussed in experiment 2 Built 2 new Docker images that used the producing and consuming load test scripts that came with Kafka On a new box, I used the Docker images to run the scripts against our Kafka cluster Hypothesis I assumed that our read and write throughput would be around the 50 MB/s reported in the other blog posts. Results Our cluster was able to handle about 5 MB/s, which was much less than I had expected. How we use this knowledge moving forward The current set up was not optimal and needed to be tweaked for better performance. This information was critical in order to make sure our Kafka cluster was ready for our traffic peaks. If I had just relied on the data from blog posts and outside sources, it could have come back to bite us while under heavy load. The results also pointed to the need for a new series of experiments in order to optimize the Kafka cluster‚Äôs throughput. The three experiments I discuss above gave invaluable information about how we need to configure our Kafka cluster. Using the scientific method helped me learn as quickly as possible about Kafka. It allowed me to expose what I did not know and learn before the system failed in production . This is a great way to get to know a new system and helped me quickly ramp up knowledge. The scientific method was very valuable and I highly recommend it for anyone attempting to learn a new system, technology, or language. Share on Twitter Share on Facebook", "date": "2016-01-27"},
{"website": "GameChanger", "title": "Creating a Slack Reaction Bot", "author": ["Brian Bernberg"], "link": "http://tech.gc.com/creating-a-slack-reaction-bot/", "abstract": "Here at GameChanger, we periodically send out surveys to our customers as a way of gauging how we‚Äôre doing and to calculate our Net Promoter Score (NPS) .  At the end of the day, our number one goal is to please our customers and measuring our NPS is a useful metric to ensure we‚Äôre working toward that goal. We use Delighted to orchestrate our NPS surveys.  Many companies send these surveys out to their customers: After assigning a numeric grade, the user also has the opportunity to add comments to their survey response: Of course, comments only benefit us if we actually see them and they aren‚Äôt sent off to a deep, dark database that no one checks!  Thankfully, Delighted provides a useful service that automatically posts NPS feedback to a Slack channel.  We have configured Delighted to automatically post a Slack message to a channel whenever an NPS survey is received with comments: When we send a survey through Delighted, we add user properties (sport, team adminisrator/fan, free/premium, etc.) so that we can determine how we‚Äôre doing with different segments of our user base.  Delighted can easily calculate an NPS result for only users with specific properties. We started to realize that seeing NPS comments in Slack was great but understanding context around the user who commented would be even more useful.  Undertanding that Delighted already had this information in the form of user properties, I spent a hack day creating a service to automatically show user properties in Slack using reactions . Reactions are the perfect way to accomplish this because they are able to communicate a lot of information in a small space. Slack and Delighted both have API‚Äôs to enable this.  The first step was to set-up a small Python/Django app on Heroku to make and process all of the various API calls.  The next step was figuring out how to know when Delighted posted an NPS message to our Slack channel.  Slack offers an Outgoing Webhook service which performs a POST to a URL whenever a new message is posted in a specified channel ‚Äì exactly what I needed! The next step was to figure out if the Slack message was from Delighted (since any Slack user can post to the channel) and if so, the e-mail of the user who sent the comment.  The user e-mail is sent in a message attachment from Delighted but unfortunately (though understandably), Slack does not include message attachments in the webhook POST.  Slack messages are uniquely identified by channel name and timestamp (both of which are included in the webhook POST).  I was unable to find a Slack endpoint to retrieve a specific message but I was able to work around this using the channels.history method, which does return message attachments: def get_message ( channel , timestamp ): results = requests . post ( SLACK_HISTORY_URL , data = { 'token' : SLACK_TOKEN , 'channel' : channel , 'latest' : timestamp , 'oldest' : timestamp , 'inclusive' : 1 }). json () messages = results . get ( 'messages' ) if len ( messages ) > 0 : return messages [ 0 ] The next step was parsing the message attachment for the user‚Äôs e-mail (using a regular expression).  If one existed, this e-mail was then used to retrieve the user properties from Delighted using the survey responses endpoint. The final step was translating these user properties to reactions and adding the appropriate reactions to the Delighted message via the reactions.add method.  I added a new emoji for each user property so that hovering over the reaction indicates which user property the reaction represents: All of this was accomplished in less than 150 lines of Python code thanks to the useful API‚Äôs Slack and Delighted provide.  We are gaining valuable insight about our customers and hopefully we‚Äôll improve our Net Promoter Scores as a result. Share on Twitter Share on Facebook", "date": "2016-02-08"},
{"website": "GameChanger", "title": "Fun With Kafka: Adding a New Box Type Part 2", "author": ["Alex Etling"], "link": "http://tech.gc.com/adding-a-new-box-type-fun-with-kafka-2/", "abstract": "At GameChanger (GC), we recently decided to use Kafka as a core piece in our new data pipeline project.  In order to take advantage of new technologies like Kafka, GC needs a clean and simple way to add new box types to our infrastructure. This is the second in a series of 2 blog posts I will be doing that explore this concept. This blog post is a direct follow up to my first blog post which covered the 5 steps for adding a new box type to GC‚Äôs infrastructure. This post will cover the specifics of how I added a Kafka cluster to our production infrastructure. At GC, our Kafka cluster consists of 3 box types: Kafka boxes, Zookeeper boxes, and SchemaRegistry boxes. Below you can see the basic setup of our Kafka cluster. Each box has a different job to make the cluster work. Zookeeper boxes are used by both the Kafka boxes and the SchemaRegistery boxes to access and store cluster membership. For example, if a new Kafka box comes online, it registers itself with the Zookeeper cluster and gets information on what Kafka boxes are currently alive. The next time a different Kafka box accesses a Zookeeper box, it will discover a new box has joined the cluster. The old box will get the IP address and the port of the new box from the Zookeeper cluster, allowing it to talk to the new box. SchemaRegistry boxes are a Confluent system used for migrating, validating, and updating schemas for your Kafka topics. Each time one our apps wants to send a message to Kafka, it first makes sure to register a schema with the Schema Registry. This registration step allows GC to make sure the data being sent is of the right format and any changes to the schema are backwards compatible. The final boxes, as you might expect, are the Kafka boxes. Kafka is a distributed, partitioned, replicated commit log. Kafka can act as a queue, provides pub sub functionality, and under certain circumstances allows for ordering of messages sent to it. All of these properties make it very useful for our data pipeline. To get the Kafka cluster up and running, all three boxes needed their own new box type. I will cover the specific setups for all three. 1. Create a new AWS launch config The first step when I added the new boxes was trivial. All three launch configurations were set up using AWS Console. I went into AWS Console Clicked EC2 Clicked Launch Configurations Clicked Create launch configuration From here, I specified GC‚Äôs standard AMI and a simple box type. The real configuration and setup happened in the final 4 steps. 2. Create a launch script that sets up a Docker box.yml As mentioned in my first post, box.ymls allow specification of app specific configurations.  Kafka, Zookeeper, and SchemaRegistry all needed to set specific ports and environment variables to run properly. Some of the most important environment variables were the urls for the Zookeeper boxes. These urls were needed by Kafka and SchemaRegistry to keep track of which boxes were in the cluster. The box.ymls also needed to expose ports outside of the Docker containers. These ports aligned with the ports specified in the docs for each system (9092 and 9997 for Kafka. 2181, 2888, and 3888 for Zookeeper. 8081 for SchemaRegisry). The box.ymls for all three systems can be seen below. Kafka: kafka : elb : kafka-production-vpc image : <gc_docker_registry>/kafka containers : kafka : ports : 9092 : 9092 9997 : 9997 env : JMX_PORT : 9997 JVMFLAGS : -Xmx4g ZOOKEEPER_SERVER_1 : <zookeeper_box1_ip>:2181 ZOOKEEPER_SERVER_2 : <zookeeper_box2_ip>:2181 ZOOKEEPER_SERVER_3 : <zookeeper_box3_ip>:2181 LOG_DIRS : /var/kafka/data CREATE_TOPICS : \" false\" Zookeeper: zookeeper : image : <gc_docker_registry>/zookeeper containers : zookeeper : volumes : /var/zookeeper : /var/zookeeper ports : 2181 : 2181 2888 : 2888 3888 : 3888 env : JVMFLAGS : -Xmx4g ZOO_LOG_DIR : /var/log/zookeeper ZOOKEEPER_SERVER_1 : <zookeeper_box1_ip>:2888:3888 ZOOKEEPER_SERVER_2 : <zookeeper_box2_ip>:2888:3888 ZOOKEEPER_SERVER_3 : <zookeeper_box3_ip>:2888:3888 SchemaRegistry schema-registry : elb : schema-registry-production-vpc image : <gc_docker_registry>/schema-registry containers : schema-registry : ports : 8081 : 8081 env : JVMFLAGS : -Xmx4g ZOOKEEPER_SERVER_1 : <zookeeper_box1_ip>:2181 ZOOKEEPER_SERVER_2 : <zookeeper_box2_ip>:2181 ZOOKEEPER_SERVER_3 : <zookeeper_box3_ip>:2181 COMPATIBILITY_LEVEL : backward 3. Create an image that will be the base of a Docker container Docker provides a simple way to specify a set of steps needed to create an image; a Dockerfile. Kafka, Zookeeper, and SchemaRegistry all had Dockerfiles with very similar structures. First, the Dockerfiles needed to load an application specific configuration file (ASCF). The ASCFs were used to specify what settings the box would run with. As an example, the Kafka ASCF described the broker.id for the box, where on the file system to save topic data, and the default replication factor. Second, environment variables that were specified in the box.yml, needed to be set in the ASCF file. For instance, all of the Zookeeper urls specified in the box.ymls were written into their respective ASCFs. Finally, the process binary was run with ASCF as an argument. The ASCF for all three of the new box types can be seen below. Kafka broker.id = BROKER_ID\nzookeeper.connect = ZOOKEEPER_CONNECT\nlog.dirs = LOG_DIRS\ndelete.topic.enable = true default.replication.factor = 3\nlog.cleaner.enable = true log.retention.hours = 2160\nnum.partitions = 64\nauto.create.topics.enable = CREATE_TOPICS Zookeeper: tickTime = 2000 initLimit = 10 syncLimit = 5 dataDir = /var/zookeeper clientPort = 2181 SchemaRegistry kafkastore.connection.url = KAFKASTORE_URL\navro.compatibility.level = COMPATIBILITY_LEVEL debug = true 4. Scale new boxes with an autoscaling group Much like step 1, step 4 was done through the AWS console. I went into AWS Console Clicked EC2 Clicked Auto Scaling Groups Clicked Create Auto Scaling group Based my auto scaling group off of the Launch Configuration from step 1 Once my new auto scaling groups were set up: Selected my autoscaling group from the list of groups Clicked Edit Changed the value in Desired to my desired number of boxes (3 for all three box types) Clicked Save After about 15 minutes, I had my new box types! 5. Set up security groups The final step, setting up security groups (SGs), ended up being one of the most challenging parts of the whole setup. This was due both to my own inexperience, and some nonintuitive SG rules that I had to learn. Initially I set up rules that let the boxes talk to each other on the ports exposed in the box.yml. After I first set up these SG rules, I was getting into strange split brain scenarios within the Kafka cluster. I could not figure out why this was happening until I learned that two boxes of the same SG (say Kafka) cannot inherently talk to each other. I needed to set up a security group rule to allow this communication. After I learned this, it was easy to get the isolated cluster up and communicating.  I was still missing one key piece though. I needed to set up rules so that the application boxes could talk to Kafka. This enabled me to send messages from our web and api boxes to Kafka. This allowed me to actually use the new Kafka cluster! Overall, the process to setup the Kafka, Zookeeper, and SchemaRegistry boxes took a few hours for each box type. It took a total of a day and a half to get a simple Kafka cluster up and running. A lot of that time was spent in each system‚Äôs docs, not on getting the boxes themselves setup.  The process was quick and easy because of the simplicity of the GC production infrastructure. This infrastructure has allowed us to add new systems as needed, upgrade existing systems, and experiment with new technologies that we believe may be valuable, like Kafka. Share on Twitter Share on Facebook", "date": "2016-01-20"},
{"website": "GameChanger", "title": "Fun With Kafka: Adding a New Box Type Part 1", "author": ["Alex Etling"], "link": "http://tech.gc.com/adding-a-new-box-type-fun-with-kafka-1/", "abstract": "At GameChanger (GC), we recently decided to use Kafka as a core piece in our new data pipeline project.  In order to take advantage of new technologies like Kafka, GC needs a clean and simple way to add new box types to our infrastructure. This is the first in a series of 2 blog posts I will be doing that explore this concept. (See Tom Leach‚Äôs post on how we switched our infrastructure to use docker for a more in depth background.) In this post I will be covering the 5 steps needed to add a new box type to production at GC. Create a new AWS launch config Create a launch script that sets up a docker box.yml Create an image that will be the base of a docker container Scale new boxes with an autoscaling group Set up security groups This post is done with an eye on getting a basic Kafka cluster set up in production, which will be discussed in the next post. Along the way I will give some insights into our infrastructure and our use of both AWS and Docker. 1. Create a new AWS launch config At GC we rely heavily on AWS‚Äô auto scaling feature. Therefore when you are creating a new box type, the first thing you need to create is a new Launch Configuration . Launch configs allow you to specify three things critical to GC‚Äôs box scaling: Amazon Machine Image (AMI), box type, and a script the box runs at startup (User Data). In our infrastructure we use a standard AMI for all of our boxes and choose a simple starting box for every new system (m3.large). The real meat of how GC scales boxes is in how we use the launch script. 2. Create a launch script that sets up a docker box.yml The launch script is important for a few reasons. First, it sets up application specific data. Ie. the script on a web box sets up web specific info, the script on an api box sets up api specific info, etc‚Ä¶ This script also needs to set up how containers should run on each box type. This is needed because most GC processes run in docker containers. Using this script, we wanted an easy way to specify both app specific and container specific configs.  We settled on a format based off of docker compose‚Äôs docker-compose.yml file. The YML config looks like the following:\n(One quick note: we set this up pre EC2 Container Service (ECS), so our decisions might look different if we were doing this project today.) services : - service_name_1 - service_name_2 apps : app_name : depends : - other_app_name elb : name_of_elastic_load_balancer image : image_to_base_container_off_of containers : container_name : restart_on_failure : False use_host_hostname : True_or_False ports : - ports_to_forward logs : - location_of_log_files volumes : - volumes_to_expose envs : - environment_variables_to_set command : optional_command_to_run_in_container This config is very similar to the docker compose config, which brings up the question: Why did we not just use docker compose (DC) in production?  One of the biggest reasons came from DC‚Äôs own docs: Compose is great for development, testing, and staging environments, as well as CI workflows. This statement strongly implies that DC should not be used in production. On top of this, there were also configuration needs that DC did not provide. One of the most important features that DC did not provide was our need for a good way to specify Elastic Load Balancers (ELB). Knowing each box‚Äôs ELB allows us to deregister a box from its ELB before we deploy containers. It also allows us to add it back once a container is deployed. With this ability, we can ensure that no traffic will be routed to our boxes while containers are down or starting up. This allows us to maintain uptime while deploying. We enable this with the elb attribute. At GC, we have a standard set of services that run on all of our boxes (rsyslog and cloudwatch statsd). These services, when specified, run with uniform configs. We wanted to cut down on copy and paste and the need to remember these configs. Using the services key, you can specify which of our services should be running on the box. The docker launch script will then take care of launching and configuring each one. As at most companies, logs at GC are very valuable. Logs provide a wealth of information, so it is important to have them somewhere they are easily searchable. We use rsyslog to take all logs on a box and send them to our log tracking service.  In order to do this, rsyslog looks in a special directory on the box /special/logs/directory .  This means that all process logs should be forwarded to this location. For the sake of simplicity, accuracy, and future proofing, we did not want to specify a docker volume mount in each box.yml. Instead, we use the logs key to specify where the application‚Äôs logs are located in the container. The docker launch script will then ensure that the application‚Äôs log directories are forwarded to the proper host directory. On top of those already mentioned, there are some other minor differences that contribute to why our yaml config is different than DC‚Äôs. These are mainly to facilitate convenience, speed, and ease when creating and launching new box types. As an example of an actual config, our web box.yml can be seen below: services : - rsyslog - cloudwatch-statsd apps : mongos : image : <gc_docker_registry>/mongos containers : mongos : use_host_hostname : True env : gcenv : prod ports : <port1> : <port1> logs : mongos : - /container_location/mongodb gcweb : depends : - mongos elb : web-production-vpc image : <gc_docker_registry>/gcweb containers : gcweb : ports : <port2> : <port2> env : gcenv : prod logs : gcweb : - /container_location/gcweb 3. Create an image that will be the base of a docker container Once you have configured your box.yml, you still need a docker image for the container to run off of. Luckily, docker makes it really easy to build new images using Dockerfiles. At GC, we make heavy use of Dockerfiles: we store all of our Dockerfiles in their respective app repos and store built and saved images in a private docker registry. We are also continually updating our docker images. A new image is built and pushed to our registry each time we commit and deploy a repo. Boxes then pull new docker images from our registry at container launch time. So when creating a new box type, all you need to do is create a new Dockerfile, use it to create an image, and push that image to our private registry. You now know about our standard box config and our process for managing docker images, but you may still be wondering how containers are actually launched on our boxes.  If docker compose is not doing it, what is reading the box.ymls and launching our containers? Since we have a custom box.yml, we made a custom python script (using docker-py ) to launch our containers. This script is put in a recipe and is run by chef (on box launch) and serf (on deploy) on all boxes. A pseudocode version of the script can be seen below: def get_apps_in_order ( box_config ): for app_name , app_config in box_config . get ( 'apps' ). iteritems (): dag . add_node ( app_name , app_config , depends_on = app_config [ 'depends' ]) return dag . topological_sort () def docker_deploy_app ( app_config ): deregister_box_from_elb ( app_config ) stop_existing_containers () start_new_containers ( app_config ) register_box_with_elb ( app_config ) def deploy_now (): box_config = read_box_yaml () ordered_deploying_apps = get_apps_in_order ( box_config ) for app_config in ordered_deploying_apps : get_lock_on_app () docker_deploy_app ( app_config ) release_lock_on_app () ensure_services () The script follows a simple structure: Get a list of all apps. The apps are sorted by dependency. So if app1 depends on app2, app1 is later in the list. Grab the lock on the app. (We use locks to ensure only one box is deployed at a time) Deploy Containers: Remove the box from the ELB Deploy the new docker containers Place the box back in the ELB Ensure that requested services are running 4. Scale new boxes with an autoscaling group Now that our boxes can launch their containers, we have just two steps left. The first is creating a new AWS Auto Scaling Group (ASG). Creating ASGs is done through the AWS Management Console , and I will go over the specifics in my next blog post. ASGs are very useful. They allow you to specify a set of scaling policies on when to launch and shutdown boxes. At GC, we use CPU, memory, and custom cloudwatch metrics as triggers to automatically scale up and down our boxes. These scaling features are nice for production ready apps, but are not needed when initially setting up an ASG. Instead you can use ASGs to manually tell AWS to scale up boxes with your new box type. 5. Set up security groups The final step to adding a new box type is setting up security groups . Security groups control IP and port access rights between boxes in your cluster and between boxes and the internet. They can be used to help organize your system and help isolate pieces to limit vulnerabilities. Like ASGs, security groups should be set up using the AWS management console. I will cover the specifics next time. Thats it! Thats all there is to adding a new box type at GC. As you can probably tell from the post, steps 1, 4, and 5 use very standard AWS features. The real meat of adding a new box type is steps 2 and 3. These focus on specifying which containers should be running on our boxes, and what should be running in each container. Along with our custom python script, these steps are the backbone of launching boxes using docker in our infrastructure. We have continually optimized these steps to make adding new boxes and services easy. I will discuss the specifics of these 5 steps when adding our new Kafka cluster in my next post. Share on Twitter Share on Facebook", "date": "2016-01-13"},
{"website": "GameChanger", "title": "From Intern to Employee", "author": ["Ami Kumar"], "link": "http://tech.gc.com/from-intern-to-employee/", "abstract": "At GameChanger, I have had the relatively unique opportunity to experience the company as both an intern and a full-time employee. I was a computer science major in college and interned at GC for two months the summer after my junior year. During my senior year, I made the decision to come back to GC after graduation, and have been working here full-time since July. Working full-time at GC has allowed me to re-evaluate last year‚Äôs internship and has given me new insights on interning in general. I chose to intern over the summer before my senior year to get real-world experience with the concepts I had been learning in classes for the past three years. I viewed it as an opportunity to supplement what I had been learning in class, and was not thinking long-term, past graduation, as few college students do. However, internships can serve as an extended evaluation period for both the employer and employee. Just as employers can use that time to decide whether you are a good fit for the company, interns can use the time to evaluate whether they see themselves there full-time. While interviews give a small and sometimes inaccurate glimpse of a company, an internship makes it possible to actually experience company culture and visualize yourself as a full-time employee. Spending two months working at a company allows you to make a more informed and confident decision about a job offer than the impression you get from an on-site interview. Choosing the right company for an internship is a difficult and critical decision. Since internships can turn into full time offers, especially during the summer before your senior year, it is important to consider companies you can see yourself at long term. To find the right company, you need to make sure you are taking full advantage of your college‚Äôs available resources to search for jobs and internships. Most college‚Äôs have career centers that post job listings and calendars of upcoming events, like career fairs. These fairs are a great way of exposing yourself to many potential jobs at once and getting a feel of a company‚Äôs environment. I found GameChanger at my college‚Äôs career fair, where I had the chance to ask questions and actually see the company‚Äôs product. I got a sense of the collaborative culture and real world applications of the product and was able to picture myself working here. I was enticed by being able to work first-hand with the product as an intern and make changes that would actually be implemented in production. Many companies I looked at and companies my friends interned at did a great job at mentoring and teaching, but never actually used interns‚Äô code. Location plays a huge factor as well. Relocating to a place you can stand for at most a summer puts you in a difficult position when weighing a full-time offer from the company, especially if you enjoyed your experience. I knew I could see myself in New York City long term, and that location would not be a deciding factor when weighing a full-time offer from GC. Interning at GC made it obvious the company excels in mentorship and highly prioritizes improvement and growth. The internship exposed the opportunities to learn at GC and contribute more to the team than just code. Since joining full-time, I have had the opportunity to participate in design decisions and sprints, be a part of the product team‚Äôs decision-making process, and work on a wide range of projects in various parts of GC‚Äôs system. Since internships are a great way to assess whether a company is right for a full-time position, it is important to prioritize getting accustomed to company flow as well as learning new computer science concepts. I had the opportunity to work closely with a mentor and my team during my internship, which gave me access to learning about professional life outside of pure programming. Of course, my responsibilities are not the same as they were last summer. As expected, I am more accountable for the work I do as a full-time employee. That can mean checking up on errors caused by changes I make or staying at work a little longer to meet a deadline. I have been given more responsibility more quickly since I joined the team full-time. Starting as a full-time employee did not feel like picking up where I left off in my internship, mostly because of the increased scope of what I now work on. One of the biggest changes is that I was added to the on-call schedule after becoming a full-time employee. Each engineering team has a different schedule, where the engineer currently on call is expected to respond to pages about critical system issues relating to his or her team‚Äôs scope at any time of day. Paging frequency is not high enough to interfere with daily life. However, dealing with pages has exposed me to various parts of our code base and deepened my understanding of how the system works as a whole. Addressing these critical issues and customer support cases has allowed me to touch parts of the code I otherwise would not have seen and given me a greater understanding of our product. I have also had a chance to be a part of the hiring process this year. I have had the opportunity to interview a candidate and help with a new hire‚Äôs orientation process. This allows me to meet new employees more quickly and have a say in the company‚Äôs hiring plan. Two months is a short time to get to know a company and find out how you want to grow at the business. As a full-time employee, I can set longer term goals and cater my employment experience towards where I want to be over the next years. My internship at GameChanger gave me a taste of what full-time employment would be like at the company and helped inform my decision of where I wanted to work after college. I am grateful I interned at a place I could see myself working full time. Share on Twitter Share on Facebook", "date": "2015-12-05"},
{"website": "GameChanger", "title": "Making Music With Baseball Data", "author": ["Philip Del Vecchio"], "link": "http://tech.gc.com/making-music-with-baseball-data/", "abstract": "What does a baseball game sound like? You‚Äôre probably imagining the crack of a bat, the cheers of the crowd, the shouts of a traveling beer salesman, and the reverberations of a morale-boosting walk-up song through the stadium. But what does the game itself sound like? Does the combination of throws, hits, and catches that make up a baseball game have a rhythm? A melody? Inspired by James Murphy (best known for his musical project LCD Soundsystem) and IBM‚Äôs project using Wimbledon tennis data, I spent our last hack day trying to programmatically create music using only data from games scored using GameChanger. Computers have been used to make music since the early 1950s, when mathematician Geoff Hill programmed the CSIRAC to play simple melodies. Computers are now ubiquitous in the music making process, used in composition, recording, synthesis, and mixing. The MUSIC I program, developed by Max Mathews at Bell Laboratories in 1957, was the first to incorporate algorithmic composition, extending the use of computers in music beyond simple playback. Since then, experimentation in algorithmic composition has included machine learning, fractals, and genetic algorithms, serving as inspiration for musicians and debate on the nature of art. The simplest process to generate sound from a game‚Äôs play-by-play data is with 1-1 mapping of events to sounds generated by a synthesizer. Each event (a ball, a strike, a hit, etc.) is translated into a pre-generated sound, triggered sequentially at a fixed tempo (in this case, 300 beats per minute). I generated the sounds using Ableton Live‚Äôs Operator, an analog modeling synthesizer. </source> Your browser does not support the audio element. The result, as you may have expected, is robotic and lifeless. Let‚Äôs try introducing context. If we differentiate between a single, double, and triple, between strikes one, two, and three, between a fly-out and a ground-out, etc., we begin to hear the beginnings of more complex musical patterns. To avoid complete cacophony, I quantized the generated pitches to notes in a major scale. Each type of event still has its own sound, but it is modulated based on the context in which it occurred. </source> Your browser does not support the audio element. But there‚Äôs more to a baseball game than the plays that occur. The players, locations, and fans contribute just as much to the feeling of a game as the in-game events themselves. Let‚Äôs try using some of the game‚Äôs metadata to seed some variation in the tempo, pitch, and dynamics (loudness) of sounds. </source> Your browser does not support the audio element. Aleatoric music is music dating back to the late 15th century but popularized by avant-garde composers in the 20th century including John Cage and Charles Ives. Aleatoric refers to the use of chance in some element of the music, whether in the composition or performance of the piece. In our case, the randomness comes not from the roll of a die or a pseudorandom number generator, but from the ‚Äúrandom‚Äù events of a baseball game. For each game, however, the music is deterministic - running the algorithm on a given game will produce the same sounds every time. What we have now is a series of notes constituting a melody and a sort of asynchronous harmony that comes from the quantization to a set key. We also have tone, determined by the original synthesizer sounds, tempo from the play loop, and dynamics from our earlier seeding. The only remaining element of traditional (i.e., European) music is rhythm. In its current state, our song lacks a time signature and a real sense of rhythm beyond the continuous beating of the sounds. Let‚Äôs add some drums from an 808 . </source> Your browser does not support the audio element. It‚Äôs no Daft Punk, but what we have now is a dancefloor-ready hit written (accidentally) by the players on a baseball field. This post was written by Philip Del Vecchio who interned with GameChanger in the Summer of 2015. Philip goes to Penn and is currently in his Senior year. Share on Twitter Share on Facebook", "date": "2015-08-06"},
{"website": "GameChanger", "title": "Experiment First, Build Later", "author": ["Eduardo Arenas"], "link": "http://tech.gc.com/experiments-service/", "abstract": "Experiments on the fan conversion team At the beginning of this year a small team at GameChanger was created with the mission to evaluate opportunities for optimizing our conversion rates for premium fans of teams, in anticipation of the 2015 amateur baseball and softball season, which peaks every year around April and May. We knew there were plenty of opportunities to explore, given that fan conversion had never been a top priority and there was a general belief that many aspects of our conversion flow could be vastly improved. In fact, many people from every area in the company started volunteering ideas, and we got to a point were he had a really long list to start working with. With no shortage of ideas and potential opportunities, and the fact that we only had about two months time to execute some of them, the biggest question we needed to answer was how could we select the ideas with the most impact on our fan conversion rates that had the lowest time cost? Timing was critical because after two months the season would be well underway and the peak would have passed. After that point, every project that made it out into production would have diminishing returns, so we really needed to work with that window of opportunity. Under these constraints, we decided that our best approach would be to take all ideas and formulate them as hypotheses we could test. Then we would sort them taking into account their potential impact if they were successful and how long it would take us to implement them. Once we had a prioritized list of ideas, we would treat each one of them as an experiment where we would test the hypotheses with as little implementation as possible. The idea was to make functioning prototypes that we could use in production for result collection, without taking into account aspects like code quality and maintainability. In general most of our experiments took around one day to set up, and then we would let them run for about a week before we reverted the changes and started analyzing results. Only when an idea was validated and we knew that there was a very high chance that implementing it would meet our expected goals, would we start working on a real, permanent implementation. For our first experiments we used Kissmetric‚Äôs AB testing library . We already used Kissmetrics for analytics data collection and reporting, so setting it up would be relatively easy and quick. The library‚Äôs functionality is pretty simple: you give it a name that identifies the experiment and a list of options and it will return a randomly assigned group from the list. var group = KM . ab ( \" experiment_name \" , [ \" control \" , \" treatment \" ]); Once a user is assigned to a group, the library will always respond with the same group the user was initially assigned to. This is important because if users are placed in a different group each time, it would be impossible to conclude if a behavior was caused by exposition to one of the specific versions or the control. After we divided our users into groups, we just needed to select what the success criteria for the experiment was (usually an event we were already tracking, like premium subscription purchases), and Kissmetrics would do the tracking and the reporting, letting us known when we had reached our desired sample size and if there was a statistically significant improvement of one of the treatments relative to the control. Kissmetrics AB Test Report Limitations We were able to launch a couple experiments using this, but we quickly noticed that Kissmetrics‚Äô library wouldn‚Äôt work for most of our experiments. While the reporting was working very well and we were already tracking our success events on their site, the AB testing library had a couple of big issues. For one thing, we found out that the service was quite slow. For the first experiment we had a one second timeout, after which the service would return the control group, as a default fallback. It was timing out almost half of the times it was called. We increased the timeout to three seconds, which can present a very noticeable delay for users, and the calls to the service seemed to timeout about ten percent of the time, which was better than before, but still too high. Another big problem was that Kissmetrics‚Äô AB testing functionality is currently only available through the front end JavaScript library. This made it hard to run experiments directly from our back end which is mostly written in Python. We needed this for some experiments that didn‚Äôt directly involve the website, like changes in email campaigns. These two problems made us realize that even with the very short time we had, it would be worth it to invest in building our own service that we could use for assigning users in groups for each experiment. GC‚Äôs Experiments Service The problem itself was very easy to solve: if the user is not yet on the experiment, randomly assign the user to one of the group options, otherwise respond with the group the user is already assigned to. A very simple web service with some basic storage would do the trick. Thanks to GameChanger‚Äôs amazing micro service architecture , we were able to build and release this to production in one day. The web service is written on Node.js and it‚Äôs backed by a DynamoDB database hosted on Amazon. It has a REST API that can be consumed from any part of our stack. Given that most of our code is written in Python, we also wrote a Python client for calling the service. The interface is somewhat similar to Kissmetrics‚Äô: provide an experiment name, a list of group options, and an identifier for the user and it will return a group for that user in the experiment. The groups will be uploaded automatically to Kissmetrics by the client so we can still use their analysis and reporting tools. group = get_experiment_group .( 'user_id' , 'experiment_name' , [ 'control' , 'treatment' ]) The fact that this service was hosted in the same virtual cloud as the rest of our stack, and that it was written with performance in mind meant that we are consistently getting responses in under 50 milliseconds. The traffic pattern for this service is very hard to predict. At any point in time the service could sit completely unused with zero traffic if there are no experiments running, or it could have very high traffic if, for example, a very popular feature is being experimented on. For this reason it relies heavily on Amazon‚Äôs EC2 autoscaling, and it is designed to fail gracefully. When anything goes wrong, the user won‚Äôt notice and will simply be assigned to a designated fallback group. Sample case: redesigned checkout page Why is all of this important? Going back to the beginning, we wanted to be sure that our assumptions were correct before actually dedicating engineering resources to developing any of the ideas we had on the board. If one of them didn‚Äôt provide a really meaningful conversion lift during testing, it was discarded and we could move on to the next one. One experiment we were glad we tested before building was our checkout redesign. The hypothesis was that the checkout page was cluttered with information and provided many ways out, and this was hurting conversion. It had testimonials, a premium features list, a satisfaction guarantee and more. Some of these pieces of information had links to other pages on our website, which meant that checking out for a premium subscription was not an enclosed experience. This is generally agreed to be undesirable for a checkout page. The team came up with a much cleaner and, in my opinion, better looking design. Everyone on the team though that this experiment would be a huge success, but as it turned out, our customers disagreed. After a couple weeks of running the experiment and three different iterations on the design, the results showed that the treatment group performed consistently worse than the control. People converted about 10% better on our old checkout page. Current checkout page New design proposed for the checkout page Conclusion This example was eye opening. It showed us the importance of testing the things we want to build before committing them. It‚Äôs important to get as much information as possible early in the process so time isn‚Äôt wasted building features and making changes that customers won‚Äôt value. We also had many successful experiments that were built after the experiments presented successful results, like making paywalls and the checkout page optimized for mobile devices. Even in those cases, we realized that performing a test before actually building the features was very useful to estimate the quantitative impact that a change could have on our goals. This approach was very successful for us. The experiments service is used regularly by teams at GameChanger as a tool to help us understand our users and the impact that each new feature has on their behaviors, and ultimately, our goals. Share on Twitter Share on Facebook", "date": "2015-07-24"},
{"website": "GameChanger", "title": "Automating a Dusty Workflow with Watchdog", "author": ["Nick Schultz"], "link": "http://tech.gc.com/dusty-workflow-automation/", "abstract": "If you haven‚Äôt heard, some of my colleagues here at GameChanger released a pretty awesome project called Dusty for running Docker-based local development environments.  I‚Äôve been using Dusty for a few weeks now, feeling out the best ways to do the things I do most often. I work a lot with the front end of our website, so I change a lot of HTML, CSS and JS, as well as messing with the Python views that serve all of that content. My Development Workflow Let‚Äôs talk about what my workflow was when I ran our app without Docker and without any VMs.  We run our main website on Django, so when I changed Python code, I‚Äôd have to restart the Django process.  If I changed HTML, I didn‚Äôt have to do anything, Django would start serving out the new HTML immediately.  We write CoffeeScript and LESS here at GameChanger, so if I changed those types of file, I‚Äôd need to re-compile those files to get them to take effect on my local copy of the site.  In our old setup we ran a Grunt process that watched for changes to the source files and then did that compilation.  So, all in all, the only thing that I needed to do on a regular basis was to bounce the Django process, and even that I could have automated away if I really needed to. Dusty Workflow Now on to Dusty.  In my Dusty dev environment our Django app runs in a Docker container.  That‚Äôs the same Docker container that we deploy to our production boxes, with a few hooks to do some dev specific things.  First, we are able to update the container‚Äôs file system with the code we want it to run locally with dusty sync .  Second, we‚Äôre able to run scripts against the container using dusty scripts .  The particular script I‚Äôm talking about is our script to run the Grunt job to compile our LESS and CoffeeScript.  Finally, I can bounce my Django process by running dusty restart .  Now that I have the elements of my Dusty workflow, let‚Äôs talk about how it works out in practice. My initial Dusty workflow required a lot of manual interaction.  My Python changes workflow is just about the same because a dusty restart does a sync beforehand.  The simplest thing from my original workflow, the change to an HTML template, now requires me to do a dusty sync to get the new HTML to the container, instead of taking effect immediately.  My CoffeeScript/LESS workflow is also slower than it was before.  We don‚Äôt want to run the ‚Äúwatcher‚Äù process inside the container, because we wouldn‚Äôt want to run the process in our production environment, so I have to run my dusty scripts command on every change I want to take effect.  Also, just like with the rest of the code, I need to sync my changes before my dusty scripts call will do anything. So, right off the bat, I‚Äôm seeing that its taking more of my time to do the same things I always did just because I‚Äôm using Dusty.  That‚Äôs not good.  So what do I do? I do what any engineer should, I automate those manual tasks.  This is probably something you‚Äôve heard over and over, but it‚Äôs absolutely true.  Always optimize the things you do the most, and when possible, automate them.  If you save 6 seconds on something you do 100 times a day, you just got 10 extra minutes.  Additionally, if you can automate the things that you‚Äôre doing manually now, you don‚Äôt spend the time thinking ‚Äúwhy isn‚Äôt this working‚Äù when you‚Äôve forgotten to do the manual step. Automation Travis (one of the creators of Dusty) introduced me to this nice Python package and command line tool called watchdog . Watchdog watches for filesystem events and lets you define some jobs to run based on those events.  I took a look at the documentation and examples and wrote the command to handle my CoffeeScript/LESS workflow needs.  It looked like this: watchmedo shell-command \\ --patterns = \"*.less;*.coffee\" \\ --recursive \\ --drop \\ --command = 'dusty sync gcweb; dusty scripts gcweb grunt' \\ /gc/gcweb Now, that‚Äôs not the ugliest thing I‚Äôve ever seen, nor is it the prettiest.  But there are two big problems.  First, I want to be able to share this with my teammates here at GameChanger and telling them to paste a complex command in their terminal is not too helpful.  Second, this only solves one part of my three part workflow.  So to fully solve my problem, I‚Äôll need to run at least two more of these commands. Luckily, watchdog can run commands from files rather than command line args. They can read in what they call ‚Äútricks‚Äù files which are yaml files that provide similar information to what is in the command above.  This solves my two problems with my first command.  I can commit these files to the repository that they are watching, which distributes them to my team.  Also you can put multiple commands into one file, which makes for far less overhead when running all of the commands.  So let‚Äôs look at what that looks like: # dusty-autosync.yaml tricks : - watchdog.tricks.ShellCommandTrick : patterns : [ \" *.less\" , \" *.coffee\" ] shell_command : ' dusty sync gcweb; dusty scripts gcweb grunt' drop_during_process : true - watchdog.tricks.ShellCommandTrick : patterns : [ \" *.html\" ] shell_command : ' dusty sync gcweb' drop_during_process : true - watchdog.tricks.ShellCommandTrick : patterns : [ \" *.py\" ] shell_command : ' dusty restart gcweb' drop_during_process : true Here you can see that when I change LESS or CoffeeScript files I sync and run my Grunt script, when I change HTML files I sync my code over, and when I change some Python code, I restart my app.  Now all I have to do is run watchmedo tricks-from dusty-autosync.yaml when I start working and when I make changes to my code my Dusty container is updated appropriately to reflect them on my local copy of the site.  I omitted part of the shell_command that is in the actual file above to be concise, but I also use the terminal-notifier command line tool to pop up a OS X notification when each of these commands finish so I never need to think ‚Äúis it done yet?‚Äù. Doing this small amount of automation has made Dusty a much more viable tool for me to use.  If you‚Äôve been using Dusty and have run into some similar annoyances, check out watchdog.  If you use Docker and haven‚Äôt used Dusty yet, check it out . Since I automated my workflow I‚Äôve noticed myself waiting on the Grunt script to finish more often than I‚Äôd like, so now I‚Äôm going to try to shave a few seconds off of it. Share on Twitter Share on Facebook", "date": "2015-07-14"},
{"website": "GameChanger", "title": "Why We Wrote Dusty", "author": ["Travis Thieman"], "link": "http://tech.gc.com/why-we-wrote-dusty/", "abstract": "Today we‚Äôre announcing Dusty , a new tool for running Docker-powered development environments on OS X. Here, I‚Äôll talk about why we needed Docker for development and how Dusty helps address some shortcomings in the other tools available. Building Services with Docker We use Docker to deploy all of our services in production. The simplicity of the Docker build process allows us to scale our engineering organization by largely removing operational concerns from our application developers. If a team can ship a functional Docker image, we can run it in production. The simple deployment model of Docker gave us the freedom to aggressively split out our monolith into many services. Typically, we‚Äôd deploy a new service on a dedicated autoscaling group and put it behind a dedicated load balancer. This one-to-one relationship of infrastructure to services was simple and worked well in production . However, this is the opposite of a typical development environment, where every service runs on a single machine. Our production tooling didn‚Äôt fit our local use case, so we were stuck with our Vagrant -based development environments after our initial move to Docker in production. Managing our VM-based environments became more difficult as we added more services. The flexibility we enjoyed with Docker in production was hard to emulate locally on a single VM, as having multiple isolated instances of a language runtime or database installed in a single VM was not easy. With no standardized way to enable or disable services, our VMs ran every service we had and got bloated. Rolling out changes to the structure of an existing service would break everyone‚Äôs environments until they applied the necessary changes. After dealing with this for a few months, we accepted that our VM-based system was untenable and that we needed to use Docker locally in order to accurately reflect what we were running in production. We were hopeful that, with the right tooling, we could come up with something that brought the benefits of Docker to our development use case. Making Docker ‚ÄúJust Work‚Äù on OS X We quickly hit a snag with the strategy of utilizing Docker for our development environments. At GameChanger, we‚Äôve created the most popular amateur sports app in the App Store . Naturally, our developers and designers all use OS X, which can‚Äôt run Docker natively. To address this, tools like boot2docker and the newer Docker Machine can create a lightweight Linux VM for running the Docker daemon. This gave us a way to use Docker on OS X, but the VM layer came with significant downsides. Previously simple tasks like accessing a service‚Äôs exposed ports or sharing files between your host filesystem and the containers were now complex. We needed to make working with Docker easy if we had any hope of selling our developers on moving from their current setups to a Docker-based solution. This is when we first realized we needed to create Dusty. These usability problems are outside the scope of the existing tools, so we needed an application sitting on top of them to glue together a working solution and provide a user-friendly interface. We do a few things with Dusty to take away the pain of Docker on OS X. Automated Port Forwarding : Dusty integrates with nginx and your hosts file to traverse the network stack from OS X, through the Docker VM, and into your running container. All the user sees is a host and port accessible from their Mac which behaves the way they expect. Fast File Syncing : Dusty uses rsync for fast file syncing between your local filesystem and the VM filesystem accessible by the containers. This replaces VirtualBox shared folders, which are incredibly slow . File Management : Dusty provides a CLI command to copy files between your OS X filesystem and running containers. These features go a long way towards making Docker pleasant to use on OS X. We can comfortably run applications inside of containers now, but there‚Äôs still more to do. We don‚Äôt just need to run applications, we need to actively develop them! Developing with Dusty Once we had Docker running well on OS X, we needed to move past just running our applications and start supporting a true development workflow. Our starting point for this was Docker Compose (n√©e Fig ), which bills itself as the preferred way to run development environments with Docker. Compose gave us a good way to configure and run large, pre-defined sets of containers, but this only partially solved our problem. We still didn‚Äôt have a way to easily override code running inside these containers. If we hard-coded these overrides into the Compose specs, we forced every developer to check out the source for every application and keep it updated. With Compose, we couldn‚Äôt run isolated tests on our new code without writing separate Compose specs. We wanted a tool to save us from the additional cruft necessary to make Compose really work for active development. We needed our tools to minimize this friction around changing code for running services. As our stack grew to include more services, we also realized we needed to make it easy for developers to run services they needed but were not actively developing. For example, our iOS app API consists of four services right now. If I‚Äôm only changing code in one of them, my tools should take care of running the other three without placing a cognitive load on me. Dusty is built on top of Compose and adds functionality to enable an efficient development workflow. Repository Management : By default, Dusty manages all required repositories for you and ensures they stay up to date. You can override any individual repository if you want to modify its code on the fly. Dusty takes care of mounting the correct version into the running containers. Dependency Resolution : Dusty knows how to stitch together your containers to come up with a running environment. When you need to restart containers, Dusty restarts them in the correct order to pick up your changes and not break things. Library Development : Dusty lets you declare development library dependencies on running applications. You can make changes to your library and Dusty will handle restarting any dependent applications so they pick up your changes. Standardized Testing : Dusty lets you store test commands in the Dusty specification for an application or library, then runs those tests in a separate container. This lets you share tests easily across a development team and reduces friction when moving between different language ecosystems. Tests can also run against their own isolated database containers. Dusty provides a powerful and flexible way of running development environments that meet the needs of a developer working on a large, service-oriented architecture. Wrap-Up With Dusty, we‚Äôre trying to make Docker a viable solution for development environments on OS X. We‚Äôve made great progress with the general usability issues of having to run Docker through a VM, and the development workflow of Dusty manages away the complexity of changing code inside running containers. We‚Äôre excited to keep improving Dusty and make it hugely useful to ourselves and the community. Dusty is available now for OS X. If you‚Äôd like to try it out, you can install Dusty and play around with our Getting Started docs, which include links to functioning example specs. If you‚Äôd like to hack on Dusty yourself, the project is open source and made available under the MIT license. Share on Twitter Share on Facebook", "date": "2015-07-07"},
{"website": "GameChanger", "title": "These Six Shocking Facts About Meetings Will Change Your Life", "author": ["Kiril Savino"], "link": "http://tech.gc.com/these-six-shocking-facts-about-meetings-will-change-your-life/", "abstract": "This post originated from an email I sent to the company. It‚Äôs posted below verbatim as the team received it. Do you like my headline? If it made you read this I won! :) Meetings are critical to our operations, but very easily become a paralyzing force on the very productivity that is the life blood of this company. I write some variation of this email every year or so, and I got triggered today! Kiril‚Äôs rules for meetings Avoid them can you walk over and talk to someone rather than calling a meeting? are you just making an announcement that can be done over Slack or Email? Make them short you almost never need an hour: 15 minute meetings often suffice Run them well have a specific goal for every meeting prepare an agenda stick to your timeframe and your topics and get out of the room send out notes at the end (or post them somewhere like Trello or Slack) Invite fewer people justify to yourself every additional person can someone ‚Äúcritical‚Äù just send notes? can you have a busy person represented by a delegate? Don‚Äôt use them to make decisions use them to gather feedback, ideas, or status across a team use them to generate alignment (a shared perspective on something) AVOID THE AFTERNOONS I care deeply that we try hard to preserve the afternoons for productive individual or group work . That means not scheduling meetings in the post-lunch-to-5pm-ish timeframe, and REALLY avoiding 2-5pm. NOTE: there‚Äôs a big difference between a meeting that‚Äôs entirely within a team, during that timeframe, if that team needs to congregate to move forward, and a meeting that spans teams. The former is often useful and makes sense, the latter is much more destructive. FAQ Q: What if I can only assemble the full group during productive hours? A: This is usually a lie. Make sure the absolute core can make it, and have others send notes or a delegate and receive notes at the end . Q: What if I don‚Äôt know the agenda in advance? A: If you‚Äôre calling a group into a meetings, try really hard to know what you‚Äôre going to talk about first. The worst thing ever is 10 people (20% of our company) in a room accomplishing nothing for an hour. The best companies have strong culture around this . Q: But isn‚Äôt a meeting here & there in the afternoon ok? A: We have 50 people here. If everyone here scheduled 1 meeting per month in the afternoon, that‚Äôs more than 1 per day. See Paul Graham‚Äôs post on how destructive that pattern is to individual contribution (his post is about engineering, but I think it applies to anyone who has meaty, difficult work to do). Q: How are decisions made if not in meetings? A: The person directly responsible for a mission or a functional area makes final decisions, after consultation with everyone necessary . Decisions made by committee just generally suck, and it‚Äôs worth looking at things like the RA(S)CI model to understand who has what role in a project. Meetings are a great way to disseminate a decision and get everyone aligned about it, though. Q: What if I really need all these people, and I have no other choice but schedule during maker time? A: OK, so this happens. If you‚Äôre stuck with a bad time, consider if you can put it off by a few days and find a better time (many things aren‚Äôt as urgent as they seem). And if you have to schedule it: make it as short and clear as possible try to book time at the very beginning or very end of the day to reduce the amount of impact Q: What if my team is blocked and needs a meeting to move forward? A: If a given team needs to convene, without inviting people from other teams, then they should do it, for as long as necessary, whenever necessary. HOWEVER: the best way to operate is to plan for that in advance, use stand-ups to surface blocking issues early in the day and address them before lunch, and think through your plan such that you‚Äôre less likely to hit massive roadblock/bubble. And if your ‚Äúit‚Äôs just my team‚Äù meetings starts slowly pulling in other people because ‚Äúwell, we need so-and-so because they have an opinion‚Äù then you‚Äôre in the business of killing everyone else‚Äôs productivity again. If you think you would do things differently, I‚Äôm all ears - as long as we are pushing toward preserving productivity and meaningful results as an organization, I can (and regularly do) change my dogma. :) Love, Kiril\nCP(roductivity)O Share on Twitter Share on Facebook", "date": "2015-06-24"},
{"website": "GameChanger", "title": "WWDC 2015 Recommendations", "author": ["Ben Yelsey"], "link": "http://tech.gc.com/wwdc-2015-recommendations/", "abstract": "I was thrilled that GameChanger sent me to WWDC 2015 (my first!). I kept track of my favorite presentations with the WWDC app, and I‚Äôve gotten recommendations for a bunch of other talks. My team requested a list of talks that I enjoyed, and I figured I‚Äôd share it with the world. Without further ado, here‚Äôs Ben‚Äôs List of WWDC 2015 Recs: Talks I enjoyed at WWDC 2015 Advanced NSOperations I didn‚Äôt know too much about NSOperation before this talk. Afterward I looked over some of our code that uses AFNetworking and felt that I understood it much better Best Practices for Progress Reporting I watched enough of this to find it well structured and informative. Would finish watching if I needed to use NSProgress for something Building Better Apps with Value Types in Swift Contains some good info, but I don‚Äôt endorse the commentary on older FP languages. I also wish they‚Äôd offered better solutions for managing intermingled value and reference types (I‚Äôd rather avoid sorcery like isUniquelyReferencedNonObjC ) Building Responsive and Efficient Apps with GCD A good technical discussion of an interesting topic. I came out of this with a better grasp of the QoS hierarchy and common GCD usage pitfalls. Can anyone tell me why GCD still has just the C API? Designing for Apple Watch I think I learned something just from the design of the talk itself. I also came out of it much more bullish on the potential of the watch Implementing UI Designs in Interface Builder This is the STACK ALL THE VIEWS!!! talk. I really want to start using stack views ASAP Introducing The New System Fonts Explaining the guiding principles behind Apple‚Äôs new San Francisco font. This might not be as interesting if you have a serious background in font design, but for me it was very educational Mysteries of Auto Layout, Part 1 Frankly I‚Äôm still mystified Mysteries of Auto Layout, Part 2 Less mystified now Optimizing Swift Performance Just throw a final on it Swift and Objective-C Interoperability A lot of good info about the updates to Objective-C. Better late than never. The error handling interop is pretty darn cool Swift in Practice Helpful outline of the new iOS versioning options, Unicorns, and some Swift evangelism that I‚Äôm on the fence about UI Testing in XCode UI Recording is the XCode7 feature I‚Äôm most excited about. This talk gives a nice overview, and describes some of the less obvious aspects of the new element query DSL Your App and Next Generation Networks Skip to 15 min for a great discussion on the UX of using the network and reducing latency at the TCP level Talks I plan to watch online when I get the chance I couldn‚Äôt physically watch ALL of the talks on location, but I got some great recommendations from friends and coworkers that I intend to follow up on. Some of these also just have catchy titles :) Introducing Search APIs Networking with NSURLSession Performance on iOS and watchOS I‚Äôm told I should skip to 15min Protocol-Oriented Programming in Swift Security And Your Apps What‚Äôs New In Core Data What‚Äôs New in Storyboards Share on Twitter Share on Facebook", "date": "2015-06-19"},
{"website": "GameChanger", "title": "What We Value in Engineers", "author": ["Tom Leach"], "link": "http://tech.gc.com/what-we-value-in-engineers/", "abstract": "One of the huge strengths of GameChanger‚Äôs open culture of feedback is that we‚Äôre relentless about continuously improving the way we do pretty much everything. Over time, we‚Äôve evolved a strong collective understanding of what it means to be a ‚Äúgood engineer‚Äù. Until now this wisdom has really only existed in people‚Äôs heads and has only been expressed verbally, an approach which doesn‚Äôt scale particularly well. So, today we‚Äôre launching What Good Engineers Do , a manifesto of sorts, an explicit declaration of what characteristics and practices we as a company value in engineers. What this manifesto is not is an arbitrary list of programming languages, tools, methodologies or achievements we believe you must have mastered to become a good engineer. We believe that smart people with the right qualities are far more valuable than experienced people with the wrong qualities. At GameChanger we plan to start using ‚ÄúWhat Good Engineers Do‚Äù as a set of success criteria against which to measure individual progress and set goals for improvement. Can you think of any qualities of good engineers which you think we‚Äôve missed? Share on Twitter Share on Facebook", "date": "2015-06-17"},
{"website": "GameChanger", "title": "Building Things Quickly With Flask", "author": ["Kristian Kristensen"], "link": "http://tech.gc.com/building-things-quickly-with-flask/", "abstract": "Last week GameChanger Engineer Erik Taubeneck presented at the Flask NYC meetup on how to build things quickly with Flask. The setup for the talk was: Flask is an amazing tool for getting a Python web project up and moving fast. Join active Flask user Erik Taubeneck for a presentation of best practices, hacks, and tools for Flask app development. Topics include the app factory pattern, context processors, configuration libraries, and how to build reusable Flask extensions. Erik went through a number of items relating to building Flask apps including: App factory pattern: which helps with testing Using Celery: which can be a bit tricky with the app factory pattern Context Processors: which reduces the need to push everything into your templates Configuration: how to use Erik‚Äôs open source project Ordbok to help configure your apps with less pain Custom Extensions: building your own extensions like Ordbok Slides from the talk are available here . Ordbok: Configuring Flask Apps Ordbok abstracts the loading of a configuration from YAML files into a Python dictionary, and also has a specific setup for use with Flask. Github: Ordbok Share on Twitter Share on Facebook", "date": "2015-06-17"},
{"website": "GameChanger", "title": "GameChanger Hosts the NY Sports Tech Meetup", "author": ["Jenny Trumbull"], "link": "http://tech.gc.com/nyst-meetup/", "abstract": "GameChanger recently had the pleasure of hosting the New York Sports Tech meetup . Jeremy Levine, founder and CEO of the fantasy gaming app Draft took the stage to introduce his product to attendees. The event had a great turnout, and it‚Äôs always fun to see both new and familiar faces representing the NYC sports tech community. If you‚Äôre interested in using GameChanger‚Äôs space to host a tech meetup, please send me (Jenny!) an email at jenny@gc.com . Share on Twitter Share on Facebook", "date": "2015-06-16"},
{"website": "GameChanger", "title": "Data Interruption Process", "author": ["Kristian Kristensen"], "link": "http://tech.gc.com/data-interruption-process/", "abstract": "We‚Äôre building out our data and analytics capabilities at GameChanger. This means that we‚Äôre facing conflicting priorities on the type of work that needs to get done. On the one hand the team needs time to focus on delivering the new products and projects that will help us tomorrow. However, we also need to make sure that the rest of the company gets the information they need while we build out. You can sum up these two constraints as: Don‚Äôt break the business Protect the team so they can make forward progress In this post we‚Äôll explain what we‚Äôve put into place in the form of process to accomplish these two goals. The Process The main process vehicle we deployed is called the Data Interruption Process . The purpose of the process is to field any incoming requests from other teams and the business and shelter the team from continual interruption. As in software, we want to limit the expensive effects of context switching. The members of the team rotate weekly as the current person on Interruption duty. Before we get into the actual work and process for the person on Interruption (who we refer to as the Interrupter), let‚Äôs take a look at the other side and see how requests get submitted. A Request We use Trello to plan and track work at GameChanger . So for this it was obvious that we needed a Trello board to hold requests. An example of this board is seen below: From the board it‚Äôs easy to get started. You can also see who‚Äôs currently On Deck, which helps to know where to direct questions. When someone has a request for the team they go to the board and copy the request template. It contains a set of questions to answer to help qualify the nature of the request. Hypothesis What are you trying to figure out? Action What will you do once you know the answer to this? Data Population - such as objects (games, teams, users, etc.) and what filters Measurements - which metrics are needed Segmentation - for example sport and age Due Date When do you need this by. Not required. By having this template in the card we‚Äôre trying to minimize the amount of roundtrips and back and forth trying to clarify what‚Äôs actually needed. We‚Äôre also trying to nudge team members towards filing requests with an actionable outcome. This is mainly to avoid the more research type questions. We also encourage requesters to plan out their requests in advance so we can avoid the ‚ÄúOMG I need this yesterday everything is on fire‚Äù -type of requests. It‚Äôs important to manage the expectations and for that reason we have a stated Service Level Agreement (SLA) of filed requests of 24 hours. This means that you can expect a response to your request within 24 hours. This does not mean that the request will be filled and completed in a day, but rather that you‚Äôll get a response on whether it can be accepted and completed and when. The Work of the Interrupter The successful Interrupter does three things: Manages requester expectations Triages daily Delivers on accepted requests This leads us to the daily routine of the person on Interruption. At least once every day before standup the Interrupter will triage incoming requests. There are four outcomes of this triage: No: Won‚Äôt do, is not appropriate, not possible, etc. Moves request to the ‚ÄúNo‚Äù lane. Need More Info: Not enough information to accept request. Interrupter will work with the requester to clarify. Goes to the ‚ÄúNeed More Info‚Äù -lane. Accepted: Request accepted and the Interrupter will work on it during the week. The Interrupter communicates with the requester about when they can expect the completed card. Goes to the ‚ÄúAccepted‚Äù -lane. Move to Product Planning: Request is deemed too big to be fulfilled through the Interruption Process and will be included in the regular Product Planning process. Moved to Product board. Once the work has been completed the card will be moved to the ‚ÄúDone‚Äù -lane and later archived. The Interrupter updates the card with the results, including details about what was done to obtain the results. Examples of this could be CSV files, SQL queries, etc. That way we keep a record of what was asked and how it was filled. We can later refer back to this if we need to run a request again, or if we want to dig into how we obtained a result. Wrap up The Data Interruption Process fits a pattern that‚Äôs useful outside of data requests. It‚Äôs basically the same pattern we employ when setting up On Call rotations and having a designated person for external contact. Keeping the balance between supporting and delivering to external stakeholders, while at the same time having enough uninterrupted time to get forward moving work done is key to running an effective team. We‚Äôve had great success in employing the Data Interruption Process, and would recommend it as a pattern to others. Share on Twitter Share on Facebook", "date": "2015-05-19"},
{"website": "GameChanger", "title": "Bayesian Statistics and Multi-armed Bandits", "author": ["Erik Taubeneck"], "link": "http://tech.gc.com/bayesian-statistics-and-multi-armed-bandits/", "abstract": "A few weeks ago, during a GameChanger Hack Day , my fellow data engineer Zach Markin and I prototyped a multi-armed bandit micro service. As a motivating example, consider the problem of deciding between these three buttons on some page of a website: There are many ways to tackle this problem: Pick what you think looks best. Pick what you think will perform best. Run a test to statistically measure which performs best, and pick it. If the thing being chosen (a ‚ÄúSign up today‚Äù button in our example) is tied to an important part of your site (signups usually are,) then it is likely worth testing. Traditionally, a test is run by splitting people into test groups (and a control group if there is an existing choice,) running the test until a certain number of people have seen all of the test options, and then making a decision based off a statistical hypothesis test . A multi-armed bandit algorithm works a bit differently. Instead of having a step where the test is running and a step where we make a decision, a multi-armed bandit is always testing and always choosing. The better a choice performs, the more often it is displayed. For example, if green gets a higher click rate, it gets displayed more. The most important advantage here is that the multi-armed bandit starts to favor the higher performing option quickly so that money isn‚Äôt lost on a poorly performing option for the entire duration of a test. To calculate just how often each color should be displayed, I wanted to find a distribution describing the probability of each color performing best. I took a Bayesian approach, based off Sergey Feldman‚Äôs and Evan Miller‚Äôs excellent blog posts on the subject. To see what I ended up doing, skip on down to the solution , but first, I‚Äôll briefly speak about why I used a Bayesian model. Why Bayes? Recently at GameChanger, we‚Äôve increased our focus on measuring the impact of our product and design changes on our customers‚Äô experiences. We‚Äôve been using KissMetrics, and specifically their A-B testing tool which uses a normal approximation to calculate certainty of results. This is an efficient approximation of the Binomial distribution , which describes the probability of seeing k successes from n trials. In our case, a user landing on a page with a test is a trial; a user performing some desired interaction on the page is a success. The certainty displayed, however, comes with a few caveats. Statistical Significance and Hypothesis Testing As mentioned above, hypothesis testing is the traditional methodology for running a statistical test. This approach is often called the frequentist approach , and it assumes that there is some true value for the variable being measured which we are attempting to discover through our test. In our case, we measure the success rates for each of the colors of the button. We test each option, and assuming their rates are different, we can measure the probability that their true values are actually different or if the difference observed is the result random chance. This requires us to choose a somewhat arbitrary level of significance (typically ‚Äú95% confidence‚Äù) before we run our tests that will determine if the results are significant or not. We also have to be aware of the risk of committing type I and type II errors. (A type I error is incorrectly rejecting a true null hypothesis, and a type II error is the failure to correctly reject a false null hypothesis. Wikipedia expands much more on this.) This complexity is not a flaw in a frequentist approach , in fact it is part of its power. A statistical significance test typically ends with a statement that some initial hypothesis is either true or false (or inconclusive) with a given probability of being correct. This complexity, however, does not easily translate to appropriate business decisions. But there is another approach. Bayesian Testing xkcd.com/1132 Bayesian statistics is built from a different view of the world altogether. The effect that we are attempting to measure (still a success rate) does not in fact have a true value but instead a probability distribution that describes its potential values. A Bayesian statistician makes use of a prior distribution from which they have reason to believe represents the distribution of the value being measured, and then updates that from the data observed. The choice of the prior is the black art of Bayesian statistics, much like the arbitrary level of significance chosen in significance testing. However, it is easy to choose a prior which will be overwhelmed by our experimental data. The difference here is a bit subtle, particularly if you don‚Äôt spend your days heads down in a statistics text book. (Something I‚Äôve been guilty of.) A hypothesis is designed from the start to reach a decision at a certain level of significance. For example, in Experimental Physics, discoveries are typically required to show statistical significance of 5 sigma , or 99.99997% certainty (on two different devices) to be declared ‚ÄúTrue‚Äù. In our example, we‚Äôd likely choose 95% certainty, run the test, and decide that the blue button (for example) is the best and move on (given the caveats discussed above). The Bayesian approach does not provide such a statement of truth. Instead, it provides a probability distribution that describes the value we are measuring. In our example, we might be able to make the statement that ‚Äúthere is an 88% chance that blue is best‚Äù or ‚Äúthere is 65% chance that blue is at least 10% better than red and green‚Äù. Additionally, the distribution is important for the multi-armed bandit algorithm; I was looking to find the individual probabilities for each color that it performs best, given the data observed. Generally, the Bayesian approach is much easier to understand and communicate. Feldman‚Äôs post mentioned earlier provides a helpful example: Which of these two statements is more appealing: (1) ‚ÄúWe rejected the null hypothesis that A=B with a p-value of 0.043.‚Äù (2) ‚ÄúThere is an 85% chance that A has a 5% lift over B .‚Äù Bayesian modeling can answer questions like (2) directly. From my experience, I tend to agree with Feldman‚Äôs analysis that questions like this second one are much easier to explain and to convert into specific and practical business advice. The hypothesis test gives a ‚Äúdecision‚Äù with reasonably difficult assumptions baked in whereas the Bayesian analysis gives useful information which can inform a decision with very few assumptions baked in. Now onto how I went about finding this probability distribution. The Solution I started with Miller‚Äôs post (mentioned earlier), which amazingly has a closed form solution for a Bayesian A/B Test. The issue, however, is that I was aiming for an A/B/C/‚Ä¶ test which has an arbitrary number of options. Simply formulating this isn‚Äôt an easy task, and I‚Äôm fairly convinced that a closed form solution to this does not exist. (I‚Äôd love to be proven wrong! If you‚Äôve solved this or have read a paper that does, please shoot me an email [and come work with us! ]) Luckily, Bayesian models can easily be simulated, and in fact that is the approach taken in Feldman‚Äôs post . In Python (with numpy installed) we can write a function to compare two tests: from numpy import mean from numpy.random import beta def prob_a_beats_b ( a , b ): # a and b each tuples: (success_count, trials_count) a_samples = beta ( a [ 0 ] + 1 , a [ 1 ] - a [ 0 ] + 1 , 10000 ) b_samples = beta ( b [ 0 ] + 1 , b [ 1 ] - b [ 0 ] + 1 , 10000 ) return mean ( a_samples > b_samples ) Here we are drawing 10000 random samples from a Beta distribution updated with the observed successes and failures. Then we simply pair these samples up in order and count how often a_sample > b_sample . For example if our red button was clicked 500 out of 1,000 times, and our blue button was clicked 4,800 out of 10,000 times, we see: In : prob_a_beats_b (( 500 , 1000 ), ( 4800 , 10000 )) Out : 0.88641999999999999 We can see that 50% > 48% easily, but the second option has much more data, so we are not entirely sure that the first option is the best. If we compare this to the closed form solution in Miller‚Äôs post , it‚Äôs quite close. In this specific case we get 0.88631554438158755 . Now we want to push this further and compare a specific test to all the rest. None of the approaches above do this directly, but it‚Äôs fairly straight forward to expand on the prob_a_beats_b function from above: from numpy import mean , maximum from numpy.random import beta def prob_a_beats_all ( a , * args ): # a and each other arg is a tuple: (success_count, trials_count) a_samples = beta ( a [ 0 ] + 1 , a [ 1 ] - a [ 0 ] + 1 , 10000 ) others = [ beta ( other [ 0 ] + 1 , other [ 1 ] - other [ 0 ] + 1 , 10000 ) for other in args ] max_others = reduce ( maximum , others ) return mean ( a_samples > max_others ) Here we‚Äôve done almost the exact same thing as the first example, except that we are comparing against multiple other options. We sample for all of them, and then take the maximum for each one. We do the same pairing, and then measure how often the a_sample > max(other_samples) . Expanding our example, we include a newly added green button which was clicked 12 out of only 30 times: In : prob_a_beats_all (( 500 , 1000 ), ( 4800 , 10000 ), ( 12 , 30 )) Out : 0.76488 We now need one more function so that we can compute all of these probabilities and get the full distribution: from numpy import mean , maximum from numpy.random import beta def prob_vector ( selections ): # dict of {selection_id: (success_count, trials_count)} weights = { k : prob_a_beas_all ( v , * [ other_v for other_k , other_v in selections . items () if k != other_k ] ) for k , v in selctions . items () } return weights This gets us very close. For our example: In : prob_vector ({ 'red' : ( 500 , 1000 ), 'blue' : ( 4800 , 10000 ), 'green' : ( 12 , 30 )}) Out : { 'red' : 0.76766000000000001 , 'blue' : 0.091800000000000007 , 'green' : 0.14032 } You may be surprised to notice that the probability of ‚Äú green is best‚Äù is greater than blue ‚Äôs. Before we address that, though, note that because this process is based off random sampling, the sum of all these probabilities is not exactly 1.0 . (In this case it is 0.99978 .) To deal with this, we‚Äôll write one more function: def normalized_prob_vector ( selctions ): # dict of {selection_id: (success_count, trials_count)} weights = prob_vector ( selections ) total = sum ( weights . values ()) return { k : v / total for k , v in weights . items ()} The output for our example now is: In : normalized_prob_vector ({ 'red' : ( 500 , 1000 ), 'blue' : ( 4800 , 10000 ), 'green' : ( 12 , 30 )}) Out : { 'red' : 0.7664006736707033 , 'blue' : 0.091907931670542953 , 'green' : 0.14169139465875372 } Awesome! Now, back to the value for green being greater than blue . If we measure the probability that blue > green , we get about 0.8 , so we should not interpret this result that green is better than blue . Instead, because we are measuring the chance that each color is the best , we are pretty certain that red is the best and even more certain that red is better than blue . However, we don‚Äôt have nearly as much information about green , and as such there is a larger chance that as more information comes in, it may end up being best. Final Thoughts There is plenty of debate over Bayesian v Frequentist approaches to statistical analysis and modeling. Ultimately though, your choice depends mostly on the context of the problem. This problem in particular was well suited for a Bayesian approach, and the ability to get results through a simulation make it easy to get to an actionable result quickly. Unfortunately though, like most things, there is no silver bullet and you will want to spend some time understanding your problems and the cost and benefits of different statistical approaches. Share on Twitter Share on Facebook", "date": "2015-05-01"},
{"website": "GameChanger", "title": "Distributed Processing With MongoDB And Mongothon", "author": ["Ben Yelsey"], "link": "http://tech.gc.com/distributed-processing-with-mongodb-and-mongothon/", "abstract": "At GameChanger we want our APIs to handle requests as quickly as possible. Ideally, they will do a minimal amount of urgent work before returning a response. We have a lot of secondary computation that we want to perform on our data, and we‚Äôve learned to queue any derivative work for later. Queueing work lets us distribute that work, but also requires that we take some care to ensure data consistency. We use MongoDB as our primary data store, which means no native multi-step transaction ability. As an alternative, Optimistic Locking (described in the MongoDB documentation as Update If Current ) is an established strategy for handling distributed processing with light contention on individual documents. To implement it we‚Äôll need: Versioning of documents Version-specific document updates which fail if the document version in the database has changed Handling of failure We‚Äôve found that with our open source Mongothon library we can make each component of the optimistic locking dataflow a natural part of a Python stack. The Real World Scenario One of the services GameChanger provides to coaches is ranking teams in a tournament. The rankings process is just expensive enough that we‚Äôd like to have a queue processor do it and store the results, rather than make a user wait for their web page to load while the rankings get calculated. To simplify matters, let‚Äôs say that each team has a 'wins' field and it‚Äôs the only data point we use to rank them 1 . We‚Äôll assume that we‚Äôve already defined our Team and Rankings models , and that Rankings contains a list of relevant teams sorted by their ranking info. Since MongoDB isn‚Äôt relational, we‚Äôre also going to want to denormalize as much as we can in order to avoid having to fetch all of the teams to perform a re-ranking. Team ({ '_id' : 1 , 'name' : 'Yankees' , 'wins' : 17 }) Team ({ '_id' : 2 , 'name' : 'Mets' , 'wins' : 5 }) Team ({ '_id' : 3 , 'name' : 'Phillies' , 'wins' : 16 }) Rankings ({ '_id' : 0 , 'teams' : [ { 'id' : 1 , 'name' : 'Yankees' , 'wins' : 17 }, { 'id' : 3 , 'name' : 'Phillies' , 'wins' : 16 }, { 'id' : 2 , 'name' : 'Mets' , 'wins' : 5 } ] }) When a team‚Äôs 'wins' count changes we‚Äôre going to want to update any Rankings document that it‚Äôs in. This update will amount to re-sorting the 'teams' list. What we‚Äôd like is to do something like def update_rankings_for_team ( rankings_id , team_id ): rankings = Rankings . find_by_id ( rankings_id ) team = Team . find_by_id ( team_id ) for flat_team in rankings [ 'teams' ]: if flat_team [ 'id' ] == team [ '_id' ]: flat_team [ 'wins' ] = team [ 'wins' ] break rankings [ 'teams' ]. sort ( key = itemgetter ( 'wins' )) rankings . save () If we only have a single queue processor this code will perform perfectly! Unfortunately, this sort of calculation is prone to race conditions when the possibility of concurrent updates is introduced. If the Yankees get one additional win, and the Phillies get three additional wins quickly after, we know that the Phillies should be ahead in the rankings once we finish processing everything. With distributed processing however, we have no control over the order in which we get the new win statistics for a team and re-sort the rankings. Here‚Äôs an example of an ordering that would lead to inaccurate rankings if we use update_rankings_for_team as is We can make update_rankings_for_team safe for a distributed environment by making it adhere to a concurrency strategy like optimistic locking. Mongothon will let us do this with minimal alterations to our original implementation. Constraining Updates With The Document Version Let‚Äôs first attach a method to the Rankings model that we can use to bump an instance‚Äôs 'version' @ Rankings . instance_method def bump_version ( self ): self [ 'version' ] = uuid . uuid4 () We‚Äôre going to attach another new method to our Rankings model that will act like Rankings.save , but will fail if the underlying document version has changed. Let‚Äôs assume that our document is known to already exist in the database 2 . We‚Äôll eventually call through to Rankings.update 3 . class StaleVersionError ( Exception ): \"\"\"Raised if a rankings save fails due to optimistic locking\"\"\" @ Rankings . instance_method def save_if_current ( self ): version = self [ 'version' ] self . bump_version () result = Rankings . update ({ '_id' : self [ '_id' ], 'version' : version }, self ) if not result [ 'updatedExisting' ]: raise StaleVersionError We can see that if no other process has altered our rankings from the time we fetched it, Rankings.save_if_current() should go through without a hitch. If some other queue processor has altered the rankings, our Rankings.update selector won‚Äôt match any documents, and our result will indicate that no existing documents were updated. Note that we have to call rankings.bump_version here; we need to update the version to make sure a concurrent process gets locked out. Coping With Failure Our Rankings.save_if_current method does exactly what we want in the success case, but leaves us out to dry a little bit when it fails. We need to retry our rankings update or risk missing out on some important rankings change. We could try catching the exceptions inside Rankings.save_if_current , but without fetching the latest version from the database we‚Äôd just fail again. If we do fetch the latest version from the database, we could try saving again with some likelihood of success, but since we won‚Äôt have reevaluated our rankings with the latest player data we‚Äôd risk overwriting updates with more up-to-date information. What would work best would be to just retry the entire queued process (in this case update_rankings_for_team ). Python‚Äôs decorators are good for this sort of thing. MAX_RETRIES = 3 def optimistic ( function ): @ functools . wraps ( function ) def retry ( * args , ** kwargs ): retries = 0 while True : try : return function ( * args , ** kwargs ) except StaleVersionError : if retries < MAX_RETRIES : retries += 1 else : raise return retry If a StaleVersionError is raised, we catch it and retry the function call. Our function (and accompanying save_if_current ) is likely to succeed on the next go-around, provided that we are in a low-contention situation. We set a maximum number of retries because it‚Äôs possible that we‚Äôve made a mistake in our assessment of the problem space: This could be a naturally high-contention situation, in which case the Optimistic Locking strategy is not a good problem fit Our concurrent processes could be aligned in such a way that they‚Äôre needlessly tripping over each other Both of these issues will be surfaced by re-raising the StaleVersionError after our maximum number of retries, since either have the potential to make version conflicts appear far more regularly than is appropriate for this data flow. We now have everything we need to turn update_rankings_for_team into a conflict-free function. @ optimistic def update_rankings_for_team ( rankings_id , team_id ): rankings = Rankings . find_by_id ( rankings_id ) team = Team . find_by_id ( team_id ) for flat_team in rankings [ 'teams' ]: if flat_team [ 'id' ] == team [ '_id' ]: flat_team [ 'wins' ] = team [ 'wins' ] break rankings [ 'teams' ]. sort ( key = itemgetter ( 'wins' )) rankings . save_if_current () It looks a lot like it did before! We had to add a decorator, and call rankings.save_if_current instead of rankings.save . There are some production use cases where we have some follow-up business code to run after the save. If we‚Äôre sure the post-save stuff won‚Äôt alter our rankings data, we could put it in on the next line. In practice however, we‚Äôve found that the save_if_current() line will generally be the last expression in a function (with any follow-up taken care of with Mongothon‚Äôs event handling ). Wrap Up Using a transaction-less database like MongoDB doesn‚Äôt mean we can‚Äôt replicate transactional behavior. What has become clear to the GameChanger engineering team is that we have to make data consistency with distributed processing easy and accessible. We‚Äôve had the tools for implementing strategies like optimistic locking for quite some time, but until we streamlined them with Mongothon they were something we went out of our way to avoid, at the cost of increased complexity elsewhere in our stack 4 . One of the broader takeaways from this is that it is possible to have a team of developers work safely and efficiently if one invests in the development platform itself. It‚Äôs easy for us to direct all available engineering time towards adding and improving the features that our users love, while ignoring the foundation that those features are built upon. Allocating development time to the foundation itself can turn out to have direct user-facing benefits when the features they use become more consistent and reliable. If you‚Äôre currently using MongoDB, and are afraid to just let go and implement a real consistency mechanism, now would be a good time to look around the ecosystem a bit. If you‚Äôre running a Python service, I suggest checking out Mongothon as a means to a simpler environment for your data. We actually use one of several different algorithms based on the needs of the tournament, which can depend on factors like win percentage and geographic region. ‚Ü© To handle ‚Äúnew‚Äù documents we‚Äôd have to lay down a constraint. New documents must have a unique, deterministic identifier; in the case of two related records like Team and Rankings , we could have each instance of Rankings always have the same '_id' as the organizing tournament. Without this property, we‚Äôd have no good way to tell if some parallel process decided to create the Rankings for our tournament all on its own. Our Rankings.update query won‚Äôt quite work for the case of a new document, so we‚Äôd use Rankings.insert when we know the document didn‚Äôt exist prior to our last check. Choosing whether to use Rankings.update or Rankings.insert can be done based on context, or with assumptions about the nature of new documents (e.g. that a new document won‚Äôt have a 'version' key set). ‚Ü© We‚Äôre using Rankings.update instead of Rankings.save because we need to include our version in our selector query. With MongoDB 2.6.8 and PyMongo 2.8, assuming a Write Concern of ‚ÄúAcknowledged‚Äù or higher, we can expect our update to return a dict with 'updatedExisting': True if the selector matches something, and 'updatedExisting': False otherwise. ‚Ü© See: long sequences of atomic updates with hard-to-find race conditions, bespoke task queue algorithms, convoluted schemas, etc ‚Ü© Share on Twitter Share on Facebook", "date": "2015-03-25"},
{"website": "GameChanger", "title": "The Benefit of Hack Days", "author": ["Phil Sarin"], "link": "http://tech.gc.com/the-benefit-of-hack-days/", "abstract": "GameChanger‚Äôs monthly Hack Days have been fun and popular since we launched them last year. They also serve an important function for our business: they open our minds to new ideas that make us smarter and more capable. Hack Days are about cultivating creativity. They aren‚Äôt about shipping production-ready software. For that reason, I don‚Äôt care whether we launch anything that we build on Hack Days. Shipping production software requires an obsession with detail. One needs to think about monitoring, alerting, testing, and logging. Those things are important, but they aren‚Äôt especially conducive to creativity. Creativity flourishes in a playful environment that we try to establish on Hack Day. We deliberately ignore details that normally consume us. Just to enhance the fun of the event, we bring in free food for the whole company. People across the company offer engineers silly bribes to encourage them to work on a particular pet project. At the end of the day, we celebrate our hacks, no matter how ridiculous, with playful demos to the whole company. If launching features isn‚Äôt the point of Hack Day, then what does a successful hack look like? It can take several forms: A seed of an idea that deserves further investment. It‚Äôs easier to argue for design and engineering resources once you‚Äôve already built something. That‚Äôs what Scott Morse did with the basketball lead flow visualization shown above. He hacked together a prototype. That prototype convinced us to prioritize finishing the job and launching it. Practicing a new technique or concept. After reading Marty Cagan‚Äôs Inspired and watching some videos from Sam Altman‚Äôs startup class at Stanford, I thought we‚Äôd benefit from measuring Net Promoter Score (NPS). Sean Wheeler and Crawford Roark joined me, and we built a system for gathering and calculating NPS. Our NPS is now the first thing presented at each company meeting. There was also an unanticipated benefit. The NPS data has become a source for recruiting users for interviews and usability testing. Pure R&D. Some really big problems need creative exploration. Our statistics engine, written in Scheme, runs both on servers and on iOS devices. Though it works, we‚Äôd like to make development easier and we‚Äôd like stat calculation to run faster. Ben Yelsey explored porting the engine to Clojure, though, in doing so, soured on the effort to support such a project across platforms. Later, Tom Leach decided to explore a reactive approach using the Reactive Extensions for JavaScript. We haven‚Äôt yet rebuilt our StatEngine but our collective R&D experiences will inform us when we do. They may also help our work on unrelated projects. Creativity is an important capability and it doesn‚Äôt happen by accident. Every company needs to find creative and timely solutions to hard problems. It‚Äôs especially hard to do so when we‚Äôre in the detail-oriented mindset needed to ship production software. We‚Äôve decided to invest time in developing our own creativity, and our monthly Hack Days are one of our sources of inspiration. Share on Twitter Share on Facebook", "date": "2015-03-12"},
{"website": "GameChanger", "title": "Scaling Engineering with Docker", "author": ["Tom Leach"], "link": "http://tech.gc.com/scaling-engineering-with-docker/", "abstract": "Recently, Travis Thieman and I gave a talk to the CTO School meetup group in NYC about our experiences moving GameChanger‚Äôs mainly Chef-based build and deploy pipeline to one based heavily around Docker . In this post, I want to dig a little deeper into one of the topics we covered: how Docker has enabled us to position our engineering team to scale out in a manner which would have been inconceivable with a more traditional CM tool like Chef, Puppet or Ansible. From seed to monolith GameChanger‚Äôs backend started out as 100% Python - a Django app with some rudimentary features running on AWS. At this sort of scale, build and deploy typically looks pretty simple - some form of manually-run hand-rolled script which pulls master and restarts some services. There‚Äôs really no need for anything more complicated than that when you‚Äôre iterating quickly on new features. Our team grew and our app acquired more and more responsibilities. Our backend was still built on Django and we were still deploying using hand-rolled scripts but with a little additional help from Fabric to automate the process across an increasing number of servers. This growth continued. About 18 months ago our backend had organically grown into a large monolithic application with far too many responsibilities each with different operational characteristics and desired SLAs. At this scale, the hand-rolled script approach to deployment had become unsustainable and we‚Äôd formed a dedicated Ops Team simply to own the process of shipping our code to production. The Ops Team Our Ops Team started out with a pretty simple mandate: the creation of a build and deploy pipeline which enabled separate teams of developers to all push new code to production without needing to care how. We built a deploy system based around Chef with numerous roles and cookbooks which allowed us to build and reconfigure the various flavours of servers we needed. (As it turns out, we determined that Chef actually created a number of deploy-time risks which started to add up as our cluster size increased, but we‚Äôll save that for another day.) With our tech stack almost exclusively Python-based, having the Ops Team completely own our deployment pipeline was sustainable as the knowledge the team needed to do their jobs remained fairly static. The Development and Ops teams worked very closely making it possible for responsibilities and knowledge to be shared - developers made Chef changes, ops engineers changed application code. In essence we were adhering to ‚ÄúDevOps‚Äù principles. From monolith to microservices About 12 months ago, we realized we needed to make a radical strategic change to our stack. The huge monolithic system which we‚Äôd built over time was showing real growing pains: Indirect coupling between functionally unrelated components caused by shared dependencies (e.g. a common database). Fragility - A bug committed to one functional area could take everything down. Python, though a great prototyping language, inhibited our ability to build faster, more real-time functionality. Ownership boundaries between functional areas and shared components were fuzzy, with quality suffering as a result. We realized that in order to continue to grow without these problems bringing our engineering velocity to a standstill, something had to change. That change was to transform our architecture to one based around microservices. A deeper exploration of microservices architectures is beyond the scope of this post, but it should be relatively easy to see why shifting GameChanger‚Äôs stack to this kind of model made sense: Splitting each functional unit into its own service, with its own persistent store decouples functional units from each other and isolates failure. Each service can be built and scaled on a case-by-case basis matching the right language to the right problem. There is no confusion over ownership boundaries as each team has ownership over the services it maintains. Clear ownership incentivizes developers to take pride and improve quality. But there is a problem - what happens to our Ops Team? Having an Ops Team work closely with developers makes sense when the technology stack and hence the shared context is relatively static. However, in a microservices world where teams need to make use of whatever technologies they need, this model starts to break down. Suddenly the Ops Team becomes overwhelmed with new technologies they need to figure out how to deploy and support. The amount of complexity they need to manage increases quickly and their productivity starts to fall as context-switching increases, and understandably there is pushback as delays are introduced. In other words, with everything needing to go through an overloaded Ops Team, the cost of deploying a new service was high. Faced with this obstacle, our development teams would naturally take one of two paths to resolve the conflict: Implement their new feature inside one of the services or technologies already supported by the Ops Team. Bypass the Ops Team completely and roll their own deployment pipeline. Option 1 is undesirable as it negates many of the benefits we‚Äôd identified as a motivating factor for moving to a microservices approach in the first place. Option 2 seems like a better choice as it removes the Ops Team as a bottleneck and preserves the flexibility of teams to implement whatever technology they choose. However, having each development team take deployment ‚Äúin-house‚Äù means that there is a lot of effort duplicated across teams (tooling, scripts, monitoring, etc). Moreover, the development team is unlikely to put much effort into its deploy pipeline, they will do just enough to get their code to production - i.e. each team hand-rolling its own bespoke scripts. This is a problem as the maintainability, debugability, etc of our pipeline is likely to be sub-optimal as a result. A third option? In an ideal world we‚Äôd want the best of both approaches: allow each development team to innovate and use whichever technology they see fit while continuing to have the Ops Team run the ‚Äúcore‚Äù build and deployment pipeline, insulated from impact of per-team technology choices. Using traditional configuration management approaches like Chef, achieving this separation of concerns is possible in theory but extremely difficult in reality. We tried this at GameChanger and ultimately found that the mere fact we were trying to deploy applications to the same underlying host images lead to a plethora of non-obvious hard-to-diagnose problems. A simple example: Services A and B are created by different teams and are both initially deployed against the node.js 0.10.0 binary using a Chef recipe. The owners of Service A require some of the new features in Node.js 0.12.0 to implement some feature and so attempt to upgrade the version of node.js deployed by the Chef recipe. This in turn breaks Service B which has not yet been updated to support 0.12.0. This is a contrived example, but it shows that while using a CM tool like Chef there is inherent indirect coupling between each service‚Äôs deployment and the underlying platform. Without a centralized team to coordinate these kinds of upgrades, the process breaks down quickly. What we really needed was an approach which isolated the concerns of service deployment from the concerns of platform deployment and reduced cost of deploying new services . Docker: to the rescue Docker allows each development team to implement services using whatever language, framework or runtime they deem appropriate. The only requirement they have to get their service to production is to provide a Docker image (plus some basic run configuration in a YAML file) to the Ops Team. Luckily, creating a Dockerfile to build an image for a simple service is a relatively painless activity, especially when compared with performing the same task in Chef. Here‚Äôs what the Dockerfile for one of our node.js services looks like: FROM docker.gamechanger.io/nodejs0.10 MAINTAINER Tom Leach ADD . /gc/allegro WORKDIR /gc/allegro RUN mv Buildfile npm-shrinkwrap.json RUN npm install RUN mkdir -p /var/log/allegro EXPOSE 80 CMD /usr/bin/node /gc/allegro/allegro.js This Dockerfile is simple to construct, simple to understand and will produce an image which can be run anywhere. The Ops Team‚Äôs responsibilities are now restricted to simply building and maintaining a pipeline for deploying Docker containers without needing to concern themselves with what code each container actually contains. The contents of the Docker image are solely the responsibility of the development team. This allows the Ops Team to focus on core deployment problems like building best-in-class CI tools, deployment scripts, log rotation and aggregation, debugging tools, etc. which can be reused across all service deployments. Moreover, this arrangement allows our engineering team to scale. We can add more and more development teams and, as long as we adhere to the rule that every shippable service must be bundled in a Docker image, we add no additional cognitive load to the Ops Team. Wrapping up In recent years, ‚ÄúDevOps‚Äù practices have risen in popularity largely in acknowledgement of a fundamental truth: the interdependence between software development and IT operations. In order to put a complex software application into production, there is often a large context which needs to be shared between developers and ops engineers (configuration, operational characteristics, system dependencies, file paths, permissions, etc). DevOps practices recognize this and seek to make operating on that shared context more efficient by eschewing documentation, silos and rigid process in favour of close collaboration and lightweight communication. While DevOps practices improve the efficiency of deploying a given tech stack, they still imply an ongoing cost which must be paid with every new tech stack that is introduced - more ops engineers. Docker fundamentally changes this equation. The container model largely eliminates the interdependence between software development and IT operations by radically shrinking the shared context both teams operate on down to the simple concept of an immutable container. This reduces the need for tight collaboration between development and ops and allows us to scale out to new technologies without paying the ongoing cost of needing more ops engineers every time. It makes engineering growth more scalable. To underline the point, the table below shows the growth of services and tech stacks at GameChanger as a result of moving to Docker. Number of services Number of tech stacks August 2014 (just after moving to Docker) 5 2 March 2015 15 6 Following our migration to Docker we have tripled the number of services and technologies we support in under 9 months. We also managed to shrink the size of our Ops Team in the process and reassign those engineers to projects which generate revenue. So, if you‚Äôre considering implementing Docker at your organisation, be cognizant not only of the technical benefits it can bring but also the impact it can have on your business‚Äôs cost structure and ability to scale if used appropriately. Share on Twitter Share on Facebook", "date": "2015-03-06"}
]