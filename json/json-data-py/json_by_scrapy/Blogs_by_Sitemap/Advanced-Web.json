[
{"website": "Advanced-Web", "title": "Using Java Generics to express variance of Collections and Functions", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/using-java-generics-to-express-variance-of-collections-and-functions/", "abstract": "I’ve recently came across Richard Warburton’s presentation called Generics and Java’s Evolution , where he explains how to use generic types in different scenarios and introduces some concepts that hopefully will be available in Java 10. It’s a great talk, and it inspired me to spend some time playing with generic types in Java, going down the rabbit hole of the co-variant and contra-variant parameterized types. Lacking imagination, I use the classic Pet example for subtyping in the code snippets for this post. Dog and Cat are subtypes of Pet, so when an expression of Pet type is required, a Dog or a Cat can be used as well. Variance refers to subtyping of complex types created from simple types, such as: We write articles like this regularly. Join our mailing list and let's keep in touch. Because early versions of Java did not include generics, for practical reasons arrays were made to be co-variant. This means if we can substitute A for B then we can substitute A[] for B[] as well. This is good, because we can pass various types of arrays to the same method, like passing an array of Dogs to a method that expect an array of Pets of any kind. If we don’t add new elements to the pets array, there is no problem, but writing to it can be problematic in this case. The code below compiles fine, but when executed it throws an ArrayStoreException because we are trying to add a Cat to an array of Dogs (in a line, where the type information says they are just Pets). Throwing an Exception in this case is really good, compared to the other alternative, to let the problem show it’s ugly face when we would read the original dogs array, and expect all elements to be a Dog. To summarize, arrays are co-variant, so writing to them can cause problems, and the Java type system can’t prevent it from happening. Generic types appeared in Java 5, and one of their most popular use-case is Generic Collections. Unlike simple arrays, the different parameterizations of generic types in Java are invariant by default. For example, a List<Pet> is a totally different thing than List<Dog> , they can not be substituted. So, the code below does not compile: The variance of the type parameter can be specified when using a generic type. (This is called use-site variance . Other languages, such as Scala support declaration-site variance that allows the creator of the generic type to specify the variance of it’s type arguments.) With ’? extends’ the type parameter can be marked to be co-variant. This can make the substitution work for Lists just as it would with simple arrays. The difference is that it’s co-variant nature is encoded in it’s type. It indicates that like in the case with arrays, we can read Pets from the List without a problem, because we know for sure that what we get is at least a Pet. But we don’t know what other conditions must be fulfilled. The parameter indicates that the type of the List is unknown, and it’s upper bound is Pet. We can’t write any kind of Pet to this List to satisfy it’s static type constraints, as this unknown type might be Dog, Cat, but it could be something more exotic, that we might not even heard of. Octopus, Shark, or a HairySpider. Compared to simple arrays, using this co-variant List turns the runtime error encountered before to a compile time one. Similarly to co-variance, with ’? super’ we can declare a contra-variance. A contravariantList is a list of something that is a superclass of Pet. As with covariance, the question mark represents that the exact type of something is unknown, Pet is it’s lower bound is in this case. This means the type might be Pet, or any of it’s ancestor types like Creatures, but it simply can be Object as well. So while we can read elements from the list, but the typechecker can not guarantee anything about their types. But we can write new Pet entries to it, because we know that the unknown type guaranteed to be substitutable with Pet. To summarize, it is safe to read from a co-variant List, but we can’t write to it, while it is safe to write a contra-variant List, but reading items from it can be problematic. The asymmetry is caused by the same type parameter ( E ) on the input and output types of the methods. It drives the minimum type requirements of the object to be passed into it as an argument, and the minimum guarantees about the object’s type it returns. Consider the following short example that illustrates co-variant types from the previous section: Of course, because of breed’s argument, Consumer is invariant with the supplied Consumer parameter, there is a compile error. Because the System.out.println method takes an Object, and returns nothing, at first I thought that the type of the System.out::println expression is equivalent to Consumer . However, if we pass it directly to the breed method, it compiles fine. This is because Java threats the substitution of lambda expressions and method references differently from simple parameterized generic types. See chapters 15.13.2 Type of a Method Reference and 15.27.3 Type of a Lambda Expression from the The Java Language Specification Java SE 8 Edition , they have a lot in common. For example, consider the snippet below: Although for some of the assignments the referred method requires different types for the argument or the return type than the parameterization of corresponding Functional Interface indicate, all assignment compile fine. In every case the argument type parameter of the Functional Interface requires a Message, or a more specific EmailMessage, which is fine, because the transform method expect at least a Message. There is no harm giving something more specific to it. The return types are aligned well too, the transform method provides an EmailMessage which is exactly what’s needed, or even more specific. Of course, providing less, or requiring more would result in a compile error. Java 8 Lambda expressions have similar behaviour if we don’t specify the argument types, and let the compiler infer them for us. So, a method or a function defined by a lambda expression is co-variant in its return value and contra-variant in its arguments. This convenience aids functional programming in Java very well. Similar rule applies for method overloading, but the argument types in that case must match exactly, possibly to aid method resolution.", "date": "2016-05-03"},
{"website": "Advanced-Web", "title": "A brief overview of hack.summit() 2016 (part 3)", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/a-brief-overview-of-hack-summit-2016-part-3/", "abstract": "This is the last post about the 2016 hack.summit() conference. As with the previous two, you can read short excerpts from three of the talks. Managing quality in large communities, an introduction to React Native, and some life advices are the topics covered today. Enjoy! You can watch the video here . The title of Joel’s presentation is “How to Have Nice Things”, which does not say much about the actual content. It’s about his observation to manage communities and have “nice” crowd-sourced projects. We write articles like this regularly. Join our mailing list and let's keep in touch. Joel’s wit and the topic itself offers a unique glimpse on the challenges of managing large communities. It’s interesting to see behind the scenes, and how simple rules and even wording can have a rippling effect and fundamentally change whether a service is successful or will fall apart. The general idea behind his talk is that while small communities are awesome, when the numbers begin to increase, so as the problems. This makes most of the organizations, be them however fascinating, begin to fall apart. His idea is that in order to keep the awesomeness, you need to make and enforce rules that keeps the non-nice things out. One of his example is Wikipedia. From the outside, it’s like everyone can write articles and it’s a collection of what’s true. It was the case for some time, but then some rules were added. The problem with the notion that anyone can add information is that Wikipedia would quickly fill up with bad facts. And that’s why Wikipedia came up with the Notability rule. It means that only verifiable information can find it’s way in, no matter other ones are just as true. This made some awkward situations where the author of a book is not a credible source of information. But in the long run, this rule albeit repulsed many editors, ultimately allowed the content to be better overall. Instead of letting everything in, the remaining editors are working hard to provide high quality content, and it helps the visitors. Joel’s next example is Linux. Linus is making sure only a fraction of all the commits get merged, and that’s essential to keep the quality high. If everyone would be allowed to bring commits to the kernel, things would fall apart quickly. And it’s something that was different when the codebase was small and only a handful of people were working on it. But as more people got involved, new rules were needed. Joel’s third example is his very own StackOverflow. It is aggressively optimized for people coming from search engines, instead of the joy of it’s users. It became the primary reasons why most developers are using it almost every day. They removed the non-nice things, so only the nice ones remain. You can watch the video here . As a React.js developer, I was enlightened to see a presentation about React Native. It’s an excellent opportunity to have a glimpse on the technology from someone who is an expert in the field. Even though I hardly believe I’ll use it in the near future, it’s an interesting piece of technology. React Native is about bringing the power of React and Javascript to mobile platforms. But contrary to the way Cordova did it, React Native is a truly native solution. With the so-called bridges , developers can interface with the native components and services, bringing true support to the underlying infrastructure. React provides an easy-to-grasp and functional programming model that makes development much easier. React Native brings the same concept to mobiles, by substituting the DOM with a tree of app components. The same event model, the same virtual representation, and the same language are used as for the web. What’s the catch? Mobile platforms are quite different, and while small apps could benefit from the common codebase, I’d doubt that providing the true Android and IOS feeling at the same time would work without headaches. Even native platforms introduce major changes every few years, it’s hard to keep up with only one of them. That said, Bonnie’s presentation is an excellent starting point that will give you a feel for the technology in less than an hour. If you are interested in either React of mobile development, you should give it a go and see for yourself. You can watch the video here . Gregg’s talk is about a list he wish he knew out of College. He is the CEO at Codeschool, worked at several places, and founded and also failed at a number of startups, so he surely speaks out of experience. The first in his list is to find a mentor . The best one is who both cares for you and is giving clear and actionable advice. The second is to surround yourself with craftsman . Go to meetups, join user groups, and go to conferences . This way you can stay up-to-date with technology and see the recent trends. People can easily get stuck what they are doing and miss out on the newest innovations. You should learn new things anyway, doing it with others is a huge plus. The fourth advice is to delegate . If there is someone who could do the task more effectively or more eagerly, pass it on. If you are a manager, even though you could do a particular thing, but that doesn’t mean that you should. Learn to delegate and do what you can do best. An example Gregg provides is about a company who hired a barista. They calculated that almost every employee drinks coffee and making them adds up to a level a dedicated professional is more affordable. The other points are more targeted at a personal level. The fifth is to stay out of your comfort zone . Do the most daunting task (also called “eat that frog”), because it usually makes the biggest difference. And don’t get carried away by filler tasks, like email checking; they are useless. The sixth advice is to turn off notifications . It’s something I’m practicing recently, and it really makes a difference. Notifications are the blight on deep work, and should be controlled. Check the emails at fixed times, and don’t let them break you out from the flow. The seventh is that the most sophisticaed solution is rarely the best solution . Developers fall into the trap of overengineering, while keeping things simple is often more valuable. Get rid of the features you’re not absolutely sure are needed. The application will be more with less. Number eight is to take risks when you can afford to . If you are just starting out and you don’t have a profligate lifestyle or a family, you can afford losing. Take risks, and learn a lot; you’ll be much better in the long run. The ninth is to don’t get sucked up into management if it’s not your passion . Most career path starts with coding and ultimately leads to the management. But it’s like military ranks; everyone eventually reach a level where he is bad at. If you don’t feel like managing people is what you’d like to do and better keep coding, do that. If you current company does not offer this path, choose another one company. The last advice, and the most powerful one, is to don’t lose sight of the power of choice . By the time you are out of college, you are a grownup. Everything you do is your choice. If others impose things on you that you don’t like, it’s your decision. You can quit your job, you can quit your relationships. Don’t act like a victim, change.", "date": "2016-04-13"},
{"website": "Advanced-Web", "title": "Dealing with async in React", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/dealing-with-async-in-react/", "abstract": "Handling asynchronous operations in React is tricky sometimes. The framework handles synchronous operations quite well, but, unlike in AngularJs, there is no support for promises and other deferred executions. If you do not test and handle the corner cases, bugs are easily go unnoticed. Temporal issues are the hardest to debug. If you don’t know where to look, then a simple bug report might not be enough. Knowing these corner cases might come handy if you are working on a non-trivial React app. This post demonstrates the basic error cases and potential solutions. Click here for a live demo. And click here for a possible solution. Let’s consider the following use case: you have a long-running process, for example a fetch from a remote database, and you display the results in the page. If the user navigates to another page, the component will be unmounted and you’ll have a warning at the console. The solution is quite simple; just add an isMounted() guard before the setState() , and the warning is gone. Click here for a live demo. The above solution is not enough if you initiate the long-running process on a props change. If the user changes the value of the prop, then the component will be still mounted. If you don’t introduce any other checks, then both async processes will be effective, possibly resulting in corrupted state. Click here for a live demo. The simplest solution you would think of is to introduce a check to see if the prop value is the same when the process is finished as was when it started. This would mitigate the issue to some extent, but if the user rapidly changes the value back and forth, she would still see some flickering. Click here for a live demo. Saving the timestamp to the component state and checking that when the deferred process finishes would solve the problem properly. It requires an extra value in the component state, but ensures that only the last process will be actually run. Click here for a live demo. If you add the prop to the component’s key is a fragile but valid solution. This renders the isMounted() guard sufficient, as if the user changes the prop then React will remove and replace the component. But this makes the component fragile, as the consistency must be guaranteed by the enclosing component. If you fail to add the key parameter somewhere, your component will cease to work. These are the basic cases where an error to deferred execution might get introduced. They all work well in a controlled environment; if you don’t click around until the operation finishes, everything looks good. But real users will twiddle with your application when you would patiently wait. And they will notice and report the errors.", "date": "2016-04-05"},
{"website": "Advanced-Web", "title": "Proper random numbers in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/proper-random-numbers-in-javascript/", "abstract": "Math.random() is thought to be “the way” for generating random numbers on the client. It should be sufficient for the common use cases, except for cryptography. The specification leaves a lot of room to browsers on how to implement it, and they do it in quite a few different ways. But sadly their implementations usually leaves a lot of room for improvement. The first and most visible problem is that it is not seedable . You can not set it to a known state in order to produce reproducible numbers. For most projects, it’s not a big deal, but as soon as you start writing automated tests, knowing what will come out of it is a must. You can circumvent it by wrapping it to a function that can be replaced when the tests are run, but providing a seed is a much cleaner way. But the biggest problem is that it provides no guarantees . The spec does not specify the minimum set of requirements you can expect when you use it. It then goes back to the greatest common divisor problem: if any of the supported browser does not support a feature you need, than you can’t use it. One interesting consequence of bad implementations is a possible deanonimization attack . It is detailed in this paper . Basically the generator is seeded when the browser starts, and by generating a bunch of random numbers, a site can calculate this seed. Because it is different in different browsers, but same for all sites, a user can be linked across different sites. The moment I realized that I need a different implementation is when I started getting collisions. Generating random UUIDs does not require strong algorithms, but I expect them to be different. Because Math.random() is a black box with magic inside, I could not take a look what’s wrong. By switching to a different generator, the problem disappeared for good. After a bit of searching, I found this excellent repository. It provides different PRNG implementations and a common interface for all of them. It has npm support (if you need UMD support too, check out my fork until it gets merged). To use it, just pull in the Mash.js along with your chosen generator. It’s as easy as this: If you have no preference, you can choose Alea. It has everything you’ll need from a PRNG. If you are using ES6 modules, after a simple npm install , you can import it as: ( Note : If you are using the repo without UMD support, you might need to import Mash.js first) You can pass the constructor a seed and it will generate values based on that. Please note that it is not a cryptographically secure generator. If you are generating keys, you should use a secure source . There are many libraries to choose from if you need better random numbers in Javascript. This is the library I’m using for some time and it’s doing it’s job well for now. If you are using another one, it’s fine. But if you rely on Math.random(), then you should consider switching to a better one.", "date": "2016-04-19"},
{"website": "Advanced-Web", "title": "A brief overview of hack.summit() 2016 (part 2)", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/a-brief-overview-of-hack-summit-2016-part-2/", "abstract": "This is the second installment of the overviews of hack.summit() talks. If you haven’t already read the first part, you can do it here . You can watch the video here . Sarah’s talk is about technical leadership. Fortunately she wrote a nice wrap-up about her talk, and hers is surely far better than mine would be. Go ahead and read the excerpt at her blog . The one thought that appealed to me the most is that every mistake is an opportunity. People tend to see obstacles everywhere and fail to realize that they can also drive positive change. A bug is an opportunity to make the software more resilient, a misunderstanding can easily form a more collaborative relationship. Try thinking this way, and you can fix many things around you. We write articles like this regularly. Join our mailing list and let's keep in touch. This concept is present in TDD too. If you see a bug, you first write a failing test for it, and only then fix the bug. The bug will be gone, but the test will stay, giving an early warning if the same bug gets reintroduced in the future. It makes the software more resilient. You can watch the video here . Hampton’s talk is about hiring people, and what guidelines the process should be based on. Hiring is a complex and important process for a company. Working in the tech industry requires two ingredients: you and a laptop. This makes hiring the right people much more important than in other industries. A good interview should be The exact structure should not matter too much. As long as there is a clear one in place, little tweaks make little differences. Behavioural is assessed by talking; you need to map the interviewee’s personality and what he is good at. And lastly, situational , as you should present some situations and ask how s/he would act. A general misconception is that people produce code, so that the hiring process should focus on programming skills. An interesting anecdote is that he applied to a freelancing site that hires only the top developers. He failed their test with 0%, even though he already launched 10-strong pages. It’s also interesting to see that hiring people at several big companies conceded that they almost always get it wrong. Diversity should be the number one goal when hiring people. Coding skills are necessary, but only at a team level. If someone is great at algorithms, while it is a nice skill, not all members are required to be. An interesting concept is positive vs negative hiring practices. The former concentrates on what the candidate is good at, while the latter tries to find aspects s/he falls short. Following this thinking, Hampton admits he used the negative one. He had a veto role , where anyone in the team can veto. He is now transitioned to the believer role , where if anyone in the team sees potential, the candidate is hired; but the believer has to mentor s/he. The last advice Hampton gives is not hire people who are openly hostile to teamwork or mentorship. Development is a team effort, and while most people like to work alone, openly against teamwork is pernicious. You can watch the video here . Jon is the #1 answerer at StackOverflow. The title of the talk is Communicating ideas that we take for granted to new developers , but it’s a bit ambiguous. This talk is not about onboarding new people to your project; in fact it is about how to communicate with people who are just starting in programming. Jon call programming experience “battle scars”. You get these as you struggle to surmount difficult problems, but finally you tackle them down. A mentor could help you on the way. He details several common topic that new developers usually struggle, and a mental model that would help them to understand the underlying concepts. This mental model can be used to tell a story; if the new developer can finish the story, then most likely understands the concept as well. The first model is to understand passing parameters by reference vs by value . This can be depicted using houses; a variable is just the address of the house. This is passing by reference - if someone paints the house, everyone sees the new colour. The address is passed by value, but the object is not. The second model is encoding . Files are just a bunch of numbers, so if you want to write something, you need a way to convert that to sequential numbers. Like colours: You can encode RGB and grayscale colours, but if you just see the file, you have no idea which one is written there. The third one is integers . Understanding binary and hexadecimal is usually a hard task to newcomers. Using dots and simply counting them makes this easier. Continuing on numbers, the fourth one is floating-point numbers . It’s a harder topic, but simplifying it to just three digits and a scale could help understand why some numbers are impossible to represent. Then he moves on to natural vs human dates . It’s a hard topic by itself, but understanding why daylight saving and time zones can screw up everything if you use human dates is the first step to use unix time where you can. Then he moves on to parallelism . The story is to fry bacon and them simmer. Data parallelism is to have all the people have a fraction of the bacon. Task parallelization is when you have Alice do the frying and Bob do the simmering.", "date": "2016-03-22"},
{"website": "Advanced-Web", "title": "Detecting errors in the browser with Selenium", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/detecting-errors-in-the-browser-with-selenium/", "abstract": "Recently I’ve been writing Selenium tests for a large web application. The tests cover the most important features, and the suite grows nicely, but some flaky tests causing much pain and making the progress slower. While investigating the problematic scenarios I’ve found that in many cases errors arise in the browser when a test randomly fails. Moreover, many passing tests also produce Javascript errors occasionally. In the latter cases the bug not surface in the test, but might cause problems later on. These random errors are best eliminated, so I decided to add checks to detect client side log messages. We write articles like this regularly. Join our mailing list and let's keep in touch. I considered two ways of reading client side log entries: There are many other ways of doing this, like opening the developer console and taking a screenshot of it or using a custom browser plug-in, but this time I’ve tried to keep things simple. I also did not want to modify the application’s error handling for the sake of the tests. Reading the logs via Selenium API is reliable and simple. All it takes is to instantiate the WebDriver with the proper settings. Then check the log messages: Unfortunately, the only important information I was able to get through this API is the log message, not the line numbers or the stack traces. On the plus side, all error messages - including the ones during page load - can be acquired. Testing small or extremely deterministic applications this might be enough, as simply reproducing the steps manually will bring forth the bug with all the relevant technical data. Of course, errors contain much more information than just a message. One way to work around the problem with the previous method is to capture the exceptions in the browser. This can be a bit brittle and cumbersome, because it needs an agent to track errors and log messages and provide a way to query them. If the agent is subject to some kind of abuse, it might sabotage the log recording or even break the tests. Because I didn’t want to modify the system under test, I inject the agent to the browser after every page load. This means the agent is unable to provide information about errors that occurred before the page is completely loaded. The agent can be injected with the executeScript method provided by the JavascriptExecutor interface. The Javascript code of the agent can be something like this: As you can see, the agent keeps track of not only the errors and log messages, but also preserves the associated stack traces. For practical reasons the agent above can be injected many times to the same page without causing any problems. This is handy, because it eliminates the need to keep track whether the script is already injected or not. The captured events can be checked easily with the following code: Because each method has some drawbacks, I use them both. Right now I just store the client side log information in the Cucumber test report to aid manual inspection of the failing tests, but it could be used to assert that there are no exceptions in the Javascript application during the tests. This approach has some limitations though, as it can work properly for testing one page applications only. When a test scenario performs an action that leads to another webpage, all previously collected data simply vanish with the fresh page load.", "date": "2016-03-29"},
{"website": "Advanced-Web", "title": "A brief overview of hack.summit() 2016 (part 1)", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/a-brief-overview-of-hack-summit-2016-part-1/", "abstract": "Hack.summit() is now over, and many interesting talks were presented. It’s a virtual conference with many prominent people speaking. You can hear from CEOs, founders, and lead developers their best practices on how they run their companies. It’s also free and, unlike real live conferences, provide the experience to the fullest even if you watch it from recording. Since there are quite a few sessions and each lasts for almost an hour, you’d hardly find the time to watch them all. This and some following posts are the overviews of the talks I watched, so that you can decide whether they are of interest for you or not. Please note , that these are not exact transcripts of the talks, and not all talks are presented here. Only the ones I found interesting. We write articles like this regularly. Join our mailing list and let's keep in touch. Also, you might need to register to hack.summit() to watch the recordings. You can watch the video here Orion is one of the founders of Heroku, a cloud platform primarily for Ruby developers. It’s widely used and it’s push-to-deploy model was a trailblazer many others have since followed. Orion preludes his presentation with some history. Heroku was an educational tool first for Ruby developers with the aim to help people try simple programs in the cloud. Then they were surprised to see that people actually uploaded their codebase and ran their business on their infrastructure. Rather than blocking this behaviour, Orion and his fellow founders realized there is a need and changed their scope. The main idea behind his speech is while the Internet itself is decentralized and designed to withstand concentrated attacks, the cloud is making it centralized to data centers. This makes it fragile and is counterproductive. One way to mitigate this trend is to shift the processing to the browsers from the servers. He then provides a very interesting proof of concept. Asm.js and Web Assembly are two prominent technologies to bring softwares to the browsers that were not designed to run in this environment. Today, you can pick a C++ program and cross-compile it to Javascript and run in the browser. And it’s exactly what he did. Data scientist use literal programming to bundle their findings with the code, making the results reproducible (it’s actually a quite important aspect). They used Python code to generate the living document, but the Python is interpreted on a server, rendering the whole process slow and tedious. Orion compiled the Python interpreter to Javascript and bundled that with the page. It made the recompilation local to the browser. Also he used WebRTC to communicate with other editors, effectively making the server-side almost unnecessary. It’s a brilliant idea and it’s good to see that such tools exists that empower the browser; these made use-cases previously impossible a realistic alternative. The last part of his speech was a speculation how to bring more responsibility to the browsers currently managed by a central server. Responsibilities like identity handling, application delivery, and database might be decentralized in the future. There are some little known technologies that address these, but they currently can not replace traditional servers. This part was a bit futuristic for me, but it was interesting to see what keeps Orion busy these days. You can watch the video here This talk is about the peculiarities of running virtual teams, one of my favourite topic. Floyd is the CEO of InfoQ, and the company is run by remote teams, so he surely speaks from experience. The talk is centered on the human side of remote teams, instead of the exact softwares to use. Also he brings up the well-known aspects of motivated work: purpose, mastery, and autonomy. Besides the exact softwares, communication is divided into sync and async forms. Email, for example, “is a tool for eventual alignment”, while Skype talking is synchronous by nature. The two key elements of running a virtual team is: Since traditional employee monitoring practices do not work remotely, both need to be addressed. Trust and transparency are also key elements. Without the latter, the former can not exist. So a virtual team must be as transparent as possible. This can be achieved with self-reporting that everyone can see. This way the results are open to everyone and everyone can assess the performance of the team. Some companies went a lot further on transparency that they disclosed the financials too. And they are still in business. Hybrid teams can hurt transparency if not managed correctly. People working at the same place tend to keep the informations informal; someone who is working remotely could easily miss out on important pieces. It is therefore important to put everything online once you are committed to build a remote team. It is also important to keep KPIs and dashboards so that everyone has a clear view on their performance and what are expected of them. To reach autonomy, the management should delegate processes instead of tasks. These develop ownership and foster motivation. Floyd then talked about the necessity of meetings and social connections. Regular meetings promote the feeling that the other team members are accessible. People tend to more easily approach a colleague face to face than writing an email. Daily meetings ensure team cohesion. While async communications work better usually, onboarding is an exception. When someone is just starting out in your team s/he can easily feel lost. Face to face meetings every day with a dedicated mentor who shows how the work is done in the team is essential. Also Floyd mentioned the importance of a team-wide social network. Since people tend to see each other quite rarely, personal connections form harder. A social network that acts as a water cooler and gives people chance to share more personal stuff helps the development of social bonds within the team. And as the members working there are actually people, it’s an important aspect. He then detailed the different kinds of meetings they are using, but I think it’s not closely related to running virtual teams. There are many helpful ideas in this presentation, and Floyd definitely mentioned some tricks to make running virtual teams easier. You can watch the video here This session’s structure is a so-called Fireside Chat, which is the fancier name for “just talking”. As a former UpWork freelancer, I’ve been particularly interested in this talk, as the company’s CEO is an authentic source of information. They covered many topics, a bit of history of the ELance-Odesk merger, and some future plans. It was interesting to hear how the predecessors of UpWork enabled developers around the world to find work remotely and be able to work with teams they could not otherwise work with. On the other side, it also allowed companies to trawl for suitable developers from a global workforce. Since then, many similar services started. What I missed was a discussion about the most pressing problems most users, freelancers and companies alike, are facing when using the platform. The noise that non-serious clients and developers bring to the system makes it really hard to find the few serious ones. There are many projects with unrealistic budget and timeline that most of the time is spent vetting them. Most of the clients I’ve worked with also lamented about the other side. It’s actually quite challenging to get started on the platform. That said, if you have the perseverance and the soft skills needed to start, it’s a great platform and you really can make a living working there. They covered many other topics, and if you’re interested in online freelancing marketplaces, you should definitely check out the whole talk.", "date": "2016-03-08"},
{"website": "Advanced-Web", "title": "Getting started with OpenCV for Java on Ubuntu", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/getting-started-with-opencv-for-java-on-ubuntu/", "abstract": "In this post I’d like to provide a brief summary about what it takes to try OpenCV from a Java application on Ubuntu . OpenCV is a great open source computer vision library, written in C++. It has great tutorials (go ahead and read them, if you haven’t already), but anyway, I’d like to share some of the gotchas I’ve encountered along the way. The source code is in a public Git repository hosted on Github. Clone it and switch to the 2.4 branch. First, create a directory for the build in the cloned directory. Then configure the project with cmake with BUILD_SHARED_LIBS parameter unset. From the docs : “When OpenCV is built as a set of static libraries (-DBUILD_SHARED_LIBS=OFF option) the Java bindings dynamic library is all-sufficient, i.e. doesn’t depend on other OpenCV libs, but includes all the OpenCV code inside.” We write articles like this regularly. Join our mailing list and let's keep in touch. Ensure that the output contains something like this in the Java section: It’s important to note , that you can continue if the Java dependencies are not set appropriately, but the compiled version will not include Java bindings . After the necessary changes rerun cmake and make sure to everything is correctly set up. Now, start the build: (The -j option specifies the number of jobs to be run simultaneously.) This is going to take a while, but eventually it creates a JAR (bin/opencv-xxx.jar) containing the Java interfaces and the files of native library. With the following steps, you can use the compiled library directly from Eclipse. First, register the OpenCV native library: For the test, create a simple Project: If everything is fine then running this program should print the 3x3 identity matrix.", "date": "2016-03-01"},
{"website": "Advanced-Web", "title": "The first steps from Grunt to Webpack", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-first-steps-from-grunt-to-webpack/", "abstract": "After Grunt, Gulp, then Browserify, Webpack is the new leading build technology for the frontend. Just as Grunt, it helps you build your codebase, and just as Browserify, it brings support to modularization; but it has more impetus than the others for now. In this post starting with a simple Grunt build, you’ll learn the basics of Webpack and the philosophy behind it. While the use case is quite similar in the two, they are conceptionally different. Grunt is a task runner, while Webpack is a module bundler. Effectively they both take your source files and do a series of transformations to generate some compiled code you can run and deploy. Also you can integrate Webpack into Grunt, so you don’t need to choose if you don’t want to. (To learn about other bundlers, click here ) Grunt is a task runner, that is, a utility to run tasks; be them compilation-related or anything else. There are many tasks for Grunt for frontent compilation, but you are free to use it for others purposes. This makes up an easy-to-comprehend architecture; basically there are tasks and configurations. This makes it a very versatile approach. Webpack on the other hand is a module bundler. It was built with javascript and web applications in mind, making it a more opinionated approach. It encourages modularization and supports the main module schemes. GWT (Google Web Toolkit) admittedly influenced it; for example it also supports code splitting. It’s main concept is fairly simple. You define entry points, and Webpack recursively resolves dependencies; then packs your app into bundles. You can define so called Loaders to define how different types of resources can be loaded (like SCSS needs some preprocessing). That’s basically it. Some advantages of Webpack over Grunt: (To learn how to write code that is easier to maintain, click here ) As jqn pointed out , the examples were seriously outdated. Based on his PR, I’ve updated them to the latest Webpack/React versions. Thanks! For the sake of example, let’s start with a basic Grunt build and then rewrite it for Webpack! The original Gruntfile can be found here I’ll start from the Grunt-based build I introduced in an earlier post. It provides two tasks: It uses Babel to transform JSX into plain Javascript, otherwise it copies everything from the src to the dist . (To get started with React, click here ) The end result can be found here As with Grunt, we need to define some dependencies for the build. A notable difference is that Webpack prefers npm dependencies over Bower ones, so the package.json would contain those too. For the devDependencies section, we’ll use the following: The dependencies section contains the runtime dependencies; we’ll only need React for now. We should define scripts here for building and development: The first one invokes Webpack. By default, it uses webpack.config.js . The watch script invokes webpack-dev-server instead of webpack. It is used during development; it does not write anything to the disc. The –hot turns on hot deployment (a page refresh would be needed after every change otherwise), and –inline tells the server to use inline mode, instead of Iframing. Finally, –watch-poll will help if hot loading is not working otherwise, more on this later. We can run these as npm run build and npm run watch , respectively. The end result can be found here The configuration file is actually a runnable Javascript, and it exports the configuration. This makes it quite versatile, as arbitrary code can be run and Webpack cares only the end result. We need to specify the entry point first, then the output; these are self-explanatory: Then to the plugins section. The one thing Webpack needs is an HTML file that actually loads it’s generated bundle. The easiest way is to use the HtmlWebpackPlugin , and pass it a template. The next section is the modules. You can specify filename patterns and you can configure the different kinds of assets in one single place. Also you can pass parameters to the loaders, like the presets for Babel. In the rules section, we can add a test for the js and jsx files that use the babel-loader . The options is passed to the loader (this case, Babel) as-is, therefore the latest and react presets will be used, along with the transform-class-properties plugin . Also we can add a loader for the styles, piping the css-loader to the style-loader . Finally, the devtool: “source-map” specifies that we want source maps too. As we have everything in place, we can write our Javascript app with all the goodness of Webpack. We can add CSS to the module with a simple require: Export the component: Then import and use it whenever needed: Thanks to the Webpack Hot Loader, if you make a change in any component, the server will update the code and automatically reloads the page. This seriously shortens the time needed to test a change. Albeit the transition to Webpack went smoother than expected, it comes with it’s own set of problems. These are mostly annoying things that might be fixed eventually and luckily only affect development. If you are using Vim to edit sources, your changes might not be detected by the hot loader unless you use –watch-poll . This is a known issues and caused by how Vim saves files. To make this working, you can configure Vim or simply use –watch-poll . (To learn more about new Vim features, click here ) The next thing leaves you wondering is where the sources are in the developer console. They reside in the webpack://. folder: This is a little surprise you’ll eventually get used to. If you use breakpoints, Webpack hot deploy holds some additional surprises for you. You can set breakpoints to your code and they get hit without problems. If you modify something in the code and the modifications are deployed, a new file is created from the modified one. Your already set breakpoints will be useless now, but you can set others in this new file. However, if you change your code another time, you are out of luck. You can not set additional breakpoints. You need to do a full reload to regain this capability; a quirk I hope will soon be fixed. Even if you manage to stop the code at a breakpoint, it is still hard to evaluate anything in the console. Even though there is an import ReactDOM from “react-dom”; line earlier and the Component properly showing up, typing ReactDOM to the console gives an Uncaught ReferenceError: ReactDOM is not defined . After a bit of trial and error, you could figure out it’s current name, like _reactDom2 , but it makes copy-pasting carefully crafted snippets back to the codebase quite hard. This is a known issue , but unfortunately a necessary one to be spec-compliant. On the bright side, it can be tackled on the browser-side. There is an issue in the Chromium tracker, and the necessary proposals are handled in to make this happen. Seems like it won’t happen anytime soon, but it’s nice to see things are progressing. Apart from the aforementioned problems, working with Webpack is a breeze. It’s opinionated configuration requires little tweaking, and it’s notion of loaders fits nicely into the frontend projects. The support for ES6 modules makes Javascript much more mature. Even though we needed to wait so many years for this, things are looking bright for the future.", "date": "2016-02-02"},
{"website": "Advanced-Web", "title": "Git to track local history, without a remote", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/git-to-track-local-history-without-a-remote/", "abstract": "For a long time I thought of Git as just another version control system, and I did not care much about its distributed nature, and that it does not require a remote repository. Almost a year ago I started to use Git to complement some of my workflows, as it can easily add version controlling with all it’s niceness to almost anything that involve files on the local disk. In a nutshell it goes as follows: It’s really simple as it only takes a few short commands to set it up, and gives tight control about file versions, thus it’s making easy to track progress. We write articles like this regularly. Join our mailing list and let's keep in touch. I use it for quite some time for various ad-hoc tasks, and I it came handy in many situations. For example if I have to quickly hack together a proof of concept about an idea, usually I spend a lot of time trying different things. This way usually a lot of stuff turns out as a dead end. Versioning saved me many times when I added another feature, as I could always revert to the last working version. And finally when I got a part working, I can see and review what lines are changed in the attempt, and history serve as a worklog to overview what’s accomplished. More importantly, there is an easy way to remove all unnecessary code that left from trying different things. I did not use this myself yet, but it seems interesting to try various concepts in different branches, then switch between and compare the results easily rather than to mess around with copies of whole directories. Another thing I use Git locally is to track memos. When I prepare for a meeting, I usually make an outline about the important subjects that I’d like to discuss. In the meeting I just write the answers into the relevant parts of the outline. By committing the file before the meeting, I can see every important information in the Git diff, while I can still see the whole document that provides context. This is also very helpful with items that span across multiple meetings. It’s really convenient to integrate this version controlling to most workflows, since many editors support Git. And even if Your favourite tool would not do so, versioning can be easily controlled from the terminal. Most of the time I use it this way, with really few Git commands: These tools are usually more than enough, but since I use Vim for general text editing I try to take advantage of the plugins released to aid Git. I find vim-gitgutter the most beneficial, as it displays recently changed lines in the sidebar. I use it to quickly skim over the file to see fresh modifications. The other plugin I use is the famous fugitive.vim , a wrapper that makes many commands more convenient to use from Vim. While I prefer to use the terminal, this plugin can greatly simplify interactions with Git related to the currently edited file. In my simple workflow I tend to use the following commands: Git is a powerful general purpose tool to track file changes, and it does not necessarily need a remote repository to work. It can be easily integrated with many tools, and it’s easy to use from the command line. Because of these it’s a good candidate to aid various local workflows with versioning support.", "date": "2016-01-28"},
{"website": "Advanced-Web", "title": "Custom CSS animations in React", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/custom-css-animations-in-react/", "abstract": "While CSS3 offers a declarative way to define animations on elements, it is not enough sometimes. Although it supports custom timing with a highly configurable quadratic Bézier curve, it does not support arbitrary functions. The general case is a Javascript function that Sadly it can not be achieved in a purely declarative way unlike the simpler functions. Fortunately there are multiple solutions, ones that can be done with the React framework. At least two different approaches exist. The first one uses some kind of a timer to periodically update the animated element (most likely using requestAnimationFrame ). This is a less hacky and precise solution. The main drawback is that the calculate function has to be called for every frame as long as the animation is running; it makes it infeasible in situations where the calculation is a resource intensive task. The other train of thought is to calculate the animation beforehand, then generate CSS3 animation keyframes. This is a more hacky solution, but has the advantage that the path has to be calculated only once per change. The drawback is that it is not precise, as there are only so many keyframes. This is a performance - precision trade-off, however it generally yields good results. The users are not likely to spot the misplaced frames if you interpolate linearly between 100 calculated points. In this post I’ll demonstrate the latter technique with a simple code. Code is available here . This demonstration centers on an animated element accepting functions for both X and Y translations. These two timing functions map the 0 - 1 interval to the same 0 - 1 interval; they control how much translations should be applied for a given juncture. The first thing is to generate the style tag and handle it’s removal when the component itself is removed. We also need to salt the class names with a random value, in order to support multiple instances. Generating the contents is the next step. We now need to decide on the precision, as the number of keyframes is specified here. The animation will be less precise if you use fewer keyframes. It can reach to a point where it is becoming increasingly noticeable. For example the above animation will look more of a diamond than a circle when you use only ten keyframes: Lastly, we need to insert and store the tag, along with the random nonce. Using the animation specified above requires a simple style attribute: It is important that you set the interpolation to linear, as it defaults to ease . Failing to override it would seriously mess up the animation. We also need to take care of the cleanup: We need to tap into the lifecycle at the componentDidMount and the componentDidUpdate events. At the updateAnimation method we can iterate on and update the keyframes based on the current results of the animation functions. Note, that modifying the keyframes this way won’t change the DOM, so it will be invisible in the source. It makes debugging a bit harder. The main drawback of this solution is that the keyframes are updated for every update of the component. This can be mitigated somewhat by adding a comparison to the componentDidUpdate callback, to check for referential equality. But for most cases, this won’t help much as the function is constructed dynamically, making a different instance every time. Functions are next to impossible to compare due to the closures they have access to, but you can add a hash to them. This should be affected by all the parameters the function uses, and must be kept in sync.", "date": "2016-01-12"},
{"website": "Advanced-Web", "title": "Global listener patterns in React", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/global-listener-patterns-in-react/", "abstract": "React properly handles it’s components out of the box, but it is not always enough. You’ll need a listener sometimes that is attached outside of the component, and it has to be removed along with the component. Failing to properly handle this can results in a few different kind of errors. This post demonstrates these errors and provide an overview of the three most fundamental patterns to deal with this. Let’s iterate on the errors first, that can potentially emerge when global listeners are not removed. Leaking, a warning, and buggy behavior are the three primary symptoms. This page shows all of these erroneous behavior in action. This one is the most straightforward. A lingering listener still consumes memory and CPU, even though it does not do anything meaningful. It can easily happen with a guard expression evaluating to false, which prevents bugs. It also makes these kind of errors hard to spot, as there are no obviously visible warnings. React emits a warning to the console when an unmounted component calls setState . If a listener is still called and it modifies the component state, then we get a notification about it. This makes it quite easy to spot. An unmounted component still has access to the props object passed initially. Thus if a listener is still attached and calls a handler in the props , it is still invoked; it can easily results in buggy behavior. Whether or not you can spot it easily is based on the handling of such calls; it might not results in any palpable effect. ( Demo here ) ( Code here ) Attaching a listener in the componentDidMount and removing it in the componentWillUnmount callback is the simplest pattern. It is a static but a quite useful approach, useful when a component uses a global event throughout it’s lifecycle. It only requires modifications in two places; thus it makes it less error-prone. In case you need a guard expression - for example for a drag and drop scenario to handle mouse up events -, then you should implement it in the handler itself. Alternatively, you can conditionally attach the listener only when needed; albeit it makes the approach more complex and yields little benefits. Also, don’t remove all listeners of a specific kind; it would prevent multiple instances of the component to exist concurrently. Pros: Cons: ( Demo here ) ( Code here ) A more dynamic pattern, and also more aligned with the React philosophy. The component’s state stores the listeners, and the lifecycle callbacks ensure that they are attached and removed when needed. Unfortunately, providing the componentDidUpdate callback is not sufficient, you also need to provide componentDidMount along with componentWillUnmount ; they are not handled with the update. Because of this, it is best practice to move the logic out to a method. With these callbacks in place, you can freely set the listeners in the state; the component itself will make sure they are attached and removed when needed. It will also make sure there will be no lingering handlers attached. This versatile approach allows a dynamic array of listeners too. A downside is that you need to tap on three lifecycle callbacks at once; it might results in errors or erroneous behavior if you forget any of these. Pros: Cons: ( Demo here ) ( Code here ) This approach uses a specialized React element that ties the lifecycle of the listener to the component’s. It properly componentize everything involved, promoting reusability across the application. This requires a minimum amount of code, and is the least error-prone. The listener element is easy to comprehend, and fully handles the lifecycle of the global listener. It is also a dynamic approach, as an arbitrary array of listeners can be attached at any time. As a bonus point, the component hierarchy will properly reflect the listeners attached. Pros: Cons: These are a few basic patterns, and all have strengths and weaknesses. Also you may use others, but I’ve found these three are the most fundamental. I recommend that you use the first solution for simple cases, where a static amount of listeners are needed; and the last one if you need a more dynamic approach.", "date": "2016-01-05"},
{"website": "Advanced-Web", "title": "Unbroken windows - overengineering in the name of clean code", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/unbroken-windows-overengineering-in-the-name-of-clean-code/", "abstract": "Although originally the Broken windows theory is about criminology and sociology, many other areas adopted the analogy. In a broader sense it illustrates the negative effect of temporarily lowered norms. More specifically in software development, it is used to emphasize the long-term benefits of a clean codebase and suggest that the it’s best to leave every work item in a better state, as minimal short-term investments can potentially save much trouble in the future. I would argue that it’s not a good idea to take this practice as a rule of thumb, and try to apply it in every case. According to the Wikipedia , the first forerunner of the theory was an experiment in 1969, when researchers parked a car in California with no license plates and the hood up. Although it clearly seemed abandoned, people ignored its presence. After the week passed, the social scientists who conducted the experiment deliberately broke one of its windows with a sledgehammer. Soon after the same people who just ignorantly passed by before, started to vandalize it. The moral of the story is that such things can happen even in an otherwise civilized community when norms are lowered. We write articles like this regularly. Join our mailing list and let's keep in touch. It’s easy to map the above example to someone’s life as a citizen. If I live in a clean and nice neighborhood, chances are that I more likely to cut my grass frequently. When the streets are littered, I might be tempted to throw away my garbage as it does not really make any difference. So far so good, but if we are to compare developers with citizens and codebases with cities, we should go one step further. Cities are maintained and developed by a lot of people from a mayor through various professionals to all those who live there. They have very different skills and responsibilities, but they work together to achieve the common goal. It might occur, but it’s rare that people contributing to a city in many different roles. There are plumbers, firemen, electricians and bus drivers, but they usually do their primary profession. Also, the boundaries of the responsibilities are clearer . It’s relatively uncommon that But unlike people in real cities, developers do various tasks at the same time on a project , like browsing the codebase looking for something, doing maintenance of maybe not well known modules, building and designing new components or fixing bugs and security issues. While doing these activities, we wear one of the different hats, considering one aspect of the problem. This can lead to narrow vision , so if we are alone, even common sense can not help to stop us from doing crazy things, like - merely driven by good intentions - non-electricians fixing power cables. In real cities, change is difficult, but the power to change almost anything code-related in a given project is constantly at our fingertips, and we use this power all the time in our daily jobs. So to make things worse, we can - and most of the time, have to - apply changes quickly to the codebase to fix a bug or meet a new business criteria. And because in software everything is a bit blurry and subjective, many times we don’t even know if the window is broken or not . Or that there are any windows. Maybe the windows would never break, but we polish them every day, because we think that if they ever get dirty, someone will come around and smash them. We fear that all it takes one broken window to let loose the chaos, where there is no turning back. Many times it leads to overcompensation and overengineering. I think this approach is contagious as much as broken windows , because if the dev culture adopts the “always aim for the nice and general as possible because it surely pays off in the future” mentality, it will surely produce biased decisions. Creating unnecessary general or polished solutions not always pay off, as predicting the future is really hard. Don’t get me wrong. I am generally agree that we should aim for nice and clean solutions. But I don’t like that the definition of “nice” and “clean” are ambiguous and are many times decided individually rather than by the team, when they shouldn’t. I think the Boy Scout Rule is a great thing to follow, leave the camp cleaner than you found it. But there is no need to tell a real scout not to rearrange the whole forest in the process, because they can’t. We need rules that keep us on the right track, and a lots of information on which the team can decide what actions to take. As real debt, technical debt introduced by imperfect solutions isn’t always a bad thing , it just has to be used strategically knowing that later you might have to pay it back. Some key points I would like to emphasize: Problems makes problems. Trying to prevent all of them can easily make more. It seems more feasible to measure, plan, and even let some windows break.", "date": "2015-12-29"},
{"website": "Advanced-Web", "title": "React basics", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/react-basics/", "abstract": "You should give React a try - in case you didn’t already. Despite my initial skepticism, it is developer friendly and straightforward. Coming from Angular, it’s like programming in a lower abstraction level; but the libraries for common problems, along with less unpleasant surprises make up for this. Componentization and the virtual DOM are React’s central concepts. Componentization is the general direction in most contemporary technologies too, like Web Components and Angular Directives. The latter makes the render and the event cycles two distinct flow, making reasoning about the code easier. This post covers most of the building blocks of a basic Tic Tac Toe game. It utilizes the most recent React version along with some useful libraries. In the end, you should have a good general understanding of the technology. As seeing the big picture is one of the most important task, here is a live, clickable demo of what we are building. Have fun! First things first, we need a build environment. Grunt is my go-to choice on front-end building. You’ll only need npm and grunt installed to use. A build is needed because React utilizes JSX; it must be transpiled to plain JS for the browser to use. Theoretically you can write Javascript by hand, but you don’t want to; JSX makes the syntax much more friendly. Also Babel comes with support to all sorts of goodies from ES6 and beyond as an added bonus; it is safe to use tomorrow’s features like arrow functions and consts. Declaring a dependency on Babel along with it’s two presets we’ll be using is the first thing to do. We then need to modify the Gruntfile so that it includes Babel. For the full Gruntfile, please refer to GitHub . This defines a grunt dev task that watches the src directory and auto-compiles to dist . Defining the components and their possible states are the first and most important part when developing a React app. React components are like Directives in the Angular world. They are HTML-like elements that encapsulate state inside, can have some attributes (called props in React), and are the main building blocks of more complex components. When thinking in React, you should visualize that props will go downwards and events upwards. The state should be declared in the topmost component that would need it, as a rule of thumb. For example, all the components need to know whether a game is started; however the actual board is only the needed by the Game . We’ll have three components for this application: These are the components and the states: TicTacToe : Header , without any state Game : (The complete source can be found here ) React.createClass , along with the configuration, defines a component. A render() method is required at a minimum. (Side note: ES6 classes can be used , but they come with some drawbacks. It is safer to use createClass for now) The render function defines how the component will look when added to the page. It utilizes JSX; you can write some fancy logic inside; we’ll cover it later. You are free to define other arbitrary functions; they can be called and passed as props. getInitialState() is a special function that initializes the this.state variable. Defaults should be put there. Use this.setState() to modify the state, which does a shallow merge of the current state and the object passed. Other methods we’ll need: This is a simple component; it’s main purpose is to bootstrap and wire in the other two. This is another simple component (source here ); it just shows some static text and, conditionally, a button. Conditional rendering can be done in several ways in JSX; a simple guard might very well be the simplest: Classes can be added using classNames in JSX. Attaching event handlers is just passing a function to the handler; the onClick attribute in this case. When clicked, React will call the function with a SyntheticEvent . Passed-in functions are simple to call, as they are in the this.props variable. PropTypes are React’s way for defining an interface and types for a component. You can specify what props you expect for the component, which are required, and what are their types. The Header is using these: If the types passed in does not conform to the ones specified here, you’ll see a runtime error in the browser console when running the application. Without static types, an erroneous configuration might slip through unnoticed, but it is a great help anyway. Defining PropTypes is optional, but highly recommended. It help catch errors in an earlier stage; also adds an easy overview how to use the component. This is where all the magic happens (code here ). It has a nontrivial render() , along with a few caveats. The primary concept when writing a render() method is that it is for a particular state and props. Dynamics are nothing to worry about at this stage. On the other hand, events are to modify the state. When you handle the events, you don’t need to worry about how it would manifest as HTML. This separation greatly simplifies coding with React. In a nutshell, the render will be called for each state change, then it’s result is compared to the virtual DOM; only the difference will be actually modified. Since Javascript is fast but DOM manipulation is not, these rerenders will be short. A pitfall is the premature optimization of the components for lighter renders; don’t do this. Shrinking a 1ms full run to a half would only result in less readable code. Don’t do it until you have good reasons to. Mutating this.state directly without using this.setState() is another pitfall. This is plain wrong, as it does not guarantee a rerender. Make sure you don’t modify objects and arrays in the state, as it would also have strange and hard-to-debug effects. To this, along with an emptyBoard() , there is also a getModifiedBoard() that returns a new array. Freezing the objects/arrays right before adding them to the state is also a best practice. Making an arbitrary amount of elements in JSX in another interesting topic. Luckily, despite it’s apparent looks, it is more Javascript than HTML. We can transform an array to elements easily then. Nesting two of these maps draws the board. Babel takes care of browser compatibility; maps and similar ES6 constructs can be used freely. Components have lifecycle events when the props and/or the state is changed. We can use componentWillReceiveProps to listen for props modifications, and componentDidUpdate for both state and props changes. The former should be used when we intent to use setState , as using it in the latter could result in an infinite loop. ComponentWillReceiveProps is called with the next props; these values can be compared with those in this.props . In this example, an empty board is generated when a game is started. componentDidUpdate gets both the previous props and the previous state. The game ending is detected here. If one of the players is winning, then we bubble this to the main component. We also detect a draw, as if no players won, but the board is full. Hovering can be done using the onMouseOver and onMouseLeave events. bind() can be used to set parameters to the event handling function. The highlightMove will then get the rowIndex and the cellIndex parameters: setState can be called multiple times, it does not affect performance. React collects all such calls and only set the state after the handler is done. A conditional class is the final piece we need for the hovering effect. Using string concatenations and ternary operators works fine for manipulating the classNames attribute , but it quickly gets tedious. Use the classnames library instead. Closing remarks, a few tips you’ll find useful when working with React:", "date": "2015-12-08"},
{"website": "Advanced-Web", "title": "3 genius visual data extraction attacks in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/3-genius-visual-data-extraction-attacks-in-javascript/", "abstract": "Have you noticed that you can’t set the height of visited links? A CSS rule that targets these links can set a few attributes but not all. Notice how both links have the same height, even though the bottom one should be taller: Querying the final style also shows the unvisited height: You can set a few properties of visited links, such as the text color: But even if it looks different Javascript can only see the unvisited styling: This is one example where a page can show something but it can not see it. A link can be styled differently depending on its visitedness attribute, but the browser guards this information from the page. A more everyday example is cross-origin images. They can be shown on a page but the browser puts all sorts of restrictions around them. I’ve seen people puzzled why the web works this way, especially when coming from other languages. When you want to show an image in Java or C++ the program needs to access the bytes of the image to show it. Without complete access, they can not display it. But Javascript and the web work differently. In HTML a simple <img src=\"...\"> shows the image without having access to the bytes first. And this opens a window to having separate permissions for showing things and to have access to things. Whether a link is visited or not is a great example of this. It dates back to the earliest browsers, and it’s still well-supported. It allows the user to see which links point to pages that were opened before. It’s a great feature when browsing Wikipedia, for example, as you can immediately distinguish links that potentially contain new information. But while it’s great for UX, it opens a gaping security hole that is hard to close. If a webpage can tell if a link is visited or not, it can access information that is not meant to be available. For example, it can check Google search URLs and that discloses whether the user searched for specific terms. These can include sensitive information and by composing a large number of these searches a webpage can deanonymize the user. Security problems can arise from other elements and not just links if they leak information to the site, like loading an image from a different domain can contain sensitive information. For example, a dynamic image can change depending on how many unread notifications you have: It works as the browser sends the cookies along with the image request that contains the session information that identifies the user to Facebook. If a site could read the response image, it could extract information about the user’s Facebook activity. That’s the reason why you can’t export the contents of a canvas after drawing a cross-origin image on it (it’s called a tainted canvas ). And the elephant in the room is, of course, IFrames. When not explicitly denied via an X-Frame-Options or a Content Security Policy , a page can be included in another page, with all login information and such. If a webpage could access any page it includes it would give it free rein over the data displayed. Browsers go to great lengths to protect information that is meant to be seen by the user but not to the webpage. But sometimes they fail in various ways and due to some bugs or clever use of what’s available attacks slip through. Let’s see some of the most interesting ones! The first one is related to the visited links described above. Not surprisingly, browsers implement measures to block information extraction. It is even acknowledged in the CSS 2.1 specification : UAs may therefore treat all links as unvisited links, or implement other measures to preserve the user’s privacy while rendering visited and unvisited links differently. This means browsers limit the type of styling a visited link can have on top of non-visited links. For example, by not allowing setting the height of the element, the webpage can not inspect the position of an element below the link to see if it points to a visited URL. But with new capabilities, vulnerabilities resurface time and time again. With the getComputedStyle call, Javascript can read the effective style of an element. And before it was fixed, a site could read the color of the link to see if it was visited or not. It was discovered in 2006 only to resurface ten years later . This is a brilliant attack to extract visual information from an IFrame or other protected resource pixel-by-pixel. The blog post from Ruslan Habalov does an excellent job of explaining the attack in detail. The gist of the vulnerability is how blend modes were implemented. Blend modes allow the page to define how elements that are on top of each other interact. This image shows a few examples: (Image from https://webdesign.tutsplus.com/tutorials/blending-modes-in-css-color-theory-and-practical-application–cms-25201 ) Notice how the middle area changes depending on the blend mode type and the 2 layers’ pixel colors. While a page can not access what an IFrame (or a cross-origin image, or a visited link) looks like, but it can position it on the page freely, even below other elements. That allows blend modes to show differently colored pixels depending on how the elements look like. But it should not result in any vulnerability since the page can not access the resulting colors of the pixels, it can only define how the browser renders them. At least, not directly. The code for the blend mode calculation in the browser was implemented to use different branches for different input colors. And since the page can control one part of the input pixels , it can then try a lot of variations and see the differences in timing . And that leaks information about the other part of the input pixels, namely the protected content. (Image from https://www.evonide.com/side-channel-attacking-browsers-through-css3-features/ ) This allows data extraction, one pixel at a time and it circumvents all browser protections against cross-origin access. This vulnerability was patched by eliminating branches from the blend mode implementation, making the algorithm run in constant time regardless of the input colors. This attack utilizes the weakest point in every IT security system: the user. This is a genius way to extract information from a different website as the user actively participates in the attack. And no web standard can protect against that. A CAPTCHA is a way to protect a website (or part of it) from bots. It is a challenge that should be easy for humans but hard for machines, like reading characters from an image. This is used to prevent automated spamming the comments section or the contact form. It looks like this: (Image from https://en.wikipedia.org/wiki/File:Captchacat.png ) Nethanel Gelernter and Amir Herzberg show in their paper a way to utilize the user’s familiarity with solving CAPTCHAs to extract information. In their implementation, they loaded data in a slightly obscured way and asked the user to type it in a textbox. For example, the cache manifest for Gmail contained the user’s email address: (Image from the paper ) Notice that the CAPTCHA is just a rearranged version of the first 15 characters of the email address (victim1813@gmai). It looks like an innocent normal CAPTCHA, but it gives this information to the website. You can no longer extract the gmail address of the user from a cache manifest file. But you can still embed a Facebook comment box on any website and that still contains the real name of the user: (Image from the paper ) Notice that the text contains the name “Inno Cent” in full. By typing it, the user inadvertently exposes their real name if they are signed in on Facebook. This attack also opens the door for all sorts of other information extraction. The authors of the paper exploited Bing’s personalized autocomplete feature that exposed search history. The image on the left shows the template with 4 areas to extract information. The image on the right shows how the “final” CAPTCHA looks like, in this case indicating that all 4 terms were searched: (Image from the paper ) This example exploited a privacy bug in Bing, but it’s not hard to imagine how it could also enable checking if a link was visited or not: just style the non-visited link to match the background. If the user can see it (and type it in the textbox) then the link was visited. The beauty of this attack is that it’s near impossible to implement a technical solution to prevent it. Fortunately, its applicability is limited, as it can only extract textual information and only a few times before the user becomes bored and leaves the site.", "date": "2021-04-27"},
{"website": "Advanced-Web", "title": "How to implement a persistent file-based cache in Node.Js", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-implement-a-persistent-file-based-cache-in-node-js/", "abstract": "Caching is a powerful way to do a process only once and thus speed up an application. For example, generate images from a PDF yields the same result for the same input file, so there is no need to run the costly process from scratch every time. Saving previous results and reusing them when appropriate can take an application that takes ages to run and make it a quick one. Caching in memory is a good starting place. It works by keeping the previous results in a variable so that it’s available the next time a costly process runs. But as memory is cleared when the process exits, it can not reuse results between restarts. File-based caching is a good solution for this. Files are persistent across restarts, providing a durable place to store results. But they also come with an extra set of problems. All file-based caching follows this general structure: It calculates the cache key and the cache directory, then checks if there is a file in that place. If there is, it reads the contents (cache hit), if there is none, then it calculates the result then writes the cache file (cache miss). Let’s break down each part! The first question is: where to store the cache files? A good cache directory is excluded from version control, and it is removed from time-to-time. There is an attempt to standardize a persistent cache location for Node.js applications in node_modules/.cache . It has an advantage over /tmp that it survives machine restarts, while it is in the node_modules directory that is usually recreatable using the package-lock.json . The find-cache-dir package provides an easy-to-use way to locate the cache directory. To initialize and get the cache directory, use this code: This uses the async lazy initializer pattern to create the directory only when needed. All caching depends on a good cache key. It must be known before running the calculation and must be different when the output is different. And, of course, should be the same when the output is the same. I found it a best practice to hash the parts before concatenation then hash the result again. Since hashing makes a fixed-length string, it is resistant to concatenation problems (such as \"ab\" + \"c\" === \"a\" + \"bc\" ). What should be in the cache key? The input data is an obvious candidate, but unlike memory-based caching, some descriptor of the process should also be included. This is to make sure that new versions of the packages invalidate the caches. For example, when I needed to cache the results of a PDF-to-images process, I needed to get the version of the external program that did the calculations ( pdftocairo ). It provides a version() call that calls the process with the -v flag to print its version. But not only the external program influences the result but also the Node.js package. Its version is in the package.json . The getVersionHash() function returns the hash of these versions: The cache key is the version hash and the source hash: sha(await getVersionHash() + sha(source)) . The cache file is the cache directory and the cache key: First, the cache logic needs to determine whether the result is cached or not. This is a check whether the file exists or not: If the result is a single file or value, it’s easy to handle the two cases: Storing multiple results is also possible, just zip what you want to cache and write the archive to the cache. I prefer the JSZip library to handle archiving in Javascript: With this solution, any number of files can be cached in a single zip file. File-based caching is a powerful tool to speed up applications. But it also makes cache-related errors to survive restarts, so extra care is necessary when implementing it.", "date": "2021-04-20"},
{"website": "Advanced-Web", "title": "Getting started with the Jest Javascript testing framework", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/getting-started-with-the-jest-javascript-testing-framework/", "abstract": "Using a testing library is entirely optional as you can always just write a test.js with code that uses console.log statements and throws Errors when something has a value that is no expected. But a testing framework provides a lot of tools to make this process easier, such as user-friendly assertions, mocks, and a CLI. And it feels familiar to whoever used that in a separate project. Jest is a framework for testing in Javascript. It focuses on simplicity and still provides everything you’d expect from such a library. I started using it recently and I found it quite good and easy-to-use. This article is about what I like in Jest and what obstacles I encountered when I started using it. In my case, I wanted to test a part of a webapp that did not depend on the DOM (some utility functions). I use WebPack with React and Typescript to compile the code to the browser. Fortunately, Jest can work with Typescript and React but it required some configuration. The problem here is that Jest and WebPack are two separate entities here where the former runs during testing and the latter when compiling for the browser. This means I needed to setup a separate transpilation pipeline for the test. Fortunately, Jest can use Babel with the babel-jest package. And Babel can transform Typescript to Javascript. One unexpected obstacle was that I also needed to setup React transpilation even though my tests did not touch React component. In my case, I had a tsx file reachable with import s from the test file: My test file imported the file to test ( animationutils.ts ) and that in turn imported a TSX ( utils.tsx ). When Babel transpiled the test code, it encountered JSX and it did not know how to handle it. Even though the test did not use any React components, they just happen to be in the same file. This is a case of blue-red functions just on the file level. During transpilation, a TSX file requires React configuration. And all the files that import TSX files too. And all the files that import files that import TSX files, and so on. As a result, I saw strange errors until I added and configured React and Typescript transpilation: What I liked in Jest is that I don’t need to manage a central list of tests as it discovers and runs all .test.js files in any directory. In a test file, you can import any file with standard import statements: Then the tests are functions with a name, usually also containing a bunch of expectations: The easiest way to run tests is to add jest to the package.json scripts: Then to call it, use: This also supports watch mode with: Instead of the tried-and-tested if (res !== 21) throw \"error\" , expectations and assertions provide a more developer-friendly way to check if a value is what it should be. To check a value: But the Jest expectations API is way more sophisticated than just checking for equality. Some examples: And to negate these expectations, insert .not. : This structure makes test easy to read, and, after some getting used to, easy to write. Mock functions are useful when you don’t want the test code to call an actual function. For example, you might not want to interface with the database and instead return a canned response: Another use-case is to make sure a given function is run. For example, I had callback functions that had expectations. I needed a way to ensure those are called: Without the mock function, I couldn’t be sure that the expect s are reached or the test passes because they are not called. Jest comes with a great watch mode. What I liked the most are the easy-to-use keyboard commands to run only failed tests or filter them by name. This makes it trivial to focus on just one aspect while ignoring all the others. Also, as its name implies, it watches files and automatically runs the tests when files are changed. Here’s a short(ish) video showing how test running and filtering works with Jest: Jest also supports snapshot testing. It is when you save a known good state and the test runner compares the actual state with the saved one. For example, the documentation shows a good example, demonstrating snapshots with a React component: And the snapshot contains the rendered tag: I have mixed feelings about snapshot testing. A good thing about it is that it’s easier to look at something and see if it’s working properly or not than trying to decipher what the expectations are from the code. On the other hand, it is much easier to just snapshot everything as-is without taking into account what the test is supposed to cover. In the above example, a test that checks whether the URL is good fails if the className is changed. I can see how this can lead to a situation where hundreds of unrelated tests are broken and it requires a lot of manual checking to update the snapshots. Jest is a great framework for testing Javascript and Typescript code. It provides everything I needed to focus on the tests instead of the testing architecture: watch mode, expectations, test discovery, and an easy-to-use CLI. While it’s not the fault of Jest, it was not trivial to set up Typescript + TSX support with Babel. This is mainly the result of having WebPack for transpilation, but it still feels like duplicate effort.", "date": "2021-05-18"},
{"website": "Advanced-Web", "title": "What is the await statement in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-the-await-statement-in-javascript/", "abstract": "Recently I got a question regarding a Promise that had two await s: Does having two await s mean the Promise will be run twice? Or is it equal to this code: To understand what happens, we need to look into what the await keyword does. And to do that, we first need to understand Promise states. Fulfilled Resolved value Rejected error Pending When a Promise is created it starts in the Pending state. It means it has no value yet. It does not matter how the Promise is constructed, be it with the Promise constructor or calling an async function. In both cases, the Promise will be Pending : When the Promise is ready, it transitions to the Resolved or the Rejected state. Collectively, these are called Settled or Fulfilled . At this point, the Promise has a value, either a result (if it is resolved) or an error (if it’s rejected). A good illustration of how a Promise moves from Pending to Resolved is to wait a bit and print it again. In this code, the Promise resolves in 100ms, so the second console.log sees it as Fulfilled : So what does await do? It waits for the Promise to be Fulfilled ( Resolved or Rejected ) and returns (or throws) its value. The simplest example is to store the result value then print it to the console: First, prom is Pending , i.e. without a value. The await prom waits until the Promise becomes Resolved (the setTimeout calls the res() function with \"result\" ) then returns the value. You can think about that line as the await prom becomes the result of the Promise: And what happens if the Promise is already Fulfilled ? The await returns with the result without waiting. To revisit the question from the beginning of this article, does the second await makes the code run slower? When this code starts, memo is Pending . So the first await memo stops and waits for it to be Fulfilled . But when the second (await memo) + e runs, memo already has a value, so it returns immediately.", "date": "2021-05-11"},
{"website": "Advanced-Web", "title": "The async lazy initializer pattern in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-async-lazy-initializer-pattern-in-javascript/", "abstract": "How to run an async function at most once and not when it’s not needed? This is where the async lazy initializer pattern helps. I needed this pattern, for example, when I needed to run an external program to get its version string. This is a “heavy” operation as it requires starting a separate process so I did not want to run it every time the version string was needed. And it’s an inherently async operation. It could be a simple Promise, but here’s the catch: it might never be needed and in that case it shouldn’t be run. Since a Promise starts running when it’s created, that is eager initialization. Here’s the code to achieve these requirements: It’s a function that returns a Promise, so it’s compatible with await : The function is lazy as the async function won’t run if nothing calls the getVersion . It is also memoized as calling it multiple times produces the same result without calling the external program multiple times. It stores the result Promise in the prom variable and returns that in subsequent invocations. Since a Promise can be used even when it’s already resolved, it can be used any number of times. Let’s extract the common part to make a reusable function: This gives an easy-to-use, no-dependencies solution. It’s a simplified version of async memoization, so any library that provides a more complete solution works here also, for example the memoizee package.", "date": "2021-04-13"},
{"website": "Advanced-Web", "title": "AWS architecture icons in an npm package", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-architecture-icons-in-an-npm-package/", "abstract": "AWS provides a set of icons that can be used in architecture diagrams for AWS-based apps. They look great, cover most of the services, and provide a familiar style for users whenever they are used. I wanted to use these icons in a React application and I found that the way AWS publishes them is not easy to integrate. These icons come in a zip file with other zip files inside it, and that makes it hard to include in an npm-based package. That’s why the aws-svg-icons package was born. The package contains all the SVG files from the asset file, recursively unzipping all archives inside. It keeps the original directory structure, except for the topmost folder, so it will be easy to update to new versions when they are released. To see what’s inside, head over to unpkg for a list: https://unpkg.com/browse/aws-svg-icons/ . To include these icons in your webapp, first install it with npm: Then in your React app import the image file: If a loader is configured to handle .svg files in the Webpack config (or any other bundler you’re using) then the Lambda icon will be shown. As an extra touch, bundlers are smart enough to know which icons are loaded and they only include those in the bundle. Unused images won’t beef up your webapp. You can browse them here: https://sashee.github.io/aws-svg-icons/index.html (Warning: >6 Mb). This page provides an easy-to-use reference to choose the icon you need. Click on any of them to copy its path to the clipboard and paste it to the import statement. It’s that easy. Here’s a short video on how to use it: You’ll notice that the same icons are in multiple sizes. It does not make sense for vector graphics, but that’s how AWS publishes them so they are included in the package as-is. There are minor differences between the sizes though. I could find only one existing package with the AWS architecture icons, react-aws-icons . It served as an inspiration for this package. The react-aws-icons package wraps each icon in a JSX element. It provides a more React-like feeling but that comes with the cost of a giant config file that generates these components from the zip package. As such, it’s hard to update to the new icon sets, especially when their structure changes. By extracting only the SVG files, the generator code is as simple as possible. Also, as bundlers package only the images used in the webapp, there is no real difference between JSX code and raw SVG files.", "date": "2021-04-06"},
{"website": "Advanced-Web", "title": "How to write a Webpack loader", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-write-a-webpack-loader/", "abstract": "Out of the box, Webpack can load many file types, such as JSON, images, and even raw data. It covers most of the use cases. In one of my projects, I needed to show a PDF file as images and I started to wonder what is the best approach for this and whether a loader would be a good solution. In a situation like this, there are two approaches. The first one is to process the file on the client-side with a suitable library or web API. This is not always possible, as the browser limits what can be done. The other approach is to do the processing on the server-side, either via a run-time API or during a compilation step, and only present the result to the client. I opted for the latter approach. But that raised another question: how? I could just run a script that takes the PDF and puts the results in a folder and use the file-loader to use the images on the client-side. But I wanted something without intermediary files. Webpack loaders offer a way to write any custom processing step between the source file and the result the frontend gets and it runs on the server-side as part of the compilation. It’s exactly what I needed: put the PDF next to the client sources and get the images ready-to-use in the frontend code. Unfortunately, writing a loader is not documented that much. There are some resources, but some pieces of the puzzle are missing. This article is about my experiences of writing a custom loader that processes an input file using an external program. A loader is a function that gets some input data, usually a file or the output of another loader, and returns something. Loaders load files in frontend code. For example, consider this import: It reads the style.css , feeds it to the css-loader , then what comes out of it is the input for the style-loader . The last one outputs a JS module which will be the Styles value. This sequential processing of loaders is roughly equivalent to UNIX pipes: Apart from producing a value, loaders can also emit files that Webpack will include in the result bundle. This is how file-loader works. It emits a file and returns the path so that the frontend code knows where to fetch it: During compilation, Webpack emits a file for the svg and the lambdaIcon value is the path to it. In the browser, the image will be a regular svg file loaded from a remote location. Loaders run during the Webpack build process. They can use the full power of Node.js and all the programs available in the system. You can run graphviz to generate graphs, MozJPEG to optimize images, or pdftocairo to create PNGs out of PDFs. As a compilation-time step, whatever you do here won’t slow down the frontend clients. All visitors can see is the result and none of the work that it required. As a loader is a mapper function, it gets a source and returns the output: The last loader in the chain should return a JS module so that when the client-side app imports it, it will be in a suitable format. That’s what the export default ${JSON.stringify()} part does. The documentation shows a way to use the loader without the need for a separate package: In my experience, it’s easier to start with a separate npm package from the start. First, you can use it inline, such as: Second, it’s likely the custom loader needs some dependencies and by having a separate package.json the libraries are separated. With npm link it’s super easy to develop an npm package locally. In the loader’s folder use npm link to create a linkable project. Then in the webapp, use npm link <packagename> to setup the symlink. And that’s it, from this moment it’s like you’d npm install ed the project every time it’s modified. Webpack converts the input to a string, which is great for text files, but not for binaries. To get the contents without conversions, exports raw : With this, the source will be a Buffer with the file’s contents. Loaders are synchronous by default and you need to switch them to async mode if you need to use await . Doing this provides a callback that you need to use with the result. This feels like an old approach, especially as Promises and async/await are mainstream now, but fortunately it’s not hard to convert to a modern structure: With this boilerplate, you can use await in the function body and errors are also propagated properly. To generate the image files from the input PDF I used the node-pdftocairo project. It operates on Buffers and returns an array of images: These images have to be included in the Webpack output so that the frontend can load them. And the most critical part of that is naming them. Emitted files should be unique . Moreover, they should be revved to provide cache-busting. Revving means the hash of the contents of the file is in its name. This allows perfect caching as the same filename is guaranteed to have the same content while different files have different contents. Built-in loaders emit files this way by default and it boosts client-side performance especially for returning visitors. There is a separate package called loader-utils that provides an interpolateName function. It makes it easy to include the hash in the filename: The [contenthash] is the hash of the content argument. Emitting the file is a simple call with the filename and the contents: This includes the images in the output bundle. But the frontend also needs to know from where to load them . This is where the result of the loader is useful. But there is one detail we need to look into first. The frontend needs the filenames of the emitted files but also their paths relative to the webapp URL. After some searching, I found references to a variable called __webpack_public_path__ , but it seems like the more supported way is to use the process.env.ASSET_PATH . With the asset path and the filename, the only task left is to export the results as a JS module: On the frontend-side, the result of the import is the path to the image. For example, a React app can show the image: In my case, I needed to output multiple images, but that’s just a matter of emitting and outputting multiple files: And the frontend-side gets an Array of image paths: The above steps form the basis of writing a custom Webpack loader. With these, you’re able to run any code during the compilation and emit files and use the results in the frontend.", "date": "2021-03-23"},
{"website": "Advanced-Web", "title": "How serverless cold starts change best practices", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-serverless-cold-starts-change-best-practices/", "abstract": "I worked on a backend project that was based on an OpenAPI document. There is a framework called openapi-backend that provides bindings for Node.Js code that handle endpoints defined in the document. It’s great as there is no need to duplicate code. Everything is in the API document. The openapi-backend runs a lot of initialization code, which is a best practice when deployed to a server as it leads to fast response times after. But this leads to terrible performance deployed as a Lambda function. This got me thinking about how serverless changes what is the best way to write backend functions and especially how to schedule initialization code. Subsequently, the openapi-backend got an option to better support serverless environments. In this article, I compare serverless and traditional environments and how moving from EC2 to Lambda changes best practices. The root of the problem is cold starts. When a backend instance is initialized, the execution environment deploys the code to a new machine and starts routing requests to it. In the case of AWS Lambda, it happens when all the current instances are busy. This is the basis of Lambda scalability: start an instance when needed, then stop it when it’s idle for too long. As a result, the first request needs to wait until the startup process is finished. Worse still, the code is “cold”, which means that the execution environment hasn’t done loading and optimizing the function’s code and all the variables are uninitialized. The former is especially pronounced in Java as the JVM needs to do quite a bit of work to load the classes. It does not help that it has to be done only once. The result is that there will be occasional slow responses when you deploy your code as Lambda functions . While most requests will be fast as they go to warm instances, some will trigger cold starts. Slow requests cause several problems. First, they might be just too long and that leads to bad UX. They can be made faster by allocating more memory that (due to the Lambda resource model ) increases the CPU power available to the function. But that also increases costs for fast requests. Second, there is a single timeout setting for a Lambda function. To accommodate slow cold starts you need to increase it way above what all the other requests would need. The openapi-backend uses an OpenAPI document as the basis of the backend code. It makes heavy use of JSON schemas to define what type of data API endpoints can receive and send. This provides validation effectively for free. During initialization, the framework compiles the schemas in the document. It does this as compiled validators are lightning-fast. But the compilation itself is rather slow. Even for a small number of schemas, it took >2 seconds which is added to the cold start time. Worse still, this delay is linear to the number of schemas, so a larger API has a longer cold start time. If you use load-balanced EC2 instances that start and stop rarely, you want to do everything as soon as possible . The best practice is to do as much initialization as possible, such as populate the caches and even sending a dummy request (called a warmup request) to execute some code paths. When the instance is started, it is ready to serve requests at full throttle. By compiling all schemas during initialization the openapi-backend project followed this best practice. Serverless, a.k.a hyper-ephemeral computing, is built on short-lived instances and that means initialization happens fairly often as the execution units are small and the need for new capacity is instantaneous. Also, cold starts are visible to users, as it happens as part of a request and not in a separate process. The best practice here is to do as little as possible and distribute the work to subsequent requests , also called lazy loading or deferred execution. This brings down the cold start times to an acceptable level. I ran some tests on the openapi-backend project and the results clearly show the contrast in behaviors. The red line is the original configuration, compiling everything as soon as possible. The green line is with deferred initialization. 0 600 1200 1800 2400 ms 0 1 2 3 4 5 request The green line shows a lot less variability in response times. The first request is still significantly longer than the rest, but this is less pronounced. With this setup, there is no need to increase the memory and the timeout settings too much.", "date": "2021-03-16"},
{"website": "Advanced-Web", "title": "AWS JS SDK v3 first impressions", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-js-sdk-v3-first-impressions/", "abstract": "I waited a few months to try out the new version of the AWS Javascript SDK to give the maintainers some time to iron out the initial issues. But since it is likely to be the de-facto way to write code that interfaces with AWS APIs, it’s finally time to start the migration. This article is about my initial impressions and thoughts after using the new version for a small project. It has parts that are similar to Michael Wittig’s experiences who has written about them not long ago. The first thing that catches the eye with the new SDK is the number of packages you can install. It’s the result of a major refactoring effort and it yields multiple benefits. But with multiple packages, the package.json will have a lot more dependencies: And imports will be a lot longer too: This is not necessarily a bad thing. I prefer the explicit over the implicit and this new packaging structure gives a lot of information about what is actually used. Which is one thing that tools such as bundlers can use to remove what is installed but not used. This is called tree shaking, and this modular structure fully supports that. This allows, for example, Webpack to produce a bundle that is a lot smaller than what was possible with the v2. This is a big plus for browser-based apps. Why I like this modularization more is because it exposes a lot of internal utilities that were previously hidden in the SDK code. For example, there is an official arn parser . Also, it makes it easier to get just parts of the functionality so that you can change how it works. For example, the AWS signature algorithm is implemented in a separate package . This possibly makes it a lot easier to implement something that is not provided out-of-the-box but relies on this algorithm. The new SDK allows a new way to construct commands. You can import the client and the operations separately, making it explicit what operations a piece of code uses. The non-modular approach still works: Separating the commands feels more of a functional approach to me, so for the time being I’ll prefer that. It might be easier to pass around functions that create commands and it requires no change to how the client is called. This is one of the more prominently advertised features of the new SDK, though I don’t think many people go and write new middlewares for the AWS clients. Middlewares are customization functions that are run whenever a client object makes a call to an AWS API. It can modify the request/response headers, add logging, caching, and all sorts of things. It opens the possibility for 3rd-parties to offer functionality that integrates into the core of the AWS SDK. Why I like this new construct is that most of the extra functionality of the client libraries is implemented in middlewares, giving an easy way to see what is happening under the hood. For example, the middleware-retry is responsible for retrying operations in case there is a failure. With a separate package, it’s easy to see which errors are retried by default . There is a new pagination util implemented as an async generator function, making my implementation effectively obsolete. Probably the people who did the actual coding realized that each service paginates differently, so they made a paginator function for each command that supports pagination . For example, the CloudWatch Logs client supports 7 pagination utils . Other than the repetition in the library code, these pagination utils make the code you write simpler. For example, a DynamoDB scan supports async for-iteration out of the box: The DocumentClient is gone from the new version. It converted between DynamoDB’s internal attribute format and native Javascript types. The net effect was that you can use items with strings, numbers, arrays, and other familiar types instead of the {S: \"string\"} , {N: 5} , and other structures. But the functionality is not gone, it’s just moved to a separate utility function . There are a marshall and an unmarshall function for this conversion. The former converts from Javascript to DynamoDB, while the former does the reverse. As a rule of thumb, when you send data to DynamoDB you’ll use the first, and when you get data from it you’ll use the second. I like this approach more than the DocumentClient. It does not replicate functionality to a separate client library, and it also makes this conversion explicit. A nice touch is first-class Promise support. No need to add .promise() to every call. Also, the global AWS.config is no longer supported to configure all services in a central place. Again, I welcome this change as that more often than not resulted in poor usage. The new SDK brings first-class TypeScript support. In practice, it’s a lot less groundbreaking than it sounds as there were types for the client libraries already. But it’s nice that they are guaranteed to be up-to-date. One of my biggest concerns is that the SDK is no longer installed in the Lambda Node.js runtimes that use only inline code. With the v2 SDK, it is extremely easy to write a short Lambda function that interacts with AWS services. While it’s better to install the aws-sdk package once your function gets a package.json , short functions that have no other dependencies could use the preinstalled one. I’m hoping there will be an official Lambda layer that adds support for this. As the release notes noted, automatically assuming a role based on a profile does not work. Unfortunately, it seems like this won’t be supported and it requires some code to work around. Another useful but missing thing (as of the time of writing) is the lack of the chainable temporary credentials provider. It’s useful to assume a role through another role, and that makes it easy to scope permissions. The new SDK brings significant improvements to the structure and the observability of how it interacts with the AWS APIs. It requires some significant changes to the projects that use it, but most of the changes make the code easier to understand and maintain.", "date": "2021-03-02"},
{"website": "Advanced-Web", "title": "How to use CloudFront Trusted Key Groups parameter and the trusted_key_group Terraform resource", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-cloudfront-trusted-key-groups-parameter-and-the-trusted-key-group-terraform-resource/", "abstract": "Recently, AWS added a powerful and well-needed function to CloudFront: Trusted Key Groups. They allow using CloudFront signed URLs without involving the account root user. Signed URLs are a way to provide controlled access to private resources . The canonical example is giving access to ebooks or other digital goods: you want to only allow downloading them for users who bought them and not everybody. Since the users don’t have AWS accounts you can’t use the traditional access control mechanism of AWS to grant/deny access to the files. Instead, you keep the files private but implement a URL signing process on the backend: the code checks if the user bought the content then returns a special URL that allows temporary access to the file. It is compatible with serverless backends, in fact, it’s the only way to send large files to the users in that case. In AWS, both S3 and CloudFront implemented signed URLs. The former uses the IAM service to delegate permissions, reusing the Access Key ID/Secret Access Key mechanism that underlies all AWS API calls. This integrates well with other parts of AWS and tools. But CloudFront signed URLs use a separate way: they rely on public/private keys where the public part is added as a trusted key and the other one is used for the signing itself. For the first 12 years, CloudFront only supported adding these keys to the account using the root user. This goes against all security best practices and made deploying solutions that relied on this signing a pain. I advocated that S3 signed URLs should be the preferred way to implement private content signing wherever possible. But CloudFront signed URLs are superior to their S3 counterparts. You can use an S3 bucket behind CloudFront as well as any other type of backend, and its signed URL mechanism can protect all of them. On the other hand, S3 can only sign URLs for objects in S3. That’s why I see the support for Trusted Key Groups as a great feature of CloudFront: you can now use signed URLs without worrying about all those problems an account-level setting would entail. Especially now that Terraform added support for the resource and the setting , it is ready for prime time. So, let’s see how to implement a proper CloudFront signed URLs solution! Here’s a screencast of how deploying and using a Terraform solution works: The first step is to generate the keys that we’ll use for the signing process. openssl is an easy way to do it, as described in the AWS docs : These create a private_key.pem and a public_key.pem , named according to their usage. If you want to generate the keys with Terraform, you can use the tls_private_key resource : Note though that it will include the keys in the Terraform state file. It’s great for quick up-and-running demos but you shouldn’t do it for a production system. Instead, add the private key as an SSM parameter manually and configure the stack with a pointer to it. There are two new resource types in CloudFront: Public keys and Key groups. The former holds the public part of the generated keys, and the latter holds a group of such keys. As we’ll discuss, the CloudFront cache behavior gets a list of the key groups to trust, so there are 2 many-to-many connections here: CloudFront distribution Trusted Key Groups Public key Key group * * This means you can add multiple keys to a group, and you can add multiple groups to a distribution’s cache behavior. It makes it easy to both rotate keys and use multiple independently managed stacks to sign URLs. On the CloudFront Console, there are two new menu entries: In the public keys menu, you can import keys you’ve generated: Then in the Key groups, you can add these keys to a group: Finally, in the CloudFront cache behavior settings you can select the new “Trusted Key Groups” option and add the key group there: And that’s it, now CloudFront will require a signature generated using one of the private keys that are reachable from the distribution setting. The same with Terraform: The backend needs the private key for the signing process, so we need a way to configure it. Environment variables are usually used for this, but they are not suitable here for two reasons. First, the private key is considered sensitive information and the environment is usually less protected. Second, the key is simply too large to fit into the limits of a variable. A better solution that solves both of these problems is to use the Systems Manager Parameter Store. It stores values and only allows access for identities that are allowed through IAM policies, which is well-supported for AWS backends . With a separate place to store the private key, the backend only needs two things: The former is an environment variable with the parameter name: The latter is a statement that allows access for the execution role of the function: To sign a CloudFront URL, the code needs 3 things: The private key is stored in SSM, so the backend needs to get the current value: This makes a call to the SSM service every time it is called, which can overload the standard throughput rate. A better approach is to implement caching on the Lambda side. To get the full URL, concatenate the scheme with the CloudFront distribution domain and the path: Finally, the ID of the public key is available from the environment directly: The aws-cloudfront-sign provides an easy-to-use function to generate the signature. The v2 AWS SDK has support to generate the URL but it’s currently missing from the v3 . Hopefully, AWS will add an official way to generate CloudFront signed URLs eventually, but until then the community project seems to work well. It’s essentially a function that gets the 3 arguments and returns a string with the URL: And that’s it, you can send this to the client and they can access the file: Signed URL support in CloudFront is a powerful access control mechanism that supports all kinds of backends. Because of this, it is superior to S3 signed URLs and should be used wherever controlled direct access is needed. With the new Trusted Key Groups setting it is now possible to deploy the whole process of URL signing without the root user. Signed URL support in CloudFront reached 1.0.", "date": "2021-05-04"},
{"website": "Advanced-Web", "title": "How to keep accurate counts in DynamoDB", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-keep-accurate-counts-in-dynamodb/", "abstract": "You have a DynamoDB table containing the registered users. But how many users you have? With an SQL-based database, that would be a simple SELECT COUNT(*) FROM users . But DynamoDB is not that and it does have native support for efficient counting. At first sight, it might seem like a deficiency. But the above SQL query hides iterating over all the records in a table, which is O(n) so what is one command might not be efficient at all. This also happens with offset pagination . Since DynamoDB does not support anything besides the scan that may perform poorly when the number of rows in a table begins to grow, it has no count(*) functionality. A bad solution in DynamoDB is to use a scan . While it accurately returns the number of elements in a table (or a subset of it) without storing it as a separate entry, it does so by reading all the items in the table . While it works for small tables, the read cost goes through the roof for any realistic production app. So don’t be tempted to use it even if it looks good on demos. Another suboptimal solution (though it’s mentioned in many places) is to use a DynamoDB stream to call a Lambda function that keeps a value stored somewhere in the database up-to-date with the changes. A stream collects modified (added, deleted, and changed) elements and sends them to a target that can be a variety of things inside AWS, such as an SNS topic, an SQS queue, or a Lambda function. A piece of code can then use these change items to increment/decrement a number. App «DynamoDB» [] «DynamoDBItems» [] «Lambda» [] Insert user publish call update count Using DynamoDB streams to update counts This solution has some nice properties. It decouples the logic of keeping a count from the main application logic. The app code does not need any changes, it writes the user records as before and it can also read the number of items. All the magic is encapsulated in the stream configuration. But it has two significant downsides. First, it is async, meaning that the count is updated later than the item. This can lead to inconsistencies as the latest changes might not be reflected in the derived value. And second, it’s not easy to make sure that the updater script runs to completion every single time. With a DDB stream + Lambda function solution you introduce quite a bit of moving parts, each having some guarantees but errors might happen. Lambda has an at-least-once processing model, but even that can fail, sending the call to the DLQ. And since errors accumulate, even an occasional mistake makes a permanent offset between the value and the records in the table. Despite these downsides, it is a good solution in many cases, especially when you only need an approximation. In that case, the asynchronicity and the occasional errors can be tolerated. But with DynamoDB transactions, we can do better. In this article, you’ll learn how to keep accurate counts in the application logic. With transactions, you can group multiple change operations and they are guaranteed to either succeed or fail together. This allows inserting/removing an item from the users table and simultaneously updating a count value to be an atomic operation. Users table ID email user1 test@example.com Counts table type (PK) count users 1 There are 3 operations we need to implement to provide counting functionality: initialization, adding, and removing elements. Since all the other operations increase or decrease a number in the counts table, we need to make sure there is a number there. This has to be done once, usually when the app is started. Users table ID email Counts table type (PK) count users 0 First, we need to check whether the count is there, and add if not: The ConditionExpression: \"attribute_not_exists(#pk)\" makes sure that the database is in the state we expect it to be. As there are no transactions between the commands (the GetItemCommand and the PutItemCommand ), another thread or app might have inserted a value. This check makes sure that it’s not overwritten. To add an item that requires changing the count value, we need a transaction to modify both at the same time. Users table ID email user1 test@example.com user2 other@example.com Counts table type (PK) count users 1 2 The ConditionExpression: attribute_not_exists(#pk) is also important here as we need to make sure that the user item is indeed inserted into the table. Without that, it might overwrite an existing record and still increase the count. To modify a number by a given value, DynamoDB supports an UpdateExpression . This has the advantage of multiple transactions can write it simultaneously without the need for ConditionExpression s. The above code can only insert a new user item but it can not update one. This is because of the ConditionExpression: \"attribute_not_exists(#pk)\" . To implement modifications, use a PutItemCommand with a condition to make sure the item exists: Deletion works similar to addition, with the exception that it needs a Delete operation. Users table ID email user1 test@example.com user2 other@example.com Counts table type (PK) count users 2 1 It needs to do 3 things: Keeping accurate counts is not easy in DynamoDB but it can be done using transactions. Make sure to use the correct ConditionExpression s in every operation to avoid accumulating errors when multiple processes are changing the same data.", "date": "2021-03-09"},
{"website": "Advanced-Web", "title": "How to fix the profile support in the AWS JS SDK v3", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-fix-the-profile-support-in-the-aws-js-sdk-v3/", "abstract": "Two months ago, AWS released the v3 of the Javascript SDK. It brings quite a few updates and unfortunately it needs a different structure than the v2, so a migration is in order for all applications that use that. But since it’s the new version and it’s likely that in the future this will be the mainstream way of writing Javascript code interfacing with the AWS APIs, I started using it for new projects. As usual, I started with a “Hello world”-style program, just to have a working baseline. This is adapted from the documentation : Well, it did not go well: It turned out that multiple tickets are opened already for this issue, some dating back to May. Upon further investigation, it is noted in the release blog post (emphasis mine): What’s missing in v3 The following features are not available in version 3 yet, we are working to add support for them:: Well, that’s a rough start. But let’s see what can be done with it! The root of the problem is the incomplete handling of profiles by the SDK. Profiles are an easy way to use roles with the AWS CLI and the SDKs. When you configure credentials for the CLI, you use long-term credentials, which are added to an IAM user. This is a pair of Access Key ID and Secret Access Key and these identify the user when you make calls to the AWS APIs. For example, this is a set of credentials configured for the CLI: This is enough to get started coding for AWS on your computer but falls short for more complicated scenarios. IAM Roles are a way to issue a set of temporary credentials. They are used for almost everything else other than giving people access to an AWS account. A profile defines a role and a source profile. The source provides the necessary keys to assume the role. For example, this configuration defines a profile that uses the above credentials to use a specific role when needed: With the above configuration, you can run commands using the role with a simple parameter to the CLI: Under the hood, it uses the source_profile ’s credentials to issue an sts:AssumeRole action to get the role’s credentials, then uses them to send a request to the S3 service. It automatically caches and refreshes the role’s keys when needed, providing an efficient workflow. Before the new SDK, it worked the same in code too. Using the AWS_PROFILE environment variable, you could use the same mechanism to use roles: Unfortunately, the v3 Javascript SDK does not support this construct, making the code depend on how the credentials are specified. The first ticket provides the base of a solution though. The problem is this line in the credential provider: It requires a function that implements how to assume a role. This is called from the credential-provider-node , but the roleAssumer comes as undefined. Why? It turns out it’s a side effect of the modularization effort. Assuming a role requires a call to the STS service. And since all the other services require credentials, all of them would depend on the STS code, which goes against the notion of only loading what is used. Because of this, the services have no way to assume a role, that’s why by default the credentials provider does not pass a roleAssumer function. After this investigation, the solution is straightforward. Construct a credentials provides that uses the STS service to assume a role when needed. It’s quite a lot of extra code, and the provider has to be passed to all the client services to add reliable support for profiles. It also increases the number of dependencies, often in a way that is not needed for the production code. For example, you might use profiles to test a Lambda function locally, but when deployed, the Lambda service assumes the role on behalf of the function. This makes the included STS code an extra and unneeded dependency.", "date": "2021-02-23"},
{"website": "Advanced-Web", "title": "EC2 VPC building blocks", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/ec2-vpc-building-blocks/", "abstract": "The VPC is the central container element in EC2 networking. It holds all the other building blocks and provides a separate environment where you can launch instances and define how they can connect to each other and the internet. VPC stands for “Virtual Private Cloud” and the cloud part is in a sense that it’s like multiple virtual data centers with routing, servers, and all the other things you’d find in a cluster of real data centers. A VPC is region-specific. There are ways to connect two or more of them across regions, called VPC peering, but that’s a more advanced feature. VPC «TraditionalServer» Data center 1 [] «TraditionalServer» Data center 2 [] «TraditionalServer» Data center 3 [] A VPC spans multiple data centers The primary configuration of a VPC is its IP address range . It determines which addresses are inside the VPC. Each machine (network interface) gets an address from this range and can find other machines using their internal IPs. This range is also internal to the VPC, as it affects only things inside but an outside observer can not see it. IP address blocks are defined in a so-called CIDR block , which is a combination of a network IP and a mask. It is covered in the next section if you are not familiar with this term. The official recommendation is to use a /16 range, such as 10.0.0.0/16, which contains 65536 addresses (10.0.0.0 - 10.0.255.255). this is usually enough for most workloads. CIDR blocks (Classless Inter-Domain Routing) are how routing works on the Internet and thus match how IP addresses are allocated. A block consists of two parts: an address and a network mask. The network mask defines how many bits identify the network in the address. To think about bits, it’s easier to convert the IP into binary form: 203.0.113.0 is 11001011.00000000.01110001.00000000. The /24 network mask defines that the first 24 bits are the network: 11001011.00000000.01110001 .00000000. This leaves the last 8 bits for addresses inside this CIDR block. The CIDR block matches all IP addresses that start with the network address. For the 203.0.113.0/24, the first address is 11001011.00000000.01110001.00000000 and the last one is 11001011.00000000.01110001.11111111, or in decimal form: 203.0.113.0 - 203.0.113.255. Due to the way the network mask is defined, the /32 network mask matches only 1 address, and the /0 mask matches all addresses. The main feature of a VPC is that it allows networked devices to communicate with each other. Let’s see first how to configure connections! A region has multiple Availability Zones (AZs, for short). These are individual data centers that are separated from each other, so (theoretically) if something affects one zone it won’t impact the others. Using multiple AZs is the basis of engineering for high availability: these data centers are integrated enough to make it easy to spread computing workload across them but separated enough that a problem won’t knock all of them offline. A subnet is part of the VPC that is AZ-specific. Think of it as a network inside a single data center. VPC AZ3 AZ2 AZ1 Subnet3 Subnet2 Subnet1 Subnets are AZ-specific A subnet also needs an IP address range, this time from the range allocated to the VPC. For example, if the VPC has a range of 10.0.0.0/16, then you can create subnets 10.0.0.0/24 and 10.0.1.0/24, each having 256 addresses in them. Each subnet loses 5 IP addresses as they are reserved in AWS, leaving a /24 subnet 251 usable addresses. The official recommendation is to use /24 for a subnet. You’ll find that subnets are usually referred to as “public” and “private”. This is not a checkbox configuration but refers to whether the subnet has access to the internet through a gateway. When the machines inside the subnet can reach out to the public net, it is a “public” subnet. If it can only reach other IPs inside the VPC then it’s a “private” one. When thinking about networking devices in a subnet it’s usually about EC2 instances. You can start an instance in a subnet and it gets an IP address from the range of the subnet, then it can communicate with other EC2 instances in the VPC. But it’s more appropriate to think about connected devices as network interfaces rather than EC2 instances. An instance can have outlets in multiple subnets, and they each behave just like a full instance. Also, there are network interfaces that are not tied to EC2 instances, such as a Lambda endpoint or an S3 gateway. VPC Subnet1 Subnet2 «TraditionalServer» EC2 instance [] «VPCElasticNetworkInterface» Network interface 1 [] «S3BucketwithObjects» S3 gateway [] «VPCElasticNetworkInterface» Network interface 2 [] An EC2 instance can have multiple network interfaces The route table defines how packets are routed inside a subnet. The VPC has a main route table that all subnets use as the default, making it easier to manage them. All the interfaces in the VPC can route to all the others, and there is no way to change that. You can use security groups and NACLs to define rules inside the VPC, but you can’t limit routing. The route table is usually only used when you use an Internet gateway or a NAT gateway. By default, connections are possible inside the VPC. Let’s see how to connect the interfaces to the internet! When you create a VPC, all the interfaces can do is to communicate with each other. To also allow them access to the internet, you need to add an internet gateway (IGW, for short) and add a route pointing to it in the route table. An IGW is also a requirement if you need public IP addresses or want to use Elastic IPs. VPC Subnet1 «VPCInternetGateway» Internet Gateway [] «TraditionalServer» EC2 instance [] «InternetAlt1» Internet [] An Internet Gateway provides connectivity to the internet An IGW is a separate entity from the VPC, and you need to attach it to make it usable. If you delete the VPC you need to also delete the IGW. After attaching the IGW to the VPC, you need to add a route so that traffic destined to the outside is routed to it. To do this, add an entry to the route table with the destination of 0.0.0.0/0 (meaning all IP addresses) and the target of the IGW. With this configuration set, the interfaces inside the VPC have internet connectivity. The other way to provide internet to the interfaces in the VPC is to use a NAT gateway. This works differently from the IGW as it does not allow connections initiated from the outside . As a result, a machine in the VPC can reach the internet and can receive responses but it is unavailable from the outside. Because of this, this setup is not suitable for things like web servers that listen for connections. The problem with this solution is pricing. While you only pay for bandwidth when you use an IGW, the NAT gateway has additional costs. You can use a NAT instance, that does the same but with software you manage and it can be cheaper but brings additional maintenance burden. Usually, you’ll want to use an IGW, and only use a NAT gateway (or instance) when you specifically need it. If you want to expose an instance to the internet, such as a web server that users can connect to, you’ll need a public IP address. You can configure the subnet (or the network interface) to assign a public address to instances, but those are tied to the lifecycle of the instance. If you terminate it or want to migrate the service to a different one, the IP changes. An Elastic IP is a stable address that you can move between instances and it is fixed until you release it. You can allocate an Elastic IP and use that for a DNS record. This way you are free to reconfigure your network without users noticing. Watch out for the pricing though. As long as an Elastic IP is attached to an instance it’s free (1 per instance), but when it’s not then there is an hourly parking fee. VPC security is to define how interfaces inside it can communicate with each other and with the world. It defines managed firewalls on the interface, and the subnet level. A security group defines what traffic is allowed for the interface. As we’ve covered in the Network interface chapter, an EC2 instance can have multiple interfaces. These can have different sets of security groups, allowing different types of traffic to flow through them. A security group allows traffic based on where it’s coming from (or going to) and what port and protocol it uses. For example, a web server can allow TCP traffic on port 80: You can also define outbound rules. For example, you can define that the web server can communicate with backend servers but not directly with the databases. These are outbound rules that define the destinations. By default, each security group gets an outbound rule that allows all traffic: Usually, you’ll want to define a stricter rule, according to the principle of least privilege. This rule denies all outgoing traffic: Security groups are stateful . This means they allow return traffic automatically, you only need to configure how a connection can be made. For example, you allow port 80 for a web server and that’s enough to make it serve HTTP requests without any outgoing rule to allow sending HTTP responses. While security groups work on the interface-level, NACLs (Network Access Control List) control traffic going in and out of a subnet . Every packet that crosses the boundary is subject to these rules. You can use NACLs to define how subnets can communicate with each other, such as which ones are allowed to reach which others. For example, you can define 3 subnets: one for web servers, one for backend servers, and one for the databases (a fairly common 3-tiered architecture). With NACLs, you can restrict that only the backend servers can communicate with the databases. Contrary to security groups, NACLs are stateless. When a packet reaches an instance, the return packet must also be allowed to go out, otherwise the sender is unable to receive the response. Let’s see an example! To allow a web server in a subnet to receive HTTP requests on port 80, you need to add an allow rule that enables incoming requests to enter the subnet: When a browser makes a request to a web server, it uses a local port in the ephemeral range. There is an official recommendation for this, which allocates ports 49152 - 65535 to listen for return traffic. But in practice, the range is usually wider than that. For example, Linux usually use 32768 - 65535, but AWS Lambda and ELB go down as low as 1024 . The safest setting is the 1024 - 65535 range. Ephemeral ports are important for return traffic . When there are multiple connections to the same server, it can use different ports so that when responses are coming back there is a way to know which are for which requests. But that also means a NACL must allow outgoing traffic on these ports. You can configure it with an outbound rule: NACLs use a priority-based rule system. You can define allow and deny rules as well, and the first one that matches the connection will be effective. In the above examples, if the first rule does not match, the second (all traffic) will, and deny the packet. VPC is a powerful tool to define how machines and interfaces can communicate with each other and with the world. You as a network architect have a lot of tools to fine-tune how IPs are assigned, which parts of the network can reach the internet, and what traffic is allowed.", "date": "2021-02-09"},
{"website": "Advanced-Web", "title": "How to use cfn-init to set up EC2 instances with CloudFormation", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-cfn-init-to-set-up-ec2-instances-with-cloudformation/", "abstract": "CloudFormation is AWS’s Infrastructure-as-Code tool that lets you deploy multiple resources based on a template file that you write. It can create and manage nearly every type of resources in AWS, such as VPCs, Lambda functions, DynamoDB tables, and EC2 instances. A well-architected application is ready to run after its deployment with CloudFormation, without requiring any additional manual steps. This works for serverless applications, permissions, and storage, but initializing EC2 instances does not fit into this pattern that easily. For example, a webserver needs to install packages, download additional files, and start services. There are tools, such as Ansible and RunCommand that solve this problem, but they are more advanced tools. In many cases, I don’t want a separate step to set up something simple but want to use the stack right after CloudFormation is done deploying it. EC2 provides the so-called “user data” which is a script that the instance runs when it starts. As you can write arbitrary code here, it allows setting up the instance with all the required packages and services. But EC2 user data is hard to use. You can edit it only when the instance is stopped, so you need to wait a lot of time to try out changes. This makes a bad developer experience. Then writing things as a bash script usually results in a mess. There are ways to bring a structure into a script, but that is more of an ad-hoc solution. Fortunately, there is a solution that integrates with CloudFormation well and yields a more usable structure: cfn-init. Cfn-init is a set of helper scripts that interface with the CloudFormation stack and does two things. First, it reads how the instance should be initialized from the CloudFormation stack and executes it. Second, it signals CloudFormation whether it’s finished or there was an error, so it fits into the lifecycle of the stack. CloudFormation waits until the initialization is complete, and it also rolls back if there was an error. It looks like this: It builds on the user data to initialize and kick off the init process. It does 3 things: The advantage of this structure is that the user data is static . You don’t need to change it when you want to install more services. The arguments of the functions are either supplied from CloudFormation directly ( --stack ${AWS::StackName} , --region ${AWS::Region} ), or references something in the template ( --resource Instance , which is the resource name, or the --configsets setup ). For step 3 to work, CloudFormation needs to know that an initialization signal is coming. This is the CreationPolicy part. Without this, CloudFormation waits only until the instance resource is created (the instance is running) and when that happens it marks it as complete. The primary building block is a config . It defines the packages , groups , users , sources , files , commands , and services , in this order. The most important from this list are: The documentation page is especially helpful when you write cfn-init scripts. Take extra caution for the ordering in a config as that’s easy to mess up. In the example above, this is the install_server config definition: This config installs the httpd package (Apache), puts a file to /var/www/html/index.html , and instructs sysvinit to start the httpd service. The result is a webserver running that serves the Hello world! file. ConfigSets are the configuration that define which configs are run and in what order. Each config runs into completion before cfn-init moves on to the next one. In the example above, the setup configSet defines that the install_server config is run: When the cfn-init command is called, you need to specify which configSet to run: --configsets setup . Let’s see what happens in the above example! First, CloudFormation creates an EC2 instance, because there is a Resource with the Type of AWS::EC2::Instance : When the instance is started, it runs the user data: When the cfn-init is run, it needs to first identify the stack the instance is part of. For this, it needs the --region , the --stack and the --resource . The first two are parameters are from CloudFormation, while the third one is the resource name. With these parameters, cfn-init grabs the metadata defined in the CloudFormation template. Then the remaining parameter identifies which configSet to run. In this case, the --configsets setup , which means the setup configSet will be run. It then looks at the configSets part and finds the setup : The first config in the set is the install_server . So it moves on and intalls the packages: Then puts the files : And finally, starts the httpd service: And finally, since the cfn-init is complete, the user data calls the cfn-signal to let CloudFormation know that the instance is done: Because of the CreationPolicy , CloudFormation creates a signal that the cfn-signal can call. The -e $? is the exit code of the previous command, which is the cfn-init , and the other arguments are to identify which signal to call. Knowing in which order cfn-init runs the parts of the configuration is essential, as it does not follow the order in which the elements appear in the template. First, the configSet defines the order of the configs. Whichever appears later in the list will be run later. Second, inside a config the elements are run in an ordering that is defined by AWS. For example, it always installs packages before running the commands . It’s a best practice to keep the template in the same order as what the cfn-init script will run them. And third, there is ordering inside a config step too, if it makes sense. It’s especially important with commands as those will be run in alphabetical ordering . It’s a best practice to start them with 01, 02, and so on. For example, installing NodeJs requires two configs because it needs to run a command first and install a package next: The setup configSet ensures that the add_nodejs_repo with the command will be the first one to run and the install_nodejs with the package will be the second. When you write the cfn-init scripts it’s better to comment out the CreationPolicy from the template. This is because if you make a mistake CloudFormation will roll back the whole stack, destroying the instance. Just don’t forget to uncomment it when you’re done. You can run the cfn-init command if you SSH into the instance too, which makes a quick deploy-try out process. Just right click on the instance and get the user data and copy-paste the cfn-init part. You can inspect the logs at /var/log/cfn-init.log and /var/log/cfn-init-cmd.log . These are extremely useful for debugging. It’s better to keep the template in a way that reflects the actual ordering of the elements. Keep the configs according to the configSet, and the parts of the config according to their implicit ordering. Also prefix the commands with 01 , 02 , and so on, just to make it easy to see which comes after which. The WordPress AWS-provided template is a great reference point: https://s3.eu-west-1.amazonaws.com/cloudformation-templates-eu-west-1/WordPress_Single_Instance.template . The command accepts a test argument that defines whether the command needs to be run or not. It’s especially useful to run something only once, such as adding test data to the database only if it’s empty. The test checks the exit code of the script, for example docker-compose --version will succeed only if docker compose is installed. To invert the exit code, use ; (( $? != 0 )) , such as test: docker-compose --version >/dev/null 2>&1; (( $? != 0 )) runs the command only if docker compose is not installed. The cfn-init scripts allow an easy way to define how to initialize an EC2 instance right in CloudFormation. It offers a modularized and more declarative way to manage packages and run commands than the user data.", "date": "2021-02-16"},
{"website": "Advanced-Web", "title": "How to properly implement unique constraints in DynamoDB", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-properly-implement-unique-constraints-in-dynamodb/", "abstract": "Let’s say an application stores user objects and as part of the item it records email addresses. A table with a single user might look like this: Users table ID email lFr1D test@example.com When other users come and register to the app, they can specify their email address as well. But that means they possibly use one that is already taken: Users table ID email lFr1D test@example.com vHPZL test@example.com This is usually a problem, as an email address might be used to login to the site, and having multiple users with the same email would cause problems. So, how to make sure the email address a user wants to register with is not already taken by an existing user? The naive solution is to query the database before the insert and throw an error if the email is already in it. The problem with this solution is a race condition . Network calls can be delayed for arbitrary durations, and the email address might be taken between the check and the insert. A solution can not depend on a prior query. In SQL, this is easy. There is a UNIQUE constraint that enforces this property on the database-level. It creates an index that the database will check before inserting the new user: But DynamoDB is not a SQL database, so this is not an option here. One solution is to make the email the key of the user object. As DynamoDB is a key-value data store, it guarantees that only a single item can have a given key. But this is a bad approach. One, you can only make one attribute the key. If users also have usernames and you want to make that unique too, you need to choose between the two. Second, emails in general make a bad key. Users might want to change the email they use for your service and that is increasingly hard if it is referenced throughout the database. Also, in the future, you might want to add the option to associate multiple email addresses with a single user. Making the email the key of the object makes this very hard to do. DynamoDB has no support for a unique constraint out-of-the-box. So, is there a way to implement this functionality using the features DynamoDB supports? Yes, there is. We need two things to make it happen. First, DynamoDB supports transactions . A transaction contains individual operations and provides the atomicity property for the whole set. It means that either all of the operations are effective, or none are. This is enforced by the database itself, so there is no race condition for checking the unique constraint and inserting a user record. The second important feature is the ability to add check conditions , such as you can define that “insert an element if no element with this key exists”. These are the only two things you need to implement the unique constraint that works reliably and without race conditions. Transactions and check conditions still work on items , so we can’t write a transaction that checks that no user has this email address . This means we need to create a new table that has the unique values (the email address, in this example) as keys. Since DynamoDB supports composite keys, where the key is made up of a partition key and a sort key, it’s best to store the type of the unique constraint as well as the value. In the case of emails, the partition key can be the email address and the sort key a constant value, such as “email”. Users table ID email lFr1D test@example.com Uniques table value (PK) type (SK) test@example.com email By adding the type to the unique constraint’s key, you can store different types in a single table. To define a unique constraint for the username too, just add items with the type “username”. Also, by defining the rarer of the two parts of the key as the partition, you’ll end up with more partitions, which is a best practice when working with DynamoDB. You can implement the same in a single-table design, a table modeling technique in which you use only a single table to store different types of items and differentiate them using prefixes. Users table PK SK email USER#lFr1D USER#lFr1D test@example.com UNIQUE#EMAIL#test@example.com UNIQUE#EMAIL#test@example.com It does not matter which style you use to store data, as they are easily interchangeable. Inserting a new user item into the database involves two operations bundled together in a transaction. The first one inserts the user item, as you’d normally do without the unique constraint. The second one inserts the email into the second table, with the condition that the item does not exist. If this condition fails, the whole transaction is canceled, which means there is a user with this email address already in the database. Let’s see how the multi-table approach looks like in code! First, we have two tables, one for the users and one for the unique constraints. In Terraform the definition might look like this: This defines the two tables and their keys. Usually, the names of the tables are passed via environment variables. Let’s assume that the USERS_TABLE and the UNIQUES_TABLE are the two names: When you work with the official Javascript SDK, you can choose between the AWS.DynamoDB class and the DocumentClient . The latter provides a subset of functions of the former, and its main selling point is that it converts between DynamoDB and Javascript types. Effectively, you don’t need to spell out the item types, such as in the ugly-looking {S: \"value\"} . In this article, I’ll opt for the DocumentClient solution. With the DocumentClient , the structure of the transaction is this: The TransactItems array defines the individual operations inside the transaction. These operations can be Put , Delete , Update , and ConditionCheck . To insert a user object, we’ll use the Put operation. A Put operation supports several options. One is the ConditionExpression that defines the preconditions for this operation and the transaction will fail if any of these conditions are not satisfied. The attribute_not_exists(#pk) check along with an ExpressionAttributeNames: {\"#pk\": \"value\"} ensures that the email is not present in the unique constraint table before inserting it. The transaction implementation: This reliably takes care of enforcing the email uniqueness when a user is inserted into the database. Let’s see how to update the unique constraint when a user changes their email address! In this case, we need to do 3 things: Notice that we need the current email address as well as the new one for this. Issue a get to retrieve the current values for the user item: There is a possible but very subtle error that can introduce a race condition and unfortunately most tutorials don’t address this problem. Notice that this get is not part of the transaction. In DynamoDB, transactions are either all-writes or all-reads, so you can’t read the value (the current email address) and be sure that it is not changed by the time the write transaction hits the database. Because of this, you need to add checks into the transaction that the state of the items (the current value of the email address) is what you’ve read moments before. This check needs a ConditionExpression that compares the email value in the database with the one read previously: This check is crucial. Without it, a user might have concurrently changed their email address and the database becomes inconsistent. The three operations, with the proper checks: Notice the third Put . It’s the same operation that we used to add a user to the database. The final use-case is to handle user deletion. In this case, their email address should be removed from the unique constraint table. This works almost the same as the modification. To delete the current email address, we need to read the value first. Then, as it’s outside the write transaction, we need to add a condition to make sure the value is the same as expected. As you’ve seen in this article, implementing a unique constraint is possible in DynamoDB but also that it requires quite some planning to do right.", "date": "2021-02-02"},
{"website": "Advanced-Web", "title": "How to speed up Puppeteer scraping with parallelization", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-speed-up-puppeteer-scraping-with-parallelization/", "abstract": "A typical scraping job gets a bunch of URLs and it needs to open a page, interact with the site, possibly by logging in and navigating around, and extract a piece of data. Puppeteer is the perfect choice for this, as it uses an actual browser that solves a whole array of edge cases. The trivial solution is to run a loop that gets the next item, run the browser to interact with the site, and write its result to some collection value. For example, the following code starts the browser, then runs the scraper code sequentially, each job in a new tab: This processes one job at a time, each waiting for the previous one to finish. This limits the memory and CPU usage during the run, but it leads to terrible performance. But since the jobs are independent it is possible to shorten the total runtime by using multiple tabs. Sequential processing for..of url1 url2 url3 But first, let’s do some refactoring! The above code works for a dummy project but is extremely fragile. If there is an error, it leaves the page and the browser open, which can easily lead to a memory leak. Fortunately, there is an elegant solution for this: the async disposer pattern . This structure implements automatic lifecycle management for a resource that handles cleaning it up when it’s not needed. To manage the browser and the page in a safer way, you can use this code: With this structure, starting and stopping the browser and the page is handled by the utility functions. This leads to a safer code. Auto-disposing the browser and the tabs withBrowser for..of withPage url1 url2 url3 The easiest way in Javascript to run multiple things in parallel is to use Promise.all . Using this built-in yields a simple structure to process an array of inputs with async functions. This code replaces the sequential for..of loop with a Promise.all : This also collects the results in an array, so there is no need to push the elements for every new result. And it also yields a nice, functional code with no loops and variables. Parallel processing with Promise.all Promise.all url1 url2 url3 The problem is that it allows too much parallelization . In the case of Puppeteer, each job opens a new tab in a browser and loads a site in it. This consumes a considerable amount of memory. By opening all the URLs at the same time, the total memory requirement goes up the more URL you want to scrape. Especially with large pages, this can quickly consume all the available RAM, and then the OS either kills the process altogether (with the OOM killer) or starts swapping which slows the whole process by orders of magnitude. Let’s see how to put a limit on the parallelization! The Bluebird promise implementation has a map function that supports a concurrency setting . By using that, you can define how many elements are processed in parallel, and it automatically starts a new one when a previous one is finished. This coordination happens in the background, and it returns the results in an array, same as the Promise.all construct. This code processes the urls 3 at a time: Starting a new job when a previous one is finished Promise.all e1 e2 e3 e4 e5 Another solution is to use RxJS and Observables to process the input array. This library has a mergeMap operator that also supports a concurrency setting. It works the same as Bluebird.map: it processes n items in parallel, starting work on a new one when a previous one is finished. One difference is that RxJS works with streams of elements , so to get an array with the results, you need to use the toArray operator. This code processes 3 items at a time: 1 2 3 4 5 mergeMap P1 P3 P2 P4 P5 The problem with the mergeMap operator is that it does not keep the ordering of the elements in the result array. This may or may not be a problem, but fortunately, it’s not hard to make a variation of it that makes sure that the result array has the same order as the input array. You can see the implementation in this article . To use it, switch the operator to the ordered implementation: 1 2 3 4 5 orderedMergeMap P1 P2 P3 P4 P5 The above solutions all produce a result array when all individual scraping jobs run to completion. But even one error case blows up the whole process. Due to error bubbling, any exception thrown inside a handler terminates the processing and discards all the results. Especially with a long job running through a long list of urls this is a waste and retrying should happen only for the failed elements. Let’s modify the scraper code to return an object which has either a result or an error field. This allows the caller to decide how to handle errors, either by discarding those results or retrying them. The easiest way is to handle this on the page-level since each job has its own browser tab: Scraping with Puppeteer is essentially an async operation as it needs to communicate with a remote process (the browser). This makes it easy to run jobs in parallel, speeding up the scraping. But as each job needs a tab in the browser, it consumes a lot of memory. Unbounded parallelization can quickly use up all the available RAM which then either terminates the process or slows it down. By using a library that allows collection processing with a cap on the available concurrency, you can control the memory usage. This makes it possible to get the benefits of parallelization while also keeping the process within the resources of the executing machine. In this article, we’ve covered Bluebird Promise.map and RxJS’s mergeMap to solve this problem as they both have an option to set the parallelization limit.", "date": "2021-01-22"},
{"website": "Advanced-Web", "title": "How to clear a DynamoDB table", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-clear-a-dynamodb-table/", "abstract": "Especially during testing, it’s a common requirement to clear the database. There is a handy command in SQL for this: DELETE FROM table , which clears all items in the table. But DynamoDB does not support this statement and there is no direct alternative either. How to delete all items in a DynamoDB table then? It turns out there is no easy way to do this and reliably deleting all items requires quite a bit of coding. But there is an alternative way too: deleting and recreating the table . This gets rid of the items inside and it also has a big advantage the other solutions don’t have: it’s free. You don’t need to issue a delete request to each item and pay for the write capacity it consumes. The problem with this solution is that you need to make sure you recreate the table exactly as it were , with all the attributes and indices. And that includes all future features too. Failing to do so might break the app. But let’s consider a more traditional approach, one that does not modify the table only the items inside it ! The basic structure is simple: get all the items , then send a delete request for each of them . This could be a few lines of code, but the way DynamoDB and its API works makes it a bit more complicated. The scan operation reads the entire table and returns all the items. It consumes read capacity units for every element, so it is usually a wasteful operation that indicates that an index is missing. But when you need all the elements, it’s the operation to use. By default, it returns all fields of every item. But the delete operation requires only the keys (hash and sort keys). To make the scan transfer less data over the wire, it supports a ProjectionExpression that allows returning only a subset of attributes for each item. Note that it does not make the scan consume less read capacity, only saves bytes transferred on the wire. But which attributes are the keys? To know this, use the describeTable operation that returns this information: The keys value contains all the necessary information for the keys. The next step is to make a scan request and get the keys for all items. This is again a bit more complicated as scan is a paginated operation. It may not return all elements, in which case it has a LastEvaluatedKey field in the response. To get more elements, use the ExclusiveStartKey property for the next request. The allItems value contains the keys for all elements. The next step is to issue a deleteItem for each key gathered in the previous step. Fortunately, DynamoDB supports the batchWriteItem operation that allows more than one DeleteRequest in a single call, reducing the HTTP overhead significantly. The batchWriteItem supports batches of size 25. This means a request has to be made for every 25 items. To make these batches, lodash provides the chunk convenience function: With these batches, the next step is to prepare the arguments for the batchWriteItem . It supports deleting from multiple tables and it needs the types and values for all keys. An example request is: Programmatically, the code below generates this structure for each batch: Finally, sending the batchWriteItem operations also needs some extra plumbing. Since they contain multiple independent requests without a guarantee that all of them will be successful, a partial apply can happen. In this case, some items are deleted and some are not. DynamoDB indicates which requests in a batch are unsuccessful in the UnprocessedItems field in the response. To make sure that all the items are processed, we need to implement a retrying strategy. It’s quite a bit of coding, but this code implements a generic way to reliably delete all items in a table: The above solution removes all elements in the table, but there are edge cases and things to note when using it. First, it reads all the items in the table. This incurs read capacity costs. Then it writes all items, which use write capacity. Deleting a table this way costs more the more elements the table has. Then this operation is not atomic . It first reads all the items, then deletes them. If items are written to the table in the meantime, they won’t be deleted. Also, it loads all keys into memory, which is a finite resource. It is practical up to several thousand (or potentially a few million) items, but after that, you need to implement a streaming version.", "date": "2021-01-19"},
{"website": "Advanced-Web", "title": "How to convert between callbacks and Promises in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-convert-between-callbacks-and-promises-in-javascript/", "abstract": "While Promises and async/await are increasingly the primary way to write asynchronous code in Javascript, callbacks are still used in many places. Several libraries that adopted that style are slow to migrate to the more modern alternative, and browser (and Node) APIs are also slow to change. For example, marked , a markdown compiler needs a callback when it’s used in asynchronous mode: Similarly, setTimeout invokes a function when the time is up: Not to mention a ton of web APIs, such as Indexed DB , FileReader , and others. Callbacks are still everywhere, and it’s a good practice to convert them to Promises especially if your code is already using async/await. Callbacks implement the continuation-passing style programming where a function instead of returning a value calls a continuation, in this case, an argument function . It is especially prevalent in Javascript as it does not support synchronous waiting. Everything that involves some future events, such as network calls, an asynchronous API, or a simple timeout is only possible by using a callback mechanism. There are several ways callbacks can work. For example, setTimeout uses a callback-first pattern: Or functions can get multiple functions and call them when appropriate: How the callback is invoked can also vary. For example, it might get multiple arguments: Also, asynchronicity might be implemented as an event emitted by an EventEmitter object: As there are multiple equally reasonable ways to implement callbacks, it was a mess at first. A de-facto standard emerged, which is now called Node-style or error-first callbacks. It is used almost everywhere in Javascript whenever a callback is needed. A Node-style callback has three characteristics: As an illustration, this function implements a Node-style callback: Notice that when there is no error, the first argument is null . This allows the caller to easily check whether the execution failed or succeeded: With a callback structure that is used almost exclusively, it is possible to convert between Promises and callbacks more easily. This is the more prominent direction as it’s a common task to integrate a callback-based function into an async/await flow. Let’s see how to do it! The Promise constructor is the low-level but universally applicable way to convert callbacks to Promises. It works for every callback style and it needs only a few lines of code. The Promise constructor gets a function with two arguments: a resolve and a reject function. When one of them is called, the Promise will settle with either a result passed to the resolve function, or an error, passed to the reject . This makes it easy to call a callback-based function and convert it to a Promise which then can be await -ed: The above examples show how to call a callback-based function and get back a Promise, but it requires wrapping every call with the Promise boilerplate. It would be better to have a function that mimics the original one but without the callback. Such a function would get the same arguments minus the callback and return the Promise. To make promisified versions of functions, it is only a matter of wrapping the Promise constructor in a function that gets the arguments before the callback: These functions are direct replacements to the callback-based originals and can be directly used in an async/await workflow: While the Promise constructor offers a universal way to transform callbacks to Promises, when the callback pattern follows the Node-style there is an easier way. The util.promisify gets the callback-based function and returns a promisified version. When the promisified function is called, the callback argument is added automatically and it also converts the result (or error) to the return value of the Promise. How it works is no magic though. It uses the Promise constructor pattern we’ve discussed above, and uses the spread syntax to allow arbitrary amount of arguments. A simplified implementation looks like this: The value of this is complicated in Javascript. It can get different values depending on how you call a function. For example, classes can have instance variables, attached to this : But when you have an object of this class, whether you call this function directly on the object or extract it to another variable makes a difference in the value of this : This affects how to promisify the methods of this object as the util.promisify requires just the function and not the whole object. Which, in turn, breaks this . For example, let’s say there is a Database object that creates a connection in its constructor then it offers methods to send queries: Using util.promisify would break the getUser function as it changes the value of this : To solve this, you can bind the object to the function, forcing the value of this : The length of a function is how many arguments it needs. Don’t confuse this with the Array’s length , as that is how many elements in the array. For example, this function needs 2 arguments, so its length is 2: It is rarely used, but some libraries depend on it having the correct value, such as memoizee , which determines how to cache the function call or marked , a Markdown compiler, to decide whether its configuration is called async or sync . While the length of the function is rarely used, it can cause problems. util.promisify does not change it, so the resulting function will have the same length as the original one, even though it needs fewer arguments. The other direction is to have a Promise-returning function (usually an async function) and you need to convert it to a callback-based one. It is much rarer than the other way around, but there are cases when it’s needed, usually when a library expects an asynchronous function and it supports only callbacks. For example, the marked library supports a highlight option that gets the code block and returns a formatted version. The highlighter gets the code, the language, and a callback argument, and it is expected to call the last one with the result. As with the promisification, there is a universal and a Node-callback-specific way to convert an async function to use callbacks. The universal way is to use an async IIFE (Immediately-Invoked Function Expression) and add use then to interface with the callback when there is a result or an error: This structure allows all callback styles as you control how a result or an error is communicated with the callback function. For Node-style callbacks, you can use the util.callbackify function that gets an async function and returns a callbackified version: This yields a convenient structure and it is suitable in most cases. Also, it changes the resulting function’s length by one, as it needs a callback as well as all the original arguments: Promises and async functions are the present, but still many things support only callbacks. While there are many ways to implement callbacks, the Node-style pattern is prevalent in Javascript. This allows utility functions to convert between Promises and callbacks mostly automatically.", "date": "2021-01-26"},
{"website": "Advanced-Web", "title": "What is end-to-end encryption and why it's such a confusing term", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-end-to-end-encryption-and-why-its-such-a-confusing-term/", "abstract": "Let’s say you sit in a coffee shop and want to check something. You connect to the local WiFi, open up a browser, and use the Google search. Conceptually, your phone is communicating with the Google servers and all you notice is that the search results appear. Internet protocols do a good job of hiding all the complexity of routing to a remote machine. browser google.com communication The browser communicates with a Google server But in reality, this connection goes through a lot of intermediary nodes , starting with the WiFi router you are connected to. browser WiFi router ISP backbone google.com Communication goes through intermediary nodes For example, a traceroute, a tool that can show the path each packet goes through to reach its destination, shows these routers along the way to the google.com server: We write articles like this regularly. Join our mailing list and let's keep in touch. These are only the ones that can be identified remotely. There are likely others that are hidden from the outside eye. As all packets pass through these nodes, whatever you send, all these nodes can read it. browser WiFi router ISP backbone google.com Can read the communication Communication goes through intermediary nodes This is why HTTPS and other encrypted communication protocols are useful. By encrypting the traffic so that only the intended recipient can decrypt it, you can use the same intermediary nodes without fearing they might listen in to the conversation. browser WiFi router ISP backbone google.com Can not read the communication HTTPS Transport-level encryption This is called transport-level encryption as it protects data in-flight , i.e. when it is transmitted between nodes. In the above example, the browser encrypts the data and only the Google server can decrypt it. It makes a private channel between the two participants. browser google.com secure communication The browser securely communicates with a Google server In the above example, the two ends (the browser and the Google server) are the only ones that can read the communication and it does not matter how many nodes it passes through and who controls them. In the coffee house example, you can be oblivious to all the routers in the communication stream as they are unable to read the things you send. This is end-to-end encryption. But what if you are communicating with a node that proxies the connection? Let’s say you found a search result interesting and clicked on the link. For the sake of example, the website you are visiting is using Cloudflare as a proxy. browser Cloudflare website HTTPS ? A website that uses Cloudflare as a proxy In this case, your browser connects to Cloudflare and Cloudflare connects to the website. The part between the browser and the proxy uses HTTPS so that the WiFi router, the ISP, and all the other parts of the Internet that stands between you and Cloudflare can not read what is sent over the wire. But what about the part between Cloudflare and the website? Is using encryption there makes this connection end-to-end encrypted? No, the connection between the browser and the website is not end-to-end encrypted as there is a box between them that can read the communication . It does not matter if all the edges use encryption if there is an intermediary node that can read the communication. Let’s switch hats and become an AWS architect! Your task is to run a website for a startup that is destined (at least in the pamphlets handed out to investors) to become the next Facebook with billions of users worldwide. You decide to use a fleet of EC2 instances behind a load balancer and CloudFront for worldwide content delivery. CloudFront works similarly to Cloudflare in the above scenario. It is a proxy that breaks the encryption and sends the traffic to the load balancer either by reencrypting it or in plain-text. Is this architecture end-to-end encrypted? Of course, you might say, as users use an encrypted connection that ends in your architecture. Architecture CloudFront Load balancer EC2 instances browser HTTPS The connection is end-to-end encrypted But wait, how is it different than the Cloudflare case? The users are communicating with your EC2 servers and there is a box that can read the traffic! browser CloudFront Load balancer EC2 instances HTTPS The connection is not end-to-end encrypted The problem with end-to-end encryption is that depending on how you pick the ends the same architecture can be considered end-to-end encrypted or not. Usually, in the case of one-to-one or group messaging, the two ends are the participants. You send a message to a friend via WhatsApp. If WhatsApp can read the message then it is not end-to-end encrypted. But what are the ends when you post the same message to the Facebook wall of a private group? In this case, end-to-end means communication with Facebook , even though you are still kind of sending a message to multiple people. It’s easy to confuse full in-flight encryption with end-to-end encryption. There are many questions about whether a connection with a TLS-terminating proxy still counts as end-to-end encrypted. It does not. If you look carefully, documentations usually not mention end-to-end encryption in these cases. In the CloudFront developer documentation : For example, you can configure CloudFront to help enforce secure, end-to-end connections using HTTPS SSL/TLS encryption . (emphasis mine) It does not say that the connection is end-to-end encrypted. It says that the connection uses TLS encryption for all communications end-to-end. This is not the same, as CloudFront can still read the requests and responses. Similarly, Cloudflare’s documentation : Cloudflare recommends end-to-end encryption of traffic between site visitors and the Cloudflare network and between Cloudflare’s network and your origin web server . (emphasis mine) An end-to-end encrypted connection to and from Cloudflare does not make the whole connection end-to-end encrypted. Again, Cloudflare can read the requests and responses. To make things more complicated, end-to-end encryption also depends on key management. When using HTTPS it’s done by the protocol itself, so we tend to think less about who generates and knows the key. But it’s also a vital part of the definition of end-to-end encryption. It is important that only the two ends know the keys and nobody else . Zoom’s is a particularly nasty case of broken key management. They said that the connection between the meeting participants is end-to-end encrypted because one end encrypts the data and only the other end decrypts it. But in their blog they say : To ensure this entire process meets the needs of our customers around the clock and around the world, Zoom currently maintains the key management system for these systems in the cloud. (emphasis mine) This means Zoom can read and modify the data sent from one participant to the other, even though it’s encrypted. This is not end-to-end encryption. After some controversy, the Federal Trade Commission investigated and forced them to implement proper end-to-end encryption. Usually, transport-level encryption is equated with HTTPS as it encrypts traffic between the two communicating parties, and wherever unencrypted HTTP is used is seen as non-secure. But it also depends on whether the data is encrypted or not. For example, a PGP-encrypted message is end-to-end encrypted no matter how it’s delivered. You can write it to a billboard and it’s still secure as only the recipient has the key to decrypt it. End-to-end encryption means that only the two ends of the communication can read the messages. It provides strong guarantees of privacy and thus it’s preferable to in-flight or at-rest encryption. But deciding whether a scheme is end-to-end encrypted or not can be tricky. It depends on the choice of the ends, where in-flight encryption is terminated, and what party manages the keys.", "date": "2021-01-15"},
{"website": "Advanced-Web", "title": "How to implement an exponential backoff retry strategy in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-implement-an-exponential-backoff-retry-strategy-in-javascript/", "abstract": "A Javascript application made a fetch call to a remote server and it failed. How should it retry the request so that it eventually succeeds while also minimizing the calls made? A backoff algorithm makes sure that when a target system can not serve a request it is not flooded with subsequent retries . It achieves this by introducing a waiting period between the retries to give the target a chance to recover. The need for a backoff builds on the observation that a service is unavailable when it is overloaded and sending more requests only exacerbates the problem. When all callers temporarily cease adding more load to the already overloaded service usually smooths the traffic spikes with only a slight delay. Waiting between retries JS Backend Failed call wait Failed call wait Success The backoff algorithm used determines how much to wait between the retries . The best configuration is actively researched in the case of network congestion, such as when a mobile network is saturated. It’s fascinating to see that different implementations yield drastically different effective bandwidth. Retrying a failed call to a remote server is a much easier problem and doing it right does not require years of research. In this article, you’ll learn how to implement a backoff solution in Javascript that is good enough for all practical purposes. But let’s first revisit the problem of choosing the backoff strategy, i.e. how much to wait between the retries ! Sending requests too soon puts more load on the potentially struggling server, while waiting too long introduces too much lag. The exponential backoff became the standard algorithm to use. It waits exponentially longer for subsequent retries, which makes the first few tries happen with only a slight delay while it reaches longer periods quickly. It has two parameters: the delay of the first period and the growth factor . For example, when the first retry waits 10ms and the subsequent ones double the previous values, the waiting times are: 10, 20, 40, 80, 160, 320, 640, 1280, … Notice that the sum of the first 3 attempts is less than 100 ms, which is barely noticeable. But it reaches >1 second in just 8 tries. Exponential backoff JS Backend Failed call wait 10 ms Failed call wait 20 ms Failed call wait 40 ms Failed call wait 80 ms Failed call wait 160 ms Success There are two pitfalls when implementing a backoff algorithm. First, make sure to wait only before retries and not the first request . Waiting first and sending a request then introduces a delay even for successful requests. And second, make sure to put a limit to the number of retries so that the code eventually gives up and throws an error. Even if the call eventually succeeds, something upstream timeouts in the meantime, and the user likely gets an error. Returning an error in a timely manner and letting the user retry the operation is a good UX practice. There are two types of operation in terms of retrying: rejection-based and progress-based. Let’s discuss each of them! This type of call either succeeds or fails and it is usually implemented as a Promise resolving or rejecting. A canonical example is a “Save” button that may fail and in that case, retrying means sending the request again. Rejection-based retrying JS Backend Call Failed Call Failed Call Success For illustration, this operation emulates a network call that fails 90% of the time: Calling this function without a retry mechanism is bad UX. The user would need to click the button repeatedly until it succeeds. A general-purpose solution that can take any async function and retry it for a few times before giving up: And to use it with the operation defined above: In the retry implementation, the depth argument indicates the number of calls made to the service so far. It determines how much to wait ( await wait(2 ** depth * 10) ) and when to give up ( if (depth > 7) {...} ). When the function calls itself it increments this value. Interestingly, this is a scenario where using return await makes a difference. Usually, it does not matter if an async function returns a Promise or its resolved value. But not in this case, as the try..catch needs to wait for the fn() call to finish to handle exceptions thrown from it. Note that this implementation retries for any error , even if it is a non-retriable one, such as sending invalid data. Another class of retriable operations is when each call might make some progress towards completion but might not reach it . A great example of this is how DynamoDB’s batchWriteItem call works. You define the items to store, and the call tries to save all of them. It then returns a list in the response indicating which elements failed. The retry operation only needs to include these missed items and not the whole request. Progress-based retrying JS Backend [1, 2, 3, 4, 5] Failed: [2, 3, 5] [2, 3, 5] Failed: [5] [5] Failed: [] Success As an illustration, this function emulates a progressing call to a remote server. It returns a progress field that goes from 0 to 1, and a result when the call is fully finished. It also needs to know the current progress , which is a parameter for the call: In this case, not reaching completion still resolves the Promise, which mimics the DynamoDB API. But a different API might reject it. Fortunately, it’s easy to convert a rejected Promise to a resolved one: .catch() . In the DynamoDB batchWriteItem example, the progress return field is the list of the unprocessed items, and the startProgress parameter is the items to save. To retry the above call until it reaches completion: To use this retrying call, use: Note that this implementation is not general-purpose as it builds on how the call returns progress. Implementing a retrying behavior with a backoff algorithm is better for UX (users see fewer errors) and also attempts to handle system overload by not flooding the called service. Using an exponential algorithm is a simple yet effective way to determine how much to wait between the calls while it also makes a sensible tradeoff between latency and the number of calls made.", "date": "2021-01-08"},
{"website": "Advanced-Web", "title": "How to test with a live domain name for free", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-test-with-a-live-domain-name-for-free/", "abstract": "Whenever I need to setup HTTPS for a system I’m working on I eventually need to use a domain. For example, when I set up a CloudFront distribution with a custom name, I need a Route53 Hosted Zone and an ACM certificate and that requires the Hosted Zone to be the authoritative name server. And for that, I need to own a domain name. Fortunately, I have one that I can use for this. But this requirement makes it hard for people to follow a tutorial involving domain names. Not everybody has a domain just lying around for testing purposes. That’s why I searched for a free alternative to owning a domain. Many services offer a free subdomain, but as I’ve found not all of them allow adding NS records, which is a requirement in most scenarios. We write articles like this regularly. Join our mailing list and let's keep in touch. The DNS system translates names to IP addresses . Since your computer can only connect to IP addresses, whenever you use a domain name this translation process starts and finds the address you need. The domain name resolution happens using a trust chain . There are several root name servers and the IP addresses of those are baked into any DNS resolver (this is called the root hints ). Whenever you enter a domain to the address bar, your browser goes to a resolver that in turn contacts the root name servers. browser m.root-nameservers.net b.hu t103.nlg.hu advancedweb.hu? IP Chain of trust in DNS resolution This is where the chain starts. The root name server responds with an NS record that points to another DNS name server . Then the resolver connects to that server, which also responds with an NS record pointing to another name server. This process goes on until it reaches the authoritative name server that responds with the IP address your browser will connect to . Because of this, you can create name servers for any domain name you want, but it will only go live if there is a chain from the root name servers . And that usually involves buying a domain name. When it becomes more pressing is when you need to configure HTTPS. You need a valid certificate , and for that, you need to prove that you own the domain name . This involves adding a record to the authoritative name server for the domain, and that requires a name server that you control and is referenced by the root name servers. And while you can use a self-signed certificate to test HTTPS for a webserver, it is not a possibility in many cases. For example, if you want to configure a CloudFront distribution to use a custom domain name, you need to add an ACM certificate. And that requires an Amazon-issued HTTPS certificate, and you can only get one if you can prove domain ownership. As a result, you need a domain name for testing the CloudFront configuration. My first thought was to find an ngrok -like service that provides a temporary subdomain where I can configure NS records. Unfortunately, ngrok itself does not support this use-case. So I opened the Free for developers list and started out the free DNS services listed there searching for a suitable one. I soon found out that most services use FreeDNS under the hood. Its webpage looks like it’s something from the early 2000s, and sure enough, it’s operational since 2001. This is a big plus, as it’s unlikely to go away in the next years, not like some modern services that come and go. I registered a free account and tried to add an NS record to a subdomain: Which did not work, apparently this feature needs additional administration and is not provided for free accounts by default: Because of this, FreeDNS and the solutions built on it are not a good fit for this use-case. The next service I tried was ClouDNS . It has been around since 2010, and unlike FreeDNS it allows adding NS records to free subdomains. After registration, I added a new DNS zone: Which supports a free zone: Gave it a name and then it shows the current name servers: While changing these NS records did not work, I could add another layer of subdomains with the name servers I needed and that worked. Let’s see how to use the ClouDNS subdomain to make a Route53 Hosted Zone the authoritative name server for a domain and then ACM to get an Amazon-issued HTTPS certificate for it! cloudns.cl route53testing.cloudns.cl dnstest.route53testing.cloudns.cl Route53 Hosted Zone NS NS NS Making the Route53 Hosted Zone the authoritative name server First, we’ll need to create a Route53 Hosted Zone for the domain name. The domain here is a subdomain of the ClouDNS zone, such as dnstest.route53testing.cloudns.cl , where cloudns.cl is the ClouDNS domain, the route53testing subdomain is the free zone, and finally the dnstest part is the name I’ll use for the NS records. The Route53 Hosted Zone lists its name servers, which will be the targets of the ClouDNS zone: On the ClouDNS console, add the NS records to the name that matches the Route53 Hosted Zone name: It takes ~1 minute to update (in theory, it can take a lot longer, but in practice it’s quite fast) then you can test that the DNS resolution reaches the AWS name servers. To see inside the hood, use dig : It’s easy to see the chain the DNS resolution followed: g.root-servers.net => cl2-tld.d-zone.ca => ns102.cloudns.net => ns-922.awsdns-51.net . That last part is the Route53 Hosted Zone, meaning it is the authoritative name server for that domain. To get an HTTPS certificate, go to ACM, enter the domain name, and choose DNS validation: This validation method requires adding a CNAME record to the domain name. Since the Route53 Hosted Zone is the authoritative name server for this domain, you need to add the record to that zone. The ACM Console makes it easy as it provides a button to do just that: The ACM service makes a DNS resolution to the subdomain in the CNAME record. You can see that it resolves to the value: The ANSWER SECTION contains the validation record. Since it matches the record ACM requires, it issues a certificate in a few seconds: This is a valid ACM certificate for a domain that is publicly resolvable and you can use it for testing anything that requires a live domain name. Testing features that require a live domain name requires some preparation. You need either a domain name you bought, or you need to opt for a free subdomain from a service that offers them. I’ve tried some such providers and found that ClouDNS allows setting NS records for a subdomain.", "date": "2021-01-05"},
{"website": "Advanced-Web", "title": "How to use DynamoDB batch write with retrying and exponential backoff", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-dynamodb-batch-write-with-retrying-and-exponential-backoff/", "abstract": "DynamoDB supports the batchWriteItem function that consolidates multiple item writes in a single request . It’s a great performance improvement as network overhead is a significant factor with DynamoDB and sending fewer requests reduce the CPU (and network) utilization of the caller. And especially with Lambda, where you are billed for the running time of the function, this translates into savings on the infrastructure. The batch write supports up to 25 write requests . They can be either a PutRequest or a DeleteRequest , and you can also mix them inside a single operation. This makes it suitable for mass writes and deletes. Note that it differs from the transactWriteItems function. While both can contain multiple writes to different items, the batchWriteItem writes the items individually and does not provide atomicity for the operations . Think of batch writing as multiple individual writes/deletes in a single network call. batchWriteItem differs from individual writes/updates/deletes in how errors are handled. When you use, for example, a putItem , it either succeeds or fails, and the latter is indicated by an exception: The source of the error can be one of many things. Maybe the service is experiencing degradation, there is a problem with the request, or there was a problem writing the item. Errors occur and you need to prepare for them. And batching amplifies the probability of errors. So, what happens when some of the items fail? DynamoDB sends back an UnprocessedItems field in the response that indicates which items failed to write. The request still throws an exception if the error affects the whole batch, but if only some items are unprocessed, it will be successful. The solution is to resend the batchWriteItem request with the unprocessed elements. And since the second batchWriteItem might also fail to process all items, a third attempt might be needed also. It’s a best practice to put an upper limit on retries, after which the operation throws an exception so that the caller won’t get stuck indefinitely. As with all distributed systems, retrying should not happen immediately. AWS also recommends a backoff algorithm: If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm . If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. If you delay the batch operation using exponential backoff, the individual requests in the batch are much more likely to succeed. Exponential backoff means the code waits longer and longer between retries. Increasing this time exponentially makes the first few tries in rapid succession while it reaches longer delays quickly. For example, waiting 10 ms after the first request, then 20 ms after that, then increase to 40, 80, 160, 320, 640, 1280, and so on. After just 8 tries it waits more than a second. Let’s make a safer batchWriteItem that implements retrying with an exponential backoff! Fortunately, the structure of the UnprocessedItems in the response matches the RequestItems in the request, so there is no transformation needed. This is a recursive algorithm that calls itself with the remaining elements while also keeping track of how many tries it made: And to use it, use the same RequestItems structure as you’d use for the batchWriteItem : The if (retryCount > 8) part controls when the function gives up. You can increase this if you want more tries as that would make the function more likely to succeed. But keep in mind that there is a tradeoff between the probability of success and speed. Each retry attempt (especially with the exponential backoff) lengthens when an exception is finally thrown. The exponential backoff is the await wait(2 ** retryCount * 10); part. It starts with 10 ms after the first try (the initial request) and doubles the wait for every subsequent request. Writing elements in a batch increases the probability of some of them won’t be saved. The batchWriteItem returns a success response in this case but it also indicates which elements weren’t processed. Using that function without checking this response field runs the risk of skipping elements that should be written to the database. A retrying algorithm with an exponential backoff makes batch writing a safer function as unprocessed elements are automatically sent again. Make sure to always handle skipped items.", "date": "2021-01-12"},
{"website": "Advanced-Web", "title": "RxJS: How to make an ordered mergeMap operator", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/rxjs-how-to-make-an-ordered-mergemap-operator/", "abstract": "In one of my projects, I needed to assemble a PDF document where different pages came from different sources and the result document had to contain all of them in order . I used the pdf-lib for everything related to documents. With this library, I can create a new document, copy pages from another one then save the result as a buffer with some simple commands: I had an async function that generated part of the result based on a config object. By calling this function with the different configs, I can generate the fragments, then use the copyPages + addPage to combine them: The problem was that the generatePages ran slow and I wanted to parallelize it. The obvious solution is to use a Promise.all with an async map and run all the generatePages concurrently. The above solution also takes advantage of an async reduce to build the result page in a serialized way. There are two problems with this solution. First, it runs all generatePages in parallel . I used Puppeteer to generate the PDF from HTML and that is memory-heavy. In practice, I could run 3-5 at a time, any more would use up all the available memory and the whole process would be a lot slower due to swapping. The second problem is that it generates all the input PDFs before it starts to assembly the result document . This added more memory pressure, so I was looking for a solution that has a controllable amount of concurrency and produces elements when they become ready and in order. The mergeMap , also known as flatMap , is a natural choice that came into my mind for this. It supports Promises and thus async functions nicely , and it has a third parameter that sets the number of parallel calls . Whenever a task is finished, it starts a new one immediately, but never more than the concurrency cap. Controlled concurrency map map e1 e1 e2 e2 e3 e3 e4 e4 This yields a nice structure: The above solution ticks the managed concurrency checkbox as well as it builds the result document when the inputs become available. But unfortunately, mergeMap emits elements as they are ready and does not keep the original ordering of elements . 1 2 3 mergeMap P1 P3 P2 This results in a document that has its pages out-of-order, which is wrong. Another RxJS operator, concatMap preserves the ordering, but it has no support for setting the amount of concurrency. It seems like there is no built-in operator to support this use-case. Fortunately, it’s not hard to use existing operators to create new ones. The operator below is an orderedMergeMap that emits the elements in-order and also supports setting the concurrency. Here’s the implementation that gets a Promise-producing mapper (an async function, usually) with a concurrency setting: Since it’s a pipeable operator, use it like any built-in one, passing the function and parallelization setting: In the above example, it feeds the reduce operator with the results in-order, no matter how long it takes for the generatePages to provide them. 1 2 3 orderedMergeMap P1 P2 P3 input mergeMap scan flatMap result 0 1 2 [r0, 0] [r2, 2] [r1, 1] { emitting : [r0], lastEmittedIndex: 0, results : []} { emitting : [], lastEmittedIndex: 0, results : [ [r2, 2] ]} { emitting : [r1, r2], lastEmittedIndex: 2, results : []} [r0] [] [r1, r2] r0 r1 r2 orderedMergeMap Internally, it uses 3 operators. First, a mergeMap calls the mapper function and it controls the concurrency. It also adds the index to the result, so that later operators know the original ordering of the elements. input mergeMap output 0 1 2 [r0, 0] [r2, 2] [r1, 1] The mergeMap produces the result and adds the original index Then a scan with a flatMap (= mergeMap ) structure that allows storing intermediate values as the result of the scan and also emitting multiple items. This is a useful structure in many situations where you don’t want to use local variables. The result of the previous scan is fed back to the next invocation, and anything that it returns in the emitting array will be extracted and emitted by the flatMap . scan flatMap result state emitting: [item1, item2] item1 item2 A scan combined with a flatMap provides internal state This structure moves all interesting logic inside the scan operator. It needs to keep track of the last element it emitted (the lastEmittedIndex ) and all the results that are out-of-order ( results ). The latter is a buffer for elements that need to wait for a previous result before they can be emitted. The implementation seems complicated, but it boils down to the emit array. It contains all results with a flag whether they can be emitted (all previous items are emitted) or not (waiting for a previous result). Then the emitting array in the result is the elements sorted by their original index that can be emitted, the lastEmittedIndex is the internal state to keep track of which elements were passed to the flatMap , and the results is the items that are still waiting. The orderedMergeMap operator provides a way to control the concurrency of the mergeMap while also preserving the ordering of the elements. The implementation above uses only standard RxJS operators. There are other solutions for this problem too, most using a local variable to store the out-of-order elements.", "date": "2021-01-01"},
{"website": "Advanced-Web", "title": "What is the principle of least privilege and why it's such a hard thing to achieve", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-the-principle-of-least-privilege-and-why-its-such-a-hard-thing-to-achieve/", "abstract": "In an access control scheme, policies can allow and deny operations. It’s a generic concept, but in the case of AWS, this control is implemented using IAM policies so that you can attach an admin user a policy that allows creating new S3 buckets. Then you can attach a policy to another user that prevents it from accessing a DynamoDB table. The less people can do in an account the more secure it is. “Least privilege” is a configuration of permissions where everybody is allowed to do what is required to fulfill their jobs but nothing else . Let’s say a user needs access to an S3 bucket. You can solve this in multiple ways, such as attaching the AdministratorAccess jolly joker policy that grants access to everything inside the account which includes reading objects from the S3 bucket. This definitely ticks the box of “give access” but it feels a bit extreme. To make things stricter, you can give only access to the S3 service which is a lot more restricted than the administrator policy. Going forward, you can give access to a single bucket and only for read operations. But there are still things to chip off from this permission set. For example, you can restrict the IP address the request must come from. We write articles like this regularly. Join our mailing list and let's keep in touch. You might also want to give time-sensitive constraints, such as limiting access to office hours. But this is not supported by AWS, so you can’t configure it. Least privilege is dependent on the service you are using. It’s easy to see why giving less access is a good idea. In the hands of a competent administrator, new users and services start with a small set of permissions. But over time, permissions tend to accumulate and no longer reflect what is strictly necessary. When somebody gets less permissions than the least privilege point defined above, they can not fulfill their jobs. A webserver might not be able to connect to the database, or a back-office worker is unable to access some documents. This is an operational problem. On the other hand, if the permissions allow more than strictly necessary, it’s a security problem . If there is a breach, this leniency gives the hacker more options. Maybe a system can access other databases in addition to the ones it uses, which makes a limited attack surface wider. These two types of problems behave differently in two important ways: what is the probability they occur, and how easy it is to attribute the problem to a person. These differences determine how permissions naturally evolve over time. There is an asymmetry between how likely an operational vs a security problem causes problems. When a policy is too strict and it prevents systems or users from doing their work, it is immediately apparent . Visitors to the website get error responses, or someone is complaining to their boss. There is a 100% chance this happens shortly after the permissions are modified. When a policy is too lenient, it enables or exacerbates a potential future security breach . One that might never come or might happen far in the future. Think about all the companies that go for years with virtually no cybersecurity incident and only get breached when they got into the spotlight. Such as in this case where an SQL injection vulnerability allowed deleting the database behind the webserver after a user opened a ticket in the Firefox issue tracker complaining about the browser warning users that the login form is served via HTTP. The site was working like this for a long time until this message appeared and the site got breached in hours: We have our own security system, and it has never been breached in more than 15 years. The longer something is not broken the more secure it seems and it also happens when security is a lot more important. For example, a memo from the space shuttle program observes this problem: The argument that the same risk was flown before without failure is often accepted as an argument for the safety of accepting it again. Security problems appear later and with a low probability. And we humans are particularly bad at assessing this. It is the “turkey problem” Nassim Taleb wrote in Black Swan: “Consider a turkey that is fed every day. Every single feeding will firm up the bird’s belief that it is the general rule of life to be fed every day by friendly members of the human race ‘looking out for its best interests,’ as a politician would say. “On the afternoon of the Wednesday before Thanksgiving, something unexpected will happen to the turkey. It will incur a revision of belief.” The process of devaluing low probability incidents also leads to “normalization of deviance” where in the absence of negative outcomes the unacceptable becomes acceptable. In 2014 an airplane overran the runway and crashed. During the investigation, it became clear that the pilot disregarded the safety checks before the flight, those that should be done before every takeoff. It became the norm because it did not result in any consequences until it did. The same thing applies to policies and IT security. A permission that allows too much access might never lead to a security incident. Day after day people get more accustomed to allowing more lenient security controls and at the same time get more confident that their systems are secure. The second factor is accountability. When an employee complains to their boss that they are no longer allowed to access a system and it hinders their work, a short investigation points to an IT admin who revoked that permission . But when a security incident years down the road happens, there is no single person to blame . Responsibility is smeared to the whole company. Security is in the hands of people who define access control on a daily basis. When they get the blame for accidentally removing essential permissions they will err on the side of their own jobs’ safety and leave excessive accesses alone . And after some time, it becomes the norm, and people who think otherwise will be the minority. When the majority accepts leniency as the norm, it transforms into peer pressure against anyone who thinks otherwise. In an extreme case, whenever someone raises doubts they get ridiculed, saying that “Our security is amazingly good” . With no negative effects, stifling security-related concerns will be “business as usual” and everybody who thinks otherwise is labeled as overly apprehensive. Unfortunately, money apparently does not solve this problem. The Twitch incident where they allowed public writes to an S3 bucket is a prime example that the big players are not immune to this. And as Twitch is a tech company, they can’t even say that they are not familiar with technology. In another case, healthcare data got exposed in a similar way. In this case, strict regulations and expensive audits that are needed to store sensitive data were not enough to stop this leak. Both of these cases happened because of exposed S3 buckets, and there are many other examples as this is happening regularly (just Google “S3 bucket negligence award”). But how can this happen? S3 buckets are private by default. Somebody had to explicitly grant public read permissions (and even disable the public access block more recently put in place by AWS). Worse still, there are only two ways to make a bucket public: either by attaching a bucket policy or setting the public-read ACL for the objects. Either of these are easily detectable by tools and most AWS security-related ones detect these misconfigurations. Public S3 buckets are both hard to configure and easy to detect , yet there is a constant stream of new breaches. What are the possible solutions? While there is no “do this and you’ll be fine” advice, adopting certain practices helps with security. Striving for simplicity is one of the most effective ones. Instead of giving permissions to individual users, using Role-Based Access Control or Attribute-Based Access Control greatly reduces the number of policies in a system. This makes it easier to reason about what each user can do and which ones are the outlying cases that need further investigation. Apart from consolidating permissions, you can also impose hard boundaries and thus reduce the number of connections between principals and resources. In AWS, if you move production resources to a separate account along with the people who need access to them, then you cut the number of fine-grained permissions to a fraction. If you have 8 developers, and 2 people responsible for operations, you can reduce the number of users in the production account from 10 to 2. Another good practice is to strive for observability . When policies are scattered around an account and possible across multiple accounts, it’s hard to get a “big picture” overview. As a result, everybody will have tunnel vision, focusing on small parts of the access control. An intuitive dashboard that shows who can do what is a great way to let anybody spot errors. Then automation helps enforce established security constraints. You can add tools that inspect the resources and spot problems, such as Config in AWS, and linters that prevents non-valid resources from deploying, such as tfsec . Automation works like tests in programming as they codify what would otherwise be an informal decision. You need to decide and code things once and they will be enforced automatically. And finally, conducting reviews , preferably by people who are not influenced by the “things have always been like this” mentality is a more costly but an effective way of keeping systems secure. Red team vs blue team wargame exercises are commonplace in cybersecurity, and you can also order third-party penetration testing . Just make sure that these are not one-time exercises but are redone periodically.", "date": "2020-12-29"},
{"website": "Advanced-Web", "title": "RxJS: How to use startWith with pairwise", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/rxjs-how-to-use-startwith-with-pairwise/", "abstract": "The pairwise operator is a great tool if you also need the previous value for every element. It uses a buffer with a size of 2, making it memory efficient as well. The problem is that it does not emit on the first element, making its results one item shorter than the input. 1 2 3 pairwise 1 2 2 3 Depending on the use-case, it might be a good thing. But if you need an element for every input item, combine it with the startWith operator: 1 2 3 startWith(0) 0 1 2 3 pairwise 0 1 1 2 2 3 This helps when you need to calculate differences with a known start point. For example, let’s say something starts from the 0 coordinate and the stream consists of points it goes to. Using pairwise in this case offers an easy way to calculate the differences: 10 50 40 pairwise 10 50 50 40 map 40 10 In this case, to know how far it moved, you need also to add the starting point of 0 with the startWith operator: 10 50 40 startWith 0 10 50 40 pairwise 0 10 10 50 50 40 map 10 40 10 Let’s say you want to draw circles on mouse clicks and also want to connect them with lines. Since lines need a start and an end coordinate, pairwise is a good operator for them. On the other hand, circles only need the current mouse coordinates. It’s tempting to combine drawing the two shapes in a subscription: The above implementation does not draw a circle on the first click as pairwise does not emit an element for that. A quick fix is to use pairwise and check the edge case when the first element is undefined: The problem with this is that it meshes two things, one that needs elements and one that needs pairs . A better solution is to separate the two and only use pairwise for the latter:", "date": "2020-12-25"},
{"website": "Advanced-Web", "title": "RxJS: The differences between first(), take(1), and single()", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/rxjs-the-differences-between-first-take-1-and-single/", "abstract": "With Observables listening for mousedown and mouseup events, implementing drag-and-drop requires knowing when the first mouseup happens after a mousedown event. For this, you need to get the first event in the mouseup stream for every event coming in the mousedown . It turns out, there are more than one ways to do this, and they seemingly do the same. The first() is the primary candidate for this use-case, but there is also take(1) that also returns the first element. Then there is the single() operator, which works a bit different but is also used to return the first element in some cases. So, what is the difference between these operators? Both of these return the first element in an Observable and they even have the same timing as they both complete the result after emitting the element. 1 2 first() 1 take(1) 1 The difference is when there are no elements in the input stream . In this case, first() throws an Error, while take(1) closes the Observable without any elements. first() take(1) When you are sure that the input Observable is not empty, it’s safer to use first() as it will report the empty case as an error. On the other hand, if an empty stream can happen, for example the element for the mouseup events is removed from the DOM, take(1) is better. This operator behaves a bit differently than the other two. It not only throws an Error for an empty stream (just like first() ) but also for one that has more than 1 element. 1 2 first() 1 single() This also changes when it completes the result Observable as it needs to wait for the second element (or the end of the input stream) to know that there is indeed only one element coming in the input. 1 first() 1 single() 1 The single() operator is a safer version of first() if you want to make sure that only a single element is emitted in the input Observable. The first() and the single() operators also support the predicate argument to filter the elements. Apart from this, first() also supports the defaultValue that it returns in case of an empty Observable. take(1) supports neither. These are merely syntactic sugar that you can emulate with the filter and the defaultIfEmpty operators: In the case of single() , providing a filter changes how it reacts to empty streams. If there are no elements in the Observable then it emits an Error. But if there are elements but they are filtered out, it emits undefined instead. The first() operator emits an Error for an empty Observable, while take(1) emits nothing. The single() operator also emits an Error for the second element in the Observable. This also changes the time it emits its result as it needs to wait for the end of the stream (or the second element).", "date": "2020-12-22"},
{"website": "Advanced-Web", "title": "What is the async disposer pattern in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-the-async-disposer-pattern-in-javascript/", "abstract": "A recurring pattern is to run some initialization code to set up some resource or configuration, then use the thing, and finally do some cleanup . It can be a global property, such as freezing the time with timekeeper , starting a Chrome browser with Puppeteer, or creating a temp directory. In all these cases, you need to make sure the modifications/resources are properly disposed of, otherwise, they might spill out to other parts of the codebase. For example, this code creates a temp directory in the system tmpdir then when it’s not needed it deletes it. This can be useful when, for example, you want to use ffmpeg to extract some frames from a video and need a directory to tell the ffmpeg command to output the images to. A similar construct is console.time that needs a console.timeEnd to output the time it took for a segment of the code to run: Or when you launch a browser, you want to make sure it’s closed when it’s not needed: All these cases share the try..finally structure. Without it, an error can jump over the cleanup logic , leaving the resource initialized (or the console timing still ticking): In other languages, such as Java, this is known as try-with-resources and it is built into the language. For example, the BufferedReader is closed after the block: This builds on the AutoCloseable interface’s close method so it’s easy to adapt to custom resource types. But there is no such structure in Javascript. Let’s see how to implement one! One problem with the try..finally structure we’ve seen above is how to return a value from inside the try block . Let’s say you want to take a screenshot of a website and want to use the resulting image later. A suboptimal solution is to declare a variable outside the block and use it to store the image: A better solution is to use a function : This is the basis of the disposer pattern. The difference is that instead of hardcoding the logic inside the try..finally block, it gets a function that implements that part: The withBrowser function contains the logic to launch and close the browser, and the fn function gets and uses the browser instance . Whenever the argument function returns, the browser is automatically closed, no additional cleanup logic is needed. This structure provides an elegant way to prevent non-closed resources hanging around. An interesting aspect of this pattern is that it is one of the few cases where there is a difference between return fn() and return await fn() . Usually, it does not matter if an async function returns a Promise or the result of the Promise. But in this case, without the await the finally block runs before the fn() call is finished. A potential problem is when there is some task is running in the argument function when it returns. This can happen when it starts an asynchronous task and does not wait for it to finish. In this case, the cleanup logic runs and closes the browser instance that can cause an error. It is usually the sign that an await is missing. The async disposer pattern is a useful abstraction when there is a resource that needs cleaning up after usage. Using a function that handles the lifecycle of the resource and separates it from the logic that uses the resource makes sure that the cleanup is run.", "date": "2020-12-11"},
{"website": "Advanced-Web", "title": "Encryption options for S3 objects", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/encryption-options-for-s3-objects/", "abstract": "Encryption in the cloud is a coin with two faces. On one side it’s all about algorithms, compliance requirements, dedicated encryption hardware, and third-party audits . For example, the AWS documentation says that enabling S3 encryption does this: Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256). When you use this feature, you are protected by the best practices and the best available algorithms. The other face of the cloud encryption coin is what is observable . If I enable this encryption, what change can I see? Is there a request that is now denied? Or do I get the encrypted data for an operation instead of the plaintext? When I think about the cloud I always imagine a big box with a lot of levers and lights. I can read the documentation to know how it works on the inside but I’m also interested in what changes if I push a particular button? In other words, if AWS decides one day that it no longer encrypts the objects, would I notice ? This article is about what are the observable benefits of encrypting S3 objects in different ways AWS supports. Let’s establish a baseline first! If you put an object into an S3 bucket, it is private. If an IAM entity (user, role, service, anonymous user) has no permission to read it, the S3 service denies access: The get-object operation goes to the S3 API which is protected by IAM. This means whether a request is allowed or not is determined by IAM policies. The easiest way to give permission to read the object is to attach a policy to the IAM user or role: With this attached, the get-object returns the object: This gives 2 outcomes. First, the user has no permission to access the object in which case the operation is denied. And second, the user has the necessary permission, in which case they can read the contents. Let’s turn on server-side encryption and see what changed! The first thing we see is that the Console shows that the object is indeed encrypted: This is also called “seamless encryption” as it’s a push-button solution. There are no keys to manage and no change in how the object is stored or retrieved. Everything is managed inside the service. The same command gets the object: And whether the contents or an Access Denied is returned is determined by the IAM policies. There is no observable difference between SSE-S3 and no encryption at all . The SSE-KMS encryption scheme moves encryption out from the S3 service into KMS, AWS’s dedicated encryption service. This is again transparent, as they communicate behind the scene. When you put an object with this encryption, the S3 service asks KMS to generate a key, then it encrypts the object with it, also storing the encrypted key next to the data. When you retrieve this object, S3 asks KMS to decrypt the key, then uses that to encrypt the object itself. This process is called “envelope encryption” and it reduces the number of calls to the dedicated hardware (KMS). There are two types of KMS encryptions: one that uses an AWS managed key (the key is called CMK in KMS’s terminology), and one that uses a customer managed key. This key is created for you when you start using SSE-KMS and you can not manage it (hence the name “AWS managed”). On the S3 Console, the object is reported as encrypted with an AWS-KMS master key: Apart from this text, the object works like before. The command to retrieve it is the same: And also there is no change in the outcomes. When the user has access via IAM policies, the object is returned, otherwise it’s an Access Denied error. There is an observable difference though but in an entirely different part of AWS. Using KMS incurs costs for every encryption and decryption and that shows up in the monthly bill. KMS allows creating and managing your own keys. This allows adding more keys apart from the AWS managed ones and you can use them to encrypt S3 objects. On the KMS Console, you see the keys you manage and you can add more here: Then when you upload the object you can specify this key to use for encryption. On the S3 Console, you’ll see that object is encrypted with this key: The command to get the object is still not changed: But now there is an observable difference in access control . If you try the above command with the user that has the s3:GetObject permission it returns an “Access Denied” error. This is because a user needs access to the KMS key as well as to the object. This is another IAM policy. To give access, you can add kms:Decrypt permission to the user. This policy also allows encryption rights so that the user can also upload objects encrypted with this key. In effect, the user now needs access to the object and the key to get the object. Other than the requirement of more permissions, the result is still either the plaintext contents of the object or an “Access Denied” error. The observable difference is the requirement of the additional permissions. When is it useful? This type of KMS encryption adds a data-centric permission in addition to the service-centric one. You can use the same key to encrypt data in Redshift, DynamoDB, and other databases and you have a central place to manage access to the data in all places. This is especially important when it’s the same data, for example when you archive it from DynamoDB to S3. In this case, if you mistakenly give access to either of the services, the KMS key still protects the data itself. But this comes with additional costs. Each customer-managed CMK costs $1/month on top of the KMS encryption/decryption costs. In this encryption scheme, you provide the encryption key when uploading the object and AWS promises to forget it immediately. As a result, you need to send the same key when you retrieve the object. This changes the command to upload the object. In addition to the bucket and the key, you also need to define the algorithm, the key, and the MD5 of the key. This is an example request that uses the gL6RsUG2fPElqDyMghs1yCrRJMJFLgR9MN key for the encryption: Strangely, the S3 Console does not report the object as Server-side encrypted: Since getting the object now requires the encryption key, you need to specify that in the request. If you don’t, the S3 service reports an error: To get the object, use the same arguments. This returns the contents of the object as the encryption is still managed by the service. To check that an invalid key does not work, let’s use a different one. In this case, S3 returns an “Access Denied” error. So, what is SSE-C in practical terms? You set a key when you upload the object, then you need to use the same key to get it later. “Server-side encryption with customer-provided encryption keys” is a fancy name for this. More colloquially, it’s called a password . In all the cases the response was in plaintext, there is no way to access the encrypted data when using server-side encryption in S3. As a result, there is no observable evidence that encryption happens at all . Because of this, it’s better to think about how using these schemes affect access control instead. With SSE-S3 and SSE-KMS when using the AWS managed CMK, access control is the same as for non-encrypted objects. With SSE-KMS when using a customer managed CMK an additional layer of access control is enabled: the access to the KMS key. This enables inter-service permission control of data. And finally, SSE-C allows a key to be set during the uploading and every retrieval needs the same key. This is effectively password-protection.", "date": "2020-12-15"},
{"website": "Advanced-Web", "title": "My AWS wishlist", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/my-aws-wishlist/", "abstract": "This time of the year is about wishes, so here are the things that would make life easier for developers working on systems running in AWS. I’m sure I’m not the first one to think about these and they cover only a small subset of all AWS services. Some things are supposed to work across all services in AWS. For example, using tags to define permissions (also called Attribute-Based Access Control) is based on tagging resources and identities and defining policies to grant access based on matching values. If you read the documentation, it seems to be a great way for scalable permission management. It seems to be a feature of the AWS cloud that you can use tags. But it’s not, it’s a feature of individual services . The reference documentation shows the services and which ones support “Authorization based on tags” and which ones do not. The result is that you can only used tags for permissions in some cases but not in others. A similar problem is with the lack of universal support for resource-based policies. Again, some services support it, some don’t so you can’t depend on being able to define permissions on the resource-side without first checking that the services you plan to use support it. Then there is CloudFormation. You can use it to automatically configure services in AWS. Except that it does not support all services, and it’s a constant stream of announcements that a new service or change becomes supported. All these things are supposed to be “AWS-wide” but in reality they are “service-specific” making it hard to reason about how to configure them and it’s a contant source of confusion. They should be supported in all services, from the start, with all updates immediately reflected on them . Many things work differently in different services that should be universal concepts in AWS. S3 bucket policies and KMS key policies both control who can do management operations on the resources, such as they can prevent deleting an S3 bucket or a KMS key. But root users are exempt from bucket policies, while if you lock yourself out with a key policy you need to contact support . Why is there a difference? And this is not an isolated thing. Most resource-based policies are effective for management operations. But a policy attached to an IAM role can only influence who can assume it and not, for example, who can delete it. Is it because it’s called “trust policy”? But it’s a policy attached to a resource, which is pretty much what resource-based policies are. Still, it’s different. Is the policy attached to a Lambda function can control lambda:DeleteFunction or only lambda:InvokeFunction ? I don’t know and I don’t see any other way of knowing this than trying it out. When the same thing works differently in different parts of AWS it makes it hard to know what is possible without extensive experimentation with every single service you intent to use. And that’s a frustrating experience. Concepts that seem AWS-wide should be supported the same way in all services. AWS provides a bunch of managed policies that allow an easy way assign permissions to users. If you want to allow only reading s3 buckets, you can add the AmazonS3ReadOnlyAccess , and if you want to allow managing Lambda functions then the AWSLambdaFullAccess . This allows service-level access control without writing any kind of policies yourself. But if want anything more granular, it becomes exponentially more difficult. IAM policies protect AWS APIs. When a request reaches an API, it first assembles a request context with all the data that is available at this moment. This includes the Principal (who is doing the request), the Action (what is being done) and the Resource (on what). Apart from these, IAM also collects a lot of metadata , such as the IP address of the connection, whether the keys used in the request are authenticated with an MFA device, the tags that are attached to the Principal and the Resource, and a lot more . In an IAM policy, you add Conditions that check the values in the request context. For example, you can deny S3 bucket deletes without MFA authentication, restrict the IP address range to the company IP block, or only allow access to S3 objects that have a specific tag attached to them. This sounds great, but in practice it’s mostly reading the AWS documentations, trying out something, seeing it not working, then trying to fix it by reading the documentations again. It’s a frustrating experience. For example, the aws:ResourceTag/<key> is a global condition key that is present in all requests. Use this key to compare the tag key-value pair that you specify in the policy with the key-value pair that is attached to the resource. Since S3 objects can be tagged, it means that this contains the object tags? Nope, there is a separate key for that, the s3:ExistingObjectTag/<key> . Or when API Gateway calls a Lambda function, you can use the aws:SourceArn to give access to a specific API Gateway. Similarly, an SQS queue can allow a specific CloudWatch Event Rule via the same condition. Does that mean that an IAM role’s trust policy can limit the Lambda function that can use it? Nope. And when you encounter something that should work but isn’t, the only resource is the documentation AWS provides. There is zero visibility what is wrong with the policies that are misbehaving. There should be a service to debug IAM requests . I’d love a “request recorder” that I could use to selectively record live requests and see all the metadata in the request context exactly how IAM sees them. This would remove the quesswork of what condition keys are available in the exact scenario I’m interested in. As an added benefit, it could also show the steps in the policy evaluation logic . The session policy is an additional security mechanism to control access. It’s a drop-capabilities style control, as the principal willingly put additional restrictions on itself. It is an optional parameter of the assume-role call: When the resulting role session makes a call, the session policy must also allow the operation, in addition to its identity policies. When is this useful? Let’s say you have a multi-tenant application, storing data for multiple customers in separate S3 buckets. The application handles all customers, so it needs access to all buckets. But when the program determines which customer is making the request, it can put a session policy to limit itself to a single customer bucket. If there is a bug later in the code, this session policy prevents compromising the other buckets. The problem is that it’s only supported for assume-role calls, so you can only use it when you assume a role. And this complicates its usage in common use-cases. A session policy should be able to be applied to already assumed sessions , such as when a Lambda function is running. This way, it would be easy to put these additional safeguards without needing to reassume the execution role. Similarly, it would be great if IAM access keys could support session policies via the get-session-token call. Currently, an IAM role’s trust policy can only define the lambda.amazonaws.com service but not the specific function. Since most execution roles are for a single function, it would be great if this could be enforced in the role itself . Cold starts, also called first requests, happen when the Lambda service needs to start a new instance of the function. When it happens, the request needs to wait the setup process when the service deploys the code to a new machine. Worse still, the request also needs to wait while the execution environment is set up and for any application-specific setup process. This makes it noticably longer than other requests. This cold start can easily be orders of magnitude slower than other requests. At one point I worked on an app that precompiled a bunch of JSON schemas during the first request. It took ~10 seconds, then all the later requests were between 50 - 100 ms. This code demonstrates this behavior: How the Lambda service works today is that the request waits until the cold start is finished. If there is 1 instance running and 2 requests comes at the same time one will take 50 ms and the other 10 seconds! If the second request is simply queued, it would be finished in 100 ms. As a result, there will always be some requests that take 10 seconds and that forces me to bring this down to an acceptable level even when I don’t expect spikes in traffic. Lambda should do a warmup request when it needs to start a new instance and queue the requests to the already running instances and only start using the new environment when it’s properly warmed up. Creating accounts via Organizations is easy to automate. It is just an API call and it also creates a role with Administrator access so that you can also script how to setup resources inside. Since separate accounts provide limits to the blast radius (the damage a security breach can do) it’s a good practice to use many of them. I remember seeing a presentation where a representative a from company told the audience that they are using thousands of AWS accounts and they have a fully automatized process to create more. That’s great but I wouldn’t trade places with the person who is tasked to get rid of those accounts when the company decides to downsize its AWS footprint. While creating accounts is a scriptable process down to the last detail, deleting an account is an entirely manual task . You need to set the root user’s password by clicking a link in a recovery email, then submitting a form to delete the account. There is no API to do any of these steps automatically. There is a great video from Ian Mckay who made a solution using Puppeteer and a cloud CAPTCHA-solving service to automate this. Another problem case is when you want to provide a lab-like environment to untrusted people. You can create a sandbox account, put a restrictive SCP to prevent excessive damage, then give access to the person to experiment with the services. But what happens when this user is finished? If you create a new account for every lab session you’ll end up with a growing number of unused accounts. If you reuse them you need to delete all resources which is a challenge. Deleting an AWS member account should be as hard as creating one. When you start using Organizations that account becomes the management account. It can create member accounts and also provide SCPs to restrict what they can do. The problem is when you start creating resources in the account before start using Organizations. The best practice is to use resources only in member accounts, but what to do when you already have things that are hard to move, such as Elastic IPs, S3 buckets, and other resources? It would be a great if the management account could be demoted to a member account with all the billing information moved to a new account. This would make adapting a multi-account setup and SCPs easy: just create a new account and promote it to be the top of the tree. Then you could add policies and start separating resources without the need to migrate anything. You can set billing notifications that let you know when the projected spending crosses a threshold. This can turn into a security control so that you’ll get an advance warning when something is consuming too much which might indicate an error or a breach. It’s great when you run production systems that you want to keep online at all costs. But what if inside a developer’s sandbox account or a side project that goes awry? It’s usually a sign of problems and the primary course of action is damage limitation then. And this is what AWS does not support at the moment. You can not put hard restrictions on the monthly bill. Considering that even a single API call can cost more than a million dollars (yep), a problem that starts out as a small breach in a non-essential account can spiral out of control and all you can do is contact AWS and ask for a refund which you may or may not get. There should be controls over the maximum possible spending in an account. I imagine it would be a 2-tiered setup. When the actual monthly bill reaches the lower one then all API calls are denied and restartable resources are stopped. This would prevent invocations to Lambda functions, stop all EC2 instances, and deny creating any new resources. Then the higher limit would completely eliminate all cost-incurring things in the account, such as S3 buckets, databases, EBS instances. These limits would allow choosing between “up and running no matter the cost” and “shut down if there is a problem”. Combined with the ability to programmatically remove an account , it would be possible to safely create an account and give it to an untrusted party. With budget controls you could make sure it can not overspend and you could delete it without a trace when it’s not needed. Also, it could prevent overusing the free tier. When new users sign up for AWS, it’s a common source of complaints that not everything is free during that period. Launching a large EC2 instance and keep it running is expensive and it requires some knowledge of the platform to know that it’s not covered in the free tier. The time it took for CloudFront to deploy even the slighest of changes used to be 25 minutes. Now it’s under 5 minutes. This is a great improvement, but it’s merely a step from “unbelievably slow” to “terribly slow” territory. During development, it should not be more than 10 seconds. I see all the reasons why it’s a very hard thing to do. But the thing is, during experimentation there is no need for the full power of the global CloudFront network. Introduce a Price Class 0 that is limited to a single region (this would also help with Lambda@Edge logging ) and push updates to it instantly. When a Lambda function is used in a CloudFront distribution it is replicated across the edge locations. This makes them run close to the users and that reduces latency. This replication happens during the deployment of the distribution and it’s managed entirely by AWS. If you’ve ever tried to destroy a stack with a Lambda@Edge function you’ve certainly met this error : It turns out that deleting the CloudFront distribution does not involve deleting the function replicas it just marks them for deletion. Then a process comes and in a few hours cleans them up. But until then, you can not delete the Lambda function with the rest of the stack. This should be an implementation detail and should not prevent deleting the function. When a request reaches the CloudFront distribution, it uses the path pattern of the cache behaviors to determine which origin to send the request to. This is called path-based routing. But the path pattern of the cache behaviors only help with choosing the origin , but they won’t change it in any ways. A request with /api/user gets matched by the /api/* path pattern, but when CloudFront sends the request to the origin it uses the full path as /api/user . Cache behaviors Origins /api/* * API bucket visitor «APIGateway2» API [] «S3Bucket» Bucket [] /api/user /api/user /api/user /index.html /index.html /index.html Request forwarding This can cause problems when the backend expects requests coming to / instead of /api/ . Apart from APIs, it’s also a problem with S3 buckets. For example, if you have app1 and app2 and want to serve their static files from /app1/ and /app2/ , respectively, then you need to prefix the files in the bucket with that path too. This forces a change in how the files are stored because how the client connects to CloudFront . The solution for now is either to adapt the backend or use Lambda@Edge to change the path of the origin request. The latter is a cleaner solution as it does not force the backend services to change solely because of how clients view the content, but it brings a lot of complications and costs. It would be great if there was a config option in the CloudFront distribution that allow changing the origin path based on a configurable pattern. Even something simple, like a regex with a capture group would do it. Specifying ^/api(/.*)$ could capture everything after the /api part and cut the prefix before forwarding. S3 supports a bucket website endpoint that exposes the contents as a website. This uses a generated URL in the form of http://<bucket>.s3-website-<region>.amazonaws.com . You need to give public access to the contents, but it provides a simple way to host web content straight out of a bucket. But this endpoint is HTTP only and does not support HTTPS. Because of this, it is not suitable for browsers, as they flag the page as “Not secure” or “Mixed content”. To solve this, you need to add TLS termination either with a CloudFront distribution or with a third-party service . These solutions also help with adding a custom domain name, but what if you don’t need it? I don’t see any constraints that makes it impossible for AWS to host an S3 static website available via HTTPS . It would make it easier to host sites or assets in a bucket with a simple configuration. SSE-S3 is one of the encryption schemes available for S3 objects. This is seamless encryption, which means all the encryption and key handling is done by the service and none of these details are observable to the consumers. It always puzzled me why this option exists. Enabling it does not change any of the API calls, and it does not prevent any accesses that were previously allowed. Turning on this option has no observable effect. Remove this setting and store all objects encrypted .", "date": "2020-12-18"},
{"website": "Advanced-Web", "title": "How Firefox's HTTPS-only mode solves the first insecure request problem", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-firefoxs-https-only-mode-solves-the-first-insecure-request-problem/", "abstract": "Firefox recently announced that it will only connect to HTTPS pages and you need to disable it explicitly if you want to use unencrypted HTTP. It’s opt-in at the moment and you need to enable it in the browser but hopefully, it will be the default behavior in the future. This change is in-line with the recent shift from positive (padlock) to negative (“Not secure” text) indicators to inform users whether the page they are visiting uses transport-layer security or not. I find Firefox’s initiative a good step toward the secure web as it solves a particularly hard problem: the insecure first request . The current behavior of most browsers when you enter a webpage ( example.com ) is to send an HTTP request to http://example.com . This uses an unencrypted channel and flags the page as not secure. Websites then can upgrade the connection to HTTPS by sending a redirect to https://example.com with a HTTP 301 status code. For the end-user, this is an invisible process as it happens automatically. We write articles like this regularly. Join our mailing list and let's keep in touch. The first request goes to the HTTP endpoint browser http ://example.com https ://example.com GET 301: redirect GET index.html Modern content delivery platforms make this setting easy. For example, Cloudflare has a dedicated Page Rule that upgrades all requests to HTTPS when needed: Similarly, CloudFront allows enabling the HTTP endpoint for the distribution but only to send the redirect: With this setup, a visitor gets to the HTTPS endpoint and sees the padlock. This gives a sense of security. But this setup still makes an unencrypted connection as the browser sends the initial request to the HTTP endpoint and this opens the possibility of a transport-layer attack. If somebody can modify the response, they can keep the visitor on HTTP, which they control, or redirect them to a different site. And all while the website deploys a secure site and does everything to prevent it. An attacker can redirect to a different site browser MITM https ://evil.com http ://example.com https ://example.com GET http://example.com 301: redirect https://evil.com GET index.html There are patchwork solutions to this problem though. The HSTS (HTTP Strict Transport Security) header tells the returning visitors to use only the HTTPS endpoint. For example, this header tells the browser to remember this setting for 7 days: If you visit a site regularly, this makes all your connections go to the HTTPS endpoint. But a site you’ve never connected to still uses HTTP. To solve this, browsers started embedding a list of sites that are known to prefer HTTPS connections. This is the HSTS preload list and websites can opt-in with the ; preload extension to the HSTS header: When a site is on this list then the browser won’t use HTTP to connect to it, only HTTPS. This solves the first request problem. As a user, you can use browser extensions that change the default behavior of the browser you are using. The HTTPS Everywhere which is maintained by the EFF uses a separate list of websites, called the HTTPS Everywhere Atlas that fulfills a similar purpose as the HSTS preload list but it’s maintained not by webmasters but contributors. Even if the owner of the site does not know about this extension, people can add it to the list, and all users of the extension benefits. A more universal solution is implemented for example in Smart HTTPS . This makes the browser connect to the HTTPS endpoint first and only if it’s unavailable try the HTTP one. If the website is available on the former this behavior prevents any insecure requests. The main flaw of this approach is that it automatically falls back to HTTP, so an attacker can block the HTTPS response then change the resulting HTTP call. With the recent change, Firefox makes a request to the HTTPS endpoint only . If it is not available, it will prompt the user if they want to load the page over HTTP. For webmasters, this means there is no need to open the HTTP endpoint just to redirect users as they won’t even connect to it. Of course, it needs that all users use a browser using HTTPS first, but hopefully, we’ll get there. This simplifies deployment a bit and it does not rely on complicated solutions like getting on the preload list. For users, it solves the first request problem for all websites that support an HTTPS endpoint. You no longer need to rely on the webmasters to submit their sites to the preload list nor install any extensions. If the site is available over HTTPS all connections to it will be secure. If the site is available only on HTTP then it will add an additional hurdle to visit as you need to disable HTTPS-only for that site . This is a strong negative indicator and might push websites to move over to HTTPS. As a user, you have the choice to visit the page knowing it will connect via HTTP. The more problematic case is when the page loads over HTTPS but it’s broken. This can happen when the resources on it do not load via HTTPS or when the site serves different content on that endpoint. Firefox’s move is a bold one but it’s definitely in the right direction. This provides a solution to a hard problem, and it makes obsolete the backstop solutions implemented today: the HSTS preload list and the browser extensions. And what if you do want somebody to intercept and modify the website? Think airports and hotels that redirect to their login page but they rely on HTTP to do this. In this case, use a dedicated page for this purpose.", "date": "2020-12-08"},
{"website": "Advanced-Web", "title": "How to cache ssm.getParameter calls in a Lambda function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-cache-ssm-getparameter-calls-in-a-lambda-function/", "abstract": "Storing secrets in the Systems Manager Parameters Store enables you to decouple the application logic from the secrets . This means the person, or process, who is doing deployments does not need to know the value of the secret, only the name of the parameter . This allows securely storing things like access keys to third-party systems, database passwords, and encryption keys. With correct permissions, the Lambda function can read the value of the parameter by making a getParameter call to the SSM service. Environment variables are a similar construct that allows passing arbitrary values to a function. But separate storage for secrets allows better decoupling. Everybody who can deploy the function by definition needs to know the values, which might not be acceptable. But while environment variables scale with the function, SSM does not. It has a maximum throughput , defined in transactions per second (TPS), which is only 40 by default. Since Lambda can handle traffic several orders of magnitude, this can be a bottleneck. AWS reacted like a good salesman, adding an option to increase the limit to 1000 TPS for a higher cost. This should be enough for almost all use-cases. But why pay for something that you can solve yourself? To add a parameter, go to the AWS Systems Manager service and select the Parameter Store item under Application Management. This lists all the parameters defined in the region and you can add new ones. The Parameter Store is a simple key-value store. Parameters have a name and a value associated. The SecureString type is a String encrypted with KMS. This allows the WithDecryption parameter that allows getting only the cyphertext. Here’s a secret with the name test-secret that is a SecureString: By default, the Lambda function can not read this value. Let’s add a policy that grants access: Finally, the function needs to know the name of the parameter. This avoids the need to hardcode this value and it allows deploying identical environments. An environment variable is a good place to store it: This is all the function needs to access the secret. Whenever the value is needed, just read the name from the environment, then issue a getParameter call: The above code issues the ssm.getParameter call every time the function is run. This limits the scalability of the function to the maximum TPS of the SSM service, which is by default 40. Let’s see how we could do better and issue a lot fewer requests to the service! Lambda reuses function instances . The first request starts the code on a new machine, but subsequent calls reach the same running instance. This enables per-instance storage, as all the variables that are declared outside the handler function are still accessible during the next call. This allows an easy caching mechanism. Initialize an object that is shared between the calls, and in the handler function use it to interface with the SSM. The cache object can then store the last retrieved value and can decide when to refresh it. Here’s a caching function that implements this: This builds on the async function serializer pattern as while getting the parameter returns a Promise, it runs only one call at a time. This makes sure that when the value is fetched from SSM it will happen only once even if multiple calls are waiting for the result. The function returns another function that fetches the parameter’s value: Cache time is a tradeoff between freshness and efficiency. The longer the value is in the cache the more time it needs to pick up the changes, but it also makes fewer calls. Also, don’t forget that this caching is per instance . While the Lambda service reuses instances whenever possible, it also liberally starts new ones and those come with separate memory storages. This makes cache time calculations dependent on the number of instances as well as the function invocations per second. Let’s say a huge application uses 1000 instances at a given time. This should be a reasonable upper limit. In this case, the 40 TPS limit of the standard throughput of the SSM translates into 25 seconds cache time. Based on this back-on-the-envelope calculation, a cache time between 30 seconds and 5 minutes should be a good value.", "date": "2020-12-04"},
{"website": "Advanced-Web", "title": "How to serialize calls to an async function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-serialize-calls-to-an-async-function/", "abstract": "Let’s say you have a function that does some calculation but only if it hasn’t been done recently, effectively caching the result for a short amount of time. In this case, subsequent calls to the function affect each other , as when there is a refresh all of them need to wait for it. An implementation that does not take concurrency into account can potentially run the refresh several times in parallel when the cache expires. This implementation is broken: When two concurrent calls come when the function needs to refresh the value, the refresh function is called twice: The correct way to handle this is to make the second function wait for the refresh process without starting a separate one. One way to do this is to serialize the calls to the caching function. This makes every function to wait for all the previous ones to finish, so multiple calls only trigger a single refresh process. This can be done in a variety of ways, but the easiest one is to make the functions run one after the other. In this case, when one needs to refresh the value, the other ones will wait for it to finish and won’t start their own jobs. Another use-case I needed a solution like this is when backend calls needed a token and that token expired after some time. When a call hit an Unauthorized error, it refreshed the token and used the new one to retry. Other backend calls needed to wait for the new token before they could be run. In this case, it wasn’t just performance-related as a new token invalidated all the old ones. The trivial solution is to use await that achieves serialization easily. After all, that’s what that keyword is for. But that requires collaboration between the calls , and that is not always possible. For example, when the function is called in response to multiple types of events, await is not possible to coordinate between them: In this case, the function call must do the coordination. The solution is to keep a queue of Promises that chains them one after the other. It is just a few lines of code and it is general purpose, allowing any function be serialized: Call 1 (...args) Call 2 (...args) fn(...args) () => {} fn(...args) () => {} Promise.resolve() .catch result .catch result .then .then Queue structure The Promise.resolve() is the start of the queue. Every other call is appended to this Promise. The queue.then(() => fn(...args)) adds the function call to the queue and it saves its result in res . It will be resolved when the current and all the previous calls are resolved . The queue = res.catch(() => {}) part makes sure that the queue won’t get stuck in rejection when one part of it is rejected. Wrapping the caching function with this serializer makes sure that a single refresh is run even for multiple calls:", "date": "2020-12-01"},
{"website": "Advanced-Web", "title": "Interactive API documentation using Swagger UI deployed with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/interactive-api-documentation-using-swagger-ui-deployed-with-terraform/", "abstract": "When a backend provides an API how do you provide documentation for people who want to use it? It can be an informal process, using documents that describe the available paths, and how each of them should be called. This can be an ad-hoc list of methods and paths: Maybe accompanied by a graphical representation: /user GET POST /user/<id> GET PUT DELETE API structure The problem with this approach is that it’s informal . People can read it and it communicates the intent, but it’s different for every API. OpenAPI (formerly Swagger) is a standardized format that describes how an API works . It captures the important parts of the documentation and defines a structure for it: The advantage of a standard format is that general-purpose tools can process it. One such tool is Swagger UI which provides interactive documentation for the API and allows easy experimentation in a familiar format. It provides a list of methods, so it’s easy to get an overview of how the API is structured: It also allows calling the methods, even with a request body: Generates copy-pastable Curl examples for the requests along with the response: And supports path parameters too: This provides developers an easy way to try out the API without first reading the informal documentation and figuring out how each part works. Let’s see how to deploy a live API with Swagger UI using Terraform! The Swagger UI runs entirely on the client-side. It requires 2 Javascript and 1 CSS files, along with the API documentation YAML (or JSON). The frontend dependencies can be hosted along with the index.html or they can be referenced from a CDN. The latter provides an easier setup as the whole thing will be one file (the index.html ) and using SRI (Subresource Integrity) makes sure they are intact. Here are the files needed: The index.html loads the swagger-ui-standalone-preset , the swagger-ui-bundle , and the swagger-ui.css from cdnjs . It’s a good practice to check for new versions and update them from time to time. To initialize the UI call the SwaggerUIBundle function, passing it the url of the API documentation file and a DOM element. Since the Swagger UI and the API documentation YAML are static files, any web hosting works for them. The simplest is to put them into an S3 bucket and enable the bucket website endpoint. The above configuration creates a bucket, enables the website endpoint, then attaches a bucket policy to allow anonymous access ( \"Principal\": \"*\" ). This gives a URL in the form of http://<bucket>.s3-website.<region>.amazonaws.com . Notice that it uses unencrypted HTTP and S3 does not support HTTPS. If you want to expose the API documentation via HTTPS (which you should), use CloudFront in front of S3. How does Swagger UI know where is the API ? It is in the YAML file, in the servers section. But when, for example, the same API is managed by Terraform and the URL is different for every deployed stack, it needs to be set dynamically. Terraform supports the templatefile function that allows defining values and inserts them into the document: In the OpenAPI YAML, use the placeholder for the server URL: When it is deployed, the YAML contains the URL of the API Gateway: To put files into a bucket, use the aws_s3_bucket_object resource. The index.html has a text/html content type, so that the browser shows it as a webpage. Then the API document is templated with the API Gateway URL. To export the URL of the Swagger UI, use an output with the website_endpoint attribute of the bucket: The above setup hosts the Swagger UI in a different domain than the API itself, which means CORS (Cross-Origin Resource Sharing). If you use an API Gateway HTTP API then it supports the cors_configuration block where you can define the CORS-related headers: Some calls require a preflight request that is an OPTIONS and the browser requires it to have a success status code. Make sure to return a 200 response from the API: Another solution is to host the Swagger UI on the same domain as the API. This can mean that the API returns the index.html and the API YAML files, or you can use CloudFront to bring everything under one domain. OpenAPI provides a standard for API documentation and tools are available to process it. Swagger UI is a project that turns the API definition into an interactive documentation page where developers can get familiar with the API quickly and can experiment with it. When Terraform manages the API it needs to wire the API URL and the documentation together and expose the website for the browser. Using an S3 bucket website and the templatefile function it is possible to manage the API endpoint and its documentation together.", "date": "2020-10-27"},
{"website": "Advanced-Web", "title": "How to implement a Lambda backend based on OpenAPI", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-implement-a-lambda-backend-based-on-openapi/", "abstract": "API documentation and implementation tend to diverge when they are maintained separately. This results in security problems where something should be validated but isn’t and usability issues such as out-of-date documentation. Let’s say there is an API running on NodeJs that allows creating users! To make sure that only the name property is present in the request, joi is the best validator out there: And to document the API, you can use OpenAPI that defines a standard format: Since the OpenAPI is a standard format, general-purpose tools can consume it and provide useful things, such as interactive documentation . The problem is that the documentation is more visible while the implementation is hidden in code. This makes it hard to make sure they are the same and that can easily lead to problems where the API implementation accepts requests the documentation would reject . The root of the problem is that one information —how to validate— lives in two places —the implementation and the documentation—and that violates the DRY (don’t repeat yourself) principle. To solve this, either generate the documentation based on the code or run the validations based on the documentation . As the OpenAPI is a standardized format, tools can read it and extract how each request should be validated. It’s just a small leap to make an OpenAPI-based validator that gets the request and makes sure that it is valid according to the documentation. The openapi-backend is a project to do just that. It reads the YAML file, runs the validators, and also extracts the parameters and does routing. The last part is done by reading the operationId parameter in the OpenAPI document and running the appropriate handler function. Initialization needs the YAML file and an object of handlers. These are the functions that process the request and return the response. Then when a request comes, extract the different parts and pass them to the api : For example, the OpenAPI documentation defines the createUser operation with a body object that has a single property: The above code extracts the request path and the body, then the openapi-backend selects the operation, validates the object, then calls the createUser handler. Apart from routing, it also extracts the parameters from the request, such as a path parameter: When the handler is called, there is no need to parse the path manually. The context object has the userid readily available: Apart from the handlers that process the operations, there are also a few others for special purposes. The notFound is called when the request does not match any operation. The notImplemented is a catch-all handler. Then the validationFailed is called when the request failed validation. These special handlers allow interfacing with the Lambda runtime, as it expects the response in a specific structure. To return appropriate response codes for invalid responses, add the necessary handlers: Let’s try out how these work! When the request is for a valid operation and the body matches the schema, everything is fine: But when there is an extra property, validation fails and returns a 400 response: And a path that does not exist in the OpenAPI document results in a 404 response: This setup uses one Lambda function as the main entry point to the API and uses internal logic to route between different handlers. This is called a monolith and this is one way to write serverless functions. The other approach is to extract different parts to separate functions, such as one for each path or even one for each operation. The monolith is better in terms of cold starts as once the function is initialized it can handle all requests. On the other hand, separate functions offer better fine-tuning of resource parameters and permissions, and also allows better visibility into how each part works. The OpenAPI documentation is a standardized format to define APIs and it can be the basis of validation and routing. This eliminates the duplication in the implementation and the documentation and allows the latter to be the source of truth. The openapi-backend project is one framework to do this, and it works well on AWS Lambda. It handles validation internally and allows you to define handler functions that provide the functionality.", "date": "2020-10-30"},
{"website": "Advanced-Web", "title": "How to use a break-glass role for sensitive admin tasks", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-a-break-glass-role-for-sensitive-admin-tasks/", "abstract": "A break-glass role is an IAM role along with a few other resources that send a notification when somebody uses it . It is suitable for protecting rare and sensitive admin tasks, such as managing a KMS key policy or administering privileged users. By using a break-glass role, the security team can instantly know when the sensitive permissions are accessed and can start damage control right away. It is a powerful tool for security monitoring an AWS account. But it works only as long as its usage is rare . When the team gets a notification every day, they are likely to disregard a legitimate alarm. Let’s see how to implement a break-glass role that sends a notification when it is used! This solution uses CloudWatch Events to watch for the AssumeRole CloudTrail event. This detects it within seconds and supports several notification targets, such as SNS topics, Lambda functions, and SQS queues. First, we need the role itself. Two types of policies are related to roles: the permission policy and the trust policy . The former defines what the role can do , which is the admin task you want to protect. Add the necessary permissions for your use-case. In code, it looks like this: The trust policy is more interesting. In the case, it trusts the current account and restricts it to a single region. Trusting the current account makes sure that users who have AssumeRole permission can use this role. To give access to admins to use this role, allow the sts:AssumeRole action to the break-glass role’s ARN as the resource. With Terraform, there is a data source called aws_caller_identity that queries the current account id. The second condition, to allow only the region the stack is deployed to , is more interesting. CloudWatch Events and STS, the service that handles the assume role operation, are both region-specific services. But the tokens STS returns are global. Since CloudWatch Events only get events when STS is used in the same region , an attacker could evade detection just by using a different region to assume the role. To prevent this, the role’s trust policy can restrict the region in which the role is assumed. Note that it does not restrict what the role can do, just which regional STS service can be used to assume it. Terraform supports a data source called aws_region that returns where the stack is deployed. This, along with the aws:RequestedRegion condition block can restrict the trust policy to a single region: Now that we have a role that can only be assumed through a single region, the next step is to detect the AssumeRole events. These events are published by CloudTrail, which means you need to have a trail to get these events. But once you have a trail, it does not matter which region you deploy the Event rule. The event pattern defines which events are matched by the rule. Each array defines the possible values in the JSON structure. In this case, the pattern we need defines these restrictions: When all the above is true for an event, the event rule triggers and notifies the targets. Event targets are the services that are notified when an event matches the rule. A lot of services can receive these events, such as SNS topics, Lambda functions, Kinesis streams. For example, publishing to an SQS queue looks like this: For a testing setup, I use an SQS queue and a bash script to print messages to the console . Then in a different terminal, I assume the role. «IAMRole» Break-glass role [] «Database» Protected resource [] «CloudTrail» Trail [] «CloudWatchEventEventBased» CloudWatch Event rule [] «SQSQueue» Queue [] «Client» Terminal [] IAM user Access Assume Event Event Publish Poll Assuming the role triggers an event on a terminal Here’s the terminal setup: To assume the role, I use the AWS CLI: When the assume role is successful, I see the event on the top terminal: The event that CloudWatch publishes contains a lot of information about who is assuming the role which can help to investigate when it is used maliciously. Here’s a somewhat redacted version of the information contained: The notifications for the break-glass role depends on resources that an attacker can potentially disable . For example, a small modification in the event pattern makes it to not match any events, effectively disabling the security benefits. Keep this in mind, as the reliability of this solution depends on the security of all the moving parts as any of them is compromised means you won’t get a notification. A break-glass role provides a good solution to rare and sensitive admin tasks . It allows the protected operation but also makes sure that whenever it is used is widely known, giving a chance to the security team to investigate. To receive events when the role is used, use CloudWatch Events with CloudTrail. This setup detects the AssumeRole events which are the entry point for a role. Make sure to restrict the region where the role can be assumed. Failing to do this allows using the role in a different region undetected.", "date": "2020-10-23"},
{"website": "Advanced-Web", "title": "How to debug CloudFront origin requests", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-debug-cloudfront-origin-requests/", "abstract": "CloudFront is a proxy between the visitors and the backend servers. When it gets a request, it forwards to one of the origins, then returns the response to the visitor. Where this process gets complicated is that CloudFront also transforms the request . It replaces the path, the headers, cookies, and the query parameters, which makes it hard to pinpoint the problem when all you see is an error message. CloudFront request/response structure Visitor «CloudFront» Distribution [] Edge locations «APIGateway2» Origin [] GET /path Headers Query param ??? response response Unfortunately, CloudFront does not provide any built-in tools to inspect what goes on the wire . It offers logs, but those do not contain the details sent to the origin. If the origin you are trying to debug writes detailed logs then you can consult them to see what is the problem. But usually, that is not the case, and this approach depends on the origin itself. An alternative solution is to use a service that provides a public endpoint and request logging , then point the CloudFront distribution to that endpoint. This gives visibility into what the origin request looks like on the wire. This article describes two solutions, one is a hosted tool, the other is self-hosted. Webhook.site is a webhook tester that gives a publicly available URL and stores the requests sent to it. It offers a nice web UI. Webhook.site for origin request debugging User «CloudFront» CloudFront [] Edge locations webhook.site Request Origin request Store request get stored requests requests Inspect requests When you go to the site it gives you a unique path under the webhook.site domain. This is the unique debug endpoint you can use: The next step is to modify the CloudFront Origin to send requests to this endpoint. Change the Origin Domain Name , the Origin Path , and make sure to use HTTPS only : Wait for the distribution to deploy, then send a request to the CloudFront domain to trigger an origin request: The webhook.site webapp automatically refreshes the list with the new request and prints all sorts of useful information about it: For example, notice the URL of the request, especially the path: Or the headers in the request, which are quite different than what the browser sent to CloudFront. One header that can cause trouble is the host : Of course, any kind of HTTP tester works, not just the browser: This curl request produces a similar origin request but is easier to configure and automate: With curl it’s easy to add headers to the viewer request and see how they are forwarded to the origin. As in illustration, let’s add an Authorization header! This is one of the headers that CloudFront handles in a special way , based on the cache behavior settings. Let’s see how CloudFront forwards this header by default: No header reaches the origin: This is the default behavior when this header is not included in the cache key. From the documentation: GET and HEAD requests – CloudFront removes the Authorization header field before forwarding the request to your origin. Let’s change the Cache Based on Selected Request Headers to “All”: The same viewer request forwards a lot more headers this time, and the Authorization is among them: But note that this setting also overwrites the Host header, which is now the domain of the CloudFront distribution instead of the origin. This can cause problems with origins that use the Host header to route between services, such as API Gateway. Similarly, you can debug how cookies and query parameters are forwarded too. These are also changed by CloudFront, depending on the cache behavior settings: Let’s send a request with a cookie and a query parameter and see what reaches the origin! With the above configuration, both the cookie and the query parameter reaches the origin: Forwarding everything is a safe way to make sure CloudFront won’t mess up the requests sent to the origin, but it also makes edge caching a lot less effective. As a rule of thumb, forward only the parts of the request that provides a different response. Instead of using a managed service you can also start a request interceptor locally. I found Request Baskets easier to start with Docker, and it provides similar functionality. To start the service and tie it to a port (55555 in this case), use this docker command: It starts the service locally, which might be sufficient if the machine you are using is accessible from the internet. If it is, then point the CloudFront origin to this service and you can start sending requests. If the machine is behind a firewall or a NAT you can use a tunnel, such as ngrok . It exposes a local port under a publicly available domain name and it uses a connection that originated from your machine so it can bypass firewalls. You don’t need to install anything, as you can run ngrok with npx: Now when there is a request to the ngrok domain ( https://d0b629c4b31b.ngrok.io ) it forwards it to the local port 55555, where the rbaskets service is listening. CloudFront origin request with ngrok and rbaskets in Docker User «CloudFront» CloudFront [] ngrok docker rbaskets <id>.cloudfront.net <id>.ngrok.io /<basket> ngrok http 55555 port 55555 -p 55555:55555 port 55555 store request in <basket> With rbaskets, you need to create a basket first. Open the ngrok domain and use the Create button to generate an empty basket: It gives a URL that you can use with CloudFront: To point the distribution to this debug endpoint, modify the Origin Domain Name and the Origin Path . Since ngrok provides an HTTP as well as an HTTPS URL you can keep the Origin Protocol Policy on what you use for the real origin. Finally, send a request to the distribution: And inspect what reaches the origin: When CloudFront forwards a request to the origin it makes modifications to it. It changes the path, the header, the cookies, and the query parameters. This behavior can lead to strange errors. When you can not inspect the logs on the origin, for example when it’s a managed service, then it’s back to trial and error as CloudFront does not provide any help to inspect what it sends. To debug origin requests effectively, you can use a webhook tester that gives a publicly accessible endpoint and logs the requests it receives. When CloudFront uses this endpoint as the origin you’ll see what is sent on the wire, making debugging possible.", "date": "2020-10-16"},
{"website": "Advanced-Web", "title": "AWS IAM deep dive: How the policy evaluation logic works", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-iam-deep-dive-how-the-policy-evaluation-logic-works/", "abstract": "IAM policies define who can do what in an AWS account. They are used everywhere, ranging from allowing users to change their passwords to giving read-only S3 access to a Lambda function all the way through having a security auditor with cross-account access to investigate. An AWS account is only as secure as its policies . Access control happens when a request reaches the AWS APIs. At this point, IAM kicks in and looks into the applicable policies and decides whether it allows or denies the request. Because of this, IAM policies do not affect when users call your webserver directly. They only protect AWS API calls . A policy is a JSON document with a strictly defined structure. It contains one or more statements which are the basic building blocks of access control. An example policy JSON looks like this: Each statement can contain an Effect , an Action , a Pricipal , a Resource , and a Condition , and depending on where it’s used, a different set is required and supported. All properties of a statement except the Effect is a filter . For every request, IAM needs to collect all applicable statements and it uses the Action , Principal , Resource , and Condition elements to do it. The Action is what the user is doing, such as downloading an object from an S3 bucket is an s3:GetObject action. The Principal is who is doing the action. For identity-based policies, where the policy is attached to a user, this element is implied and is missing from the statement. The Resource is the target of the operation, such as an S3 bucket, or an object inside the bucket. A resource-based policy can only define the resource it is attached to or resources under it (such as objects inside the bucket, but it can not affect other buckets). Finally, the Condition allows other constraints on the request context. Using this element allows fine-grained control, but requires a lot of trial-and-error to get it right. All the above elements have a Not counterpart that matches everything but. While Action: [\"s3:GetObject\"] matches only the s3:GetObject action, the NotAction: [\"s3:GetObject\"] matches all the actions, except s3:GetObject . NotPrincipal and NotResource work similarly, while Condition supports Not… versions of operators, such as StringLike has a StringNotLike counterpart. The counterpart of the filters is the request context which IAM constructs when it needs to evaluate access. It contains all the information of the request , and the policy filters are matched against its contents. For example, when a user tries to download an object from an S3 bucket, the request context contains the Principal , the Resource , the Action , and other metadata about the call. Thinking about a request in terms of a list of attributes makes it easy to reason about how the filters apply to it , especially when you use Not… elements or Conditions. For example, the above request context is matched by this policy, attached to user1 (in which case the Principal is implied): And by this one ( NotPrincipal means everybody except ): After collecting the effective statements using the filters, it uses the policy evaluation logic to decide if the request is allowed. The AWS documentation has a descriptive image to summarize how it works: It seems complicated, but most of the chart is just about two simple principles: All the other parts are to show which policies are enough and which ones are optional . There are different types of policies, depending on what they are attached to: Account «S3Bucket» Bucket [] «User» User [] «IAMRole» Role [] «IAMPermissions» Session Policy [] «IAMPermissions» Permission Boundary [] «IAMPermissions» Resource Policy [] «IAMPermissions» Identity Policy [] «IAMPermissions» Service Control Policy [] assume role assume role read IAM policy types The SCP, the permissions boundary, and the session-based policy are specific to a use-case, and they are optional. If you don’t use them then they don’t affect the policy evaluation, IAM skips over them. The two most important types are the identity-based and resource-based policies . The majority of policies are of these two types. When we take out the parts that are for narrow use-cases the evaluation logic becomes a lot simpler: Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? Identity-, and resource-based policies evaluation The key point is that it is enough if either the resource policy or the identity policy allows the operation . This bucket policy allows the user to read the objects even if the user has no permissions on its own: This is useful especially when there is no Principal, such as making a bucket public: Some resources, such as IAM Roles and KMS keys are more restricted. In these cases, the resource policy needs to either allow the identity or the account in the form of arn:aws:iam::<accountid>:root . If the policy does not allow either the identity or the account then the request will be denied. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows the identity? Allows the account? Is there an Allow? Some resource-based policy must delegate to the account When a request reaches the AWS APIs, IAM collects all the applicable policies. It also constructs a request context containing all the info regarding the request. When it moves into the evaluation phase, it first uses the Action , Principal , Resource , and Condition filters to select the applicable policies. Then it follows the policy evaluation logic to decide whether the matching policies deny or allow the operation. The two most important policy types are resource, and identity-based policies. These two make up the vast majority of all policies used, and the other types are for specific use-cases.", "date": "2020-10-13"},
{"website": "Advanced-Web", "title": "IAM policy evaluation logic explained with examples", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/iam-policy-evaluation-logic-explained-with-examples/", "abstract": "IAM follows a defined course when it decides whether a given request is allowed or denied. The basic configuration block is IAM policies, which contain statements that grant/deny permissions. The first step IAM does is it constructs a request context which contains all the details, such as who is making the request (the Principal), what is the action (Action), and on which resource (Resource), along with a bunch of others metadata, such as the IP address, whether the identity is authenticated with MFA, and several other things. The exact parameters are unfortunately not visible, and the only available resource is the reference documentation . This makes it more of a trial-and-error to get some insight into the values for a request. Thinking of requests as lists of name-value pairs helps understanding how policy statements match them. In the next step, IAM collects all the statements that match the request context . Each statement has a set of filters , in the form of the Action , Principal , Resource , and Condition properties. The statements that match the request context will be included in the policy evaluation logic . Whether the request is allowed or denied depends on the type of matching statements and their Effect s. The two most important policy types are resource- and identity-based policies, as all the others (SCP, permissions boundary, session policy) are for specific use-cases and they are optional in the evaluation flow. The decision for most resources follow this process: Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? Identity-, and resource-based policies evaluation In this article, we’ll look into a few policies and see how IAM reaches a conclusion when it evaluates them. We’ll use a 2-step process for each scenario. First, we construct the request context from the known values and see which policies apply to that request. Second, we’ll use the above process to see what decision IAM reaches and why. I’ll use policy and statement interchangeably. A policy is a container for statements and the statements are the permissions, but in everyday speech, it’s more natural to say “a policy allows this action” than “a statement allows this action”. Let’s start with a simple example! A user has a policy that allows access to an object in an S3 bucket: user bucket «policy» { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/text.txt\" } aws s3api get-object User with an identity policy The user then gets the object using the AWS CLI: This sends a request to an AWS API signed with the user’s keys. IAM then starts to evaluate access. The request contains the user, the action, and the resource. In the policy, the Principal is implied, as it is attached to the user. All the filters in the policy match the request context: Request Principal: <iam>/user Action: s3:GetObject Resource:<bucket>/text.txt Identity policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/text.txt\" } The identity policy matches Now that we know that this policy applies to this request, let’s start the flow and see how a decision is made! There is no policy with \"Effect\": \"Deny\" . There is no resource-based policy either, so the execution reaches the identity-based policy evaluation step. Since there is a policy that has an \"Effect\": \"Allow\" , the final decision is “Allow”: Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\" , \"Resource\": \"<bucket>/text.txt\" } Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? The identity-based policy allows the operation Let’s see an example of how a resource-based policy can restrict access! In this example, the S3 bucket has a bucket policy (which is a resource-based policy) to deny access for all users except for a specific one. user bucket «policy» { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/text.txt\" } «policy» { \"Effect\": \"Deny\", \"NotPrincipal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } aws s3api get-object The bucket has a policy The request is made by user who is not bucket_admin , so the bucket policy is in effect. Request Principal: <iam>/user Action: s3:GetObject Resource:<bucket>/text.txt Identity Policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/text.txt\" } Resource policy { \"Effect\": \"Deny\", \"NotPrincipal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } Both policies apply In the evaluation flow, the request is denied due to the explicit deny. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\" , \"Resource\": \"<bucket>/text.txt\" } Denied Denied Allowed Allowed Resource-based policy { \" Effect\": \"Deny\" , \"NotPrincipal\": { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } Is there a Deny? No deny Allows? Is there an Allow? The resource policy denies the operation Let’s see how things change when the bucket_admin user makes the request! The deny policy does not apply, but neither does the user ’s is identity policy. bucket_admin bucket «policy» { \"Effect\": \"Deny\", \"NotPrincipal\": { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } aws s3api get-object The user has no access when there is no allow policy When the bucket_admin user makes the request, no policies apply. Request Principal: <iam>/bucket_admin Action: s3:GetObject Resource:<bucket>/text.txt Resource policy { \"Effect\": \"Deny\", \"NotPrincipal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } Does not apply The policy does not apply to the bucket_admin user In the evaluation flow, no policy denies access but none allows it either. The result will be a deny, which is called “an implicit deny”. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? No policy allows the operation Let’s add a statement to the bucket policy to give access to the bucket_admin user! bucket_admin bucket «policy» { \"Effect\": \"Deny\", \"NotPrincipal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } «policy» { \"Effect\": \"Allow\", \"Principal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": [ \"s3:GetObject\" ], \"Resource\": \"<bucket>/text.txt\" } aws s3api get-object A resource policy can allow an action In this case, the deny still does not apply, because of the NotPrincipal element, but the other statement does because the Resource , the Action , and the Principal all match. Request Principal: <iam>/bucket_admin Action: s3:GetObject Resource:<bucket>/text.txt Resource Policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Principal\" : { \"AWS\": \" <iam>/bucket_admin \" }, \"Resource\": \"<bucket>/text.txt\" } Resource policy { \"Effect\": \"Deny\", \"NotPrincipal\" : { \"AWS\": \"<iam>/bucket_admin\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"<bucket>/text.txt\" } The new policy matches The evaluation flow reaches the resource-based policy check as there is no explicit Deny. And as there is a statement with an Allow, the request is allowed. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Allowed Allowed Resource-based policy policy: { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\" , \"Principal\": { \"AWS\": \"<iam>/bucket_admin\" }, \"Resource\": \"<bucket>/text.txt\" } Is there a Deny? No deny Allows? Is there an Allow? A resource-based policy allows the operation A Condition block allows more fine-grained filtering. This is a diverse topic as there are many global and service-specific condition keys and which ones are included in the request context is an opaque process. Let’s see an easy example involving object tags! To give access to all object that has access=secret tag to a user is possible with the s3:ExistingObjectTag/access condition key: bucket secret.txt access=secret restricted.txt access=restricted user «policy» { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"secret\" } } } aws s3api get-object A policy can define conditions When the user tries to get an object that is tagged with access=secret , the policy matches: Request Principal: <iam>/user Action: s3:GetObject Resource:<bucket>/secret.txt s3:ExistingObjectTag/access: secret Identity Policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \" s3:ExistingObjectTag/access\": \"secret\" } } } The object tag matches the value With a policy that allows the operation, the final decision is to allow. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies policy: { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\" , \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"secret\" } } } Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? The policy allows the operation But let’s see what happens when the target object has a different tag value! The policy won’t match because the Condition does not match the request context. Request Principal: <iam>/user Action: s3:GetObject Resource:<bucket>/restricted.txt s3:ExistingObjectTag/access: restricted Identity Policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"secret\" } } } The object has a different tag value And as no policy allows the operation it is denied. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? No policy allows the operation Instead of hardcoding a tag value, a tag-based access control scheme (commonly known as Attribute-Based Access Control, or ABAC for short) can use tags defined for the principal. This makes a highly scalable permission scheme, as adding and removing tags from resources and identities grants and removes the permissions instead of having to modify the policies attached to them. Let’s see an example where the user has an access=restricted tag attached, and a policy that allows reading objects with the same tag! bucket secret.txt access=secret restricted.txt access=restricted user access=restricted «policy» { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"${aws:PrincipalTag/access}\" } } } aws s3api get-object Tag-based access control The request context contains both the resource and the principal tags, so the Condition can match them with the placeholder in the IAM policy. In this case, both the resource and the user have access=restricted . Request Principal: <iam>/user Action: s3:GetObject Resource:<bucket>/restricted.txt s3:ExistingObjectTag/access: restricted aws:PrincipalTag/access: restricted Identity Policy { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"${aws:PrincipalTag/access}\" } } } The principal and the resource tag matches As there is an Allow policy, the request is allowed. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\" , \"Resource\": \"<bucket>/*\", \"Condition\": { \"StringEquals\": { \"s3:ExistingObjectTag/access\": \"${aws:PrincipalTag/access}\" } } } Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows? Is there an Allow? The operation is allowed This example showed that when the tag value is equal for the resource and the identity the policy matches. It’s easy to see how it doesn’t when the two values are different. In the S3 bucket example, either the identity- or the resource-based policy is enough to give access. This is the case for most resource types, but there are exceptions. An IAM role’s trust policy needs to allow the action explicitly, it’s not enough that the identity policy allows it. The resource policy can allow in two ways. It can allow the user explicitly, such as \"Principal\": \"<iam>/user\" . In this case, the operation is allowed and there is no need for an identity policy. This is how it works for less strict resources. The other way is to allow the account in the form of \"Principal\": \"arn:aws:iam::<accountid>:root\" . This delegates access control to the identity policies to decide. If a trust policy does not allow either the requesting identity or the account then the request is denied. This is how the policy evaluation flow looks like for these resources: Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies Denied Denied Denied Allowed Allowed Resource-based policy Is there a Deny? No deny Allows the identity? Allows the account? Is there an Allow? Some resource-based policy must delegate to the account Let’s see how a user can assume a role when its trust policy specifies the account! role user «policy» { \"Action\": [ \"sts:AssumeRole\" ], \"Effect\": \"Allow\", \"Resource\": \"<role>\", } «policy» { \"Action\": [ \"sts:AssumeRole\" ], \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<accountid>:root\" } } aws sts assume-role A role's trust policy allows the account The evaluation logic considers both policies, and the combination of them allows the operation. Collect all applicable policies Filters by Principal, Resource, Action, Condition Identity-based policies { \"Action\": [ \"sts:AssumeRole\" ], \"Effect\": \"Allow\" , \"Resource\": \"<role>\" } Denied Denied Denied Allowed Allowed Resource-based policy { \"Effect\": \"Allow\" , \"Principal\": { \"AWS\": \"arn:aws:iam::<accountid>:root \" }, \"Action\": \"sts:AssumeRole\" } Is there a Deny? No deny Allows the identity? Allows the account? Is there an Allow? The operation is allowed Another resource that has strict policies is KMS keys. The AWS documentation mentions that since a key policy controls management access to it you can lock yourself out and have to contact support to regain control. IAM policies control access to everything inside an AWS account and they are the main security controls. I found that thinking of them in terms of a request context and a control flow simplifies a lot.", "date": "2020-10-20"},
{"website": "Advanced-Web", "title": "AWS IAM deep dive: How IAM users and groups work", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-iam-deep-dive-how-iam-users-and-groups-work/", "abstract": "IAM users are the essential building blocks when an AWS account is used by more than one person. They provide people inside the organization a way to log in to the management Console and manage resources and also to use the AWS APIs with access keys. In this article, we’ll look into how users can access an account, and how to define what they are allowed to do. Users are access points to the AWS account . They are usually associated with a person who needs access to do his duties. Creating a separate user for everybody is a good security practice, not only because it prevents the sharing of secrets (passwords and access keys) but also that it allows seeing who did what in an after-the-incident investigation. Every IAM user has a username and a set of permissions in the form of IAM policies. When they log in they also need to know the account id, so that they are namespaced between accounts. Users can interact with the account in two ways : using the Console and through APIs (and SDKs). To use either of them, a user needs a secret . Signing in to the Console requires a password while using the APIs require access keys . When you create a new user, you need to select which of these accesses you want to provide initially. You can change it for existing users too. New users usually start with Console access as that provides insight into what is going on inside the account and offers forms to manipulate and create resources. But everything that can be done on the Console can be done through the APIs too. An access key is equivalent to a password but for the AWS APIs. These APIs have the same capabilities as the Console, so everything you can do by logging in you can also do programmatically (and more, in some cases). The access keys for a user are listed on the Security Credentials tab: You’ll see a list of access keys and a button to create a new one: An access key consists of two parts: an Access key ID and a Secret access key . You’ll see both when the key is created: And the list shows the newly created key: A user can have 2 access keys at a time and they both provide identical access to the account. To rotate a key, create a new one, update the systems wherever it is embedded, then delete the old one. The primary use case for access keys is to use the AWS CLI to manage the account using the command line. It supports the credentials file that stores the keys. To add the access keys to a CLI configuration, use aws configure , then copy-paste the keys : Then whenever you issue a command, it picks up these credentials and uses the access of the user: Most tools that build on AWS APIs support this location too. Terraform will work when the AWS CLI is configured, and also SDKs will pick up the credentials from there. Alternatively, you can use environment variables to overwrite the config file. Set the AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and clear the AWS_SESSION_TOKEN when you issue the command: Note that the credentials file, located at ~/.aws/credentials , contains the keys in plain text . Whoever has access to this file can copy them and have access to the account in the name of the user. Because of this, access keys are different than passwords which are only known by the users. Everything the user does is subject to IAM policies, and it includes everything done with the access keys. You can even require MFA authentication via policies. The access key is comprised of 2 parts: the Access key ID and the Secret access key . The former identifies the account and the user and it is not secret. Whenever there is a request to AWS, it is included in it in plain text. The Secret access key, on the other hand, is secret. It is only used to calculate a signature for a request made with an access key and it can not be recovered by observing what is going on the wire. CLI AWS Access key ID Secret access key request HMAC Signed request Secret access key Signed request HMAC match? add signature send Access key ID and Secret access key usage When users want to log in to the Console they need to use their passwords if they have one. A best practice is to let the users manage their own secrets, which means an administrator should only set the initial password and force change on the first login. But users might choose a password that is not secure enough, and there is little administrator oversight over it. An account-wide password policy helps by requiring certain character classes (lowercase, uppercase, numbers, special characters) and rotation. When a user has an MFA device attached it is also required during the sign-in process. You don’t need policies to enforce it, contrary to API access. There is only one kind of IAM user in AWS but conceptionally we can distinguish two distinct usages. Natural users are tied to a person and gives access to the account. This type of user usually needs Console access and a wider range of permissions. Then there are technical users to provide a non-AWS service access to some protected resource in the account . Because it is for a specific step in a workflow, the permissions can be locked down, usually just to one action on one resource. Third-party system AWS backend IAM User topic policy Action: SNS:Publish Resource: <topic> visitor call Access keys Allowed A technical user gives programmatic access to the AWS account For example, you might have some script running in the Google cloud that needs to publish a message to an SNS topic. You can deploy a Lambda function behind a public API Gateway to publish these messages, or you can create a technical user, generate an access key, and embed that into the script. For services inside AWS, you don’t need users as those support roles to get access to the account. What users can do inside the account is controlled by IAM policies . For example, you can give read access to objects in a specific bucket: user bucket policy Action: s3:GetObject Resource: <bucket> Allowed A policy attached to the user gives access to a bucket When a policy is attached to a user, a group, or a role it is called an identity-based policy. These policies define what the identity can do , as opposed to resource-based ones that control access from the resource-side. As a rule of thumb, when you create a new IAM user it has no access to the account and you need to attach policies to selectively grant permissions. IAM policies allow fine-grained control but that requires careful fine-tuning and a familiarity with the services you want to give access to. AWS provides a set of managed policies that allows a range spanning from Administrator down to read/write for individual services. These are great to get started as you don’t need to touch JSON. But if you want to move to a finer granularity, such as allow reading from a specific bucket, you need to get familiar with the policy language. The goal of policies is to define the least privilege where everything the user needs is allowed but nothing else. This reduces the blast radius , the impact of a breach, to the minimum without hindering the users to perform their work. But, in practice, permissions tend to accumulate over time as it’s the more urgent problem to solve when someone is lacking access to a resource. Groups implement the Role-based access control (RBAC) scheme where users are assigned into groups and only these groups get permissions. What each user can do is then determined by membership instead of individual policies. Don’t confuse the naming here. A role inside AWS is an entirely different concept. This access-control mechanism simplifies permissions when you have a lot of similar users in the account. If you attach policies to each of them individually you’ll end up with a lot of silos and you need to keep all of them up-to-date to prevent unneeded permissions. If you create groups and assign users to them you’ll have a lot fewer identities and policies to maintain. Engineering QA Bob Alice Elena Dave bucket policy AmazonS3FullAccess policy AmazonS3ReadOnlyAccess full access read-only access Groups with permissions As a best practice, use groups and only give access to them instead of individual users. This allows easier scaling when a department is expanding as you only need to create a user and assign it to the group. When somebody moves to a new role inside the organization, you can reassign the memberships and that reassigns the permissions too. IAM users provide an entry point to the account, either by having a password to sign in to the Console or using access keys to use the AWS APIs. They can be for natural persons to do their jobs or embedded into a third-party system to give programmatic access to the account. IAM policies attached to users control what permissions they have. The granularity can range from full access down to just a single action on a single resource. Groups allow assigning permissions to a set of users. Instead of managing policies for each user individually, you can create groups and define access through membership.", "date": "2020-10-09"},
{"website": "Advanced-Web", "title": "How CloudFront determines the origin request URL", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-cloudfront-determines-the-origin-request-url/", "abstract": "CloudFront transforms the requests it sends to the origins. How it’s done depends on the origin config, the cache behavior config, and the viewer request path (the one that the visitor sends to CloudFront). Among these transformations, how the URL is constructed is the most important, and especially what the request path will be in the origin request. The path of the request selects the directory and the object key for an S3 origin, such as /index.html is different than /bucket/index.html . In REST APIs, the path selects the resource to query, such as /user is different than /group . In this article, we’ll look into what configurations in CloudFront influence the request it sends to the origin. We’ll discuss the path in detail as that can easily cause problems. When you add an origin, a few settings affect the URL sent to that origin. These specify the beginning of the URL (scheme, host, port, and the beginning of the path) and as these are properties of the origin it affects all requests. The requests to this origin go to https://example.com:443/path . Let’s break down each part! The scheme comes from the Viewer Protocol Policy . It can be HTTP, HTTPS, or it can match the viewer request. In this case, it is HTTPS only, so the URL starts with https:// . The domain is straightforward, it’s the Origin Domain Name . The port is selected from the two port selectors, HTTP Port and HTTPS Port , by what scheme is used. Since HTTPS is configured for this domain, the HTTPS Port is the effective one here, which is 443. Since this is the default for HTTPS connections, it will be used when if you don’t specify the port at all. https://example.com is the same as https://example.com:443 . Finally, the Origin Path defines the start of the path . Every request to this origin will start with this. The rest of the path comes from the viewer request. Its path is appended to the Origin Path and that makes the full path. A few examples (the /path comes from the Origin Path setting): You can see that the /path that comes from the origin settings is always present and the path from the incoming request is copied in full . Cache behaviors Origins /api/* * API bucket visitor «APIGateway2» API [] «S3Bucket» Bucket [] /api/user /api/user /api/user /index.html /index.html /index.html Request forwarding And we’ve arrived at the part that causes the most confusion. How does the cache behavior affect the path? Every behavior has a Path Pattern that defines what paths it can serve. Under a distribution, the list of cache behaviors gives an overview of these path patterns: The Path Pattern does not directly influence the path of the origin request but it does define which behaviors can handle the request. And since the cache behavior selects the origin, the viewer request path will always match the behavior’s path pattern . For example, if the behavior has a path pattern of /api/* and the origin path is /path then all paths will start with /path/api/ . A few examples: In many cases, this can be a problem as it means two origins both expecting requests with paths going to the root ( / ) can not be easily integrated into a CloudFront distribution. Let’s say there are two S3 buckets, and you want to host their contents under /bucket1 and /bucket2 . The two cache behaviors for these buckets have path patterns of /bucket1/* and /bucket2/* . When a request hits the first behavior, the origin request goes to <bucket1>/bucket1/... , and for the second behavior, it sends it to <bucket2>/bucket2/... . To serve files from these buckets, you need to put the contents to the bucket1 and the bucket2 folders. Unfortunately, CloudFront does not provide an easy way to remove parts of the path from the origin request. You can use Lambda@Edge , but that is a complicated solution for a seemingly simple problem. The URL CloudFront sends to the origin is determined by the origin config and the viewer request. While the cache behavior does not change the request path, it still determines which requests it can handle. As a consequence, the requests sent to the origin will match the path pattern of the behavior.", "date": "2020-09-25"},
{"website": "Advanced-Web", "title": "Granularity levels in AWS IAM policies", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/granularity-levels-in-aws-iam-policies/", "abstract": "When everything is allowed for everybody in an account it’s great for usability but horrible for security. When everything is denied then the account is completely secure but useless. The sweet spot is in the middle, where everything that is needed is allowed but everything else is denied. This is called the least privilege. The main control over what can be done in an account is via IAM policies. They can allow and restrict users based on different elements in the policy statement. In this article, we’ll look into the granularity levels of how you can define permissions. Starting from the most coarse “allow everything” down to the finest details, such as “allow getting a specific object from an S3 bucket when the user is MFA authenticated”. AWS provides a set of policies that follow the one-size-fits-all approach. They are suitable for all accounts, but they are coarsest. The best thing about AWS managed policies is that you don’t need to think about policy structure or even write any JSON . Just select an existing policy available for all accounts, attach it to a users/groups/roles and they get the permissions. Several policies are available, ranging from the “allow everything” level down to “allow read/write to individual services”. The jolly joker, this policy allows everything that can be allowed for an IAM user. Adding this to a user gives it free roam across the account. Similar to the AdministratorAccess, but it does not grant access to IAM. You can selectively allow access to individual services using these service-based AWS managed policies. If you know a user only needs access to S3, then you can attach the AmazonS3FullAccess policy that allows everything inside S3 but does not grant allow using any other service. The IAMFullAccess is an exception. With it, the user can change its own permissions, making this managed policy effectively equivalent to the admin access . To further restrict service-level access, you can use the ReadOnlyAccess policy versions. These allow only reading data and resource states but does not allow any modification. For example, the AmazonS3ReadOnlyAccess allows Listing and Getting things inside S3: AWS managed policies offer granularity only up to this point and to specify permissions in more detail you need to write custom policies. It’s a more involved process, but it also unlocks greater control over what is allowed. The IAM console provides an editor to help set up things. There is a visual editor where you can select the services, actions, resources, and conditions and it offers some context-sensitive help and links to the relevant docs. Or, alternatively, you can edit the JSON manually: Here’s the general structure of a policy: Policies are made of statements . In each statement, there is an Effect that is either Allow or Deny whether the statement grants or restricts permissions. All the other parts of the statement are filters . They define which requests are matched when IAM decides whether to allow or deny an action. Actions define what the user is doing , such as listing an S3 bucket, terminating an EC2 instance, or creating a new IAM user. By specifying individual actions you can restrict what is allowed beyond the all/read-only levels. For example, S3 defines the s3:GetObject and the s3:ListBucket actions, the first allowing downloading objects, while the latter getting the list of objects. This is more granular than read-only access as it does not allow reading the bucket policy among other things. The Resource element scopes down the permissions to individual resources. All the above statements allowed access to all buckets . To allow listing and downloading from a single bucket, specify its ARN: The resource type is dependent on the action, such as s3:ListBucket works on the bucket-level while s3:GetObject on the object-level . The above statement specifies 2 actions and 2 resources, but since they need different resource types they are two separate action-resource pairs. To illustrate this, let’s separate the two: Consult the IAM reference to see what are the ARN formats for the resources, and actions table for which actions require what resource type. You can further restrict the applicable resources by using subresources. For example, you can specify individual objects inside an S3 bucket, or use wildcards to specify a group of them. This policy allows reading objects from the public folder: Conditions provide filters on the request context, such as the time of the request, whether the user authenticated with MFA or the resource has the given tag. To illustrate this, this policy allows downloading objects only when the user used an MFA device to log in: Another example is to allow downloading objects that are tagged with readable : Conditions are highly dependent on the operation as some support many things to filter on, some don’t. The IAM reference provides a good starting point to see what conditions are available, and there are many guides in the docs to show how to achieve fine-grained policies. Adding conditions to statements open a lot of possibilities but they are frustrating to write. There is no way of debugging when something is not working as expected, so you can only guess the values of the condition elements through trial-and-error. When you give access to the account you can use AWS managed policies for coarse-grained access control. These range from the “everything included” to read/write access to individual services. If you want more granular policies, you need to write them yourself, either using the visual editor or manually writing the JSON. With custom policies, you can define individual actions and resources, and you can also add conditions to the request context.", "date": "2020-10-06"},
{"website": "Advanced-Web", "title": "How CloudFront speeds up content delivery", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-cloudfront-speeds-up-content-delivery/", "abstract": "Let’s see a typical web-based architecture! Usually, the app is hosted in an AWS region, using EC2 servers or Lambdas behind an API Gateway. When visitors browse the page the connections go all the way to this region. For visitors close to the AWS region the app is hosted in the latency is low and the connection happens quickly. The packets need to travel a short distance and that is fast. But for visitors far from the region , potentially in a different continent, the latency is much higher . The result is, especially for the first page, a significantly longer waiting time. Let’s see how adding CloudFront to the mix speeds up connections for this second group of visitors! CloudFront uses the edge network of AWS, consisting of more than 200 data centers all over the world. (Image taken from https://aws.amazon.com/cloudfront/features/ ) When visitors connect to a CloudFront distribution, they are connecting to an edge that is closest to them. This means no matter where your webapp is hosted, the connection will be made to a server that is close. Having data centers closer to users is nice, but the data still lives in the central region, so packets still need to travel all the way. How does adding a new box to the architecture lowers latency for faraway visitors then? The answer is that it makes handshakes shorter . When a visitor goes to your site, the browser needs to establish a connection before it can send the HTTP request. Not counting the DNS resolution, there is one roundtrip needed for the TCP connection, and an additional for the TLS . Versions preceding TLSv1.3 require 2 roundtrips for the TLS connection. Handshake timing for a direct connection Visitor «EC2» Servers [ 50ms ] 0ms TCP 50 ms TLS 100 ms HTTP GET index.html 150 ms When the server is closer each roundtrip is shorter, and every millisecond saved has an outsized effect on the connection time. The only request that needs to travel the full distance is the HTTP request/response, but the 3 before that need to go only to the edge location. Handshake timing with edge locations Visitor «CloudFront» CloudFront [ 10ms ] Edge locations «EC2» Servers [ 50ms ] 0ms TCP 10 ms TLS 20 ms HTTP GET GET index.html index.html 70 ms The edge server needs to establish a connection to the central server (called the origin connection) but this can be reused across the visitor connections . If there is a steady stream of traffic then no one needs to wait for the long handshakes. For a custom origin you can configure how long this connection will be kept open using the Origin Keep-alive Timeout setting: Apart from reusing the origin connection between users, edge locations also have an advantage in terms of connection speed and predictability. They use the AWS backbone network , which is a web of optical cables connecting AWS data centers all over the world with a dedicated connection. (Image taken from https://aws.amazon.com/about-aws/global-infrastructure/global_network/ ) Visitor «CloudFront» CloudFront [] Edge locations «EC2» Servers [] Internet AWS Backbone Origin connections use the AWS Backbone When an edge location forwards the request to the origin server it uses the backbone network instead of the public internet. Packets travel roughly the same distance, but on a congestion-free connection. While the public internet is an oversubscribed network where congestions happen, meaning the packets can be delayed or lost when a link is overloaded, on a congestion-free network this can not happen (at least during normal operation). The result is less variability in latency , also called jitter. Apart from bringing connections close to users and using a dedicated network, CloudFront also allows caching on the edge . This is called edge caching or proxy caching. When a file has been already requested by a user, based on the cache behavior configuration, the edge can choose not to contact the origin for a later request for the same file but use its local cache. This completely eliminates the distance effect on latency. 50ms 10ms Visitor «CloudFront» CloudFront [ 10ms ] Edge locations «EC2» Servers [ 50ms ] GET GET index.html index.html GET Found in cache index.html 50ms 10ms Proxy caching While it’s a powerful concept, it can result in stalled content or even security vulnerabilities . Using CloudFront adds a global network of data centers to your content delivery pipeline. This brings connections closer to the visitors, no matter how far they are from the central servers. This setup shortens the TCP and the TLS handshakes and with the reuse of the edge <-> server connection between users this significantly reduces the response time for the first request. Edge locations connect to AWS regions via the AWS backbone network, which offers a dedicated, congestion-free connection. This results in more predictable latency. Finally, edge locations can also cache content close to users. This can completely eliminate the distance effect for some content.", "date": "2020-09-22"},
{"website": "Advanced-Web", "title": "The AWS account administrator's guide to MFA", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-aws-account-administrators-guide-to-mfa/", "abstract": "Multi-factor authentication adds another requirement for users to access their accounts. People are terrible at choosing a secure password, but they are excellent to spot when a physical thing is missing. MFA builds on this strength by requiring something you have (a phone or a device) as well as something you know (the password). You can use MFA throughout the AWS account. You can add a device to the root account as well as to individual IAM users. As an administrator, you can monitor that users have their devices enabled, as well as enforce their usage to access protected resources in an account. This article discusses what are the options for MFA in AWS, how to check and enforce its usage across the users, and how to handle a lost device. In AWS, each IAM user and the root user can have an MFA device . When there is one associated with the user, signing in to the Console requires using it . MFA devices can be TOTP-based and U2F. They are fundamentally different, so let’s see each of them! TOTP is short for Time-based One-time Password, which is a device or an app that shows a 6-digit number every 30 seconds. When you log in, you need to input the numbers you see in addition to the password. (Image from https://authy.com/blog/authy-onetouch-simply-strong-security/ ) It works by having a secret key shared between the device and the server , and a synchronized clock. The digits you see on the device are the hash of the shared secret and the current value of the clock rounded to 30 seconds. Since both of these data are present on both sides the server can verify the digits. When you add an MFA device to an AWS account, you need to get the secret key to both sides. It is more apparent when adding a virtual device: Note the secret key at the bottom. The QR code is just a convenient way to transfer this to the phone as many apps allow reading these codes. Authentication works by using this secret and the clock: Device Server secret clock token HMAC secret clock token HMAC Shared key compare Add MFA TOTP-based MFA The easiest way to start using MFA is to use a virtual token, which is usually an app that runs on your phone. After synchronizing the secret to the app it generates the digits you can use to log in. The second factor is your phone in this case. There are several apps to choose from. Google Authenticator is probably the best-known, but there is Authy , LastPass Authenticator , andOTP , and even web-based ones, such as totp.app . Note that technically it is still something you know, the shared secret, and it can be recovered from your phone . This has advantages, such as the ability to backup the codes to another device or the cloud , but comes with a slight compromise on the security. If an attacker has access to your phone he can extract the codes and later use them to sign in to your account. To make sure a TOTP secret can not be copied you can buy dedicated hardware. This is manufactured with the secret included so there is no need to transfer it to the device. All you need to do is to type the serial number and two codes, and the device is associated with your account. (Image from https://cpl.thalesgroup.com/access-management/authenticators/safenet-otp-110 ) These devices are tamper-proof to some extent but also tamper-evident , meaning it’s hard to extract the secret but you’ll also notice that someone tried to do it. But because of this, contrary to virtual devices, there is no way to backup it . If you lose it or break it, it’s gone for good. Notice that these devices come with a battery and they are not internet-connected . The secret is preloaded and they have a clock and their only connection with the outside world is through their LCD screen. Sometimes their clocks can drift, meaning the timestamp is different on the device than the server. You can resynchronize the device in that case. Reading the comments, it’s a surprisingly common problem. While every virtual TOTP works with every service supporting TOTP, it’s not true for hardware tokens. Since the secret is stored on the manufacturer’s servers, the service needs to contact them to check the token. Make sure to check which models AWS supports before you buy the tokens . U2F is based on a challenge-response authentication using private and public keys generated on the device. Because of this, an active connection is needed between the device and the server . (Image from https://www.yubico.com/blog/yubicos-u2f-key-wrapping/ ) From a technical perspective, this is a superior solution to the TOTP tokens. But there is no way to backup it, and it needs a connection and that always makes things more complicated . Using it is straightforward, just press the button. It even comes integrated into some laptops . U2F is universal, so every token work with every service. One AWS-specific downside is that you can not use it with the CLI , only to sign in to the Console. When it comes to administering IAM users you can look at the IAM console to see if they have MFA enabled: This list also shows whether they are using virtual or hardware tokens. The Credential report that you can download also contains MFA usage, but only in a true/false way. You can enforce the usage of MFA with IAM policies . To require it, use the \"Bool\": {\"aws:MultiFactorAuthPresent\": \"true\"} condition. You can add this to Allowed statements to make sure they are only allowed with MFA: Or you can Deny operations not listed when MFA is missing: Of course, you can use this condition on resource-based policies too. This prevents deleting objects without MFA: Using IAM policies to enforce MFA usage not only makes sure users add MFA devices to their accounts but also limits what they can do with the CLI. When MFA is enabled, it prevents signing in to the Console, but it does not affect access keys . By denying actions when MFA is not used you can make sure the device has to be used for all operations be it via the Console or the APIs. Unfortunately, there is no way to distinguish between different types of tokens. If you distribute hardware tokens to users but they decide to replace it with a virtual one you can only detect it using the IAM console but can not make the policies deny access in that case. Since it’s best practice to let users set up their passwords , it’s also a good measure to allow them to add their own MFA device. The AWS documentation has a fairly lengthy policy how to allow a user to add an MFA device and also denying everything that is not related to that without MFA auth. When you allow a user to set up an MFA device you need to allow certain operations without MFA . This is because when you create the user there is no device associated with it. But you must not add the ability to remove a device without MFA authentication. This would allow an attacker to remove the user’s device and add his own instead. MFA protects the Console login by default, and the above policy also prevents any action without using the second factor. But since API access is based on long-lived access tokens, how to use MFA with them? You can use the aws sts get-session-token call which supports the --serial-number <arn> and the --token-code <value> arguments and returns a temporary credentials set. You can use these keys to interact with the AWS APIs where MFA protection is needed. Cross-account access is done by creating a role that trusts the other account. This makes sure that you don’t need to exchange secrets (passwords or access keys), only the account ID, and, optionally, an external ID . You can make the policy stricter by requiring MFA authentication for the user who is assuming the cross-account role. Target Account Trusted Account «IAMRole» Cross-account role [] «Database» Protected resource [] IAM user Access Assume Cross-account role setup This helps, but it’s not the same degree of security as requiring MFA in the account you manage. This policy requires MFA authentication, but you have no control over the device that is used. If there is a vulnerability in the trusted account where an attacker can replace the MFA device with his own then he can use this role with that. The Achilles’ heel of MFA is what happens when the user loses the device. For some services, it is a matter of contacting the support, which defeats the security benefits, and on the other end of the spectrum, you can lose access to the service . IAM users can usually go to an account admin and ask for a replacement. This makes it just a matter of some inconvenience, as there is always a user above them. If the affected account is the root account AWS has a process to recover the account in this case. Adding MFA to an IAM user greatly enhances its security and enforcing MFA usage across an account helps when users are targeted by an attack. You, as an administrator, can list who is using a second factor and you can also enforce its usage through IAM policies.", "date": "2020-10-02"},
{"website": "Advanced-Web", "title": "How to manage IAM user passwords", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-manage-iam-user-passwords/", "abstract": "As an AWS account administrator, how IAM users manage their passwords is a black box. You can not see if they use secure practices to make sure an attacker can not impersonate them. AWS offers a few tools to gain insight into this process and allows some control over what is allowed as a password and how long each one can be used before setting a new one. In this article, we’ll look into these reports and controls that help enforce secure practices. When you, the administrator, create users you’ll see the IAM users sign-in link on the IAM page: This is a URL that contains the account ID: When IAM users sign in , they need to know which account to sign in to, and using the above link fills that part automatically. When you create users you need to communicate this link. First, let’s see what information you can get for how users use their passwords! This can give some insight into how to tweak the policies. When you list the users on the IAM page you can see how old their password is. This is a good starting point if you want to check password rotation. Apart from the password age, you can see when it was used last time: For a better overview, you can download the Credential report , which lists all users, when they used and rotated their passwords, among a few other things. It downloads a CSV file, containing several columns for the users. Apart from clicking a button on the console, you can automate this using the CLI also. The Password policy is the primary control over what passwords the users can set. You can find in under the Account settings , and as you can guess this is a global setting for the account. You can’t enforce different rules for different users. There are a bunch of settings here, all serious-looking that limit what users can set as their passwords. The form starts with requirements for the password itself, such as size restrictions and required character classes (lowercase, uppercase, number, special character). This seems reasonable, but none of this prevents the common corporate passwords, such as Pa$$word12 that ticks all the boxes yet it’s a weak one. The second part of the form specifies password expiration . You can use this to enforce rotation. The most interesting option is the “Password expiration requires administrator reset” option. This can be useful when someone leaves the company but somehow the account is not deleted then it will be inactivated after a set amount of time. Also note that access keys are not affected by password expiration , so that a key created will be usable even after the user is unable to log in. These are the possible options, let’s see what are the recommendations! The NIST is the official body that looks into what service providers should allow helping users manage their passwords securely, and it has these recommendations : Verifiers SHOULD NOT impose other composition rules (e.g., requiring mixtures of different character types or prohibiting consecutively repeated characters) for memorized secrets. Verifiers SHOULD NOT require memorized secrets to be changed arbitrarily (e.g., periodically). According to the NIST, all but the length restriction should not be used . To be fair, password strength is more important for offline cracking which is unlikely to happen with AWS IAM. But I’d love to see an option to disallow known leaked password, for example those contained in the Pwned passwords database. The NIST recommends this too: When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised. For example, the list MAY include, but is not limited to: Another problem with the password policy is that users don’t see what it is. The UI only tells that your password does not comply, but it does not help you how to make it work: The password policy enforces the rules when the password is set , so if you change the composition rules they will be effective the next time the users change their passwords. You can enforce a password reset though. As usual, users need IAM policies to manage their passwords. The iam:ChangePassword is the Action that allows changing the password . The ChangePassword allows setting the user’s own password. To allow setting passwords for other users , you need the iam:ChangeLoginProfile permission. As the NIST guideline states: verifiers SHALL force a change if there is evidence of compromise of the authenticator. AWS provides this on the IAM console for each user: It is effective on the next login. When you create a new IAM user and allow Console access you need to set the initial password. You can choose between an autogenerated or a custom password. The more important bit it the “Require password reset” checkbox. When the new users log in the first time, they need to change their passwords. The best practice for new users is to use an autogenerated password and require a reset on the first login: With this process, if you send the initial password in, for example, an email, after the first login it won’t be a secret anymore and only the user will know the password. AWS allows some insight into how users manage their passwords. The most detailed is the Credential report, which lists all users and gives information about when they last changed and last used their passwords. On the enforcement side, the Password policy allows account-wide settings that require character classes, length, and rotation. These are the only controls you have over IAM users’ passwords. Also, you want to make sure only the users know their passwords. To achieve this for new users, enforce a reset on the first login.", "date": "2020-09-15"},
{"website": "Advanced-Web", "title": "How to use async/await with postMessage", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-await-with-postmessage/", "abstract": "The postMessage call allows an asynchronous communication channel between different browsing contexts, such as with IFrames and web workers, where direct function calls don’t work. It works by sending a message to the other side, then the receiving end can listen to message events. For example, the page can communicate with an IFrame via postMessage and send events to each others’ windows. The iframe.contentWindow.postMessage() call sends a message to the IFrame, while the window.parent.postMessage() sends a message back to the main page. The two ends can listen to messages from the other using window.addEventListener(\"message\", (event) => {...}) . page IFrame window.addEventListener(\"message\", (event) => { ... }) js window.addEventListener(\"message\", (event) => { ... }) js iframe.contentWindow.postMessage() window.parent.postMessage() page-IFrame communication For web workers, each worker has a separate message handler. The page can send a message to a specific worker using the worker.postMessage() call and listen for events from that worker using worker.addEventListener(\"message\", (event) => {...}) . On the other side, the worker sends and receives events using the global functions postMessage() and addEventListener() : Both communicating with an IFrame and with a worker has problems. First, sending the request and handling the response is separated. The receiving side uses a global (or a per-worker) event listener , which is shared between the calls. This is good for “notification-style” messages where one end wants to notify the other that something happened but not expecting a response. But notification-style messaging is rare . What is usually needed is a “request-response-style” messaging. For example, one end uses an access token that the other one can refresh. The communication consists of a request to refresh and a response to that with the refreshed token. Or even when a user clicks on a button and the other side needs to handle this. This example seems like a “notification-style” message, but when the button needs to be disabled while the operation takes place (such as a save button) or there is a possibility of an error that the sender needs to know about, it’s now a request-response. The global (or per-worker) message handler is not suited for pairing responses to requests. The ideal solution would be to hide all these complications behind an async function call and use await to wait for the result: Let’s see how to make such a function! The first task is to know which request triggered a given response. The problem with using the global event handler is that it’s all too easy to rely on only one communication happening at a time. When you test your webapp, you test one thing at a time but users won’t be that considerate. You can’t assume only one request-response will happen at any one time. Non-multiplexed channel page code IFrame code msg1 msg2 OK ERROR Which operation failed? One solution is to use request identifiers to pair the responses to the requests. Request ids page code IFrame code {id: 1, ...} {id: 2, ...} {id: 2, result: \"OK\"} {id: 1, result: \"ERROR\"} Request 1 failed This works, and even though it requires some coding, this is a good solution. Fortunately, there is a better solution built into the language. MessageChannel allows a dedicated communication channel attached to the postMessage call. The sending end can listen for messages on that channel which is separated from all other calls, and the receiving end can send its responses through the MessagePort it got. A MessageChannel creates two MessagePorts, one for each end of the communication. Each port supports both the onmessage and the postMessage , similar to the basic cross-context communication channels. To attach a message handler to a port, use port.onmessage = (event) => {...} . If you use addEventListener then you need to start the channel too: To attach a port to the postMessage call, use the second argument: postMessage(data, [channel.port2]) . To use this port on the other end, use event.ports[0].postMessage() . By creating a new MessageChannel for each request, pairing the response is solved as whatever comes through that channel is the response to this particular request. An often overlooked aspect of request-response communication is error handling. When the receiving end has a problem and throws an Error, it should be propagated to the sender so that it can handle it appropriately. The problem with postMessage is that it can only send one type of message, there is no postError call. How to signal an error then? A possible solution is similar to how Node-style callbacks propagate errors using only a single function. These callbacks use an error and a result value together, and only one of them is defined: To implement the same with messages, use an object with error and result properties. When the former is non-null that indicates that an error happened. Don’t forget to wrap the receiving end in a try-catch to propagate runtime errors. With a separated response channel and error propagation, it’s easy to wrap the call in a Promise constructor. With a Promise hiding all the complexities of the remote call, everything that works with async/await works with these calls too: And the Promise rejects when it should, allowing proper error handling on the sending end: One restriction is what can be sent through the postMessage call. It uses the structured clone algorithm which supports complex objects but not everything . The postMessage call allows communication between different contexts, such as cross-domain IFrames and web workers. But this is a rudimentary channel that lacks support for request-response style messages and error propagation. With the use of MessageChannel and a result structure that allows returning errors, it’s possible to implement a robust solution. Taking one step further, using the Promise constructor to hide the complexities of the message handling allows the use of async/await.", "date": "2020-09-11"},
{"website": "Advanced-Web", "title": "How CloudFront routing works", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-cloudfront-routing-works/", "abstract": "CloudFront is a proxy that sits between the users and the backend servers, called origins. When a request comes in, CloudFront forwards it to one of the origins. Let’s see what parts of the distribution configuration decides how the routing happens! Without CloudFront, each origin has its own name or IP address where it can be accessed and clients connect to them directly. For example, EC2 servers can have Elastic IPs, an API Gateway has its own domain under https://<id>.execute-api.<region>.amazonaws.com . Visitor «EC2» Servers [] «S3Bucket» Static assets [] «APIGateway2» API [] 10.0.0.1 example.com ...execute-api. amazonaws.com Direct access But when these services are behind CloudFront, they use only one domain, either the default <id>.cloudfront.net or a custom one. Because of this, host-based routing is not possible. Visitor «CloudFront» CloudFront [] Edge locations «EC2» Servers [] «S3Bucket» Static assets [] «APIGateway2» API [] example.com /server / /api Path-based routing Routing in CloudFront is based on the path of the request . A request that goes to https://<id>.cloudfront.net/api/users has the path /api/users . This is the distinguishing factor that decides which backend server the request goes. Cache behaviors are the unit of configuration that decides what happens with an incoming request. They define how to transform a request and the response, how to cache, what to include or exclude, and most important, which origin to forward to . Each behavior has a path pattern that defines what paths it can handle. This is a filter expression, an incoming request either matches this pattern or not. A path pattern supports the * and ? wildcards, where the former matches 0 or more characters and the latter exactly one. This is not a regex engine and don’t plan to write complicated patterns here. Usually, path patterns fall into one of three categories: As there can be more than one cache behavior that matches a given path ( /api/image.jpg is matched by both /api/* and *.jpg ), CloudFront needs to break this tie. Because of this, there is an ordering between the behaviors . There is exactly one that has the default ( * ) path pattern, which it called the default cache behavior . This matches all the requests and it is always the last one. When a request reaches the distribution, CloudFront starts from the top and tries to match the path patterns for each cache behavior. The first one that matches wins . Each cache behavior defines an origin via its Origin ID. The first matching behavior’s origin will be used for the request . To find the origin configuration, select the origin with the matching Origin ID. This contains the domain where CloudFront forwards the request.", "date": "2020-09-18"},
{"website": "Advanced-Web", "title": "How to use a custom domain with CloudFront and Route53", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-a-custom-domain-with-cloudfront-and-route53/", "abstract": "CloudFront provides a unique URL for every distribution, but that is not something you’d tell your visitors to type in their browser’s search bar. With a custom domain registered, you can add that to the distribution and host your webapp on a custom domain, with HTTPS and using AWS’s global distribution network. This article partially follows the process described in the previous one but uses only AWS services instead of relying on Cloudflare to route to CloudFront. In this scenario, there is a Route53 Hosted Zone created for a domain: Then on the domain provider’s side (Cloudflare in my case) there are NS records to delegate to the Route53 zone: Setting up NS records to point to the Hosted Zone’s nameservers make Route53 the authoritative nameserver for this domain. Whatever records the Hosted Zone has are the records resolvers will get. You may notice that I’m not using the apex ( myawsexperiments.com ) domain but a subdomain ( test.myawsexperiments.com ). This is because Cloudflare does not support adding NS records to the root. There is an option on the web interface but it does not work. The solution described below works the same for the apex too if you can make the Hosted Zone the authoritative nameserver. Apart from the initial DNS setup, there is a CloudFront distribution that we’ll serve under a custom name: The solution involves multiple steps. First, we need to use the AWS Certificate Manager (ACM) to issue a certificate for the domain. This requires modifying the Hosted Zone by adding a validation record. Second, with the certificate in place, we need to add it to the CloudFront distribution. And finally, add the necessary records to the Hosted Zone to point to the distribution. CloudFront Distribution DNS registrar Route53 Hosted zone ACM NS CNAME validation Certificate ALIAS Custom domain with Route53 and CloudFront For a certificate to work with CloudFront, it has to be issued in the us-east-1 region independent of where the distribution is. Before you do anything, make sure to switch to this region. When you request a certificate, ACM first asks for the domain name: Choose DNS validation, then ACM provides the record to add to the domain. In this case, there is a button that auto-updates the Hosted Zone with the validation: If you inspect the Hosted Zone, there is a new CNAME record created by ACM: It takes ACM a couple of seconds to verify the domain ownership and issue the certificate. When it’s done, the certificate goes into “Issued” status: With a certificate issued, the next step is to add it to the distribution. First, add an Alternate Domain Name, then select “Custom SSL Certificate” and select the certificate created in the last step: Make sure to select the SNI option and not use dedicated IPs . No clients need them and using them is extremely expensive. Now that the CloudFront distribution is ready to serve the traffic coming on a custom domain, the last step is to configure Route53 to send that traffic. Add a new A record, select that it is an Alias and select the distribution’s domain from the list. If the interface does not list it for various reasons, just copy-paste it from the distribution’s page. There are a couple of things happening here. An A record is added to the domain, but it does not have a fixed value instead it returns whatever happens to be the CloudFront distribution’s A records when the request was made. This makes it possible to avoid hardcoding any fixed IP address and instead wire the infrastucture using logical names. This fixes a shortcoming in DNS, namely that a CNAME can not be added to the apex, but unfortunately it only works for AWS services. When a visitor goes to the custom domain, his browser contacts Cloudflare first. Then, by getting the NS records pointing to Route53, the browser contacts the Hosted Zone. It looks up the CloudFront distribution’s IP address and returns an A record. The browser then connects to CloudFront and gets the page: Cloudflare Route53 Hosted Zone CloudFront Distribution visitor example.com NS A Alias The CloudFront distribution is routed through DNS All this technical sorcery leads to a rather uninteresting result. When someone visits the domain, he’ll get the website via HTTPS: One thing this setup lacks that the original had is IPv6 support. CloudFront supports it natively, it’s just a checkbox: But the custom domain does not return an IPv6 address because there is no AAAA record added to the Hosted Zone. Fortunately, Route53 also supports the Alias setting for AAAA records, so IPv6 support is just a matter of a few clicks: Testing the domain shows it’s IPv6 ready: The final setup with all the necessary records added looks like this: The NS and SOA records are mandatory elements and they are added when the Hosted Zone is created. The CNAME is the validation record for ACM and it makes sure the certificate can be renewed when needed. And finally, the A and AAAA records point to the CloudFront distribution so that visitors will be served by that. Both are ALIAS records. All AWS services come with pricing tables and a lot of fine print, so it’s a best practice to also take some time and consider the costs. In this case, it’s surprisingly cheap. ACM is free for public certificates, which is the kind used here. CloudFront also has no additional costs for a custom domain on top of the bandwidth and request pricing that is charged for every distribution. The only additional cost is Route53. Every hosted zone costs $0.5 per month, and there is a cost for queries but that’s significantly cheaper than CloudFront request pricing ($1 vs $0.4) and as DNS queries might be cached on multiple levels it might be cheaper still. Overall, adding a custom domain to a CloudFront distribution incurs some additional charges but not much.", "date": "2020-09-08"},
{"website": "Advanced-Web", "title": "How to use a custom domain on CloudFront with Cloudflare-managed DNS", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-a-custom-domain-on-cloudfront-with-cloudflare-managed-dns/", "abstract": "Using a custom domain with a hosting provider seems like an easy thing to configure, but somehow it usually gets a lot more complicated. In this article you’ll learn how to configure a CloudFront distribution to use a domain managed by Cloudflare and what are the important points along the way. This solution is based on a fairly staightforward setup. The domain is registered on Cloudflare and it is its name server. Then the site itself is hosted in a CloudFront distribution that provides access to AWS’s global delivery network. When you create a CloudFront distribution by default it provides a URL that is under the cloudfront.net domain: This hosts the site under a randomly generated URL: While CloudFront by default provides a certificate and a publicly accessible endpoint, it’s not suited for user-facing websites. Fortunately, it’s possible to host the same distribution under a domain you own with full HTTPS support. To make this work, you’ll need a certificate registered in AWS Certificate Manager (ACM). Then you can add the certificate and the custom domain to the distribution. Finally, the DNS needs to point to the CloudFront URL so that when visitors are coming to the site they go to CloudFront. visitor Cloudflare domain CloudFront distribution ACM (1) validate ownership resolve domain (3) resolve CNAME (2) certificate Cloudflare with CloudFront Let’s see these steps! A certificate is needed not only to provide HTTPS support but for CloudFront to allow adding the custom domain. Public certificates are free to request and use. The certificate for CloudFront must be in the us-east-1 region , and it doesn’t matter in what region the distribution is created. Make sure to change to us-east-1 when you request the certificate. During the process, you need to enter the domain first: Then you need to verify that you own the domain. Certificates are powerful things and it’s a major breach if an attacker can get one. You can choose between DNS and email validation. The former is done by adding a CNAME record to the domain. This is the preferred way as as long as the record is in place AWS can refresh the certificate. Email-based validation sends an email with a link to manually verify ownership. This proves that you have access to the email addresses that are associated with the domain’s managers, such as hostmaster@domain . The downside of email validation is that it is a point-in-time proof and you need to manually reverify it from time-to-time. With DNS validation, ACM provides the validation record’s name and value: Go to the domain provider, in this case it’s Cloudflare, and add the record. Make sure that it is only used for DNS resolution only and is not proxied. After some time the validation happens and the certificate’s status becomes “Issued”: With the ACM certificate in place go back and configure the CloudFront distribution. Add the domain name to the Alternate Domain Names box, then select Custom SSL Certificate then the certificate. By using a custom domain, CloudFront offers dedicated IP addresses for “legacy clients” for a hefty price. Make sure to not use this, there are no clients out there who need dedicated IPs. The final step is to point the domain to the CloudFront distribution’s URL. To do this, add another CNAME record, this time to the root with value of the distribution’s domain. Make sure that you don’t use proxying. This seems like a trivial thing, but there are many things going on why it’s possible. By the DNS standard, CNAME records can not be used for the root of the domain ( example.com ) only to subdomains ( www.example.com ). But in the case of Cloudflare, it support CNAME flattening which makes it possible to add this CNAME record. Another complication is that Cloudflare does not support NS records on the root, so you can not make another nameserver manage this domain. You can’t just add a Route53 Hosted Zone and let that resolve the CloudFront distribution’s domain. But these problems only affect the root domain ( example.com ) and not subdomains. It is perfectly fine to add a CNAME to a subdomain, such as www.example.com . And why not use proxying? CloudFront already provides a global distribution network, so there is not much benefit in routing all traffic through Cloudflare’s network too. And it might bring complications down the road. With all these setup, opening the custom domain in the browser shows that it works: It might take some time for the DNS to update everywhere, but it should happen eventually. It usually helps to clear the browser’s DNS cache. In chrome, go to chrome://net-internals , select DNS, then click on the “Clear host cache” button. Inspecting the certificate, the “Issued By” block shows that it is from Amazon: Now you have a CloudFront distribution on a custom domain.", "date": "2020-09-04"},
{"website": "Advanced-Web", "title": "What is CNAME flattening and how it helps redirecting the apex domain", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-cname-flattening-and-how-it-helps-redirecting-the-apex-domain/", "abstract": "The primary purpose of DNS is to translate a domain name to an IP address. It works by sending back an A record with the IP so that the client knows where to connect. This works great when you have a server with a fixed IP that hosts your website. The browser contacts the name server, gets the IP, then downloads the page from your server. user Name server server (1) www.example.com (2) 1.1.1.1 (3) GET (4) index.html A DNS provider translates a name to an IP address Things get more complicated when the server is hosted by someone else. There might still be an IP address, but hardcoding that makes it hard for the hosting provider to change it. We write articles like this regularly. Join our mailing list and let's keep in touch. This is where CNAME records help. They are a pointer to another domain and tells the browser to look up that and use the resulting A record. The hosting provider can give you a domain instead of an IP address and with this setup it is free to change the A records without prior notice. hosting provider Name server for other.service.com server user Name server for example.com (1) www.example.com (2) CNAME other.service.com (3) other.service.com (4) 1.1.1.1 (5) GET (6) index.html CNAME redirection The problem is that while you can use CNAME records for subdomains ( www.example.com ), they are forbidden on the apex ( example.com ) by the DNS standard. While the www subdomain seems like something that is the legacy of the past, there are many cases where you need to use it. Certain hosting providers offer A records as well as CNAMEs. For example, GitHub Pages provides 4 IP addresses in case you want to use the apex domain. But also notice that they provide that as the third option. Similarly, Netlify also documents an IP address that you can use. This time it’s only 1 IP. Note that by providing these IP addresses, these services give up some flexibility. Now that many sites depend on them they can’t change these records easily. It’s also harder to add more addresses, for example for load balancing. On the other hand, other services don’t provide an IP address to use in an A record. For example, to host an S3 bucket website on a custom domain you can only use a CNAME record as AWS does not provide a stable pool of IPs. Similarly, CloudFront provides only a domain name of the distibution and no dedicated IP address to use for DNS. You can easily set up a subdomain with a CNAME record pointing to the distribution’s domain, but this can not be done for the apex. Of course, every DNS resolution results in a set of A records: But don’t be tempted to hardcode them for the apex domain. By not having them documented, they can change without prior notice and that breaks your site. And they do change: Fortunately, some DNS services started coming up with a solution to this problem. CNAME flattening from Cloudflare was probably the first to support redirecting the apex. After that, other providers started implementing solutions under slightly different names, such as ALIAS and ANAME . For example, a Cloudflare domain can be configured with a CNAME on the apex: Under the hood, Cloudflare dynamically resolves the A records for the target domain. When someone queries the domain, only A records are returned, which is in-line with the DNS spec: Cloudflare also makes sure to respect the TTLs of the records it used to get to the IP addresses. This way they are automatically refreshed and kept synchronized with the target. Unfortunately, there is no standards-based support for CNAMEs@root. All the above are workarounds implemented by the DNS providers but this functionality is far from being the norm. And as far as I know, there is no movement on making a standardized solution. Because of this, you need to know what your DNS provider supports. Because of the lack of standardization when a provider supports ALIAS/ANAME/CNAME flattening you need to check if that is the same as what you think it is. For example, Route53 supports ALIAS records but that only works for AWS services and it is not a general solution.", "date": "2020-09-01"},
{"website": "Advanced-Web", "title": "AWS IAM deep dive: Identity-based, and Resource-based policies", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-iam-deep-dive-identity-based-and-resource-based-policies/", "abstract": "The IAM policy is the cornerstone of security of an AWS account. There is an oft-repeated notion of the shared responsibility model that stipulates that certain parts of security is the responsibility of the cloud provider, such as preventing malicious parties physical access to the infrastructure. On the other hand, the cloud provider gives you security controls and it is your responsibility to use them properly. The main security control is IAM and it is used everywhere inside AWS. With IAM you are not only able to create users for people who need access to the account but also to define what each user has access to. There are several types of policies, but the two most important ones are the identity-, and the resource-based policies. Whatever you do in your AWS account you’ll need to use these. Identity-based policies are the easier of the two, as some variation of it is present in most systems. They define what an identity can do , such as a user has access to an S3 bucket. There are two types of identities in AWS: users and roles . By attaching a policy to an identity you can give it permissions to access resources. For example, you can attach this policy to allow a user to read an object in an S3 bucket: Since the policy is attached to an identity, a user in this case, there is no Principal element in the policy. On the other hand, the Resource defines the object in the bucket that the user can access. user bucket policy Action: s3:GetObject Resource: <bucket> Allowed A policy attached to the user gives access to a bucket Note that groups are not identities but a way to attach policies to multiple users. There is only one type of user that can be created, but depending on how they are used we can distinguish natural and technical users. Natural users are created for people. Bob, who is responsible for the S3 buckets, gets a user with the permissions needed to do his work. These type of users usually have a password to sign in to the console and sometimes access keys to allow programmatic access to the account. Since the user is created and managed by a single person, that person is responsible for it. Also, it’s likely that multiple people have the same job, so permissions are usually attached to groups instead of the users themselves. Bob IAM User bucket policy Action: s3:GetObject Resource: <bucket> uses Allowed A natural user is tied to a person Technical users allow other services access to resources inside the account. For example, a web application hosted outside of AWS where users can send notifications to an SNS topic. In this case, you can create a new user, generate an access key and include that in the web application’s backend. Then you attach a policy so that it can publish a notification to the topic. Third-party system AWS backend IAM User topic policy Action: SNS:Publish Resource: <topic> visitor call Access keys Allowed A technical user gives programmatic access to the AWS account Roles are temporary credentials that other identities (users and roles) can assume to gain access to the permissions the role has . For example, when a bucket can only be accessed by a role then if a user can assume that role he then can access the bucket indirectly. user IAM user Role policy Action: s3:GetObject Resource: <bucket> bucket Uses Assumes Allowed A role gives access to resources There are many use-cases for roles inside AWS. To give access to your account, you can use a role that can be used by identities in another account. This way you don’t need to create users and transfer secrets (passwords or access keys) to grant cross-account access. Another use-case is users logging in via SAML or Cognito. Instead of creating a user for every single person who can log in, you define a role and all users assume it. Then you can give permissions to the role and all users signed in this way will gain those permissions. Also, AWS services use roles to gain permissions to access your account. For example, CloudTrail can put logs into a CloudWatch Log group, but it needs permission to do so. On the Trail config, you need to specify a role: This role allows the CloudTrail service to assume it: And the role has the policies to access CloudWatch Logs: This way you have full control over what a given service can do. And this pattern appears in many places. A Lambda function uses a role in a similar way, the S3 bucket replication gains permissions to write a bucket using a role, and AWS Organizations creates a role in the member account so that you can initialize the resources inside it. What if there is no identity to attach the policies to? This is the case for anonymous access, and also when an AWS service does not use a service role, such as an API Gateway. When there is no identity, identity-based policies can not be used. Many, but not all, AWS services support resource-based policies . These are policies attached to the resource and these can give access when there is no identity. S3 buckets support bucket policies which can be used to give anonymous access to the bucket: The * Principal means “everybody” that includes non-users. Another example is how Lambda allows an API Gateway to call it . This gives the lambda:InvokeFunction permission to one API: Similarly, the role’s trust policy is a resource-based policy . This allows services and identities to assume the role. For example, a cross-account access role can use this trust policy to allow access from a different account: Another example is to allow the Lambda service to use this role: The Action in resource-based policies can be one that is relevant to the service. A bucket policy can allow s3:GetObject , a role the sts:AssumeRole , while a Lambda the lambda:InvokeFunction . And the Resource, when needed, can only start with the resource’s own ARN. This makes sense as one resource can only give access to itself or things below it but not to unrelated resources. How the resource defines who gets access and how specific this control is depends on the service. The Principal element is always present and that can define users, roles, services, other accounts, and a few other things . But, especially with services, this is not granular enough. For example, when a role’s trust policy defines the Lambda service, it does not specify which Lambda function to trust. Similarly, the role for the CloudTrail allows the cloudtrail.amazonaws.com service, but there is no further restrictions on which trail . Some resource policies support conditions to allow only a given resource. For example, the policy for a Lambda function is able to selectively allow the API which can call it: Unfortunately, there is no generic way to know which resource-based policies support further restrictions on the caller using conditions. You need to check the AWS-provided policy examples to see what conditions are possible. When both types of policies are effective then you can use both to give and restrict access. When an IAM user wants to get an object from an S3 bucket then both the policy attached to the user and the one defined on the bucket are effective. With most services if either of the policies give access to an action then it will be granted. The notable exceptions are KMS key policies and cross-account role trust policies where both the resource and the identity needs an explicit Allow. As resource-based policies can grant access to identities, the notion that a user can not do anything without attaching policies to it is not true. For example, a bucket policy can allow a user to get objects and that is effective even if the user has no permissions on its own: user1 policy Action: s3:GetObject Principal: <user1> bucket Allowed A bucket policy allows access to a user On the other hand, policies can also deny access and that trumps everything else. When a deny policy is attached to an identity then it defines what that identity can not do . When such a policy is attached to a resource it specifies who can not access that resource. Denies in resource policies are powerful security controls. You can define a set of identities and you can be sure that for anyone outside that the access will be denied. For example, this policy denies access to everybody except for a given user: The NotPrincipal element matches everybody who is not a given principal and applies a Deny. As long as this policy is in effect, you can be sure that no matter what permissions other users have they can not access this object. Even administrators are denied. user1 admin policy Action: s3:GetObject NotPrincipal : <user1> Effect: Deny policy Action: s3:GetObject policy Action: * bucket Allowed Denied A bucket policy limits access to a single user But, of course, there is a catch here. A user with enough permissions can remove or alter this policy, or add a new access key to the user who is allowed access. That would be a destructive process that leaves an easily-identifiable trace in CloudTrail but it is very hard to prevent. It’s a best practice to look at permissions not as a static property of the system but something that can change in ways encoded in itself. That said, a resource policy that denies access is still a powerful construct that makes it easier to think about security. IAM policies are powerful controls and knowing what types are available is critical for a secure environment. Identity-based policies are more familiar, but resource-based ones usually offer even stricter controls.", "date": "2020-08-28"},
{"website": "Advanced-Web", "title": "How to add HTTPS for an S3 bucket website with Cloudflare", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-add-https-for-an-s3-bucket-website-with-cloudflare/", "abstract": "AWS S3 provides a simple way to store files in a highly available object storage and it also supports setting up static websites. If you don’t need dynamic content then using an S3 bucket is an attractive hosting option. You pay practically nothing for storage and you can forget about scalability. There are two problems with this approach. It does not support using your own domain name, and connected to it, it does not support HTTPS. To solve these problems, you need to use another service along with S3 that provide these missing features. One option is Cloudflare that offers a proxy that supports S3 bucket websites and it can be used with a custom domain name. We write articles like this regularly. Join our mailing list and let's keep in touch. This article describes the steps you need to do to set up your own static website hosted in S3 with Cloudflare providing the encryption and the DNS setup. As a prerequisite, you’ll need your own domain name, registered at Cloudflare or elsewhere. To host the static website on your own domain with Cloudflare, you need to set up the two services, S3 and Cloudflare. The former stores the website and provides a HTTP interface while the latter manages the proxy and the HTTPS setup. S3 bucket index.html style.css Domain provider Cloudflare Bucket website Bucket policy NS CNAME S3 bucket website with Cloudflare The first step is to create a bucket and upload the contents. This can be done with the CLI or manually using the Console. The name of the bucket is important. It must be the domain of the site otherwise it won’t work. This is because S3 uses the Host header that the browser sends to know which bucket to use and that header is the site’s domain. Next, you need to unblock public access. A bucket website inherently exposes all the objects and those have to allow anonymous access. In the recent years security breaches due to a misconfigured bucket became more numerous, such as this and this breaches, and AWS started to do something about it. Now there is a prominent label on the bucket when its policy allows public access, which is a good thing. It allows someone to verify that a bucket is private without decyphering the bucket policy. The other hurdle is that public access is blocked for new buckets and you need to explicitly disable that to allow public access. I have mixed feelings about this, as buckets are private by default and you need to enable public access. Now you need to enable it in two places instead of one. So to make the bucket website work, make sure to disable the public access block: Then, of course, you need a bucket policy to allow anonymous access: And finally, enable bucket website hosting. Note the endpoint URL as that is where the website is available: When you open the URL in the browser, it works: But notice two things. First, it is reported as “Not secure”, meaning it uses HTTP instead of HTTPS. Second, it uses the bucket website URL instead of your own domain. With bucket websites this is the furthest you can go. S3 does not allow any configuration to host the site on a different domain, and there is definitely no way to add a certificate to provide HTTPS. You need to use a different service for these features. Fortunately, there are several ways to go from here. In the rest of this article, we’ll look into how to configure Cloudflare to provide the missing features. To use Cloudflare for your domain, you need to use it as the nameserver for the domain. If you’ve transferred your domain to Cloudflare then you don’t need to do anything else. If you use a different domain provider you need to add two NS records pointing to Cloudflare. The exact process depends on the domain provider, but there is detailed documentation on the Cloudflare docs. Then you need to setup Cloudflare DNS. Add a CNAME record at the root with the domain name of the S3 bucket website (the part without http://). Make sure that the “Proxy status” is “Proxied”. There are quite a few things going on behind these settings. First, by the DNS standard, CNAME is not allowed on the apex of the domain. The apex it the “naked” domain, which is how it is registered, such as example.com . In contrast, www.example.com is not the apex. This would prevent CNAME redirection and prompt you to use a subdomain such as www . To work around this, Cloudflare provides CNAME flattening . This resolves the A records the CNAME is pointing to and returns those. As a result, you can use CNAMEs on the apex while maintaining compliance with the standard. Second, the content points to the domain of the S3 bucket website, but this is not what defines which bucket it points to. Instead, it uses the Host header of the HTTP request which is the domain name managed by Cloudflare ( example.com ). Interestingly, it would work if you only specify the end of the bucket website domain ( s3-website-eu-west-1.amazonaws.com ). And third, the proxy setting defines that Cloudflare will stand between the visitor and the bucket website. This is how it can provide HTTPS. Since the bucket website uses plain HTTP, you need to configure “Flexible” mode: This setup provides an encrypted connection to the visitors but an unencrypted one to the bucket website. Finally, as a best practice, enable automatic redirection from HTTP to HTTPS. This makes sure that traffic will be encrypted but the website will work even if accessed via HTTP: With all these setup, opening the domain in the browser shows the website via a secure connection: The browser shows the padlock, but the encryption is not end-to-end. While the traffic between the visitors and Cloudflare uses HTTPS, the part between Cloudflare and S3 does not. Instead, it uses the public Internet to fetch the website plain-text. This suggests a false sense of security. If an attacker can capture the traffic between the browser and Cloudflare and also between Cloudflare and S3 he can correlate the requests made by a given user. This defeats the purpose of encryption. On the other hand, the most critical part of the connection is near the user. The public WiFi network that can be easily monitored or the hotel network traffic that flows through a single router is way more dangerous than the data flowing between the two data centers. The main advantage is that the website is available on your own domain name and it hat the padlock icon. Cost-wise it’s almost free. You need to pay for storage, requests, and bandwidth on the S3 side, but Cloudflare is free for this use-case. A great thing about this setup is that you don’t need to configure the fine details of HTTPS. There are quite a few options and it’s easy to make mistakes . There is even an online tester to verify your setup. With Cloudflare managing the certificate and the encryption it makes sure the setup is secure. With Cloudflare’s CNAME flattening feature it is possible to use the apex domain without the risk of breaking some clients. This is a problem some DNS providers have and it’s a pain to solve. Also, this setup routes traffic through Cloudflare’s network and that offers more than just routing. It adds caching, provides some DDoS protection, as well as you can specify always-on error pages. The main disadvantage is that you need to use two different services. This makes it harder to maintain in the long run, but in my experience there is not much things to do after the initial setup. And the lack of end-to-end encryption compromises some security.", "date": "2020-08-18"},
{"website": "Advanced-Web", "title": "How to route to an arbitrary S3 bucket website with Cloudflare Workers", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-route-to-an-arbitrary-s3-bucket-website-with-cloudflare-workers/", "abstract": "I happily used Cloudflare with S3 bucket website to host static sites on my own domain with HTTPS practically for free, as described in the previous article . This setup works quite well and requires no maintenance. But I’ve noticed a strange thing during the setup. The name of the bucket has to exactly match the domain otherwise it won’t work. Even more strangely, the value of the CNAME seems to not matter. My setup looks like this, notice that the domain and the name of the bucket are different: Cloudflare S3 mywebsite-cloudflare-test myawsexperiments.com index.html style.css Bucket website CNAME The domain and the bucket are different The CNAME is set to the bucket website’s domain: But when trying to open the page, it results in an S3 error: We write articles like this regularly. Join our mailing list and let's keep in touch. Notice that it tries to find the bucket website for the domain instead of the one that is specified in the CNAME. Why is that? It turned out that S3 uses the Host header instead of the URL to find the bucket. And because the Host header flows through Cloudflare without any changes, S3 website tries to get the bucket using the domain. When a browser makes a request, the domain and the Host header matches: But in case they are different, S3 uses the header to find the bucket: user S3 website bucket1 bucket2 URL: bucket1 .s3-website... Host: bucket2 .s3-website... The Host header specifies the bucket In fact, the bucket name in the website URL can be omitted. As long as the region is the correct one S3 will find the bucket: What it has to do with Cloudflare? In “Proxied” mode, Cloudflare sends a request to the CNAME domain but it uses the same Host header as it got from the visitor’s browser. user S3 website Cloudflare myawsexperiments.com mywebsite-cloudflare-test URL: myawsexperiments.com Host: myawsexperiments.com URL: mywebsite-cloudflare-test.s3-website... Host: myawsexperiments.com Host Cloudflare forwards the Host header Because of this behavior, you need to use the same name for the bucket as the domain. But buckets must be globally unique, which means it might be taken from you or you need to use a different one for other reasons. Cloudflare supports a page rule to rewrite the Host header, but that is only available on their paid plans. Fortunately, Cloudflare also supports computing on the edge, called Workers, and that allows fine control over how the website is fetched. And to a limit, workers are available on the free plan. To set up a bucket website, you need to create a bucket, upload the files, then allow anonymous access. For more details, see the previous article . Then activate the website hosting and take note of the Endpoint URL: Now that the S3 side is set up, configure Cloudflare. The DNS and the HTTPS settings are the same as before . The last part is to create a new worker that fetches the bucket website contents with the correct Host header. This solution is based on this comment . Go to the Workers page on Cloudflare and create a new one with this code: This sends an unrelated request to the S3 bucket website which makes the Host header to be the bucket’s name instead of the custom domain. Note that this is a rather simple implementation that only takes the request path into consideration and won’t forward query parameters and cookies. This is fine for a static site hosted in S3 but it’s not a universal solution. Then create a new route and associate the worker with that: user S3 website Cloudflare worker myawsexperiments.com mywebsite-cloudflare-test URL: myawsexperiments.com Host: myawsexperiments.com URL: mywebsite-cloudflare-test.s3-website... Host: myawsexperiments.com URL: mywebsite-cloudflare-test.s3-website... Host: mywebsite-cloudflare-test.s3-website... Host Cloudflare worker sends the request with the correct Host Now when there is a request to the domain the worker fetches the correct bucket website: Workers offer a great way to customize how Cloudflare proxy works, but they are not free. On the other hand, there is a generous free tier that should be enough for small sites. In the free tier, the first 100k worker request per day is free, but it applies to the account , meaning if you have multiple sites their usage adds up. Also, keep in mind that it is per request , so if your site loads 10 CSS and JS files in addition to the HTML, a visit will consume 11 requests. Apart from the daily limit, there is also a burst limit of 1000/min, which translates to ~17 request/sec if sustained for a longer time. This does not sound much, making the free tier only suitable for small sites. To make the most of the free tier, make sure you minimize the amount of requests going to Cloudflare. Concatenating static files, revving, and utilizing client-side caches help a lot. And when the free tier runs out you can always start paying based on usage. While it’s not as attractive as a forever free plan without workers, but at least it scales with usage. Cloudflare workers is an excellent feature as it allows arbitrary code to affect how routing works. With just some coding you can overwrite how it works with S3 bucket websites.", "date": "2020-08-25"},
{"website": "Advanced-Web", "title": "The consequences of memory allocation size for a Lambda function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-consequences-of-memory-allocation-size-for-a-lambda-function/", "abstract": "The memory allocation for a Lambda function can go from 128 MB to over 3 GB. This directly affects the memory available to the function and also increases the price so you are encouraged to set it as low as possible. But this setting is not just about memory, as it affects CPU allocation, running time, CPU variability, and function scalability. The available memory directly affects the function. Giving it too much is wasteful, while too little can lead to memory-related errors. Fortunately, when Lambda logs the function execution to CloudWatch Logs, it also reports the maximum memory used: This gives a good estimate on how much of the allocated memory is used. Furthermore, CloudWatch Logs Insights can graph these values so that it’s easy to see the outliers over a longer period of time. This filter expression reports the mimimum and maximum of the memory usage, aggregated in 10 minutes: This gives a graph similar to this one: Knowing the actual memory usage is a good starting point for optimizing the allocation. When the available memory is more than the used then you can safely lower the setting without worrying about errors. On the other hand, the used memory usually does not indicate a hard requirement . Most runtimes are using garbage-collected languages, which means they use more memory than what is strictly necessary and run the GC when needed. A lower memory allocation means more frequent GC runs which might slow down the function, but won’t result in memory-related errors. Knowing the true minimum requires experimentations with different settings. A side effect of allocating memory is that it also allocates the CPU power available to the function. According to AWS, a full vCPU core is allocated for 1.792 MB of allocated RAM, and it scales linearly with memory. This yields ~7% vCPU for 128 MB of RAM. I believe AWS uses standardized machines for Lambda and the function gets some percentage of its resources, so by allocating memory you are in fact allocating the other resources with it too. And they just decided that memory is easier to grasp than some sort of vMachine metric that translates to memory and vCPU. As more RAM means more CPU power it also translates to shorter running times. Cost of a function invocation is <duration> * <allocated memory> . But the duration is rounded up to the next 100ms mark and that makes a curious economic situation. Since the allocated memory linearly affects the allocated CPU which in turn at most inversely change the duration (doubling the memory doubles CPU, which in turn at most halves the duration), increasing memory wouldn’t mean cheaper execution. But with the discretized duration, it can. If you can decrease the duration to fall into a lower bracket, that can have a significant effect on the overall cost. The difference between a runtime of 105 ms vs 95 ms might be small in terms of additional CPU power, but the billed duration is halved. There are even tools that measure the runtime for different configurations and help with coming up with a sweet spot. More CPU means shorter running time. After all, serverless still runs on a computer and we’re used to the thought that a computer finishes sooner if it’s faster. But this does not translate directly to all kinds of functions. Increased CPU means shorter running times only if the CPU is the limiting factor . There are workloads where this is true, but most functions are not like that. A typical function validates and transforms requests and responses and communicate with other services. This means most of the time is spent waiting which won’t benefit from a faster CPU. Let’s look at this function execution: The total running time was 2.8 seconds, but 2.6 out of that was calling S3 and DynamoDB. Adding more processing power speeds up some part of those calls, but the effect would be a lot less than anticipated. That’s why most benchmarks profiling the effect of memory to running time do things like prime number testing or Fibonacci sequence generation. Keep in mind that those findings might not directly translate to your workload. A common misconception is that above 1.8 GB the function gets access to a second CPU core. This is based on the AWS documentation stating that 1.792 MB translates to 1 vCPU core: Lambda allocates CPU power linearly in proportion to the amount of memory configured. At 1,792 MB, a function has the equivalent of one full vCPU (one vCPU-second of credits per second). But notice that it does not mention the number of available CPU cores or the capacity of the physical core. In reality, all functions have access to 2 cores no matter how much memory is allocated: What happens is that the function gets more CPU credits than it can spend using only one core . When that happens, having more credits won’t translate to faster runtimes unless the second core is also used. According to benchmarks this happens above 2.048 MBs. If your function can not utilize the second core then it won’t get CPU benefits above that setting. NodeJS, for example, uses only one core for the event loop. While certain operations, like reading from the filesystem, are multithreaded, the function code won’t expand to the second core. Unless you use web workers or webassembly, your code is unlikely to benefit from increasing the allocated CPU beyond saturating one core. An interesting find is that the CPU credits seems to affect only the minimum amount of CPU the function can use, but when there is spare capacity it can run faster. A particularly well-made benchmark found that the function with 128 MB RAM allocated occassionally runs as fast as another with 12 times the processing power. While this test was done in 2017 and there is no official documentation about this behavior, it might work differently at this moment or can change in the future. If this behavior is still true, that leads to functions that are sometimes fast but usually slow. While using spare capacity seems like a good idea, excess variability in the running time can lead to unwanted side effects. If you happen to profile the function during a fast period you’ll see incorrect results. Also, it might trigger a warning in a monitoring system that suddenly sees increased response times. Allocating more RAM increases the processing power and that decreases the wriggle room which leads to smaller variations. Choosing between fewer and faster functions and more but slower ones has an impact on the scalability. AWS’s promo line for Lambda on the console is “Run Code without Thinking about Servers”, but that is misleading. In a hypothetical perfect serverless architecture, choosing between faster and slower functions has no consequences besides the response time. But in reality using fewer instances has a few additional benefits. First, Lambda can not scale from zero to infinity in an instant. There is a burst concurrency limit that is between 500 and 3000, and after that there is a scalability limit of 500 / minute. With fewer function instances, these limits translate to more processing power. Another benefit of fewer instances is that functions can share data between invocations but not between instances . This comes handy in cases where the function needs an in-memory cache, for example to keep under the Parameter Store limits . Fewer instances make fewer calls to the service. And of course, cold starts happen rarer when there are fewer instances. The memory setting for a Lambda function is not just about memory but instead a proportion of all the resources that virtual executions share. Most notably, it affects the allocated CPU. Bring in the cost model of Lambda and you end up with a lot of small changes behind a single number.", "date": "2020-08-04"},
{"website": "Advanced-Web", "title": "How to completely lock down an AWS account with a service control policy (SCP)", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-completely-lock-down-an-aws-account-with-a-service-control-policy-scp/", "abstract": "Cloud accounts are usually seen as machinery, humming away 24/7, working all the time to fulfill a business need. This is certainly true for production applications, but most workloads are not like this. A developer goes to an office in the morning, works until late afternoon then goes home. When he is not working, the account just sits there, doing practically nothing. And this happens for longer periods of time too. Someone might be working on a project then moves to another one just to come back to the first one months later. The problem is that these inactive accounts still have all the power of a normal account, and they significantly increase the attack surface. It doesn’t matter if an account is unused for months if a misplaced access key leads that it starts mining Bitcoin. It would be better to inactivate AWS accounts when they are not needed. Fortunately, it is possible with service control policies. Service control policies (SCPs) allow outside control over the account . Because of this, this is the most powerful policy type in AWS as there is no way around them, even the root user is limited. When an SCP is in place there is no way to remove it or negate its effect without accessing the master account. The SCP is a feature of AWS Organizations which is the service that allows creating multiple accounts without going through the usual process of registration. With Organizations, you can create a new account programmatically and it allows managing hundreds and thousands of accounts. The account that has billing information filled in is called the master account . This is the one you created the traditional way and charges your bank card. The accounts created using Organizations are called member accounts . These are dependent on the master account for billing. Organizations allow the master account to attach SCPs to member accounts to limit what they can do. These are IAM policies that are evaluated before anything else in the policy evaluation logic. With SCPs an account can be limited in which regions it can use resources, what services it can use, and even what resources it can not delete or modify. Without access to the master account these policies can not be changed or removed. This makes SCPs a great tool to enforce limitations in an account. The master account can not be limited by an SCP. To prevent an account from being used an SCP that denies everything can be used: This puts an explicit deny on all actions on all resources, so everything that can be denied will be. Certain things like logging in or running the sts get-caller-identity can not be denied but everything related to resources can. Note the policy ID as we’ll need that to lock down an account: Now that the account structure and the lockdown policy is ready, let’s see how to use the AWS CLI to lock and unlock a member account! Locking an account is attaching the lockdown policy to the account. Using the master account , use this call: This attaches the policy and that prevents anything from happening inside the account. No new instances can be run, no information can be queried about any resource inside it, even Lambda functions that are already added won’t work. Here is how the IAM console looks like with an admin account: And the best thing is that these restrictions also apply to the root user. In practice, as long as this policy is attached the account is unusable in any ways. To unlock the account and get back to normal all you need to do is detach the policy with the master account: Let’s bring this a step further! With some scripting it is possible to unlock the account for a shell session and auto-lock it when the session ends. Effectively, this setup minimizes the time the account is usable to when it’s needed which is perfect for a development account. Here’s a simple script to achieve that: It unlocks the account then starts zsh . When the shell finishes by typing exit , the account gets locked automatically. Furthermore, it sets the AWS_PROFILE environment variable so that AWS CLI calls inside the shell session go to the member account. The AWS_SDK_LOAD_CONFIG is needed for the AWS SDK to support the profile setting. A corresponding profile can be configured for the AWS CLI: The above setting works when you have a user specified in the credentials file. But in my case it used the AWS_ACCESS_KEY_ID , the AWS_SECRET_ACCESS_KEY , and the AWS_SESSION_TOKEN environment variables and this setup does not play well with profiles. When both the keys and the profile is set in the environment the former takes precedence and that prevents the jump into the account. The solution is to create a temporary credentials file and unset the keys from the environment: Service control policies are powerful security controls that are greatly underutilized. With some configurations, an account that is only usable when needed is possible, and a setup like this greatly improves security.", "date": "2020-08-11"},
{"website": "Advanced-Web", "title": "Generating a crossfaded slideshow video from images with ffmpeg and melt", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/generating-a-crossfaded-slideshow-video-from-images-with-ffmpeg-and-melt/", "abstract": "When I was working on a video course I needed a way to turn a series of slides into a video, preferably with a nice transition in between. I had the slides in PNG format, and I also had the timing information, i.e. when the transition to the next slide should happen. Then I started looking for a solution to turn those images into a video and that turned out to be more involved than I initially thought. This article is the summary of my approaches and how I found each of them. There is a GitHub repository where you can find the code and try the approaches detailed below. The step before jumping into the exploration of video generation is to programmatically generate images. Fortunately, it’s rather easy with node-canvas . The sample images the sample code generates are giant numbers in 1920x1080 resolution. We write articles like this regularly. Join our mailing list and let's keep in touch. The result looks like this, with configurable number of images and durations: And how does the part that gets the images and outputs the video looks like? I ended up implementing it in 3 different ways as I experienced runaway memory consumption with the first two. Note that the three solutions described above generates a video that looks the same , but I did not compare them frame-by-frame and there might be some differences in length. The first solution that looked promising was this excellent answer . It turned out to be exactly what I need and it wasn’t hard to implement in NodeJS. This solution uses ffmpeg to convert the images into a crossfaded video slideshow: The good thing about ffmpeg is that it is everywhere and it can be installed easily: It works by constructing a series of filters that overlay the next image with a fade-in effect. The input images are looped with the -loop 1 so that they are treated as a video instead of a single frame. Then they are repeated for a duration they are shown ( -t 32.5 ). The magic is inside the filter_complex , which seems quite complicated but only until you know the structure. Each filter defines one step in a pipeline and it has inputs and an output. The structure follows this pattern: [input1][input2]...filter[output]; . Some filters, like the fade needs only one input, while the overlay requires two. The [0] , [1] , … are the input files, and later filters can use the outputs of earlier ones. The first filter is this: [1]fade=d=0.5:t=in:alpha=1,setpts=PTS-STARTPTS+32/TB[f0]; . This gets the second image, then adds a fade-in effect that starts at 32 seconds (defined in the setpts parameter). Then it outputs the result as f0 . Then the second filter does the same but it gets the third image ( [2] ) then adds the fade-in effect at 64 seconds. Its output is f1 . When all but the first image is faded, it moves on to construct an overlay filter chain. The [0][f0]overlay[bg1]; gets the first image and the faded-in second image and runs the overlay filter. Its result is bg1 . Then the last filter overlays f1 over bg1 and outputs v . Since there are only 3 images in the slideshow, this is the last part. Finally, -map \"[v]\" tells ffmpeg that the result video is the one that comes out as v . Visually, this filter configuration looks like this: [0] [1] [2] [f0] [f1] [bg1] [v] fade fade overlay overlay Fade and overlay filters With this structure, it’s easy to add more images. Just define the new input, add a new fade filter that generates the faded-in version, then add a new overlay that merges the current result and the new image. It does the job quite well for a few input images. But it uses more memory the more images it gets, and my dev box with ~2GBs of RAM runs out at ~50 inputs. And this is a problem as sometimes I have more than 50 slides. So I had to restart my search for a solution. Then I stumbled on this comment that suggests that a new ffmpeg filter, xfade is better suited for this kind of job. One downside is that xfade is a new addition to ffmpeg and it is not in the mainstream repositories. But hopefully it won’t be a problem in a few years. Since this is a solution specifically to crossfade inputs, it is easier to implement : It uses only the xfade filter that gets two inputs and configurations for the duration and the start time, and outputs the result. The first filter gets the first and the second images ( [0][1] ), adds a crossfade at 32 seconds and names the output f1 . The next filter gets this and the third image, adds another crossfade and outputs as v . Then the -map \"[v]\" tells ffmpeg that this is the final result. [0] [1] [2] [f1] [v] xfade xfade Xfade filter While this solution is easier to implement, it turned out to be as bad in terms of memory consumption as the previous one. I’ve been using Shotcut, a libre video editor, and that is build on the MLT framework. It generates an XML file then uses a dedicated tool called qmelt to render the video. qmelt , it turned out, is a simple wrapper around melt , which is the MLT framework’s dedicated CLI tool to process videos. Since Shotcut is capable of crossfading between images and rendering itself is not dependent on Shotcut, I should be able to generate the XML programmatically and feed that to melt and that should have the same effect. After reading the documentation, melt can be configured with CLI arguments alone instead of an XML file for simple cases. And crossfading between images is supported. This made it quite easy to configure : The order of arguments are important here. The <file> out=... lines are the input files, where the out number defines the number of frames the image is visible. Then the -mix 12 -mixer luma mixes the last two parts with a length of 12 frames. This yields a simple structure: add 2 images, do a crossfade, add another image, do another crossfade, and so on. 0.png 1.png 2.png 0+1 0+1+2 mix mix Melt structure And the good news is that the memory usage seems not dependent on the number of inputs or the video length. This makes it a suitable solution. Melt is included in the major repositories, and besides a few warnings that it needs a display, it works fine in a server environment. At this point I had three solutions that produced videos that looked the same and I had a hunch that ffmpeg somehow consumed more memory as the number of input images increased. But I wasn’t sure whether that was due to the fact that it also increased the overall length of the video or just the number was the culprit. So I run some benchmarks to see into this. First, let’s see how the number of images affects the running time! Each image is shown for 2 seconds, so each additional image increases the total video length by a fixed amount. 0 150 300 450 600 Render time (s) 0 15 30 45 60 Number of images ffmpeg with xframe ffmpeg with fade + overlay melt The lines are straight lines up until a point where the two ffmpeg-based solutions shoot up. I suspect that is the point where the available memory runs out and the system starts using the swap space. With melt, there is no break in the line and I didn’t observe increased memory usage as the tests run. But let’s see if the memory usage is the result of the number of input images or the total length of the video! To measure that, I used 3 images but changed how long each image is shown. This keeps the number of inputs fixed but the video length changes. 0 200 400 600 800 Render time (s) 0 35 70 105 140 Seconds each image is shown ffmpeg with xframe ffmpeg with fade + overlay melt All three configs are straight lines, so it seems like the video length does not matter. I was surprised to see how versatile ffmpeg is with all its filters and its stream hierarchy. Once I understood the structure of the filter_complex it was easy to see how the parts work together and provide the crossfade effect. But ultimately, it turned out not to be the good tool for the job. Another surprise came when I saw how powerful the MLT framework is. This time it was enough to use the CLI parameters, but I see the potential of assembling a more complex XML and feeding it to melt to implement all sorts of effects that are usually only available in visual editors. Do you have an idea how to fix the ffmpeg memory usage? You can play with the code in the GitHub repository and let me know if I missed something important!", "date": "2020-07-21"},
{"website": "Advanced-Web", "title": "How to use async functions with gapi, the Google Drive client, and the file picker", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-gapi-the-google-drive-client-and-the-file-picker/", "abstract": "Quite some time has passed since I last worked with Google APIs but now I had a project to use them. I needed a solution that involved choosing a folder inside Google Drive and listing the contents inside it. Simple enough, but when I looked at the reference implementations I felt like going back a few years. Async/await is mainstream now, providing a code structure that is simpler and easier to read, but all official code examples used a convoluted mess of callbacks for the various async tasks such as initializing a library or waiting for a result. Sure, the example codes work and that is their primary purpose. But it took me some time to clean up everything and come up with a solution I’m happy with. This article walks through a GDrive-based webapp and describes how each part can work with async functions and Promises to provide a modern application structure . What I needed was a simple GDrive-based workflow. First, the user needs to log in, granting access to read the files. Then the app shows a file picker to allow choosing a folder. Finally, it fetches a list of all files inside the selected folder. This yields 3 distinct steps in regard to functionality, but there are several more async steps along the way, most of them for initializing or loading a component. The full async workflow looks like this: Load gapi Log in Init auth2 Logged in? Show login button yes no login Select a folder Init picker Show the Picker pick folder List contents Get list more results? finished List a folder in Google Drive The first step is to load the library script called gapi. This is the main entry point for all Google-related API calls and it is in a js file loaded from https://apis.google.com/js/api.js . Loading scripts is usually a trivial thing, just add the <script> tag and the file is loaded for scripts following it. But loading third-party scripts should be done asynchronously not to block everything else. This is done by adding the async property. But how to make sure the gapi is loaded when it is needed? Most examples use the onload to call an init function and put the code inside that function, such as: This is a “push” model , as the script loading calls the function, which has some drawbacks . What if there are multiple scripts to load before running the code? In that case, it requires some additional code to make sure everything is loaded when the init script is run. Or what if some code can be run before the gapi is needed? In that case, the push model slows down execution. And what about error handling and graceful degradation? Fortunately, it’s not hard to make a Promise that resolves when the gapi is loaded. It follows the deferred pattern which was widely used before async/await was mainstream and it works by extracting the resolve/reject callbacks of the Promise constructor so that they can be called from the outside. This is how to wrap a Promise around the gapi loading: With this structure, a simple await gapiPromise ensures that the gapi is loaded, properly providing errors where they can be handled, and does not need to restructure the app around an init function. The login flow can go in one of two ways. Since the state is managed by Google, the user might be already logged in. In that case, the app only needs to check this. But when the user is not logged in, the app needs to show a button the initiates the auth flow and waits until the signin happens. Let’s see how to write a Promise that handles all this and when it resolves the user is logged in! First, anything related to authentication needs the auth2 client loaded and initialized. The first part uses gapi.load that is strictly callback-based. The examples usually pass a single function that is called when the client is loaded. But this function also accepts an object with a callback and an onerror handlers. Using that allows proper error propagation . The second part is the gapi.client.init that provides a then-able result, compatible with await : The next step is to wait for the user to sign in. Since the auth flow has to be initiated by a user action, we need a button: Then there are two scenarios. First, if the user is already logged in, just move on. If not, then show the button, and wait for a change in the signed in status. When this code finishes the user is signed in. Same as the authorization, the picker needs to be loaded first. It uses the same gapi.load call with the callback object: The next step is to construct and show the picker. Most of it is configuration to allow selecting folders, add the auth token, and show the component: The interesting part is the setCallback function. This is called when the user selects something or cancels the dialog. When that happens it resolves the Promise with the selected item or rejects it. When this function is finished, folder holds the selected folder or an error indicating that the dialog was dismissed without selecting anything. The last part of the solution is to issue a list call to get the objects from GDrive. It supports the q parameter that lets you define the search term. To list only files that are in the selected folder, use: But the list does not necessarily return all the files that match the query. It is a paginated operation, which means it returns only a limited number of results along with a nextPageToken if there are more. Then a separate list call with the token can continue the fetch, until no items remain. Because of this, multiple calls might be needed to reliably get all the matched items . Fortunately, it is the same pattern as other paginated operations use, such as the AWS API, so the same solution using an async generator works here also: And use it to get all the files: The fields argument specifies what properties will be included in the response. A file object is quite large with lots of values, but it’s likely your app only needs a few of them. Use this argument to remove what is not needed and that cuts the response to a fraction of its original size. But make sure to include the nextPageToken as that is needed for the pagination. With a few Promises, the Google libraries can be fit into a modern async/await workflow. And it well-worth the effort as the code becomes a lot cleaner and easier to understand and maintain.", "date": "2020-07-28"},
{"website": "Advanced-Web", "title": "Refresh the browser when a text appears on the terminal", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/refresh-the-browser-when-a-text-appears-on-the-terminal/", "abstract": "An unoptimized webdev workflow usually repeats the following pattern: This workflow has two distinct steps: 2-5 is about restarting the application while 6-8 reloads the browser . Notice that only the first step has any real meaning, while all the others are boilerplate. Every developer can do these steps by heart as this is what we do the most. It might take only 2 seconds but over time this adds up. And not only the time it takes is a problem but also the mental energy to switch from “developer mode” to “restart mode”. I’ve found that automating these steps increases my productivity way more than what the saved time would mean . Because of this, I always keep an eye on tools that help minimize manual work. We write articles like this regularly. Join our mailing list and let's keep in touch. This article explains the following piece of code, that watches for a given string to appear on the console and reloads the browser : Some tools are sophisticated enough to do both steps, rebuilding and refreshing, for you. For example, Webpack watches the files and whenever it detects a change it runs an incremental build and reloads the page. This takes care of all the steps after the save. See how it works when running npx webpack-dev-server and changing something: You can see that whenever there is a change, Webpack automatically picks it up, rebuilds the code, then reloads the browser. This functionality is integrated into the devserver and you don’t need to configure anything. It just works. Webpack dev server handles the build and the reload webpack-dev-server webpack-dev-server build build browser browser change rebuild reload reload The main drawback of Webpack is that it’s for a specific type of project. It boosts productivity for a complex frontend webapp but it can not be used for a backend function or a Lua script. There are tools to automate one or the other of the two processes that are not tied to a specific type of project. One of my favorites is nodemon . It watches files and restarts the app whenever it detect a change. I even use it with Webpack sometimes, especially when I’m working on the webpack.config.js , as that needs a full restart. To automate the other part, there is node-livereload that works similar to nodemon: it watches files and sends a signal when it detects a change. On the browser-side, either a Javascript snippet or a browser extension then receives that and reloads the page. The best thing about these tools is that they are not dependent on a narrow set of technologies. Whenever there is a need to run something when a file is changed, nodemon can handle it. Similarly, whenever a browser refresh is needed, node-livereload is the tool. The neatest thing I used nodemon for was to watch a Lua file and synchronize it with an ESP8266 microcontroller. I modified a line, hit save, and after a few seconds the text on the small LCD panel next to me changed. It made an otherwise involved process fully automated and suddenly it became fun to write IoT applications. But there is a catch. When one tool does the restarting and the other the reloading of the browser, how does the second tool know when the first one is finished ? As both of them detect file changes they are triggered at the same time, which creates a race condition. The application should be fully started when the browser sends the request, but livereload does not know when that happens. For example, an Express-based server might work something like this: Refreshing the browser before the app is initialized results in an error page. So, livereload should wait a little before triggering the refresh. Running npx livereload . -w 2000 instructs livereload to wait 2 seconds. The app is initialized in this time: In the video, the app takes ~2 seconds to initialize. But since livereload waits a similar amount, the change in the code is reflected in the browser automatically. browser app running restarting running restart time nodemon livereload waiting delay 0 1 2 3 restart reload GET Delayed reload When the restart time is known and is fairly constant, setting a delay is a good workaround to make sure the browser is not refreshed too soon. But this is not always the case. Another example is Jekyll which is the engine behind this blog. A file watcher that does an incremental update is built-in, so there is no need to restart it. When it detects a change, it prints a message that it regenerates the site: And when it’s ready, it prints another message: It usually takes ~3 seconds, but sometimes it’s a bit longer, depending on what changed. When using livereload, we need to take this into account and use a delay that is greater than most rebuild times. For example, this uses 5 seconds: You can see in the video that there is a significant delay between the end of the rebuilding and the reloading. browser Jekyll running regenerating running regenerate time livereload waiting delay 0 1 2 3 4 5 6 reload GET The delay is longer than the regenerate time This is an interesting case of tail latency amplification. If the rebuild usually takes 3 seconds but sometimes it can go up to 5, the delay should be 5 seconds. But this makes every change slower. The problem is that livereload is not driven by the event that the rebuild is complete but that the file is changed and this behavior only gives the delay setting to play with. There is some internal event mechanism in Jekyll and nodemon which in theory can be used to trigger a browser refresh, but that would be a lot of work and that would only work with these two applications. But there is a universal event bus that can connect the two parts: the console. Jekyll prints an easily-identifiable message when it’s ready, just like the Express app above. This is the basis of the manual workflow also. Restart the app then wait for the message that it is finished, then reload the page. Making livereload to trigger when a given text appears on the consloe would make it fully event-driven and minimize the wasted time by waiting too long. And with just a bit of bash magic, it’s possible to come up with a universal solution. When used it with jekyll serve | ... , it results in a timely reload: You can see that the delay between the end of the rebuilding and the browser refresh is gone and since there is no hardcoded value it adapts to the length of the Jekyll refresh. Refresh when the trigger word appears on the console jekyll jekyll grep grep livereload livereload browser browser change Regenerating: 1 file(s) changed at ... _drafts/reload_console.md ...done in 3.1 seconds. reload The configurable part is the parameter of the grep , in this case it watches for the done text. The script works by first using tee to clone the output, so it is also printed to the console. Then it uses grep to look for the trigger text ( tee >(grep ...) ). The --line-buffered is needed to disable buffering as we need every instance of the trigger text as soon as it appears. The filtered stream is then redirected to a construct that starts livereload ( > >(npx livereload ...) ). Livereload can only watch files and <(cat) -ee ' ' converts the stdin to a file that it can use. My favorite development tools are little snippets that are not dependent on any particular technology and can be just thrown into the mix to achieve something useful. Event-driven refreshing of the browser is a great example of such a tool. It does not depend on any event framework and can be easily configured to support all sorts of use-cases.", "date": "2020-07-14"},
{"website": "Advanced-Web", "title": "Testing Bash scripts with the Bats testing framework", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/testing-bash-scripts-with-the-bats-testing-framework/", "abstract": "There are many testing frameworks for Bash, however, some of them are not actively maintained or are used only by a small group of people. Which one should we choose? Or should we just build our custom implementation? Recently I started working on a project which is written in Bash and a bit more complex than the typical scripts I’m used to. The project already had some test scripts to exercise its behavior. As the test suite grew, common functionality—such as assertions—was extracted to make the tests easier to maintain. It served us great in the beginning, but it quickly led to building our custom framework that lacked documentation and some basic features. This is why we decided to switch to an open-source testing framework. We write articles like this regularly. Join our mailing list and let's keep in touch. The first framework I’ve investigated was Bats. It is a well-established contender in the scene of Bash testing frameworks as it’s been around since 2011 and it has a solid user base. The original repository was discontinued in 2016, but it was forked to the bats-core organization to ensure the project’s maintenance and to collect useful third-party libraries. Here’s how a test case looks like: The custom @test annotation makes test cases easier to read because one can use a proper sentence to describe their intention instead of having to encode it to a function name. This syntax is special: Bats transforms the test cases to valid Bash code before executing them. Bats provides the run command to wrap command execution and capture its result code and output. It’s convenient because most of the assertions (like assert_output in the example) can work with these values without having to pass them as an explicit argument. Test cases work in “strict mode”: they will pass if all commands in the test case finish with a zero exit code, otherwise it will be marked as a failure. To illustrate this, the previous snippet could be written without using Bats assertions: Bats itself embraces this pattern and recommends Bash conditions to express assertions. By default it does not come with any built-in assertions, they are defined in the bats-core/bats-assert extension. Both approaches have their benefits. Generally, I prefer to use dedicated assertions because they make the tests easier to read , but occasionally it’s quite nice that basically anything can be easily expressed with conditions. Another thing to consider is error reporting . With the dedicated assertions the report contains the full context, including the expected and the actual values: This is not true for simple conditions: Finally, when it comes to defining custom assertions , with conditionals it’s possible to express almost anything. One thing to keep in mind is that tests will fail on the first non-zero exit code, so calls to helper commands have to be defined accordingly. It’s also possible to define custom assertions similar to the ones provided in bats-assert . The bats-support library provides common functions for error reporting and output formatting, and with load shared test code can be imported to the test cases ( doc ). Tests can be executed simply by pointing Bats to the directory where the test files reside: It can also consider subdirectories with the --recursive flag . With --filter one can specify a regex, and only tests with a matching name will be executed. Bats only considers files with the .bats extension. This is not a huge problem, but editors have to be adapted a bit to offer Bash syntax highlight and the usual features for these files. It’s an important feature of a test framework to protect tests from each other by preventing state leak. Bats achieves this by executing each test case in its own process . With this, state, Bash options and mocks defined in tests are not visible to other test cases. Regarding custom Bash options, Bats has no problem with sourcing scripts that set custom Bash options. Even if options like -e or -u are set, it does not cause problems for the test framework. This is important when it comes to unit testing individual functions as it ensures that they work similarly in a unit test environment as they would when the script is executed normally. Because a non-zero exit code results in a failed test, I thought that all unit tested functions are inherently run with the errexit mode set. Luckily this is not the case. The run construct provides a sandbox to the function under test. Usually this means that whatever Bash options you use, they will be applied to the unit tested functions without Bats overriding anything. One exception to this is the -e option, which seems to be always unset no matter how hard I try to pass it to the function under test. However, I’ve had some surprises before how -e works when testing functions , so I’m not blaming Bats for this quirk. One area where Bats could improve is reporting . Test cases are not separated by test files, which can make the output messy for larger projects involving multiple test files: Also, while it’s possible to define before/after hooks to run code around each test scenario, Bats does not support beforeAll/afterAll construct where global setup and teardown logic could be added. Finally, although the bats-core project has good and up-to-date documentation , the third-party libraries, including bats-assert seem to lag behind. In many cases, they link to old, obsolete repositories and documents that were not updated in the last couple of years. So far I’m satisfied with Bats. It’s a mature and feature-rich testing framework. Its huge user base and extensibility make it a very compelling choice, and none of its shortcomings was a dealbreaker for me.", "date": "2020-06-30"},
{"website": "Advanced-Web", "title": "How to change a blog's permalinks and not lose all organic traffic", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-change-a-blogs-permalinks-and-not-lose-all-organic-traffic/", "abstract": "During 2019 we decided to change the URL of all the posts in this blog. While this seems like a simple change in the configs, it has the potential to invalidate all our backlinks and thus zero out the organic traffic . Because of this, we had to plan everything very carefully. But while the migration itself is a point-in-time event, we also had to make sure we don’t end up with a system that promotes accidental breaking of backlinks during day-to-day use . This can happen, for example, when the URL of an article is based on a setting that is seemingly unrelated to it. We don’t want broken backlinks when fixing a typo in the article title. In this article, I’ll detail how we managed to do the migration and what was the thought process behind each decision. This is a Jekyll-based blog, but the underlying ideas are transferrable to other blog engines. We write articles like this regularly. Join our mailing list and let's keep in touch. Here are the results: The migration happened on Dec 15 (highlighted), and you can see that reduction in organic traffic only happened from the beginning of Christmas until the end of the holiday season. After that, it went back up, like nothing happened. No traffic was lost because of the migration. We used the most common permalink structure that comes with Jekyll, which is /:categories/:year/:month/:day/:title/ . This is the easiest to use for anybody new to blogging as it provides the least friction. And it’s called pretty in the presets and even if you omit the setting it will default to a structure similar to this. And this structure has the most important aspect of permalinks: the URLs are permanent . Jekyll uses the filenames for the publication date too, so unless you go ahead and rename files in the _posts folder you can be sure that your links won’t change, ever. Notice the :title part of the config. It has nothing to do with the actual title of the post but is the part of the filename that comes after the publication date. This structure gives URLs such as https://advancedweb.hu/2019/07/17/upload_signed_urls_differences/ . But the filename is usually considered an implementation-specific thing and we used that to distinguish posts from each other. When I start writing an article I have a vague idea of what it will be about and I give the file a similarly vague name. And that ended up in the URL. Why is that a problem? SEO, of course. The common advice is to use - instead of _ in the URL as the former is considered a word break by search engines while the latter is not. Another best practice is to put keywords to the URL of the page as it is shown in the search results. Similarly, the publication date should not be included in the URL. We have a lot of evergreen articles, but more importantly, the date is already shown in the search snippet: Showing the date and the filename goes against the recommendations so we decided to change the structure. But while it seems like an easy thing, it turned out to be quite complicated. Changing all URLs to new ones invalidates all our backlinks. And while SEO is more of an art than science, everybody agrees that backlinks are the #1 booster of organic traffic. Breaking all our backlinks would zero our organic traffic, which is where the majority of our visitors are coming from. visitor /2019/07/17/upload_signed_urls_differences/ /differences-between-put-and-post-s3-signed-urls/ Migrate content 1) Click on link 2) 404 Changing the URL breaks links The solution is to redirect the old URLs to the new ones which preserve the backlinks. People usually say that a redirect robs some of the backlink’s strength but we decided we’ll win more from the SEO-friendly URLs than what we’re losing on the redirects. Redirect can happen in multiple ways. First, the web server can send back an HTTP 301 along with the new URL in the Location header. This is the preferred method as it’s almost transparent and should be supported in every user agent and crawler. visitor /2019/07/17/upload_signed_urls_differences/ /differences-between-put-and-post-s3-signed-urls/ Migrate content 1) Click on link 2) 301 3) GET 4) Content Redirect makes sure links work Another solution is to place an HTML file that instructs the browser to change the location upon parsing the page. For example, the jekyll-redirect-from plugin, which we ended up using, generates the following file: This HTML uses multiple ways to achieve its purpose. First, it has a <meta http-equiv=\"refresh\"> which is the primary way to send the browser to a different URL. If it does not work for some reason, there is a <script>location=\"...\"</script> that uses Javascript to do the same. And finally, there is a link the user can click on. This should take care of any browser or crawler that hits this page. visitor /2019/07/17/upload_signed_urls_differences/ /differences-between-put-and-post-s3-signed-urls/ Migrate content 1) Click on link 2) Redirect HTML 3) GET 4) Content An HTML file redirects to the new URL But it’s not just about redirection but also to let the crawlers know that this page is on another URL. The <link rel=\"canonical\"> specifies the target URL to be the real source of the content while the <meta name=\"robots\" content=\"noindex\"> instructs the crawler not to index this page. Sending the HTTP 301 status code is the preferred way of redirecting, as that is expected to be recognized by all browsers and tools. Just look at all those edge cases handled in the HTML file that is so elegantly taken care of by a single number. But there are cases where sending a status code can not be done, for example when the page is hosted on Github Pages, which is where our blog currently is. In this case, while we can put arbitrary HTML files, we can not define status codes. The file-based approach is compatible with every kind of hosting solution. From the perspective of the user, there is no difference between the two approaches. The browser handles them the same and both require a GET request to the original URL and then another one to the target location. As for SEO, there is also no difference. Crawlers support the <meta http-equiv=\"refresh\"> just like the 301 status code. The barebones solution is to generate a bunch of redirect HTML files that instruct the browser to move to the target location. This can be done point-in-time with some scripting, but the main downside is that the post and the redirect files are separated. A need for another redirect later means another file and knowing which of them redirects to a specific post requires inspecting all of them. /2019/07/17/upload_signed_urls_differences/index.html /differences-between-put-and-post-s3-signed-urls/ redirect A file redirects to the post url But there is an excellent plugin for Jekyll that solves this problem quite elegantly: jekyll-redirect-from . It lets defining the redirect URLs in the front matter of the post : The plugin generates a redirect HTML file for every path in the redirect_from array and sets the target to the current URL. With this, it nicely handles when the URL is changed multiple times without a chain of redirects. «generated» /2019/07/17/upload_signed_urls_differences/index.html /differences-between-put-and-post-s3-signed-urls/ redirect_from: - /2019/07/17/upload_signed_urls_differences/ generate redirect The redirection file is generated We had a slight problem with the above redirect solution. It generated a bunch of pages that appeared in the sitemap. This was due to how we generated the sitemap.txt . It was a few lines of Liquid template that iterated over all the pages and outputted their URLs. Fortunately, the jekyll-redirect-plugin works with the jekyll-sitemap-plugin to ignore these generated files from the sitemap. After installing the plugin, the generated files disappeared from the sitemap. And we started using the XML format which might be better overall. Now that we have a mechanism to change the URLs without losing backlinks, it’s time to think about what the new URLs should be . The most common approach is to use the title but convert it to contain only lowercase characters and numbers separated by dashes. For example, Medium.com uses this approach: The conversion of a title to alphanumeric characters with dashes is called slugification and the result is called a slug . This is not an exact algorithm and different libraries implement it differently. For example, the title “Tips & Tricks” can be slugified to “tips-tricks” but a more sophisticated version would produce “tips-and-tricks” . Notice the random characters appended to the title of the Medium article’s URL. That is because different titles can have the same slugs and that part makes sure the URLs don’t clash. A simple example is “Tips & Tricks” and “Tips and Tricks” may both have the slug “tips-and-tricks” . But while Medium needs to consider clashing URLs, it’s not an issue for us. Article titles are quite distinct, and if not, we can just use some code to warn us about it. This leaves us with the ideal permalink algorithm: the slug of the article title. But in Jekyll, permalinks can only use a fixed set of placeholders . While they can also be defined in the front matter of each page , but that overwrites the whole path while the global config can define what is before and after the slug. For example, we might change our URLs to remove the trailing slash ( https://advancedweb.hu/2019/07/17/upload_signed_urls_differences/ ) which would require a change in every single post. But Jekyll has a slug placeholder for permalinks which makes the permalink: /:categories/:slug/ config possible. Its documentation: Slugified title from the document’s filename (any character except numbers and letters is replaced as hyphen). May be overridden via the document’s slug front matter. It still uses the filename and not the title, but at least it can be overwritten in the front matter of each page. This can be the basis for a solution. The slug front matter defines the URL of the post, but we need to be extra careful to make sure it is defined as Jekyll would just fall back to the slugified filename. This is a problem, but fortunately, a simple hook can make sure it can never happen: But there is still one edge case. Remember that Medium.com uses a hash appended to the URL to make sure every URL is unique. A similar solution is possible, but an even easier one is to make sure that all slug s are different: The above two hooks make sure all the advantages of the filename-based permalinks are retained. First, the latter generator makes sure the slug s are unique, which makes the URLs unique too. Second, a dedicated parameter controls the URL which is not changed accidentally. When the slug is changed, it’s easy to remember to add the current one to the redirect_from thus preserving backlinks. slug: old_url slug: new_url redirect_from: - /old_url/ Change the slug and add a redirect_from entry The above safety mechanisms are enough to make it unlikely that we lose a backlink even if we rename posts and change their URLs. But I reckoned we can do better. Defining the slug is still a manual process without any guarantees that it stays in sync with the title it is supposed to be derived from. A better approach is to make a hook that checks not just that the slug is defined but also that it is the result of slugifying the title . With this, every change to the title makes sure the slug and thus the URL, is also changed. This can also make sure the old value is added to the redirect_from . The current version that does this and a few other bits: It displays an error if the slug is not what it supposed to be, with the proposed configuration changes. The resulting workflow is as simple as possible. When we work on a post, it shows what the slug should be, but it’s just a hint. When an article is published, the URL is guaranteed to be an SEO-friendly version of the title. And if we need to change the title, it prints what needs to be changed in a copy-pasteable way. title: A great article slug: a-great-article title: An awesome article slug: an-awesome-article redirect_from: - /a-great-article/ Changing the title prompts changes to the slug and redirect_from Just like any other blogs, we have an RSS stream that maintains a machine-readable list of the newest posts. RSS readers periodically check it to see if there are any new things. An RSS reader knows when something is “new” when there is a new guid appearing in the stream. And this can be a problem. We used the id of the post to define the guid : The id should be a permanent identifier of the post, but checking the source reveals that it is based on the slug when that is defined: Before we started using the slug front matter the id was only dependent on the filename, such as /2019/07/17/upload_signed_urls_differences . But now, whenever we change the slug , such as when we change the title of a post, it will change the id and RSS readers see it as a new post. Because of this, post.id is no longer suitable to be part of the guid . A simple solution is to define another front matter which overwrites the post.id when defined. When the slug changes, we just need to define this rss_guid to be the first published slug : slug: old_url slug: new_url rss_guid: /old_url redirect_from: - /old_url/ Change the rss_guid The generator above takes this into consideration and outputs what needs to be changed in this parameter. Changing URLs is risky and requires careful planning. Make sure you place redirects from the old URLs to the new ones, either by HTTP 301 status codes or an HTML file that does the redirection. Also, make sure to use a system that makes it unlikely that an unrelated change breaks backlinks. But handled with care, URLs can be changed.", "date": "2020-06-23"},
{"website": "Advanced-Web", "title": "Are S3 signed URLs secure?", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/are-s3-signed-urls-secure/", "abstract": "Signed URLs is a mechanism to distribute files when streaming them through the backend is not an option. This is especially the case in a Function-as-a-Service model, such as AWS Lambda. It works by separating the access control from the actual download by moving the latter to a dedicated service, in AWS’s case to S3. URL signing is primarily a security feature as it allows a targeted distribution of private files and as such any problems with it likely result in serious problems. This is why it’s important to know what can go wrong and what impact an improper implementation might have on the whole system. Let’s see how the signing algorithm works and what can impact its security! Signing a URL works by using the Secret Access Key to calculate a signature for a request then on AWS’s side, the same key is used to check if the signatures match. The key itself never travels over the network and in this sense, it’s not like a password. One potential problem with this scheme is when an attacker can recover the secret key from a signed URL. Whether this is possible or not is dependent on the actual hash algorithm used in the protocol. In AWS’s case, it is HMAC-SHA256, which is considered a strong hash function with a key that is long and seemingly random enough. Recovering the secret key from the signature is not possible. Moreover, modifying a portion of the URL, such as the filename, and fabricate a signature is also not possible. The algorithm is secure. If you follow the news you know that AWS is in the process of deprecating the signature version 2 and is migrating to version 4 . This might indicate that there are some problems with the old version that warrants the deprecation. But the v2 also uses HMAC-SHA256, along with HMAC-SHA1, so there are no weaknesses on that algorithm either. The benefit of the change is that version 4 provides better compartmentalization inside AWS by calculating a signing key from the Secret Access Key for each day, region, and service. With this, AWS does not need to distribute the Secret Access Keys to each region just the signing keys. If a region is compromised somehow, the Secret Access Key is still secure. This change brings no additional security to the end-user. The algorithm used in the signing process is secure, but that does not mean that just by using signed URLs makes everything magically better. Several things can go wrong and especially when multiple services are working together even a small misconfiguration can result in security problems. Let’s see the most common and impactful problems and how to fix them! Instead of flaws in the algorithm, the most likely source of a security incident is when an attacker can obtain a proper signed URL from your backend . In this case, AWS won’t detect any malfeasance and will be happy to send the file. The good news is that it is entirely in your control as you define the rules. Signing a URL means you allow downloading a file, so it’s especially important to implement proper access checking . Treat a signed URL as the file itself and do a thorough access checking. For example, a serious vulnerability is not checking the file at all but signing anything that the client requests, such as /sign?path=ebook.pdf . If there is no button to alter this call then it’s not immediately obvious, but if an attacker changes this to /sign?path=otherbook.pdf and gets a valid signed URL as a response then the access check is failing. An even worse scenario is when the backend can be tricked to sign a URL for a file outside the set of files users can download . For example, imagine 2 directories in a bucket: logs and ebooks . Users can download ebooks but they should not have access to the logs. But because there might be only one signer entity, usually a Lambda role, it might have access to both directories. If the backend can be tricked to sign a URL outside the ebooks directory, users can get access to the logs with confidential information. A solution to this problem is to use a dedicated role that only has access to the files that users can download and nothing else. This separates the permissions of the signer entity from the execution role. See this post on how to implement this. The first security mistake usually made with signed URLs is to use a long expiration time. This turns them into capability URLs usually without any consideration of the consequences. Short expiration times make sure that the clients can lose only the current signed URLs and not the ones signed in the past . Most cloud providers limit the maximum validity of the URL, usually to 7 days. This prompts people to find more secure solutions and not sign URLs valid for years, a practice not uncommon prior to when a short maximum was enforced. Using long expiration times is not just insecure but also unnecessary. Signed URLs should be used right after signing which makes the default of 15 minutes more than enough. The proper implementation is not signing and letting the user use the URL later but make sure the signing happens just before use. A possible solution is to send an HTTP redirect from the signing endpoint which automatically forwards the browser to the file. See this post for implementation details. A signed URL represents the right to download a file just by itself, there is no additional access checking to the user . In this sense, the URL is equivalent to the file itself , as with the former you can download the latter. It is important to make sure only the intended recipient has the URL. URLs can be compromised in-flight, especially when transferring them via HTTP instead of HTTPS. The web is already moving towards using HTTPS everywhere, and that secures the signed URLs while they reach the user. Make sure you use HTTPS between the users and the backend. Second, they can show up in the browser’s history or the user can intentionally copy-paste and save them so a later compromise may leak the URLs also. The expiration mechanism of signed URLs helps with this as they become useless after a certain time has passed. Using a short expiration time makes sure a security breach does not compromise old URLs as they are already expired and useless. Make sure to use expiration times as short as possible. Since signed URLs are time-sensitive—they expire—caching can be a problem in some cases. Caching can happen in two places: on the client-side and in a proxy . Client-side caching of signed URLs means that if somebody has already downloaded a file he might still be able to download it again even if the URL is expired. Browsers utilize memory and disk-based caches that can still have the file and return it without contacting S3 to check validity. This is usually not a problem as the user already has the file. Misconfigured proxy caching , on the other hand, is a serious problem . Let’s say you use CloudFront as a single point of contact and you utilize edge caching to speed up access for users. This provides a ton of benefits, like HTTP/2 support, no additional TLS handshake, simplified CSP, no CORS problems , and caching. When a user downloads a file the next user does not need to go all the way to the bucket but get it from a cache in closer proximity. But what if a URL is expired? If it is found in the cache then the second request won’t go all the way to S3 to check it. As a result, if somebody accesses a signed URL when it is valid it will be available for everybody close to that location. If an attacker gains access to an expired signed URL he can try to find an edge where the file is still available. bucket edge1 edge2 edge3 hacker 1) download (denied) 2) download (denied) 3) download (success) An expired URL can be cached on the edge The solution is to set the proxy cache TTLs carefully . In CloudFront’s case, you can set the min, the max, and the default TTLs. With these three, you can specify how long a given file will be cached on the edges. The safest option is to disable proxy caching altogether by setting all TTLs to 0: If you want edge caching then you need to limit cache expiration to an acceptable limit. But keep in mind that proxy caching time is added to the expiration time of the signed URL as if an object is added just before expiration it will be available for the duration of the cache. If you use the default 15 minutes for URL expiration and set up 30 minutes caching on CloudFront the effective expiration time will be between 15 - 45 minutes. S3 valid expired URL expiration time CloudFront cached Cache TTL User URL is available 0 10 15 40 request An expired URL is available from the cache See this post for how to implement proper caching for signed URLs. But a proxy can be misconfigured in an even worse way than just extending expiration. If the cache key does not include all query parameters then after somebody accessed a signed URL, everybody will get access to the file , even without a valid signature! Incorrect caching settings user2 user2 user user edge edge edge cache edge cache GET /file.pdf? X-Amz-Signature=... PUT /file.pdf Cached as /file.pdf without the signature GET /file.pdf GET /file.pdf Found in the cache file.pdf file.pdf If you use proxy caching, make sure that it does not strip any query parameters. In the case of CloudFront, it’s the Query String Forwarding and Caching . The dangerous setting here is the Forward all, cache based on whitelist . Make sure you cache and forward all query parameters. Since singed URLs work differently than sending the file from the backend certain problems come with it. These are things that you can not do much about, but being aware of them helps to assess the security of your services. While you control access to the file via the signing procedure, you don’t control bandwidth . If you sign a URL then you can’t specify how many times it will be used and by whom. Note that this is not an information leak as the user can send the file itself after downloading it. This problem is related to cost. A user distributes the signed URL instead of the file backend backend user user user2 user2 user3 user3 S3 S3 file signed URL signed URL signed URL signed URL file signed URL file Especially with large files, a user might want to send it to other users but instead of using his own bandwidth, he figures he can send the signed URL and they can download directly from S3. Sounds reasonable, but you, the service provider, foot the bill. Short expiration helps in this case. If the URL is valid for only 15 minutes then, while still theoretically valid, it’s a lot less likely. Why is this a problem with signed URLs but not with the traditional solution where files are flowing through the backend? Two reasons. First, in that case, the access control check and the download are not separated. For a user to download a file available only to another user they must share login credentials which is a lot less likely. Second, by having all the traffic going through the backend you can monitor how many times a file is downloaded and who, as in which user and from what IP address, downloads it. If you see suspicious behavior then you can deny the download. While the Secret Access Key is not recoverable from the URL, the Access Key ID is part of the query parameters. It is not a problem as that is not considered a secret, but it still reveals some information about how the backend is implemented. Observe the differences in the URL when signing with a role and with an IAM user. Using a role to sign a URL: While signing with an IAM user: Notice the X-Amz-Credential parameter which is the Access Key ID. A role starts with ASIA while for a user it’s AKIA . Why is this a problem? The best practice is to use roles for anything automated instead of users and signing a URL on the backend is definitely in this category. Using an IAM user can indicate problems in the signing process which is then observable for the users. Also, by comparing the Access Key IDs used for a length of time, an attacker might know the renewal policy which is also an implementation detail. There is no way to revoke individual URLs. There is an access check on the S3 side but that only checks whether the signer entity is allowed to get the file. You can remove that permission but that invalidates all signed URLs. Using short expiration times help to shorten the downtime. Revoke the permissions, wait 15 minutes, then add it back. This way all previously signed URLs are expired and they can not be used to download any files. Keep in mind that if you use proxy caching that files might be available there. But ultimately there is little you can do to prevent a misissued signed URL from being used. Configured properly, signed URLs are secure. Four areas need care: But if each of them gets the necessary attention, distributing files via URL signing is a safe solution.", "date": "2020-06-16"},
{"website": "Advanced-Web", "title": "Function tests in Bash and the errexit mode", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/function-tests-in-bash-and-the-errexit-mode/", "abstract": "Leaking Bash options can cause serious headaches when unit testing shell scripts because sourcing a test file might completely change the behavior of the subsequent commands used in the test. A leaking set -e might be especially troublesome as it can cause the test case to terminate early. However, there’s one more problem with it: in some cases set -e does not work with subshells. In unit testing it’s common to invoke a function in a subshell to capture its output: In fact most Bash testing frameworks — such as Bats or bash_unit — recommend this practice. Due to inconsistencies related to the -e option, it might be possible that when a function is executed like this, it will not take -e into account, not even when set -e is defined in the sourced script. We write articles like this regularly. Join our mailing list and let's keep in touch. Here is an example where this problem surfaces. The script queries a URL to print the response’s HTTP result code. It is designed to terminate silently in case of errors, such as HTTP 404. This behavior is achieved with two things: So far so good, works as expected. Let’s write some function-level tests for getResponseCode . Let’s jump directly into the negative test case where things get interesting. As a start, I wrote the following using bash_unit : I expected this test case to fail because of sourcing query.sh the test case inherits the script’s errexit option and I knew that getResponseCode will exit with 22 . However, it failed for a different reason: 😲 😱 😳 For some reason, in this test scenario getResponseCode behaved differently than when it was called during normal execution. At first sight I suspected that maybe the test framework does something that negates the effects of errexit , but the problem persisted even when I removed it from the equation: Being puzzled, I’ve posted this problem to r/bash where experienced veterans revealed the truth. This strange behavior is a quirk of Bash’s errexit option: subshells do not inherit the -e option unless Bash is running in POSIX mode. Subshells spawned to execute command substitutions inherit the value of the -e option from the parent shell. When not in posix mode, bash clears the -e option in such subshells. Examining the Bash POSIX Mode manual revealed that POSIX mode is not the default unless the script is invoked as sh . So what’s going on here? My original test case was executed via bash_unit . Here’s how the source code of the test framework starts: When I replaced bash_unit with a plain Bash solution to load and exercise the function. If you scroll up to the end of the last section, you can see the full source code, but the important detail is in the first line: None of these are invoked as sh … so can it be that POSIX mode was not enabled in these cases? I’ve replaced the first line of the plain Bash solution to invoke sh rather than bash … … and the snippet started to behave as I expected. Such small thing changes how Bash works big time. If you are interested in all the 59 ways how POSIX mode (or the lack of it) might change your script’s behavior, just check the manual. 🙊 🙈 🙉 POSIX mode can be enabled explicitly with set -o posix . Alternatively, it’s possible to make subshells inherit -e without using POSIX mode with shopt -s inherit_errexit , but that’s only available in newer Bash versions. First when I bumped into the Use Bash Strict Mode article I started using -e , as it seemed to bring the capabilities of Bash closer to a real programming language. This might be true for shorter scripts, but for anything just a little bit more complex its downsides start to appear.", "date": "2020-06-09"},
{"website": "Advanced-Web", "title": "Tips to debug NodeJS inside Docker", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/tips-to-debug-nodejs-inside-docker/", "abstract": "When building a NodeJS app there are a lot of tools available to help with the development process. A single line can restart the code when anything is changed, files created by the app are easy to inspect, and there is a full-blown debugger baked into Chrome that can be attached to a running instance. With just a minimal amount of setup when I add a console.log I see the result in a few seconds, or add a breakpoint and observe the internal state. But every time I make the transition and move the app into Docker, it seems like all my tools are broken. Even simple things like using Ctrl-C to stop the app do not work, not to mention that the whole thing becomes opaque. I usually start the development using only the host machine, without any further abstractions. But more often than not eventually I need a dependency that might not be installed and I don’t want to clutter the system I use daily. Or I want to fix the versions to make sure the setup is reproducible on different machines. Dockerizing the app improves it architecturally, but I often feel that it’s a giant step backwards in terms of developer productivity. This article describes a few tips that brings the same developer experience of a non-Dockerized app to one that runs inside a container. The first thing every development environment needs is an easy way to restart the system under development to see the changes. With a NodeJS running without any further abstractions this is provided out of the box, just press Ctrl-C, up arrow, then enter. This takes care of killing the app and restarting it. But when it’s running inside Docker this simple practice does not work anymore. Let’s see the causes and how to remedy them! While running a NodeJS app does not require a separate build step thanks to the interpreted nature of the language, Docker needs to build the image before it can be run . As a result, you need to use two commands instead of one. What’s worse is that Docker assigns a random name to the newly created image and it’s surprisingly hard to extract and use that in the next command. Fortunately, Docker supports image tags which lets you define a fixed name for the image which can be reused for the docker run command. Using image tagging, this builds and runs the code in one command: Now that the container is running, it’s time to stop it. Unfortunately, Ctrl-C does not work anymore. The first instinct I have in this situation is to open a new terminal window, use docker ps to find out the container ID, then use docker kill <id> or docker stop <id> to stop it. But this kills not only the container but all the productivity I had without Docker. The root cause is NodeJS handles the TERM signal differently in this case and instead of shutting down it ignores it. Fortunately, the signals are available inside the app and it can handle them by shutting down. To get back the original working, capture the SIGINT and SIGTERM signals and exit the process: For this to work, run the container with the -i flag, and you might want to remove this code in production. With the above tricks we are almost back to the restarting experience of the pre-Docker setup. But depending on how the Docker build is structured, the build process is possibly a lot slower than before. This happens when npm install runs for every build, even when no dependencies are changed. A typical first iteration of a Dockerfile is to setup the environment, then add the code, then initialize it. In case of a NodeJS app, the last step is usually to run npm. While this script works and it faithfully reproduces how a non-Dockerized build runs, it is not optimal in terms of rebuilding. Docker utilizes a build cache that stores the result of each step when it builds the Dockerfile. When the state is the same before running a step than when it was cached, Docker skips the step and uses the cached result. But when the COPY moves all the files into the image, the state before the RUN npm ci will depend on all of the files. Even if you modify a single line in the index.js which has nothing to do with npm, Docker won’t use the cache. And the result is that rebuilding the image is painfully long. The solution is to separate the package.json and the package-lock.json from the other parts of the codebase, as suggested in the docs . This allows Docker to use the cached result of RUN npm ci unless the package jsons are different. Also create a .dockerignore file and list the node_modules in it. This makes sure that the installed dependencies are not copied to the image. With this setup, npm ci will only run when needed and won’t slow down the build process unnecessarily. This brings down the incremental build time to a few seconds, which is acceptable for development. One of my favorite development tool is nodemon as it saves me tons of time in 2-seconds increments. It watches the files and restarts the app when one of them changes. It does not sound groundbreaking, but those manual restarts that take 2 seconds each quickly add up. It was designed to work with NodeJS projects, but it can be configured to support any setup. And it can also be used with npx so no prior installation is needed besides a recent npm . To restart the container whenever a file is changed, use: The --signal defines that the SIGTERM will be used to stop the app. With the code snippet that catches and kills the process, this terminates the app. The -e lists the file extensions to watch. Since nodemon has defaults with a NodeJS app in mind, you can use this to support other scenarios. Then the -exec defines the command to run. In this case it builds and starts the container. When using this command, every change results in a restart automatically. No more Ctrl-C, up arrow, enter needed. Another feature of Docker that hinders development is that it uses a separate filesystem. In a particular case the app I worked on used a lot of intermediary files in the /tmp directory. When the app was running on the host machine it was easy to inspect these files. With Docker, it’s a bit more complicated. Fortunately, there are multiple ways to regain visibility of the files. The easiest way is to get a shell inside the container and use the standard Linux tools to inspect the files. Docker supports this natively. First, use docker ps to get the container ID, then open a bash shell inside the container: While Linux tools are great to inspect files, they are insufficient in some cases. For example, when the file I’m interested in is a video then terminal-based tools are not enough. In this case, I need to transfer the files to the host in some way. The easiest way is to mount a debug directory that is shared by the two systems and copying files there makes them available on the host. This command creates a debug directory on the host and mounts it to /tmp/debug : With this setup, use the terminal to cp the files there or you can also programmatically export files with fs.copyFile(<src>, path.join(\"/tmp\", \"debug\", <filename>)) . It’s also possible to get serious and attach a debugger, such as Chrome’s built-in Devtools. For this to work, start the app with the --inspect flag: Using 0.0.0.0 opens the debugger to every host and I’m unsure it’s needed because of Docker or my development setup that requires multiple port forwarding. Make sure that you understand that it opens the debugger to every host which means everyone who can connect to the host machine can attach to it. The --inspect instructs NodeJS to listen on the 9229 port for debuggers, but we also need to expose that from the container: With these two configs, open Chrome and connect to the app. You can do this either by using the Node logo on the top left: Or open chrome://inspect and select the app: When the debugger is attached, you can set breakpoints, inspect runtime state, and you can do pretty much everything that you can for local apps running inside Chrome. I’ve found that stopping the execution at some point then opening a shell inside the container to inspect the files is a powerful way to debug apps. With some preparations, you can have the best of both worlds: the isolation and reproducibility of a Dockerized application, and the ease of development of one that is running natively.", "date": "2020-07-07"},
{"website": "Advanced-Web", "title": "How to use the aws_apigatewayv2_api to add an HTTP API to a Lambda function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-the-aws-apigatewayv2-api-to-add-an-http-api-to-a-lambda-function/", "abstract": "The AWS API Gateway HTTP APIs, a simplified version of the REST APIs, recently went GA and offer a lot of improvements over the current Lambda integration solution. With Terraform support for the new resource type, it’s time to see how it works and how simple it got to add an API to a Lambda function. The new API requires only 2 resources, the API itself and a permission to invoke the function: As a comparison, the same with a REST API needs a lot more resources: With HTTP APIs all that boilerplate is hidden behind a quick-start config that takes care of setting up the trivial stuff. If you want to use something more advanced, such as JSON models or an authorizer, you can configure those too. I’ve always felt that I just need one simple feature and I can do anything else in code. That simple thing is to give me a URL that calls the function. With REST APIs that core functionality requires too much boilerplate. Now with HTTP APIs that got a lot easier. But this simplification comes with changes that affect how to properly use the new type of APIs and how to integrate with other services. There are only two moving parts required for an HTTP API: the ARN of the Lambda function and a unique name for the API. The AWS Terraform provider is inconsistent when resource names are required and when they are optional. For example, an S3 bucket will be named something unique and random if the argument is missing. Others, such as the aws_apigatewayv2_api , require a name. I prefer when a resource does not require a parameter like this. The problem with names is that they must be unique and the deployment fails when there is a collision . An auto-generated name handles this by default, but when it must be provided it’s tempting to hardcode something in the config. But with a fixed name, the same stack can not be deployed to the same region and possibly to the same account more than once at a time . AWS throws a ResourceConflictException when there is a collision. Fortunately, the random_id resource can help by generating a unique string for each deployment . This reproduces how an optional resource name works. My first experience with the simplified HTTP API was how hard it is to know what went wrong. On the first try, I got an “Internal server error”, and no other clue. Logging in to the Console, I saw the request did not reach the Lambda as no log entries were in the function’s log. And after activating the request logging all I saw was 500 errors without explanation. What helped eventually was the usual trick to debug IaC: creating a new API manually using the Console and comparing the two to find what difference might result in the errors I’m seeing. Eventually I figured there was an error with the Lambda permission and the API started working after I fixed that. Apart from the catch-all “Internal server error”, I encountered “Not Found” errors also. These happen when there is a problem with the routes and the API does not match the request URL. A longer-term problem is that it’s near impossible to google its name as it’s too generic. This makes it especially hard to find solutions for known problems. And not just because of the naming, but the official documentation meshes the API Gateway HTTP API with the REST API which makes it hard to find examples specifically for the former. With the new API comes a new Lambda payload format. This defines the event argument of the function and the required elements in the output. Fortunately, the old format is still supported, so an existing function can still be used with the new API without rewriting it. But new functions should use the new one. The changes in the output format are the easier to cover as it only got easier. If you want to return a JSON, you no longer need to specify the statusCode and the Content-Type boilerplates as those will be automatically set to 200 and application/json , respectively. Of course, you can overwrite them if you need, making this change backwards-compatible. The input format , on the other hand, is changed in non-backwards compatible ways. The documentation offers an example where it’s easy to compare the two formats. Most of it is moving parameters around , such as the event.path which is now at event.requestContext.http.path and the event.httpMethod is moved to event.requestContext.http.method . Another useful change is the first-class handling of cookies . The event.cookies contains the cookies sent with the request and the cookies property in the response is automatically converted into set-cookie headers. A less used but no less impactful change is how duplicate headers and query parameters are sent to the Lambda function. These are no longer parsed for you but simply concatenated with commas. A possible problem is that these are not escaped in any ways, so a query with ?a=b,,&a=,c will be sent as: If that can be a problem, the rawQueryString contains the unprocessed query from where you can extract the values correctly. With the new $default stage the domain name is finally enough to call an API and there is no need to pass the stage name around too. Before the $default stage, the URL consisted of the domain and a stage ( https://<api_id>.execute-api.<region>.amazonaws.com/<stage>/ ) and the root path does not reach the API. Now with $default specified as the stage name, the request to https://<api_id>.execute-api.<region>.amazonaws.com/ is handled by that stage. As this simplifies the URL it makes the integration easier with other services. For example, the origin_path argument is not needed when using CloudFront . Apart from the $default stage, there is also a $default route that simplifies catch-all path patterns. This is similar to the {proxy+} construct but it handles the root pattern also. With a REST API, if you wanted to handle both <url>/stage/ and <url>/stage/path you’d need to use an additional resource under the root and two methods and two integrations: That’s a lot of boilerplate copy-pasted whenever a new API is needed. With the $default route, all this is gone. Compared to the REST APIs, HTTP APIs are cheaper and quite significantly so. The starting price is $1 per million requests, while the same is $3.5 for REST APIs. It’s a significant reduction but of course, the actual impact is dependent on the ratio of this cost item to the overall infrastructure. But let’s see how it compares to the Lambda function that it exposes as an API! With 128 MB of memory allocated, the function costs $0.2 for every 100 ms it runs per million requests. Apart from that, there is an additional $0.2 / million request cost. With the HTTP API, $1 / million cost is equal to a function that runs between 300-400 ms. For a REST API, the same function can run for ~1.6 seconds to reach the API’s $3.5 / million price tag. To put it this way: you get a free ~1.2 seconds of runtime for your function (with 128 MB RAM) if you use the new HTTP API. With simplified configuration, HTTP APIs also offer less overhead to the request latency, as described in the release announcement . Faster is always better, but the latency experienced by end-users is affected by a ton of other things too. The Internet is not famous for its low jitter, DNS resolution takes an unknown number of hops, TCP and TLS require roundtrips but their exact number is determined by the protocol used, and also Lambda can spin up a new instance any time which incurs a slow cold start. But the part API Gateway adds to the mix is a bit smaller now. On a more practical side, HTTP APIs do not support caching and edge optimization. I believe it’s not a great loss as you can always add CloudFront manually, which is what “edge optimized” meant anyway, and configure it to provide both. This also gives a bit more control over how things work. Since HTTP APIs bring support to the $default stage and a route key that offers fine control over the paths of the API, it changes how it can be integrated with CloudFront compared to REST APIs . And because of the way the path_pattern is handled in CloudFront, we need to consider how an HTTP API works with the default cache behavior as well as with a non-default one. With the $default stage it’s enough to define the domain name and there is no need to define the origin_path argument. A request to the path https://<url>/testroute has the path properly filled: The path_pattern is required for a non-default behavior, as this parameter configures the routing between the origins. But this is only used to determine which behavior to match but the full path is sent to the origin. This messes up the request path the Lambda function sees, as it will get the requests other than the / . user CloudFront default => origin_1 /api/* => origin_2 origin_1 origin_2 /login /api/login /login /api/login The full path is sent to the origin For example, if the path_pattern is /api/* then the function will get /api/login as well as /api/ , but never the / or /login . And if the function expects the requests relative to the root, it won’t work. A trivial solution is to add the path pattern to every path checking in the function’s code. This works, but it limits the portability of the code. A more robust solution is to use Lambda@Edge to cut the routing part of the origin request. This works for all types of origins but it incurs additional costs and complexity. But with HTTP APIs it’s possible to use the route_key to match only paths under the path_pattern . For example, to send only the request under the /test/ path to the function, use the ANY /test/{proxy+} route key: With the above config, the event object contains the matched part in the pathParameters.proxy property. A request to https://<url>/test/testroute yields this object: To support both cases on the Lambda function’s side we need some special care when checking the request path. The path is either in the requestContext.http.path or in the pathParameters.proxy . To handle both cases, the function has to check the latter first and fall back to the former: With this code, it does not matter if the function is reached via a default or a non-default behavior, the realPath will be relative to the correct path. For example, when mapped to the default_cache_behavior and a request comes to https://<url>/example then realPath will be /example . If the same function is mapped in an ordered_cache_behavior with a path_pattern of /test/* , a request to https://<url>/test/example , the realPath will be set correctly to /example . Overall, I’m happy with the new API type. It makes the common use case easier, faster, and cheaper, which is always a good thing. Now I don’t need to copy-paste a screenful of Terraform code to make a Lambda function available via HTTP. HTTP APIs also have a lot of configuration options but those are optional. You can still configure them to fit a lot of use-cases if you need to, but the common case is now reduced to just a few lines of code.", "date": "2020-06-02"},
{"website": "Advanced-Web", "title": "What is the optimal password length", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-is-the-optimal-password-length/", "abstract": "The more the better, of course, and with a password manager it’s trivially easy to generate and autofill a password of any length. But should that be something with hundreds of characters just to be sure, or is there a sensible lower limit to use as a rule of thumb? Here is a typical password generator interface: Note the length part which can go from around 8 all the way up to 100, while other ones can go much higher. What is a good setting to use for passwords? But to understand what a secure password is, let’s see what happens on the other side! We write articles like this regularly. Join our mailing list and let's keep in touch. When you create an account, the service you’re registering to stores the password in one of many forms. It can put it directly into the database (also known as plain-text) or hash it using one of the many available algorithms . Some of the most used hash algorithms include: The upside of storing hashes instead of the passwords themselves is that the passwords are not in the database . And they shouldn’t be, as you only need to prove that you know yours but it does not matter what it is. When you log in, the provided password is hashed with the same algorithm and if the result matches the value stored then you’ve proven you know the password. And in the case of a database breach, passwords are not recoverable. Storing hashes user user service service db db register Pa$$w0Rd1992 5e8539... login Pa$$w0Rd1992 5e8539... OK OK Password cracking is when an attacker tries to reverse the hash function and restore the password from the hash. With a good hashing algorithm, it’s not possible to recover the password, but nothing can be done against trying out various inputs to see if they yield the same result. If such a match is found, the password is recovered from the hash. Password cracking cracker cracker hash hash Pa$$w0Rd1990 db21d7... Pa$$w0Rd1991 247cb4... Pa$$w0Rd1992 5e8539... And choosing a good algorithm makes a difference here . SHA-1 was designed for speed, which helps the cracking process. Bcrypt, Scrypt, and Argon2 were designed to be costly in various ways to make cracking as slow as possible, especially on dedicated hardware. And the difference is huge. Considering only the speed, an SHA-1 hashed password that can not be cracked is like this: 0OVTrv62y2dLJahXjd4FVg81 . A safe password using a properly configured Argon2 hash: Pa$$w0Rd1992 . As you can see, choosing the right hashing algorithm can make an otherwise weak password an uncrackable one. And remember that this is dependent only on how the service you’re registering to is implemented . And you have no way of knowing on which part of the spectrum the implementation stands. You can ask, but chances are they won’t even respond or say that they “take security seriously” or something similar that does not mean anything. Do you think companies take security seriously and use a good hash algorithm instead of a crappy one? Look at the list of breached databases, especially the hashes that were used. Many of them still used MD5, most used SHA-1, and some used bcrypt. Some even stored passwords in plain text. That’s the reality you should assume. There is a bias here as we only know what hash was used for breached databases and it’s likely that the companies that use a weak algorithm also fail to safeguard their infrastructure. But just look at that list, I’m sure you’ll find familiar names you wouldn’t have thought of having weak security. Just because a company is looking big and reputable does not mean they will do the right thing. Where does that leave you as a user? With plain text passwords, you can not do anything. If the database is gone your password strength does not matter. With properly configured algorithms it also does not matter much how secure your password is, not considering trivial cases like 12345 and asdf . But between them , especially with SHA-1, your choice matters . The hashing function is not suited for passwords in general, but if you use a secure password you can make up for the shortcomings of the algo . * It depends on the configuration. These hashes have various moving pieces that affect their strength, but when configured properly they can thwart cracking attempts. The bottom line: if you use a strong password then you are protected in more breaches than with a weak one. And since you don’t know how secure the password storage is, you can not be sure what is “secure enough” for a given service. So assume the worst where your decision of the password still makes a difference. OK, but why should you care if you use a password manager and generate a unique password for every site? In this case, you are not vulnerable to credential stuffing which is when a known email/password pair is checked on other services in the hope that they are reused. And since password reuse is one of the biggest problems, this is a serious threat. Credential stuffing Hacked service Hacked service hacker hacker service service email1/password1 email2/password2 login: email1/password1 Denied login: email2/password2 OK But generating a new password for every site protects from this. And a database is stolen, everything inside it is known to the hackers, why should you still protect the password? The problem is when you don’t know that the database is breached and you continue to use the service. In this case, hackers have access to all your future activity on that site. You might add a credit card later and they still know about it. A strong password means they can’t log in with your credentials and can not compromise your future activity. Usage after breach user user service service hacker hacker register email/password hack database crack password login email/password add sensitive info login email/password sensitive info Password strength is all about entropy, which is a numerical representation of how much randomness it contains. As we are working with large numbers, so instead of saying there are 1,099,511,627,776 (2^40) different variations, it’s easier to say that it has 40 bits of entropy. And as password cracking is all about the number of variations as the more there is the more time it takes to try out all the possibilities. For random characters generated by password managers entropy is easy to calculate: log2(<number of different characters> ^ <length>) . The length is trivial, but what are the number of different characters ? It depends on the character classes a password has. As an example, a password of length 10 containing a random mix of lower and uppercase letters has log2(52 ^ 10) = 57 bits of entropy. The above math expression can be simplified to see how much entropy a single character of a given class brings to the overall strength using the expression log2(n ^ m) = m * log2(n) . This yields: <length> * log2(<number of different characters>) , where the second part is the entropy per character. The above table, using this formula: To calculate the strength of a password, consider the character classes it is made of, get the entropy number from the table and multiply by the length. The example above (lower + uppercase letters of length 10) yields 5.7 * 10 = 57 bits . But if you increase the length to 14, the entropy jumps to 79.8 bits. But if you keep the length at 10 but add numbers and special characters, the total entropy will be 64 bits. The above expression offers a quick way to calculate how much entropy a password has, but comes with a caveat. It only applies when the characters are independent of each other , which is only true for generated passwords. The password H8QavhV2gu satisfies this criterion, so it has 57 bits of entropy. But ones that are easier to remember, such as Pa$$word11 , while having the same length and more character classes has a lot less entropy. A cracker does not need to try all the combinations only the words from a dictionary with some transformations. Therefore any calculation based on multiplying the length with the entropy of the character class is only valid for generated passwords . The more entropy a password has the harder it is to crack it, but what is enough ? The general wisdom is that ~16 characters should be more than enough for a password, which yields between 95 - 102 bits, depending on whether special characters are included or not. But what is the threshold? 80 bits? 60 bits? Or even using 102 bits is too low? There is another algorithm that is similar to a bad password hashing algorithm in terms of speed but is way better studied: the AES encryption. This is used to encrypt everything secret in all sorts of government and military institutions and therefore its strength is well considered. And it’s fast, so if a key with a specific amount of entropy can not be cracked for AES then it will be good for a password with a bad (but not broken) hash. The NIST (National Institute of Standards and Technology) is the entity that defines which key sizes are good for the foreseeable future. Their recommendation is AES-128 for “2019 - 2030 & beyond”. Which, as the name implies, has 128 bits of entropy. Another recommendation specifically for key sizes is to use at least 112 bits of entropy: For the Federal Government, a security strength of at least 112 bits is required at this time for applying cryptographic protection (e.g., for encrypting or signing data). To get 128 bits of entropy using lower and uppercase letters and numbers, a length of 22 (5.95 * 22 = 131 bits) is needed. I tend not to use special characters because they break word boundary . This means selecting them requires 3 clicks instead of 2 and that can produce errors where I accidentally don’t paste part of the password to the input field. With just characters and numbers, a double-click always selects the whole password. Some sites define the maximum password length which prevents you from using 22 characters. There are cases where it goes to extreme lengths, like requiring exactly 5 digits. In this case, use the maximum length available, there is little else you can do. There are also recommendations for the service how to handle passwords and limiting the length is clearly against them. The NIST says : Allow at least 64 characters in length to support the use of passphrases. Encourage users to make memorized secrets as lengthy as they want, using any characters they like (including spaces), thus aiding memorization. And remember that the service can store passwords in a way ranging from terrible to superb and they won’t tell you exactly how they do this? A short maximum password length gives the impression that they are on the worse end of that spectrum. Strong passwords are needed even if you don’t reuse them. Strongness is measured in entropy and you should aim for 128 bits of it. A lowercase + uppercase + numbers password with a length of 22 will be above this threshold. This should protect you in case of a data breach where the provider uses a weak but unbroken hashing algorithm.", "date": "2020-05-26"},
{"website": "Advanced-Web", "title": "How to mock in Bash tests", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/how-to-mock-in-bash-tests/", "abstract": "For Bash scripts, almost everything is about side-effects. They typically work with system resources such as the network or the file system. Commands that behave differently based on external factors factors—like du or ls —have to be controlled by the test to ensure reproducibility. This post covers 3 alternatives to mocking: using aliases, functions, and PATH override. Live examples can be found in the testing-in-bash/mocking GitHub repo. To illustrate it, let’s see the improved hello function that greets users differently on Fridays. A test for this function would return different results depending on the day it’s executed. To get the same results on every day the date function has to be mocked. We write articles like this regularly. Join our mailing list and let's keep in touch. In his post, Dave Nicolette recommends alias to create mocks. By default, aliases are only expanded in interactive shell sessions. In scripts, this behavior has to be explicitly enabled with the expand_aliases shell option using shopt Note that defining the mocks and sourcing the script to be tested can be done in any order, the script will use the alias over the original command. One limitation of this approach is that aliases can’t have arguments so they can only respond in one predefined way. If greeting.sh would use date in multiple places, I’d be in trouble with this approach. Also, in this concrete example, the script under test calls date +%A , but it’s impossible to narrow the alias down to these arguments and have something like alias \"date +%A\"=\"...\" . Another problem is that the alias has to take care of the potential arguments that are passed to it. For example, the following simple alias would only work in the simplest case where the mock does not receive any arguments. This is obviously not ideal. I’ve added true to the alias to swallow the extra arguments. For defining mocks a more robust approach is to use functions , as recommended in this post . In this case, exporting the mocks should happen after importing the script to be tested. By using functions rather than mocks all the previously mentioned problems are solved. Functions ignore additional parameters, so there’s no need to worry about the extra +%A in our case. Moreover, it’s also easy to create complex mocks that can respond in multiple ways depending on the parameters. Note: the export -f is to ensure that sub-shells can also use the mock. However, even with functions things can get complicated if a single mock function has to mock all usages for a common command, like date . This can quickly result in huge, hard-to-maintain mock code. A better practice is to refactor the original script a bit, wrap low-level commands in higher-level functions that capture part of the business domain, and simply mock that. For example, a potential improvement in the greeting.sh could be to introduce the day_of_week function. It also improves code readability since it’s much easier to figure out what date +%A does. Now it’s possible to mock day_of_week instead of date , which will result in a much simpler mock, and less chance of accidentally mocking something we’d not want to. It’s important to keep in mind that mocks can change the behavior of the tests too. For example, mocking date could very easily interfere with the tests if the test framework uses that command for reporting. Aliases and custom functions only work with unit tests. They can only override commands and functions if the code of the script file is sourced into the unit test. They have no effect when a script file is simply executed. However, in this case, it’s still possible to mock commands by overriding the PATH variable, supplying custom executables . A similar mock with the previous functionality could be defined by creating a date executable with the following contents: Such mock can be used by putting this executable to the highest precedence on PATH : The PATH based mocking provides a bit less than the alternatives. First, it only works with external executables, it does not allow mocking functions defined in the script file we are testing. Additionally, a separate file has to be maintained for each mock, which makes them separated from the test cases where they are used, thus probably harder to maintain. Also while it’s still easy to return a dummy response from such mock, it’s a bit harder to set up expectations about how the mock was called. A script might work with the stdin, stdout and stderr data streams to interact with the user. Luckily, it’s really easy to test them using pipes and redirection . In the first test to capture the stderr instead of stdout with command substitution I’ve redirected the stderr to the stdout, and stdout to /dev/null . In the second test where only the stdout was needed, I’ve redirected stderr to /dev/null to silence everything that comes out of the greet.sh . Also, I used printf instead of echo to explicitly control the new lines. This can come in handy if the script expects multiple lines on the stdin. Unit testing Bash scripts can greatly enhance developer experience. Because Bash scripts are all about side effects, mocking is an important part of effective testing. In this post we’ve covered 3 alternatives that makes this possible.", "date": "2020-05-19"},
{"website": "Advanced-Web", "title": "Unit testing Bash scripts", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/unit-testing-bash-scripts/", "abstract": "Unit testing in Bash is perhaps even more important than in most other languages. Because it has so many sharp edges it’s even more to benefit from a test suite during development. Although Bash comes with its own unique set of challenges when it comes to modularization, it’s possible to isolate and test individual components. In Bash you can define functions to capture part of the business logic. Functions can be invoked from the same script file but it’s also possible to import definitions to other scripts by using the source command. These tools provide the foundations for unit testing. We write articles like this regularly. Join our mailing list and let's keep in touch. (This example does not leverage a testing framework, but conceptually it would look similarly.) However, it’s not quite that simple in Bash. Even if there are several functions in a script file they are typically also invoked by the same script. In these case sourcing the file would not only include the function definitions but it would also execute them immediately . Let’s see two approaches on how to deal with this. A possible solution is to separate the script into multiple files by extracting the function definitions into a separate file and leaving the rest behind. The file with the definitions could be easily unit tested. Slicing up a big file into multiple, smaller chunks is probably not a bad idea anyway; it’s easier to reuse and understand smaller modules. Let’s create a more modular version of the previous example by creating the following two files in the same directory. informal_module.sh : greeting.sh : It looks clean and nice, but unfortunately, it would only work in some cases . source works relative to the current working directory (where the script is invoked from) rather than to the location of the script itself. Depending on which directory we are at the time when we execute the previous example, it might fail to include the definitions from informal_module.sh . According to Stack Overflow ( 1 , 2 ), the BASH_SOURCE variable can be used reliably to determine the path to the directory of the script file. This can be used to include other scripts relative to its location. Let’s see how greeting.sh can be modified to overcome this problem. Another alternative suggested by Darin London is to do what Python does and modify the script to detect if it’s being run directly , or if it is just being sourced . This script can be executed normally to do its job, but if it’s sourced to a test file only the function definitions will be imported without doing anything else. The set builtin is capable of changing how Bash works. Some examples are: Depending on your programming style, setting some of these options at the beginning of a script might be considered as a best practice (see the Unofficial Bash Strict Mode ), or something to be avoided completely ( Why doesn’t set -e do what I expected? , What are the advantages and disadvantages of using set -u? ). In any case, it’s important to keep this in mind when designing for testing and modularity, because sourcing a file that calls set might globally change how Bash interprets commands. Bash options can make it harder to design reusable modules, because they have to work nicely with different combinations, depending on which file are they imported to. It’s even worse when a sourced function changes one of the options, thus changing the behavior of the original and other included scripts work. For unit testing where scripts files are sourced to the test case, this can also be a problem. Consider this example where we use diff in a test to compare files: If greeting.sh does not change the Bash options all is fine. However, if it defines set -e the test will misbehave, as it will terminate immediately if diff finishes with a non-zero exit code. Similar problems can arise when setting -u as it changes how undefined variables can be detected . It’s possible to query the current Bash options with shopt -s and set -o , and according to this SO post it’s also possible to save and later restore shell options, but it might not work in all environments and adds quite some complexity. However, this might come in handy in cases where a function is sourced into multiple scripts, using different options. Although a good testing framework can help a lot by isolating the individual test cases from each other, keep Bash options in mind when designing your tests. Testing Bash scripts is a rarely used practice, after all, Bash doesn’t make it any easier. However, general best practices for software development, such as modularity and unit testing—even if they are not always easy to achieve—can greatly enhance the experience of programming such scripts.", "date": "2020-05-12"},
{"website": "Advanced-Web", "title": "Avoid excessive costs and save money with S3 storage classes", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/avoid-excessive-costs-and-save-money-with-s3-storage-classes/", "abstract": "Every object in S3 has a storage class that identifies the way AWS stores it. It affects durability, availability, access patterns, and of course, cost. Back then S3 standard was the only one available later joined by Glacier as a separate service. Then it got merged into S3 and additional classes were added. Now there are options for one-zone storage for recreateable data, IA for infrequently accessed objects, and Glacier is split into two. Storage classes compromise on an aspect in order to save money . The standard class offers the best durability, availability, and retrieval capacity. If you can live with something worse, you’ll pay less. For example, if you don’t need instant access then Glacier saves a lot on storage. If you can handle lost objects then the one-zone classes are cheaper than standard but using them increase the risk that something disappears. And infrequent access (IA) offers greater retrieval but lower storage costs. To make things more interesting, there are lifecycle rules to automatically move objects between storage classes . A common use-case is to move objects to classes with lower storage costs as they age and eventually delete them. Standard Infrequent access Glacier upload 30 days 1 year 10 years Bucket lifecycle rules But storage class pricing is, let’s say, complicated. It’s not just some value is lower in the pricing table and some are higher, but there are other intricacies that greatly affect the total cost. These are minimum billable object sizes, minimum storage times, and retrieval costs . And it’s not exactly trivial to see the best option for a given use-case. To make things worse, if you don’t take these into account you might end up paying a lot more than the standard price . In this guide, you’ll see how to avoid the pitfalls with the S3-IA storage class. S3 IA (Infrequent Access) has a simple offer on the tin: for higher retrieval costs it offers cheaper storage. It is logical to use this for infrequently accessed objects and it will cost less in this case. Let’s see the numbers ! In us-east-1 , which is usually the cheapest region, these are the current prices (May 2020): This is a significant price reduction on storage, but there is a retrieval fee. 0 2 4 6 8 $/month/GB 0 1 2 3 Retrievals/month S3 Standard The bottom line: you get the storage almost at half price, but if you read the data once per month then you’ll lose all the savings . “Infrequently” means less than once per month. My first thought was to store images that are served via CloudFront and set up caching but I had to realize this storage class is not for this use-case. There is another difference, but it’s hidden in the documentation . If you look at the availability row, you’ll see that the SLA is lower : 99.9% vs 99%. This does not sound bad, but that means a difference of ~9 hours vs more than 3 days of downtime every year. Moreover, there is an asterisk on the pricing page which always means complications. And this is not an exception. There is a minimum billable object size of 128KB. Every object that is smaller than this will be rounded up to this size. In effect, if the object is 70KB or less it will cost more to store it even if you don’t access it even once! 0 1 2 3 4 $/month/GB 40 70 100 130 160 190 220 Object size (KB) S3 Standard There is also a minimum billable storage time of 30 days. If you delete an object before 30 days you’ll still pay for the full period. Because of the combination of retrieval cost, minimum object size and minimum storage time, using this storage class can cost a lot more than standard. Notice that there is no upper limit on these extra expenses. Don’t upload objects to IA unless you are certain it’s the right thing to do in your use-case. Because of the pitfalls of the IA storage class, there is a separate one that aims to fix its shortcomings by making sure only infrequently accessed objects are using that. After all, this is the main problem with the IA class: if something goes wrong even a few accidental reads nullify the saving accumulated for years. Meet Intelligent-Tiering. It has 2 tiers internally, one for frequently accessed objects and one for infrequently accessed ones. If an object is not read in the first tier for 30 days it will be moved to the second. And it will be moved back once it is accessed which makes sure repeated reads don’t incur an additional fee. Frequently accessed Infrequently accessed put object 30 days inactivity read Intelligent-Tiering The net effect is that you won’t pay more if an object is accessed frequently but still benefit from the reduced storage costs for other ones. It avoids the worst case scenario of IA . For a price, of course, in this case 0.25¢ / 1000 objects . This makes it sensitive to the average object size, as for small objects this extra fee is more pronounced. As a rule of thumb, if the object is less than 250 KB in size it costs more to use Intelligent-Tiering than the standard class. 0 0.8 1.6 2.4 3.2 $/month/GB 130 330 530 730 930 1130 Object size (KB) S3 Standard But the 128KB restriction still applies here, but not that objects will be billed for more size than they are but those smaller than this threshold will never be moved to the infrequently accessed tier. Effectively, you’ll still pay the monitoring cost but without any benefits . Moreover, the 30 days minimum rule is also applied here, so make sure things won’t usually be deleted before a month. Intelligent-Tiering helps avoid the main pitfalls of the IA storage class, but it still has problems. If the average object size is small then the extra cost outweights the benefits. And the age restriction makes it potentially more expensive. As Intelligent-Tiering helps with some problems of IA, Bucket lifecycle rules help with shortcomings of the Intelligent-Tiering. S3 can move objects from one storage class to another based on a set of rules defined for the bucket but are evaluated for individual objects . This is great as you can set it and then forget about it and will do its thing, moving objects where they need to be. The rules can filter objects by prefixes, tags and age and set a target storage class . Standard Infrequent access Glacier upload legal/, 30 days backups/, 7 days 30 days Bucket lifecycle rules with prefix and age filters Prefixes can be used to apply to only some directory and not all objects. Maybe you have different types of objects in a single bucket, prefix filters help to differentiate them. Standard IA Glacier legal/taxes.pdf backups/dbdump.zip misc/avatar.jpg backups/dbdump.zip legal/taxes.pdf backups/dbdump.zip 30 days 7 days 30 days Bucket lifecycle rules applying to objects But there are additional constraints that apply that are not readily apparent from the lifecycle rule but implicit to the target storage class . For example, objects won’t move into IA or Intelligent-Tiering if they are less than 128KB or younger than 30 days. These are implicit restrictions and can not be overriden. By combining Intelligent-Tiering and lifecycle rules we can engineer a solution that moves only the objects to the storage class that actually benefit from it. With a simple rule, almost all benefits can be retained without most of the pitfalls described above. A lifecycle rule that moves objects to Intelligent-Tiering after 30 days will do the following for different kinds of objects: Bucket lifecycle rules with Intelligent-Tiering Intelligent-Tiering Standard Standard Frequently accessed Frequently accessed Infrequently accessed Infrequently accessed < 128KB put object deleted in 30 days put object delete object Accessed sometimes put object 30 days 30 days read read Infrequently accessed put object 30 days 30 days This makes sure that in no event an object costs more than the standard class and the compromise is only the reduced availability SLA for objects moved to the Intelligent-Tiering. Well, almost. If the average object age before deletion is 31 days then it will still cost more. But that is not a realistic scenario. A more probable problem is when the object size is between 128K and 250K , in which case the lifecycle rule moves it to the Intelligent-Tiering class but the per object monitoring cost will be larger than the savings on storage. Unfortunately, there is no easy solution to this problem. The best is to use tag filters in the bucket lifecycle and put that tag only on larger objects. But that requires handling on the object instead of on the bucket level. To configure it on the Console: The XML for the same config: And finally with Terraform: Don’t upload objects to the IA or Intelligent-Tiering storage classes directly as they have lower limits for age and size which can bump up the costs significantly. Especially don’t use IA as a single read per month will nullify the lower storage costs and multiple reads will cost a lot more. Use the standard storage class for new objects and configure a lifecycle rule that moves objects to Intelligent-Tiering. This setup takes care of the costly edge cases while still reduces the cost when possible.", "date": "2020-05-05"},
{"website": "Advanced-Web", "title": "How to refactor a Promise chain to async functions", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-refactor-a-promise-chain-to-async-functions/", "abstract": "Javascript Promises are kind of a legacy things now that async/await has widespread support, but they are still the engine that drives everything asynchronous. But constructing one and using the then functions for chaining is increasingly rare. This prompts refactoring from a Promise-based chain to an async/await construct. For example, this async code uses chaining: The same functionality, rewritten to a series of await s: This yields not only shorter but also more familiar code as it looks like a synchronous one. All the complexities are handled by the async/await constructs. But when I convert a then -based structure to async/await, I always have a feeling that the original code was better at scoping the variables inside the steps, while the async/await version leaks them . Let’s see a hypothetical multi-step proccessing that gets and resizes the avatar for a user id: getUser getAvatar resize send 15 user image resizedImage Promise chain Each step can use asynchronous operations, like connecting to a database or using a remote API. By using then s, the variables declared inside a function are local to that function and are not accessible from other steps . This makes the code easier to understand as the number of variables in scope is limited. For example, if getting an image uses a fetch and stores the result in a value, it won’t be visible in the next step: This is true for the arguments also: Converting the above code to async/await is trivial, just add await s before the steps and assign the results to a variable: The code is shorter and doesn’t look asynchronous at all. But now every variable is in scope for the whole function. If one step needs to store something, nothing prevents the next step to access it. Also, every result for a previous step is accessible: The original structure of an async pipeline with clearly defined steps and boundaries is lost in the conversion. Since one of the bigger problems is variable scoping, it can be solved by reintroducing a function for each step. This keeps the variables declared inside from leaking to the next step. The structure for an async IIFE: The example above using this approach would look like this: This takes care of variable scoping, but the preceding results are still available. But the bigger problem with this approach is that it looks ugly. The synchronous-looking structure is still there, but with so much boilerplate it doesn’t look familiar at all. A different approach is to use a structure similar to functional collection pipelines . It requires separate functions for each step, then an async reduce to call each of them in order. This structure not only keeps everything separated as every function has access only to its arguments, but also promotes code reuse. The first step is to move the steps to separate async functions: When a function needs not just the previous result but also some other parameters, use a higher-order function that gets these extra parameters first and then the previous result: Note that all of the above functions conform to the structure of either async (prevResult) => ...nextResult or (parameters) => async (prevResult) => ...nextResult . And the latter can be converted to the former by calling it with the parameters. With functions that get the previous result to produce a Promise with the next one, a reduce can call them while also handling await -ing the results: In this example, the functions define the steps and the value flows through them. The 15 is the initial value ( userId in the previous examples), and the result of the reduce is the result of the last function. This structure preserves the clearly defined steps of the original Promise chain-based implementation while also takes advantage of the async functions. While Promises are not deprecated at all, using async/await instead of them yields code that is easier to understand. But rewriting everything that is using Promises by replacing then s with await s usually yields a structure that is less maintainable in the long run. Using an async reduce helps retain the original structure.", "date": "2020-04-28"},
{"website": "Advanced-Web", "title": "Debugging Bash scripts", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/debugging-bash-scripts/", "abstract": "When I first had to debug a Bash script I was completely lost. I just wished that I’d have my usual debugging toolbox for this environment. Luckily, it supports most of the methods available for other languages. This post is about how to lint, trace and debug shell scripts. In many cases, a seemingly perfect script does not work because of a single missing semicolon or whitespace. On top of that, many constructs in Bash have weird edge cases, and some commands might not work the same in all environments. Linters statically analyze the code to pinpoint many of these situations. There’s a lot to learn from the almost instant feedback they provide. We write articles like this regularly. Join our mailing list and let's keep in touch. ShellCheck is such a tool for Bash. To demonstrate it, let’s see this (broken) script: Executing it results in this error: ./awesome.sh: line 3: var: command not found . Which is a bit cryptic. Let’s see what ShellCheck has to say about it: Ah, that’s more helpful! ShellCheck has more than 300 rules to catch issues like this, and it even supports autofixes for some of the cases. You can install it locally , or use the official Docker image . Oftentimes figuring out the control flow of a script is not obvious, but you have to find out which part of the code was executed. One approach is the good old print debugging , inserting echo statements all over the code. This debugging style is quite straight-forward and easy to use. One possible gotcha is that echoing to the standard output of a function might change the program’s behavior. Because the error stream is less used, usually it’s a good idea to use it for debugging. A more robust approach is to use logger to write to the system log. For more complex scripts, an easier alternative is to enable tracing. When tracing is enabled each command and its arguments are printed before they are executed, so the entire flow of the program can be observed. Let’s see this in action. For the sake of this example, consider the following script that calculates the price of the ice cream. The base price for each portion is 100, but when you order at least 3 portions, you get a discount on odd days. The script expects the number of portions as an argument and prints the total price. Reading through the script to figure out why it produced such a result might not always be easy. Let’s see how tracing can help better understand this program. To enable it, add set -x to the beginning of the script, or simply pass -x as a parameter to Bash. Tracing can be enabled and disabled at any given line, so it’s possible to reduce the debug output to certain parts. It’s also possible to customize the trace messages by setting the PS4 variable. This allows getting more information. The following example enhances tracing output to include the name of the file, function and line number: Sometimes it’s just handy to stop the program execution at any given point to execute commands step-by-step and see how they behave. Luckily, this is possible in Bash—to some extent. Similarly to print debugging, one can add extra read commands to the code to stop script execution until manual intervention. This might come in handy if you’d like to examine the effects of a script at a given step. It’s also possible to step through the whole script (or parts of a script) by adding read as a debug trap. It’s best used with tracing to see which commands were executed. Coming back to the ice cream example, this is what it looks like to debug the calculatePrice function: Debugging a Bash script is not an easy task. This language has more rough edges than the others I’ve used, and typically the tooling is just a text editor. In this context, it’s really important to know the tools available to make this challenging task more efficient.", "date": "2020-04-14"},
{"website": "Advanced-Web", "title": "Easily run CLI commands as an AWS role with AWSudo", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/easily-run-cli-commands-as-an-aws-role-with-awsudo/", "abstract": "AWSudo is an easy-to-use CLI utility to run a command as an AWS role. It works similar to the traditional sudo used widely in most Linux-based systems, which provides an easy way to run something as the root user. When an apt install does not work because of permissions, use sudo apt install and it’s on its way to install the package. AWSudo does the same, but instead of switching to the local root user, it switches AWS entities by assuming a role. Of course, this functionality is nothing novel. If you regularly use roles you might already have profiles in your .aws/config file which does pretty much the same. What is different with AWSudo is that you don’t need to hardcode permanent role ARNs to use it. They can come from an external program too. Such as Terraform. The way I found AWSudo is when I wanted to limit my admin user’s privileges to what is absolutely necessary. For this, a Terraform module creates a role with the minimal set of permissions and outputs its ARN. With this in place, I could use npx awsudo $(terraform output role) ... to run something with the role. No need to modify the global AWS CLI config. Roles are temporary credentials that can be assumed by another entity, entity being another role, an IAM user, or a service, which has permissions on their own. As its name implies, they are especially suited to grant a small set of permissions just for a specific task. And unlike AWS users, they depend on another entity to provide the authorization via the trust policy . For example, a security team can have access to an account via a role. The role specifies who can assume it via the trust policy and what it can do via permissions. This way there is no need to create users for individual people and, more importantly, they don’t need to keep a new pair of username-password safe. There are two use-cases for roles. The first is to gain permissions you don’t have . For example, your user might not have access to an S3 bucket containing sensitive information, but you can assume an auditor role that has. This is also the case with cross-account roles as you have no access to a different account but you can assume a role that has permissions to access resources inside. Using roles to gain new permissions STS STS user user S3 S3 User credentials Access denied User credentials Assume role Role credentials Role credentials result The second use-case is to limit your permissions to what is absolutely required. Let’s say you have an admin account but you know you’ll only need access to a specific resource, such as to upload a document to a specific S3 bucket. In this case, you might want to limit the access range not to accidentally upload to the wrong bucket. If you have no access to the wrong bucket, you can not upload to the wrong bucket. Using roles to limit permissions STS STS admin admin Production bucket Production bucket Admin credentials result Admin credentials Assume role Role credentials Role credentials Access denied It also applies to running a program that you don’t completely trust. Would you give free rein to something you’ve just found on the Internet? If it’s a security assessment tool, it might not need write permissions at all. If it’s a bucket analyzer, you can allow only specific buckets for it to analyze. If a tool has no access to a resource it can not do harm regarding that resource. AWSudo is in the npm registry and offers a CLI, which ticks off all the requirements for running it with npx . This means you don’t need to install anything and it has zero system dependencies besides a recent npm. To run a command as a given role, use: Just like with sudo , it is a prefix before the command and you can run just about anything. AWSudo assume role and run command STS STS awsudo awsudo AWS API AWS API Assume <roleARN> Role credentials Role credentials <command> result For example, all AWS CLI commands can be prefixed. To upload a file, use: But it’s not limited to the AWS CLI. To run a javascript code with the role’s credentials, use npx awsudo <roleARN> node index.js . Similarly, it is equally easy to run something more complex, like a dev workflow that watches the source directory and restarts a script: In this example, AWSudo assumes a role and runs Nodemon, which in turn runs node index.js and restarts it when anything is changed inside src . The roleARN is the only required argument for AWSudo, and that identifies the role to assume. In the Terraform example above, the aws_iam_role.role.arn is the created role’s ARN, which is outputted as the role output. To assume this role with AWSudo, use npx awsudo $(terraform output role) <command> . This works by first running terraform output role which prints the role’s ARN then it is used as the first argument of AWSudo. To find out a role’s ARN outside Terraform, the aws iam list-roles gives back all the roles in the account. Inside each object there is a property called Arn which is what you need here. To find out a role’s ARN by its name, you can use jq : And this can be easily inserted to AWSudo similarly as the Terraform output. But finding out the role’s ARN heavily depends on your situation and where that role comes from. It’s different when it’s created by a resource manager, like CloudFormation or Terraform, or when it’s already created for you. While running one-off commands is easy with AWSudo, each run makes a call to AWS STS to assume the role and get the credentials. I don’t know of about any limits regarding this, but if you know you’ll need multiple commands it’s even easier to run a subshell. With this, you’ll have a shell with the role assumed. Inside this shell, you can use all the usual commands without any changes. AWSudo assume role and run an interactive subshell STS STS awsudo awsudo bash AWS API AWS API Assume <roleARN> Role credentials Role credentials bash Role credentials aws s3 ls list Role credentials aws s3 rm OK exit As an example, I needed to remove some objects from a bucket managed by Terraform because I accidentally uploaded them with the wrong configuration. Since this was a one-off act, I needed a playground-style way for exploring and fixing things. Of course, any other shell can be used. If you use zsh , use that instead of bash . The default duration is 15 minutes when using AWSudo, which is plenty for one-off commands but will be too short when running things in a subshell. To increase it, use -d <seconds> argument before the role ARN. One hour has 3600 seconds, so specifying -d 3600 gives you a longer time. Roles can also limit their time. This is called the MaxSessionDuration and it can be a range between 1 and 12 hours with 1 hour being the default. When you try to use a longer time than the allowed maximum, the operation will fail. Fortunately, it is apparent when you try to assume the role with a longer requested duration than the role’s maximum: If you get this error, use a lower value. The inner mechanics are quite simple and easy-to-understand. When an AWS role is assumed, it returns 3 important parameters: The AccessKeyId , the SecretAccessKey , and the SessionToken . Then if you set these parameters as environment variables, the AWS CLI will pick them up and use the role. The AWSudo library simply hides these details and provides an easier-to-remember construct. While AWSudo is open source, running it imposes risks, especially since it’s a security-related tool. If the maintainers got hacked and a malicious new version gets published on npm then it can steal your access keys easily. And since npx uses the latest version by default, it’s an automatic process on your end. A malicious party can publish a compromised version user user awsudo npm awsudo npm hacker hacker npx awsudo ... version x.y.z publish x.y.z+1 npx awsudo ... version x.y.z+1 Compromised credentials To mitigate this risk, you can pin down the version you are using to some trusted one: npx awsudo@1.3.2 ... . This protects against a published malicious version but the command is harder to remember. AWSudo is a tool that makes certain operations with roles a lot easier. Since it does not require any installation and has a command structure that is easy to memorize, it’s a great help for anybody working on AWS. It helps with using roles in the CLI without any permanent config to your AWS CLI. This is especially suited to roles created by Terraform or CloudFormation and one-off tasks that require more exploration than planning.", "date": "2020-04-21"},
{"website": "Advanced-Web", "title": "Secure tempfiles in NodeJS without dependencies", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/secure-tempfiles-in-nodejs-without-dependencies/", "abstract": "Recently, I’ve been working on a static site generator, and I needed to implement how the sitemap.xml is generated. The whole project was a Node HTTP server, generating the whole site on-the-fly, no files are written to the disk. To generate a sitemap I decided to use a crawler that maps the site and returns the XML. When there is a request to /sitemap.xml , the crawler does its thing, then the HTTP server sends back the XML. This fits nicely into the functional architecture. One library to crawl the site and generate the sitemap is the sitemap generator package . But this requires a file to save the generated sitemap, and I could not find an easy way to extract the result otherwise. Well, just put the sitemap somewhere, then delete it later. From the outside, it looks like a functional solution. Sitemap generator with a tempfile js js sitemap generator sitemap generator sitemap.xml /tmp/sitemap.xml write sitemap.xml read delete sitemap Sometime later I worked on a PDF generator and I needed to convert the pages of a PDF to images. Luckily, there is Ghostscript which is the de-facto tool for anything related to PDFs. Its documentation is rather lengthy, and it discusses how Ghostscript can be used with pipes instead of files for input/output. But the problem is that the PDF -> image generation produces multiple files and there is no mention of how to handle that with pipes. This left me with a solution similar to the one above, just with a temp directory: The solution is the same as above: create a temporary directory, make Ghostscript put the files there, read the files, then delete them. So it all boils down to one step: to create a temporary directory. Sounds simple, right? os.tmpdir() returns the directory where programs can put there stuff that will be cleaned up after some time, and from there it’s just a matter of generating a sufficiently random filename. When I started working on this post my plan was to introduce the Tempy library and how easily it solves this problem. Then I started reading. And man, tempfiles are a mess. Let’s jump into the solution first, then discuss why it works this way in the following chapters. I wanted a solution that is: And here is the result, with exactly zero dependencies: To use it, copy-paste it into your project and use the two functions, depending on whether you need a file or a directory. The sitemap generator works like this: And using with Ghostscript: So, the above code looks simple, why do I say tempfiles are hard? Well, this piece of code has a very high thinking/characters ratio, that’s for sure. And while there are libraries out there that provide more or less the same functionality ( tempy and tmp are the two main ones), but I’ve found they don’t promote secure usage and they are a bit heavyweight as most of their dependencies are now built into Node itself. This mostly boils down to file permissions which is the primary problem with tempfiles. And it’s surprisingly hard to set them up right . There are two kinds of problems with software: one that affects functionality and one that does not. Since something is considered “broken” when it does not work, the first category gets way more attention. Even the most rudimentary solution to tempfiles works just fine, it will pass QA and will be marked as “Done”. But the other category of problems is way more interesting and encompasses most security problems. The tempfile solution works just fine, but do you know no rogue process is watching for newly created files in /tmp and siphons off anything that gets written there? The PDF with the sensitive info that a user has just uploaded, for example. Rogue process reading tempfiles js js tempfile hacker hacker write data tempfile read delete And it is especially hard to see this opportunity exists with short-lived tempfiles that are deleted a few milliseconds later. The tmp library goes to great lengths to set restrictive permissions on tempfiles so that only the owner has access to them. But unfortunately, it’s not enough. I noticed that the sitemap generator library just overwrites the file with more permissive access: So it’s not enough to create a file that is locked down as something down the line can just overwrite it with a different permission set. And you can’t assume libraries will just do the “right thing”. Overwriting file permissions js js sitemap generator sitemap generator sitemap.xml hacker hacker create with 600 sitemap.xml 600 u:rw- g:--- o:--- sitemap.xml overwrite with 664 664 u:rw- g:rw- o:r-- read read delete The solution is to use the execute permission of a parent directory. Lacking this permission prevents the users from doing anything inside, no matter what permissions the files have. This yields a solution to the tempfile problem: instead of a file, create a directory that is locked down and put the file there. No matter how that file is used down the line and what permissions it gets, it stays secure. Tempdir permissions js js sitemap generator sitemap generator tempdir tempdir/ sitemap.xml hacker hacker create with 700 tempdir 700 u:rwx g:--- o:--- tempdir/ sitemap.xml create with 664 tempdir/ sitemap.xml 664 u:rw- g:rw- o:r-- read Don't have execute on the directory! read delete delete Putting files to /tmp works in most cases , but is it universal? No, for example, Windows uses a different place, and also systems might want to designate a different directory. So don’t assume /tmp is the place to put tempfiles. The os.tmpdir is better, as it picks up the tmp location in a configurable way. But it seems like there are problems with it too as it can be a symlink in some cases. Fortunately, Node provides a utility to solve the symlink problem, so combining the two parts, the reliable solution to find out the tempdir location is: This one is more of a functional than a security problem as if the same code runs in parallel it can overwrite the files previously created. No matter if NodeJS is single-threaded, if you use async fs functions (which you should), the net effect is parallel execution. Because of this, you need to make sure to include randomness in the filename. Concurrent processes js js tempfile generate write tempfile generate write read delete return read error But even with random filenames, insufficient randomness can affect security in two ways. First, if an attacker can know the filename in advance, he can create the file in advance with lax permissions or some predetermined content that can corrupt something downstream. Creating the file in advance js js tempfile.txt hacker hacker create tempfile.txt write read delete Second, he might deny creating it repeatedly, leading to errors in the application itself. Denying file creation js js tempfile_1.txt tempfile_2.txt tempfile_3.txt hacker hacker create tempfile_1.txt Already exists create tempfile_2.txt Already exists create tempfile_3.txt Already exists Error Tempfile libraries go great lengths to provide randomness for the file path, usually relying on crypto . This is random enough, but it is not needed. It is a solved problem already, and there is the NodeJS function fs.mkdtemp that not only takes care of randomness and uniqueness but also of setting the necessary permissions. Just make sure to use path.sep at the end for platform independence. Tempy takes care of this, but tmp does not. The temp folder is handled by the operating system and it makes sure it will be cleaned up eventually. Combined with the unique filenames discussed above, it makes sure it won’t run out of space and getting errors. But it still means that clutter accumulates, even though with an upper limit. Seems like there is no need to dispose of the files when they are not needed in the application. This is the route Tempy takes. But there are two problems with this approach. The first one is practical. The OS cleans up eventually, but when? The temp directory might have a lot of files which slows down everything that operates on the directory level, such as ls . And when the OS does clean it up, it has no way of knowing when something is really not needed and there is a slight chance it will remove something it shouldn’t. Just by deleting the files no longer needed, the application can be a good citizen who cleans up after itself. The second problem is related to security. If tempfiles contain sensitive information then you want to get rid of them as soon as possible, even when using the correct permissions. If there is a flop, you don’t want to expose a long list of sensitive files. With auto-cleanup at least you limit the potential fallout. And this auto-cleanup does not have to be complicated either and it can be made the default. If you are familiar with Java, the try-with-resources is a similar construct. It works by passing a function and wrap the call to that function with a try-finally construct. The finally makes sure no matter how the function invocation ends, the cleanup is called: Disposer pattern js js disposer disposer /tmp/VqDL/ fn fn fn create /tmp/VqDL/ /tmp/VqDL/file Use the tempfile... result delete result This makes auto-cleanup the default without thinking about it. And since it’s an async function, it supports async/await and Promises too. The tmp library goes to great lengths to also delete the files when the process exits. This closes a rare condition where shutting down the app leaves tempfiles as in that case the finally does not run. But it’s still not a bulletproof solution as killing the app can’t clean up. The rimraf library is the de-facto standard to recursively and reliably delete a directory. It does not seem to do much apart from calling rm -rf , but there are surprisingly many edge cases it needs to handle. But starting from Node 12, it is built-in into the platform as the fs.rmdir(path, {recursive: true}) function. Interestingly, it’s rimraf simply copied . Libraries that do cleanup usually use await to make sure the files are deleted before returning. I believe there is no need to wait for the cleanup just make sure it will be done eventually. Delete timing js js withTempFile withTempFile tempdir create tempdir Use tempdir alt [rmdir] result delete [await rmdir] delete result The more significant difference is whether an error should mean the call is unsuccessful or will it be treated as an uncaught Promise rejection. Using tempfiles should be a last resort and it’s better to use pipes in most cases. But sometimes they are inevitable, and in this case, make sure you take into account all the nuances especially when potentially sensitive data is involved.", "date": "2020-04-07"},
{"website": "Advanced-Web", "title": "Comparing async Javascript functions and callbacks on AWS Lambda", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/comparing-async-javascript-functions-and-callbacks-on-aws-lambda/", "abstract": "Initially, Lambda only supported callbacks with Javascript handlers. Back then in 2014 it was the common way to handle asynchronicity as async/await was nowhere near as mainstream as it is today. Moreover, the SDK functions also used callbacks so it fits into the trend. But we’ve come a long way since then and now callbacks are becoming an anti-pattern with async functions taking their place. Fortunately, Lambda supports them just fine now. Let’s see what are the problems with callbacks when running in Lambda and how async functions can replace them! When callbacks are used, the last argument of the handler is a function which you need to call. It uses the Node-style callback pattern, so it expects two arguments: (error, result) . To signal an error, use callback(error) , and when everything is fine, return the result with callback(null, result) . Simple enough, but there are many problems with this approach. While it works fine for a “Hello world”, it is just not the best choice for anything serious. Two distinctly Lambda-related things pop up when it comes to callbacks and Lambda. The first is context.done() which is still part of many copy-pastable code samples but it’s deprecated . The second one is the context.callbackWaitsForEmptyEventLoop . Both are related to tasks still running when the callback function is called . In a normal Node environment, things started when a request comes in will be executed even if the response is sent. A normal HTTP server would work like this: In this case, the response does not wait for the putObject to finish. But it does not change much, the result is sent to the user as soon as possible, and the object will be in the bucket eventually. putObject finishes after the response is sent user user HTTP server HTTP server S3 S3 request putObject res.end(result) response OK But Lambda is a different environment and there is no “do something after the response” thing. Which leaves it two options: The first one is to wait for the background task to finish . Since it’s so easy to forget something important still running when using callbacks, this is the default. Lambda timeout user user Lambda Lambda S3 S3 request putObject callback(null, response) timeout Error OK The second option is to stop the tasks which is exactly what context.callbackWaitsForEmptyEventLoop = false does. And historically, context.done() and context.succeed() . Because of the default behavior, you can see timeout errors even if the callback is run in a timely fashion. Something is still going on, and Lambda will wait for it. Such as calling a different function that has a longer timeout and running time will timeout this one even though the callback is run. A bad fix is to set the context.callbackWaitsForEmptyEventLoop = false and instruct the runtime to stop execution when the callback is run. What happens here? No one knows. context.callbackWaitsForEmptyEventLoop = false user user Lambda Lambda S3 S3 request putObject callback(null, response) response ??? The thing is, Lambda freezes the execution and might reuse it later. So the call may go through immediately, it may be finished later when a separate request triggers the same function, or might never resume as the execution environment kills the process altogether. There are no guarantees . Also, excessively relying on callbacks is bad in general not just in the context of a Lambda function. Have you heard about callback hell? As you could tell, it’s not a nice place. Here’s how it looks like: Every async operation adds more depth and it quickly becomes really, really hard to understand what’s going on. For some time you think you can grasp it, but forget it. Also, look at the error handling ! Even one forgotten check is enough to wreak havoc to the whole operation and instead of the failure reason you’ll see timeout errors. The solution, of course, is to use Promises and async/await. That solves all problems with the callback pattern. To use an async function, just skip the last parameter and convert it to async/await: No more extra indentation for each async call, and errors are propagated just fine without any extra work. Happy path ddb.query s3.putObject ddb.putItem OK OK resolve reject reject reject Error propagation in async functions Parallelization with the above benefits is also easy with Promise.all : Parallelization is important not just to provide a speedy response to the user hence improving the UX but also because you are billed for the total time the function executes. Waiting for a network request to come from an external service just so that the function can start waiting for a totally unrelated other one is a waste of money. Sequential and parallel queries Sequential Parallel Handler Handler DynamoDB DynamoDB Handler Handler DynamoDB DynamoDB request request query query1 query2 result result1 result2 result query result result Also, the AWS SDK provides functions that return Promises , which can be inserted into an async/await flow naturally. While the default is still the callbacks, most function either support the .promise() to convert it to one: or provides a different function for that: But async/await only solves problems with callbacks and the original problem was what to do with tasks still running when the function returns ? After all, async functions can still linger around unawaited for. What to do with the putObject call that is still running when the result is ready? In this case, we are back to square one. But there is an important distinction. The default behavior for callbacks is to wait for everything, but for async functions it is to return immediately . I believe this is because it’s so much easier to make an error of not waiting for something when callbacks are used than with async functions. But keep in mind that this behavior is not configurable, there is no property in context to change it. Execution is frozen when the async function returns user user Lambda Lambda S3 S3 request putObject response Frozen So, how to wait for background tasks to finish with async functions? With await , of course: Execution is frozen when the async function returns user user Lambda Lambda S3 S3 request putObject await OK response If you have callbacks, you can convert them to Promises with the Promise constructor and wait for the result: This gives an easy way to wait for pretty much anything, be it non-standard callbacks or streams to finish. Callbacks are the past, don’t use them. They have problems on their own, such as callback hell and poor error propagation and that can easily lead to problems in Lambda functions. Use async functions instead as they are supported by both the Lambda environment and most of the AWS SDK, making asynchronous code shorter and safer. But keep in mind that the Lambda environment is still not exactly like a Node server as there are no guarantees what happens to background tasks not finished by the time the response is sent. This requires some planning, but properly await -ing for everything is the key here.", "date": "2020-03-31"},
{"website": "Advanced-Web", "title": "How to use async functions with Array.filter in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-array-filter-in-javascript/", "abstract": "In the first article , we’ve covered how async/await helps with async commands but it offers little help when it comes to asynchronously processing collections . In this post, we’ll look into the filter function, that has probably the most unintuitive way to support async functions. The filter function keeps only the elements that pass a condition. It gets a function, this time it’s called a predicate , and this function returns true/false (truthy and falsy, to be more precise) values. The resulting collection only contains the elements where the predicate returned true. The async version is a bit more complicated this time and it works in two phases. The first one maps the array through the predicate function asynchronously, producing true/false values. Then the second step is a synchronous filter that uses the results from the first step. async results [F, T, F, T, F] arr [1, 2, 3, 4, 5] result [2, 4] (map) i % 2 === 0 (filter) results[index] Async filter Or a one-liner implementation: Async filter with map map asyncFilter asyncFilter asyncMap asyncMap e1 e1 e2 e2 e3 e3 filter filter Step 1 [1, 2, 3] false true false [F, T, F] Step 2 [1, 2, 3] [F, T, F] [2] The above implementation runs all of the predicate functions concurrently. This is usually fine, but, as with all other functions, it might strain some resources too hard. Fortunately, since the above implementation relies on map , the same concurrency controls can be used . Instead of using an async map with a sync filter , an async reduce could also do the job . Since it’s just one function, the structure is even easier though it does not provide the same level of control. First, start with an empty array ( [] ). Then run the next element through the predicate function and if it passes, append it to the array. If not, skip it. [ ] [...[], ...[]] predicate(1) => false [...[], ...[2]] predicate(2) => true [...[2], ...[]] predicate(3) => false [ ] 1 2 3 Promise([2]) Async filter with reduce Async filter with reduce asyncFilter asyncFilter reduce i % 2 === 0 reduce i % 2 === 0 e1 e1 e2 e2 e3 e3 [1, 2, 3] await predicate(e) await memo await memo [] [2] [2] [2] Notice that the await predicate(e) comes before the await memo , which means those will be called in parallel. To wait for a predicate function to finish before calling the next one, change the order of the await s: This implementation waits for the previous element then conditionally append an element depending on the result of the predicate ( ...[e] or ...[] ). Async filter with reduce running sequentially asyncFilter asyncFilter reduce i % 2 === 0 reduce i % 2 === 0 e1 e1 e2 e2 e3 e3 [1, 2, 3] await memo await memo await predicate(1) [] await predicate(2) [2] await predicate(3) [2] [2] While an async filter is possible, it works in a way that looks strange at first. And while concurrency controls are still available, they need a bit more planning than for other async functions.", "date": "2020-03-10"},
{"website": "Advanced-Web", "title": "How to use async functions with Array.some and every in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-array-some-and-every-in-javascript/", "abstract": "In the first article , we’ve covered how async/await helps with async commands but it offers little help when it comes to asynchronously processing collections . In this post, we’ll look into the some and the every functions that are used for a more efficient reduce when the result is a boolean value. These functions get an iteratee function, just like the filter , but they return a single true/false, depending on whether the predicate returned a specific value. In the case of the some , if any of the predicates returned true, the result will be true. For the every function, if any returned false, the result will be false. Considering only the result, these functions can be emulated with an async filter , which is already covered in a previous article how to convert to async. Filter-based async some asyncSome asyncSome filter i % 2 === 0 filter i % 2 === 0 e1 e1 e2 e2 e3 e3 [1, 2, 3] false true false [2] true But there is an important difference between the built-in some / every functions and the filter -based implementations. When there is an element that returns true for a some , it short-circuits and does not process the remaining elements: Synchronous some some some e1 e1 e2 e2 e3 e3 false true Does not check true Similarly, every stops after the first false result: Let’s see how to code an async version that works in a similar way and does the least amount of work! The best solution is to use an async for iteration that returns as soon as it finds a truthy result: For-based async some asyncSome asyncSome e1 e1 e2 e2 e3 e3 false true Does not check true For the first element predicate(e) returns true, it concludes the for-loop. The similar structure works for every , it’s just a matter of negating the conditions: Whenever there is a false value returned by predicate(e) , the function is ended without checking the other elements. The short-circuiting implementation processes the elements sequentially, which is efficient in terms of resource usage, but it might result in longer execution. For example, if the iteratee sends requests via a network, it might take some time to send them one at a time. On the other hand, while it might result in more requests sent, sending all of them at the same time would be faster. The some and the every functions are easy to approximate with an async filter but to follow the synchronous version faithfully, an async for loop is a better choice.", "date": "2020-03-24"},
{"website": "Advanced-Web", "title": "How to use async functions with Array.forEach in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-array-foreach-in-javascript/", "abstract": "In the first article , we’ve covered how async/await helps with async commands but it offers little help when it comes to asynchronously processing collections . In this post, we’ll look into the forEach function, which is helpful when you need to run a piece of code for every element in a collection. The forEach function is similar to the map , but instead of transforming the values and using the results, it runs the function for each element and discards the result. Effectively, the important part is the side effects of calling the function. For example, printing each element to the console, synchronously: As the result is not important, using an async function as the iteratee would work: Async forEach js js forEach forEach e1 e1 e2 e2 e3 e3 Finished async 3 2 1 But, not unsurprisingly, the function is called asynchronously , and the program execution goes past the call. This is an important difference from the sync version, as, by the time the next line is executed, the synchronous forEach is already done, while the async version is not. That’s why the “Finished async” log appears before the elements. To wait for all the function calls to finish before moving on, use a map with a Promise.all and discard the results: Async forEach, waiting for the results js js map map e1 e1 e2 e2 e3 e3 3 2 1 Finished async With this change, the “Finished async” comes last. But notice that the iteratee functions are called in parallel. To faithfully follow the synchronous forEach , use a reduce with an await memo first: Async forEach, sequential processing js js reduce reduce e1 e1 e2 e2 e3 e3 1 2 3 Finished async This way the elements are processed in-order, one after the other, and the program execution waits for the whole array to finish before moving on. The async forEach is easy to use, but whether you should use a forEach , a map , or a reduce depends on the requirements for timing. If you just want to run the functions no matter when, use a forEach . If you want to make sure it finishes before moving on, use a map . And finally, if you need to run them one by one, use a reduce .", "date": "2020-03-03"},
{"website": "Advanced-Web", "title": "Asynchronous array functions in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/asynchronous-array-functions-in-javascript/", "abstract": "The new async/await is an immensely useful language feature that hides the complexity of asynchronous code and makes it look like it’s working in a synchronous way. The code example below does not look complicated at all, just a few await s here and there. But under the hood it’s sending commands to a separate process and controlling its lifecycle in an event-driven way: Since Javascript is single-threaded , asynchronicity is a lot more important than in other languages. In Java, for example, you can create a new Thread and then it does not matter if some commands block it. But in Javascript, blocking is not an option, that’s why there is no sleep function, just a callback-based setTimeout . This becomes overly apparent when you do too much computation and it freezes the page which is when Chrome shows the “Page unresponsive” popup. Apart from web development, when I was working with the ESP8266 WiFi-enabled hobby IoT chip, my program needed to stop executing every few milliseconds so that the WiFi code could do its thing, otherwise it would lose the connection. That’s why callbacks and Promises, an abstraction over callbacks, is so prevalent in the language. And async/await is another language feature that makes it a lot easier to write asynchronous code. But while async/await is great to make async commands look like synchronous ones, collection processing is not that simple. It’s not just adding an async before the function passed to Array.reduce and it will magically work correctly. But without async functions, you can not use await and provide a result later which is required for things like reading a database, making network connections, reading files, and a whole bunch of other things. Let’s see some examples! When all the functions you need to use are synchronous, it’s easy. For example, a string can be doubled without any asynchronicity involved: If the function is async, it’s also easy for a single item using await . For example, to calculate the SHA-256 hash, Javascript provides a digest() async function: This looks almost identical to the previous call, the only difference is an await . But for collections, handling asynchronicity becomes different. To calculate the double for each element in a collection of strings is simple with a map : But to calculate the hash of each string in a collection, it does not work: This is an everyday problem as async functions make a bigger category than synchronous ones. A sync function can work in an async environment, but an async one can not be converted to work in a synchronous way. For a more realistic scenario, querying a database is inherently async as it needs to make network connections. To get the score for a single userId from a hypothetical database is simple: But how to query the database for a collection of userId s? And it’s not just about using map to transform one value to another. Is the user’s score above 3? But is any of the users’ score above 3? Or what is the average score? To make things more interesting, adding an async to a map at least gives some indication what is changed: But a filter just does something entirely wrong: Because of this, async collection processing requires some effort, and it’s different depending on what kind of function you want to use. An async map works markedly differently than an async filter or and async reduce . In this series, you’ll learn how each of them works and how to efficiently use them. But first, let’s talk about for loops! I don’t like for loops as they tend to promote bad coding practices, like nested loops that do a lot of things at once or continue / break statements scattered around that quickly descend into an unmaintainable mess. Also, a more functional approach with functions like map / filter / reduce promote a style where one function does only one thing and everything inside it is scoped and stateless. And the functional approach is not only possible 99% of the time but it comes with no perceivable performance drop and it also yields simpler code (well, at least when you know the functions involved). But async/await is in the remaining 1%. For loops have a distinctive feature, as they don’t rely on calling a function. In effect, you can use await inside the loop and it will just work. As for loops are a generic tool, they can be used for all kinds of requirements when working with collections. But their downside is still present, and while it’s not trivial to adapt the functional approach to async/await, once you start seeing the general pattern it’s not that hard either. In the following articles in this series, we’ll look into how the commonly used array functions can be used in an async way. Stay tuned!", "date": "2020-02-11"},
{"website": "Advanced-Web", "title": "How to use async functions with Array.reduce in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-array-reduce-in-javascript/", "abstract": "In the first article , we’ve covered how async/await helps with async commands but it offers little help when it comes to asynchronously processing collections . In this post, we’ll look into the reduce function, which is the most versatile collection function as it can emulate all the other ones. Reduce iteratively constructs a value and returns it, which is not necessarily a collection. That’s where the name comes from, as it reduces a collection to a value. The iteratee function gets the previous result, called memo in the examples below, and the current value, e . The following function sums the elements, starting with 0 (the second argument of reduce ): 0 0 + 1 1 + 2 3 + 3 0 1 2 3 6 The reduce function An async version is almost the same, but it returns a Promise on each iteration, so memo will be the Promise of the previous result. The iteratee function needs to await it in order to calculate the next result: 0 (await 0) + 1 (await Promise(1)) + 2 (await Promise(3)) + 3 0 1 2 3 Promise(6) Async reduce function With the structure of async (memo, e) => await memo , the reduce can handle any async functions and it can be await ed. Concurrency has an interesting property when it comes to reduce . In the synchronous version, elements are processed one-by-one, which is not surprising as they rely on the previous result. But when an async reduce is run, all the iteratee functions start running in parallel and wait for the previous result ( await memo ) only when needed. In the example above, all the sleep s happen in parallel, as the await memo , which makes the function to wait for the previous one to finish, comes later. Async reduce with \"await memo\" last reduce reduce e1 e1 e2 e2 e3 e3 sleep(10) await await 1 3 6 But when the await memo comes first, the functions run sequentially: Async reduce with \"await memo\" first reduce reduce e1 e1 e2 e2 e3 e3 await await sleep(10) 1 sleep(10) 3 sleep(10) 6 This behavior is usually not a problem as it naturally means everything that is not dependent on the previous result will be calculated immediately, and only the dependent parts are waiting for the previous value. But in some cases, it might be unfeasible to do something ahead of time. For example, I had a piece of code that prints different PDFs and concatenates them into one single file using the pdf-lib library. This implementation runs the resource-intensive printPDF function in parallel: I noticed that when I have many pages to print, it would consume too much memory and slow down the overall process. A simple change made the printPDF calls wait for the previous one to finish: The reduce function is easy to convert into an async function, but parallelism can be tricky to figure out. Fortunately, it rarely breaks anything, but in some resource-intensive or rate-limited operations knowing how the functions are called is essential.", "date": "2020-02-18"},
{"website": "Advanced-Web", "title": "How to use async functions with Array.map in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-async-functions-with-array-map-in-javascript/", "abstract": "In the first article , we’ve covered how async/await helps with async commands but it offers little help when it comes to asynchronously processing collections . In this post, we’ll look into the map function, which is the most commonly used as it transforms data from one form to another. The map is the easiest and most common collection function. It runs each element through an iteratee function and returns an array with the results. The synchronous version that adds one to each element: An async version needs to do two things. First, it needs to map every item to a Promise with the new value, which is what adding async before the function does. And second, it needs to wait for all the Promises then collect the results in an Array. Fortunately, the Promise.all built-in call is exactly what we need for step 2. This makes the general pattern of an async map to be Promise.all(arr.map(async (...) => ...)) . An async implementation doing the same as the sync one: Async map map map e1 e1 e2 e2 e3 e3 1 2 3 2 3 4 The above implementation runs the iteratee function in parallel for each element of the array. This is usually fine, but in some cases, it might consume too much resources. This can happen when the async function hits an API or consumes too much RAM that it’s not feasible to run too many at once. While an async map is easy to write, adding concurrency controls is more involved. In the next few examples, we’ll look into different solutions. The easiest way is to group elements and process the groups one by one. This gives you control of the maximum amount of parallel tasks that can run at once. But since one group has to finish before the next one starts, the slowest element in each group becomes the limiting factor. Mapping in groups Group 1 Group 2 Group 3 map map e1 e1 e2 e2 e3 e3 e4 e4 e5 e5 To make groups, the example below uses the groupBy implementation from Underscore.js . Many libraries provide an implementation and they are mostly interchangeable. The exception is Lodash, as its groupBy does not pass the index of the item . If you are not familiar with groupBy , it runs each element through an iteratee function and returns an object with the keys as the results and the values as lists of the elements that produced that value. To make groups of at most n elements, an iteratee Math.floor(i / n) where i is the index of the element will do. As an example, a group of size 3 will map the elements like this: In Javascript: The last group might be smaller than the others, but all groups are guaranteed not to exceed the maximum group size. To map one group, the usual Promise.all(group.map(...)) construct is fine. To map the groups sequentially, we need a reduce that concatenates the previous results ( memo ) with the results of the current group: This implementation is based on the fact that the await memo , which waits for the previous result, will be completed before moving on to the next line. The full implementation that implements batching: Another type of concurrency control is to run at most n tasks in parallel, and start a new one whenever one is finished. Controlled concurrency map map e1 e1 e2 e2 e3 e3 e4 e4 I couldn’t come up with a simple implementation for this, but fortunately, the Bluebird library provides one out of the box. This makes it straightforward, just import the library and use the Promise.map function that has support for the concurrency option. In the example below the concurrency is limited to 2 , which means 2 tasks are started immediately, then whenever one is finished, a new one is started until there are none left: Sometimes any concurrency is too much and the elements should be processed one after the other. Mapping sequentially reduce reduce e1 e1 e2 e2 e3 e3 e4 e4 A trivial implementation is to use Bluebird’s Promise with a concurrency of 1. But for this case, it does not warrant including a library as a simple reduce would do the job: Make sure to await the memo before await -ing anything else, as without that it will still run concurrently! The map function is easy to convert to async as the Promise.all built-in does the heavy lifting. But controlling concurrency requires some planning.", "date": "2020-02-25"},
{"website": "Advanced-Web", "title": "How to lazy load and initialize elements using an Intersection Observer", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-lazy-load-and-initialize-elements-using-an-intersection-observer/", "abstract": "One upside of using low-end hardware is that I’m the first to notice when something is slow and I’m kinda forced to do something about it. Usually, companies follow Joel’s advice and give programmers the best tools money can buy, in which case there is a separation from end-users. On a high-end smartphone using WiFi with a fiber connection, nothing is slow, but your average visitor might not have that. I have a site with a few Leaflet maps here and there and I realized that initializing even a few maps on page load makes a noticeable bump when I open the page. One map is not a problem, but five is. So I started looking for solutions for how to reduce the amount of work done on page load. The obvious approach is lazy loading . Why initialize an element on the bottom of the page when the majority of users don’t even scroll down that far? By implementing lazy loading, the site not only will load faster , but it will also consume less bandwidth overall. In terms of code, initializing a Leaflet map looks like this: I wanted to run this code only when the element is visible and not before. After a bit of searching, I’ve found the almost ideal solution specifically engineered to solve this problem: the Intersection Observer . Its name sounds like something only available on Chrome behind a vendor prefix, but in reality, it has wide support . While it seems like it won’t change too much, at the time of writing it was in “Working Draft” status, which means it can change significantly. Without the Intersection Observer, detecting when an element becomes visible can be done by listening to scroll and resize events and calculating whether the element’s bounding rectangle intersects with the viewport, while also taking into account any scrollable elements and iframes. Detecting visibility change with scroll events Scroll event Scroll event Event handler Event handler getBoundingClientRect getBoundingClientRect viewport element parent root scrollable container element parent root This approach, while it works, is wrong on two accounts. First, it is extremely ineffective as running a bunch of getBoundingClientRect() s for every scroll event, that can come multiple times a second, consumes a lot of CPU. And second, it’s quite hard to do it right , considering all the edge cases of scrolling and clipping elements. Meet the Intersection Observer, which abstracts all that away. You just define which element you want to check, and it will invoke a callback when something interesting happened. No more excessive event checking, and it takes care of all the elements between the target and the viewport. Detecting visibility change with Intersection Observer Intersection Observer Intersection Observer callback callback observe(element) intersection changed To create and use an Intersection Observer, create a new object with a callback function: When the callback is run, it does not mean that the element is visible. That’s why you need to check the isIntersecting property: Now that the Observer is in place, start observing an element by calling observe : To stop it, use disconnect : These are the basics, and they are everything required to make a lazy initializer function that notifies a callback when an element becomes visible: For my use case with Leaflet, enabling lazy loading requires just two lines of extra code: An edge case is when the element is visible immediately on page load. Fortunately, Intersection Observer handles this case. The above script runs just fine, no matter when the element is located. An important aspect of lazy loading is element sizing. For example, when you load an image only specifying the width, its height is determined by the aspect ratio of the image itself. If you load the image lazily, this might mean a different height before and after loading which in turn moves everything below it . Placeholder element sizing image image placeholder image size User is scrolling visible network request User is scrolling past response image size placeholder image size !== image size reflow content below the image It usually does not manifest as a problem during development and testing. If the image takes a split second to load, or at least to start loading, it has its final height by the time you scroll through it. But the situation for a new visitor might be different. Imagine a low-end device with an Internet connection having > 1-second pings, and empty browser and DNS caches. Having elements changing their sizes is a UX nightmare, as you are trying to read something then suddenly it goes up a bit, then down again. I experience it with ads especially on mobile, and it’s just terrible. When you implement a lazy-loading solution, make sure to set the dimensions for the placeholder right . Apart from Leaflet, another great candidate for lazy loading is images. They tend to be one of the biggest contributors to page size, and they tend to be scattered all over the page. Not loading the bottom ones during page load can save a lot of bandwidth. But remember to include something in the HTML with the right dimensions on page load to make sure content below the image does not jump around. This can be as simple as a single transparent pixel, or as fancy as a downsized version of the original image. For the former, here is a base64-encoded version: Let’s construct a reusable solution that works for any img tags! To store the URL of the image, a data- attribute is a good solution. Then when the lazy loading script kicks in, it just replaces the src with the data- property. For the examples, I’ll use a data-lazyload=\"<url>\" construct. Lazy loading images Intersection Observer Intersection Observer img img src=<placeholder> data-lazyload=<image> Create observer visible src=<image> network request To get all images for lazy loading, use: document.querySelectorAll(\"img[data-lazyload]\") . This returns a list with all the img s that have the data-lazyload property. Then it’s just a matter of calling the lazyInit function and setting the src attribute: Don’t forget to set the width/height of the element either via width=\"...\" height=\"...\" attributes or CSS. The lazyInit implementation fires off the callback when the target element becomes visible. This means the user is likely to notice the loading process and, especially on slower connections, might need to wait a bit. It would be better to start loading a little bit ahead of time so that by the time the viewport reaches the element it is fully loaded. Just-in-time loading Intersection Observer Intersection Observer element element Not loaded User is scrolling Element visible network request response Loaded Of course, it becomes a UX-bandwidth tradeoff. If elements are loaded too soon then it wastes bandwidth, if they are loaded too late it degrades UX. Ahead-of-time loading Intersection Observer Intersection Observer element element Not loaded User is scrolling network request response Loaded User is scrolling Element visible Already loaded Intersection Observer supports a solution for this. You can specify a margin that is applied to the element when calculating intersections. For example, you can specify a margin of 100px and that will fire the callback when the target is less than 100 pixels from the viewport. This makes HTML+CSS hacks like invisible absolute-positioned elements around the target unnecessary. To set this margin, use the second argument of the constructor: It accepts the same values as the margin CSS property, so “100px” is 100 pixels in all directions, while “100px 0 0 0” adds 100 pixels only to the top. The problem with rootMargin is that it does not play well with IFrames. Unlike the basic functionality of the Intersection Observer, it does not work the same when a page is embedded in another one. Take a look at this page , hosted on GistRun that is using IFrames to show a GitHub gist in action. As you scroll down the page, you’ll see when the Intersection Observer starts loading the image and initializing the map. You’ll see that there is no margin. The same gist, but without an IFrame is available here . The same logging, and you’ll see that now the callback fires well before the target element is in view. Seems like there are inconsistencies between the browsers and whether the margins are negative or positive. Some tickets are tracking this behavior and some progress on fixing this . But unfortunately, the fix is mainly concerned about allowing the IFrame’s document to be a valid root (by default it’s the viewport of the browser), which still means it will work differently depending on whether the page is loaded in an IFrame or not. Fortunately, the Intersection Observer still works, it just ignores the rootMargin . For lazy loading, that means a slightly degraded user experience, but the elements will be initialized nevertheless. But keep this issue in mind as it might break functionality for a use-case other than lazy loading. There are some other config properties for the Intersection Observer which are not needed for lazy loading but no article on this subject is complete without at least mentioning them. The root specifies the element thats intersection with the target we are interested in. If unspecified it is the viewport, which is exactly what is needed for lazy loading. The spec defines how to take scrolling and clipping elements between the root and the target into account, so it just automagically works the way you expect. The threshold is the required percentage of intersection for the callback to be called. The default is 0, which means any part of the element comes into view, the callback will be fired. To require half of the element to be intersected, use: Another sytactic sugar is that you can use an array to report on multiple stages: threshold: [0.25, 0.5, 0.75] . The MDN article has an awesome example that helps with visualizing how the threshold works. Notice that scrolling the IFrame or scrolling the page does not make a difference, the intersections are correctly reported. Don’t forget that the value of 1 means the callback is only run when the entire element is visible. If the element is bigger than the viewport (the root), it might never happen. The Intersection Observer replaces the complicated and resource-intensive scroll and resize events for lazy loading. Apart from its handling of the additional margin inside IFrames, it abstracts away the edge cases and provides a terse API. While it’s not a full replacement to other events, whenever it’s power is enough, it should be the preferred solution.", "date": "2020-02-04"},
{"website": "Advanced-Web", "title": "Using external libraries in JShell", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/using-external-libraries-in-jshell/", "abstract": "After I’ve experimented with JShell to solve mathematical puzzles with the Java’s Standard Library, I set out to use it for two other use cases: investigate external libraries, and use the REPL experiment with classes defined in a project. Ideally, JShell itself would offer a resolution mechanism to grab external dependencies but currently, there’s no such feature. There’s a custom version of JShell which provides the /resolve command to download artifacts, but it’s not maintained anymore. Although external JAR files can be imported via classpath , this alone is pretty cumbersome to try third-party libraries. A typical library is available on a Maven repository, and normally it’s consumed by a dependency management tool. So first, you have to download the JAR file manually, and put it on the classpath. If it has any dependencies, those have to be downloaded as well. Finally, you have to craft the classpath yourself. We write articles like this regularly. Join our mailing list and let's keep in touch. Also, I’ve found out the hard way that there’s a strange bug in JShell: packages in classpath don’t always appear in completions . If the classpath is specified via the --class-path argument, autocompletions will not list the external packages. However, if the classpath is specified using the CLASSPATH environment variable, it works: To summarize, while it’s possible to download dependencies by hand and craft the classpath manually, it’s really tedious. To make things a bit easier, I decided to see how JShell could be integrated into a Maven or Gradle project which already solves the problem of dependency resolution. A plugin exists for both build tools ( jshell-maven-plugin , jshell-gradle-plugin ) that allows launching JShell with the project’s classpath. One problem with these plugins is that they are using the --class-path argument under the hood, so they are affected by the bug I’ve mentioned in the previous section. The solution is simple: even without the plugins, it’s quite easy to get the project’s classpath directly from the build tool and use it in the CLASSPATH environment variable. For a Maven project, the Maven Dependency Plugin ’s build-classpath goal can be used for this purpose (note that the usual verbose is suppressed by the grep command which leaves only the classpath): It’s not as easy for Gradle, but with a simple task, you can print the classpath… … which can be used to fill the environment variable (again, the -q flag is necessary to suppress the other noises from the build log): Both of these approaches allow us to load external libraries and experiment with classes defined in the project . Occasionally I want to quickly check how a library works without opening an IDE or altering a project. To make this easier, I’ve put a custom bash script called jshell-resolve to my PATH that expects one or more Maven Coordinates as arguments, resolves them using a generated Gradle build and starts JShell with all the artifacts available on the classpath. For example, with jshell-resolve com.google.guava:guava:28.2-jre JShell starts with Guava: The script launches a Docker container including a JDK, Gradle and an empty Gradle project with the specified dependencies. The classpath will only contain the specified packages. In IntelliJ IDEA there’s built-in support for JShell, which can be launched with the project’s classpath. Although the experience is a bit different than the terminal-based shell, as there are separate input and output windows. (For an example, see the Java 9 and IntelliJ IDEA post from the JetBrains blog .) The support is not so nice for Eclipse though. I’ve tried to use JShell as an External Tool suggested by this post , but autocompletion did not work in the Console window. Sadly, I’ve found no viable alternatives other than the old Scrapbook Page feature, (suggested in this SO thread ) which is not quite the same as JShell. This post covered alternatives about accessing your classes in JShell. It’s really great to see a shift from the IDE oriented Java language to having a REPL, which is a great addition to the ecosystem.", "date": "2020-01-14"},
{"website": "Advanced-Web", "title": "How to manage Lambda log groups with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-manage-lambda-log-groups-with-terraform/", "abstract": "Lambda automatically creates all log resources when a function is created which enables permanent logging even if you do nothing special to enable it. This is convenient as logging is an essential debugging tool, and you don’t want to realize you don’t have any records when you need them. Lambda puts its logs straight into CloudWatch Logs, and that is using 2 layers to organize logs. The upper layer is the log groups and it contains the log streams , which in turn is a container for the log events . With Lambda, there is one log group for each function, and multiple streams are created under it, (at least) one for each version. Lambda CloudWatch Logs Function Function version execution Log group Log stream Log events Lambda logging to CloudWatch Logs The Lambda service creates these resources but it needs permission to do so. The AWSLambdaBasicExecutionRole defines the basic permissions for functions, and it allows creating each of the log resources: If you attach this managed policy or a policy with these permissions, the Lambda service creates all resources related to logs. Lambda creates the log group Lambda function Lambda function Log group create create Log group invoke log There are 2 problems with the default approach. First, the log group created by Lambda does not set any expiration to the log messages. This is a good default, but since it incurs costs to store logs, in most cases it is unnecessary. Unfortunately, there is no way to control this default. The second problem is that if you use an IaC solution to deploy the architecture, like Terraform or CloudFormation, log groups are not managed by that . When you destroy the stack the function is gone, but the log group it created is not. And all the log messages are also kept there, forever. The log group exists after the function is gone Terraform Terraform Lambda function Lambda function Log group create create Log group destroy Still there From a cost perspective, it is not a big problem as these logs tend to be small. But clutter is costly in the long term, as everything that queries the log groups will need to filter out an increasing amount of noise. A better approach would be to make sure the logs are gone when the function is deleted, except for cases when you configure it otherwise. And this is possible with Terraform. But as usual, it needs some planning. In this article, you’ll learn how log groups creation works and how to make Terraform manage its lifecycle so that you have full control of whether it will survive the function or not. I’ll use the code from this GitHub repository . It covers all 3 use-cases that are described below. Since the examples use Terraform as well as the AWS CLI, there are two environment variables to control the region. The AWS_REGION is for Terraform, while the AWS_DEFAULT_REGION is for CLI. Make sure to set both of them to the same region: First, let’s see how Lambda works by default! This is the default case and used for most functions. Lambda-managed log groups Terraform Terraform Lambda Lambda CloudWatch Logs CloudWatch Logs create create log group lambda invoke log logs describe-log-groups [log group] destroy logs describe-log-groups [log group] The function needs permission to create all log resources, either via the managed role, or a custom policy: Then deploy the stack, and invoke the function: Inspecting the log group for the function: The log group is created automatically, and it does not expire (the retentionInDays parameter is not set). Since it is not managed by Terraform, destroying the stack does not delete the log group: A straightforward solution is to not let Lambda create a log group, which means there is no log group remaining after deleting the function, but it also means no logging. Interestingly, it does not affect the function execution in any ways, it just does not put the logs anywhere. Disabled logging Terraform Terraform Lambda Lambda CloudWatch Logs CloudWatch Logs create No permission to create a log group lambda invoke Can't log logs describe-log-groups [] destroy logs describe-log-groups [] I don’t recommend using this, but it’s interesting to see that it works. Use a role that lacks the CreateLogGroup permission: Create the stack: The function still works: But there is no log group: Unsurprisingly, deleting the stack does not leave a log group: This solution builds on the previous step. If you want Terraform to manage the log group resource, you need to make sure Lambda won’t create it accidentally. Terraform-managed log group Terraform Terraform Lambda Lambda CloudWatch Logs CloudWatch Logs create no permission to create a log group create log group lambda invoke log logs describe-log-groups [log group] destroy destroy log group logs describe-log-groups [] This means that just like before, you need to remove the CreateLogGroup permission from the function. Making Terraform manage the log group allows you to define its parameters, such as the retentionInDays which controls the auto-expiration of log messages. The log group has a fixed name of /aws/lambda/<function name> , and this is the only thing that connects it to the function itself. Creating the stack and invoking the function works: Also, there is a log group that gives the function a place to log: Note the retentionInDays which reflects the property in the Terraform config. Destroying the stack and inspecting the log group again: The log group is gone with function. While this works for normal Lambda functions, it isn’t enough for Lambda@Edge logging. Since an edge function runs in different regions, it creates a log group in each of them with the name of /aws/lambda/us-east-1.<function name> when there is a request to the function. This means Terraform has to create a log group in every region , which is not supported at this moment. By removing the CreateLogGroup permission and adding an aws_cloudwatch_log_group resource with the correct name, Terraform can manage all Lambda logging resources. This enables you to define the properties of the log resources as well as clean them up when the stack is deleted.", "date": "2020-01-21"},
{"website": "Advanced-Web", "title": "Automatically schedule emails on MailChimp with RSS and AWS Lambda", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/automatically-schedule-emails-on-mailchimp-with-rss-and-aws-lambda/", "abstract": "For quite some time now, I’ve been writing the JS Tips newsletter , which is a WebDev-related tip sent out every week. Initially, I scheduled the tips manually, creating an email campaign on MailChimp every week. The emails are in Markdown, so I needed to generate the HTML, insert it into a new mail, fill in the title and optionally the preview text, and finally set the time. It didn’t take a lot of time, but it’s tedious to do it by hand. No wonder I wanted to automate it, both the Markdown -> HTML conversion and the scheduling. Week 1 Week 2 email1.md email1.html MailChimp campaign 1 email2.md email2.html MailChimp campaign 2 convert schedule convert schedule Converting and scheduling by hand So I had a bunch of emails , and I wanted an automated solution that sends them out to the audience one at a time on a weekly basis . Then when all of the mails are sent, it starts over from the first one, effectively creating an infinite loop. This way, when people sign up they’ll start getting emails from that point and eventually, they’ll get all the content. Email 1 Email 2 Email 3 Email 1 Email 2 ... Sending emails infinitely A solution like this is perfect for non-time-sensitive and non-linear content , which can be a lot of things. A few examples are tips to do something, language features of a programming language, independent lessons for regex features, phrases for learning a foreign language or even just short, independent stories. Anything that does not need to be consumed in a serialized form. What kind of content is not a good fit ? A serialized newsletter , where one email builds upon the previous one, such as a lesson series. For this, you can use MailChimp Automations. Another type that is not a good fit is time-sensitive emails , like news. You don’t want to send old content to subscribers. The solution I’ll describe below uses RSS, which is independent of MailChimp. An alternative solution would use MailChimp’s signup Automation where after signing up, the recipient gets the predefined series of emails. Signup automation MailChimp MailChimp Subscriber 1 Subscriber 1 Subscriber 2 Subscriber 2 Subscribe Email 1 Email 2 Email 3 Subscribe Email 1 Email 2 Email 3 Signup Automation lets you define a series of emails to send to new subscribers. You can then add all your emails and you can be sure recipients will get them in order. The upside is ordering . You can have a linear content that builds on the previous email. But there are two downsides . First, it won’t go infinitely as no matter how many emails you add, the automation runs out at one point. Especially if you have a lot of emails, people forget about a specific one by the time it comes around. You can say that it’s just noise after that, but people can unsubscribe when they don’t get any value out of the emails. And your newsletter won’t dry up at one point. The second and bigger downside is that subscribers are out-of-sync , which makes keeping the content up-to-date more difficult. If you have 100 emails in your newsletter and the subscribers are distributed in time, you need to do regular checks to make sure everything is still relevant. With synchronized sending, you just need to make sure the next one is updated. Let’s dive into the solution! The solution is based on an RSS feed and MailChimp’s “blog updates” automation. Using the MailChimp API to automatically schedule emails would also work, but I prefer solutions that are less vendor-dependent. RSS is a machine-readable format for content updates , especially suited to notify systems about blog updates. While it is less used by end-users now, exemplified by the shut down of Google Reader , it is widely supported by blog engines, such as WordPress , Jekyll , Ghost , Medium , just to name a few. Its implementation is a file called rss.xml with some information about the blog itself and its latest contents. On MailChimp’s side, it can send emails for blog updates, making it easy to automate a newsletter. And it uses the RSS feed of the blog to know when a new article is published. RSS is in XML format with a defined structure: It has the blog’s title ( channel/title ), the blog posts ( channel/item* ), the individual posts’ titles ( channel/item/title ), the contents ( channel/item/description ), the publication date ( channel/item/pubDate ), and a guid ( channel/item/guid ). The guid is the identifier of the post, as everything else can change it makes sure that consumers of the RSS channel do not get duplicated content. There are other properties , but these are the important ones for this solution. MailChimp periodically (usually once per day) fetches the rss.xml file and check if any new entries are found. It knows the last time it fetched the contents, and compare the pubDate and the guid properties inside the item . MailChimp sending an email for new content MailChimp MailChimp rss.xml rss.xml subscribers subscribers Check Check new item Email Check When MailChimp finds a new entry, it compiles an email based on the template you define on MailChimp’s side. In the template, you can use placeholder values that will be substituted with the corresponding data from the rss.xml before sending the email. There are several such merge tags , but we’ll use only a few: rss.xml channel item* MailChimp title title description guid pubDate *|RSSFEED:TITLE|* *|RSSITEM:TITLE|* *|RSSITEM:CONTENT_FULL|* RSS -> MailChimp merge tags RSS was designed for blogs in mind, but who said an rss.xml can’t be auto-generated? The idea is this: generate an rss.xml with Lambda and return a single item with the content you want to send out. Then configure MailChimp to regularly check this feed and send out an email to the audience when this item changes. Scheduling emails with Lambda MailChimp MailChimp Lambda Lambda Subscribers Subscribers Get rss.xml <item> Email 1 </item> Email 1 Get rss.xml <item> Email 1 </item> No email Get rss.xml <item> Email 2 </item> Email 2 For example, if you have 3 emails you want to send out, one every day, and repeat them infinitely, then on the first day the rss.xml contains one item with the content of the first email, with a publication date and guid of that day. On the second day, the single item ’s content is the second email’s, and the publication date and guid is the date of that day. Same on the third day but for the third email. Then on the fourth day, the item ’s content is the first email’s content, but the publication date and the guid are that of the current day’s. MailChimp will see a new blog entry whenever Lambda returns a new item, pastes the content and the title using merge tags and sends out an email to the audience, fully automated. With this solution, you can have a rotating email newsletter, sending emails out without any manual work. First, let’s see how a Lambda implementation would look like! You can find the whole code in this GitHub repo . This solution is a Lambda function behind a public API Gateway endpoint, all managed by Terraform. Since the API Gateway and the Terraform config follows a fairly standard structure, I’ll focus on the Lambda function in this article. You can find the full implementation in the repository . The main.tf is the primary entry point for Terraform and it defines all resources and configurations. The Lambda implementation is in src/index.js , while the dependencies are defined in src/package.json and src/package-lock.json . The contents of the emails are in the src/content/1.html, 2.html, 10.html . The rss package offers a concise API to generate the rss.xml . I’ve found that even if it seems like a simple enough case to just concatenate strings, it’s usually a good idea to use a library that does the escaping. Also, there is gray-matter to parse the title of the email out of the front-matter. Using front-matter to define metadata allows a self-contained file: Which MailChimp will render like this (note the Subject line too): Two things to note here. First, the title: part on the top is part of the subject of the email. Also note the <style> block that defines CSS rules. I’ve found that style blocks might not be supported by emails, so it’s better to convert it to style=\"...\" attributes. The inline-css library does this. And finally, moment helps with date manipulation with a nice API. While in this example the emails are in HTML, it’s quite easy to add Markdown support too. Let’s move on to the actual implementation! The first step is to interface with API Gateway by providing a result object that it expects. exports.handler statusCode body headers Lambda structure (1/3) This calls the getRSS() function and adds the required properties to the result. Separating the parts specific to API Gateway allows a separate entry point to call the main logic. This helps with development by calling it without deployment: Just don’t forget to either move it to a separate file or remove it before deployment. The getRSS function returns the XML as a string. exports.handler statusCode body headers getRSS() rss Lambda structure (2/3) The current content, date, and title are returned from the getContent() call, then this function creates the RSS feed structure. The feed’s title will be the value of the *|RSSFEED:TITLE|* merge tag, which can be used to build part of the email subject. The bulk of the magic is in the getContent() implementation which generates the current item. exports.handler statusCode body headers getRSS() rss getContent() content date title Lambda structure (3/3) Let’s build it piece-by-piece! First, calculate the number of days since the newsletter started: This defines a newsletter with a new email every day. But this is just up to the implementation to define a different timing. To send an email every week , then calculate the difference in weeks, then don’t forget to take this into account for the publication date: The next step is to read the corresponding file. As the emails are in the contents folder and their names define the ordering ( 1.html , 2.html , …), make sure to use a numerical-aware ordering when sorting the filenames so that 10 will be after 2 . Fortunately, Javascript has built-in support for this with the comparator function new Intl.Collator(undefined, {numeric: true}).compare . To get the filename indexed by the difference in days: Then with the filename, read the title and the contents: Finally, run inline-css in case any <style> blocks are defined: The whole implementation looks like this: To deploy the function to AWS you need to configure an API Gateway and do the usual chores of providing a HTTP API for a function. This is out-of-scope for this article, as there is nothing special here. See the implementation in the GitHub project. The sample code uses an external data source to automatically fetch the NPM dependencies , so you don’t need to run it manually. Of course, you need npm installed on the machine, as well as Terraform. To deploy the sample project, run: Now that there is a generated rss.xml , let’s move on to MailChimp-side! In your MailChimp account, go to Campaigns and create a new campaign: Select Email: Under the Automated tab, select “Share blog updates”: The important input is the “RSS feed URL”, which is the URL output from Terraform. For the time, select “Every day” even if you do a weekly newsletter. This controls how often MailChimp checks for updates and that should be as often as possible. The periodicity of the newsletter should be controlled by the Lambda function. After the RSS settings, configure the campaign just like you would any other campaign and use merge tags for dynamic data. For the subject line, I suggest [*|RSSFEED:TITLE|*] *|RSSITEM:TITLE|* , which generates [Newsletter title] Email title . For the email contents, use the *|RSSITEM:CONTENT_FULL|* merge tag to insert the contents of the email: Also, check the preview to make sure everything looks good: Then start the campaign and offer visitors a place to subscribe. They will get the emails from the time they are subscribed. I let the sample project run for a while, and I got the emails like clockwork. As you can see, it starts over from the first one when it reaches the last email. Lambda is billed for usage, and as MailChimp checks the feed once per day per campaign, it will run for a few seconds a day . This is a minuscule amount. The same goes for the bandwidth cost. As for MailChimp, you pay for the audience and emails sent, an RSS-based sending solution brings no additional costs. Bottom line : practically free. Since the same emails are sent out over and over again, they might require some updates. Links might get dead, new versions of software released, or the content goes out-of-date. That’s why I like to get the next email before it goes out to everybody else. This way I have the chance to fix any problems and the subscribers only get up-to-date content. To solve this, I could’ve used a separate function and duplicate everything, but I opted for a query parameter that shifts the date of the epoch by a number of days. Then I added a new MailChimp campaign with a segment that has only me as the recipient. The <rss url> that Terraform outputs is the real feed that everybody gets. But the <rss url>?daysOffset=3 is another feed that sends out the emails 3 days prior. This is the feed for the testing campaign. To add a parameter to Lambda, get it from the queryStringParameters : Then when calculating the time of the first email, subtract this many days: With this solution, you are not constrained to a single testing feed. You can use <rss url>?daysOffset=1 , along with <rss url>?daysOffset=3 and 5 . On the MailChimp side, create a new campaign for each testing email. Make sure to filter the audience so that only you will get these emails. Optionally, give it a distinctive title, such as “ [*|RSSFEED:TITLE|* -1] *|RSSITEM:TITLE|* ” and “ [*|RSSFEED:TITLE|* -3] *|RSSITEM:TITLE|* ” so that it’s immediately obvious in how many days the email will go live. Testing emails Live campaign Testing campaign MailChimp MailChimp rss.xml rss.xml Subscribers Subscribers rss.xml? daysOffset=1 rss.xml? daysOffset=1 You You Day 1 Live campaign Check Email 1 Email 1 Testing campaign Check Email 2 Email 2 Day 2 Live campaign Check Email 2 Email 2 Testing campaign Check Email 3 Email 3 You can have any number of such testing campaigns. Let’s talk about the security implications of adding this parameter! While the RSS URL does not show in the emails that go to the audience, it is not a secret either. With the daysOffset parameter, if somebody gets access to the URL, he can fetch all the email at once. Because of this, it’s a good idea to specify a limit to it, just like for any user-provided value: I did not need to include images to my email series, but it should be possible. Since the content is generated by the Lambda function over which you have full control, it shouldn’t be hard to include images. I would make sure that they are defined as data URIs, so instead of <img src=\"image.jpg\"> , it is <img src=\"data:image/jpg;base64,...\" . This way there is no need to add another endpoint just to return the images. The solution described above is entirely stateless . It does not record when a particular email goes out and also does not keep track of future emails. This makes it easier to deploy the solution, and you don’t need to worry about losing a database if you make changes. But what happens if you add a new email to the series ? In this case, the previous cycles will be longer, and that will result in a jump backward in the queue. For example, if you have sent out your email series twice, adding an email will result in a jump back 2 emails. The same happens for removing an email, just in reverse. Subscribers will experience a jump forward in the queue. This might be the less problematic case as they don’t know what email they did not get . How to add or remove emails then? Make sure you change the epoch value also. Change the date to the imaginary first email of the modified series. This is an effective and user-friendly way to send content to an arbitrary email list. It requires a bit of planning, especially when you need to add or remove parts of the sequence, but then you can practically forget about it. You can also add bells and whistles, like Markdown support, images, even dynamic content.", "date": "2020-01-28"},
{"website": "Advanced-Web", "title": "Prototyping with JShell", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/prototyping-with-jshell/", "abstract": "JShell (Java Shell) is an interactive command-line REPL (Read-Evaluate-Print Loop) for Java, available since Java 9. The motivations for having such a tool for the Java ecosystem can be found in JEP 222 : The Java developer ecosystem is heavily IDE-oriented. I welcome the shift to make Java more friendly to simple text editors, and having a REPL is a step in this direction. Although it’s certainly great for learning the language I was more curious about what JShell can add to the toolbox of the Java developer’s day-to-day business. Because reports on JShell with hands-on experience beyond a simple hello world are so rare, I thought it would be worth to try to solve actual problems and see how it fares. We write articles like this regularly. Join our mailing list and let's keep in touch. I choose Project Euler ’s first few problems for this challenge, as many of its problems can be solved with a short program using the standard libraries. The first good surprise was how lenient JShell is when it comes to accepting code snippets. Terminal semicolons can be omitted and expressions can be entered without enclosing them into a method. Also, there’s no need to worry about many of the commonly used packages, as they are imported by default : Variables can be redeclared , so it’s easy to fix any line of code (including a declaration) by loading it from history (up key) and changing the code. Tab completion can cycle through overloaded variants of the method and show JavaDoc . These features provided a solid ground for my trials, especially when I decided to write a one-liner solely with the Streams API to come up with a solution for a given problem. Another cool feature is that the whole session can be dumped into a file. This came in handy when I took a break but wanted to reuse my snippets later. For simple one-liners JShell is awesome, but when something more complex is needed, things get less and less comfortable. One such case is implementing interfaces or extending classes. The first pain point I’ve bumped into is that autocompletion does not list interfaces upon creating anonymous classes. This might be something that JShell will address later as the same works for abstract classes. A bigger problem is that there’s no aid to override the correct methods in the new class . An IDE provides many features that support this: None of these are supported by JShell. In my case, it was not a huge deal to figure out the method signature, because LongSupplier is well-documented, and it even mentions that the method I had to implement is getAsLong . However, for more complex cases and most 3rd party libraries, this would have been a bigger problem. Another thing that caught me off-guard once is that autocomplete silently dies when it encounters a syntax error instead of complaining about the problem. For example, because Long.getLong returns a Long , the autocomplete in the following example shows methods of the Long object: However, by introducing an extra parenthesis, it will just print the list of all available classes and packages. Of course, this is easy to spot, but there are more subtle cases: Luckily, the workaround is simple: evaluate the line in question, and the error will reveal itself: Using JShell to evaluate one-liners is a breeze, but defining methods that span over multiple lines is a real pain . JShell’s history does not support multi-line snippets . Whenever you define a method, you can’t easily recall the whole definition from history, but you have to do that line-by-line. It does not handle indentations automatically , and whenever autocompletion is used, search results are breaking the output, so you can no longer see the whole method. Also, if you spot a mistake in the previously entered line, you just can’t go back and fix it. So, when I had to define a method that required a bit more lines of code, I’ve opened an external editor with the /edit command. Of course, breaking out of JShell means losing all its nice features like the built-in autocompletion but I thought that defining a single method shouldn’t be too hard. I’ve hit /edit to open Vim, typed my code, hit save, only to realize that somewhere I’ve made a small mistake, which made my snippet invalid. Oh okay, it happens, let’s just reopen the code. However, when I listed the available snippets with /list , the code was gone. Because it was not valid, it was not even saved. In order to fix my error, I had to retype the whole thing. JShell is a great addition to the ecosystem, with the potential to aid new joiners as well as professionals on their day-to-day job. However, in its current form, it feels only half-baked. Many of its features, like the autocompletion, are well-thought and nice to use. On the other hand, its quirks make development hard really soon. I hope that in the future these features will be streamlined, and JShell can become an addition or even a replacement to and IDE based workflow.", "date": "2019-12-24"},
{"website": "Advanced-Web", "title": "How to deploy a single-page application with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-deploy-a-single-page-application-with-terraform/", "abstract": "As covered in the previous article , Terraform can run a build script as part of the state refresh process, i.e. during terraform plan and terraform apply . This capability opens the way to automatically compile a project in a way that is fully integrated with other resources. You can integrate Webpack that builds a React app, compile Typescript or CoffeScript, and many more scenarios become possible without an additional build step. Just a terraform apply and everything is built and deployed. build npm ci && npm run build deploy S3 bucket terraform apply But that is only the first half of deploying a single-page application. Running the build scripts generates the static files, now let’s see how to deploy them into an S3 bucket ! In this example, the frontend application is a React app built with Webpack in the frontend directory. The build is run by an npm ci followed by an npm run build , where we can also pass parameters from Terraform with the -- --env construct. Terraform resources external resources shell query program {...} result The compiled files are placed to the dist folder and the data source outputs its name. Let’s see how to use the resulting files! First, we need a bucket to store the files : Terraform aws_s3_bucket The way files are served to the end-user can be one of several approaches which is out of scope for this article. The easiest way is to configure a bucket website, but that is unsuited for a production environment on its own due to a lack of HTTPS support. A better solution is to use a CloudFront distribution with an S3 origin to serve the app as static files. With the frontend_build external data source, the destination directory is accessible on the \"${data.external.frontend_build.working_dir}/${data.external.frontend_build.result.dest}\" path. Since this explicitly depends on the data source itself (it uses the result object), the build is guaranteed to be run. To upload the files Terraform offers the aws_s3_bucket_object resource which can be combined with the fileset function , which is a new addition to Terraform, to iterate over the files. for_each = fileset aws_s3_bucket_object.0 aws_s3_bucket_object.1 aws_s3_bucket_object.2 aws_s3_bucket With a for_each it can generate a resource for every file found in the directory: In a for_each , the each.value contains the filename, which is the perfect candidate for the key attribute and along with the directory it identifies the source file. Also, as these resources use the output of the data source there is a direct dependency between them. There is no need for depends_on attributes. As with all resources, Terraform runs an update only when an attribute is changed . This is why, for example, you need to define the source_code_hash attribute for Lambda functions. src/index.html dist/index.html Object build etag: 1 deploy change build etag: 2 deploy Similarly, since the filename might not change when the file itself is updated, a similar mechanism is needed here. In this case, it’s called etag , but it works just the same: The filemd5 calculates a hash of the parameter file and that will be the etag of the object. When the file changes , the hash changes and, in turn, Terraform updates the resource . Without a proper content type set for the objects, they are downloaded but not shown. To make the frontend app work, we need to set it too for each object. content_type index.html text/html html app.js text/javascript js style.css text/css css extension lookup extension lookup extension lookup I tried to find a more robust way to do this but it seems no module gives back the mime type for a given file. That means I had to resort to a lookup table that maps extension to content types: Of if you prefer an error if an extension is missing from the mime_type_mappings , use: This is hardly an optimal solution, but it works. With the help of the external data source and the fileset function, Terraform can compile an app and upload it to an S3 bucket from where the frontend application can be served. All this while maintaining the integrity of the state and making sure the app is always up-to-date.", "date": "2019-12-17"},
{"website": "Advanced-Web", "title": "How to solve CORS problems when redirecting to S3 signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-solve-cors-problems-when-redirecting-to-s3-signed-urls/", "abstract": "The implementation of signed URLs on the frontend usually uses a 2-phase fetch. First, there is a request to the backend, asking to sign an S3 URL. Then a separate request is sent to the bucket to fetch the file. 2-phase fetch Frontend Frontend API API S3 S3 1) request file check access and sign URL signed URL 2) fetch file file When I implement a solution like this, I always have a strange feeling that something is not right. Implementing signed URLs should be transparent to the frontend, but with a 2-phase fetch, it is not. Also, separating the signing and the usage encourages bad practices, like signing ahead of time and increasing expiration time. Why can’t the backend respond with a temporary redirect, such as a 303, with the location of the file? The browser would automatically follow the link and it would be transparent to the frontend code. HTTP redirect Frontend Frontend API API S3 S3 1) request file check access and sign URL HTTP redirect automatic redirect fetch file file I did some investigation, and the reason is CORS (Cross-Origin Resource Sharing), which is a security mechanism that comes into play when the frontend makes requests to a different domain. But it is configurable, it just requires some planning. A common architecture uses 3 domains: one for the frontend, one for the backend, and one for the bucket the files are served from. This setup requires setting up for CORS, which means some headers must be returned from the backend and from the bucket. Domain 1 Domain 2 Domain 3 Frontend Backend S3 bucket Cross-origin Cross-origin The 2-phase fetch makes this process simple. The first fetch is affected only by its config and the headers return from the backend . Then the second fetch is only by its config and the headers returned from the bucket . The usual case for the first fetch is to set credentials: \"include\" as without it there will be no cookies sent which makes it hard to check access. In this case, the backend has to respond with both Access-Control-Allow-Origin: <domain> and Access-Control-Allow-Credentials: true , which makes it fully CORS-enabled and lets the frontend to read the body of the response. Note that the Origin can not be * when Allow-Credentials is true . This is according to the standard. It is vital to check the Origin header as failing to do so would let any domain to get access to the private files! Then the second fetch does not need credentials and only needs to get Access-Control-Allow-Origin: * from the bucket. Even this is not required for opaque responses, such as showing an image. But if you need to get programmatic access to the contents, you need to enable CORS. CORS headers for a 2-phase fetch Frontend Frontend API API S3 S3 1) {credentials: \"include\"} Access-Control-Allow-Origin: <domain> Access-Control-Allow-Credentials: true 2) {} Access-Control-Allow-Origin: * This is a straightforward setup CORS-wise as there is no need to consider how the backend and the bucket respond to a redirected request. Let’s see how instead of sending the URL in the body, sending a redirect would work! There are three options to send a temporary redirect: either a 302, a 303, or a 307 status code would do it. According to MDN , the difference is how they handle redirecting non-GET requests : 302 can change it to a GET, though there are no guarantees. 303 forces the redirected request to be a GET. 307 does not change it. In most cases, either of them could be good as the backend gets a GET request. But what if you want to download something using a POST? For example, a tracked download which is conceptionally a non-idempotent operation. But S3 requires a GET request, so in this case, a 303 status code has to be used. As I find it more robust, I’ll use 303 in the examples below. Let’s start with the most common scenario, which is to include the credentials in the request to the backend. It then signs a URL and sends a redirect. The call in this case is: Since this is a CORS request with credentials included, the backend has to respond with two headers: If either of them is missing or different, the browser won’t allow access to the response body. The bucket can be configured in one of three ways: The bucket can not send back Access-Control-Allow-Credentials , which is a limitation of S3, but fortunately, it is not needed. The following table shows whether the request body could be read for every configuration. The rows show what headers the API sends: it does not send any CORS-related headers, on the second row it sends Access-Control-Allow-Origin: * , while on the last row it sends Access-Control-Allow-Origin: <domain> and Access-Control-Allow-Credentials: true . The columns correspond to the bucket CORS configurations . On the first column, there are no CORS-related headers configured for the bucket. Then on the next column, it sends back Access-Control-Allow-Origin: * , while on the third it’s Access-Control-Allow-Origin: null . No surprise on the API-side, both Access-Control-Allow-Credentials: true and Access-Control-Allow-Origin: <frontend URL> is required. But on the bucket-side, only Access-Control-Allow-Origin: null works. Why the null origin? It turns out that redirecting to a different domain is a privacy-sensitive operation and as such the Origin header is not sent. Allowing the null origin seems to be opening an attack vector, but it is not ; CORS headers when credentials are included Frontend Frontend API API S3 S3 credentials: \"include\" HTTP redirect Access-Control-Allow-Origin: <frontend> Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: null A Lambda backend implementation: And the bucket CORS configuration, managed with Terraform: Let’s see what changes when credentials are not included in the request! While I don’t recommend this setup as access control is the most important part of a backend that provides signed URLs, it’s important to see how things change in this case. In this case, the API still needs to return CORS headers, but a simple Access-Control-Allow-Origin: * would suffice. After running the tests, these are the results: No surprise on the API-side, when CORS header allows the frontend origin the request works. But on the bucket side, both * and null works! But even if null works, I wouldn’t use that. CORS headers without credentials Frontend Frontend API API S3 S3 HTTP redirect Access-Control-Allow-Origin: * Access-Control-Allow-Origin: * A Lambda function: And the bucket config: A step towards simplifying infrastructure and CORS is to use fewer domains. Let’s investigate a setup where the frontend and the backend are on the same domain but the private bucket is on a different one. This can be configured with CloudFront . Domain 1 Domain 2 Frontend Backend S3 bucket Cross-origin In this case, there is no need to specify credentials: \"include\" as the request to the backend will be same-origin. Also, the backend does not need to send back any CORS-related headers. But the redirected request to the bucket still needs some configuration. The following table shows the results for this configuration: No surprise on the API-side, no headers are required. On the bucket configuration, a simple Access-Control-Allow-Origin: * is sufficient. The null origin does not work in this case. CORS headers for a 2-domains setup Same domain Frontend Frontend API API S3 S3 HTTP redirect Access-Control-Allow-Origin: * The Lambda code: And the bucket config: But this config can be simplified even further. The private bucket can be served via CloudFront also which means everything is under a single domain. Domain 1 Frontend Backend S3 bucket No cross-domain requests, no CORS-related problems. CORS headers for a 1-domain setup Same domain Frontend Frontend API API S3 S3 HTTP redirect Cross-domain requests require some planning to prevent CORS-related errors, but ultimately it is a matter of sending back the right headers. I was surprised to see that the null origin is the only one to work in the 3-domains scenario, but that is a valid configuration also. My recommendation is to use CloudFront to bring the different services under one domain and that, among bringing other benefits, solves CORS.", "date": "2020-01-07"},
{"website": "Advanced-Web", "title": "How to run a build script with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-run-a-build-script-with-terraform/", "abstract": "Terraform is to deploy infrastructure, and as a consequence, it can copy code that does not require compilation. As an illustration, a Lambda function does not even need to be in a separate directory but it can be embedded in the Terraform code. terraform archive_file terraform apply This works as long as you don’t need too many files, which point usually comes during the first npm install . But this only changes one parameter of the archive_file data source: src/ main.js node_modules/ archive_file source src src/main.js src/node_modules/ ... terraform archive_file: src terraform apply But what if the code requires compilation? Let’s say you want to use a different language that compiles to Javascript, or, in case of a frontend app, want to use React with JSX? With the source files: Running the build script: Generates the deploy artifacts: src frontend/package.json frontend/webpack.config.js frontend/node_modules/ ... dist frontend/ dist /index.html frontend/ dist /main.hash.js ... terraform archive_file: frontend/dist npm run build terraform apply But now you can’t just zip a directory with Terraform and expect it to work as it depends on whether the build script ( npm run build ) is run before that. Fortunately, there is a way to run scripts from Terraform. Terraform offers the external data source which is a configurable gateway between Terraform and the system it’s running on. You can define scripts here and as long as they conform to the data source’s API, it is considered a first-class citizen. Terraform resources external resources shell query program {...} result This is the command Terraform runs when it refreshes the state of this data source. Of course, you can not assume that a given program is available so if you use jq , npm , or anything else, it is still a dependency you need to take care of manually. The easiest way to specify the command is to invoke bash with it and use the HEREDOC syntax so there is no need to escape quotes: The result must be written to stdout and it has to be a valid JSON object. Other resources can use the properties: Stderr will be printed if the exit code is not 0 (there is an error). This means the original stdout should be redirected so that you’ll know what went wrong if there is an error. The above considerations give a standard structure for the program attribute as: For example, to run npm ci and npm run build then output the destination directory, use: This attribute sets the working directory the program is run. For example, if the frontend project is in the frontend directory, set working_dir as: We write articles like this regularly. Join our mailing list and let's keep in touch. By using the ${path.module} as a prefix it works both inside modules and top-level configuration files. Also, the value of working_dir is exported so other resources can depend on it. Other resources in the Terraform file can depend and use the results of the script. In this example, the result is a JSON with the destination directory. Why output a static value even though it could be specified on the consumers’ side? By using data.external.frontend_build.result.dest , it creates a dependency between the consumer and the external data source, which means there is no need to use the error-prone depends_on construct. But there is another catch. Instead of using an absolute path like this: Output a relative path , relative to the working_dir attribute and combine the two on the consumers’ side: Why? In the first case, the state file will contain the full path , which is different for different environments. Using a relative path makes sure that the state file is not machine-dependent. So far we’ve covered how to run a script and how to use its outputs. Let’s consider the last missing piece: how to provide input parameters? To pass attributes, use the query : The structure gets passed via the standard input as a JSON, which can be parsed with tools such as jq . For example, to pass values to a Webpack build: Of course, outputs of other resources can also be input parameters: But in this case, there is a gotcha. Since Terraform does not know the number of files if it depends on resources, a for_each over the output files will yield an error: When trying to deploy with the command: Will not work: In this case, you can do a 2-step deployment by specifying a -target argument for the terraform command: This deploys everything up to the build so that the number of files is now known for the next terraform apply . Should you use Terraform to run external build scripts? Embedding the build scripts this way has the nice property that after a git clone you can jump straight to deploy , no need to run anything else by hand. It also makes sure that everything that requires building is built before calculating what to deploy, which prevents accidentally overwriting a live resource with older code. Also, as you can pass arguments via the query object that depends on other resources you can bake state-dependent information into the built artifacts . For example, the frontend code knows the backend URL without resorting to fetching a configuration object or passing a query parameter on runtime. The obvious drawback is speed . If you have a build process like this it will be run for every state refresh, making that considerably longer than before. 30-60 seconds of compilation time is not unheard of and waiting an extra minute for every terraform plan can impact developer productivity. Also, 2-step deployment is a pain. On the other hand, that is extra functionality that you wouldn’t have without running the build from Terraform.", "date": "2019-12-10"},
{"website": "Advanced-Web", "title": "How to integrate PlantUML into other software", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-integrate-plantuml-into-other-software/", "abstract": "I like text-based image generation for illustrating a point in an article. And I’ve found that putting images, any type of image, to break the text flow helps a lot with understanding. Even when I see some cat pictures strewn over it helps to avoid the wall-of-text effect. But why stop at unrelated cat pictures? I experimented with draw.io and I found it to be an excellent free software. But the problem with a separate tool is that it segments the workflow of writing. Making an illustration draw.io article Draw.io Draw Export image Article Insert I draw the image, I export it, insert into the article. But what if I see a typo? Fixing a typo draw.io article Draw.io Fix typo Re-export image Article Update image I need to go back to the drawing board, fix the mistake, export it again, then overwrite the original image. It is just a pain to do it, and this is exacerbated by time as I hardly feel like doing this for old articles. Compare that with a typo in the text. I just edit the file and I’m done. No wonder why I see typos persisting inside images eternally but not in the text. Fixing a typo in the text article Fix typo The other issue with draw.io is that it is graphics-based, which means I need to use the mouse which I don’t like. The solution is, of course, a text-based and integrated solution where illustrations are no different than the text content of the article. PlantUML checks the first box, and with a little bit of coding, I could check the other one. The result? I write this: And this appears: me PlantUML Start writing Write text Write a diagram If there is a typo, I can just fix it just like any other part of the article. It will be changed during the next render, automatically, without the need to leave the editor at any point. Let’s see how to write a piece of code that gets the textual representation and outputs an SVG with the illustration! The main problem with PlantUML is that it is a Java application. This is not an issue if you want to integrate it into a Java codebase, but I left that ecosystem a long time ago. Using it from NodeJS or any other non-JVM-based language requires calling the executable file and handle the input/output streams. Installing PlantUML is trivial as it’s in the Ubuntu repository: apt install plantuml . Problems arise when you want to run it as a daemon that keeps listening for diagrams on its standard input and outputs the images on its standard output. The primary mode of operation is to process a directory of inputs and generate PNGs to an output directory then exit. But, well, we are talking about a Java application which is notoriously slow to start. The start -> generate images -> stop mode of operation quickly becomes too slow for practical purposes. A better way is to keep it running and feed it the diagram descriptions when they come. PlantUML comes with a bunch of command-line arguments and a daemon-like operation requires several to be used. Since I prefer vector images to rasterized ones, this is one of the arguments I use every time. This instructs PlantUML to generate SVG files instead of the default PNG. -tsvg PlantUML diagram SVG This starts PlantUML in pipe mode instead of the directory-processing sort. It gets the diagram description on the standard input and outputs the results on the standard output. -pipe PlantUML <Stdin> diagram <Stdout> result With just the -pipe option PlantUML generates an image on stdout if everything goes well, but if not then generates an error message on stderr along with the image on stdout. -pipeNoStderr PlantUML Default Wrong diagram <Stdout> image <Stderr> ERROR -pipeNoStderr Wrong diagram <Stdout> ERROR This is a problem as the invoking code can not be sure when an image is coming on stdout that there will be no error message on stderr following it later. This creates a race condition. With the -pipeNoStderr , the error message will be sent on stdout and no image is generated in an erroneous case. Finally, this argument lets you define a string that will be printed after every image. Without that, there is no way to know for sure when one diagram is processed as the result comes in chunks (remember, we are reading the stdout of an external program). -pipedelimitor PlantUML diagram 1 <Stdout> image 1 <Stdout> delimitor End of diagram 1 diagram 2 <Stdout> image 2 <Stdout> delimitor End of diagram 2 With the -pipedelimitor in place, we just need to generate a sufficiently random value and check for it. This is not for PlantUML but for the JVM. Killing the app will also kill the PlantUML application and Java generates a hs_err file with some details in this case. As a result, it will litter the working directory with these files. Setting the JAVA_TOOL_OPTIONS: -XX:+SuppressFatalErrorMessage parameter instructs the JVM to not write the hs_err files. Using the tools above, a NodeJs implementation for a function that gets the definition and return an SVG (or an error) looks like this: It starts PlantUML only once, and it also utilizes a memory cache to skip regeneration of an already processed diagram. It also serializes the calls so that multiple concurrent invocations won’t be a problem even if PlantUML does not handle this case. It returns a Promise which makes it easy to insert it into an asynchronous workflow. And by rejecting the promise in case of errors it will work just as expected when calling it with await . The downside is that it won’t shut down PlantUML ever, which is fine for a static website generator but might not be ideal for other use cases. The blog you are reading right now is powered by Jekyll, which uses Ruby. The Ruby implementation of the above code: Programmatic image generation is awesome and I hope more people will use them to illustrate the points in blog posts. PlantUML is just one tool for this, and if you know others that work similarly, please leave a comment. I’m looking for more variety when it comes to illustrations.", "date": "2019-12-31"},
{"website": "Advanced-Web", "title": "How to target subscribers in an SNS topic", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-target-subscribers-in-an-sns-topic/", "abstract": "Let’s say you have a monitoring app that pings customers’ pages and sends a notification when there is a problem. In AWS, the service for notifications is SNS, where you create a topic, customers subscribe to it, then you can push messages to the topic. Single topic monitoring SNS Topic Customer 1 Customer 2 message message message But in this case, every customer gets every message , which is not desirable for services with many users. Multiple topics monitoring SNS Topic SNS Topic Customer 1 Customer 2 message message One solution is to create separate topics for each customer but that means you’ll have a ton of topics. An alternative service to use is SES, which is the email-sending platform in AWS. It lets you send individual emails so that you can target the affected customer. But compared to SNS it is a lot more involved to set up. Not only you need to send a support request to be able to send emails but also you need to handle bounces, concurrency, and email errors. Compared to creating a topic and adding the subscribers, it is a lot of extra work. Fortunately, there is a way to target a specific customer in an SNS topic. And it requires just a little bit of initial setup. When you add a subscriber you can also specify a filter policy . This allows you to specify filtering criteria to control which messages the subscriber will get when publishing to the topic. With Terraform, the policy is the filter_policy attribute, which expects a JSON map: monitoring SNS Topic Customer 1 Customer 2 {target: \"customer_1\" } message This policy makes sure that the subscriber only gets the message if the target attribute is either customer_1 or all . The latter is a best practice so that the whole topic can also be targeted. When a second customer comes and you subscribe him to the topic, just give a different id: monitoring SNS Topic Customer 1 Customer 2 {target: \"customer_2\" } message If you add subscribers programmatically, use the Attributes property with the subscribe operation. In the above scenario, sending a message with target: \"customer_1\" sends only to the first customer, target: \"customer_2\" to the second one, and target: \"all\" to both of them. What value to use in the filter policy? My first intuition was to use the subscription ARN so that every single subscription is targetable without any other stored value. Customer 1 monitoring SNS Topic OPS team CTO Slack channel Customer 2 {target: \"customer_1\" } But a better approach is to use a customer identifier that you already use throughout the application. In this case, you keep the door open for having a customer with multiple subscriptions to the same events. For example, an alarm sent to everyone in the ops team. If you already have a list it’s not easy to move subscribers around as they need to reconfirm their subscription. But fortunately, the filter policy can be added for an existing subscription . This means if you want to migrate from a single-customer list to a multiplexed one, you can do so without affecting any third parties. In NodeJS, use the AttributeValue property of the setTopicAttributes call. With the AWS CLI, the same would be the set-subscription-attributes command. Filtering the messages is only one end of the solution, as you need a way to specify who should get the message during publishing a notification . This is done with the MessageAttributes property of publish : The key of the property ( target ) is the key in the filter policy, the data type is String , and the StringValue defines the actual parameter. With the help of the filter policy, you can consolidate all notifications into a single topic and can still target subscribers. This makes it easy to integrate SNS notifications into your app and, for simple cases, makes it possible to avoid all the complexity a proper email solution would entail.", "date": "2019-12-03"},
{"website": "Advanced-Web", "title": "Encryption in the cloud", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/encryption-in-the-cloud/", "abstract": "In many projects, one of the key aspects of perceived data security was whether the data was encrypted or not. I remember a particular case when the management insisted that the data must be encrypted locally before writing to the database but then nobody cared where the key was stored or whether it was protected or not. But as long as the developer could point to the rows and show that they are in an unreadable form, everybody was assured. All while the API was happy to serve everything in plain text without checking the requester. The term “encryption” is usually used with files. In the case of non-encrypted files, everybody who has them can read the information inside. sensitive information file In terms of access control, the only question is whether you have the file or not: have the file? yes no Granted Denied With encryption, you can split the required part into two, the encrypted file and the key, with an added benefit of the latter being significantly smaller than the former. To get access to the information, you need both: sensitive information sensitive information file encrypted file key encryption In terms of access control, you have finer control over who has the information: have the file? yes no have the key? yes no Granted Denied Denied Therefore, not taking account problems with the algorithms, an encrypted file is just as secure as the key itself. If you protect the key you can control access to the information. If you put the key next to the encrypted file, encryption means nothing. In the cloud, you usually don’t get access to the encrypted files themselves. For example, S3 either sends you the object in plain-text or denies the operation if you don’t have access to both the file and the key. Apart from the naming and a promise, there is no observable proof that the file is stored encrypted. Encryption then is not about splitting a file into two, but about splitting the permissions into two. You need access not only to the object but also to the key. access to the object? yes no access to the key? yes no Granted Denied Denied When it comes to encryption inside AWS, KMS is the service to use. It stands for Key Management System and it manages the keys and the policies. Key policies define who can use the key . This is the part of the access control that is added with encryption. In the case of an encrypted S3 object, you use Bucket/IAM policies to grant access to the object, then you use Key/IAM policies to grant access to the key. If a requestor lacks either one, the access is denied. S3 IAM KMS bucket Bucket Policy IAM Policy key Key Policy Since one part of the permissions is detached from the datastore service, KMS opens the way to define permissions across services. If you store data in S3 but archive it to Redshift, you need to make sure the permissions are kept in sync. Someone who has access to one of the services gets access to the data which increases the likelihood of leaks. With KMS you can use the same key for both S3 and Redshift, giving you centralized control over the data access. This would be hard to enforce without key policies. S3 KMS Redshift bucket key Key Policy database Since from a practical point of view, encryption does nothing else but splits permissions into two required parts, it does not make data more secure. It just gives you more control . If you use too broad permissions or you don’t properly check access on your APIs then it does not matter if the data is stored encrypted or plain-text. Instead of asking “Is the data encrypted?” , start asking “Who has access to the data?” . After some thinking, you’ll see that you can make that list shorter by utilizing the encryption infrastructure. That is when your systems will be more secure.", "date": "2019-10-29"},
{"website": "Advanced-Web", "title": "Working with structured data in Java", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/working-with-structured-data-in-java/", "abstract": "Java is great to implement business logic, but it has some weak points when it comes to working with datasets. These tasks are usually associated with having to deal with lots of boilerplate code related to POJOs, and updating each property programmatically. In this post we’ll look how Jackson , Apache Commons Lang , JSON-P and Guava comes to the rescue. All code examples can be found in this GitHub repo . Suppose that there’s a data source providing records in JSON format: Jackson is an actively developed JSON parser and serializer library. It has a data binding package that supports JSON among other formats. We write articles like this regularly. Join our mailing list and let's keep in touch. If the schema of the data is not important, or there’s a high variation of available fields then mapping each of them into a Map<String, Object> might be the simplest way to get started. Jackson can update the values of an object, based on the values in another object or a Map. For example, we might load another record from a different source or create one programmatically and use its values to modify an existing record: The code snippet above updates the original Map with the values of the updates Map. If a value is key is not present in the Map, it will be not modified. Updating entries in a Map is not a big deal, but the updateValue method can also use Java classes to supply the override values. Note: the signature of this method a bit misleading, as it returns the updated object. However, this is simply a reference to the original object, which is also modified . There are many solutions to calculate the difference between two Maps. In this post, I use the Guava library to do that: The following snippet calculates the difference between two Maps: A problem with the previous approach is that it’s hard to work with the mapped data objects. As we use Maps, we are completely bypassing type and schema checking . Also, if we are to do anything with the values, most likely the code has to be polluted with casts and instanceof checks . We can mitigate this problem by reading the value into a Java class: Out of the box it works with Plain Old Java Objects or POJOs , meaning the class we are using has to have setters and getters for each field we want to work with, and also has the default constructor. This behavior can be fine-tuned with configuration to support immutable classes that only have public final fields. An additional benefit of using Java classes is that Jackson will throw an error when it encounters an unknown property . This is a great safeguard against typos and unsanitized data. If needed, this behavior can be turned off: It’s a common use-case to override some properties of an object without affecting others. With Jackson, we can do just that: As a result of this snipped, the name field of the original object will be updated to “John”, without changing its other values. Similar to the Map based example, a call to the updateValue will update the original object. This technique can be also used to copy all non-null fields of an object into another object: A typical pain point with data classes is that you manually have to provide equals and hashCode methods in order to work with conventional equality checks. Moreover, this infrastructure is not enough to get a sense of what is different in case of inequality. DiffBuilder from Apache Commons Lang 3 is designed to solve exactly this problem. The following snippet illustrates how to get the difference of two objects: The library also has the ToStringBuilder , which enables pretty-printing objects without an explicit toString method: As an alternative, Jackson might be used to transform the object to its JSON representation. All techniques presented in this post support flat data structures, but working with hierarchical data can be more challenging. For example, merging nested Maps with Jackson works as expected: But for POJOs, merging only handles shallow data structures by default. To deep merge POJOs the object mapper has to be configured with one of the following: Without this, the whole top-level property will be overwritten, and the unspecified properties will be replaced by null: On top of that, diffing with Maps.difference and DiffBuilder does not detect changes in the nested structures. If one property is different for a nested Map or POJO, the whole field will be marked as different. For example: Notice, that in the previous example the country field is the same in both addresses, adding some noise to the report. These diff results can be improved with some tricks: Alternatively, convert the data to JSON and use JSON-P , which does provide deep comparison. To do that, add the following dependencies: Then, use it as follows: Jackson, Guava and Apache Commons Lang libraries provide handy features to process and update ad-hoc hierarchical data.", "date": "2019-10-08"},
{"website": "Advanced-Web", "title": "How to define Lambda code with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-define-lambda-code-with-terraform/", "abstract": "Terraform’s archive_file is a versatile data source that can be used to create a zip file that can be feed into Lambda. It offers several ways to define the files inside the archive and also several attributes to consume the archive. Its API is relatively simple. You define the data source with the zip type and an output filename that is usually a temporary file: Then you can use the output_path for the file itself and the output_base64sha256 (or one of the similar functions) to detect changes to the contents: Let’s see a few examples of how to define the contents of the archive and how to attach it to a Lambda function! First, let’s see the cases where the archive is attached as a deployment package to the aws_lambda_function ’s filename attribute. These examples add a few files into the archive. These are useful for small functions. archive_file main.js <<EOF ... EOF If you have only a few files to upload and they are small, you can include them directly in the Terraform module. This works by including one or more source blocks and using the heredoc syntax to define the code. This is a convenient way for functions no longer than a couple of lines. Make sure that the output_path is unique inside the module. If multiple archive_file s are writing to the same path they can collide. Terraform interpolations are working inside the template, which uses the same syntax as Javascript template interpolations ( ${...} ). To escape on the Terraform-side, use $${...} . The Lambda function needs the source_code_hash to signal when the zip file is changed . Terraform does not check the contents of the file that’s why a separate argument is needed. You can add multiple files with multiple source blocks. archive_file main.js file() src/main.js When you have longer functions, directly including them in other Terraform codes becomes problematic. If you still only have a few files, you can use the file() function that inserts the contents from the filesystem. This way there is no need to escape ${...} as file() takes care of that automatically You might be tempted to use the templatefile function to provide values, but don’t. Use environment variables for that. archive_file src/ source_dir main.js With a more complex function, you’ll likely reach the limit of listing individual files, especially when you start using NPM and that generates a sizeable node_modules directory. To include a whole directory, use the source_dir argument: This works great for most projects, but the configuration options are very limited. You can not include more than one directory inside an archive, and you also can’t exclude subfolders or files. aws_lambda_function S3 Bucket archive_file s3_bucket s3_key S3 Object source Another possibility is to use an S3 object to store the code instead of attaching it directly to the function. On the Lambda side, you can use the s3_bucket and the s3_key arguments to reference the archive. Notice that you need to add the archive hash in two places : first, Terraform needs to update the S3 object ( etag ), then it needs to update the Lambda ( source_code_hash ). If you omit any of them you’ll see the old code is running after an update. After CloudFormation’s awful package step , Terraform’s archive_file is a blessing. No matter which approach you use, you’ll end up with a speedy update process and, with just a bit of planning, no stalled code.", "date": "2019-10-15"},
{"website": "Advanced-Web", "title": "How to use API Gateway with CloudFront", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-api-gateway-with-cloudfront/", "abstract": "CloudFront is a great tool for bringing all the different parts of your application under one domain. It does it by allowing different origins (backends) to be defined and then path patterns can be defined that routes to different origins. But use it with API Gateway and you’ll see some unique problems. Cache Behaviors Origins /api/* API Gateway domain_name origin_path = '/stage' CloudFront apigw When you have an API Gateway and a CloudFront Distribution, you need to define an origin first: Then a cache behavior that targets that origin under a path: If you then visit the <cloudfront domain>/api/ URL it will be served by the API Gateway. APIs are usually not cacheable so it’s a sensible default to disable proxy caching on the CloudFront side. This can be done by specifying all cache TTLs as 0: The same goes for cookies and query parameters. Usually, you want everything to be available for your API so it’s best to forward everything: invoke_url path domain scheme /<stageName> <restApiId>.execute-api.<region>.amazonaws.com https:// This is where things get interesting. The origin’s domain_name attribute requires only a domain, but the invoke_url defines the whole URL: https://<restApiId>.execute-api.<region>.amazonaws.com/<stageName> . To extract the domain, you can use the replace function with a regular expression: When CloudFront constructs the URL for the backend, you can specify three parts: origin URL path origin_path domain client URL path domain /api/users /stage <restApiId>.execute-api.<region>.amazonaws.com /api/users <distribution>.cloudfront.net CloudFront constructs the URL to the origin by replacing the distribution URL with the domain_name + origin_path , then it appends the path . In the above example if the client opened <distribution>.cloudfront.net/api/users , then the final URL is <restApiId>.execute-api.<region>.amazonaws.com/stage/api/users . Depending on the path pattern and the API Gateway stage name, there are 3 cases. This is when you define the cache behavior as the default. In this case, you need to add the stage name as the origin_path : origin URL path origin_path domain client URL path domain /users /stage <restApiId>.execute-api.<region>.amazonaws.com /users <distribution>.cloudfront.net This is a simple case, as each path will be translated directly to under the stage. In the case of a Lambda function, event.path will be the full path. In this case, the path_pattern is also the API Gateway stage name. This way, there is no need for origin_path as every forwarded path starts with the stage: origin URL path domain client URL path domain /stage/users <restApiId>.execute-api.<region>.amazonaws.com /stage/users <distribution>.cloudfront.net In this case, you need to set the origin_path to the stage name: origin URL path origin_path domain client URL path domain /api/users /stage <restApiId>.execute-api.<region>.amazonaws.com /api/users <distribution>.cloudfront.net This way both the stage name and the path_pattern will be appended to the final URL. As a result, there is no way to remove the /api/ part of the URL sent to the origin without relying on Lambda@Edge. This can be a problem when your backend assumes all requests will come to / instead of /path_pattern/ . Path translation can be a source of confusion when you try to integrate API Gateway with CloudFront. I hope this overview will help you with debugging.", "date": "2019-09-24"},
{"website": "Advanced-Web", "title": "How to use Lambda@Edge with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-lambda-edge-with-terraform/", "abstract": "Lambda@Edge is advertised mainly as a tool that brings processing closer to the users thus increasing speed. But as a developer, I see a different use-case: to influence how CloudFront works. Without functions, CloudFront offers only a handful of configuration options. You can add origins and cache behaviors to set up routing, but you’ll run out of options as soon as you need anything beyond the basics. This is when Lambda comes handy. You can modify the requests and the responses any way you’d like, which opens up ways to fix most of the shortcomings of CloudFront config. There are some differences from a regular Lambda function though, and some new limitations you should know about before you start using edge functions. The usual Lambda resources are needed: an archive_file to hold the code, an aws_iam_role for the execution role, an aws_iam_policy_document for the function’s permissions and an aws_iam_role_policy to wire the last two together. This is what you need for any Lambda function, so let’s concentrate on the differences! archive_file aws_lambda_function 2) publish = true 3) region = us-east-1 aws_iam_policy_document aws_iam_role 1) assume_role_policy aws_iam_role_policy CloudFront The aws_iam_role ’s assume role policy must include both lambda.amazonaws.com and edgelambda.amazonaws.com : Since Lambda@Edge requires a specific version to be referenced, you need to instruct Terraform to publish a new version for every change. To do this, use publish = true : The function must be deployed in the us-east-1 region . If you deploy the whole stack there then it’s not a problem, but it’s better to make sure that is not a requirement. Fortunately, Terraform can deploy resources to multiple regions, which is exactly what we need here. First, define a provider with the us-east-1 region specified: Then make the function use this provider: This makes sure this function goes to us-east-1 no matter where the rest of the resources are deployed. To associate this function with a distribution, add a lambda_function_association to the cache_behavior : The lambda_arn must include the version , that’s why the qualified_arn has to be used here. The event_type must be one of the 4 defined trigger point: viewer-request , origin-request , viewer-response , and origin-response . Browser Browser CloudFront CloudFront Origin Origin viewer-request origin-request origin-response viewer-response In this case, I want to change how CloudFront calls the origin, so I specify the origin-request trigger. Now that we have all the resources in place, let’s see an example! CloudFront appends the full path to the origin request which can be a problem, for example, when your API expects requests starting from the root ( / ) instead of some other path. With a fairly common configuration of an API Gateway with the /api/* pattern, a request to /api/users goes to, well, /api/users . But then you need to make sure your API is able to handle this path and does not expect /users instead. <distribution>.cloudfront.net/ api/users <restApiId>.execute-api.<region>.amazonaws.com/stage /api/users domain_name = ... origin_path = \"/stage It would be better to just strip the /api path from the request sent to the origin. And that’s when Lambda@Edge comes useful. The code is straightforward. The event.Records[0].cf.request.uri contains the path /api/users and we need to strip the /api part from the start: Lambda@Edge <restApiId>.execute-api.<region>.amazonaws.com/stage /api/users <distribution>.cloudfront.net/ api/users <restApiId>.execute-api.<region>.amazonaws.com/stage /users domain_name = ... origin_path = \"/stage\" request.uri.replace(/^\\/api/, \"\") Seems like the pricing is deliberately made to make it hard to compare traditional and Lambda@Edge pricing. The request charges are straightforward: $0.2 vs $0.6. But for duration prices, one is in GB-seconds and 100ms increments the other one is 128MB-seconds and 50ms increments. In GB-seconds it’s $0.0000166667 vs $0.00005001, which is again three times the price. In total, Lambda@Edge is three times as expensive as a normal Lambda . But for simple cases, like transforming a request, the math is a bit different. Lambda@Edge is metered at 50ms increments and if you don’t use any external services then it’s likely you’ll never exceed that. That means for every 1 million requests you’ll pay ~$0.9 extra. When you try to destroy a stack with Lambda@Edge functions, you’ll see this error message: When CloudFront starts using the Lambda function it replicates it to the global network. This happens during that ~20 minutes the distribution is deploying. But when you delete the distribution, while it still takes ~20 minutes, the replicated functions are not deleted but only scheduled for deletion. In effect, Lambda complains about the replicas and refuses to delete the function. You need to wait a few hours (!) after you delete the distribution to delete the function. Keep trying to terraform destroy until you succeed. It’s funny to see that even this was an improvement over how it originally worked. AWS devs did not think people would want to delete Lambda@Edge functions, like, ever. Lambda@Edge gives you the missing piece of CloudFront configurations. Apart from request rewriting, you can add cache or security headers, define more advanced routing policies, optimize content, and a lot more. It is a valuable tool when working with CloudFront.", "date": "2019-10-01"},
{"website": "Advanced-Web", "title": "How to reproduce a Lambda function's environment variables locally", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-reproduce-a-lambda-functions-environment-variables-locally/", "abstract": "Getting started with Lambdas usually involves a lot of trial-and-error which means a lot of redeploys. Change something, deploy, refresh in the browser. While the deployment can be fast, it still takes multiple seconds, not to mention it’s way harder to set breakpoints and see what the code actually does. There are projects to simulate the software environment and the event / context objects, but there are more important environment variables that are missing: the ones you assign to the function. I was looking for a way to run the Lambda code locally with the environment variables set and also with the permissions the live function will get. You can read about such a solution in this post. If you don’t use an emulated environment that sets the event / context arguments then the first thing is to separate the main application code from the Lambda handler so that it can be reused for a local application. Let’s say you have a function that handles GET requests and it uses the request path. main Lambda handler handler() index Node Server createServer() handler handleGET(path) Application code Then the handler.js does all the heavy lifting, but without any Lambda-specific code: The benefit of this separation is that a non-Lambda code can also call this handler. For example, a NodeJs HTTP server can use the application code to produce the same responses as the Lambda. With a simple node index.js , you’ll see the same output. A small part of the code (the main.js ) can only be tested by uploading to the Lambda service, but the majority is separated. With a setup similar to this one it’s easy to develop locally and upload to the real environment only occasionally. A Lambda function is unlikely to exist on itself. Usually, it interacts with other services, like databases such as S3 or DynamoDB, or message queues like SNS and SQS. Their addressing is passed as environment variables. For example, a function might get an S3 bucket’s generated name so that it can be used as a database to read objects. BUCKET=... KEY=... S3 bucket Lambda function S3 Object BUCKET KEY When you deploy this infrastructure, the real Lambda will have access to the object via process.env.BUCKET and process.env.KEY while the local one does not. If you want to write automated tests you can do the same separation between the function and the AWS services as we did with the handler code. But when you just want to get the function out of the door and develop as rapidly as possible, it is not good. But the solution is actually not hard. Terraform keeps track of the environment inside the state so that you just need to extract that and set at the current environment . BUCKET=... KEY=... Local Server Terraform state BUCKET=... KEY=... Copy variables To get the environment variables for a given $FUNCTION , you can use this script: This uses some black magic to process the Terraform state output, but it sets the ENV_VARS variable to a list of export statements: Then all you need is to run these before calling node index.js : An important environment variable not managed by Terraform is the AWS_REGION . This influences how AWS SDK services behave, therefore it is important to mimic the production setting. AWS_REGION=... Local Server Terraform state arn=... Parse ARN The region the function is deployed to is not present in the Terraform state as a separate value. But fortunately, the arn is, and it has a fixed structure: arn:<partition>:<region>:<service>:<region>:<account>:<resource> . By parsing the Lambda function’s ARN we can set the environment variable: But Lambda functions use another crucial set of environment variables: the execution role’s AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and the AWS_SESSION_TOKEN . Unfortunately, these are not available in the Terraform state file, so we need a different solution here. Let’s say the function wants to read the object from the bucket: The function’s role has access to the object, but how to reproduce that locally? The solution is to assume the role the same way the Lambda service would do. In that case, the local server will have credentials with the same policies as the real thing. But the role’s assume role policy only lists the Lambda service: And no matter what permissions you have, AWS will deny the assume-role operation. As a solution, you can temporarily relax the requirements giving access to all users in the account . But it should not be hardcoded as it is a security vulnerability if you forget to remove it. To achieve this I’ve made a small Terraform module that takes care of modifying the assume role policy based on an attribute. policy policy module module assume_role_policy assume_role_policy lambda.amazonaws.com alt [dev_mode = true] lambda.amazonaws.com Account ID [dev_mode = false] lambda.amazonaws.com To use it, declare the module and pass the required arguments: Optionally, you can also pass a policy if you want to customize it. By default it allows lambda.amazonaws.com service to assume the role. If you pass true as the dev_mode argument, you’ll be able to assume the role locally. The last step is to get the role’s ARN from the Terraform state, assume the role, and set the resulting credentials to the environment. AWS_ACCESS_KEY_ID=... AWS_SECRET_ACCESS_KEY=... AWS_SESSION_TOKEN=... Local Server Terraform state role=<arn> AWS dev_mode = true Assume role Credentials The script to do that: Finally, combine the two set of environment variables and start the local server: AWS_REGION=... AWS_ACCESS_KEY_ID=... AWS_SECRET_ACCESS_KEY=... AWS_SESSION_TOKEN=... BUCKET=... KEY=... Local Server Terraform state (1) arn=... (2) role=<arn> (3) BUCKET=... (3) KEY=... AWS dev_mode = true (1) Parse ARN (3) Copy variables (2) Assume role (2) Credentials The complete function is this: And to use it, first pass the Terraform resource’s address followed by the command to run: The environment variables and the roles are all in place locally. The ability to run your Lambda functions locally without deploying it every time there is a change is essential for an efficient workflow. Luckily, with the combination of the Terraform API and the AWS CLI, it is possible to reproduce the essential environment variables that let the locally run function interact with other services the same way the deployed one does.", "date": "2019-10-22"},
{"website": "Advanced-Web", "title": "Magento 2 - Programmatically recreate the missing Catalog Search index table", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/magento-2-programmatically-recreate-the-missing-catalog-search-index-table/", "abstract": "After a Magento upgrade, the mg3_catalogsearch_fulltext_scope1 table disappeared from the MySQL database of my Magento 2-based webshop. As a result, all product-related pages were returning HTTP 500 error code, and I could not create or update products in the admin panel. The original database was broken, so I’ve cloned it and used this script to recreate the missing index . Because I had no clue about the exact schema of the missing database table, I decided to recreate it programmatically using Magento’s built-in indexer service. Also, I could not use any command-line tools to fix the situation, as the webshop is running on a cheap host provider with no SSH access to the machine where Magento or the underlying MySQL database is hosted. We write articles like this regularly. Join our mailing list and let's keep in touch. As a first step, I created a simple PHP script based on Magento’s index.php to have access to Magento’s classes and services. Next, based on this Stack Exchange thread I’ve added a snippet to create the Catalog Search index table using IndexerHandlerFactory. I’ve put this script next to the index.php , into the /home/<username>/public_html directory, hoping that after executing it, I get my indexes back. But then I faced with another issue: the upgrade only did a half-ass job deleting mg3_catalogsearch_fulltext_scope1 . An attempt to recreate the table also failed: Trying to drop the said tablespace via phpMyAdmin also failed: According to Wikipedia , “a tablespace is a storage location where the actual data underlying database objects can be kept” . Based on this Stackoverflow discussion, in case of MySQL they can get “stuck”, and the best chance to remove them is to delete the corresponding file and restart MySQL. As I don’t have SSH access this was not an option. So I decided to leave the original database behind and create a clone with phpMyAdmin , hoping that it copies the schema and the data, but not the faulty tablespaces. After I reconfigured Magento to use the new database and ran the script, it indeed created the missing database table, and the website started to work again. However, I quickly realized that while all products are editable in the admin panel, they are not visible in the webshop UI. No wonder why: although now the index table is in place, it’s empty. So I edited the PHP script to populate the indexes as well: ( source ) For some reason, the Category caches were not refreshed, so I had to save on one of the products in the admin panel to get the menus working again. Finally, I removed my PHP script from the server, and the Webshop lived happily ever after. Despite I had no SSH access I could solve the problem by deploying custom PHP scripts. If you do so, make sure to clean up troubleshooting related scripts at the end. The end.", "date": "2019-09-17"},
{"website": "Advanced-Web", "title": "AWS Config notifications with CloudWatch Events", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-config-notifications-with-cloudwatch-events/", "abstract": "Recently I got a question about how to set up a mechanism to automatically respond to a resource being marked as non-compliant by a Config Rule. AWS recently rolled out a feature for that, but it’s so new at the time of writing even Terraform had no support for it . But this is not a new functionality. You could get notifications when a resource becomes non-compliant without this new remediation feature , you just had to use a different service, namely CloudWatch Events. With that, you can listen to changes and forward the notification to all sorts of other services such as Lambda, SNS, SQS. And with them, you can implement auto-remediation. Targets Resource Resource Config Rule Config Rule CloudWatch Events CloudWatch Events Lambda Lambda SNS SNS SQS SQS Change Run Rule check Non-compliant event Notify invoke notify publish So, you have a Config Rule set up and want to know when it reports something is non-compliant. How to do that? CloudWatch Events publishes an event when a change occurs ( COMPLIANT <=> NON_COMPLIANT ) with all sorts of details. You just need to specify an Event Rule and filter for the Config Rules Compliance Change event and the Config Rule Arn: The whole event object looks like this: The important parts are detail.resourceId and detail.newEvaluationResult.complianceType . The former is the resource, in this case a bucket, the latter is COMPLIANT / NON_COMPLIANT , indicating the result of the validation. The event pattern above matches both the resource becoming compliant and non-compliant. To handle only one direction you can add a filter to the detail.newEvaluationResult.complianceType in the pattern. With CloudWatch events you can set up all kinds of notifications CloudWatch supports. For auto-remediation, you might want to add a Lambda function that does something to rectify the situation. You can set up SNS notifications so that you get an email or a Slack notification. Let’s see how it works in action! For a test setup, I have a simple Config Rule: it checks if a given bucket has SSE (encryption) enabled. This is an AWS-provided rule, so I just need to reference it. Then I’ll use an SQS queue as the event target and a CLI script to see the events as they come. Bucket Encryption: on/off Config Rule Event Rule SQS Queue Checks encryption Publish compliance changes Publish message This part sets up a bucket and a Config Rule scoped to the bucket only. To reproduce this example you’ll need Config enabled for the region for config rules to work. The scope makes sure that only the specified bucket is checked so that I have a controlled test environment. The next thing is to set up the Event Rule. The configRuleARN filter makes sure that only the above rule will trigger a notification. As a target for the events there is an SQS queue (see the linked source for the full example) and a queue policy applied so that CloudWatch can send messages. This part wires them together: Finally, the bucket and the queue are exported to use them in scripts: To monitor the SQS queue for new messages I use the following script: This continually polls the queue and outputs the resource and the compliance status to the console. CLI CLI Bucket Bucket get-bucket-encryption alt [Has encryption] Success COMPLIANT delete-bucket-encryption NON_COMPLIANT [Encryption disabled] Error NON_COMPLIANT put-bucket-encryption COMPLIANT The bucket is NON_COMPLIANT by default (no default encryption set). To change this, I need to set the bucket encryption. And to make it non-compliant again, delete it. The following script toggles the compliance status by querying the current encryption and sets or deletes it: Even though the Config Rule is event-driven, it still takes a few minutes for the message to appear. But then I can observe when the bucket is becoming compliant and non-compliant: The above example uses an SQS queue to print compliance changes of a resource to the console. Instead of just logging, you can define a Lambda function to automatically fix the problem. This opens the way for writing security rules that are guaranteed to be enforced.", "date": "2019-09-05"},
{"website": "Advanced-Web", "title": "How to route to multiple origins with CloudFront", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-route-to-multiple-origins-with-cloudfront/", "abstract": "Recently I got a question about Cloudfront where there were multiple backends and the asker wanted to bring everything under a single distribution. I then realized that AWS’s naming does not help much in this case. The different backends are called origins which is plausible if you consider that is the origin of the data. But then where to set up path-based routing? Exactly, that is (partially) what cache behaviors are good for. With path-based routing, you can cover a lot of different use-cases. One particularly common is to serve static assets from an S3 bucket and use API Gateway for dynamic content under /api/ : Cache Behaviors Origins /api/ /* S3 Bucket API Gateway CloudFront But with a different setup, you bring multiple applications under a single domain. This example routes /app1/ to the first app with its own API and /app2/ to the second one, with a separate API: App 1 Cache Behaviors Origins App 2 Cache Behaviors Origins /app1/api/ /app1/ App 1 API Gateway App 1 Bucket /app2/api/ /app2/ App 2 API Gateway App 2 Bucket CloudFront Or you can host multiple APIs under different paths for a single app: Cache Behaviors Origins /service1_api/ /service2_api/ /* S3 Bucket API Gateway 1 API Gateway 2 CloudFront These are just some examples of how to use CloudFront as an aggregator for all the other pieces of architecture. Since all other pieces (S3 bucket, API Gateways, EC2 instances) have their own URLs, why should you use CloudFront in front of them? After all, deploying it is a nightmare with its 20-minutes-long waiting cycle. But architecturally, it brings a lot of benefits placing a distribution in front of the other resources. It brings everything under a single domain , so you don’t need to worry about CORS errors. It makes your application faster . It can cache static content, it uses HTTP2 , and it can reuse the origin connection , speeding up even non-cacheable requests. It is also cheaper . And finally, the location configuration is at CloudFront , this means you don’t need to tell the clients where to find the APIs. In CloudFront’s terms, you’ll need to define an Origin for each backend you’ll use and a Cache Behavior for each path. This separation helps when you want to define multiple behaviors for a single origin, like caching *.min.js resources longer than other static assets. For this use-case, you define a single origin (for example, an S3 bucket) and define a behavior for minified assets ( *.min.js ) with a cache TTL set to a long time, and a default behavior ( * ) with short TTL. Let’s see an example of how an assets - API separation would look like in Terraform! We have an S3 bucket with the static assets and an API Gateway that serves the dynamic content. The API will be accessible under /api/ and outside that path will be the bucket’s contents. Cache Behaviors Origins 4) /api/ 3) /* 1) S3 Bucket 2) API Gateway CloudFront apigw s3 Origins are the backend configuration for CloudFront, they describe how to grab the content. To wire them to cache behaviors they have an origin_id that acts as an identifier. For an S3 origin, you need the domain name of the bucket and optionally (but recommended) an Origin Access Identity. Two things to notice here. First, the bucket_regional_domain_name ( <bucket>.s3-eu-west-1.amazonaws.com ) is the preferred way over bucket_domain_name ( <bucket>.s3.amazonaws.com ). You might see the latter is also working, but it is not guaranteed for all regions . Also, it takes some time to propagate the DNS so the regional endpoint is simply faster. The second is the Origin Access Identity that grants bucket read access but only if accessed through CloudFront. While it does not seem a big security gain, it’s best practice to lock down non-intended routes. The API gateway is a custom origin, which is any regular HTTP endpoint. As such, it requires some extra configurations: The domain_name is a bit tricky here as the invoke_url contains both the scheme ( https:// ) and the stage name but CloudFront accepts only the domain. The replace function extracts the domain from the URL. Another thing is the custom_origin_config where the above 4 are all required parameters. Even though https-only is specified, you need to specify the HTTP port. You can also specify the origin_path which gets appended to the domain_name . This is useful for API Gateway stage name. In this example, the stage name is api which is the same as the path_pattern . There are two types of cache behaviors: default and ordered. When a request arrives CloudFront tries to match the path to the ordered cache behaviors one by one until a match is found. If none matches, it will use the default. The ordering is important in cases where a given path matches multiple behaviors, like images/* and *.jpg . In this case, it matters which one is the first. The default behavior catches everything, so you don’t need to specify a path_pattern . You can think that it’s a hardcoded * . Let’s set up first the static assets, which will be served as the default behavior. The target_origin_id specifies which Origin to use for this path. Since S3 does not care about cookies or query parameters (except for signed URLs), it is safe to not forward them. This helps with caching. To match paths under /api/ , we need an ordered_cache_behavior : This disables all caching and forwards all cookies and query parameters, just how you’d expect for an API endpoint. The target origin is specified as apigw . Note that the path pattern is /api/* , which does not match /api but matches /api/ . Another important thing is that CloudFront won’t remove the path pattern when forwarding to the origin. For example, if your origin domain is example.com , then with the above path pattern you can only send requests to example.com/api/... but not outside /api/ . You can use Lambda@Edge to modify the URL before sending it to the origin or handle this case on your backend. CloudFront routing allows bringing all the pieces of architecture under one entry point. This simplifies client-side and brings benefits in terms of speed, caching, and price. And after you get familiar with the terminology, it’s a relatively straightforward process.", "date": "2019-09-10"},
{"website": "Advanced-Web", "title": "How to use the AWS SQS CLI to receive messages", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-the-aws-sqs-cli-to-receive-messages/", "abstract": "I started using SQS to see events from CloudWatch as they come. My requirements were straightforward: whenever there is a message, print it to the console. Even an event-driven architecture was not required as I’m perfectly fine with polling during development. Well, I was expecting a simple command to set up the queue then another one to fetch the messages. But, as usual, it’s a bit more complicated than that. Even for this simple task of printing messages to the console as they come, I needed to do some bash scripting. This is an overview of the challenges and lessons I’ve learned during the process. If you’re just starting with SQS it should help you hit the ground running. The first task is to set up a queue and start sending messages. SQS queue operations use the queue URL to identify the queue. This stands in contrast to other AWS services as they usually use the ARN (Amazon Resource Name). To create a new queue and store its URL, use: To delete it: To send a message : To get a message currently in the queue: If you run this code for a few times, you’ll get both messages eventually. Let’s make a polling loop to get the messages every second! The messages are repeating ad infinity. SQS SQS CLI CLI message 1 message 2 message 1 message 2 It turned out that receiving a message does not delete it, and worse, there is no CLI switch to do it automatically. This means you need a separate call to the delete-messages with the ReceiptHandle parameter. If you use FIFO queues you’ll notice a somewhat different behavior but that is also the result of the same mechanism. But first, let’s see how to set up a FIFO queue! To create a FIFO queue you need to change three things compared to a standard queue. First, you need to pass the \"FifoQueue\": \"true\" attribute . Then the name must end with .fifo . The last thing is optional. By specifying the \"ContentBasedDeduplication\": \"true\" attribute then you don’t need to specify the Message Deduplication ID when you send messages. To send messages to a FIFO queue, you need to specify the message-group-id parameter . FIFO ordering is guaranteed only inside the groups, so you can think of it as multiple FIFO inside a single queue. If you don’t want to multiplex multiple streams inside a single queue, you can use a constant value for the message group. Using that means all the messages will be ordered. Now if you poll the queue, you’ll see that only the first message is coming again and again: SQS SQS CLI CLI message 1 message 1 This is because (surprise) FIFO ordering! As long as you don’t delete the earlier message, subsequent ones won’t be delivered. To delete the messages after receiving them, call the delete-message with the ReceiptHandle parameter: But here comes the problem. The receive-message returns both the ReceiptHandle and the Body and you need the former for the delete-message call and the latter as the output. Fortunately, by using a shell variable and a subshell to scope it, it is possible to do both: This command outputs the body of the message while taking care of deleting it from the queue. You can use this script instead of a simple receive-message . CLI CLI SQS SQS receive-message { \"ReceiptHandle\": \"...\", \"Body\": \"...\" } delete-message <ReceiptHandle> Process <Body> First, it gets a message and stores it in the MSG variable: Then it needs to check if the variable is not empty, as the SQS CLI returns no output for no messages and an array otherwise. Then it extracts the ReceiptHandle : Using the ReceiptHandle s, to delete the messages: Then finally extracts the Body from the message and outputs that: To watch for messages, wrap in a while-loop: Using this script, you can see the messages nearly real-time. Especially when working with other AWS services it’s likely that you’ll work predominantly with JSON messages. This will be the Body but in escaped form: If you use jq then you can use the fromjson function to convert it to JSON format: Or you can pipe it to another jq: SQS is an easy tool to see how events are unfolding during development without relying on an email subscription to an SNS topic or a Lambda function and its logs. Once you know the basics, a queue is easy to set up and with some scripting, it can be made terminal-friendly.", "date": "2019-08-20"},
{"website": "Advanced-Web", "title": "How to use unique resource names with Terraform", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-unique-resource-names-with-terraform/", "abstract": "When a name is optional for a resource then, just like CloudFormation, Terraform will generate a random one when you deploy the stack. This makes it easy to deploy these resources and makes sure that you can safely deploy the same module multiple times. For example, you can safely omit the bucket name: The name is different each time the resource is deployed: But several resource types require a name to be specified. And, unfortunately, most examples hardcode a value there making the stack less reusable. In the case of the aws_lambda_function resource type, the function_name is marked as ( Required ) in the documentation. In this case, you must specify a name, and it’s tempting to put a constant string there: But when you deploy the same module again, apply fails: There is a terraform resource called random_id that generates a random value when you first apply the module. Using that, you can generate a different name every time you create a new stack. Using the ${random_id.id.hex} value whenever you need to hardcode a string helps avoid the clashing of names. Deploying the above lambda multiple times is now safe to do: The same applies to the AWS Config Rule . Instead of hardcoding a name, use randomness: In the case of an SQS Queue, the name is optional . However, if you create a FIFO queue then the name must end with .fifo . In this case, the auto-generated name will result in an error. Non-unique naming is a particularly insidious problem that usually manifests itself later in the product lifecycle. Seems like everything is working fine, but then you can’t launch a dev environment in the same AWS account because of this. But with a pinch of randomness, you can avoid running into these problems.", "date": "2019-08-27"},
{"website": "Advanced-Web", "title": "How to clean up Lambda logs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-clean-up-lambda-logs/", "abstract": "An annoying feature of Lambda is that its logs tend to accumulate over time. The messages are pushed to CloudWatch Logs for storage organized in groups, but nothing clears those when the function is gone. In fact, by default, even the retention period is not set meaning all the logs will be kept forever. It does not mean too much cost for development though. A few invocations mean an insignificant amount of storage used. But the increasing amount of log groups make it harder to navigate the CloudWatch Logs console, and also the CLI needs to download more data while paginating through results. It is clutter, which has its costs even if not on storage. In this article, I’ll introduce a simple Node.js-based tool to detect Lambda log groups those functions are already deleted. Fortunately, Lambda uses a predictable naming pattern. This makes it easy to spot which log groups are created for a function and which have a different purpose. This naming pattern means that /aws/lambda/<function name> belongs to <function name> . If <function name> does not exist anymore, that makes the log group a candidate for deletion. We just need to get all the log groups with the /aws/lamba/ prefix along with all the functions and match them. One thing that complicates this process is that Lambda@Edge logs are stored in the region of the request instead of the function. These logs are prefixed with the function’s region, so a function that is deployed to the us-east-1 region (the only one supported at the time of writing) named EdgeTest will log to a group called /aws/lambda/us-east-1.EdgeTest in all regions. In effect, we need to consider all functions in all regions. Log groups have two important attributes. The first is the number of stored bytes which indicates how much storage it consumes. This is an eventually consistent value, and as such, it needs some time to reflect recent changes. The second one is the retention period , defined in days. Any log entry that is older than this is automatically deleted. To get the log groups without a matching Lambda function taking into account both regional and Edge functions, use this script : This generates a JSON structure with the log groups grouped by regions: If you prefer a columnar format that is better suited for CLI tools, use jq to transform it: With a result of region - log group name - stored bytes - retention in days : To get a list of potentially unused log groups that have no retention period set: These should be the ones that are not used but will be stored forever: To set the retention period to 180 days for all of them, pipe the output to the AWS CLI. Remove the | sh part first to validate what it will do before you run it. To get the list of empty (no bytes are stored) log groups: These are without a function logging here and without any log entries. These should be safe to delete. To delete them, pipe the output to the AWS CLI. Again, double check that you really want those groups to be gone by removing the | sh part. If there is a retention period set and the number of stored bytes is 0 that means there were no writes in that period. When there is no matching lambda function to write there, it should be safe to delete. If there are logs messages but no retention period , you should set sensible limits. What is a sensible limit? It depends. For a production function that might be years. For one that is used only during development, a few weeks should be plenty. As a rule of thumb, I believe 180 days should be a good default. A safe process should be as follows: Doing this periodically would guarantee that no logs will be stuck forever after the Lambda function is deleted.", "date": "2019-07-23"},
{"website": "Advanced-Web", "title": "How to paginate the AWS JS SDK using async generators", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-paginate-the-aws-js-sdk-using-async-generators/", "abstract": "The functions of the AWS SDK that return lists are paginated operations. That means that you get a limited number of elements in one call along with a token to get the next batch. This can be a problem if you are not aware of it, as you might get all the elements during development but your function might break in the future. But while getting the first batch is just await lambda.listFunctions().promise() , paginating until all the pages are retrieved requires more work. I was curious about how to make a function that returns all elements irrespective of how many pages it needs to fetch to get them while also keeping the await -able structure. In the case of Lambda functions, the lambda.listFunctions call returns a structure with a list of all your lambdas if you don’t have too many of them (at most 50): To simulate a paginated case, you can set the MaxItems parameter: This time a NextMarker is also returned, indicating there are more items: To get the next batch, provide the last marker: Then do so until no NextMarker is returned. Async generators are a relatively new feature of Javascript. They are like traditional generator functions, but they are async, meaning you can await inside them. To collect all the Lambda functions no matter how many calls are needed, use: The most important thing is to keep track of the NextMarker returned by the last call and use that for making the next one. For the first call, Marker should be undefined , and to differentiate between the first and the last one (the one that returns no NextMarker ), a Symbol is a safe option as it cannot be returned by the API. After the call, we need to yield the functions returned: The yield* makes sure that each element is returned as a separate value by the generator. Finally, a for await..of loop collects the results and returns them as an Array: To use it, just call the function and wait for the resulting Promise to resolve: The same Marker / NextMarker pattern appears throughout the AWS SDK. But unfortunately, the naming is different for different services. For example, getting the CloudWatch Logs log groups you need to provide a nextToken parameter . This makes it impossible to support all the listing functions with a generic wrapper. Luckily, as the pattern is the same, we can make a wrapper function that handles everything but the naming: It follows the same structure as before, but it gets an fn parameter that does the actual API call and returns the list and the marker. To get all the Lambda functions with this wrapper: Adapted to the log groups: Paginated outputs can be problematic if you are not aware of them and even then it’s not straightforward to use them in a simple way. Fortunately, with async generators, you can have the same 1-function-call await -able structure similar to other methods in the SDK.", "date": "2019-07-30"},
{"website": "Advanced-Web", "title": "Adding continuous rendering to the PlantUML server", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/adding-continuous-rendering-to-the-plantuml-server/", "abstract": "As of late, I’ve re-discovered PlantUML and how much easier it is to generate diagrams with text instead of clicking around with the mouse. I haven’t been following its development for quite some years now, and I’m amazed by how many more diagram types it supports. And also there is official support for all kinds of AWS icons which comes handy for communicating architecture. But editing the diagrams is still painfully slow. The online demo server is a form with a submit button so that every time you make an edit you need to click on the button. Which brings the focus away from the editor, so you also need to click back to it: It’s just slow. There is an online service at the bottom of the main page that does auto-regeneration, but that caps the number of images you can generate. And if you start using it, you’ll keep hitting the limit: But hey, this is a client-side problem. And as it builds on HTML and Javascript, it can be modified without touching the server-side. And the best thing is that the same techniques can be used for all sorts of frontend-augmentations. If you find yourself doing the same thing over and over again, there is a good chance that could be automated without touching any of the more complicated parts of the system. You usually don’t even need to check out the source code. Getting notifications when there is a change on the page, hiding and showing elements, sending HTTP requests are all possible and while every frontend is different, these tools can be adapted to most of them. So, my quest was to write a simple script that transforms the edit -> submit -> see workflow to an auto-regenerating one: Since the online service is capped, you need to run your own PlantUML server for this: After this, all you need is a browser: (also on GitHub ) The three main parts are: This one is a bit tricky, as the editor is not a simple <textarea> but a CodeMirror instance. It offers an API but the variable that holds the editor instance is not available. But it does support forms so it must use simple HTML elements. It works by capturing the form’s onsubmit event and then update a hidden <textarea>. The solution is straightforward then: programmatically submit the form then prevent it from sending after the CodeMirror code ran. Submit button Submit button Form Form CodeMirror CodeMirror <textarea> <textarea> click() onsubmit update value preventDefault() prevents submit Translated to code: With the current diagram code, the next step is to emulate the form submit with a fetch then extract the new image URL from the result. Then we need to extract the URL from the response, and finally replace the <img> src. js js Backend Backend HTML POST /form <html> extract <img> src HTML image URL replace <img> src The form is submitted as a POST request to /form with the body containing the diagram code in the form of text=... : The html has a similar structure as the current page, so all we need is a DOMParser to extract the <img> element and its attribute: And finally, update the attribute on the current page: Note : This approach parses the HTML returned by the form instead of constructing the new image src locally as the online demo server does. That approach works also but would require some libraries to include which I didn’t want to do. Luckily, the local server is fast enough so it does not do any noticeable slowdown. The last piece is to detect when the code in the editor is changed and only update the image when it did. It requires three components: 1) Mutation Observer 1) Mutation Observer js js Backend Backend HTML 2) throttle DOMChanged events POST /form <html> 3) check if valid extract <img> src HTML image URL replace <img> src A MutationObserver is a neat and versatile tool that is especially suited for hacks like this. It needs a DOM element, some configuration, and a callback function, and it will call the function whenever the element is changed. It can detect attribute as well as element changes for the whole subtree. But it will call the function for all changes, which can flood the listener. To prevent this, the function should be throttled , meaning it will be called once every X ms. When you write the code it won’t send a POST request for every character, but it will be fast that you still see the latest result. There are many implementations of the throttle method. You can find ready-made implementations in different libraries , but it’s not that long to copy-paste either. I’ve found a simple one here . With the function in place, the only thing left is to rate-limit the MutationObserver callback: The last problem to solve is the asynchronicity of the fetch call. There are no guarantees that network responses come back in the order they are dispatched, and that can result in overwriting with an earlier response. MutationObserver MutationObserver Request 1 Request 2 Request 1 fetch Request 2 fetch return return overwrites Request 2's result To handle this, the easiest solution is to introduce a requestID and check if the latest dispatched ID matches. If the last requestID is different, simply discard the result. MutationObserver MutationObserver Request 1 Request 2 Request 1 last request: 1 fetch Request 2 last request: 2 fetch return last request == 2 return last request !== 1 cancel In code, the easiest requestID is the current time. Speed makes or breaks a tool. If I need to manually do something to see the results and it takes a considerable amount of time then I’m unlikely to use it. PlantUML is just one example that I recently looked into, but many others rely on a web interface. And sometimes all it needs is an hour of hacking to convert something that is nice-but-slow into something that is a pleasure to use. The techniques described in this post are some of the building blocks.", "date": "2019-08-14"},
{"website": "Advanced-Web", "title": "Differences between PUT and POST S3 signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/differences-between-put-and-post-s3-signed-urls/", "abstract": "Both types of signed URLs fulfill the same goal: provide a serverless-friendly and controlled way for users to upload files directly to S3 buckets. The process is also the same for both as the backend needs to sign the request after validating that the user is authorized then the browser sends the file directly to S3. And finally, both can be used from Javascript equally well. These are the similarities, but the technical details are different. And these differences determine the amount of fine-tuning each solution supports. PUT URLs encode everything in the URL itself as there is nothing else communicated back to the client. This means fewer variables can be set, but on the other hand, they are trivial to use from the browser: POST URLs use multiple fields for different kinds of information. The signing algorithm returns a list of fields along with the URL itself and the client must send those to S3 as well. As this means submitting a form, it is a bit more involved on the client-side: For PUT URLs the signing must be done for a specific content type. That means you either hardcode a content type on the backend, for example, application/xml if you want to allow users to upload XML documents, or the client must send the desired content type as part of the signing request. This is usually the case when you want to allow uploading images but don’t want to specify the exact format (png, jpg, etc.). For POST URLs the policy supports a prefix constraint as well as an exact match. For the previous case, you can enforce a prefix and the client can upload any content type that matches that: In case of PUT URLs , you have no control over the size of the uploaded file. For POST URLs you can set an allowed range in the policy: If you don’t want to use Javascript you should use POST URLs. You can set the redirect URL so that clients will be automatically redirected after uploading the file. If both are for the same purpose, which one to choose? Use POST URLs . While PUT URLs are simpler to use, they lack some features POST URLs provide. And since constructing a form and submitting from Javascript is just a few lines of code, it shouldn’t be a problem.", "date": "2019-07-17"},
{"website": "Advanced-Web", "title": "How to use S3 POST signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-s3-post-signed-urls/", "abstract": "POST signed URLs enable the same use case as PUT URLs: when you want a scalable and controlled way of letting users upload files directly to S3. The main difference is the technical implementation of how these URLs work. While PUT URLs provide a destination to upload files without any other required parts, POST URLs are made for forms that can send multiple fields. But that does not mean POST URLs have to be used in HTML <form> s. With modern Javascript APIs, it is possible to completely replace PUT signed URLs with POST ones without any change in functionality. And POST URLs provide greater configurability. The process consists of 2 steps, first the backend signs a URL and sends it to the client then the client uploads directly to S3. Server Server Client Client S3 S3 Sign URL 1 Sign request 2 Signed URL Upload Object 3 POST Object In the case of the JS SDK, the function that returns the signed URL is s3.createPresignedPost . It returns an object with a url property and a fields object with the required form fields: When used with Express, this code generates and returns a signed URL the clients can use to upload files: No surprise here, this is the bucket the file will be uploaded to. Make sure to give the necessary permissions to put objects into this bucket to the backend. And since browsers are subject to CORS, you need to set a CORS policy for the bucket: The name of the object. As with all upload URLs, make it random and do not let the client to influence it: Unlike for PUT URLs, you can set the acceptable content length. The range is in bytes, so if for example you want to only allow files less than 1 megabytes, use this: The content type of the uploaded file. It can be set to a fixed value but also to a prefix: Prefixing is especially important when the clients can upload different types of the same category, like any image. You can also require a given key-value metadata to be set for the object. These can be arbitrary values, for example you can associate a user id with each uploaded file using it. If you require a specific value to be present then add that to the field object so that the clients know what to set. The metadata can be queried for the key: Since signed POSTs are mainly for forms it’s a bit more involved to upload a file purely from Javascript but nothing difficult. First, get the signed URL and the associated fields from the backend: Then construct the FormData: The Content-Type must be set if the POST policy contains a Condition for it. One possible pitfall is that the file must be the last element. The error message says nothing about the real problem when things are after that. Finally, send the POST to the signed URL: POST URLs are direct replacements to PUT URLs and they remedy their shortcomings. The backend can put restrictions on the content length as well as it handles content types better. Also, it supports proper metadata which is something you couldn’t achieve with PUT URLs.", "date": "2019-07-02"},
{"website": "Advanced-Web", "title": "How to use S3 PUT signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-s3-put-signed-urls/", "abstract": "For traditional applications when a client needs to upload a file it has to go through your backend. During the migration to the cloud, that usually means your backend uploads the same data to S3, doubling the bandwidth requirements. Signed upload URLs solve this problem. The clients upload data directly to S3 in a controlled and secure way, relieving your backend. All upload URLs work in 2 steps: Server Server Client Client S3 S3 Sign URL 1 Sign request 2 Signed URL Upload Object 3 PUT Object This way the backend has control who can upload what but it does not need to handle the data itself. Using the JS SDK and an Express server, a PUT URL signing looks like this, validating the request first and then calling the getSignedUrl function: As with other forms of signed URLs, signing means the signer does the operation. This means that the backend needs access to perform the operation. Also, this operation is entirely local . There is no call to AWS to generate the signed URL and there are no checks whether the keys used for signing grant access to the operation. The bucket to upload to. Don’t forget to add a policy that grants upload access to the bucket. As the client will send requests to the bucket endpoint it needs CORS (Cross-Origin Resource Sharing) set up: Without it, the client will not be able to issue a PUT to the bucket. But that is only the case for browsers, as other clients don’t enforce the same origin policy (like curl). This is the object name that will be uploaded. Do not let the client influence it . Generate a sufficiently random string and use that for the key. This sets the Content-Type of the uploaded object. There are 2 scenarios here. The first is when you fix the content type, for example to image/jpeg : But what if the client wants to upload a png? In that case, you need to get the intended content type from the client during signing and validate it: Do not allow the client to fully specify the content type . Either make it constant or validate it before signing the request. Unfortunately, there is no way to put limits on the file size with PUT URLs. There is a Content-Length setting, but the JS SDK does not support it. This opens up an attack where a user requests a signed URL and then uploads huge files. Since you’ll pay for both the network costs as well as the storage, this can drain your budget. The only thing you can do is track who uploaded what (for example, by including the user id in the key) so that at least you can stop it. You can add arbitrary key-value pairs to the object which will be stored in the object metadata. This is a great place to keep track of who uploaded the object for later investigation. The server must check if a user is allowed to upload files . Since with the possession of a signed URL anyone can upload files, there is no other check performed on the AWS side. Do not let anonymous users get signed URLs. On the frontend, just fetch the signed URL and issue a PUT request with the file you want to upload: As with all features, the fewer things clients can set the better. The Key should be set by the server with sufficient randomness. That prevents enumeration attacks in case you don’t protect downloading files by other means. The content type should be locked down as much as possible. For an endpoint where users can upload avatars, image/* is plenty. Also, make sure you can track each object back to who uploaded it . In the case of PUT URLs, it usually means using the filename. Signed upload URLs are a cloud-native way of handling file uploads that can scale indefinitely. But you always need to check the parameters before you do the signing as that can easily lead to security problems.", "date": "2019-06-25"},
{"website": "Advanced-Web", "title": "Limit permissions with roles for signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/limit-permissions-with-roles-for-signed-urls/", "abstract": "When you sign a URL with the global credentials (execution role for Lambda/ECS, instance profile for EC2, or IAM users) you get all the available permissions that come with it. If a user can trick your backend to sign a URL for a different bucket, for example, that could lead to security vulnerabilities. Using different sets of permissions for different purposes can help enforce the least privilege. What are the use cases? You have 2 buckets with 2 endpoints to sign URLs for them: In this case, you want to make sure that clients cannot trick one of your endpoints to sign a request for the other bucket . If you use the global credentials then both endpoints have access to both buckets and only the checks in the code enforce they only grant access to the correct bucket. If you use separate roles that have access to only one of the buckets your endpoints will be more secure. Another example is when you want to make sure a URL can only be signed with a given prefix , such as for objects starting with user/ . The global credentials have access to the whole bucket, but using a role makes sure that no matter how insecure your signer function is it cannot get objects outside the prefix. And finally, you can lock down a bucket to a specific role which means it’s easier to write and reason about the bucket policy. When you don’t specify any credentials the AWS SDK uses the global ones. This is usually how you get the S3 service: Later, when you sign the URL, the global credentials will be used: Thus it will have access to everything the global credentials can access. To use different credentials for a given service use the {credentials: ...} parameter in the constructor: When you use this object to sign URLs it will use the provided credentials . Roles are temporary credentials so you need to refresh them every now and then. Fortunately, there is AWS.ChainableTemporaryCredentials which does that for you transparently: This uses the global credentials to assume a role identified by roleArn then use that to sign URLs. Assuming a role is an asynchronous operation, which means the credentials it provides are not available immediately . AWS SDK operations can usually be called both async or sync. For example, getSignedUrl supports both: If you use the sync version you need to make sure the credentials provider has a chance to make the calls it needs to get valid credentials . This can be done with getPromise() . Wait for the resulting promise to resolve and then you can safely use the sync version: Another problem is the expiration. Signed URLs expire in two ways : First, the signature can expire. When you sign a URL, you specify how long it will be valid: The default is 15 minutes if you omit it. This expiration time is an observable property of the URL as it is included in the query part: &Expires=... . And when it expires it gives back an AccessDenied error: The second way URLs expire is when the signer loses access to the operation . In the case of roles it is when the credentials expire. It defaults to 1 hour. When the credentials expire it gives back a different error: Since access keys for IAM users do not expire you might not even notice there is a problem during development. The two ways of expirations both determine how long the URLs are valid. If the signature is valid for 15 minutes but the session has only 5 minutes remaining the effective duration is only 5 minutes. This is usually a problem for URLs signed close to the end of the session which is one of the worst kind of bugs to debug. Fortunately, there is a parameter that controls the role refresh behavior. It is called expiryWindow and it defines how many seconds before expiration the role should be refreshed . In effect, the credentials will be valid for at least this many seconds. Setting this property makes sure the credentials will expire no sooner than the signature on the URL . Using the least privilege is a good security practice, especially for endpoints that clients can influence. Using separate roles for these specific tasks can prevent problems if the endpoint is compromised.", "date": "2019-06-18"},
{"website": "Advanced-Web", "title": "How to get near-realtime logs for a Terraform-managed Lambda function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-get-near-realtime-logs-for-a-terraform-managed-lambda-function/", "abstract": "There is a recurring thing that happens nearly every time I work on a serverless function behind an API Gateway. I deploy the function, open a browser and navigate to the URL, and finally see an Internal server error . Bummer, something is broken. But what? Let’s check the logs! But to do that, I need to open CloudWatch logs, find the function, then find the appropriate log stream, and finally, I’ll see the logs. And when I deploy a new version with a potential fix, I need to do it over again as the stream name is changed. This kills development time as well as my mood, especially when I have multiple managed functions. Fortunately, all the above can be automated with the AWS CLI making the process nearly seamless. As a working environment, I’ll use Terraform to manage the resources with a function definition as follows: The name of the lambda resource is aws_lambda_function.function . So to get the logs for this particular function, use this one-liner: This prints out the latest log messages for the function. No more opening the browser to see what’s wrong: Getting the logs is a great start, but during development you’ll run this command a lot of times. To make it more convenient, it can be run in the background, constantly polling the latest logs. Make a file called logs.sh with almost the same the content: Notice the last line: get_logs \"$@\" . This passes all parameters to the function so that it can be used by other programs, in this case, watch . And call it with watch : This makes sure that if the log stream is changed, for example when you deploy a new version, it will use the latest one. You can run this in the background and see the latest logs almost instantly. The first step is to extract the name of the function from the Terraform state: The terraform state show <resource> shows all the attributes for the function. sed removes coloring characters, then awk then finds the appropriate value and outputs it. This then gets stored in the FUNCTIONNAME variable. Since we know that the log group name is /aws/lambda/$FUNCTIONNAME , we only need to find the log stream that is currently in use. Different versions of the same function write to different streams, so we need to find all of them and get the one with the last write. Gives back an array of logStreams under the group: With several streams, we need to sort by the lastEventTimestamp and get the name with the latest timestamp: Having both the log group and the log stream we can get the log events: This outputs the last 100 log events (set a different limit if you want more/less). Without a limit, it can download up to 1 MB of logs and with watch that can consume quite a large portion of bandwidth. The gsub(...) is used to remove the trailing newlines from the messages. Both Terraform and AWS provide the tools to write short CLI solutions for recurring problems. With a simple script like this one, you can more easily debug the problems with lambdas, making the whole process a lot less annoying.", "date": "2019-06-11"},
{"website": "Advanced-Web", "title": "Why AWS access and secret keys should not be in the codebase", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/why-aws-access-and-secret-keys-should-not-be-in-the-codebase/", "abstract": "I’ve seen a few applications with hardcoded AWS credentials. Even though it is something that should not happen, I can see why this pattern emerges from time to time. When there is nobody with adequate experience with the cloud it is the easiest and the “just works” solution that ticks all checkboxes. And not until much later, just before the product goes to production will it surface (well, I’ve seen teams where it wouldn’t have been a problem even then). After all, the easiest thing to start moving to the cloud is for an admin to create an IAM user, assign Administrator privileges, generate an access key, and finally insert this small snippet of code so that everybody can start working: Every project is running late, and trivial security problems are easier to blame on unknown “hackers” than a missed deadline. Don’t succumb to this. At the very least, delete that line and use environment variables for this: As simple as that, this actually remedies most of the problems hardcoding keys brings. First, by hardcoding your keys into the codebase you’ll commit them to version control . This is a red flag and an indication of poor handling of secrets. On its own, it does not immediately pose a security risk but makes another one far-far more impactful. Accidentally exposing git history leading to production account compromise is not entirely unheard of. Second, hardcoded keys get precedence over those set by other means, making your application unable to adapt to the environment . This is the problem people start noticing when they prepare a production environment and find out that they can not configure separate permissions. The worse part is when they don’t bother with this and use the same keys for all workloads. And lastly, as you can only hardcode permanent credentials that means you can not take advantage of the short-lived tokens most services support. Lambda will use a key that expires in a short time, as well as EC2 and ECS. Exposing these tokens brings a lot smaller problem than exposing a long-lived one. In short: the environment . Environment variables are precisely for this scenario, and they can be different for production/staging/development. Running your code on a server, possibly serving your clients? Depending on where you put your code, there are a few scenarios. You might be tempted to use an environment parser and set the keys from there: The AWS SDK handles it internally and manually setting this makes it harder to use other means, like the credentials file. For short-lived temporary credentials, there is a third variable you need to set: the AWS_SESSION_TOKEN . It is easy to forget this as it is not present for IAM users, but required when a role is involved, for example, in a Lambda function. If your code is running on Lambda/EC2/ECS or another service that supports execution roles, you should use that. Just create a role with the necessary permissions and assign it to your function . Nothing else is needed, the runtime will make sure everything is set up for you. If your code is running outside AWS then you can’t take advantage of the execution role functionality. In this case, set these variables in the environment : The AWS_SESSION_TOKEN is present when you are using temporary credentials. The easiest way is to run aws configure and set the keys there. Every invocation will use these credentials, even for the CLI. Running this command writes the .aws/credentials file: If you need more than one set of keys, add named profiles : To specify the active one set the AWS_PROFILE variable before running the app: If you do not want to use the credentials file, you can also set the variables directly. Set these parameters: And run your app using them: Note that the values are likely to be visible in the shell history, so don’t assume they are kept secret. Apart from the credentials, hardcoding a region can also lead to problems and it’s also unnecessary. Services running on AWS use the same region they are deployed to. Explicitly specifying the region makes your code less portable. To set the region, surprise!, use environment variables : There is another variable called AWS_DEFAULT_REGION , which the CLI is using. This can lead to problems as there is no indication which is using which, so it’s safest to set both: You can also specify the region in the credentials file, even for profiles: Access and secret keys, as well as the region, should not be hardcoded. It is hardly ever justified, it is against the official recommendations, and can lead to problems and vulnerabilities later down the road. Just remove all the AWS.config.credentials from your codebase and run aws configure to set up a local development environment. This simple practice makes sure you won’t have problems when you go into production as you can have separate comfigurations for different environments.", "date": "2019-05-28"},
{"website": "Advanced-Web", "title": "Cacheable S3 signed URLs", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/cacheable-s3-signed-urls/", "abstract": "Let’s say you have an image on a customers-only corner of your website. To ease the bandwidth requirements on your servers, you decided to move the image to S3 and use signed URLs to selectively grant access to it. This can be a small thing like an avatar, or a bigger one, for example, some promotional content. This works great, the users see the images without downloading them from your servers directly. But when you test your page and open the Network tab you see that the same image is downloaded over and over again when navigating between the pages: This wastes bandwidth on the clients’ side and money on yours. What is going on here? The signature on the signed URL is time-based . The first image the user downloads has the URL like this: But when he comes back later the URL changes: As you can see the Expires parameter is different and with it the Signature and the security token. And because the URL is different, the browser treats them as two separate images. When you sign an URL, the AWS SDK gets the current time and calculates the expiration based on that. And since it has a precision of one second the user will get different URLs every time: This is not a problem during one-time downloading of large files, the primary use-case of signed URLs, but when the same user might repeatedly request the same file it can be a problem. Let’s see what we can do to remedy this! The idea is to round the time the URLs are signed , effectively traveling back in time , so that ones created close together will be exactly the same. As an illustration, if the backend signs the URL at 11:05 it behaves as if it signed it at 11:00. If the client comes back and requests the image at 11:07, round that back to 11:00 so that the two URLs will match exactly. And when the same client comes back at 11:35, he’ll get a different URL: With this technique, the browser does not need to download the files every time from S3 but use the cached version: This obviously changes the real expiration time of a signed URL. A URL signed at 11:05 dated back to 11:00 will be valid for 5 minutes less than one that was signed at 11:00. If you keep the default expiration of 15 minutes for S3 URLs and round to the last 10-minute mark, the effective validity will fluctuate between 5-15 minutes. By default, creating a signed URL using the JS SDK is by calling the s3.getSignedUrl function: To round the time down to the last 10-minute mark, you need to make the AWS SDK think that the current time is an earlier instant. To achieve this, timekeeper provides a nice API: Fortunately, the s3.getSignedUrl is synchronous, so freezing the time does not interfere with other time-dependent parts of the codebase. But keep in mind not to use the callback or the Promise version of the function as those do not come with the same guarantees. Make sure that the expiration time is greater than the rounding . If, for example, the validity of the signature is 15 minutes but you round down to the last 30 minutes, an URL signed at 11:17 is already expired: The default signature validity is 15 minutes making rounding to the last 10-minute mark a good option. Don’t use the same value for both, as that would create a race condition if the signature is done close to the end of the period. The effective validity is [(expiration - rounding), expiration] , for example for the scenario above (the signature is valid for 15 minutes, rounding is to 10-minute marks), the effective interval is 5-15 minutes. If the default seems too low, just increase the two values. An expiration time of 7 hours and rounding to 6 hours gives plenty of time for the browser to use the cached version. When you use signed URLs in a way that a single user might download the file multiple times, make sure you use cache-friendly signatures. With a little bit of planning, they are easy to set up, and they play well with CloudFront caches also, making the user experience even better.", "date": "2019-06-04"},
{"website": "Advanced-Web", "title": "Why a multi-account setup is essential for secure systems", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/why-a-multi-account-setup-is-essential-for-secure-systems/", "abstract": "All too often I see companies having opened an AWS account jump right in and start deploying systems. This is fine when the project is in the development phase without production resources like databases filled with customer data or third-party integrations. But time goes on and at one point the system under development goes into production and while everybody is happy that it finally happened – and quite possibly the easing of the time pressure – few think about how to it will be secure in the long run. More often than not, this single account will be used for continuing development next to the production resources. And the problem is that this setup can not be secure. If you use only a single account that usually means you have the production resources along with the development ones, especially the developer users themselves. This scenario is the default as you get one account by default but you need to take action to get another one. The main question here is whether developers have access to the production resources . The executive thinking is that developers might need something from there, for example to reproduce a bug, so give them all the access they might require. But if developers have access to production resources then you put immense trust in them. And it is a bad situation for everyone. Development is about experimentation. A new database might be needed to spin up or torn down. Or a process may go wrong eventually and consumes more resources than it is supposed to. But if you put a developer into a place where he can accidentally wreak havoc to the business that would limit these experimentations. On the other hand, do you really want to give all your secrets and data to every new employee, even if they are just onboarding? You wouldn’t give your confidential documents to them, so why treat customer data any different? Not to mention data privacy concerns. If your developers don’t have access to the production resources , that means you utilize some sort of access control to lock down your precious data. That’s a good start, but unfortunately it is practically impossible to do so in a way that still allows free experimentation. Let’s see an example! Let’s say your production environment uses a Lambda function to access some data, for example to read something from an S3 bucket. To allow it to do so you define an execution role with the necessary permissions and assign it the function. But what prevents the developer to deploy a different Lambda function that uses the same role? Well, you might limit the developer account so that it can not pass that role ( iam:PassRole permissions). This is a good second step, but what prevents the same developer to create a role that does have the permission to associate a Lambda function with any role? If you limit this than you limit the developer account in a way that interferes with the experiments. As an alternative, you can use Permission Boundaries, but now you’re entering an intricately complicated territory. In short, it is insanely hard to find all cracks in permissions . If that same production Lambda connects to a database and the credentials can be extracted from the code (sloppy dev practice) then even read-only access is an information leak. There is also the notion that a user has access to everything that the services he can access can access . If you have no access to a bucket but you can SSH into an instance that has, then in effect you have access to the bucket. Mapping all these transitive permissions is also extremely hard and they change constantly. Also, don’t rely on CloudTrail as an effective deterrent. For a competent attacker, it’s all too easy to plausibly deny any wrongdoing and blame an unknown third-party. You might see that your developer is the culprit, but if he contends that his access keys were compromised you can not really do anything about it. For the operations team, it’s an even more problematic setup. Ops require guarantees and how can you guarantee that a resource is protected and is only accessible in some predefined ways and by a predefined set of people? Since it is nearly impossible to track down effective permissions for people who need to experiment with a wide range of resource types (see above), it is similarly nearly impossible to make invariant assertions about the production environment. A multi-account setup solves all the above problems. Developers are free to experiment in their own sandboxed accounts with a well-defined blast radius. Granted, they can still start costly resources and keep them running, but that is nowhere near what a lost database would cost. Ops can have guarantees , as only a fraction of the employees (even from their own ranks) can have any access to the production account. And since they don’t need to be able to experiment they can be locked down to specific workflows. No IAM access for most of the team simplifies the permissions a lot. There is a scenario where privilege escalation can happen even for a multi-account setup. If the codebase (or part of it) is automatically deployed from a repository, then as per transitive permissions, anybody who has push access will have access to the production resources . This can compromise security and break the access control guarantees that would otherwise be present. While systems tend to evolve naturally to a single-account setup, it is a flawed organization of resources. Make some plans during the go-live event how the continued development will take place and how to keep it secure.", "date": "2019-05-14"},
{"website": "Advanced-Web", "title": "Taking notes on a conference with smartphone and Bluetooth keyboard", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/taking-notes-on-a-conference-with-smartphone-and-bluetooth-keyboard/", "abstract": "This month I’ve attended the Craft Conference. Last time I used my laptop to take notes, but this time, I tried a new setup to save some space in my backpack: an Android smartphone with a Bluetooth keyboard. There are many apps available to take notes in the Android ecosystem. I chose Google Documents because it works offline and synchronizes content with my Drive account which I already have. On top of that it has two ergonomic features that I really like: However, using the virtual keyboard can be a bottleneck in typing regardless of the chosen app. At some sessions, I only took a few quick notes in which case it’s okay. But in many cases, I wanted to quickly write anything that comes to mind. We write articles like this regularly. Join our mailing list and let's keep in touch. For this reason, I bought a Bluetooth keyboard . There are many models to choose from. I recommend checking the reviews in case you are planning to buy one to see which one fits your needs. I have an Ewent EW3162 which is lightweight and has a similar size what a 13.3” laptop’s keyboard has. I quickly tried it before the conference: I could easily pair it with my phone, and taking notes on it is just amazing compared to the virtual keyboard. Yay! All I had to wait a few days to try it on the conference… where I faced a few surprises. Screen lock is great for typical usage. After all, it conserves energy. However, when you use a physical keyboard it’s quite inconvenient to unlock the phone when it’s not in your hands. There are a couple alternatives to deal with this. Your best bet is to use the Smart Lock functionality to suppress locking when the Bluetooth keyboard is connected. It allows the screen to turn off after a while, but the device can be unlocked by hitting space on the keyboard. You can also unlock it by swiping on the screen, there’s no need for patterns and the usual mechanisms. On downside is that after unlocking the device, the app might start in portrait mode , and based on its current orientation, it might turn the screen into landscape mode. I prefer landscape mode while typing, and put my phone on the desk, or something almost completely horizontal surface. Because of this, when I unlock the phone, the app shows up 90 degrees rotated, in portrait mode. Unfortunately, I did not find a built-in way to force landscape layout on my phone, so I’ve installed the Rotation Control app from the store, which allows exactly this. As an alternative to all of this, you might temporarily disable screen lock in the Settings , or install one of the many No Screen Off apps from store which allows toggling screen locking. This is potentially less secure than the Smart Lock : if you lose your phone unlocked, others might easily access your data. Anyway, if you are to use it, make sure to re-enable the screen lock afterward. By default, the virtual keyboard is active even if you connect a physical one. Being visible is not more than a small inconvenience, because the virtual keyboard fades out once you start typing. However, it also gives the same autocorrect functionality, which does more harm than good. Many times I experienced that it makes the whole system slower, and if I typed too fast sometimes it inserted the same word multiple times. Also, while my big fingers make me clumsy at the virtual keyboard, I am much more accurate at a physical one, so usually all the autocorrect could do for me is to occasionally replace some valid words with “better” alternatives. Digging on the Internet I found that most virtual keyboards come with a setting where one can disable this behavior. I have SwiftKey installed on my phone, but I had no luck with it: I could disable auto correct for the virtual keyboard part, but it still kicked in when I used the physical keyboard. Because I am otherwise satisfied with SwiftKey , I decided to try an alternative virtual keyboard I can switch to when I use the physical keyboard. So, I installed the Null Keyboard from the Android Store. With multiple keyboards installed, you can select which one to use once you focus on a text input. From version 7.0 Android can display more than one app at the same time. I rarely use this feature because the virtual keyboard requires a great portion of the screen, hiding both windows. However, with a physical keyboard this feature can be easily used for some quick googling: Compared to a laptop, this setup is not only smaller, but also has less power consumption thus lasts longer. While my laptop usually barely stands an 8-hour conference, I still had my phone above 40% power level after a whole day. The keyboard is powered by AAA batteries that can supply multiple days of typing, but I bring an additional pack of them along with a phone charging cable and a powerbank just to be on the safe side. To make it more comfortable, I have a plastic clipboard which serves as a stand for the phone and the keyboard. The clip at the top of the clipboard serves as a small pad that slightly rotates the display in my direction, making it easier to read. (Of course, buying one of the phone cases that is big enough to hold a keyboard would also be an option.) I recommend trying this setup to make the most ouf of your next conf.", "date": "2019-05-21"},
{"website": "Advanced-Web", "title": "CloudFormation CLI workflows", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/cloudformation-cli-workflows/", "abstract": "Despite my ambivalent feeling about CloudFormation I use it a lot, but managing stacks through the Console is a pain. Fortunately, this service enjoys the same CLI support most other ones do, so it is just a matter of scripting to make it more developer-friendly. There are several projects that provide a better experience from the terminal, but for simple needs I prefer simple tools. Here you can find a couple of what I use regularly for various common tasks. Almost all the scripts below use jq , which you can usually install with a simple apt install -y jq , along with the AWS CLI and some basic tools available on most distributions. Note: In all the examples I use $STACK_NAME variable as the current stack name. You can set it by defining it as follows: Since CloudFormation is region-specific, each region provides an isolated environment. To change the region , either specify it for the aws command: or define the AWS_DEFAULT_REGION environment variable: The most used use-case is to deploy a template file to a new stack or update an existing one. You can use the deploy command for these tasks: Here’s how it looks like: It needs these parameters at the minimum: In this case, I defined cloudformation.yml as the template and $STACK_NAME as the name of the new stack. You may notice the --capabilities argument of the above command. This is used to explicitly acknowledge that the stack creates IAM resources. There are 2 possible values: CAPABILITY_IAM and CAPABILITY_NAMED_IAM . The latter is required when you define a name for an IAM resource, for example, the RoleName : When a stack requires parameters, you can supply them like this: Under the hood, the deploy command does 2 things: it creates a changeset that describes what to do, then executes it. To skip the second one, use this flag: Instead of a full deploy cycle, it stops when the changeset is created: After the changeset is ready, view it with the command: It gives back a lot of information, and usually, it’s enough to see what resources are changed. To filter for this, define a query : It gives back a smaller JSON describing how each resource will be affected when the changeset is executed: It is a good practice to review what will be changed before actually executing them, especially when updating a critical stack. Let’s construct such script! It needs to perform 4 steps: The 2 missing steps are (2) and (4). To get the last changeset’s Arn for a given stack: And to delete a changeset: With the above building blocks, this rather complex script can check what a template would change: It looks like this in action: To get the events related to a given stack, use this command: It gives back a large JSON, detailing what happened to each resource: This contains all the informations, but it is hard to get an overview what happened to what. To extract the valuable data and format it to a columnar format, use this snippet: This shows a table with less information but that is easier to get a glimpse of the events: Note: timezone is not supported by jq at the moment, so it shows the time in UTC. To see how things unfold near real-time, use watch : When I deploy something using the AWS Console, I just press refresh every other second to see how the resources are appearing at my behest. The beauty of the CLI is that I don’t need to manually click. To deploy a stack and get the events refreshed automatically, use: This is how it looks like in action: Isn’t that awesome? To get a list of the stacks in the current region, use: Deleted stacks are shown for some time so we need to filter them out. As usual, this can be watched, which can be useful when you deploy/delete multiple stacks: Stack deletion is easy with this script: Combined with watching, it looks like this in action: To list the resources managed in a stack, use: This prints in a columnar format, extracted from the JSON the AWS CLI returns: As usual, it can be watched: To watch the resources during a deploy, you can combine the above script with the deployment: With this you can see how the deployment unfolds: When you want to use the outputs of a stack, the AWS CLI returns them as an array of objects. To extract an output based on its name, use jq’s select : This returns the value of the parameter:", "date": "2019-05-07"},
{"website": "Advanced-Web", "title": "How Service Control Policies help to secure AWS accounts", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-service-control-policies-help-to-secure-aws-accounts/", "abstract": "The recommended architecture is to have resources only in member accounts (those created via Organizations) and not in the master (the one you’ve just opened). Apart from the easier separation of resources, this setup also allows the use of Service Control Policies (SCPs for short) to limit what each account can do. Along with identity-based and resource-based policies, SCPs are policy types that can limit permissions. But they control the account as a whole , instead of an identity or a resource. SCPs allow security controls not achievable with the other policy types. But SCPs do not affect the master account . So the first thing you should do right after opening an AWS account and logging in the first time is to go to Organizations, enable it, click on the link in the email, then create a new member account. You need to input a new email address, but you can use the + alias if your email provider supports it as Gmail does . And finally, use the role to get into the newly created account and create a new user with Administrator privileges. The problem with postponing creating and using a member account is that it is unlikely you’ll do it later. When you have resources like elastic IPs along with multiple users and policies, migrating them is likely to break something. But starting out with a member account is trivial. It takes ~5 minutes and you can practically forget about this setup until you need it. If you find yourself in the common situation when you started using the master account you still have a few options. First, you can register a new account and invite the old one as a member. The downside is that you need to use a different email address that might not be the official company address. But this is also a viable way. The other option is to create a new member account and then move the resources over there. You might want to start gradually, but a migration means URLs and IPs will change. Using only member accounts for resources is a way more secure approach than using the master account. These accounts can be limited in a way that is outside their control . For the master account, you can apply 2 types of policies: identity-, and resource-based ones. The former applies to users, roles, and groups, and control what they can do. The latter is attached to a resource like an S3 bucket and controls who can access it. But there is no policy type that restricts what the account as a whole can do. What if you want to make sure no S3 buckets can be created? None of the available policy types can guarantee that. SCPs open the way for this kind of control. And since they are imposed on the member from the organization, there is no way around them. Not even the root account can do something denied by an SCP. But the master account can not be limited by SCPs. You can attach them but they won’t be effective. SCPs bring solutions to a whole set of policy problems that the other controls can not address. Let’s see a few examples, but I’m sure these are just the tip of the iceberg. AWS allows all regions to be used, and there are quite a few of them now. But it is unusual to use more than one, especially for development. But from a security perspective, having them usable offers a great way to hide malicious resources. Disabling regions you don’t use is a best practice then. Just keep in mind that global services are served from the us-east-1 region, so you might need to enable those. No matter how many bells a malicious action rings if an intruder can disable security-related services. If you have a CloudWatch Events Rule that sends an email when CloudTrail is disabled, an attacker can delete that first. You can define strict permissions that safeguard these services but you can’t forbid all kinds of tinkering with them. But with SCPs you can deny deleting or altering any resource. If you have an appropriate policy enforced on the account, even the root user is unable to delete the Events Rule. This guarantees security no matter how policies and roles evolve over time inside the account. SCPs make it possible for an account to have no control over some of its resources. Some services can be really expensive or downright disastrous if left available. Ordering a large reserved instance severely affects your bottom line. Or locking a Glacier Vault with a malicious vault lock and start pumping data into it is a particularly vicious act. With SCPs you can disable these services altogether in a future-proof way. You can also whitelist available instance types. Even if your account becomes a bitcoin mining machine it won’t be a particularly effective one with only t3 instances in its disposal. Take that 5 minutes to set up Organizations when you create your account. Even if you don’t need the additional security controls this setup brings, they will be available for you. But if you don’t do it it will be progressively harder the more resources you add to the master account.", "date": "2019-04-24"},
{"website": "Advanced-Web", "title": "How to check if your Reserved Instances are used", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-check-if-your-reserved-instances-are-used/", "abstract": "Maybe you have a few instances that are running 24/7 and you were thinking about using Reserved Instances (RIs for short) to save a significant amount of money. After all, according to AWS, a reduction of up to 75% is quite significant. Then you made the leap and paid for the reservation, and you are sleeping better knowing that the next bill will be considerably smaller than the last one. Let’s see how it looks like in the EC2 console! Here are 2 instances that are currently enjoying the reduced hourly rates: Or maybe it’s easier to see from the Reserved Instances page: Could you spot the indicator that these instances are charged way less than the sticker price? Don’t blame yourself if you don’t. The thing is, there is absolutely no indication that the purchased RIs are actually used. If the instance has matching properties, then it will be a “reserved instance”, but if there are any differences then you’ll be charged for both the instance and the reservation . If you bought a RI for a t3.micro but later decided you need a bigger instance, you’ll pay way more if you forget to change the RI too. And you’ll get no notifications about the mismatch, except for the next bill. Fortunately, there is a metric that can be queried that tracks the proportion of the purchased RIs that are actually used. But typical AWS, you can find it not where all the other metrics are found, in CloudWatch, but by using 2 new and kinda unrelated services: AWS Cost Explorer and AWS Budgets. First, you need to enable the AWS Cost Explorer. To do this, find it on the Console, enable it, then wait at least 24 hours, as it needs to perform some sort of dark magic to produce the metrics. After 24 hours, check back to the console as there is no notification when it becomes ready to use. RI utilization is a numerical value that indicates what percentage of the purchased RIs are used at a given moment. If you run your instances 24/7 then you want to keep it as close to 100% as possible. To access this report, select the Reservation Utilization menu item on the AWS Cost Explorer console: After that, you’ll see the graph. A happy state you want to maintain looks like this: Now that you can see the chart, the next thing is to set up notifications to know when it shows a grimmer picture. To do that, go to AWS Billing , and select Budgets : The next thing is to create a budget . First, you need to select the type, which is Reservation budget in this case: Then fill in the budget details: For the Utilization threshold it is better to set to 99 instead of 100, as the latter would trigger for intermittent shutdowns resulting in false positives. After the budget is set up, the last thing is to specify how you’d like to be notified when the threshold is crossed: Email or SNS. Email is easier, as there is no auto-completion for SNS here, and possibly you need to change the topic policy too. But it is certainly an option. After you’ve filled this out, confirm & create the budget. Now you’ll get a notification whenever there is a mismatch between the reservation and the instances, possibly saving some money. Reserved Instances offer a great way to save some money, but it’s all too easy to misconfigure something and pay even more than without reservation. This is especially the case when one person in the team handles the EC2 instances, and another person handles RIs. But with some initial setup, you can rest assured that you’ll avoid this situation.", "date": "2019-04-30"},
{"website": "Advanced-Web", "title": "AWS: How to limit Lambda and API Gateway scalability", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-limit-lambda-and-api-gateway-scalability/", "abstract": "When you develop a new serverless function or an API, you should limit the scalability . You don’t need the whole cloud to power a function that you call by hand once every few seconds. On the other hand, scary stories about a runaway Lambda function generated a bill so large it would deter anyone from trying out cloud-scale computing are plenty. During development, limit the resources available to you. Software engineering is mostly about trying things out, and you want to do that without the fear of such large bills. For the production environment you can release these limits, hopefully your codebase is tested enough that such mayhem is unlikely to happen. The two services mainly related to serverless computing are Lambda and API Gateway . You can place restraints on both of them, and you should. The two services use an entirely different model for limiting executions. And their pricing structure is so different you wouldn’t think they are designed by the same company. This yields different means of imposing limits on their scalability. Let’s see their pricing model and a back-of-the-envelope calculation for the worst case scenario. Lambda uses the notion of concurrent executions , which are the number of servers processing the requests. The default limit is 1000 and it is an account-wide restriction. As this is the limit on concurrency, the exact amount of calls depends on the length of each execution. For example, an average of 100ms means the upper limit is 10k requests/sec. But since the pricing model is also based on the execution time, this puts a limits on the costs. To calculate the total cost for Lambda, we need to consider the allocated memory for the function. This goes from 128MB up until 3008MB. Let’s see the impact on the bottom line if all hell broke loose! The current pricing is $0.00001667 / GB-sec. The total cost for the duration of the functions for every hour for the two extemes of memory allocations are then: There is another pricing factor that is dependent on the number of executions, but usually that does not play a significant role. We can then assume that if something goes wrong, the function will consume between $7 and $180 an hour . API Gateway, on the other hand, puts limits on the number of requests , irrespective of their length. It uses an algorithm called the token bucket algorithm. There are two parameters influencing the throttling. The first is the steady-state limit which determines how many requests per second can be served if they come in continually for a long time. The other one is the burst limit which is the amount of deviation allowed. The default is 10000/sec for the steady-state limit and 5000 for bursting. For the pricing, the steady-state limit is more important. The current price is $3.5 for every million requests. The worst-case cost for an hour is then: Just like the pricing is entirely different for the two services, the options to limit their scalability also show no resemblance. Lambda uses the notion of reserved concurrency . In my mind, when something is “reserved” that means a guaranteed minimum. When you reserve a table at a restaurant, you can still use other ones on the condition that they are unoccupied. But in AWS terms, “reserved” also means “maximum” . If you configure a function to use reserved concurrency then it will have access only to that pool. The concurrency pool is an account-wide resource which reservations deplete. If you use a lot of rate-limited functions, that would affect non-rate-limited ones too. But for safeguarding development, you should be fine. On the Console , under the function settings, you’ll find the Concurrency settings: If you use CloudFormation , look for the ReservedConcurrentExecutions . This sets the same parameter. What is a good value to set it? Setting it to 1 can be too low even for development. But I believe anything more than 10 would be an overkill. For API Gateway you can set the two kinds of limits for a Stage . If you want more granular control, you can also override them for individual methods. On the Console , select the Stage and enable throttling: If you use CloudFormation , look for the MethodSettings for the Stage, and define the ThrottlingBurstLimit / ThrottlingRateLimit : What would be good values for development? Set the steady-limit to some longer-term maximum, even 1/sec should be enough . On the other hand, increase the burst limit to the thousands . With this setup, it is unlikely you’ll ever hit limiting, but in case a runaway process hits the API too hard it only depletes the burst limit with negligible costs incurred. Wielding the full power of the cloud, you can easily find yourself in a costly situation. Fortunately, it is not hard to do something about it, and I suggest to start every new project with caps on resource usage. This makes sure you won’t be the next one appealing to AWS support to reduce the damage.", "date": "2019-04-10"},
{"website": "Advanced-Web", "title": "AWS: How to query the available CPU credits for t2/t3 instances", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-query-the-available-cpu-credits-for-t2-t3-instances/", "abstract": "The instances in the t2 and the t3 instance family, i.e. the instance types that start with either t2. or t3. , are burstable ones. That means the instance collects CPU credits over time that can be used later. If you use less than what you get in the long run – the baseline performance, between 5% and 40% of the available CPUs – you won’t even notice how this system works. But if you use more than that , the instance will either get throttled or you’ll get charged for the excess usage. It is useful to know how much horsepower an instance has. But unfortunately, looking from the instance itself it is hard to know whether the instance is overused or not. But as this data can be queried from CloudWatch , it is just a matter of scripting to get an up-to-date overview of the numbers. The credit balance is automatically posted to CloudWatch by AWS under the AWS/EC2 namespace . It has a fixed interval of 5 minutes and it can not be lowered even when using detailed monitoring. The metric that keeps track of the available credits is called CPUCreditBalance . When using the AWS CLI, the dimension parameter can be used to filter for the instance ID , like this: --dimensions Name=InstanceId,Value=$INSTANCE_ID . But how do you know the instance ID? It can be queried from other APIs, or if you request it from the instance itself then you can use the metadata service: INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id) . Another set of required parameters is the date interval consisting of the start and the end time. Both are expected in ISO-8601 format, which is readily available using date --iso-8601=seconds . The end-time is the current time so that it returns the most recent data point: --end-time $(date --iso-8601=seconds) . The start-time should be at least 5 minutes in the past: --start-time $(date --iso-8601=seconds -d \"10 mins ago\") . It is better to specify a longer, but not too long, interval and then sort and filter the results than use a too short one and risk not consistently getting back a data point. The only exception is when the instance was recently started, in which case there will be no metrics available. The --statistics defines how the data points are aggregated. Since by specifying --period 300 there will be no aggregation, this call will get the raw data points. Therefore it does not make a difference what you choose , apart from the counted ones. I’ll use Maximum in these examples. Just make sure to handle the appropriate one in the response. The CloudWatch CLI call is then: This returns a JSON similar to this one: Notice that there are 2 data points in this example. With some jq magic it is easy to extract the data with the highest timestamp : The full script that returns the CPU credits for an instance is: If you have unlimited mode enabled then you can also account for the surplus balance . This is a similar metric, but instead of tracking the remaining CPU credits it tracks the negative balance. The concept is the same, but the metric name is CPUSurplusCreditBalance : Then to calculate the real credit balance , subtract the surplus balance from the positive one. This can be safely done as one of the two metrics is always 0. The maximum amount of CPU credits affects how much the instance can accumulate as well as when you’ll be charged for the surplus credits . I consider it vital info. The maximum value is how many credits the instance gets in 24 hours . There are tables in the AWS docs, but I couldn’t find them in a parseable format. To remedy this, I made a JSON file that you can use to query this data. If you see any discrepancy between the docs and the JSON, please open a PR . With this repository in place you can use it to get the hourly collected credits for burstable instance types. To calculate the maximum is just a matter of multiplication: Where to get the instance type? If you use the metadata service: INSTANCE_TYPE=$(curl -s http://169.254.169.254/latest/meta-data/instance-type) . Now that you have both the CPU_BALANCE and the MAX_CREDITS , it is time to print them: This will print an informative status, for example: With some scripting you can get the up-to-date status of your instance. It can be easily integrated with tmux, zsh, or any other tool you use. This can be a convenient indicator where you stand in terms of bursting capabilities.", "date": "2019-04-16"},
{"website": "Advanced-Web", "title": "AWS: Increase instance security by allowing SSH only from your IP", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-increase-instance-security-by-allowing-ssh-only-from-your-ip/", "abstract": "By blocking port 22 on your instance attackers can not brute force it or exploit eventual vulnerabilities. But then how do you access it? With some shell scripting, you can allow access specifically from your IP, thwarting attacks against the SSH server. The usual process is to keep port 22 always open to the world, especially when you have no fixed-IP corporate network, just in case you need to SSH into it. While SSH is quite secure, closing the port prevents the bad guys from even trying to get into. And when you need to get into the instance, you can open port 22 to your specific IP address so that you can do so without friction. Let’s see what scripting is required to implement this solution! Here is the full script, you just need to input the instance-id . Run this before you SSH in normally and it will make sure port 22 is opened for you. You need jq to be installed, which you can do usually with a simple sudo apt install jq . After you’re finished, you can clean up: Let’s see how each part is working together to keep port 22 secure! The first step is to get the security group ID associated with the instance. This is the string with the format of sg-xxxxxx . To do this, you need to query the instance, either by the instance-id or some tags, such as the name of the instance. To get the security group ID by name, use this: And when you have the instance-id , use this: The [0] part gets the first security group (from the first instance, if the query returns multiple). If you have more than one and want to use a different one than the first, you need to filter for that. Fortunately, jq is more than capable to do so. After this line is run SG will hold the security group ID. The second task is to get the current external IP of the machine you are connecting from. This may be the IP your device reports or maybe your router supports returning it, but the most reliable solution is to use an external service. There are several such services each with its own level of reliability. One service that you can use is ifconfig.me , and I’ll use that in this solution. To get the IP, simply use curl -s ifconfig.me . But I’ve realized that in some cases it fails to return the IP, especially when the WiFi is connecting. So I needed to put a check that retries automatically. With a while loop, it looks like this: The MYIP will hold the current external IP. The next step is to query the security group and see what IPs it currently allows: The allowed IPs are associated with port ranges . In this situation I only care about exact matches to port 22 (FromPort == 22 and ToPort == 22), but there can be rules that allow a wider range including port 22. You need to take care of those if you have such rules. The CIDRS will contain the allowed CIDR ranges. What is a CIDR that is referenced here? It is an address range instead of being a specific one. In our case, the /32 suffix defines a single address. The next step is some housekeeping, as you should remove unneeded rules. These might be remnants of previous runs or rules allowed separately. The only address we need to connect to the instance is “$MYIP/32”, so let’s remove all the others: And finally, make sure to authorize your own IP. In case if it is already present in the $CIDRS then this step is skipped. This can happen when you reconnect to the instance without changing your IP. By this time, everything is set up, and you can SSH into the instance without problems. As housekeeping, you can also remove your address after you’re done with this script: While key-based SSH is considered to be secure even if it is open to the world, minimizing the attack surface is a good practice nevertheless. Fortunately, with a minimal amount of scripting you can lock down port 22 on your instance but still be able to SSH into it whenever needed.", "date": "2019-04-02"},
{"website": "Advanced-Web", "title": "AWS: How to get notified on compromised credentials", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-get-notified-on-compromised-credentials/", "abstract": "When you create Access Keys, you are basically blind how they are used. You can see the effects , like new instances starting or things disappearing, but to see how the key is used you need a separate service. CloudTrail records all calls to the AWS APIs and logs some metadata, such as the caller, the result, and a few others. Using this information you can get an insight into how your keys are used, and whether they have landed in the wrong hands. One of the most important things is to know when something is denied . During normal operations, there should be no denials as everything is tested and can work in one way only. But when an attacker gets access to a key he usually tries some innocuous calls to see if it works. If you followed the least privilege principle, this call will most likely be denied. And this is the chance to detect the compromise. Without CloudTrail you won’t get any indication that something is wrong. But with some preparations, you could get a notification and then you can act on it. The solution described in this post follows the principles on how to detect IAM user logins . CloudTrail entries are delivered into CloudWatch Logs and you can use a Metric Filter to produce a numerical value how many events match a given filter. This, in turn, is pushed into CloudWatch , where an Alarm can send a notification when it crosses a threshold, typically when it is greater than 0. An alternative approach uses CloudWatch Events to collect and forward API calls to a Lambda which can act on the suspicious ones. You can also use CloudWatch Events directly to filter on the access denied events and publish a notification without a Lambda. These are both valid approaches that correct some of the shortcomings of the Metric Filters. Why did I choose the Metric Filters then? There are two fundamental problems with CloudWatch Events. First, there is no easy way to rate-limit it. If you do some heavy processing when an Access Key is used in a way it shouldn’t have been used, then an attacker can flood your account with such attempts and, due to the cost, you can not really do anything than disable it. The second issue is that CloudWatch events are region-specific , while CloudWatch Logs can collect CloudTrail events from all regions. An attacker is likely to use the most recently added region hoping you don’t cover all of them. A region-agnostic approach is a more future-proof way of detection then. First, you need CloudTrail enabled and a CloudWatch Logs Group set up as the target. Second, you need a Metric Filter that counts the events where the errorCode is AccessDenied . For the Filter Pattern, use: This pushes a metric with a chosen name and namespace. In effect, when there is an attempt to use a key that is denied, the metric will be 1. To get a notification on this occasion, a traditional CloudWatch Alarm can be used. Simply set up to treat missing data as OK , and alert when the metric is greater than 0 . Of course, there is a ready-to-deploy CloudFormation template that does all the heavy lifting of setting up the resources for you, You need to have CloudTrail set up to deliver the logs into CloudWatch Logs , and have an SNS topic that sends you an email. Then deploy the template , provide the required parameters, wait a bit and you are all set up. There are two problems with this approach. First and foremost, you’ll only get a notification after a significant amount of time as CloudTrail delays logging by 15-20 minutes (!). In some cases, this can be a problem, but if you set up permissions properly and you require MFA for the Access Keys , then you should be fine. The other problem is that you’ll only get information about the existence of a denied attempt but without any details . For the full CloudTrail event you need to go to the CloudWatch Logs console and filter the logs. Fortunately, the URL for this seems to be fairly constant and thus the CloudFormation template outputs it. When you need to see the events just click on the link, or bookmark it in advance. Detection is a cornerstone of a secure environment and access key leakage is one of the worst security incidents. This template provides a deploy-and-forget solution that can alert you when there is a problem with your keys.", "date": "2019-03-26"},
{"website": "Advanced-Web", "title": "Optimized SPA deployment CloudFormation template", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/optimized-spa-deployment-cloudformation-template/", "abstract": "A typical SPA deployment consists of two types of assets: dynamic and static. The former, usually the index.html , can change its content upon version change, while the latter, files like 7a5467.js , cannot without renaming. Modern compilers support this use-case with a technique called revving, that renames the file and all references to it to contain the hash of the contents . With this setup, the static files are non-cached, while all the other assets can be safely kept forever. Configuring a CDN this way can greatly enhance the speed your visitors experience when browsing your site. Let’s first discuss the various advantages of using a CDN for asset distribution. First, it provides proxy caching which helps when multiple users are accessing the site from close proximity from each other. After a user requests a given resource that can be cached there is no need to go to the origin the next time but return it from this cache. Second, a CDN provides edge locations which are closer to the users. This means faster TCP handshake and TLS negotiation as they happen with less latency. Another benefit is the CDN’s ability to reuse the connection to the origin , so even when it needs to fetch the content from there, the handshake from the user does not need to go all the way. This reduces the perceived latency significantly. Also, a CDN can provide HTTP/2 support even if the origin can not. This is the case with S3 buckets, as those are accessible only via HTTP/1.1. And lastly, while not strictly an advantage of the CDN, properly set HTTP cache headers mean the client does not even need to download the static files the second time. The index.html is considered a dynamic asset, and as a rule of thumb, it should not be cached at all. But in some circumstances, a short cache time can provide great benefits without running the risk of a stale cache. In this case, the index.html changes only after deployments , and it is the same for all users . Caching, therefore, is a tradeoff between freshness after deployment and cache performance. With a cache time of just 1 second, you can cap the load on your origin, no matter how many visitors you have. There are a limited number of edge caches (11, at the time of writing) which means your origin can have at most 11 requests/second. And the cost is only a single second after each deployment. The naive approach of hosting the files directly from S3 yields these results measured by Chrome Audits: With an optimized setup utilizing a CDN , the same test yields much better results: And this improvement is recorded only by simulating a first load and not taking into account the client-side caching for returning visitors and the other advantages of using a CDN. Deploy the cloudformation template to the us-east-1 region . It will take ~20 minutes to finish as creating a CloudFormation Distribution is an unusually slow process. After the stack is deployed, it outputs the URL and an S3 bucket. Whatever you put into the bucket will be served through the CDN and all files, except for the index.html , will be cached on both the proxy and the client. To deploy a site to an S3 bucket, use this script: When you want to delete the stack, you need to keep 2 things in mind: First, the bucket has to be empty . Unfortunately, there is no built-in way to clear a bucket before deleting it, so you need to do it manually: Second, you need to delete it in 2 stages . It uses Lambda@Edge to add the cache headers and that needs a few hours (!) to propagate deletion. Issue a delete to the stack, then wait a bit, and delete it again. The solution is built on a CloudFront distribution and a Lambda@Edge function to provide the cache headers. This way, there is no need to add any metadata to the uploaded files, everything will be taken care of using only the filenames. The stack creates a bucket that holds the files. An Origin Access Identity is configured so that users can not access the contents directly and the bucket is configured to allow only the OAI. While the information in the bucket is public, it is a best practice to minimize the surface of your deployments. A distribution is set up configured with the bucket and the OAI. It uses HTTP/2, compression, a redirect to HTTPS, and optimal cache header forwarding. One interesting problem I faced is that the global S3 endpoint for a bucket <bucket>.s3.amazonaws.com takes a few hours (!) to come online and before that happens it returns a redirect. A better approach is to use a regional endpoint of <bucket>.s3.<region>.amazonaws.com that comes online immediately. Because of this, the template uses the regional endpoint. A function is called on the edge for the origin-response event. This makes sure that its result is also cached and not run for every single request. This practice lowers the cost to virtually nothing. As I’ve learned, Lambda@Edge requires the function to be in the us-east-1 region and won’t work in any other one. Due to this, you need to deploy the stack in that region. Also, the distribution requires a specific version of the function which the template takes care of. While SPA speed is usually more influenced by client-side processing and API speed, asset delivery is also a factor. Fortunately, with just some preparations, you can seriously speed it up. With a template like the one detailed in this post handy, you can easily set up your delivery CDN and pare down delay your customers experience.", "date": "2019-03-19"},
{"website": "Advanced-Web", "title": "How to automate development tasks using NPX", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-automate-development-tasks-using-npx/", "abstract": "When I look at what I do as a developer, most of the things are repeated actions that I don’t automate because “this is the last time I need to do this”. Editing a file, restarting some server, switching to the browser, refreshing and navigating it. Then I make another small change and do the same process again. There is a bias I see throughout my work. It goes like this: I feel that the task is almost finished, so I don’t automate. But then it is not, and I find myself working on the same task an hour and 20 refreshes later. If I look back what I did in that hour, I mostly wasted time on things I could automate in 5 minutes. To battle this, the best thing is to make these automations as easy as possible. Did you know that in economics there is an enormous difference between ridiculously cheap and free? The same applies to scripts. The difference between creating a package.json and add some scripts versus a one-liner that can be copy-pasted can not be overestimated. We write articles like this regularly. Join our mailing list and let's keep in touch. In this mindset, I learned about NPX, the NPM package runner. This is the perfect tool for one-liners as you get the full ecosystem of the CLI tools in the NPM repository but you don’t need to install anything. Just learn some of the most helpful tools and you will be able to automate the mundane work when you start working on a task. NPX can replace globally installed packages. Instead of the usual route of doing an npm i -g package then use the package , you can achieve the same with npx package . There are a few configurations NPX supports. When you need multiple packages installed specify them with -p <package> . In this case, you need to define the package to run separately: For example, webpack-cli requires webpack to run but it is a peer dependency, so you need to install both: The default case of npx package translates to: If you need to run a specific version of a package, define it with package@version . The downside of using NPX is that it runs an install every time which takes a few seconds. In cases when you need it to run as fast as possible, this can be a problem. Let’s see a few use-cases for NPX! And contrary to all other articles about NPX, this list won’t include cowsay . I find nodemon to be an indispensable tool in my toolkit. It watches files and whenever they change it runs a command. Simple, and I’m sure there are million other ones, but with NPX nodemon can be run as a one-liner and it just works. To run the project every time a source file is changed, use: One source of problems is that it watches only js files. To specify other extensions, use the -e <extensions> argument. For a React+TS project, you’d want -e ts,tsx . Another gem that supercharges the development workflow: No more alt-tab, ctrl-r. By the time you get there, the browser is already refreshed. Note that you need a browser extension installed and activated. JS obfuscation can not be easier than this: It runs the code through javascript-obfuscator , the only open-source project I know about that provides robust obfuscation. Use all the goodness of Typescript and compile it back to traditional Javascript: While it is better to have a package.json and have scripts that run Webpack, it is nevertheless possible to run it without all that: Serve the current directory as a web server: Do you also have an API along with your static assets? Specify another URL that will be used for everything but the local files: In case you need HTTPS, generate a self-signed certificate and specify that: To run your code through a minifier: How to know if a dependency has a new version? Run this: To run two things in parallel, use the concurrently package: You can easily run http-server and livereload together with this: To have a minified script printed out in a more readable format, use the js-beautify project: Syntax highlighting can also easy readability: You can also combine the above two: Easily compile SCSS into CSS: Run eslint with the default configuration: Easily format your code without installing anything with prettier : Or with standard : To show your work to someone, tunnel a local port through the internet with localtunnel : By combining it with http-server you can showcase a local project without installing anything: When you need to code short functions, setting up a full-fledged package.json and all the dependencies and scripts just to use a better technology usually does not pay off. But with the above simple one-liners, you can easily compose useful short scripts. Have you ever wanted to use Typescript for Lambda? To continually compile and exec your code, you can use this script: And when you are ready to deploy, compile and minify with this script: I’ve been using NPX packages for quite a while, and I see enormous benefits. With just a few tools I can automate most of the non-creative work I do. That leaves me with more time to do my job.", "date": "2019-03-12"},
{"website": "Advanced-Web", "title": "Keep costs under control when using t3 instances", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/keep-costs-under-control-when-using-t3-instances/", "abstract": "I’m sure you know the t3.nano instance type in EC2. It has access to 2 vCPUs, has moderate bandwidth, and is generally an all-purpose virtual machine capable of handling light workloads. You need to pay separately for block storage and network usage, but considering only the CPU it will cost at most $72 a month. Yes, you read that right. The t3.nano can cost you $72 a month. The product page shows $0.0052 an hour, but digging into the documentation shows that the $72 is indeed possible. Let’s see how! The burstable instances, which cover all with the t3. prefix, accumulate CPU credits over time. These represent the share of the physical CPU it can consume and this process makes sure that no client on the multitenant machine starves the others, all while maintaining a predictable system. Credits accumulate over time towards a maximum, which is the amount the instance earns in 24 hours. And since different instance types get different amounts, this maximum also varies. A t3.nano gets 6 per hour with a maximum of 144, while a t3.2xlarge gets 192 with a cap at 4608. One credit equals to 1 minute of a full vCPU core, so a t3.nano can use it 6 minutes every hour sustainably. When the credits run out, the instance is throttled. This is called standard mode . When the T2/T3 Unlimited mode is turned on, instances can also get surplus credits . After all normal credits are used, these surplus ones get accumulated. And when they reach the maximum, which is the same for both types, you get charged for the excess. On a graph they look like this: You can notice that you can burst the instance twice as long. The two types of CPU credits are separate metrics, but essentially they track a single value. After all, if one is non-zero, the other is guaranteed to be 0. In this sense, surplus credits are negative credits : The way I like to think about them is that CPU credits track the “savings” while surplus credits represent “debt”. And when you get into debt too much, you’ll get charged for it. In unlimited mode, you get charged for surplus credits over the maximum. Fortunately, this cost does not vary between instance types and regions, it’s $0.05 for every vCPU hour (60 credits). The problem is that this system makes compute costs dependent on how you use the instance , while in standard mode you know it exactly beforehand. Let’s see the case for the t3.nano instance! The baseline price for the instance is $0.0052 an hour. On a monthly basis, that will set you back $0.0052 * 24 * 30 = $3.744 . This is the number you usually associate with the cost of the instance. Calculating the surplus credits is a bit more involved. The t3.nano has 2 vCPUs and the baseline performance is 5% (this is the amount covered by the generation of CPU credits). The maximum cost is then 2 * 0.95 * 24 * 30 * $0.05 = $68.4 . Yes, that’s an 18-fold difference. The problem is usually not that your app needs just a little bit more juice to handle all the visitors but when something goes wrong and a runaway process consumes 100% CPU non-stop. In standard mode, you’ll notice it eventually when the CPU throttling kicks in and your app stops responding. But in unlimited mode, albeit slower, the instance is likely to continue working as no throttling takes place. It’s likely that the first indication that something is wrong comes with the next AWS bill . And where you get to choose between the two? At the bottom of the page when you launch an EC2 instance: And it is checked by default for all t3 instances. Fortunately, there are CloudWatch metrics you can track to detect such problems. The CPUCreditsBalance is available for both standard and unlimited mode, while the CPUSurplusCreditsBalance and the CPUSurplusCreditsCharged are for unlimited mode only. I recommend setting an alarm for the CPUSurplusCreditsCharged at the very least. This will alarm you when the CPU is getting involved in your bottom line and gives you the early warning that something is wrong. For standard mode, you might already have some sort of monitoring for your app that alerts you when it goes unresponsive or too slow. But setting an alarm for when the CPUCreditsBalance runs too low is also a good idea. For mission-critical systems, yes. Unlimited mode keeps the system responsive even when it is able to do so barely until you wake up in the morning and have a chance to investigate. Paying for a few surplus credits to bridge the time from having a runaway process and being able to remedy it. But don’t forget to set up an alarm so that you know there is a problem. The other valid use-case is when you consistently use more CPU than the baseline performance. In this case, you plan for this cost. And since paying for surplus credits is cheaper than switching to a larger instance with a higher baseline, it saves you money. In every other case , opt for the standard mode to avoid costly surprises.", "date": "2019-03-05"},
{"website": "Advanced-Web", "title": "AWS: How to secure access keys with MFA", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-secure-access-keys-with-mfa/", "abstract": "One of the most catastrophic of the AWS account security breaches is not sophisticated hacking involving 0-day vulnerabilities traded on the deep web by high-profile hackers. It is when you post your access and secret keys in plain text to the public . After all, it’s so easy to test with some hard-coded keys and accidentally push it to the VCS. There are numerous crawlers constantly searching GitHub and other public sources to catch these keys. Not just by the bad guys, AWS itself also runs these bots trawling for leaked credentials and I’ve heard about stories that they contacted account owners in case of a catch. The problem with access keys is that they do not require anything else to be usable. If you have the key pair, you can use them to do anything the user has access to. When you use the console, you have an option to require an MFA device to log in to the account along with the username/password pair. Securing console access is then only a matter of associating an MFA device with the user. With this in place, the account is secured. I wondered, is there a way to do the same for access keys also? Yes, there is. Contrary to the console logins, adding an MFA device does not immediately affect access keys. But it can be checked in an IAM policy to achieve a similar effect. Here is a policy for this, you can simply copy-paste it: It is loosely based on an example policy provided by AWS. What does it do exactly? The first part, AllowIndividualUserToListOnlyTheirOwnMFA allows the user to list their own MFA devices. This is to make scripting easier, as starting a session requires the serial number of the device. With this call it can be looked up dynamically. The next part, BlockIndividualUserToListOtherMFAsWithoutMFA makes sure that no other MFA devices are accessible if the session is not itself authenticated with an MFA device. This sounds confusing, but this implements the least privilege. The non-MFA authenticated user should not have access to anything that is not strictly required for this one task: to authenticate with an MFA device. Everything else should be denied, including listing other MFA devices. The last part, the BlockMostAccessUnlessSignedInWithMFA extends this concept to all other services. Notice that the policy does not give any permissions apart from the bare minimum. In this sense, it is a blacklisting policy that’s sole purpose is to limit other policies attached to the user. For example, you can use this along with the Administrator managed policy to have an MFA-protected admin. In AWS when you authenticate with an MFA device you need to start a session . This is a call to the sts:get-session-token endpoint and it returns an AWS_ACCESS_KEY_ID , an AWS_SECRET_ACCESS_KEY , and an AWS_SESSION_TOKEN . Since the AWS CLI uses environment variables when they are present, you can set the session parameters in the environment. Subsequent calls to the APIs will use the temporary credentials of the session instead of permanent ones from the config. How temporary? By default, a session is valid for 12 hours , which should be plenty for a workday. And if it expires, you can get another set with a new MFA code. To make things easier, it is best to have a shell script ready to do the mundane work: Only jq is required apart from the AWS CLI which you can usually install with apt install jq or something similar. To use it, provide the MFA token and source the result : Unfortunately, it does not work with U2F MFA devices, only virtual ones. That means you need to use your phone and Google Authenticator, Authy (my preference), or a similar app. To deauth the session and get back to the permanent credentials: A curious case is that there are 3 calls in the script above, but the policy only allows the iam:ListMFADevices and neither of the sts:GetCallerIdentity nor the sts:GetCallerIdentity . As it turned out those calls are always allowed, even if you explicitly deny them . This is one of those quirks that make working with IAM policies, well, interesting . The original problem was privileges from leaked keys which this solution effectively prevents. But the cherry on top is that you can also monitor lost keys. CloudTrail logs all denied accesses to the AWS APIs and using that you can monitor keys. This can signal a lost key which you can then replace. How secure is the above solution? Using an MFA device immediately raises the question of what happens if the device is stolen/lost ? As long as there is an admin above the user who could assign a new device all is well. And since these keys are usually used by developers, it does not pose a problem. If the device is lost, report it to management and they will replace it for you. One downside is that you can’t use U2F keys , only virtual ones and those are not guaranteed to be copy-proof. If someone gets the MFA secret code, he can recreate the device. And lastly, security cannot be assessed without taking convenience into the equation. The above solution requires the developers to enter a code once every day then they can use their account just how they would without this protection. I believe this is a reasonable compromise, considering the enormous gains. MFA is one of the best tools to prevent hacking into an account, and with some setup and tooling, you can secure developer accounts with minimal inconveniences.", "date": "2019-02-26"},
{"website": "Advanced-Web", "title": "Managing EC2 servers at scale: Ansible and RunCommand", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/managing-ec2-servers-at-scale-ansible-and-runcommand/", "abstract": "I still remember the first time I had to deploy a changeset to a production server. It was at my first job roughly ten years ago, and I wasn’t usually doing operations. I had a colleague who did the deployments, but he was missing that day. I was handed a paper with a list with ~25 bullet points, each describing a manual task I had to do. Fortunately, I am quite good at following a process, so everything went smoothly. It was much later when I learned that with each manual step a process gets exponentially fragile. For 1-server deployments I can manage a shell-script that applies the latest changes, but a fleet requires an entirely different mindset and tooling. I recently did a course on Ansible, during which I had a feeling that I’ve already heard about this problem. The EC2 Systems Manager RunCommand is a similar service that offers a solution to this specific issue. I was intrigued enough to assemble a test environment to test them side-by-side. I describe the setup and what I learned below. To follow along, deploy this CloudFormation template . The only thing you’ll need is a KeyPair to log in to the instances with Ansible. In the samples below, you’ll need to replace <privatekey> with the path to the private key, and for the shell scripts, you need to provide it as the first parameter. The template spins up 2-2 instances, one fleet for the Ansible tests, and another one for RunCommand. Ansible augments what you’re doing manually: SSH into a machine, run some command, inspect the output. But instead of doing it one server at a time, it supports parallelization. Also instead of running ad-hoc commands one-by-one, it supports running a playbook which is a yaml-formatted file describing the process. It provides an easy-to-read and very fast way of running commands on multiple servers. Ansible uses an inventory file ( ansible.cfg ) that describes what instances you manage. This is enough for a static deployment of fixed servers, but it does not cover the dynamic nature of the cloud. For example, AutoScaling can add/remove instances at any time. Instead of a static file describing the instances, it also supports a Python script that can generate the list on-the-fly. Even better, it supports a comma-separated list of servers directly from the CLI. With some AWS CLI magic, you can collect the IPs of the servers for a given tag: How does it work? The ec2 describe-instances returns a lot of information regarding your instances. To make it more developer-friendly, the list can be filtered and queried. The former specifies the servers to be included, while the latter transforms the response to leave out unnecessary data. The Name=tag:Group,Values=ansible-test filter includes only the instances that has the ansible-test as their Group tag. Then the Reservations[].Instances[].NetworkInterfaces[].PrivateIpAddresses[].Association.PublicIp gives back only their public IPs. Then some jq magic (which is a must-have if you work with the AWS CLI) prepends the username before the IPs and join them with a comma. The end result will be something like this: Ansible uses SSH to log in to each server. Because of this, you (or the control node) needs SSH access to the managed servers. As this is usually done using a keyfile, it provides a secure way of logging in. If you want to tighten security even more, it naturally supports bastion hosts. Let’s run some experiments! Let’s start with the mother of all troubleshooting techniques, the ping. To see it in action, use this script . The -o StrictHostKeyChecking=no is required if this is the first time you SSH into the instance. Remember the warning The authenticity of host ... can not be established and you need to type yes to finally connect? It makes that go away. Something that more resembles a real-world scenario is an Apache installation with a website. A playbook for that: This installs the httpd package, copies the index.html file to the instance, starts the server, then finally registers it as a service. To run this, instead of the -m ping all , define the playbook yaml. Use this script to try it out. After it is run, an Apache is set up on both the servers, and can be accessed: RunCommand requires (1) the SSM agent to be installed and (2) some permissions to be assigned to the instance. If you use an Amazon-provided AMI, (1) is already taken care of. To attach the permissions to the instance, create a role for the instance and attach the arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM AWS-managed policy. It contains all the permissions SSM needs, including running commands. If everything is set up successfully, the instances will show up under the Managed Instances in the Systems Manager console: Notice that there is no inbound port 22 allowed to the instances. This means that you don’t even need SSH to manage the servers. To follow along, here is the script . To send a shell command, you can use the AWS-RunShellScript command document. You can set what is run, along with a filter and a query, similar to what we’ve used in the Ansible part. The --query \"Command.CommandId\" transforms the output to only include the id of the command. We’ll need that to check the status and the output later. Unlike Ansible, RunCommand is fully asynchronous. When you send a command you get a CommandId which you can use to check the status. To wait for the completion of a command: This does not handle failure cases, so it is far from being a production-ready solution. But it shows the fundamental approach. When the status is Success , get the output: Ansible is a dream to work with, especially compared to SSM RunCommand. It is fast , synchronous , and the playbook syntax is surprisingly easy to write. The documentation is also decent, I could find everything I needed with a relatively short amount of searching. Since it is based on SSH , there were no cryptic errors and product-specific problems. Security-wise it should be as secure as any other server-management solutions. If you use key-based authentication (as opposed to password-based) it should be practically unbreakable and by relaying through a bastion host it could be further hardened. As opposed to RunCommand, the instances need no further permissions . The file uploading is usually a pain point to handle correctly, but with Ansible it was a breeze. There are also plugins to customize the contents to each server. From the ops perspective, RunCommand is the better one as it is integrated into the ecosystem . As it does not require any route to the host itself, a production server can have only the ports opened to perform its purpose. It is also more integrated with other AWS services, like IAM policies and CloudTrail . You can create multiple documents for each task and fine-tune who can run which . And everything will be logged automatically. If I had to choose between the two, I’ll definitely go for Ansible. It is a solution that is easy to use, and you can hit the ground running with it. With its playbook (and role) syntax, you can define complex workflows. For the longer term, RunCommand has a lot of advantages, especially because it is deeply integrated with AWS. But as it is hardly usable without tooling, it requires more time to setup. Especially due to its asynchronous nature, it is a lot slower to run a sequence of commands than any other solution.", "date": "2019-02-12"},
{"website": "Advanced-Web", "title": "AWS: How to get notified on IAM user logins", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-get-notified-on-iam-user-logins/", "abstract": "The approach with the Event Rule, described in the previous post, works only for the root user. If you try to remove the criterion, you’ll get the impression that it is working, but contrary to the root user, IAM users can choose the region they use to log in. That means an Event Rule defined in the us-east-1 region will only notify you about logins in that region. One approach to solve this is to define a rule in every region. But then you need to be careful to cover future regions as well, which is hard to automate. As a side note, if you ever find yourself wanting to get into an account silently, make sure to specify the newest region to do so. Maybe the account owner forgot to cover that. A more robust approach is to use CloudTrail with CloudWatch. CloudTrail can deliver all the API events into a single CloudWatch logs stream, and you can set up a Metric filter and an Alarm that will, in turn, notify you of any future logins. Let’s see how it works, step by step! First, go to the CloudTrail console, and set up a new Trail if you haven’t done so already. Make sure to configure the CloudWatch log delivery along the way and also check the Apply trail to all regions . See here for a detailed tutorial. You can have the same one you’ve set up for the root login notifications, but don’t forget to update the topic policy. If you use a new topic, you don’t have to do anything. But for an existing topic, make sure that the default statement is included: Adding a new policy overwrites the default one, so you need to manually add it back. Also, make sure to subscribe to the topic and confirm your subscription after that. If you have CloudTrail and an SNS topic set up, you can use this CloudFormation template to set up everything else automatically. In this case, skip Steps 3 and 4. Now that CloudTrail events are delivered to CloudWatch logs, the next step is to set up a Metric filter. This is an expression that creates a numeric metric in CloudWatch. To do it, go to the CloudWatch Logs console, find the logs group CloudTrail is using, and on the Metric Filter column, click on the 0 filters . Then add a new filter, for the Filter Pattern use {$.eventName = \"ConsoleLogin\"} , then in the Metric Details, assign a Namespace and a Metric Name. Now that you have a metric that tracks whenever a ConsoleLogin event appears in the CloudTrail logs, you need to add an Alarm to notify the SNS topic. In the Metric Filter card, click on the Create Alarm link that brings you down directly to the CloudWatch Alarms console, prefilling almost everything in the process. For the Details , use > 0 for 1 out of 1 datapoints , and Treat missing data as: good . Finally, in the Actions , add the SNS topic for the ALARM state. Now that you have everything set up, take it for a spin! Log out, then log in, get a coffee, then after 15-20 minutes, you’ll get an email saying the Alarm is in ALARM state. Also go ahead and try to log in in different regions, the alarm will be set off. Why this delay when the root login notification was instantaneous? CloudTrail is a near-realtime service, which is a fancy name for non-realtime. It delivers events with a delay of ~15 minutes. The other problem is that the Metric Filter delivers a numerical value, which is suboptimal for a point-in-time event like a console login. In effect, if there are no logins for a while then there is one, it detects it. But if multiple logins are happening close to each other you’ll only get one notification instead of one for every occasion. One way to handle this is to filter on the users. You might want to monitor only a privileged user’s sign-ins, and not everyone’s. This should help cut the noise, especially when several people are using the same account. Detecting intrusions is an important security measure, and logins to the console are one of the most serious ones. While the above solution is not optimal in several ways, especially due to the long delay between the action and the notification, it does provide some insight into what is happening in your account.", "date": "2019-02-05"},
{"website": "Advanced-Web", "title": "AWS: How to get notified on root account login", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/aws-how-to-get-notified-on-root-account-login/", "abstract": "One of the first mistake people do is to use the root account for day-to-day tasks. It makes sense, as AWS provides this user, so why not use it? But it is so powerful and replacing it with a lesser user is so easy, there is hardly any justifiable reason not to get rid of it. But when it comes to security, knowing when something is broken is of equal importance. You don’t use the root user, but it is still accessible. If somebody steals the credentials and use it, you must know that immediately. And there is a way to do so. To use CloudTrail events in CloudWatch Events, you need to enable CloudTrail first. There is no notification if you don’t do that, but you won’t get any events. (Thanks to Christian Salway for catching that) In a nutshell, you need to define a CloudWatch Events rule to detect the login action and send an SNS notification. Since the event dispatch and the notification is separated, you can set up more than a simple email alert, like calling a Lambda function or send an SMS. The trick is to use the us-east-1 region , as that is where the root users log in. Setting up the Event Rule in that region is essential. You can use this CloudFormation template to have the notification set up for you automatically. Just don’t forget to switch to the us-east-1 region before deploying it, and subscribe to the resulting SNS topic after. If you want to do it manually, follow these steps. First, switch to the us-east-1 region before doing anything else. Then create an SNS topic , subscribe to it, then confirm the subscription. Unfortunately, the default topic policy does not allow CloudWatch Events to publish to the topic. To allow it, replace the topic policy with this one: If you add this to an existing topic, be careful. If there is no policy attached, there is a default one, which allows most services in the account to publish to the topic. By adding an explicit policy, this default is lost, and that might break other publishers. The event is called AWS Console Sign In via CloudTrail . Still in the us-east-1 region, go to the CloudWatch console, and under Events and Rules , add a new rule. For the Event pattern , use this: Finally, add the SNS topic as the target. Log out and log back in with the root user. An email will arrive seconds after.", "date": "2019-01-30"},
{"website": "Advanced-Web", "title": "The rocky path to delete an AWS Organizations member account", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-rocky-path-to-delete-an-aws-organizations-member-account/", "abstract": "Organizations is a wonderful service in the AWS ecosystem, both for usability and security. While the cloud provides a lot of security controls within an account, a multi-account setup is far superior in several ways. With multiple accounts, you don’t need to worry about setting up policies to prevent developers from accessing or disrupting critical resources, as they can be hermetically separated from the production account. This is a rare occasion where security and convenience are aligned: the production system is guaranteed to be left alone, while the devs can experiment with the technology without over-exacting controls. The account creation process can be scripted all the way down to bootstrapping the new accounts with resources. The process is straightforward: provide an email address for the root user, name a role in the new account, and you are practically done. With full API and CLI support, it can be automated. Also, with the role in the new account, you can script everything else you might need. With a click of a button, you can have a new AWS account with everything required. The cherry on top is that you can also define Service Control Policies, that limit what the new account can do. Want to scope it down to only one region, or disable certain services? You can do it easily. The only downside is that the new account is still considered a new account, so there is a few hours waiting period before you can launch instances. This is some kind of auto-check and it is still required even though the master account has already passed this. Everything was fine, then one day one of your developers is leaving the company. You then need to get rid of his environment from the Organization. This is the point when you realize that while creating an account is extremely easy, closing it down is burdensome. The problems start with the root account. Unfortunately, the process for closing a member account is the same as closing a regular one: And after the process, you can’t use the same email ever again. Did you register the account with the email address of the leaving dev? Tough luck. You need to manually contact AWS to change the email. Since you didn’t need to provide a password during account creation, how to sign in? It turned out that you need to request a password reset to the registered email and then you can set a password and finally log in. When you create an account, there is no confirmation at all . Mistyped a character? Contact support. And the worst part is that it’s likely you’ll only realize it when you try to sign in to close the account. Fortunately, you can take advantage of the name+...@gmail.com pattern, if you use Gmail. This address is the same as name@gmail.com , effectively giving you an infinite amount of email addresses, all mapped to the same Google account. When an account is closed, there is a 90 days waiting period , during which the account is only “suspended” and it still counts towards your limit of accounts. Well, what is the limit exactly? The docs say it “varies”. Googling around it seems like it is around 10-20. Luckily, you can contact AWS to increase the limit, and as some companies reportedly use thousands of accounts, there should be no hard upper limits. But if you don’t want to do that, there is a way to remove the account from the Organization. The process is to make it a standalone account, and after that, it can be removed from the master. Transforming to a standalone account is providing the data that is required for a regular account, but wasn’t required for the member kind. Billing address, bank card data, things like that. After entering those, the account can be removed. All this chore just to let it die alone. And it also requires the root user, so there is no way around that. Never, ever register a new account with an email address you don’t own. Seems like a good idea, but it will come back at you hard when you need to close it down. Also, even though you can technically reuse an account, i.e. give an existing environment to a new dev, I don’t recommend it. If you hit the member account limit, request an increase and aim for a clean state.", "date": "2019-01-22"},
{"website": "Advanced-Web", "title": "How to use the PhysicalResourceId for CloudFormation Custom Resources", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-the-physicalresourceid-for-cloudformation-custom-resources/", "abstract": "Working with custom resources in CloudFormation is mostly a straightforward task. After you have the template for the Lambda function and the necessary permissions set up once, it is mostly copy-paste and handling the lifecycle is a matter of API reading. But it took me a lot of time to wrap my head around the PhysicalResourceId and what are the best candidates for this parameter. At first, I didn’t give too much attention to it. It resulted in a lot of frustration and I soon realized that overlooking its importance cause a lot of headache down the road. Numerous times I watched the underlying resource simply disappear then had to wait for CloudFormation to offer the option to delete the stack without properly cleaning everything. After giving it much thought, I identified two distinct use-cases that should cover pretty much every scenario. But first and foremost, do not use the context.logStreamName from the official examples. It will appear to be working but it will break eventually. First, let’s recap what exactly is the PhysicalResourceId and what is it good for. It is a parameter that you get with the lifecycle events and you also return it. The value of the event.PhysicalResourceId during the events: Empty. Plain and simple. The value is what you returned in the Create step. But depending on what you return here, two things can happen: If you return the same one , nothing else happens. It will be a regular update . But if you return a different one , CloudFormation will issue a Delete with the old parameters. In effect, this will be a replace . The value will be the one you returned in the Create step. As you can see, the PhysicalResourceId is used for two things: First, it communicates a piece of information between the lifecycle steps. What you returned in the Create step, you’ll get in the Update and the Delete. And second, it controls whether a resource is updated or replaced . And this can cause problems, as in some cases it can delete the underlying resource with all its state. What to set it then? This is the usual case. When you create a resource, it usually returns an id. For example, when a GitHub gist is created , you issue a POST and the API returns an id: And in the response, there is a property called id: This is a critical piece of information, as you’ll need it later for updating/deleting the gist: As it identifies the underlying resource (the gist), it is a perfect candidate for the PhysicalResourceId. In the Create step, return the id: Then for Updates, you can either return the same one ( update ) or create a new gist and return the id of that ( replace ): The important thing here is not to delete the existing resource . When a new PhysicalResourceId is returned, CloudFormation will issue a Delete, and that should take care of cleaning it up. In this scenario, unfortunately, you can not detect the case when there is a problem and CloudFormation retries creating the resource when it already exists. In that case, you can end up with an orphaned resource, i.e. a resource that is not managed by any CloudFormation resource. The other use-case is when you specify the id of the underlying resource. This is the case for S3 objects, but to stick to our previous example, it applies to GitHub repositories. Repositories are identified by the account name and the repository name. When you create a repository, you provide the name in the body: Then when you want to delete it, you only need the name again (and the account name, but you can get that from a separate call): A common mistake is to define the name fully in the CloudFormation template and use that directly. While this gives full control over the naming to the user of the template, this practice easily causes irreversible problems. The problematic cases are renaming the resource in the template and copy-pasting it over a different name: If both resources map to the same repository, then the updates will become unreliable and they can easily be stuck. To avoid the clashing of names, instead of naming it repo , add something unique, such as repo-bf3f42 . How should you create the second part? First of all, don’t make it random . While it is an easy choice (those characters look like random, right?), but that makes your function non-idempotent . If there is a failure, CloudFormation retries creating the resource and this can be caused by something outside your control, like a failure of the computer running the Lambda code. If you introduce randomness, you can end up with orphaned resources. While it should be rare, it’s better to prepare for this scenario. Instead of a random, let’s use a pseudo-random part based on a subset of parameters available to the function. As a bare minimum, you should use the StackId and the LogicalResourceId . These two together make the resource unique and make sure the names can not clash. You can use a hash function for the calculation, like this one: If you use the StackId along with the LogicalResourceId as inputs to calculate the hash, there will be no replaces to the resource, only updates . In some cases, you might want to replace the underlying resource instead of updating it, this being the case for properties that can not be updated. To add parameters that control when a replace should be done instead of an update add them to the hash when you calculate the id: But don’t forget to check if the id is changed and create a new resource if it was: So, what should go into the PhysicalResourceId? For a resource that is defined like this: Then you can choose between two approaches: Either add the name to the hash and use the variable part as the id ( bf3f42 ) or use the full name of the resource ( repo-bf3f42 ). In both cases the effect is the same: whenever the name is changed, the resource will be replaced. Personally, I prefer the first approach, as that will name the PhysicalResourceId the same as the underlying resource. Choosing the PhysicalResourceId is critical when you define your own custom resources. It can make or break the reliability of your code, but it is often overlooked. I hope this guide helps you choose the right one for your use-case.", "date": "2019-01-16"},
{"website": "Advanced-Web", "title": "Custom resources in CloudFormation templates: Lessons learned", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/custom-resources-in-cloudformation-templates-lessons-learned/", "abstract": "Working with custom resources opens up a new dimension of CloudFormation. Along with the built-in support for most AWS resources, you can add support to all sorts of other things. This also removes the limitation that CloudFormation can only handle resources in the AWS cloud; you can manage GitHub repositories, MailChimp campaigns, and many other third-party resources. During my work with custom resources, I’ve learned some best practices. You can choose between an SNS topic and a Lambda function when you implement the logic to handle the lifecycle. The former is somewhat more versatile, but I’ve found that using a Lambda also contained in the template to be less problematic. The resources tend to create/update/delete well within the time limit of the execution, and having everything defined in a single place eliminates the guesswork which code is run. If you have a Custom resource, you can’t change the ServiceToken later. If you do need to change it, remove the resource from the template, and create a new one with the new token. This is usually not a problem, as by the time you deploy to production you already know which Lambda to use, but it came up once. As a consequence, you can’t rename the Lambda resource as it would change its Arn, which is what the ServiceToken is. When you use a template like this: And later want to rename the Lambda resource, without changing anything else the update will fail: If you create a custom resource with a given Lambda code, when you push new versions only the new code will run. This can easily mean orphaned resources, especially when you change how you locate them. For example, if the custom resource is an S3 object, then you change how you calculate the key. In this case, even though the OldResourceProperties contains the old value, the new Lambda code will not be able to locate and delete the object. As an illustration, when you change the property from Filename to Key , you might need to check the bucket manually. In the previous version, you used Filename as the key of the object: But later changed it to Key : The updated code will look for Key and will not find the object defined by the Filename . As a result, the existing object might be unaccounted for in the future. Changes in the Properties section triggers an update to the resource. But if you only change the Lambda code, that does not change the resource itself, so you won’t see any effects. During development, you should add a dummy parameter that you can change to force an update. With a resource like this: When you change the Lambda code, make sure to change the parameter also: This will trigger an update lifecycle step and you’ll be able to observe the effects of the new code. Since the handler function is fully asynchronous, without an explicit failure, it timeouts eventually. The bad news is that it can take 30-60 minutes, in which time you can’t even delete the stack. On the bright side, it will timeout eventually preventing getting stuck indefinitely. Moreover, if the stack deletion fails, CloudFormation offers the exclusion of the problematic resources. Just don’t forget to delete them manually. Because of the timeout, always add error handling first, right after you start writing your function. This safety mechanism makes sure an exception does not result in a lengthy timeout. The easiest way is to write your function in a try-catch: Instead of naming the underlying resource directly from the template, you should also add a variable part. This makes sure that if you have two resources in the template, they map to different underlying resources. Instead of naming the object example.txt , use something like example-gg3jfds.txt . You can see this in the built-in resources too, for example, when you create an S3 bucket. It comes especially important when you rename the resource in the template. CloudFormation first creates the new resource, then issues a delete to the old one. And since the parameters are the same, except for the LogicalResourceId, this process will delete the underlying resource, even though it is still in the template. The PhysicalResourceId is the identifier you give to the resource during the create step. During an update, you have a chance to return a new one, in which case there will be a delete step after the update. Since this is something you need to plan, you need to be mindful what you use for this id. But whatever you choose, do not use the official example of context.logStreamName , which is unfortunately got copy-pasted to many other tutorials. The name of the log stream is not tied to the lifecycle of the resource in any ways, and it can change without notice. If, for example, you deploy a new Lambda version, it will change. You don’t want to delete your resources the next time you update your stack. If you don’t want to give it much thought, use the LogicalResourceId . This makes sure that there will be no deletion unless you explicitly delete the resource from the template, which should be a good starting point. The cfn-response module defines a function that implements the basic logic of sending the response to CloudFormation. The catch is, it might not be available, depending on how you defined the code in the template. If you use the ZipFile property, the cfn-response module is available. But if the code is in S3, which is where aws cloudformation package puts it, the module is not available. In that case you can copy-paste from the official docs", "date": "2019-01-08"},
{"website": "Advanced-Web", "title": "How to manage S3 Objects in CloudFormation templates", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-manage-s3-objects-in-cloudformation-templates/", "abstract": "You can create and manage the full lifecycle of an S3 bucket within a CloudFormation template. But there is no resource type that can create an object in it. How to write one? We’ll build a solution upon Custom Resources, which can add support for arbitrary resources using a Lambda function as a handler for the lifecycle. As the baseline, the solution is built on the basics detailed in the previous article . Check that out for an introduction on how to write custom resources. Let’s see how a simple implementation would look like! Obviously, we’ll need a bucket to store the objects: And as we’ll use the Lambda function to write into this bucket, we need to amend the execution role: Then a custom resource with a custom type, something like this: As for Properties, we need the Bucket, the Content, and the parts of the Key. It’s best practice not to define the key in its entirety but leave space for some random part. Let’s define the key as a prefix and a suffix, which yield the full key using this scheme: {KeyPrefix}-{Random}{KeySuffix} . Now that we have everything needed on the template side, let’s move on to the actual implementation! First, we need the AWS SDK and the S3 object to interface with S3: At this point, there is no need to supply any credentials, as it will be used from the Lambda execution role. Since we only have the prefix and a suffix of the key, we need to construct it. Between these two, let’s have a pseudo-random part. For that, a hash function is a good candidate: concatenate the parameters, take the hash of the resulting string and get only the first few characters. The StackId should be included, so that different stacks have different keys. Failing to do so will make copying and deploying the same template problematic. Then the LogicalResourceId should be also included, so that if two resources in a single template have the same properties they won’t clash. This would be a suitable implementation for calculating the random part: The problem with a truly random as opposed to a pseudo-random part is that it makes the function non-idempotent. If there is a problem, CloudFormation may call this function several times, and if it creates different resources you might end up orphaned ones that will be left when you delete the stack. In most cases, hashes are a safer solution. When you have multiple CloudFormation resources that map to the same underlying resource, deleting one of them will delete the resource for all of them. This is a situation that is very hard to recover from. Also, if you rename a resource in the template, CloudFormation will issue a delete, easily resulting in the above situation. Now that we have everything for the key, let’s construct it: Now that we have everything in place, let’s see how to handle the 3 lifecycle events: Create , Update , and Delete . Since there is no way to rename an object, Create and Update works the same: Delete , on the other hand, issues a deleteObject : The PhysicalResourceId is what identifies the object itself. In S3, the bucket and the key must be unique, so this is a good candidate for the id: Since the key is calculated, it should be added as a return value. To achieve this, add a Data: {Key} to the response body. Then this can be referenced in the template, allowing other resources and stacks to dynamically link to the object: In this post, you’ve learned how to use custom CloudFormation resources to add support for S3 objects. While this is a simplified implementation that does not support all aspects of S3, it is a robust implementation that can be a baseline that you can adapt to your specific use-case.", "date": "2019-01-01"},
{"website": "Advanced-Web", "title": "How to manage custom CloudFormation resources with Lambda", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-manage-custom-cloudformation-resources-with-lambda/", "abstract": "CloudFormation is a powerful tool, and has broad support to other AWS services. Essentially, it is the main part of the infrastructure-as-code concept within the AWS environment. It is great and supports many services out of the box. But what if you need something that it has no support? This is when custom resources come in handy. They let you define a Lambda function that handles the lifecycle of the resource, be it any type, even outside AWS. Let’s see how to make it work and a minimal config that you’ll need to get started with custom resources! To define a custom resource, use the prefix Custom:: and give it a name. It can have parameters and outputs, just like any other regular resource: Until now, it looks like a regular resource with a logical name (resource name) of Custom and a type of Custom::CustomResource . Let’s see what you’ll need to make a Lambda function manage its lifecycle! On the CloudFormation side, you’ll need a Lambda function and an execution role for it. This part is the same as any other Lambda function. The first part, the execution role defines the trust policy that allows the Lambda service to assume it, as well as a set of policies the function has access to: This part is standard for all kinds of functions. Then we need the Lambda resource itself, defining the Runtime, the Role, the Code, and the Handler: As the code is defined as a local path, you’ll need to run aws cloudformation package ... to zip and upload the code to S3. Until this point, this is exactly how you’d define any Lambda function. To connect the function to the custom resource, pass its Arn as the ServiceToken parameter: This is the part that wires the resource to the function. Now that we have everything set up on the CloudFormation side, let’s move on to the actual code. An important concept is that the resource handling is asynchronous . The function gets a responseURL and it needs to send a PUT with some required parameters. This signals CloudFormation that the resource is ready (or there is a problem). At the bare minimum, you’ll need to send the request with these parameters: For the full code on how to send the request and provide all the parameters, see here . It is based on the AWS sample code . Now that everything required is in place, let’s see how to handle each lifecycle transition! When the resource is created, for example the first time you deploy the template, it gets a Create event. In this case, the event looks like this: The \"RequestType\": \"Create\" is what makes it a Create event. Also, take a look at the ResourceProperties section. It contains the Properties from the CloudFormation template. The !Ref -s and other functions are resolved by the time the Lambda is called, so if you referenced other resources in the template, you can use them here. As an example, this is how you’d reference an S3 bucket. When you remove the resource from the template, the lifecycle event is a “Delete”. The event is almost the same as the “Create”, but the RequestType is “Delete”. Use this to remove any underlying resource that is associated with this custom resource. When there is a change in the template for the custom resource, you’ll get an “Update”. In this step, you’ll not only get the current Properties, but also the previous ones, so you know what is changed. For example, by changing Param2 from val2 to val3 : Use the OldResourceProperties to know what is modified. In some cases, you can simply delete the old resource and handle the event as a Create. This is usually the case for stateless resources that can not be changed. If you send some values with the response in the Data object, those will be available in the template. For example, if you want to send a parameter named Out , use Data: {Out: ...} . Then you can reference it in the template with the !GetAtt Custom.Out function: This is the underlying identifier of the resource. It is important to understand how it works as it can cause a lot of unexpected behavior. As a rule of thumb, it should be different for different underlying resources. For example, if you have a GitHub repository, it should be the name. If you manage an S3 object, it should be the full path. For both cases, the id uniquely represents the resource and makes sure that different ones make different ids. You also need to send it back when you send back the response. Its lifecycle is the following: Let’s see an example! Our custom resource has 2 parameters: Let’s calculate the PhysicalResourceId from only Param1: In this case, changing Param2 updates the same resource, but changing Param1 creates a new one (from CloudFormation’s perspective). This is roughly the case when you have an S3 Object and Param1 is the key, while Param2 is the contents. By chaning Param2 to val3 , there will an “Update”: But setting Param1: val4 , after this “Update”: There will also be a “Delete” with the old parameters: Keep this in mind when you decide on a PhysicalResourceId . What should you use then? It should be different for a different resource, and same for the same resource. If you manage an S3 Object, it can be the bucket and the key of the object. If the resource is a GitHub gist, use the id returned from GitHub. Custom resources are powerful additions to CloudFormation. They open support for all sorts of resources, and not just what it supports out of the box. Handled with care, you can define any resource type, even third-party ones. They require some boilerplate, especially considering the Lambda function, and should be tested extensively. But using them gives you the freedom to define anything. Want to create a Mailchimp campaign from CloudFormation? Or a GitHub repository? A Slack channel? These are all possible, and they can be made first-class citizens.", "date": "2018-12-25"},
{"website": "Advanced-Web", "title": "Customizable monitoring script on AWS Lambda", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/customizable-monitoring-script-on-aws-lambda/", "abstract": "I have a few applications that I operate and monitoring them is a basic requirement. I utilized a script running on an EC2 instance in a Docker container that pinged the services and pushed some values to CloudWatch. Then I added some alarms to let me know if there is a problem. Even though it was also monitoring itself, it wasn’t a problem as the alarm went off even for missing data. It was working well, but I felt I shouldn’t need a server to run a rather simple script every few minutes. This use-case is the perfect candidate for a serverless application: runs occasionally, and it runs fast. Let’s see how to migrate it to a serverless deployment! For a website, common candidates to are the response status and the response length. I chose the latter and since an error response is usually much smaller than a normal one, I can set up an alarm based on that. But there are more complicated scenarios: I wanted a script that can be deployed serverless with minimal effort, and can adapt to support a wide range of use-cases. The monitoring script is based on NodeJS and as such, it can use all the rich ecosystem of npm. First, clone the repository from GitHub. The main functionality is in the handler.js . You’ll find everything there to make it work. As an example, the provided handler gets the response length of google.com and puts it to CloudWatch: By adding more functions that retrieve values from various sources, you can implement any monitoring you’d like. For local development, call the functions that produce the values, without calling the CloudWatch code. This way by the time you finally deploy you’ll have the critical parts tested. The script uses CloudFormation to deploy everything in one step. This is particularly useful, as even this simple Lambda function requires multiple resources. By packaging everything inside a template file, all the boilerplate is handled alongside the main functionality. The provided template calls the function every minute, but can be configured. The last important part is the policy of the Lambda execution role. In the sample template, it has access to the CloudWatch Logs in order to enable logging, as well as to put data into CloudWatch. Don’t forget to add more permissions if any is needed. As with most CloudFormation templates, you need some setting up: Since the code must be uploaded to an S3 bucket, the first step is to install all the npm dependencies and package the template. Use the script below, just fill in the bucket name where the AWS CLI will upload the lambda code. Make sure that the bucket is in the same region where you’ll deploy the template. To deploy the template, use this script, and fill in the stack name: After the deployment is successful, you’ll see the values appearing on the CloudWatch console. You can then go ahead and define alarms that will trigger when there is a problem. This script is a simple solution to monitor your resources in a highly versatile way. You can define any custom code that produces a value to put into CloudWatch, so that it can easily support most use-cases. Deployed with CloudFormation also ensures that all the resources required to perform its function are handled together.", "date": "2018-12-11"},
{"website": "Advanced-Web", "title": "The anatomy of a CloudFormation template with a simple Lambda function", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-anatomy-of-a-cloudformation-template-with-a-simple-lambda-function/", "abstract": "Managing a Lambda function with CloudFormation is a non-trivial task. You need to define several resources to make it work. I’ll use this monitoring script to show the building blocks of this simple function. The functionality is rather simple: have a function and automatically call it on a scheduled rate. Why use CloudFormation? When you need multiple resources for a single functionality, without a centralized descriptor they remain unrelated. Defining and handling them one-by-one makes it hard to reproduce the functionality, and harder still to clean everything up when they are not needed. With a CloudFormation template, you can deploy/update/remove all of them together. Resources are the main building blocks of any CloudFormation template. As a rule of thumb, anything that you’d define on the Console manually is one Resource. The function is the main functionality. To define it, we need a resource of type AWS::Lambda::Function : Use the Properties block to define the Code , the Runtime , and the Handler (this defines which of the exported function to call): The Code is referenced in a local path, but for deployment CloudFormation requires it to be in S3. But since managing the code in a remote location is burdensome (not to mention it’s easy to accidentally overwrite existing versions), the AWS CLI provides a package function to do it just before deployment: This command zips then uploads the code, replacing the Code block with the S3 URL in the process. The resulting template is written to the output.yml file, as per the --output-template-file argument. This can be uploaded to CloudFormation directly. Now that there is a function, we need a resource to invoke it regularly. The AWS::Events::Rule is our friend here: Since we need to reference which function to call, we need a way to get the Arn (the resource identifier) of the LambdaFunction resource. To get a parameter of a resource, use the !GetAtt function: How to know which properties are supported and required for a given resource? Consult the documentation . There, you can see what you need to define, and also what return values you can reference with the !GetAtt . Especially, look at the examples provided by AWS to figure out how to use a given resource. The two resources above implement the main functionality. But we need another two, first, a permission to invoke the function, and second, an execution role that the function will use. To give permission to the Events Rule to call the function, we need an AWS::Lambda::Permission resource: Same as before, we need to reference resources defined in the template for the FunctionName and the SourceArn properties. The Action defines the scope of the permission ( lambda:InvokeFunction ), and the Principal defines the entity to which the permission is granted. In this case, the Principal is events.amazonaws.com as the function will be triggered by an event. If, for example, the Lambda were triggered by an S3 event, the Principal would be s3.amazonaws.com . The other permission we need is the execution role. Contrary to the previous permission, this is a role that the Lambda function will assume when run. You can define what other AWS resources it has access to. It has two important parts. The AssumeRolePolicyDocument is the so-called trust policy . This defines who can assume this role. For our Lambda function to work, we need to allow the lambda.amazonaws.com service to do the sts:AssumeRole action: This part is mainly constant, you’d use this for any Lambda function. But wait! Where is it specified that this role can only be assumed by this specific function? It is not. In effect, this role can be assumed by any Lambda. You need to be very mindful whom you give access to the iam:PassRole action as that is what controls which roles can be specified by each user. See this article for more info. The Policies part defines what the function can do: This policy allows the function to write its logs to CloudWatch Logs. Since logging is usually an important part, you want to include this statement in every Lambda role. If you need to give access to any other service you need to add those here. And finally, associate the role with the Lambda function: To make parts of the template parameterizable, you can use the Parameters section. For example, if you want to make the interval configurable, you need to define a Parameter: And to access it, use !Ref <parameter> : Now that everything is in place, let’s see how you could deploy the template. First, you need to run a package that zips the local Lambda code, uploads it to S3, and modifies the template file accordingly: Make sure to use the same region for the packaged code as you’ll use for the template itself. When you have the packaged template, deploy it: The --capabilities CAPABILITY_IAM is required since the template creates IAM resources, and the CLI issues an error if you don’t explicitly acknowledge it. Run it, wait a few minutes, and you have a function that is run every minute. Writing a CloudFormation template is a time-consuming process. You might want to use some sort of a designer or a framework to generate it for you. But knowing how these templates work under the hood come handy when you need to debug a problem.", "date": "2018-12-18"},
{"website": "Advanced-Web", "title": "How to secure different types of CloudFront origins", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-secure-different-types-of-cloudfront-origins/", "abstract": "When CloudFront goes to the origin to fetch a content it behaves like any other third-party service. In effect, by default, whatever CloudFront has access to, everyone else has too. Usually, this is a problem. How to protect the origin so that only CloudFront has access and can not be accessed directly? It depends on the origin itself. This is the most straightforward case, and it is the most likely to pop up when you search for serving private content. You have a private bucket, but you want CloudFront to have access to it. This is the only usage of the Origin Access Identity, which is a principal that you can reference in the bucket’s resource policy. This is how to make it work : You create an OAI, assign it to the distribution, then update the bucket policy. This gives access to that distribution while also keeping the bucket private. This is when you have an API and you want to restrict access only to the distribution. API Gateway supports restricting access by API-keys, which means there is a header that must be sent along with the request. CloudFront can set a custom header when it accesses the origin so that it can send the x-api-key with the secret value. The API is kept private, as it requires the secret, but when accessed through CloudFront the request goes through. VPC security is usually done by security groups, and you can configure them to allow only CloudFront IPs. There is an official list of IPs that the service use and also a notification so that you can handle the future changes to the list. Actually, there is an official script to manage the settings of a security group to only allow CloudFront IPs. It seems good, but unfortunately, this is not a good solution. It makes sure that only CloudFront can access the instances, but does not limit it to your distribution . In effect, any CloudFront distribution can access it, even outside your account. Which means, anybody can create a new distribution, point it to your instance, and have access. If you have an ALB, then you can configure WAF to require a custom header to be present in order to allow the request. Then add that header to the distribution, and you are all set. The downside? Cost. WAF has a fixed monthly cost, and for small applications, it could make the majority of the bill. See here for a guide on how to set it up. If you have instances that you can configure, you can implement an API-key-like functionality. See the next section for the details. When you have a custom backend, be it Apache, Nginx, or a NodeJs server that you can configure freely, then the easiest way is to implement the API-key-like functionality. Set a custom header on the CloudFront distribution with a secret value, then configure your backend to only allow requests with that header set. This way, you give access to CloudFront, but forbid direct access. The downside is that your backend will receive the direct request and need to handle it. It can result in increased server load, and in some cases, might allow easier overloading attacks. Limiting public access to your services is a good practice, and if you have CloudFront, restricting direct access is desirable. It is especially important when you utilize signed URLs, but even if you don’t, you should do it this way.", "date": "2018-12-04"},
{"website": "Advanced-Web", "title": "How S3 Signed URLs work", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-s3-signed-urls-work/", "abstract": "Let’s say you have an object in an S3 bucket which is set to be private (i.e. no anonymous access). Then you want to share it with people who have no AWS accounts, for example, subscribed visitors to your website. This can be a video course that only paying users can access, or an EBook that requires subscription. Making the object public would open it up to anybody, but keeping it private denies all non-AWS users. Bucket policies do not help either, as the users are not IAM users. How to solve this problem? This is where signed URLs come into play. You sign the access URL on your backend, and distribute that only to the subscribed users. By making the signature valid for only a short period of time, you can also prevent reusing the same URL. This way, you have explicit control over who has access to the resource. Let’s see how this works code-wise! As a test setup, I created a bucket called pres-url-test and uploaded a test.txt object to it. I also made it private (no anonymous access) but added an IAM user that has access via an IAM policy. As everything is set up, I grabbed the access and the secret key of the IAM user and used this code to generate a signature: This code uses the AWS SDK, which works from both the browser and NodeJS. In the former case, you can include the SDK via a script tag, like this: <script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.1.12.min.js\"></script> . In case of NodeJS, use NPM and install from here: https://www.npmjs.com/package/aws-sdk . One common misconception about signed URLs is making them requires a request to AWS. In reality, they work fully offline , so you can sign any bucket and object with any access and secret keys, there is no check that the resulting URL will work. You need to provide the region , the keys , the bucket , and the object key for the signer. Optionally, you can also provide an expiration time , where the default is 15 minutes. A resulting URL looks like this: Using that URL opens the file even for anonymous users. This works by signing an operation , in this case, this is the S3 getObject with the bucket and the object key as parameters. You can sign other operations too, for example PUT allows uploading new objects. If you look at the URL, you can find the Access key, but the Secret key is only used to generate the Signature part. In this regard, the access/secret key par is not like a username/password. The signed URL on itself does not give access to the object in the S3 bucket, but it sends the request as the signer user . Effectively, it works the same as if the signer issued it. So the presigned URL is only effective if the signer user has access to the action ( getObject ) and the resource (bucket + key), therefore, if the permission is revoken later, the signed URL stops working . But if the permission is restored after that, it will be working again. In short, the signed URL is not guaranteed to work until it expires, and in fact, there are no guarantees that it works at all. It is all dependent on the permissions of the signer user. This brings us to the problem of revoking signed URLs. The signature itself can not be invalidated, you need to wait until it expires. But you can revoke the signer user’s permission to the object. If you find yourself in the situation when a signed URL is compromised, revoke the permission. But this will prevent all signed URLs from working that were issued by that user for that resource. Which is suboptimal, but at least you can mitigate further damage. For the getObject operation, which is what you’ll use for distributing files, you can set the bucket and the object, along with the expiration time (defaults to 15 minutes). I couldn’t find reference what is the maximum expiration time, but you shouldn’t use long ones. Because of the way permissions are checked, you should use a separate IAM user that only has permissions to view the files you want to share. Usually, you’ll have a bucket with all the protected files, so the IAM user will have access to read the whole bucket. Use the least privilige . You need to protect the secret key . You can sign an URL in the browser, but sending the access+secret keys to the users defeats the whole process. Use a server-side signing endpoint , like a Lambda function that checks if the user has access to the resource (i.e. he is a subscriber). URLs should expire as soon as possible . 15 minutes is the default, and it should be more than enough for most use cases. Sign the URL just before the user needs it, and that way you don’t need to worry about expiration. For example, when the user wants to download a protected resource, generate the URL when he clicks on the Download button. Presigned URLs provide a solution to constraining access to private content contained in S3 without the need to stream it through your backend. Using them relieves your backend from having to distribute large files. But this is essentially a security feature, and as such, you need to be careful how you implement it. Know how they work and follow the best practices.", "date": "2018-10-30"},
{"website": "Advanced-Web", "title": "How to handle mixed content with CSP", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-handle-mixed-content-with-csp/", "abstract": "Mixed content happens when some resource on a page is loaded via HTTP, while the site itself uses HTTPS. It’s a problem because that means there are files requested on an unencrypted channel, which breaks the consistency guarantees of HTTPS. And also the browser shows your site as insecure, which is usually what prompts people to start doing something about it: If your site is https ://example.com, then an image tag like this: <img src=\"http://other.com\"/> makes the content to be mixed. If you only use https:// URLs, there is no mixed content. We write articles like this regularly. Join our mailing list and let's keep in touch. CSP (Content Security Policy) provides two ways to handle this scenario. Either instruct the browser to block the unencrypted content, or auto-upgrade them. Let’s see what is the default behavior, and how each CSP setting works! The browser makes a distinction between active and passive content. Active is something that runs, like a <script> , while passive is something that is only shown, for example, an <img> . Browsers, by default, block active mixed content but allow passive ones. While a sophisticated attacker can deceive even with a suitable image, modifying scripts can wreak havoc to a page without much effort. But any kind of mixed content is reported by marking the site not secure. One exception is localhost , where all mixed content is allowed. Keep that in mind when testing locally. With this policy directive, the passive mixed contents will be blocked too: This is propagated inside IFrames too, so this header prevents the mixed content warning altogether, but at the expense of breaking the site if there are unencrypted contents. If you want to upgrade all http resources to https ones, specify this header: In practice, it rewrites mixed content such as an <img src=\"http://...> to <img src=\"https://...> . This takes care of rewriting resource paths, but will leave references like an <a href=\"http://...\"> . Of course, you need to make sure all assets are accessible on the same URL with https . Taking advantage of the reporting feature of CSP, you can also detect mixed content on your site. This works by forbidding unencrypted connections, but only report the violations instead of blocking them. When the users crawl the content, you’ll have a report of all the mixed content problems. The only issue is that block-all-mixed-content does not work with a report-only directive. So you’ll need to whitelist https and not http . Use a header similar to this one: You can also combine the last two approaches to provide better user experience while also getting the list of mixed content. The following policy reports the mixed content, while also upgrades to https : This provides an immediate fix for mixed content, while also gives you the ability to fix the root cause. CSP is effective inside IFrames too. If your site blocks all mixed content, IFrames are no longer able to load resources from http . But it also promotes the upgrade-insecure-requests too, automatically rewriting all insecure URLs. For a scenario when the IFrame itself comes with a header that blocks mixed content, the auto-upgrade from the page takes precedence. In effect, resources will be loaded over https instead of being blocked. Scott Helme’s Report URI is an excellent service for aggregating the reports for CSP. It takes a few minutes to set up, then you don’t need to provide any infrastructure yourself. Using an external service also helps to avoid DDoS-ing yourself. These reports can be numerous, and all your users will send them. Sending them to your server directly could easily overwhelm it. Mixed content indicates a problem on your site, and you should fix the root cause as soon as possible. A stopgap solution is to set up a CSP that auto-upgrades all requests and reports them. This way, you can fix all problems eventually, all while providing a secure browsing for your users.", "date": "2018-10-23"},
{"website": "Advanced-Web", "title": "How I failed to implement CSP", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-i-failed-to-implement-csp/", "abstract": "I keep hearing about CSP (Content Security Policy) and how great it is. It offers fine control over what is allowed on the site and what connections can be made by the users’ browsers. Its primary purpose is to give a tool to developers to prevent malicious scripts from doing any damage and to limit what trusted ones are allowed to do. It’s great in theory and I was sold from the moment I first read about it. The next logical step is to implement it for this site, which should be easy, right? While I could’ve written a CSP policy by hand, I decided to use Scott Helme’s report-uri CSP wizard which can generate it for me. We write articles like this regularly. Join our mailing list and let's keep in touch. The service utilizes two features of CSP: The wizard then collects the reports and lists the rules needed. After deployment, just wait for the users to crawl the site and you’ll have a policy made for you that allows everything that is needed. This sounds the easiest way to get up and running. After running the wizard for a few days, I got a ton of such rules (99 and still counting): These rules are obviously not required by this site, but the visitors’ browsers’ loaded them while browsing. What are these? These are browser extensions that load some additional stuff on every page. Should I allow them? Nope, these are false positives. But having them clutter the wizard makes it hard to spot what is really needed. For example, at one point code.jquery.com appeared on the list. Should I allow or block it? I don’t think I added a feature that loads jQuery from a remote site, but there is a chance I forgot something. Blocking it would then break a feature. But it’s more likely to be a false positive. You might have noticed that there is Facebook chat below the posts. It is loaded in an IFrame, and it loads the resources (images, scripts, styles) it needs. What rules are needed for Facebook chat to work? I could just allow everything that seems related to Facebook, or I could just inspect what my browser reports when I open a page. There are two problems with both approaches. First, can I be sure that the same rules are required for all users? What if Facebook uses different domains for some users, for example if they are located in a specific geographic region, or have a setting that provides them a different experience? And second, how do I know if the rules required are constant? What if Facebook decides to use a new CDN URL or loads some scripts from a different domain, which in turn breaks functionality on my site? There is a similar security feature, called SRI (Subresource Integrity). In a nutshell, it works by defining the resource’s expected content’s hash and the browser refuses to load if the content does not match it. For third-party dependencies that do not use versioned assets, you can not be sure they don’t change it without notice. Hardcoding the hash would break functionality in that case. CSP is great for sites that control everything they serve. In that case, after some trial and error, the required rules can be collected and since they don’t change unless the site itself changes, they won’t break functionality. But as soon as you include something from a third-party, this premise breaks. If a security feature breaks functionality, the first reaction is to disable it. With CSP, while you control what third-parties can do on your site, you also open the possibility for them to break it. After the first 3 a.m. call that “something is broken”, you’ll value reliability much more than security.", "date": "2018-10-09"},
{"website": "Advanced-Web", "title": "How to securely check your LastPass vault against the Pwned Passwords database", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-securely-check-your-lastpass-vault-against-the-pwned-passwords-database/", "abstract": "Ever since I saw that there is a database called Pwned Passwords in which everyone can check if a particular password is leaked, I wanted to know if any of mine is compromised. Unfortunately, while LastPass reports if I have accounts that are affected in a leak, it does not check the passwords themselves. Since I can export my LastPass vault as a csv file, it was only a step forward to write a one-liner that runs the check for every password contained. It’s Linux only, and while I’m not a bash expert, I believe it does its job well and is built on top of simple programs. We write articles like this regularly. Join our mailing list and let's keep in touch. It might not handle all edge cases perfectly, especially when the password contains a comma, but that should be rare. The database itself is run by Troy Hunt . He uploads leaked passwords from known data breaches and maintains an API where you can securely check if a given password is in any of the data sets. If it is, then do not use it, and if you are providing a service, you should prevent your users from using it. How can such a service work without sending the password itself? When you check a candidate, instead of sending it directly, you send the hash of it. This way the sensitive information never leaves your computer. First, you need to export the LastPass vault using More options -> Advanced -> Export -> LastPass CSV File , then copy-paste the contents to a text editor and save it. As for the location, it’s best to choose an ephemeral place, like /tmp , as it will get wiped automatically. For the name, I’ll use fp.csv in the examples. You’ll need some command-line tools that might not be available by default. Install them if you don’t have them already: Then open a terminal, navigate to the location of the file and run this script : It takes a minute or two, but it will output every compromised url-password pairs. Finally, don’t forget to delete the LastPass CSV : Change the compromised passwords. I’ve found this gist that provides a similar functionality, but using Python. For one-time tasks like this I prefer a one-liner bash script rather than something I need to download and run, but it should be functionally equivalent (and might handle some edge-cases better). To check a password, you actually check the SHA-1 of it, so no secret is transferred plain-text. Also, the API uses a so-called k-anonimity model, which in a nutshell works like this: when querying a hash, you supply only the first 5 characters of it, and get back a list of all known hashes with that prefix. This makes it unlikely for the service provider to know if a compromised password is found in the database. Personally, I find it fascinating, read more about it here if interested. As you should not run anything on your passwords you don’t understand, let’s break down the script, step by step. Read the csv, get the url and the password fields, skip the first line (which is the header), then filter all the rows without a password (for example, secure notes): Iterate over all rows: Calculate the hash: This might need some more explanation. The echo $line | rev | cut -d, -f1 | rev outputs only the password (remember, the url and the password is separated by a comma). Then the sha1sum produces the hash itself, but with some extra data. The awk part cuts that off, and the tr converts to all uppercase, as the API returns everything that way. Make the call to the API: The echo $hash | cut -c1-5 returns the first 5 characters of the hash, then the curl fetches the result. The last part, || echo makes sure that an error is printed if there is a problem with the call. Find the hash without the prefix in the result: And finally, check if the grep returned any matches, in which case the hash is present in the database. The echo \"${line}\" prints the url-password pair in that case:", "date": "2018-10-02"},
{"website": "Advanced-Web", "title": "How to compare file revisions with fugitive.vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/how-to-compare-file-revisions-with-fugitive-vim/", "abstract": "With git-log it’s easy to quickly skim history in the form of patches, but it’s a bit harder to have a quick look at different revisions of a file or to see the difference between two arbitrary versions. fugitive.vim provides tools to do just that. Glog is a wrapper around git log <current file> . Upon execution it shows all commits that affect the opened file, then it loads the latest revision into the buffer. It also populates the quickfix list with the latest revisions of the file. There are multiple options to investigate the file history. We write articles like this regularly. Join our mailing list and let's keep in touch. You can use copen right after Glog to open the quickfix window where you can select any version to load it into the buffer. As an alternative, you can use cnext and cprev to cycle through the revisions of the file. The revisions are presented in a read-only buffer. You can return to the working copy of the file with the Gedit command. Some tips: After you’ve selected a revision, you can easily compare it to any other version with Gdiff . Use Gvdiff instead of Gdiff to open the diff vertically aligned. fugitive.vim nicely augments the command-line git workflow. Its commands are oriented around the currently edited file, which makes it handy for many tasks, for example checking file history or revision information .", "date": "2018-09-25"},
{"website": "Advanced-Web", "title": "Why certificate pinning with HPKP is a bad idea", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/why-certificate-pinning-with-hpkp-is-a-bad-idea/", "abstract": "HPKP (HTTP Public Key Pinning) is an HTTP header that lets you pin an HTTPS certificate so that returning visitors only accept a chain that contains that. If an attacker gains control over a valid certificate, even if he manages to send that to your users, their browsers will refuse to load the content. It then can protect against rogue certs. Sounds great? Nope, this is a terrible idea. But first, let’s see how it works! The header itself defines the hash of the certificate to pin and a max-age: You can pin multiple certificates, this way you can provide a backup one too. This is by providing multiple pin-sha256 parts in the header. We write articles like this regularly. Join our mailing list and let's keep in touch. There is an optional includeSubdomains option which enables the header to affect the subdomains too. In effect, you must use the same certificate in the chain for all of them, or at least one of the pinned ones. You can add more certificates to the header at any time if you need to, but keep in mind that you need to wait the max-age to expire to be sure that no visitor have the old one. And also you need to make a more thorough testing, as a user may or may not have visited the domain with the header before. As with other security-related headers, it can do automated reporting on violations. This works by specifying a report-uri part with an URL, and the browser will send a POST request when a violation occurs. It offers an automated way for you to know something is wrong. If you want reporting only, use the Public-Key-Pins-Report-Only header. This does not prevent loading, but tells the browser to send the report. At least you know something is wrong without affecting the user experience. Use an intermediate certificate instead of the final one. This way you can still rotate without modifying the header. Also make sure that its validity exceeds the max-age of the header. Sounds great so far? Nope, it’s actually a terrible idea and you shouldn’t use it and even regret that some browsers actually support it. At the time of writing, only Chrome and Firefox supported it , with no signs of other vendors jumping in. So, what are the problems with it? First, if you ever lose the cert , you’ll prevent users from accessing your site. Depending on the max-age, this might be a catastrophic failure that could wreck your business. Also, if an adversary gets to know the private key, you can not revoke and replace the certificate, as the users continue to expect that to be present in the chain. The gist of the problem is that you can not really predict if you’ll need to replace the cert. If you do need, either because you lost it, someone stole it, or the CA got distrusted (hello StartCom SSL), you won’t be able to. Sounds like a calculated risk, but business-wise it’s too great. Hardly any means can justify this. And finally, it does not solve the first request problem, as new visitors still accept any valid certificate. The problems described above are something you have control over. You chose to use the header, and you bear the consequences. But the mere existence of the standard opens ways for an attacker to hurt you. What if someone gets access to your site and manages to insert an HTTP header? In this attack, the adversary pins a certificate with a long max-age and forgets the key. In effect, it is a nuclear option to take a site off the Internet. Similar to the suicide, but instead of forgetting the key, he demands ransom for it. Unless the owner of the domain pays and retrieves the pinned certificate, return users are blocked. And even after that, the attacker still owns the keys, compromising security. And you, as the owner of the site, can do nothing about it. Because of the above problems, the header is currently being deprecated. Chrome already caps the max-age to 60 days , and will remove support altogether soon. I suspect Firefox will follow suit too. I couldn’t not notice how often Scott Helme’s blog came up during my research for this article. His insightful posts on web security helped me a lot in understanding the big picture of the technologies and I can’t recommend it enough. HPKP is an excellent example of a security feature that sounds great first, but ultimately does more harm than good. You don’t want to use it, as it brings unacceptable risks for most businesses, and even it’s optional, the support from browsers opens new types of attacks.", "date": "2018-08-28"},
{"website": "Advanced-Web", "title": "HTTPS security best practices", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/https-security-best-practices/", "abstract": "The single most important tip regarding HTTPS is to use it. It really should be a no-brainer, and as browsers (and search engines!) are moving to discriminate non-encrypted websites, the incentive is even bigger. If you don’t use HTTPS yet, look up Let’s Encrypt and start using it. Mixed content is when your site is available via HTTPS, but uses some resources that are loaded via HTTP. It’s easy to check your site for mixed content: the browser makes it very clear that something is wrong. This usually happens when you hardcode http:// URLs, then migrate your site to HTTPS. In this case, there are no problems before the migration, as the contents go unencrypted, but results in mixed content after. We write articles like this regularly. Join our mailing list and let's keep in touch. Use https:// URLs in the first place if they are available, or just leave the scheme out and use // . That instructs the browser to use the same scheme as the page, i.e., http:// for HTTP, and https:// for HTTPS. HTTPS is not something with a binary state, so it’s not enough to just turn it on. There are many configuration options that affect various aspects of the encryption itself. Fortunately, there are some sites that test your config and provide recommendations how to fix some problems. One such is SSL Server Test from Qualys, which runs a robust set of checks. Be sure to check out your HTTPS config from time to time, as new vulnerabilities and best practices can emerge. There are also several HTTP headers that control aspects with security implications. While not all of them are related to HTTPS, several are. One site you can use to check yours is the Security Headers , which offers descriptions why a particular header would be important. A recent addition to the certificate issuance process is the so-called Certificate Transparency. It means that whenever a certificate is issued for a domain, it must be submitted to a public log. In effect, you can see all the certs for your domain. The best part is that you can even subscribe to notifications and get an email when a new certificate is issued. Again, there are several such services available, for example, there is one from Facebook A common misconception is that if you don’t use HTTP for anything but to redirect to HTTPS, then you are secured. But if an adversary intercepts that initial HTTP request and can modify it, he can serve mailicious content instead of the redirect. Therefore, the first request is still vulnerable. There are headers that can mitigate this, and we’ll cover them too. But first, let’s focus on the case when they are not used. If an attacker can modify the request, then there is little you can do (apart from HSTS). But usually it’s more likely he can read it but not modify it. To prevent an attack when an attacker can listen in to the traffic, there are a few best practices. When you redirect to HTTPS, don’t send any content along with the redirect. Any text you send is sent in plain text, so it’s best to minimize it. Sending the content plus the redirect (for example, the home page) is no good. Any cookie that is not marked as secure will be sent via HTTP as well as HTTPS. That, in turn, is something an attacker can use to impersonate the user on the HTTPS site. Make sure you use secure cookies . Yes, most of the time. The browsers, by default, request the HTTP site first, so you need to support that. There is an exception though. If you have an API endpoint, then you can (and should) disable HTTP altogether. Why? Browsers follow redirects, but API clients might not, or might redirect a POST as a GET. You don’t want some clients to work and some don’t. Also, for API clients, you provide the scheme by hand, so any consumer can use HTTPS only . Well, considering the things described above, it paints a worrysome picture. No matter what you do, the first request will be vulnerable. Fortunately, the HSTS header (HTTP Strict Transport Security) aims to remedy this. It works by indicating that the browser should not use HTTP in subsequent requests but only HTTPS. This is done using a response header on the HTTPS response: In effect, return visitors will be protected even if they try to load the site via HTTP. This part controls how long the header is effective. After this time, the browser will forget the header and request the HTTP site again. It is renewed every time the user visits the page. It is there to provide an escape hatch in case you want to roll back HTTPS. 604800 is a week. If you use this, regular visitors will be protected continuously. 2592000 is one month. 31536000 is a year. You shouldn’t use more than this. It’s long enough, and for example, Chrome caps it to one year , so there is no point setting it any longer. If you specify it, subdomains will be protected too. For example, if you send the header for example.com : Then it will be effective for example.com , www.example.com , secure.example.com , www.secure.example.com , and all the other subdomains. Should you use it? Well, it depends. It seems like a great thing, but can cause problems. For example, http://sub.example.com may work for some users but not for others, depending whether they have visited example.com before. A user who got the HSTS header will request only the HTTPS site, while others will happily fetch the HTTP one. If this subdomain does not support HTTPS, it makes a difference. Also keep in mind that if you set it for a domain, you lose the ability to not support HTTPS for all subdomains. Your only escape plan is to wait for the header to expire, but that might take a long time. The problem with HSTS is that it only protects returning visitors, but the very first request will still be vulnerable. There is a list of domains that browsers know to serve the HSTS header without visiting them first. Google maintains such a preload list , that is included in Chrome and other browsers. This built-in preload list solves the problem with the first request, as if your site is on that list, the browser does not even try to fetch it via HTTP. To get on the list, you need to send the HSTS header: This solves a particularly hard problem, but you need to excercise caution. Getting removed from the preload list is non-trivial, and you still need to wait for the header to expire (1 year!). On the other hand, setting up HTTPS is not that hard, and services like Let’s Encrypt make it easier. You should opt in, but in small increments. Use a small max-age and work your way up 1 week should be more than enough for a start. Run your site for some time and increase it to 2 weeks, then a month, then several months. When you reach 1 year, add preload. Use includeSubDomains Enforce HTTPS on subdomains early on, and check all the domains. Don’t disable HTTP redirection Even if you get on the preload list, you still can not disable HTTP. Browsers that don’t support HSTS or the preload list will still try to load your site via HTTP by default. Those would stop working if you disable it.", "date": "2018-08-21"},
{"website": "Advanced-Web", "title": "Dive into Git history with fugitive.vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/dive-into-git-history-with-fugitive-vim/", "abstract": "I prefer to use Git via its command line interface, but some tasks just feel too cumbersome to do solely with it. Lately, I needed to use git-blame a lot, but it’s not especially handy to use. Because I use Vim a lot, it made sense to look into the features of fugitive.vim and found Gblame that provides multiple ways to quickly find relevant bits of history. The git-blame from the git CLI opens a new pager to show the revision information and the source code side-by-side. Most of the time I need this information when I am already looking at a given line in another editor, wondering why things work the way they do. Leaving the editor and browsing the whole source file in a different presentation is tedious. It also lacks syntax highlighting, so you’ll end up looking at a totally different representation of the source file as before. We write articles like this regularly. Join our mailing list and let's keep in touch. Another problem is that if you’d like to dive into a commit, you have to type or paste its ID for the next command. Luckily, fugitive.vim provides the Gblame command that makes life a bit easier. It shows the revision information right next to the file in question in a panel that’s linked to the source file. You can simply jump to the commit that contains the changes for a given line. Looking at the other changes and the commit message it’s quite easy to get the context. Some general tips: So it’s convenient, especially if you already have a Vim open. But there are more reasons to use it: you can easily dive deep into the history. Supposing that you saw a suspicious line in the blame information, and you would like to see a version of the file where that change was committed. You can do just that by hitting - after you’ve selected a commit on the blame window. Another useful feature is to be able to see the evolution of a given line over time. By pressing P you can reblame the file on the parent of the selected commit. This updates the contents of the buffer with the previous version of the file. Both commands keep the blame tab open and refreshes the information therein, so you can continue to investigate the commits that resulted in the selected version. fugitive.vim is a powerful Git wrapper. I’ve only started to scratch its surface because I only needed a more handy way to interact with git-blame Once you start using it, make sure to read its help page ( :h fugitive ), because it has a very rich functionality so you can find multiple ways to enhance your Git experience.", "date": "2018-09-11"},
{"website": "Advanced-Web", "title": "How to use Let's Encrypt with Node.js and Express", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/how-to-use-lets-encrypt-with-node-js-and-express/", "abstract": "Unlike Apache and Nginx, Let’s Encrypt has no way of autoconfiguring your Node.js app, as it can work in arbitrary ways, while the former two usually follow a predefined (and machine readable) configuration. How to configure a Node.js Express server to handle Let’s Encrypt HTTP authorization then? As usual, there are several use cases, depending on your current configuration. To start an HTTPS server, you’ll need a certificate and the private key. Read them from the filesystem, and fire up the server. For example: We write articles like this regularly. Join our mailing list and let's keep in touch. This example uses the mz/fs library, which provides Promised filesystem methods. Depending on your requirements, you might want a graceful shutdown handler. As the above server reads the certificate on startup, it won’t use a new one unless restarted. To make sure the connections are not terminated abruptly, you can use a library : Note that it will not automagically handle websockets and long-running connections. If you use any of them, you need to implement some sort of auto-retrying in the client applications. Example If your app does not use HTTP (port 80), which might be the case for API-only endpoints, it is straightforward to configure Let’s Encrypt. If you use the above example with the certificates and the graceful shutdown, you are already set up Node-wise. For certbot, use standalone authorization to get the initial certificate, then start the app: To renew, use the --deploy-hook to restart the app: Example If your app uses both HTTP and HTTPS and you don’t want to modify it in any way to support Let’s Encrypt, then just shut it down during certificate renewal. The downside is a few seconds of downtime roughly every 60 days. If the clients use some kind of auto-retrying, they might not even notice. To get the initial certificate, use the same script as in the previous case: The only difference is that during renewal stop the app in the --pre-hook and start it again with the --post-hook : Example For most of the apps, this is the most likely scenario. When you serve all your content on HTTPS, but still use HTTP just to redirect the visitors. This is the preferred way for public-facing websites, as browsers still request the HTTP site first. If there were no redirect, users would be welcomed with an error page. Also, in most cases, there is a public folder that is served as-is. This is usually where the assets, like the CSS, JS, HTML, and image files are. This setup allows Let’s Encrypt to use that public library for the HTTP auth. During that, it places a file into the directory, and a remote auth server tries to fetch it via HTTP, gets redirected to HTTPS, and the auth will be successful. Node-wise, you need an HTTP server that does the redirect: If you use graceful shutdown, don’t forget to use that for the HTTP server too: Since Let’s Encrypt requires your app to be running even for the first auth, you need to handle the case when no certificate is present. A simple solution uses the pem library to generate a self-signed certificate if none is found: The certbot configuration uses webroot auth, and you need to specify the path of the public folder: The --deploy-hook restarts the app when a new certificate is available. The renew works the same, so you don’t even need to specify any parameters: Example If you use HTTP and you don’t have a dedicated static files folder, then you need to configure a path for the Let’s Encrypt auth challenge. This basically means some kind of public folder, but with a limited scope. This enables Let’s Encrypt to authorize, but don’t expose anything else. To serve a folder under the /.well-known/acme-challenge path, use: The HTTP server config that does the redirect as well as the challenge is like: Since you still use the webroot auth, you need to handle the no-cert case, the same as above. The certbot configs are also the same as above: and All the above cases are based on automatic renewals that also restart the server. In some cases this might not be acceptable, as they happen albeit rarely, still somewhat randomly. If the restarts take a long time, that could cause problems even though they only run roughly every other month. One solution is to schedule the renewals to a part of the day when a restart is acceptable. Another solution is to forget renewals altogether and periodically restart the server in a maintenance window. The restart then updates the certificates before starting the app. In this case, you don’t need to worry about the authorization process and the graceful shutdown. In fact, you don’t need any changes to your server apart from using the current Let’s Encrypt certificate. Just use the standalone script from Scenario #1, and you are all set up: The --keep-until-expiring flag instructs certbot to keep the certificate if it is not near expiry. Usually, this is the right choice, as it prevents multiple restarts to use up your quota. But if you want to make sure that after every restart you have a new certificate, use --force-renewal instead. I don’t recommend it though, as you might easily find yourself rate-limited.", "date": "2018-07-24"},
{"website": "Advanced-Web", "title": "Using Let's Encrypt with Supervisor", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/using-lets-encrypt-with-supervisor/", "abstract": "Let’s Encrypt works great with Supervisor, as it provides easy orchestration and some basic scheduling that the certificate management requires. Configuring it is also not rocket science; just identify the environment your app is running in, and choose a suitable workflow. This can even be done with minimal changes to your app, and in a Docker-friendly way. For automating Let’s Encrypt, you generally need 3 program: A basic supervisord.conf which we’ll customize looks like this: Depending on the authorization flow you use, the exact parameters and when each program start/stop differ. The app is to be managed by supervisor. This is usually the case, but for example Apache spawns child processes and that makes it out of the scope of supervisor. Make sure that supervisorctl restart app and supervisorctl stop app works. We write articles like this regularly. Join our mailing list and let's keep in touch. The letsencrypt program handles the initial certificate. It does not autorestart, as it is run only once. The --keep-until-expiring flag makes sure that if there is an existing certificate, it will use that instead of getting a new one. And finally, letsencrypt-renew runs every day and does the renewals. It autorestarts, but does not autostarts, so it needs a kickoff. This is the easiest case. The app runs neither during getting a certificate, nor when a renewal is happening. The only program that autostarts is the letsencrypt , and it starts both the app and the letsencrypt-renew . Do not use --post-hook or --deploy-hook , as they won’t run if a valid certificate already exists. Instead, use the ... && ... construct, that runs every time after the program is finished. The other part is to configure how to renew the certificate. In this case, during renewal, the app is not running. To achieve this, use the --pre-hook and --post-hook . Since it was not defined, renewal also uses the standalone auth. If you can configure your app so that Let’s Encrypt can use the webroot auth, then you can start it and only need to restart on new certificates. Make sure you handle the case when you have no valid certificates, for example, use a self-signed one. In this case, app and letsencrypt autostarts. Use --deploy-hook to restart the app, as most server software won’t pick up a new certificate automatically. And since the renewal flow works the same, it does not need any parameters. There is one problem with this setup. A race condition is present between the app and letsencryt , as the latter needs the former to run. If your app needs more time to start serving static files, you need to start letsencrypt after that with supervisorctl start letsencrypt instead of autostarting it. Another solution is to use a sleep before letsencrypt , like sleep 5m && ... . But that hardcodes an upper limit, which is fragile. A better solution is to use a script that waits for the server to start, like the wait-on that checks a HTTP service: This will ping the parameter URL and only run letsencrypt when it is alive.", "date": "2018-07-10"},
{"website": "Advanced-Web", "title": "Supervisor with Docker: Lessons learned", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/supervisor-with-docker-lessons-learned/", "abstract": "Docker is a great way to componentize an app along different aspects. But I found that when I still need to start multiple programs in a single container, it is better to use supervisor instead of cron scripts. Use supervisor if: If you have a single process, use Docker to start it. If you don’t need coordination, then separate them to different containers. Also, if you only need coordination as a fixed startup order, Docker already provides that. For a database and an app, they can live inside different containers without problems. Or a monitoring script that runs independently. In those cases, don’t look at supervisor. We write articles like this regularly. Join our mailing list and let's keep in touch. But if, for example, you use Apache with Let’s Encrypt, you need the webserver, the certificate issuance script, and the renewal script. And they require coordination, as they dynamically depend on each other. In this case, supervisor comes handy, and knowing how to configure it will be beneficial. First, install it inside a Docker container: Then provide a configuration file , like this basic one: The important part here is the nodaemon=true , which instructs supervisor to start in the foreground instead of as a service. This lets Docker to manage its lifecycle. By default, supervisor logs into files, but that does not play well with Docker. It is better to use the standard output streams. If you just want to see all log messages, redirect stderr to stdout, and print that: Use this if you don’t need any special handling for the error messages. In this case, any logs, be it stdout or stderr, will be logged as-is. If you use Apache, you’ll soon find out that shutting it down with supervisor seems to have no effect. This is because in order to avoid using root for serving the content, Apache spawns child processes that supervisor has no knowledge about. In effect, supervisor can start Apache, but can’t shut it down properly. To make supervisor to kill the child processes too, use killasgroup and stopasgroup : Also make sure you run Apache in the foreground with -DFOREGROUND The default behavior is that supervisor starts all programs during startup. To prevent this, use autostart=false : Similarly, it tries to restart programs only if they exit with an error. You might want to change it to restart every time, and you can do that with autorestart=true : You can also combine the above two: It will not start the program, but will keep it running. To control the state of programs, use supervisorctl : If you need an easy scheduling without any hard guarantees, you can combine autorestart and sleep : This will run the script roughly every day, but drifts slowly. If you need something more exact, use cron. With these simple tools, you can define more complex flows. To start a program first and run another one after that, you can combine autostart with the && ... construct: To stop an app during the time another one is running, use stop and start : This way, when app1 is running, app2 is shut down.", "date": "2018-07-03"},
{"website": "Advanced-Web", "title": "Let's Encrypt hooks use cases", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/lets-encrypt-hooks-use-cases/", "abstract": "Hooks are the main elements of adapting certbot to your particular use case. I found a few application setups that depending on whether you can support authorizations while your app is running or not, there are different kind of hooks you should use. Learn them and when to use them in this post. There are 3 hooks you should know about, and by using them you’d be able to cover most of the use cases. The pre-hook runs before authorization. That means every time certbot tries to get a new certificate. We write articles like this regularly. Join our mailing list and let's keep in touch. The post-hook runs after authorization. You can use these two to setup and tear down the environment required for getting a certificate. And finally, deploy-hook runs when there is a new certificate. The flow looks like this: In effect, the pre-hook and the post-hook does not run when the authorization is not required. This is the case when the certificate is not near expiry and certbot decides it’s not time to renew it. If you run the renew script every day, then the hooks will only run every other month. The deploy-hook does not run if there is no new certificate. For example, if you use certbot certonly --keep-until-expiring , then you shouldn’t rely on this hook to start your app. In that case, use certbot ... && ... . certbot renew gets the same hooks set for certonly . If all the hooks are the same, that makes a far easier configuration for renewal, as you don’t need to input them again. If you don’t want a particular hook to run for renew, overwrite it like this: ---post-hook=\"\" . This can come handy when the renewal flow is different than getting an initial certificate. In this case, the app starts before certbot runs, and also running during the renewal process. Use the deploy-hook to restart it after getting a new certificate, but otherwise it is a straightforward scenario. As there is no difference between certonly and renew , you don’t need to supply any arguments to the latter: Keep in mind that you need a certificate even before certbot runs. You can use a self-signed one, but this is something you need to handle. In some cases, you don’t want to restart the app randomly when there is a new certificate. But as the deploy-hook can be anything, it can send an email so that you can schedule a maintenance restart. In this case, the app isn’t running during the authorization process, making port 80 available, and thus the standalone auth possible. Starting the app works like this: And the renewal: For the former, you don’t need any hooks, just start the app after it is run: By the time the app is running, the certificate will be there. Two important parts to note here. The first is the --keep-until-expiring , as certbot would prompt for what to do if a certificate exists. This makes sure that if there is a valid cert, that would be used. The other is that there is no deploy-hook here, but the && ... ` construct. This is essential, as the hook would not run if there are no new certificates. For the renewal , use hooks to stop/start the app: This is a combination of the previous two cases. It gets a certificate before starting the app (that is, with the standalone auth), and then manages to renew the certificate with the app running. In this case, the app does not need to handle an initial no-certificate scenario, as by the time it is started a valid one is already in place. In effect, you don’t need a self-signed one. The startup flow: The renewal flow: Getting an initial cert can be done using this script: And the renewal could work like this: For renewal, you can use any auth flow your app supports, but webroot is the most likely. In this case, while the downtime can be smaller during renewals, the certonly and the renew flows are completely different. This makes errors for the latter a lot less apparent, and it’s likely it will take 2 months to see that something is broken. I believe it’s better to use the same auth for both flows, making the configuration easier to understand. For most apps, case #2 is the way to go. The only downside is the short downtime every now and then, but unless you are working in some extremely high-SLA industry, it is just fine. Also, if the app uses some kind of auto-retrying, the users might not even notice it. Also, if you want, you can ditch the renew altogether and schedule a maintenance restart every 60 days. The startup script takes care of renewals too if a certificate is nearing expiry. The only thing to keep in mind that Let’s Encrypt might shorten the lifespan of its certs, so it’s better not to hardcode it.", "date": "2018-06-26"},
{"website": "Advanced-Web", "title": "Working with the system clipboard in Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/working-with-the-system-clipboard-in-vim/", "abstract": "If you are new to Vim, you might be surprised when you try to copy bits of text from your editor to paste them into other applications. I remember I was. Observing from another application, simply yanking a line will seemingly end up nowhere. Of course, there are several options to do this essential task, but there are a few of gotchas along the way. If you use Vim as your primary editor, the most convenient way to copy around is to yank text directly to the system clipboard. Living in the 21st century, you are most likely accustomed to the fact that there is only one clipboard to store the stuff that you Ctrl + c . This is not the case in Vim. Vim has multiple registers where you can store information separately. Each register has a unique identifier. That identifier has to be referred when you wish to store or retrieve information: We write articles like this regularly. Join our mailing list and let's keep in touch. For example, paste to, then copy from the r register with \"ry then \"rp . Conveniently Vim allows to omit the \" and the register identifier, so you can simply use y and p . In these cases, the data will be accessed from the default ( 0 ) register. For our purposes, we need the + register, as it is special in the sense that it’s associated with the system clipboard. The content yanked to it will be accessible to other applications. Similarly, you can use it to paste text copied to the clipboard by other applications. To work with it, essentially you have two options: There is one caveat though, Vim has to be compiled with clipboard support for this to work, and many distros come with vim package that does not have this feature. Check the clipboard support of your Vim with vim --version and look for the +clipboard or +xterm_clipboard flags. If you see either of those, then your Vim supports the access to system clipboard and you are good to go. If not, you better look for an alternative package, like vim-gtk on Debian or vim-X11 on Red Hat Linux. Check out this VimTip for more info on this topic. If you are stuck with a version that does not come with this feature you still have some options. The pasting part is pretty trivial: just jump into insert mode and press Shift + Insert . If you are inserting source code, it might worth enabling paste mode with :set paste to preserve indentation. (You can disable paste mode with :set nopaste later.) Copying can be a bit trickier as Vim might try to handle the mouse on its own. Most terminal emulators have the option to capture mouse interactions rather than passing it to the running application. You can take advantage of that to copy a piece of text. For example with Konsole simply hold down the Shift key then select and copy the text from the terminal, as you would normally do. Of course, if you didn’t enable mouse support, then you can simply copy text from the terminal as usual. As a last resort, you can use :!cat % to pipe the currently edited file to cat , displaying it entirely outside of vim. In this case, it’s easy to copy multiple pages as well.", "date": "2018-06-12"},
{"website": "Advanced-Web", "title": "When to use Let's Encrypt's webroot and standalone authorization", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/when-to-use-lets-encrypts-webroot-and-standalone-authorization/", "abstract": "Probably the two most used custom authorizations when it comes to Let’s Encrypt is the webroot and the standalone . The former is the preferred in many scenarios, but knowing when it falls short is essential when you analyze your requirements. But knowing about the latter makes some setups far easier to implement, and also to tackle some complicated ones. In this post you’ll learn how they work and when to use each of them. Webroot authentication works by designating a folder which contents are available publicly . Certbot then places a file there then pings a remote server that tries to fetch it. If it is successful, then Let’s Encrypt issues the certificate, as you’ve proven ownership of the domain. We write articles like this regularly. Join our mailing list and let's keep in touch. That’s why it’s called webroot , as you need to specify the root of the web-serving domain. The path the remote server tries to fetch the file from is known, it is http://domain/.well-known/acme-challenge/<file> . One important aspect is the http part. Event though all your visitors might use HTTPS only, Let’s Encrypt uses HTTP by default, and it can not be configured . But it follows redirects . That means that if you have a redirection in place, which you are likely to have to make your users use the secure endpoint, then you don’t need to reconfigure anything on that end. In the cases when HTTP and HTTPS behave differently, or you don’t use HTTP at all, this needs some further considerations. Also, Let’s Encrypt auth does not respect HSTS , even when it’s preloaded, nor it checks the validity of the certificates. In effect, even though no visitors uses HTTP, Let’s Encrypt will. But you can use a self-signed certificate before a valid one is issued to you. To specify webroot auth and the directory, use: You use HTTP, even for a simple redirection And there is a folder that is served as the webroot For static content, this is usually where the index.html is. Effectively, this way you have a webroot but with limited scope. This authorization temporarily starts a webserver on port 80, just for the time of the auth process. As this is a standalone and self-contained solution, this is the easiest to configure, if you don’t use that port. For an API endpoint, you might only use HTTPS, where port 80 is free, and you can use this to get the certificate. But unfortunately, most of the time port 80 is in use. In this case, standalone auth is the way to go. Basically, you have nothing to change in your app, and the certificate issuance/renewal process will just work. Keep in mind that you need to restart the app when a new cert is issued, as most servers will continue to use the old ones. To specify the standalone auth and restart the app on a new certificate, use: If port 80 is used, either use webroot auth instead, or shut down the app during renewals. The latter sounds drastic, but in reality very few users will even notice. The renewals happen around every 2 months, and a new certificate usually requires an app restart, so you’ll have some downtime either way. As the whole auth process takes less than half a minute, it’s usually better to just embrace this. But ultimately, this is your call whether or not that is acceptable. To use standalone authorization and shut down the app during the process, use: You don’t use port 80 Or you don’t mind a less than 1 minute downtime every two months With these two auth challenges, you can easily configure an evergreen certificate for most use cases. But what if you need something more customized? Certbot supports several hooks that can adapt to most use cases, and we’ll cover them in the next post. Stay tuned!", "date": "2018-06-05"},
{"website": "Advanced-Web", "title": "Let's Encrypt tips", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/lets-encrypt-tips/", "abstract": "Working with Let’s Encrypt is usually straightforward, but as with any at least moderately complicated technology, it comes with its own set of problems. While I set up a production server for automatic renewals, I found myself using some practices over and over. This is a compilation of these tips and tricks. Let’s Encrypt uses HTTP when checking for the authorization. If you redirect to HTTPS, it will follow it, but you can not configure it to use HTTPS only. It won’t honor HSTS even on the preload list, and won’t verify the certificate itself. This makes it possible to bootstrap the process with a self-signed certificate before a valid one is issued. We write articles like this regularly. Join our mailing list and let's keep in touch. As a result, it’s not enough to set up webroot authentication that works on HTTPS. If you use Apache/Nginx, use the autoconfigure plugins or the webroot challenge, described in the previous post . If you opt for the latter, don’t forget that you need to manually configure HTTPS. And don’t forget to restart the server after getting a new cert. If you don’t use HTTP at all, use the standalone authenticator. It works by spinning up a temporary server for the length of the domain validation. It is the easiest solution of all, but requires port 80 to be free, which is usually only the case for API servers. Let’s Encrypt puts the certificates to the /etc/letsencrypt/live directory. It’s best to either symlink them or use directly from this directory. It’s all too easy to focus on how you get a valid certificate, but you should focus on renewals instead. For example, by using the standalone auth before firing up your service, you are all set up. But this does not cover how you renew when your app occupies port 80. In this case, either shut it down during the renewal, or get the initial cert with the app running. If you use a setup for the initial auth and use an entirely different one for renewals, you run the risk of some hard-to-debug problems. As the former happens immediately, while for the latter you need to wait ~60 days, it is hard to see if everything is working fine. On the bright side, if you provided your address you’ll get an email if the renewal fails, giving you ample time to react. During the registration process, you have the chance to provide an email address. If the certificate is not renewed on time, you’ll get an email reminding you to do it. In effect, you’ll know there is a problem and have a chance to react on time. Most apps don’t automatically pick up new certificates. This means that even if there is a newer one, they keep using the old one. You need to make sure that either the app is restarted, or at least is sent a signal to reload the certs. Failing to do so results in a successful renewal, so there will be no warning emails from Let’s Encrypt, but an expired certificate presented to the users. The ---staging parameter can be provided for a test environment. This issues non-trusted certificates, but if they come through, you can be sure that when you switch to the live servers your setup will continue to work. If you request a lot of certificates, you can easily find yourself rate limited and without a valid production certificate. Do all your testing with non-production certs, and switch to live ones when you validated that everything is working properly. Certificate Transparency means all issuance must be added to CT logs. In effect, you can check all the certificates issued for your domain, and even get email alerts for new ones. There are multiple services sending out notifications, for example Facebook operates one . Signing up helps you instantly see if an unexpected one is issued, and that might indicate a security breach. Make sure that you handle the bootstrapping scenario when you don’t have a valid certificate. Either use a self-signed one for the duration of the first authorization, of get the first valid one before firing up the HTTPS service. Store them in a volume that survives restarts and redeployments, but you don’t need to back it up. Let’s Encrypt certificates can be recreated easily, so you don’t need to worry about losing them. For example, during a server migration you can simply request new ones. That said, you shouldn’t request a new cert after every restart. That would easily mean you find yourself rate limited during debugging an unrelated problem. If you use Docker, use a named volume for /etc/letsencrypt and you’re set up.", "date": "2018-05-29"},
{"website": "Advanced-Web", "title": "Getting started with Let's Encrypt", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/getting-started-with-lets-encrypt/", "abstract": "Let’s Encrypt provides an easy and automatable way to get valid and trusted SSL certificates for your webserver. Traditionally, getting a certificate consists of registering to a trusted provider, validating that you really own the domain, then installing the cert to your server, then repeating the process every 1-3 years. This proved to be a fragile and insecure practice. In contrast, with Let’s Encrypt you configure your server so that it can request and install the certificate without manual intervention. As the validity is capped, it happens every few months. In effect, Let’s Encrypt makes certificate issuance and management more of a configuration problem than a business one. And as such, you need to know the technical details to successfully configure certbot so that the services you operate can get, renew, and use certificates. Gone are the days when you just emailed the CTO to get a new cert and he sent you one. We write articles like this regularly. Join our mailing list and let's keep in touch. In this and the following posts, you’ll learn the basics of Let’s Encrypt verification, and how to handle more challenging scenarios Let’s Encrypt’s certbot works with Apache and Nginx out of the box for most configurations. If you use either of them, use the appropriate authenticator, and you are practically done setting up HTTPS. It works by reading the config file and making the necessary changes automatically. The only manual task is to add a crontab entry to run the renewal script every day. But this approach won’t work in some cases. If you use a custom backend, like Node.js, to handle HTTP, then certbot has no way of configuring it automatically. Or the Apache/Nginx config may be too complex, figuring out the necessary changes is not possible. Or you don’t want some third-party programs to mess with your configs. These are all common cases when Let’s Encrypt won’t work out-of-the-box, and you need just a bit more planning to make it work. The easiest, and in most cases the best, approach is to terminate SSL before the server. If you use some kind of load balancer, chances are it supports this. In a nutshell, add a server (or a service) that runs a web server, configure Let’s Encrypt on that, and let the connection to your real front-end servers go unencrypted. This is a simple solution, as all the burden of encryption and certificate management is moved out of your app, and in some cases you can use the autoconfigure plugins of certbot. In the case of having a cluster of servers and a load balancer (for example, you use ELB with AutoScaling on AWS), handling certificate at the single ingress point simplify things greatly. The obvious downside is that the connection is not encrypted from end-to-end . If all your servers are in a closed ecosystem, like AWS or Google Cloud, this might not be a problem. But routing the unencrypted traffic through the public internet is something you need to avoid. As a rule of thumb, if you have a cluster within a single ecosystem, terminate SSL at the load balancer. But if you have a single server, it brings more problems than it solves. Let’s Encrypt uses the ACME protocol (no connection to the Coyote and the Road Runner cartoons). The HTTP auth works like this: In effect, all you need to do is serve that file on that URL. That’s why it works with simple Apache and Nginx configs. Certbot can read where the docroot is, and places the file there. It will be available to the remote server. For a custom server, you need to tell certbot where a publicly available path is. Then after getting a certificate, how to restart the server. The easiest way is to use webroot setting with a deploy-hook : And to renew: If you don’t have a public path and you can’t easily configure one, then things get interesting, and we’ll cover those scenarios in later posts. Let’s Encrypt’s DNS challenge is a convenient way of authorization that does not require config changes in the servers. It works by putting a TXT record on your domain’s DNS. But modifying DNS is a nontrivial task. Some services offer an API, for example AWS Route 53, that you can use in hook scipts. But if the DNS provider you are using does not, you are out of luck of automating it. Another downside is that even though the process is automated via an API, you need to supply API keys to the instance itself. In contrast, the HTTP challenge does not require access to any secrets (except for the certificate itself, but if an attacker gains control of the instance, he can request one). And AFAIK, DNS API’s usually can’t be locked down to allow only reconfiguring TXT records. This means, if an attacker gains access to the API keys, he can reconfigure the whole domain, opening up a whole lot of attack vectors. Tip: use this only if the HTTP auth is difficult for some reason. This was a way to cope with difficult http redirections scenarios, but it is disabled now. If you google around, you’ll find articles and answers describing and recommending it, but know that they are no longer relevant.", "date": "2018-05-22"},
{"website": "Advanced-Web", "title": "Tips on window management in Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/tips-on-window-management-in-vim/", "abstract": "Vim has a powerful window system that makes it easy to work with multiple files. This post is a short reference on how to open tabs and split windows quickly with the built-in commands, and with two popular plugins, CtrlP and NERDTree . To open a file in the current window, use :e <path> . If you use the command without an argument, it will reload the current file in the current window, which is useful if the file is changed outside of your Vim session. To open a file in a new tab, use :tabedit <path> . Using it without arguments results in an empty tab. I usually close tabs with :q , but you can use :tabclose as well. The difference is that with :tabclose you can’t accidentally close your last tab. Another nice command is :tabonly that kill all tabs, except the currently selected one. All opened tabs can be closed at once with :qa . We write articles like this regularly. Join our mailing list and let's keep in touch. You can cycle between tabs with Ctrl+PgDown and Ctrl+PgUp . There are other ways to work with tabs effectively, but I don’t use them because I usually have very few tabs open at a time. If you are a heavy tab user, I recommend the following posts: Splitting a window in half is useful if you have to reference contents from other files, or that you have to jump between multiple files in rapid succession. On smaller displays, I rarely use them, but on a large screen, they can come handy. To split the current window horizontally or vertically, use the :split or :vsplit commands. If issued without arguments, it opens the current file in the new window. At first, this might not sound interesting, but it can be handy if you have to edit a large file in multiple different places, or you have to refer to other parts of the same file while editing. It’s also good to know that Vim synchronizes the windows opened for the same file, so you can freely edit in any of them without accidentally overwriting something. Ctrl+w s and Ctrl+w v are shortcuts for :split and :vsplit . Note: The same thing can be achieved with tabs by :tabedit % , as the % refers to the current file in commands. To open a new file in a split, you can just pass the path as an argument to :split or :vsplit . To switch focus from one window to another, use Ctrl+w <direction> , where direction can be an arrow key or one of the h j k l characters. To resize a window, first navigate to it by using the commands above, then use any of the following: For more examples, check this post . Every window can be extracted from the layout, and be opened in a separate tab with Ctrl+w T . I use two plugins for file-related operations: CtrlP and NERDTree . I use CtrlP to quickly find and open files, and NERDTree to explore and manipulate the folder/file structure. With CtrlP , just select an entry from the list then press the following to open the file according to your needs: To open a file in the current window, you just need to simply select an entry from the list and press enter. NERDTree also provides everything necessary to use tabs and splits. I find the default action keys a bit counter-intuitive, but the help menu ( ? ) is always there to help: Most of the features covered in this post are accessible by a mouse as well. For example, you can easily switch between tabs by clicking on their title or resize window simply by moving the borders of the windows with drag and drop. It’s a popular opinion that you should use the keyboard exclusively when interacting with Vim. I find it to be true most of the time, but if you are just starting to work with multiple windows it might be useful to enable mouse support with :set mouse=a . Vim has powerful mechanisms to support editing sessions that span through multiple files. If you know the commands and plugins, tabs and multiple windows can be opened in a blink. For more information, make sure to check out the built-in help, it contains everything you need. For example, check :help :split or :help windows .", "date": "2018-05-15"},
{"website": "Advanced-Web", "title": "Use last-position-jump to recover context after reopening a file with Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/use-last-position-jump-to-recover-context-after-reopening-a-file-with-vim/", "abstract": "Lately, I am watching screencasts from Destroy All Software . There are a lot of great learning materials, but what’s even more important it is really entertaining to watch these Vim centric coding sessions. In one of the videos Gary mentions an interesting Vim feature, that is very similar to Persistent Undo . Persistent Undo enhances Vim experience by saving the history for each file, making the undo/redo continue to work after you close and reopen the editor. The feature I’d like to highlight today is a simple script found in the official help pages: We write articles like this regularly. Join our mailing list and let's keep in touch. If you paste the snippet to your .vimrc , Vim will save the last cursor position when you close a file, and if you later reopen it, the cursor automatically jumps there. It’s inevitable that you have to interact with multiple files. Usually, there are files that you might want to edit only a few times, and you don’t want to keep them open in a buffer. Both of these features are beneficial if you use Vim for longer coding sessions, and I highly recommend them. When you close a buffer you don’t lose all the context and if you reopen a file later, you can continue exactly where you left off. For me, it means that I don’t have to think about which buffers to keep open, so I usually close every tab and panel that I don’t use at the given moment. I even close Vim if I have some work to do in the terminal, rather than keeping multiple terminals open all the time just for the sake of history.", "date": "2018-05-08"},
{"website": "Advanced-Web", "title": "The need for an additional refactor phase in TDD", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/the-need-for-an-additional-refactor-phase-in-tdd/", "abstract": "I’ve used TDD for a long time and in a few different settings. I usually can’t develop whole projects driven by tests, but for many problems, it is my preferred problem-solving approach. Practice makes it better, but I still don’t feel too confident about the short red-green-refactor cycles. I am constantly uncertain about the refactoring phase. Which technically is refactoring, but I think it would be more honest to call it cleanup rather than just plain refactoring . It provides an opportunity to polish the changes made in the cycle, remove fakes, and clean accumulated mess from previous cycles. It doesn’t have exact goals rather than “make it better”. For me it’s mostly based on looking for code smells and try to go for clean code rules. We write articles like this regularly. Join our mailing list and let's keep in touch. While it’s a necessary thing to do, this phase doesn’t address my need to modify existing code to provide better support for new requirements . This is usually a pain point for me. Driven by tests, I come up with the next requirement when I write a failing test. When I switch to the production code I often find myself in a situation where I would make small rearrangements in the code before I could comfortably start working on the next feature. In this case, I do the following: This seems clunky, but I find it more efficient than fighting my instincts and try to make the initial test pass. While this breaks the red-green-refactor flow, these extra steps can be done pretty quickly. However, I was wondering if this is experienced by others as well, and what to do about it. Maybe a possible solution could be to aim for a more general code by creating smaller methods, but it feels like a bit overdoing. In the refactor phase I try to make the production code appealing to the (my) eyes roughly based on clean code guidelines, rather than taking them as a strict rulebook. Do you face this when doing TDD?", "date": "2018-03-27"},
{"website": "Advanced-Web", "title": "Two ways of skipping tests in Maven", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/two-ways-of-skipping-tests-in-maven/", "abstract": "Let me share an interesting story that we faced at work recently. We have a complex application with multiple modules, and we wanted to set up a new dev environment. It’s a big project, and we are usually in a hurry (or just lazy), so to save time we tried to check out the latest version of the project and build it without running the test suites. We passed the -Dmaven.test.skip parameter to the build to the desired effect. But the build failed, complaining about a missing internal test library, found in one of the modules. This gave us a hint to rebuild the project without skipping the tests, and the build succeeded. It was quite early in the morning and this really got me by surprise. The story seemed really interesting because skipping the tests used to have the opposite effect. We write articles like this regularly. Join our mailing list and let's keep in touch. Turns out there are multiple ways of skipping tests with Maven. We normally use skipTests , but maven.test.skip seemed just as fine at the moment. According to the documentation of the maven-surefire-plugin , you can skip the tests with the following flags: The first option ignores all test sources altogether . They are not compiled and thus no test artifacts are produced. The last two are synonyms. They just don’t run the tests, but all test files are compiled . This made a difference in our case because one of our modules depends on the test artifact of another. A module usually export one artifact, typically a jar file assembled from its main classes and resources. However, it’s common to build artifacts for the source code and Javadoc as well. (For example, it’s a requirement for publishing to Maven Central.) In addition, you can also generate an artifact from the test classes and resources as well, for example, to run test cases in different modules or to share a piece of test library. For example, see this project where one module exports all its Cucumber test cases for others to consume it. Here is what the documentation says about the maven.test.skip flag: Set this to “true” to bypass unit tests entirely. Its use is NOT RECOMMENDED, especially if you enable it using the “maven.test.skip” property, because maven.test.skip disables both running the tests and compiling the tests. Consider using the skipTests parameter instead. The bottom line is that you shouldn’t skip test execution in the first place, but if you have to, use the skipTests flag as it compiles the tests at least, just skipping their execution. Not using maven.test.skip has an additional benefit that you can be sure that at least your test classes can be compiled.", "date": "2018-03-20"},
{"website": "Advanced-Web", "title": "Best practices on how to work with collections in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/best-practices-on-how-to-work-with-collections-in-javascript/", "abstract": "Why some projects are clean, easy-to-read, and performant, while others a convoluted mess? Why, when making a modification, in some codebases everything falls into place immediately, while in others it’s more like walking on a minefield? Writing clean code is mostly about restricting yourself from cutting corners and thus enforcing guarantees. The freedom to approach the problems from many different directions brings the responsibility not only to make the code run and do so in a performant manner but also to make it maintainable in the long run. This list is a compilation of best practices to follow and antipatterns to avoid in order to lower the long-time cost of code involving collections. They are mostly based on principles originating from functional programming, so if you have the same background, you might find them familiar or even trivial. A deeper understanding of how collections work helped me transition from producing code that takes more time to understand than to write, to one that is easy-to-read. I hope this compilation helps you do the same. A common mistake is to write functions that do many things: Instead, break them into smaller parts, each responsible for one part of the logic: (To learn how to use these functions, click here ) Modifying the elements of the collection seems like an optimization, as you need one less iteration. But doing so offsets the WTFs/minute code quality indicator in the wrong direction. Don’t modify the elements in the iteratee: Instead, do a map and copy the object: Treat the input data as immutable. Processing should not change it in any way. As a speedup, you can use a collection implementation that embraces immutability. With structural sharing, they can offer better performance than simple copying. When you need only a subset of the parameter objects, use destructuring. Instead of: use: Object destructuring only works with objects. There is a proposal to add support to arbitrary structures, but unfortunately, it had not gained traction. Iterables, on the other hand, are fully supported. As a result, you can destructure not only Arrays, but for example, ImmutableJs Lists, generator functions, and custom types. The iteratee should not modify anything and should use as little data bar its parameters as possible. Instead of writing a state like this: Use a different structure that doesn’t rely on it: (To learn how to use constants and make your code more readable, click here ) While Javascript’s basic functions process the elements in the order they are present in the array, your code should not depend on this behavior. The iteration order is not necessarily stable in every library and definitely not in every language. Treat every iteratee function as if they are run in multiple processes concurrently, and that will set your thinking that works no matter what technology you’ll use in the future. Instead of: Use the index argument most of the functions get: The closed array is usually accessible in the iteratee function but use the parameter instead. Don’t do this to make a collection of points into a circle: Do this instead: This practice makes the code more portable. The less of the environment the functions are using, the better. Optimization is usually a tradeoff between speed and readability. It’s tempting to focus on the former at the expense of the latter, but it’s usually a bad practice. Collections are usually smaller than you think they are, and the speed of the processing is usually faster. Unless you know something is slow, don’t make it faster. Clean but slow code is easier to speed up than a fast convoluted mess to maintain. Prefer readability, then measure. If the profiler shows a bottleneck, optimize only that bit. Streaming–or lazy–processing is when only the minimum amount of processing is done to produce each result element, instead of running the steps to completion before starting the next one. When you map an Array twice, all the elements are processed each time: Before the second map runs, all the elements had been processed by the preceding step. Contrast that with a streaming implementation, using ImmutableJs: Every element is processed completely before work on the next one starts. (To learn how to use ES6 generator functions, click here ) What seems like a performance boost and some savings in terms of memory, in practice, most of the time, the difference is negligible. On the other hand, you can easily end up with an infinite loop, more complex code, or even degrading performance. Unless you know the performance benefits, or you work with structures you otherwise can’t (for example, infinite ones), avoid streaming processing. (Want to know how to use generator functions with ImmutableJs? click here ) Arrays have most of the basic processing functions. If all you need is a map , filter , reduce , or some , use them directly on the Array without any library or wrapper. If, in the future, you’ll need a function that is missing, you can refactor them easily. In cases when you need to use a library of functions, opt for one that is extensible. It might not seem important the first time you want to use last or a similarly absent function, but later when you have complex steps, you’ll find it increasingly difficult to migrate to one. ImmutableJs’s update and Lodash/FP are my go-to choices. (To learn how Lodash/FP works, click here ) reduce is the most versatile function used to process collections. It can emulate the others, as the return value can be anything. But that doesn’t mean you should use it when a more specialized function is available. While this code, using reduce : Is the same as this one, using a filter and a map : The latter communicates the intention far better. As a rule of thumb, whenever you write a reduce that returns a collection instead of a value, consider whether a different function would be a better alternative. As usual, there are some exceptions, but using them usually results in more readable code. A flat structure is when most of the code is on the same indentation level. Using collection pipelines, it is usually a feature you get for free: In some cases, this flat structure is compromised: The flat structure is gone, and as a result, the order of operations is messed up. This should be a red flag that something is wrong. This is the scenario when using an extensible collection pipeline implementation pays off. Those support adding functions while retaining the structure. For example: As usual, there are exceptions. For example, when you need processing inside a processing step: In complex use cases, structures like this emerge. If you want, you can move the iteratees to separate functions: Whether you want to move complex steps to separate functions or not is a matter of preference and style. Some people prefer a lot of small functions with descriptive names, some argue that having everything in one place is better. As for myself, I prefer monoliths as long as I can understand the functionality without much difficulty. When all the functions follow a purely functional style, refactoring them is trivial. There is an exception though. If you need the same step in multiple places, move that to a separate function instead of copy-pasting. reduce has an optional second argument: the initial value. It seems like it is only useful when undefined is not suitable, but there is a critical difference. When an Array has no elements, reduce without an initial value does not return undefined , but throws an Error instead: But with an initial value set, the result is that value: You should always supply an initial value for reduce if there is a sensible one. Arrow functions have shorter forms, making them more suitable for one-liners. Instead of writing out the full structure: If there is only a return statement, the block is optional: For simple steps, prefer the latter. This usually makes them one-liners, and those are easier to scan. Instead of: Use: The parentheses around the parameters are also optional, making the construct even shorter. If you need a piece of functionality in several places, move that to a separate function and use that, don’t copy-paste. Instead of: Do: For reusing complex steps (for example, filter + map ), use a pipeline implementation that supports that. Knowing the algorithmic cost of each function helps estimate the overall complexity of the pipeline. A few commonly used functions: Generally, the complexities are added for subsequent steps and multiplied for nested ones. For a flat structure like this: The complexity is O(n + n + n) , which is still O(n) . For a nested structure: The complexity is O(n * n) . As a rule of thumb, having O(n) and O(n * log(n)) should be scalable, O(n^2) will only work for small arrays, anything bigger can easily be a bottleneck. Apart from a few general guidelines, best practices are the result of the environment too. If, for example, you work with memory-constrained embedded systems, your rules might be entirely different. Also, software craftsmanship is more of an art than exact science, and as such, everything you write has your distinctive style. But I believe if you have no well-defined reason not to, following the principles above will help you write better code. Is there anything you’d add? Let me know, and I’ll update this post.", "date": "2018-03-13"},
{"website": "Advanced-Web", "title": "Dependency Injection Boundaries", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/dependency-injection-boundaries/", "abstract": "I recently read the article Java: Spring Dependency Injection Patterns . The author considers the pros and cons of using Field , Setter , and Constructor Injection patterns. It’s a great post, I recommend checking it if you haven’t already. Reading it and its discussion on Reddit inspired me to write my 2 cents on this topic. The post lists compelling reasons which injection pattern to use, but in my experience, it rarely matters which technique you choose first. In my previous projects, refactoring a class from Field to Constructor Injection was really painless. I could be lazy and start with Field Injection, then gradually upgrade when I hit a roadblock, for example regarding testability. I think the more interesting question is: Should you even add the new dependency or not? Adding a new dependency is way too easy using any of the injection patterns, and it’s comforting to be able to get the desired functionality in virtually any part of the codebase simply by @Autowiring it. We write articles like this regularly. Join our mailing list and let's keep in touch. But adding an additional dependency is an architectural decision to be made. One extra dependency can result in a big chunk of extra downstream dependencies. The properties of each of these downstream dependencies affect how you can use, configure or test your component. Sometimes this does not matter at all. For example, if you are shipping your application in one module, and relying mostly on end-to-end testing strategies (such as browser-based tests or a live staging server) to verify it, it might not be a big deal if some additional edges appear in the dependency graph. However, there are some reasons to stop and think about how to align new modules in an existing system. In a recent project, I’ve come across two of them: testing and reusability. This one is pretty obvious. If a module has no dynamically wired dependencies, it’s the easiest to work with. There are multiple options on how to test components with dependencies, but not having them frees you from that hassle. There is no need to start the DI container or manually supply or mock its environment. It’s also easy to use in production code. You can use the DI container for convenience, but you don’t have to as a simple new will do. It’s not always possible to have zero dependencies, but one should be picky about what to depend on. I think purity in a sense much like the concept of functional programming. The more side effect a module make, the harder it is to work with. For example: Or, as an alternative, you have to mock, but I think that leads to new problems. I am not a big fan of mocking in general, but I see its potential more in designing new modules, rather than using it as a surgical device when I have to break down a big dependency graph. Anyway, having an impure component anywhere in the dependencies of your component affects the purity of your component as well. A module’s purity is the lowest common denominator of its own purity and everything in its downstream dependency graph. In the example above, it’s no use that the business logic of the @Service in the middle is pure. If you’d like to use (or test) it in its full glory, you have to deal with impure code because in the background it makes contact with an impure component. It has two important consequences when it comes to adding an extra dependency: Another problem with injected dependencies is that sometimes you have to reuse a component in multiple projects in a slightly different way. In many cases, you have no other choice but to make this customization with the DI, by supplying different dependencies to the component. (Or supplying different dependencies to the dependencies of the module.) While this is certainly an option, it’s usually not convenient, and it gets even worse when you have to have multiple flavors of the same components live side by side in the same environment. It also hinders maintainability as the configuration is made behind the magic curtain of the DI, not necessarily apparent when you look at the code. If you decide that adding the new dependency is risky, reducing the depth of the dependency graph can provide good alternatives. For example, suppose you have a web application about movies and it has a dashboard that shows a summary of all the data you have. If you have a service component that calculates the average length of the movies found in a database for the dashboard, you might be better off if you just pass the relevant dataset to this component, rather than letting it talk to the database access service to get the necessary records for itself. The version on the right, without the injected dependency is easier to use later. It’s up to the caller to decide what data to pass, and it can use whatever query or service to get this data. It’s also easier to test, as the test-case have a multitude of options to get test data. If you have simple or no dependencies, then you can easily use the service without the DI container altogether. One might even go to the extremes and use the DI solely to wire infrastructure components (such as database, API’s, and external connections), and use pure components for all business logic. Of course, it’s not a silver bullet solution and it has downsides to consider as well. The more layers you have to pass the required data through, the more likely that the resulting structure will be hard to modify and tightly coupled. If you are interested, I’d recommend watching From Dependency injection to dependency rejection from Mark Seemann, an excellent talk on a similar subject. The convenience of the DI is a great thing, and in most cases, it simplifies the mental model one has to maintain when working with parts of the whole codebase. However, as anything, it has pros and cons. You should always consider the dependency graph of your application. Keep an eye on the properties of each component, and strategically decide where you draw the boundaries of the dependency injection.", "date": "2018-03-06"},
{"website": "Advanced-Web", "title": "JUnit and Cucumber test reports based on source code and behavior", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/junit-and-cucumber-test-reports-based-on-source-code-and-behavior/", "abstract": "Scott provides detailed failure messages for Java tests and does so without the use of complex assertion libraries. It can greatly increase productivity, because it allows the developers to express the tests more naturally without losing the much-needed information that tells what went wrong in a scenario. Consider the following test with its dead simple assertion: As you can see, all information is presented on the source code of the test method as comments. All important data is captured by this report even without specialized assertions. Access to this data is really beneficial if you catch a broken test on the CI server and you don’t have anything else but the failure report to reproduce the issue. We write articles like this regularly. Join our mailing list and let's keep in touch. This is not bad, but the most recent version of Scott takes it one step further. Today I’d like to write about a new feature that can help to trace issues with end-to-end tests. For a baseline, this is how Cucumber reports an error: Just dropping Scott into the mix, the very same test produces a more informative report: Scott for Cucumber tracks whole scenarios , and in case of a failure, it prints the details of every step involved. This is true even if the same glue method is involved in multiple steps. Because E2E tests are slower than unit and API tests, usually it’s not feasible in a large system to run all of them locally after each change. Moreover, they are usually more prone to timing related issues, resulting in flaky tests. This means that it’s much more likely to see a failing E2E test in the CI environment than a failing unit test. Scott’s report can provide valuable information to make it easier to reproduce and fix these issues based on the error messages they produce. The above example uses Cucumber’s HTML reporter to present the enhanced reports, but the other renderers are supported as well, so the information is accessible in the Jenkins build or the command line. I hope you enjoyed this post, and give Scott a try. This example project contains the necessary configuration to use Scott with Cucumber. See it’s pom.xml for the project setup and the FeatureTest class for the @CucumberOptions . For more information on about how to use Scott, please the usage guide. In the future, I plan to extend Scott’s capabilities with support for more testing tools and frameworks. If you have suggestions, please open an issue .", "date": "2018-02-20"},
{"website": "Advanced-Web", "title": "Introducing: Array#Extras email course", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/introducing-array-extras-email-course/", "abstract": "From the posts and guides in this blog, you can tell that I love working with collections. This is probably the aspect of programming that changed the most how I write code in the past decade. And there are many ways to do it wrong and a few how to do it right. For most of the day-to-day coding, for loops are a thing of the past, and the vast majority of them can be written in a way that is better suited for people than machines. But this way of handling collections requires a few building blocks. These are functions that are simple in isolation, but composing them forms the arbitrarily complex logic of modern programs. This course introduces these building blocks and shows how to solve everyday tasks using them. In 14 emails, you will learn: The course is 100% free and without any commitments. Unsubscribe at any time. Check it out and give it a spin.", "date": "2018-02-08"},
{"website": "Advanced-Web", "title": "Effective debugging with breakpoints", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/effective-debugging-with-breakpoints/", "abstract": "Previously in this tutorial series, we discussed manual verification and print debugging as possible approaches to problem-solving. Those techniques can provide the necessary insight in many cases, but they can also be very limiting when you have to understand the inner flow of a complex program or framework you are not familiar with. Today, I’d like to discuss interactive debugging, that can be a great help in these cases. This process introduces an additional dependency to the development flow compared to the simple compile-run-manual check cycles, as it requires a debug connection to the application. While remote debugging is possible in most stacks, the typical use case is to do it locally with an IDE. We write articles like this regularly. Join our mailing list and let's keep in touch. In order to use it, you have to: Each of these steps can be done in multiple ways depending on your concrete use-case, so let’s dive a little bit deeper into the details. The breakpoints can be deployed anytime, even in the middle of an already started debugging session. You can start the program before or after you deploy the breakpoints, and you can even add more breakpoints while investigating an already paused program. There are multiple types of breakpoints you can use. The most commonly used unconditional breakpoint always stops the execution when it’s hit. This breakpoint can be deployed in a single click on the sidebar. It’s frequently used, as it’s simple and typically provide everything you need to zoom into any line of code. While it’s mostly fine, sometimes it can be limiting that it stops the execution every time. This is OK if the method in question is called only once, or you don’t care about which execution you inspect. Consider the following program that calculates the geometric mean of scores for the given movies. Usually, it works well, but imagine one day it starts to behave oddly: it returns 0 all the time. Placing a simple breakpoint at the return statement reveals that it returns zero because the value of the product variable is 0.0. It’s possible if the rating is 0 for at least one movie. But which movie is it? Placing an unconditional breakpoint inside the for loop would needlessly make the program stop for many times. (Consider hundreds of movies.) The solution is to specify a movie.getRating().score == 0 condition for the breakpoint, that stops right at the bogus movie. Conditional breakpoints are breakpoints parameterized to trigger only when certain criteria are met. The condition can be specified in the host language, making it really flexible. The breakpoint conditions don’t have to be simple statements like x == 42 all the time. They can have many lines of code that will be executed as if they were part of the program. Tracepoints are one example of using “clever” conditions, where the condition is to execute a log statement and return false. They never suspend the running program, just print a debug message to the standard output. Many IDEs have support for Tracepoints, making them easy to deploy. In Eclipse, you can toggle them with Alt+t . The Tracepoints are generated to System.out.println the name of the method they are placed in, but this can be configured. You can think of Tracepoints as some sort of ephemeral log statements, with a lifetime limited to the debugging session. It’s worth checking out the details in the breakpoint properties window because there are multiple ways to fine-tune breakpoint conditions, for example: Certain breakpoints are tied to events, rather than deployed to a specific line of code. For example, you can pause the execution when an exception is thrown anywhere in the application with Exception Breakpoints . In the movies example, it can be handy if the geometricMeanOfRatings method starts to throw NullPointerExceptions , and you are keen to find the Movie object for which the getRating returns null instead of a real value. Another type of event-based breakpoints is the Watchpoint , which can be used to stop the program when a given field is accessed or modified. A breakpoint can stop the current thread or the whole virtual machine depending on its configuration. Usually, this isn’t something that you should keep an eye on, but it can make a difference in multithreaded environments, such as web applications. Or, when you remote-debug an application used by other developers. In case of Java, by default Eclipse suspends the thread that hit the breakpoint while IntelliJ IDEA suspends the whole JVM. The values of variables and fields from the current and parent scopes when the program is paused are accessible. Usually, there is a dedicated Variables view that summarizes them, but you can check the value of each one by simply hovering over it with the mouse. You can see them change if you continue the execution step-by-step, one line at a time. Or, you can continue the whole program, to run until it hits the next breakpoint when you can inspect the variables again. I do this quite often when I need to investigate values on two distinct parts of the execution. It’s well worth to memorize the keyboard shortcuts for the most commonly used debugging actions, so you can pay attention to the code you debug rather than the quirks of your IDE. It’s important to note, that generally the values are rendered by the object’s toString method, so it’s always a good practice to define them carefully. If you use IntelliJ (or Chrome if you debug Javascript) you can see the values of the variables printed for each line as comments. While it does not print a lot of potentially interesting data, usually it’s more than enough to see the result of an assignment. Besides the toString version of the object, you can also check it’s internal structure. This is (by definition) tightly coupled with internal details of the implementation, so it’s not always the most convenient way to get information. For example, the next image illustrates how the data in a HashMap looks like: Luckily, there is a neat feature in Eclipse called Show logical structure that allows to view objects in another, more meaningful structure. It can be configured manually, but it also comes with handy defaults for commonly used data types. Let’s see how the HashMap is doing with show logical structure enabled: The difference in information density is pretty clear. If you find yourself clicking too much in the variable view looking for something in the nested structures, consider defining a saner logical structure presentation for them. IDEs allow not only to inspect but to change variables and fields on the fly. This can be handy for a number of reasons. Imagine you’ve posted a web form, and now inspecting the processing logic on the server application. A few if-statements deep in the code you realize that you forgot to set a field to the desired value. Now a different code path is going to be executed. In this case, changing the value of the field while debugging can save you some time. On top of that, you can execute arbitrary code at any given point in time. This is really cool because you can query APIs and experiment with the results. If the method you experiment with has no side effects, then it’s even better, as you can try it as many times as you’d like, and it does not corrupt the following executions. I really like how easy it is to debug functional style code. You can run these experiments, in multiple ways, depending on your needs. While stepping through the codebase I often find myself wandering in the bowels of frameworks and libraries that my application depends on. (This is quite frequent if I keep pressing Step into .) I am usually not that interested in their internals - first I want to debug the code that I wrote. Luckily there is an option to use Step Filtering in Eclipse. (I did not find a similar feature in IntelliJ for Java.) With this feature, one can configure which packages to skip when stepping through the code. The list of packages can be configured in the Java > Debug > Step filtering menu. The filtering behavior can be toggled with the Use Step Filters (Shift+F5) button. Another interesting feature I’ve found both in Eclipse and IntelliJ is Drop to frame . This command allows re-entering the selected stack frame. It essentially enables to replay every instruction from a given point. This can be helpful to go back to the start of a method if you missed something you’d like to see. This is not time travelling in the sense that the previous changes to the global state will not disappear. So keep in mind that the execution might take a different path on the second try. As you can see, debugging is a very flexible tool, there are many ways to stop and inspect an application. In the next sections, I try to collect some ideas about when it’s a good idea to use it. Debugging is the Swiss Army knife of a developer . Breakpoints are excellent tools to query internal state or to call APIs at a given point of the code. With dynamic code evaluation, a method or function can be easily called with the relevant input data to investigate its results. You don’t have to restart the application to toggle breakpoints or to do new kinds of investigations, you can also change your debugging strategy while in a middle of a debugging session. Also, if you don’t exactly know what you are looking for, you can easily inspect all variables and methods available from the given line of code. It’s really useful when working with new code or large codebases. Tracepoints provide the benefits of logging, but it does not require to restart the application after adding a log statement . Toggling tracepoints in your IDE is all you need to adjust what is visible on the console. This also means that you don’t have to clutter your code with print statements using this technique. Typically it’s most feasible to use breakpoints in local debugging sessions only , and it requires manual intervention to interrogate the system under debugging to get the relevant data each time the program stops at a breakpoint. This two makes really hard to catch sporadic events happening over a long time. Because the system has to be manually poked each time, it does not scale well. If you debug something today, then tomorrow you can’t depend on this knowledge as the system might behave completely differently. If you have to modify a complex application with a lot of desired properties and internal invariants, you might quickly get overwhelmed by the many little details you have to keep an eye on every time you modify something. In this case writing some tests might be a better alternative, as they can be executed repeatedly, without much effort, and they are much better at locking down desired behaviors of the software. Debugging is a flexible tool. Because you can debug pretty much everything, relying excessively on this technique does not reward carefully planned design . It’s so powerful that it allows to succeed with very bad designs for a while, but it limits the ability to involve new developers . In the end, it might result in harder to maintain systems that only those can touch effectively who knows all their internal details. I am not saying debugging is evil, but don’t overuse it. Try to split the complex logic into isolated units, write unit tests for them, and avoid global state if you can. Robert C. Martin shares similar views about debuggers. The following quote is from his post Debuggers are a wasteful Timesink from 2003: I consider debuggers to be a drug – an addiction. Programmers can get into the horrible habit of depending on the debugger instead of on their brain. IMHO a debugger is a tool of last resort. Once you have exhausted every other avenue of diagnosis, and have given very careful thought to just rewriting the offending code, then you may need a debugger. I know this comment is a bit old, but I hear similar arguments often, and I think it has truth in it. Debugging is a powerful tool with a lot of capabilities. I think it’s essential for any developer to learn to use it effectively. With the growing number of frameworks and libraries, it’s essential to be able to take a peek under the hood. However, try not to debug too much, and don’t rely solely on the ability that you can step through the whole codebase once you get familiar with it.", "date": "2018-02-06"},
{"website": "Advanced-Web", "title": "Reuse code with domain-specific steps in collection pipelines", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/reuse-code-with-domain-specific-steps-in-collection-pipelines/", "abstract": "So far, we’ve only looked into how to add new functions to collection pipelines. In this post, we’ll look into why such extensions might be necessary. We’ve used a rather simple head function as an example, which raises the obvious question: What if a library provides all the fundamental steps? In that case, extending the pipeline with new functions is a non-issue, and all the problems we’ve talked about previously are non-existent. If you only want to use generic, building-block type functions, this might be true. In that case, choose a library that provides all the functions you need. But for complex apps, functions that are specific to the domain might be needed to promote code reuse. In that case, no library can provide them out of the box. This post argues that even though libraries can provide all the necessary building blocks for collection processing, being conscious about extending the pipelines promotes code reuse and thus a cleaner architecture. In simple cases, moving the iteratee to a separate function is enough. As an example, filter an array of users by some non-trivial condition ( Try it ): If you need this filtering in many places, refactor it to a function ( Try it ): The problem is when the change itself does not fit into a filter , map , or the other functions that are provided by the pipeline. For example, filter the non-active users and also assign an activity rating to the objects. This operation is a filter , followed by a map , so it does not fit into either of them ( Try it ): Moving this to a separate function is usually done with a ([users]) => [users] signature ( Try it ): But in this case, this function does no longer fit into the pipeline: Instead of a flat structure, we’re back to square one. Extending Arrays is an anti-pattern, as we’ve seen in a previous post in this series. Implementing and using chaining comes with its own problems. But function composition offers a solution. As a recap, we have a flow function that operates similar to the UNIX pipe, composing the argument functions. And the map and filter functions are partially applied, i.e., they get the iteratee on the first call and the collection itself on the second one. The above example, rewritten to use functional composition ( Try it ): To move this to a reusable function, just extract the pipeline itself: This value is itself a valid pipeline, and can be part of a larger one: This way, arbitrarily deep pipelines are possible, without losing the benefits. Parts used in multiple places can be moved to a central location, promoting code reuse. ImmutableJs provides a function called update that can be used to achieve the same effect. It gets the whole collection and returns a new one ( Try it ): With update , ImmutableJs collections are easy to extend with domain-specific steps. Extending a collection pipeline with custom processing steps is an everyday task in complex apps. Doing it right promotes code reuse. The most important step is to know the libraries. If they provide a way of adding custom steps, like ImmutableJs does, use that. If not, use functional composition instead of Array#Extras for any non-trivial processing.", "date": "2018-01-02"},
{"website": "Advanced-Web", "title": "The silver bullet of collection pipelines: Functional composition", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/the-silver-bullet-of-collection-pipelines-functional-composition/", "abstract": "In the previous episodes, we’ve seen that collection processing is both crucial and nontrivial to get right. We’ve already seen some potential solutions, and where they fall short. In this post, we’ll look into how concepts from functional programming can help solve this problem. The basic idea is that a collection pipeline is ultimately a function that gets a collection and returns a new collection. Something like this: Thinking about pipelines this way, we can merge two pipelines and still get a valid one: This is like UNIX’s pipe . pipeline1 gets the input collection, pipeline2 gets the output of pipeline1 , and the final result is what comes out of pipeline2 . Using composition, with only the fundamental pipeline functions, arbitrarily complex ones can be built on top of them. Let’s translate this abstract concept into programming! Since the pipelines are simple functions, merging is a function composition. Let’s write a function that gets two parameter functions ( f and g ) and returns a composite one! ( Try it ): The arg is passed to f , and it’s result is passed to g , and it’s result is the final result. A simple example is to add 1 to a number, then double it: But if we were to use this implementation to compose 3 or more functions, multiple flow s would be needed: Instead, generalize the implementation to handle arbitrary amount of functions: And to use it: Now that we can compose them, if we have the fundamental collection processing functions, arbitrarily complex ones can be built from them. But how to modify our map and filter implementations to conform to the required signature, while staying generic? The solution is higher-order functions. This is when a function takes or returns another function. Since the map and filter both need a collection and an iteratee , the key is to supply these parameters in different runs. Since the pipeline function gets the collection, the iteratee has to be the first one. This is called “iteratee first, data last”. This way, the universality of the functions are retained, while making them compatible with the pipeline signature. The modified map implementation: And the filter : Now we have all the pieces to make a collection pipeline ( Try it ). To have a map that adds three to every number: And a filter that keeps only the even numbers: And a composed pipeline that increments by three first, then drops the odd numbers: And finally, to use it, just invoke it with the collection: Of course, all these values are optional. The same pipeline: There are various libraries that embrace these concepts. One is the Lodash/fp package ( Try it ): Another one is Ramda ( Try it ): Functional composition has all the benefits of chaining, all while retaining the ability to easily extend it with new functions. Adding a new one is just a matter of conforming to the interface of (coll) => coll , making it a localized change. Also, unlike chaining, it has no magic. With a few lines of code, you can write all the required helper functions.", "date": "2017-12-19"},
{"website": "Advanced-Web", "title": "Demystifying chaining in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/demystifying-chaining-in-javascript/", "abstract": "Chaining is when you wrap a collection, define the pipeline, then extract the result in the end. If you know Lodash’s or Underscore.js’s chain method, then you already know how to use it. In this post, we’ll look into how it works, and implement this function in a somewhat simplified way. In contrast to Array#Extras, chaining does not require any existing functions present in the Arrays themselves. In fact, it is a generic concept that can work on any data types and can add any processing functions. The structure is wrapping , processing , and extracting the result . The first part is done by the chain method, the second one is the collection pipeline itself (made of map s, filter s, and the likes), and the final part is the value() call. An example processing ( Try it ): At first sight, it’s like magic. How to convert the map and the filter functions we already have to chainable steps? The crux of the implementation is, obviously, the chain method. This function gets a collection and returns an object that has all the processing functions and a fluent interface. Let’s build this function step by step! ( Try it ) Keep in mind that this is a simplified version, and you can make it in different ways. The 0th step is to have the map and filter functions ready! Let’s just pull the ones we’ve already used before. The first step is to have a chain function that gets a collection and returns an object: The next step is to keep track of the current state of the collection, and provide a way to extract it with a value() call. With only this in place, we’ve already made the wrapping and the extracting parts possible. The final step is to attach the processing functions in a chainable way. To do this, add a function with the same name, which gets some arguments and calls the processing functions ( map or filter in our case) with the current collection ( _currentArr ) and the arguments. To make it easier to digest, let’s consider only one function, the map ! To make it chainable, the wrapper needs a function called “map” that: To add all the processing functions, use a loop: Now that we know all the parts, putting them together results in a fully functional chain implementation: And its usage: In case you have a library of functions, just list them as processing functions and attach to the wrapper object. This way, the chain function can be customized with all the required processing steps. But what if you use a third-party library, like Lodash or Underscore.js, and want to extend it? Most libraries provide a way of adding functions. But they either modify the library itself, essentially polluting it, which we’ve already seen is not a good idea, or return a new chain method that makes the syntax awkward. There is one other thing that might seem wrong with this solution. It’s on this line: func.name returns the name of the function. But what if you use an aggressive minifier that changes the names of the functions in order to save some bytes? In that case, func.name returns something other than “map” or “filter”. As a result, be extra cautious with variable renaming when you use chaining. Chaining is a great solution if you have a library of functions and want to provide a fluent interface for them. But keep in mind the tradeoffs. First, chaining is hard to extend if you use a third-party solution. And second, it might interfere with the minifier. In the next episode, we’ll look into a way of processing collections that is clear, easy to extend, and non-polluting.", "date": "2017-12-12"},
{"website": "Advanced-Web", "title": "What to expect when you decide to migrate from Javascript to Typescript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/what-to-expect-when-you-decide-to-migrate-from-javascript-to-typescript/", "abstract": "I finally made the leap. I always felt Javascript’s dynamic typing was a compromise in the wrong direction. Complex software need all the tools to prevent and detect errors, and a sound type system is a powerful one. Coming from Java to Javascript, one of my main concern was the missing of this safety net. As a result, I paid close attention (and much hope) to the alternatives and solutions to bring static typing to the web. Dart –formerly Dash–, JSDoc, Flow, and then Typescript all have a solution, but an entirely different approach. The hardest part of web development is that the goal is not to find a useful piece of technology. Rather, it is to identify the next big thing that will be the de-facto standard a year from now. If you choose wrongly, you need to refactor and rewrite over and over again. I liked Dart’s approach, but it failed to become mainstream. While there are some advocates, I don’t see Flow to emerge as the winner either. As for Typescript, I see more and more projects embracing it. I feel confident that this trend continues and I won’t need to migrate to the next big thing that adds types to Javascript. While the migration went smoother than expected, it’s still a whole different world. There were pleasant and not so pleasant surprises which required extensive Googling and some serious head scratching. Below is the summary of the experience so that you can know in advance what the ups and downs are. I hope it will help you avoid an issue I faced or at least give you an overview. Example Typescript has a type called tuple , which is a fixed-length Array with fixed types at each position. When you know a value has exactly two elements, the first being a number, and the second a string, it provides better type checking: Despite their apparent usefulness, I was surprised to see that type inference does not use them at all: Why not a tuple of [number, number] and [number, string] , respectively? As it turns out, Typescript plays safe here and assigns the broader type automatically. To use tuples declare the types as such: Another surprise was that even when a tuple type is used, over-indexing does not produce an error: In my experience, using tuples brings a bit stricter type checking, but there are some edge cases. Also, they require writing out the types explicitly, which is cumbersome at times. Example Element implicitly has an 'any' type because type '...' has no index signature. This error was the first bumper needed some head-scratching to solve. As it turns out, objects are not indexable by default. That, in turn, forbids the [...] property access: Index signature can be added using [index: string]: string (or any other value type). But to add it, you also need to copy the entire type, which is a whole lot of extra code, especially for larger or nested objects: Example I use ImmutableJs extensively and found it strange that Seq’s filter’s iteratee gets the value as optional. I think it is fixed in 4.0.0., but at the time of writing, it was still in rc. The iteratee function has (e?: T) types. With strictNullChecks enabled, this code raises an error, as e ’s type is number | undefined : I know it can’t be undefined, but how to tell Typescript about it? Cast it explicitly to a number: While this solution works, it’s long, and what if the stored type change? In that case, all the casting has to be modified too. Later I found the Typescript-specific non-null assertion operator , which eliminates undefined and null from the type: I’m still not perfectly happy with it, as I need to add that to all the usages and I still can not declare the parameter itself as non-undefined. Later still I found that I can amend libraries themselves by adding new overloads to functions. That enabled me to fix the filter function, which in turn made casting and non-null assertions unnecessary. Example Typescript embraces classes and using them as types is easy: All the types are inferred, and all the usages are type-checked. But what if I don’t want a class but a function that returns an object? In that case, I need to manually provide an interface, as while the function’s return type is inferred, I can not use it as a type elsewhere. This forces me to maintain two parallel structures. Example I need to transform objects into Arrays and back in several places, and Typescript has a hard time figuring out what happens with the types. Depending on the library and the steps, the inferred types are considerably different. When converting an object to pairs and back without using any library, no types are inferred: Doing the same with Underscore, the result is an empty object, which is just a small step in the right direction: Lodash figures out the best: But throw a map into the mix, and the inference is now less helpful: I got errors complaining about import _ from \"underscore\" and a few other libraries. Even though it was an entirely valid import, Typescript thought otherwise. Googling around, I found some docs saying that the import = require(...) construct would solve the issue. Unfortunately, with the \"module\": \"es2015\" setting, this does not work, and with Webpack supporting ES6 modules I didn’t want to go back to UMD. Another solution is to use import * as _ from \"underscore\" . This works, but in the not-so-rare case when the export is a function, for example, in this case , it does not work. Searching further, I finally found a suitable solution: setting allowSyntheticDefaultImports to true. This setting took care of the missing exports, and everything went fine from there. Example On several occasions, Typescript managed to surprise me how accurate its type inference is. But then it struggled in some other cases. I had an issue with typing that sparked a question , that was answered quickly afterward. For this perfectly fine code, the compiler reports an error: The solution is to make both of the classes implement an interface with the function having both sets of expected parameters. Check out the linked answer for the code sample. Example Although on rare occasions, I needed to cast a value to an entirely different type. This issue also required some searching. For example, when I have a variable that Typescript considers a number, but I know it’s actually a string, casting it reports an error: To cast to anything, cast to any then to the target type: Example You can create a new type using type <name> = ... . This construct is great if you want to reuse the same definition in multiple places. For example, type userId = string; declares that the userId type is a string. From there on, you can use it just like any other type: This is great, but to my surprise, it is just an alias. It does not prevent passing any strings as userId : I could not find a way to prevent that without adding too much code and type assertions. In Typescript’s terminology, this stricter custom type checking is called nominal type matching . There is an ongoing ticket to support this. It would be great if there was a way to declare enforced types. That way, if you have userId , itemId , and they are both strings, type checking would make sure that you don’t accidentally mix them. Example The next lesson was how to use typecasting along with destructuring. I used UnderscoreJs’s partition function that separated null properties from non-nulls: nonNulls is guaranteed to not have any nulls as values, while nulls has only nulls. Typescript’s inference assigned string | null for both of them. How could I cast them? First, I tried to cast the arguments, like the way I do for variable declarations: const [nonNulls: ..., nulls: ...] , but with little success. Then I figured casting the result of the function might work: It worked. As a second surprise, type checking does not work on the right of the as . Example Many libraries provide functions that need to work with many different sets of arguments. One such is partial from lodash, which needs to work for an arbitrary amount of prefilled and placeholder parameters. While the types support several permutations , they do stop at some point, leading to seemingly awkward inferred types. With a function getting three arguments: This partial application infers the types correctly: But shuffling the arguments easily falls back to the catch-all case: Example Using lodash, this is valid JS: But Typescript reports an Error, as the types of the 1-ary flow are missing. Granted, the flow function can be removed without any change to the functionality. So it was just a minor inconvenience. But it was surprising at first that valid Javascript produced a type error. My setup was a fairly typical Webpack + Babel combo, and throwing Typescript to the mix was surprisingly straightforward. There are two loaders to choose from: awesome-typescript-loader and ts-loader . I opted for the latter, and had zero problems with it; also, the build did not slow down by any perceivable degree. The integration into the build pipeline as a whole took only a few minutes. Example Many libraries provide functions that need to work with many different sets of arguments. One such is partial from lodash, which needs to work for an arbitrary amount of prefilled and placeholder parameters. While the types support several permutations , they do stop at some point, leading to seemingly awkward inferred types. With a function getting three arguments: This partial application infers the types correctly: But shuffling the arguments easily falls back to the catch-all case: On several occasions in the process of adding types, Typescript reported an error, and it was indeed a potential bug hidden in the codebase. Bugs like these usually follow a similar mental pattern: The system works, and nobody knows about the flaw. Then one day the bug gets revealed, making a mystery how on earth could this had worked in the first place. Type errors were like this. It seemed like some function never got called with some parameters, and that hid the potential problems. While fragile, the code worked. This was the time I saw the power of static typing and how fragile Javascript’s way is. It revealed edge cases that could wreak havoc on the function if called in a specific way. It was only a coincidence it did not manifest itself as a bug. But with types, even the possibility could be avoided. Typescript’s strict modes are strange, to say the least. I’m sure had I followed its evolution, they would make perfect sense, but having jumped in later, they are a mess. By default, Typescript tries to infer the types, and if it is unable to do so, it assigns the type any , which essentially disables checking. Also, there is null / undefined handling. By default, if something is potentially null or undefined it does not raise an error. This makes it easier to migrate to Typescript, but TypeErrors that could be detected slip through. To both these problems, there is a configuration option that forces checking. They are disabled by default not to break existing programs during an update, but that fragments the codebase. Code written using more lenient settings produce errors when moved to a codebase with stricter ones. Adhering to the strictest possible settings should be the goal. This not only catches the most bugs but also makes the code as portable as possible. But having to turn them on came as a surprise. On the other hand, maintaining backward compatibility is a challenge for every evolving technology, and making it optional with a setting seems like a good compromise. I’m sure I’ll be grateful for this when I have a sizeable collection of .ts files. With all the strict flags enabled, migration to Typescript is like pulling a spider web; you keep finding files you need to convert to get rid of all errors. I’m sure with more lenient settings a phased migration is possible, but in the long run, aiming for the strictest checks pays off. This is because a .ts cannot import a .js without types definitions. So if you rewrite one file, you also need to do so for all its dependencies. And so on, until all the connections are converted. This made the first step of the migration a rather large one, a bit larger than I felt comfortable. As an alternative, you can write .d.ts files for some of the js’s, making demarcation lines between the two worlds. This works and adds just a little overhead, so opt for this approach if you feel overwhelmed by the amount of work. Example Some .js files were too complicated to rewrite in the first run, but I’d read that I can write the types without converting the file itself. It wasn’t immediately apparent how to do it, and required some searching and trial-and-error, but figured out finally. All you have to do is create a .d.ts file next to the .js with the same name (for filename.js , create filename.d.ts ), and declare a module with the types. Typescript will automatically use it and you don’t need to convert the file to .ts . Example To get types for a library that does not bring its own (for example, ImmutableJs comes with types included), installing @types/libname usually works. @types is an organization with community-supported types, and as such, covers many popular libraries. But even when there is no definition file from the community, it’s surprisingly easy to write one and use it in your project. One of the selling points of static typing is that a function’s usages can be tracked. If the return type changes, all the non-conforming usages are reported as errors. Contrary to standard Javascript, if you change the result, for example, from Array to an ImmutableJs List, all usages are immediately reported as errors. Say goodbye to grepping the function name and hoping to find all the usages this way. Just change the result, fix all the reported errors, and it’s done. Example While the types for libraries were mostly OK, there were some problems. But as it turned out, amending a library is not hard either. As with Seq , List.filter ’s iteratee’s parameter is declared as optional: To fix the signature, add a .d.ts file: This fixes all usages. After all the first impressions and many hours of searching for solutions to the above problems, I still feel Typescript is worth it. Most of the issues can be traced back to the dynamic nature of Javascript, and I hope that in the future more and more libraries will embrace a safer way of coding with less moving parts. Do you have a more straightforward solution to any of the above problems? Let me know, and I’ll update the post with it. Thanks!", "date": "2018-02-27"},
{"website": "Advanced-Web", "title": "Where Array#Extras fall short", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/where-array-extras-fall-short/", "abstract": "Arrays already have map , filter , reduce and some other functions. The example from the first post can then be simplified to ( Try it ): This is a clear, concise solution, without lingering variables or awkward syntax. The problem is that Arrays are hard to extend with new functions. And this is a serious shortcoming, as having some functions does not mean that you’ll never need anything else. What if you need head , that returns the first element? A simple solution is to add the function to the Array prototype, amending all Arrays at once ( Try it ): But this is a polluting operation. When this code runs, it has far-reaching effects, modifying all Arrays. It can be a problem in multiple ways. What if different people want to define the function in different ways? Even in this simple example, should head throw an Error or return undefined for an empty Array? Also, were libraries to follow this practice, that would create a mess on a whole new level. Moreover, if the function is defined just before usage, it is a side-effect. If it is defined globally, that adds to the surprises people not familiar with the codebase experience. These surprises are what makes onboarding progressively harder. Consider the following scenario ( Try it ): If you decide to get rid of the function1 call, code that comes after it that also relies on the first version will break. A possible approach is to undo the pollution after the function is called ( Try it ): This might work in some situations but is flawed in several ways. First, it is ugly. Wrapping everything in a try...finally just to avoid side effects is a clear sign that we’re doing something wrong here. But also, it has more localized problems. What if there is a function call that uses head ? In that case, depending on the caller, the function may use different versions ( Try it ): When all you need to use is present in the Array prototype, use that. It provides the cleanest collection processing you can ever have. But this approach falls short when something is missing. The lazy solution is to cut corners and augment the prototype, but this approach comes with long-term costs hardly justified by the benefits. But don’t worry, there are other solutions to this problem. We’ll look into them in the next posts.", "date": "2017-12-05"},
{"website": "Advanced-Web", "title": "2 cases where Babel fixes your code but it shouldn't", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/2-cases-where-babel-fixes-your-code-but-it-shouldnt/", "abstract": "Did you know that Babel inadvertently fixes some errors? Me neither. In one of my projects, I switched from babel-preset-latest to babel-preset-env without expecting any problems. But this is what happened. Since I don’t care about legacy browsers, it immediately switched off all plugins and introduced two bugs in the codebase. After some investigation, it turned out that some transpilations “fix” some edge cases. And in turn, switching them off surfaced the errors. When you use block-scoped values, like a const , Babel rewrites them to traditional var s. It takes care of block-scoping, so most of the times they behave as they should. But it does not handle a specific scenario. For this input code ( Try it ): Babel outputs this: It’s almost identical; only the const has been rewritten to var . But if you run it, the babelified code sets c to undefined , while the original one throws an Error. The difference between const and var – apart from the scoping – is how they are hoisted . The var declaration is moved to the top of the scope and set undefined . Therefore, c is accessible inside the function call. On the other hand, const is moved to the top but left uninitialized . Inside the function call, c is still uninitialized, it is in the so-called temporal dead zone , so accessing it throws an Error. The other edge case is how iterables are handled. Consider the following code ( Try it ): o is a so-called Array-like structure. It has a length property, and the numerical keys return the appropriate elements. In this case, it is an empty array (length: 0). Running this code results in an error: The array destructuring operator works only on iterables. Array-likes are supported by some built-in functions, but they are not enough in this case. Since iterables are required to have a [Symbol.iterator] function and o clearly does not have it, the error is expected. Babel produces the following code for the above input: The key part is the _toConsumableArray function. It makes a full-fledged Array out of its parameter. Since Array.isArray(o) is false , it returns Array.from(o) . That function accepts an Array-like and returns an Array. Since then, it behaves exactly like [] . Therefore, the code returns an empty Array, instead of throwing an Error. There might be other edge cases that get “fixed” by transpilation, and cases like these might surface errors in an update. This might cause problems bumping even minor versions. For example, babel-preset-env supports time-dependent queries, like last x versions . With a new browser version, a plugin might get disabled, surfacing an error. Fortunately, these errors are rare, and they are both known issues . But don’t expect them to just go away. They are hard to implement right, and therefore, it’s more likely you’ll update sooner than the fix is released. By knowing errors like these exist, give attention to plugins that get enabled/disabled during an update.", "date": "2017-10-31"},
{"website": "Advanced-Web", "title": "Generators and ImmutableJS", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/generators-and-immutablejs/", "abstract": "Iterables are great in theory, but there are only so many language constructs supporting them. For loops are great if you still live in the 90’s, but map , filter , reduce and the like will be missed instantly otherwise. In practice, the first thing you’ll do is to convert them into something more useful. ImmutableJS embraces the iterable protocol. Not only that all data structures are standard iterables, but they also allow construction from another iterable. And since generators make iterables, they are fully supported too. Let’s revisit the generator function from the last post ( Try it ): To construct a List, pass the generated iterable: The same works for a Set too: This way, you have full-fledged Lists or Sets with their rich API directly from a generator. But generators can be infinite, which is not suitable for traditional Lists or Sets. On the other hand, Seqs are lazy, and in turn support lazy collections, making them suitable to handle potentially infinite iterables too. At first sight, Seqs are the ideal tool to augment iterables. For an infinite generator, let’s pull one of our previous examples ( Try it ): To construct a Seq from a generator, pass the generated iterable: From now on, it is just like any other Seq: In case the generator does nothing but produces a stream of numbers, ImmutableJS also has a built-in class called Range : They work the same, but Range does not need the generator function. For simple cases, use Range , but for more complex ones that can not be covered with a Range , for example, a Fibonacci sequence, use a generator. Let’s do an experiment with Seqs constructed from generators! In the previous post, we’ve seen that generators can be iterated over only once. What about a Seq, initialized from a generator? After calculating the sum of the first 10 even numbers, let’s reuse the same Seq and calculate the first 20 even numbers too! ( Try it ): It works. But why? The Seq can not pull the first 10 even numbers twice from the generator, but somehow it still manages to calculate the correct result. It turns out that the Seq caches the iterable so that it can produce reproducible results. Usually, it’s a good thing. In particular, they fix the generators’ peculiarity of being both iterables and iterators. On the other hand, ignored elements still consume memory. For example, skip the first 100000 elements and get only 1 item after them. The generator and the Range produce the same Seq: But while the Range does not consume more memory than before, the Seq with the generator is. Keep this in mind when you need to skip a lot of elements. Another problematic use case is when you have a generator that is continuously running in your app, for example, a random number generator. If you wrap it in a Seq, it will slowly leak memory. ImmutableJS supports iterables and generators with all its collection types. You can build a List from a generator, or wrap with a Seq for a more developer-friendly API. This will work, but watch out for the iterable-caching behavior of Seqs. In some rare cases, this could make an algorithm memory-bound, or the app to leak.", "date": "2017-10-03"},
{"website": "Advanced-Web", "title": "Generators in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/generators-in-javascript/", "abstract": "In the previous post, we’ve covered iterables. We’ve seen that they have a function that returns iterators, which are then used to access the elements of a collection. But they require a lot of boilerplate code to make them work. This post introduces generator functions, which is a standard and well-supported way to create iterators and do that with minimal additional code. To see what’s the problem with iterables, let’s revisit an example from the previous post ( Try it ): It’s 11 lines, and most of it is boilerplate that is only there to make the protocol happy. Generator functions are denoted with an asterisk as part of the function declaration. Whenever the generator yield s a value, the function is paused. And later, when the next() is called, it is resumed. This pause/resume behavior is unique in JS, as these are the only type of functions that offer this behavior. It also opens the possibilities for other use cases besides iterables. But in this post, we’ll concentrate only on this use case. The above iterable defined as a generator function ( Try it ): There are a few things to notice about generators: First, the generator function has to be called to produce an iterable. As a result, it can handle arguments, in contrast with the Symbol.iterator function Second, the protocol boilerplate is completely missing. Since it produces standard iterators, the value and the done properties are there, but they are handled inside the generator function and the for..of loop. And finally, the function’s end finishes the iterator. Generators also support infinite iterators, just like the manually-written iterables. Since the function is paused, without endlessly calling the next() method, it won’t stuck in an infinite loop. For example, a generator that outputs all natural numbers ( Try it ): The while(true) construct is fairly common when working with generators. It takes a little time to override the old habits and get used to them. You might have noticed something strange. Calling the generator function returns an iterable, as it can be used in a for.of loop, and it can produce multiple iterators. But one call starts only one pausable function call. What happens when the iterable produced by a generator function creates multiple iterators? The unfortunate answer is that generators are both iterators and iterables, and their Symbol.iterator function simply returns itself. Why is this a problem? When working with an iterator, you know it’s a one-shot thing. Each element will appear only once, and after the iterator is exhausted, it’s useless. On the other hand, iteration over an iterable should not produce side effects. And exhausting its elements is surely that. For example, multiple iterations over an array are possible, but not on an iterable generated by a generator ( Try it ): Because of this, you can not simply pass generators around instead of other iterables, as iterations will change them. To remedy this, wrap the generator into an iterable that makes a fresh instance for every iteration: Generators are a powerful and terse way to define iterables. Their pause/resume model makes them a lot easier to be understood that traditional next() calls. All while retaining the ability to define infinite collections, which are a lot more useful than you might initially think. The drawback of generators is their liberal approach of being both iterables and iterators, and blurring the lines between the two. In everyday programming, it’s more common to make one-shot generator functions than reusing them, but this is something you need to look out for if you choose to use them.", "date": "2017-09-26"},
{"website": "Advanced-Web", "title": "Use Persistent Undo in Vim for maximum comfort", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/use-persistent-undo-in-vim-for-maximum-comfort/", "abstract": "From version 7.3 , Vim supports persistent undo . With this feature enabled, Vim automatically saves history to external files, making the undo/redo functionality continue to work after restarting Vim or closing a buffer. This makes Vim much more comfortable if you use it as a core part of your development workflow, especially if you use it with other terminal applications. Before I learned about persistent undo, I felt uncomfortable to make changes spanning across multiple files. I don’t like to keep many buffers open, so I rather jump from file to file with fzf or use the shell to navigate around and occasionally open Vim when I need to edit something. Having short editing sessions, I can’t access recent history information after I reopen a file. This info is often crucial, as a simple undo/redo combination allows me to recover some context about what I’ve been doing in that file before. We write articles like this regularly. Join our mailing list and let's keep in touch. In this aspect, persistent undo increases the comfort level of the lightweight Vim + terminal combination to be a bit closer to what IDEs provide; tools that I usually don’t restart too often and provide a consistent feeling by keeping track of the stuff I do in them in a session. Adding set undofile is sufficient to enable this feature. However, by default Vim creates the history files next to the files that are being edited. I don’t really like this, so I use the set undodir option as well to specify the exact location of these files. Taking the idea from this StackExchange thread I also added some instructions to my .vimrc file to create a temporary directory to store my history files. Because I don’t want to bother about deleting the old entries, I put the undo directory to the /tmp folder as it’s subject to system level cleanups. This way I can’t count on the history files to survive system restarts, but it’s fine for me, they are usually needed for one coding session. If you store them elsewhere, make sure to create a Cron job to periodically delete them. It’s also worth mentioning that it is best to create the undo directory with 700 permissions ( drwx------ ), as it grants all rights to the owner, but nothing to anybody else. So, to summarize, I’ve added the following to my .vimrc file: As far as I can tell now, having undofile enabled provides a great deal of comfort for free. If you use Vim solely to edit configuration files it might not be a big deal, but in any other case I recommend this feature.", "date": "2017-09-19"},
{"website": "Advanced-Web", "title": "Iterables in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/iterables-in-javascript/", "abstract": "In the previous post, we’ve covered what iterators are, and how JS defines a protocol for them. We’ve seen that they provide a one-shot traversal over a collection. But they require a lot of boilerplate to make them work. This post introduces the iterable protocol, which is a standardized way to make iterators. Just like with iterators, making an object to conform to the iterable protocol is just a matter of adding a function. Its name is Symbol.iterator , and it needs to be zero-arity and has to return an iterator. Think of it as a multiple-use factory to make one-shot iterators. To wrap the iterator from the previous post into an iterable ( Try it ): Using this iterable, the 1, 2, and 3 elements can be iterated over and over again: Think of iterables as collections that can be iterated over, multiple times. There are language constructs that make this iteration a lot friendlier. The two example we’ll look into are the for..of loop and array destructuring. There are also some functions that accept iterables, for example, the Array.from() : While not all third-party libraries that provide collections embrace the iterable protocol, some do. For example, ImmutableJS collections are all iterables, making them compatible with the for..of loop: Also, you can use any iterable to construct Immutable Lists: In effect, wherever you expect an iterable, you can use Arrays, custom-built iterables, Immutable collections, and any other compatible third-party data types. Iterables are what make iterators actually useful. They abstract away the essence of collections, that is the ability to produce elements. If you need just that, different implementations become interchangeable on the language level. On the downside, hand-writing iterables is still tedious. In the next post, you’ll learn the basics of generators, which are a special kind of functions that make this syntax a lot more developer-friendly.", "date": "2017-09-05"},
{"website": "Advanced-Web", "title": "Never lose a change again: Undo branches in Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/never-lose-a-change-again-undo-branches-in-vim/", "abstract": "I often find myself in a situation where I hit undo a couple of times to peek back in time, but losing the ability to redo my way back due to inserting some text by accident. This caused some unpleasant moments until I’ve learned about Vim’s powerful undo/redo system. Undo tree was introduced in Vim 7.0., from that version the history is stored in a tree rather than a stack . The root of the tree refers to the original version of the file (no changes), and every time you change something a new node is added to the graph. The latest leaf node represents the last change you have made. For example, the history looks like the following if you add line after line without using undo: We write articles like this regularly. Join our mailing list and let's keep in touch. The path from the latest leaf node to the root contains all the changes you’ve made to reach the current state of the file. With undo ( u ) and redo ( <C-r> ) you can walk upwards and downwards this path. Using these commands alone Vim behaves like most editors that have the usual undo stack. However, if you change something after an undo action, the tree grows a new branch rather than simply discarding the previous future elements. Vim keeps all your changes, but with u and <C-r> you can’t reach the nodes that are outside the path to the root. However, Vim has the ability to navigate the changes in the order they were made. With g- you can move to the previous item in time, while g+ moves to the next one. This can be a solution to the problem of the accidentally lost changes while undoing around: hitting g- in this case can take you back to the latest version you need. g- and g+ is neat, however discovering the whole history graph with them might be inconvenient. Gundo.vim provides an easy to use way to navigate in the undo branches . It requires Python and a Vim built with Python support. The plugin provides a nice graphical layout of the undo branches , making the navigation in the changes trivial: I’ve mapped the <leader>u to toggle this view, and I also had to set the g:gundo_prefer_python3 to 1, because I am using it with Python 3. Vim has a powerful undo/redo system that makes sure that you don’t lose any changes. Use the g- / g+ if you get stuck with u / <C-r> . If you need total control over your history don’t forget to install Gundo.vim.", "date": "2017-09-12"},
{"website": "Advanced-Web", "title": "Iterators in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/iterators-in-javascript/", "abstract": "Iterators are a language-independent protocol to traverse a collection of elements. It abstracts away the intricacies of the underlying collection. Despite Arrays being the primary way to store a collection of data in JS, there are many other structures, each with their own strengths and weaknesses. Interestingly, while the principles of the iterator protocol are universally applicable to most languages, some others, for example, Java, embrace it more deeply. Most likely because its standard library readily provides multiple List, Stack, Queue, and Tree implementations, while in JS we only have Arrays, and just only more recently, Sets and Maps. It’s also surprisingly hard to find a decent third-party library that provides these structures, let alone embrace the now-standard iteration protocol. No wonder why Java programmers are more algorithmically conscious. An iterator is a stream of elements, and we don’t care how they are produced. Since JS uses duck typing, making an object an iterator is a matter of having a conforming next function, that returns an object like {value: ..., done: bool} . The value is the next element, and done indicates if the end of the iterator is reached. The following object is a valid iterator that returns 1, 2, then 3 ( Try it ): Two important things to notice here. First, the done: true comes after the last element . This is because the iterator might need to evaluate some more elements to realize there are no more to output. For example, consider an iterator that outputs all primes less than 10. After 7, it needs to check 8 and 9 to see that 7 was the last one. Second, iterators have state . That is, when an iterator is used once, they can not be used again. Think of them as one-shot traversal over a collection. A simple function that returns an iterator for an array (without taking advantage that arrays are iterables, which will be covered in the next post) ( Try it ): And use it like: In day to day programming, iterators can be recreated if needed. For example, iteration over an array multiple times is possible. In some rare cases, iterators can not be reproduced. For example, for a function that returns a stream of numbers starting from the current timestamp: In this case, no two iterators output the same set of elements. The last example showed that iterators can be infinite, as it never returns {done: true} . This is something traditional arrays can not achieve. On the other hand, it’s easy to make infinite loops, like this: It’s always a best practice to break the loop at some point if you expect a possibly infinite iterator. Iterators are a powerful concept, yet they are underutilized in JS. In the form we’ve discussed in this post they are tedious to use. In the next post, you’ll learn the basics of iterables and the language constructs that support them.", "date": "2017-08-29"},
{"website": "Advanced-Web", "title": "How to create a good Pull Request", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/how-to-create-a-good-pull-request/", "abstract": "Lately, I spend a lot of my time reviewing merge requests. I found that in many cases even a quick check proves that something important is missing, causing communication overhead. In this post, I present a simple workflow to create merge requests that are easy to review and contain minimal errors. The key takeaways are: In the following example, there is a develop branch that serves as the main line of development for our project. This is where eventually all code will be merged, and this is the basis for future releases, so it’s best to keep it in good shape all the time. We write articles like this regularly. Join our mailing list and let's keep in touch. To make it easy to review and avoid adding unfinished code to the develop branch, we use a separate topic branch for every new functionality. Although we didn’t start coding just yet, we already made an important decision: we had to give a name for the branch we’ll be working on. Choosing this name carefully will make the life of the maintainers and code reviewers much easier . I recommend the <branch-type>-<issue-id> format. Use hyphens (-) to separate words in the branch names . Many CI environments use the branch names to create folders, so it’s best to avoid filesystem path separator characters. Create your modifications as it feels the most natural to you. It’s best to strive for a solution that you can try as early as possible. I prefer to do the plumbing and the hard parts first and refine the solution to its completeness, but ultimately it’s up to you to decide how you break down and solve the problem. Just make sure that while you prepare the modifications commit early and often . Coming up with a solution usually requires some experimentation. If you happen to break anything along the way, you can simply revert to a previous version of your code . This reminds me how we played the classic X-COM: Enemy Unknown strategy game back in the days. We always kept several save files around, each taken before a significant decision or strategic move. If things didn’t go as planned, it was easy to go back a few hours or days in game time. (The image above is from Wikipedia. By Source (WP:NFCC#4), Fair use, https://en.wikipedia.org/w/index.php?curid=36710697) There is a tool called git-bisect that might help if you are not sure when a bug was introduced to the system. It provides an interactive menu to perform a binary search to find the commit in question. Most developers are interrupted many times a day — after all, most software is developed collaboratively. When I get back to my computer to continue working and I have no clue about what I was doing before, I usually do the following to reconstruct the context : The former usually works only for a few files, and I can only get back the most recent modifications. On the other hand, git diff shows everything since my last commit. The commit messages can serve as a work log that can come in handy later. You can utilize these benefits the most if you: Adding everything to a single commit after bashing the keyboard for half a day probably doesn’t provide much information about what really happened, and you have a lot less option if you have to revert something. It’s easier to conceive a short sentence about your work while you are currently at it than come back later and try to guess why a particular change was necessary. See this style guide for excellent commit messages. Make sure that you don’t add more than one logical change to a single commit. Later this will make the changes much easier to review, while you have a more fine-grained work log and checkpoints from previous states of your work. For example, Uncle Bob’s Boy Scout Rule is a great guiding principle that tells “always leave the campground cleaner than you found it” . However, if you do this in the same commit with another change, you can’t revert this commit without losing some of your work unintentionally. Also, make sure not to change the formatting of whole files when doing small changes in the codebase . From time to time ask yourself: should I create a checkpoint? To decide, you might check git status and git diff to review your latest uncommitted changes. If there is some part of the solution that you consider to be complete (for example a new module, a logical change or cleanup in the existing code), you can use git add <file-or-folder-path> to stage these changes for the next commit . If a single file contains changes that belong to different logical units, you can use git add --patch <file-path> to interactively stage only parts of a file. Be careful with git add . as it stages everything in the current directory. If you accidentally staged a file, you can unstage it with git reset <file-path> or git reset --patch <file-path> . Once you are finished, check again to ensure that only the necessary changes are staged with git status and git diff --cached . I do this all the time to double check and because git diff doesn’t show the contents of the new untracked files by default. With these commands, I can review every line that I am about to commit. I am doing this in the terminal not in the IDE because the different presentation gives me a new perspective on how my changes will actually look. In this review I tend to: If everything looks fine, create the commit with git commit . The -m flag might be great to quickly add one-line commit messages, but don’t use it if you feel that you need space to express your thoughts. If you happen to realize that you forgot something or misspelled the commit message, you can add more changes and modify the message with git commit --amend . This might seem tedious, but in practice, it rarely takes more than a few seconds each time , and it provides additional opportunities to review your work. Here are a few commands that I use often if I need to go back to a previously working state. Note, that most of these commands are destructive in some way. Use them carefully and always watch out for your non-committed changes. When in doubt, create a backup first! The following command simply drops all non-committed changes of the related files. First, use git log to determine what commit you’d like to revert to. To see what files are affected in each commit use git log --name-status , or to see the changes themselves use git log -p . After you have a commit ID, use the following command to get the earlier version of the file: If you check the state of your working directory with git status and git diff --cached after the command, you can see that a change has been made to the related files to produce an earlier state, and these changes are already staged. If you are lucky enough that your mistakes are isolated in a single commit, you can revert them easily: This creates a new commit that contains the opposite changes of the commit to be reverted. With the -n flag the command creates and stages the changes, but does not commit them automatically. In many occasions, it might be useful to try alternative ideas, and see where they go. This can be especially handy when doing different performance optimizations or trying alternative versions of a UI. To create a new branch based on the tip of the branch you are working on: If you’d like to branch off from an earlier commit, then do the following: After this, you can easily change branches with git checkout <branch-name> The first step is to integrate fresh changes from the develop branch to the topic branch, and ensure that the new feature works well with the most recent code as well. If your topic branch lives long enough, it’s a good idea to merge the new code from time to time to keep up with the main line of development. This task can be accomplished easily with git merge or git rebase . Choosing one over the other can be a matter of taste and project conventions. (For some food for thought see this tutorial and this post from Atlassian about the topic.) For the sake of simplicity, I use git merge in this case. At this point, you have to resolve all conflicts (see git mergetool ). With git log or git diff you can check what commits are about to be merged to your branch. I use the following right before the merge to see the new commits: It’s useful to peek into incoming changes to avoid unpleasant merge conflicts. However, if you run into a merge conflict that you’d finish later, you can abort the merge with git merge --abort . Don’t forget that after the merge it’s your responsibility to test that your code integrates well with the rest of the software. Now the topic branch contains all code in the project with your additions. The next step is to review these soon-to-be published changes. The first step is to build the project and run all the automated tests to see that everything works as expected. If the project lacks decent automated test coverage, it’s best to do some manual checking as well. Next, ensure that everything that is required for the scope is finished . Finally, with following command you can examine every modification in your branch: Use this for a self code review. As a rule of thumb, look for things that need improvement or look suspicious. It’s also might be a good idea to create your own checklist based on the project conventions and your habits. If anything is missing or broken fix it and add a new commit. If everything looks fine, it’s time to hit git push and ask for a review. (Of course, if you are lucky enough to have a CI server that builds and tests your topic branch then push frequently.) The problems found by the reviewer should be handled the same way as the problems caught with your personal code review. This simple workflow is mainly about continuously reviewing your work, and mindfully adding new elements to the project’s history. When I review code this is one of the things I value the most. There are many aspects of effectively using Git, and it’s really flexible. Make sure to shape your conventions and processes to tailor it to your needs.", "date": "2017-08-22"},
{"website": "Advanced-Web", "title": "Introducing: Weekly JS Tips & Tricks", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/introducing-weekly-js-tips-and-tricks/", "abstract": "If you like the articles on this blog, I’m sure you’ll love my new weekly newsletter of short tips on Javascript and WebDev in general. They are based on the experiences I’m gaining while working with SPAs and backend technologies, both work-related and hobby projects. They are short and practical, usually accompanied by click-to-try examples. Along with plain old (and new) Javascript, Promises, RxJs, Lodash, React, and similar mainstream technologies are also covered. Learn The best part: it’s free and without any hidden commitments. Unsubscribe at any time. Check it out and give it a spin.", "date": "2017-08-15"},
{"website": "Advanced-Web", "title": "3 ways to set up author information in Git", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/3-ways-to-set-up-author-information-in-git/", "abstract": "Setting up author info is one of the first tasks to do before you can start using Git. Doing it right and carefully can be much value when digging into the project history later on. After installing it, you have to set your name and email address in order to use Git. This information will be attached to each commit you make, so without it, you won’t be able to create commits. It’s important to specify your name and email correctly because it is the only way for developers to find you in case they have any question regarding your changeset. We write articles like this regularly. Join our mailing list and let's keep in touch. If you are not sure what your current user.name or user.email is, you can check them with the following command: Note that it lists other settings as well, such as aliases and the default push action. If you have projects that you’d like to contribute as a different author, you can configure these repositories to use unique user.name and user.email by omitting the --global flag from the previous commands: This can come in handy if you have hobby projects on your work machine. Git does not support switching between preconfigured authors, so if you need to change the author name or email frequently, I suggest creating an alias or a script for it. (Check this post for some useful tips.) As with global settings, the repository-specific configurations can also be listed with the git config --list command. In this case, the user.name and the user.email will be listed twice: the former occurrence of each setting is the global, while the latter is the repository-specific value. Git will always use the last values from this list. Git currently does not support multiple authors for a single commit, but there are several options to overcome this shortcoming. In this section I’ll go through some of them: The easiest solution is to list all the authors in the user.name setting. This can work well, if the pair-programming session lasts for a couple of hours, spanning multiple commits, as it has to be set only once. Just don’t forget to revert to your original settings afterward. Another solution might be to mention the other author in each relevant commit message. As it’s mentioned in the Git Wiki’s Commit Message Conventions section, Trailers provide a way to include additional information in a header-like, searchable, easy-to-parse format at the end of a commit message. Consider this example commit message: Conveniently, there is a tool to add trailers, called git-interpret-trailers . Custom commit message templates provide another way to add trailers, so you don’t have to append it every time you commit. Just create a simple text file, for example, .git-commit-template with the Co-authored-by trailer described above, then run the following command: (Of course, you can set it up globally just like any other git config.) At the end of the pairing session, the commit template can be reset to the default with git config --unset commit.template . Finally, you can augment your commit messages all at once with the necessary co-author information just before pushing with interactive rebasing. Be careful with this though, make sure to only edit unpushed commits, or commits on private branches. As it rewrites Git history, it can cause problems if others are on the affected branch. Git store the name and the email of two persons for each commit: the committer and the author. The difference between the two is that the author is the person who wrote the changes, while the committer is the person who uploaded them the repository. You can list this information with git-log : Typically they are the same if you commit your changes by yourself, but this can be a handy tool if somebody sends you a patch via e-mail. You can preserve the identity of the original author while taking credit for the commit: This infrastructure might be used for pair programming as well as to mark the participants, but I think the other alternatives mentioned in this post are better because most of the tools don’t show both the committer and the author by default. Git supports various workflows and can be bent to work in most situations. It’s worth the time to carefully craft appropriate metadata associated with each commit, as they can provide valuable information later on.", "date": "2017-07-25"},
{"website": "Advanced-Web", "title": "Why Webpack 2's Tree Shaking is not as effective as you think", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/why-webpack-2s-tree-shaking-is-not-as-effective-as-you-think/", "abstract": "As WebPack 2 barrels forward, Tree Shaking — or more technically, the removal of unused exports using static analysis — is finding its way to the mainstream. Developers are putting their hopes high, as it promises to solve the pressing problem of bloated packages. Usually, only a fraction of code is actually needed from each dependency but their entire codebase is bundled, increasing the size. The promise of Tree Shaking is to remove this clutter during the build, allowing developers to add dependencies without worrying about the user experience. But even a quick search indicates that it might not work that well in practice. What causes this discrepancy? Let’s find out! We write articles like this regularly. Join our mailing list and let's keep in touch. Let’s consider an example on how it works. As it optimizes exports, add a lib.js which exposes two variables: Then add an entry.js , which imports them but then uses only a : Since b is not used in entry.js , it can be removed, but it’s nothing new; UglifyJS already does that. But then the export b in lib.js is also no longer used, therefore it can be removed too. Inspecting the bundle, A_VAL_ES6 is present, but B_VAL_ES6 is not. When talking about Tree Shaking, I have to mention rollup.js . As far as I know, it was the first full-fledged bundler that supported the concept of exports removal. It is available for more than a year now, but I’m yet to hear the success stories of massive amounts of saved bytes. The code is available if you want to reproduce the results. Lodash-es is the ES6-compatible version of Lodash. It has the same functionality, but instead of exporting in the UMD module format, it uses ES6 modules. Its main simply re-exports every module, and as Lodash is a “utility belt” collection, it should not matter if an individual part or the whole bundle is included. Therefore, import {map} from \"lodash-es\"; and import map from \"lodash-es/map\"; should be equivalent. To set up a testbed, add a package.json with dependencies to WebPack, Babel, and lodash-es: Then add a webpack.config.js with some minimal WebPack bootstrapping: Note the {modules: false} part. Babel converts everything to CommonJS by default, and while it is a great way to achieve the widest compatibility, it prevents exports analysis. This config turns it off, as WebPack supports native ES6 modules from version 2. After the boilerplate is in place, add an entry.js with the import: Running npm run build and inspecting the bundle, it weights 139.224 bytes. Then change the import to the individual module: The result is a mere 25.531 bytes. This indicates that Tree Shaking is less effective in a real life situation than optimizing by hand. The code is available to test drive both module formats. To understand the limitations of static analysis, we need to look into the differences between CommonJS and ES6 modules. Note: Libraries mostly use UMD for better compatibility, which is usually interpreted as AMD and not CommonJS. But since they behave the same to the extent of this rundown, and CommonJS is, well, more common, I’ll use that as an illustration. In a CommonJS environment, the exports object is all that matters. After running the code, the contents of that object will be exported. For simple cases, it’s common to simply set the properties: This will export both a and b . But CommonJS can set the exports dynamically: This will export i0 , i1 , i2 , i3 , and i4 . It even does not require exports to be deterministic: This will export rand based on luck. This dynamic nature of CommonJS nicely fits the dynamic nature of JavaScript itself but completely defeats static analysis. As an illustration, add a lib_commonjs.js that exports two values: Then modify the entry.js to import both, but only use one: After bundling and inspecting the results, both A_VAL_COMMONJS and B_VAL_COMMONJS are present in the file; nothing was removed. This indicates that Tree Shaking is not working for any CommonJS module. Since many libraries exports primarily in AMD/CommonJS, no big gains can be expected until that changes. ES6 modules are static in nature. They must be deterministic and no dynamic exports are allowed. This opens the way for static analysis. To see it in action, we already have a lib.js that exports two values: Then modify the entry.js to import both, but use only one: Inspecting the bundle, only A_VAL_ES6 is there, b ’s value is removed. The root of the problem is side effects . In many use-cases importing a library does not necessarily result in a bundled piece of code that is completely separated from the rest of the app. For example, using the css-loader to import css from 'file.css'; , the contents of the variable is not important; but the style is already applied to the document. If WebPack would remove all unused dependencies regardless of side effects, imports like this would break. As a consequence, side effects must be retained. They provide expected functionality when they write the console, add a style tag or modify the HTML in other ways, assign global variables, and so on. But there is another category of code, improperly identified as side effects. An Object.freeze does not modify anything, so as all function calls that are pure. These should be also be removed. As an illustration, modify the lib.js to export a frozen version of the value: This minute change results in the inclusion of B_VAL_ES6 in the bundle. Similarly, a simple function call also triggers this behavior: This results in the inclusion of almost the whole library, even though there are no side effects. Side effects are hard to identify properly. There is some ongoing work , though. Bundlers choose the safe path and rather include unneeded code than break the app. This results in bigger bundles and less effective code. But Tree Shaking might still save a few bytes, and it’s a good thing; everything that lowers the bandwidth requirements of a webapp is a change in the positive direction. But in practice, its efficacy is a lot less than the expectations. People are working on solutions to better identify side effects. But their dynamic nature prevents a solution that works for every codebase. Instead, there are certain heuristics that can be used. There is also a proposal to annotate pure functions, but community-wide support is unlikely to happen anytime soon. Tree Shaking might help a bit, but small bundles are still far away. As jdalton pointed out , babel-plugin-lodash and lodash-webpack-plugin cherry-pick what you actually use from Lodash. To save some bytes, they are definitely worth checking out.", "date": "2017-02-07"},
{"website": "Advanced-Web", "title": "Getting started with Browserify", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/getting-started-with-browserify/", "abstract": "Browserify bundles your code and brings all the power of Node to the browser. It not only lets you require files, but also provides a range of Node libraries for a seamless front-end dev experience. If you’ve used to write code for the server-side, you’ll find it familiar. The easiest way to get started is to install globally with NPM : (Note: You might need sudo ) Then create an index.html , with a reference to the bundle.js Browserify will build: Finally, an app.js , that will be packaged: We write articles like this regularly. Join our mailing list and let's keep in touch. To bundle it , use the global Browserify install, specify the main entry point and the output file: This creates the bundle.js , that is referenced in the index.html. Host the files in an HTTP server of your choice, and verify it works. Let’s modify our Hello world to use some third-party library ! In this example, lodash is used to transform a map in a cross-browser way. Note the Node-style require call. Browserify supports the CommonJS module format, the same you use on the server. Building the app with browserify app.js -o bundle.js raises an error. Unmet dependencies are compile-time errors , rather than run-time ones. Browserify supports the NPM dependency resolution process out of the box, therefore installing lodash is done with NPM: The bundle call browserify app.js -o bundle.js will succeed this time, and the code works. Since Browserify supports CommonJS, it allows breaking the app into modules. For a simple example, a list.js can export a value : And the app.js can import and use it, just like any other module: Unlike JSPM, Browserify does not require using its own CLI tools to manage dependencies; everything is managed by NPM. This makes it easy to integrate into NPM scripts , as you only use a handful (ideally two, one for production and one for development) of scripts. The first step is to init an NPM package . There is a built-in next-next-finish-style wizard you can use: After you have a package, npm install <library> --save will add the library to the dependencies section in the package.json. Use --save-dev to install to the devDependencies section, as it’s compile-time. Add browserify as a dependency to the package.json : Then add a build script that calls Browserify to produce the bundle.js : After this, npm run build will regenerate the bundle.js . The upside is that Browserify is no longer needed globally , it is managed on a per-project basis . Any developer who has NPM set up can check out your code, run npm install followed by npm run build , and they have a freshly-built bundle. Constant manual rebuilds are tedious and hinder development. There are some tools that add automatic watch and rebuild functionality to Browserify, one such is watchify (note that most Browserify tools end with -ify). To install it , list it as a dependency (or devDependency) in the package.json , then run npm install . Then add a new script that will be the “dev mode” from now on: (Note: With -v , watchify is going to print a message to the console for each rebuild. It’s quite useful to see if it correctly picks up the changes) Then npm run dev can be used during development , as it monitors changes to the app.js as well as its dependencies and automatically rebuilds the bundle. It is suitable for development, as you only need to reload the browser. To further aid the development, add the --debug flag which includes source maps into the bundle. It’s super useful. A distinctive feature of Browserify is that it replaces some Node-only libraries and values with browser-compatible counterparts. As a result, code written specifically for the server-side might work in the browser. For example, the Buffer module, which is missing from the browsers, is usable: There are quite a few modules supported. This gives a good chance of cross-compiling server-side modules. Apart from modules, some node-only variables are also supported. One such is __filename , which contains the currently executing file. As an illustration, this prints the js file it is defined in: The __dirname also works. Transforms are the primary way to add functionality to Browserify. If you need to support something, first take a look at the list of transforms , then google around at GitHub; it’s likely that you will find a suitable one. Transforms are installed via NPM , therefore you can simply list them in the package.json . As an illustration, let’s configure the versionify transform, which replaces a placeholder string with the package version. To install, add it to the devDependencies section, then run npm install : After the transform is successfully installed, use -t from the CLI to run it during the Browserify build: -t browserify-versionify . The default placeholder is __VERSION__ . To test it, simply print the string: Transforms can also be configured. In this case, the placeholder string is a variable. To pass configuration via the CLI , use the array notation; for example to change the placeholder, use -t [ browserify-versionify --placeholder __NPMVER__ ] . This changes the placeholder to __NPMVER__ . The downside of using the CLI to add transforms is that you need to add them to all your NPM scripts. Since it’s likely that all transforms are needed for all the scripts, it’s better not to copy-paste the config, but define them in a single place. Fortunately, Browserify can be configured using the browserify property in the package.json . Transforms, along with their configuration, can be added there, and all scripts will use them as defaults. To add the versionify transform with a custom placeholder, add the following to the package.json : CSS and SCSS support can be added with the scssify transform. To install , list it as a dependency in the package.json , then run npm install : Then add the transform to the browserify property in the package.json : Now that the configuration is ready, let’s add some CSS. Create a style.css with some basic but easily-recognisable content: Then require it from the app.js : After a build and a refresh , the background is red, indicating that the CSS is loaded. To demonstrate SCSS support , modify the stylesheet and rename it to style.scss : And modify the app.js to require the new file: After a rebuild and refresh , the background is now blue, indicating the SCSS compilation is configured. In order to integrate Bootstrap, add it as a dependency to the package.json , then run npm install : To add the Bootstrap CSS , reference it using a relative path: Unfortunately, Browserify does not package assets , so fonts are still referenced at /fonts . In effect, you need to serve them from under the node_modules directory, which requires some server-side config. Bootstrap javascript requires jQuery to be present in the global scope, so the first step is to install that, by adding \"jquery\": \"3.1.1\" as a dependency and then run npm install . From there, you have two options. The first is to require jQuery and attach it to the global scope manually: The other one is to use browserify-shim to do this automagically wherever it’s needed: Then use it as: This takes care of putting jQuery to the global scope whenever the bootstrap.js is needed. The downside is that it still pollutes the global scope with jQuery , so it’s still far from ideal. Browserify does not do any production packaging ; you can deploy the same bundle.js you use during development. On top of that, you can add any third-party minification, cache busting, and obfuscation as you’d like. One notable exception is uglifyify , which can minify individual modules and not just the whole bundle, and it can save a few bytes on conditional requires. To minimize the code size, you should use this transform as part of the production build, and also use UglifyJS on the final bundle. On a side note, keep in mind that UglifyJS does not support the new language features of ES6 . Therefore, if you use any of them, you’ll need a transpiler like Babel to process it. There is a vibrant community around Browserify, and it results in many transforms . Basically, if your use case is not that special, it’s likely that there is a project for that already. It’s easy to get started . Just a few lines of configuration and you can use all the goodness of CommonJS, which is a great step towards modularized applications. It also integrates with NPM . Apart from the bundling, you don’t need to use the Browserify CLI, and all dependencies are managed solely by NPM. With just a few simple scripts, you can make the build fully transparent, which in turn eases developer onboarding. And finally, it can bring Node-only modules to the browser . If you have modules you want to reuse, it is possible to simply browserify them. There are many limitations, but it’s a chance that might save you a considerable amount of time. While Browserify was the leading bundler in the JS ecosystem, its usage is declining now. It is still a major player, and it is unlikely to go away in the near future, but it’s losing ground every month. The handling of static resources is far from complete. It packages CSS with a transform, but images, fonts, and other static assets are not bundled; you need to take special care to handle them yourself. And finally, only CommonJS is supported out of the box. This might not seem a big drawback, but third-party projects might not support it. Configure Babel if you need other module formats. It’s not a coincidence that Browserify was the leading bundler for years. It solves the modularization problem nicely and follows the best practices from Node development. It’s widely supported by a great community, and it stood the test of time. But taking into account its declining popularity and the alternatives, I wouldn’t choose it for my next project. But if you use it, there is no need to migrate your projects. It will serve you well in the times to come.", "date": "2017-01-31"},
{"website": "Advanced-Web", "title": "Copy-pasting in Vim without +clipboard", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/copy-pasting-in-vim-without-clipboard/", "abstract": "Disclaimer : this post contains some hacks that might be useful if you stuck with a Vim version that does not support +clipboard . If you control your development / operation environment, then install an appropriate version like vim-gtk on Debian or vim-X11 on Red Hat Linux and read this post instead. One typical problem with Vim, is that once somebody enter a piece text, it’s not always trivial to get that back without saving it to a file and opening it with another tool. Many solutions can be found on various forums. They are all great, but I was looking for something more intuitive. For example, a method is to directly copy to the system clipboard with “+y . Unfortunately, Vim have to be compiled with a special feature, which is not always the case. Even worse, copying this way requires a different command than yanking to the unnamed register (the default behavior), so additional shortcuts have to be memorized even for the simplest of tasks. There are many different scenarios where copy-pasting is required, and I would like to keep the trivial cases as intuitive as possible. We write articles like this regularly. Join our mailing list and let's keep in touch. If Vim supports the system clipboard you can set it as the default with set clipboard=unnamedplus , but it can also be inconvenient. For example to change a piece of text inside parenthesis with something from a Stack Overflow comment, you can’t do the following: After the third step the contents of the clipboard will be overwritten with the recently cleared old text, so the newly found solution is vanished. In this post I’d like to share my configuration related to copy-pasting, that allows Vim to step aside and let the terminal handle the text directly. A typical copy-pasting task is to get a single line (or a few words) of code from Vim, and search for it online. Unfortunately, set mouse=a can prevent that, so the first thing to do, is to disable Vim’s mouse support. Although it’s not enabled by default, it’s common to use it. I enabled it as soon as I found this feature Nowadays, I aim to use the mouse as rarely as I can, so I dismissed it. Now I can use the terminal emulator’s facilities to copy text from the current page, just as I would do from any other command line application. This simple thing solves my most frequently arising problem and it requires no additional commands. As a bonus, I can use other features of the terminal too, such as opening links directly in the browser. Tip: if you prefer to keep the mouse support, just press shift while selecting to temporarily disable Vim’s mouse-awareness and let the terminal handle the interaction. Copying multiple lines is tricky when you have a sidebar on either side of the text, so it’s useful to create key mappings that can toggle these things. I mapped the F11 key to toggle line numbers and the gitgutter . (Multiple commands can be executed by separating them with <bar>.) With every distractions removed, one can easily copy a whole screenful of stuff from Vim. For the rare cases where I need to copy multiple pages, I created a mapping to pass the currently edited file to cat . With F12 I can temporarily dismiss Vim, and view it simply in the terminal. Taking advantage of it’s scrolling, even multiple pages can be copied easily. With this configuration copy-pasting is easy, and for the most part, it doesn’t require special key combinations to be memorized. Even moving large amounts of text is possible with a few additional mappings. Happy Vimming!", "date": "2017-01-24"},
{"website": "Advanced-Web", "title": "How to create a Theme Plugin for Magento", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/how-to-create-a-theme-plugin-for-magento/", "abstract": "Magento is one of the most powerful, actively developed open source webshop engines around. It has rich built-in functionality and a wide variety of plugins and themes available online. Because it’s based on PHP, it can even be deployed to cheap hosting services . These were the main reasons I chose it for a recent project, despite the fact that I rarely code in PHP, and I know other platforms much better. Also, it has a huge community, and amazing documentation , so finding resources online is really easy. We write articles like this regularly. Join our mailing list and let's keep in touch. In this post, I collected some tips about how to get started with Magento and create a simple site with a custom theme. Many service providers maintain packages and tools for it, so you can install Magento easily with cPanel in a few clicks . However, if you decide to roll your own server, it’s easy to install Magento yourself. The documentation mentions that while it’s possible to directly change the design files in the Magento installation, it might complicate upgrades in the future. Instead, the design changes should be contained in a separate Theme plugin. To create a custom Theme, first create a folder in the following location: app/design/frontend/<Vendor>/<Theme-Name> . In the example below I use advancedweb as the Vendor and my-first-theme for theme name. Then create the following 3 files that contain the necessary information to declare and register your theme: In the theme.xml you declare the theme, and specify its parent: The composer.json is required to distribute your Plugin as a Composer package. The official guide suggests that this file is optional, but I could not get my theme working without it. It lists what packages are required for your plugin to work and points Magento to the registration.php , that has contains some initialization code needed by Magento to load the theme: When these files are in place, the Theme can be applied to the whole store in the Stores / Configuration panel. The theme can be applied to single pages or shop items too. This is handy when you have to add small design changes for just one page. For this, simply create a new child theme that contains the necessary changes, and use that in the page. Before adding anything to the theme it’s important to note that Magento in production mode aggressively caches the static view files to provide better performance. This is really inconvenient during design development, so make sure to run Magento in development mode . Otherwise, you have to flush Magento Cache after even simplest of CSS changes in the System / Cache Management admin panel. (Also flush Static and Javascript/CSS Cache files before issuing full cache clean.) Now the theme doesn’t do anything special yet, it looks just like the default one: So let’s add some custom styling rules to make it a bit more unique. First, create a web/css folder and a custom.css file in the theme folder. For this simple example, this file will contain all our style modifications. Then create a default_head_blocks.xml file in the Magento_Theme/layout folder. In this file, you can specify the CSS files to be loaded with each page. The same goes for the script files, just add them to your theme and register it in the XML configuration above. Make sure to copy this file from the parent theme you’re using to avoid accidentally leaving something out. Set your Magento application to developer mode if you are modifying the theme files, or just clear Magento Caches in the System / Cache Management administration panel after you are done with the modifications to see the result. The logo and the favicon can be uploaded on the Stores / Configuration / Design admin page. It’s worth checking out the parameters you can tune, especially in the HTML head section. The footer content can be defined on this page as well. You can simply change the copyright text, or inject custom HTML code. The localization options are also found in the Store configuration panel. In the General page, you can set the Country, State and Locale options. The options regarding prices can be found under the separate Currency Setup category. You can ship your custom localization files with the theme by adding CSV files to the i18n folder in your theme. The name of the CSV file should be in Magento’s locale format, like en_US.csv, and it has to contain two columns. The first column is the localization key, while the second one is the localized value in the given language. What’s interesting in Magento is, that the localization key is not some arbitrary keyword, but the original English version of a given text, like “Shopping Options” . This makes it very easy to translate or customize most labels in the webshop. As we’ve seen in this post, it’s quite easy to create a custom theme for Magento, making it possible to tune the look and feel and client-side scripts. With a Theme Plugin, you can easily create unique sites for your visitors.", "date": "2017-06-13"},
{"website": "Advanced-Web", "title": "Profile-based optimization techniques in the JVM", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/profile-based-optimization-techniques-in-the-jvm/", "abstract": "Tuning the performance of Java applications is not always trivial. Even a seemingly simple program can work differently under the hood than we might expect it when we just skim its code. While it’s not necessarily a life-saver in everyday coding, it’s worth knowing the optimizations performed by the JVM when it comes to performance optimization. The JVM uses deferred compiling. While javac merely translates Java to Bytecode with minimal optimizations, the JVM does the real compilation Just In Time (JIT) to produce performant native code. The benefit of this approach is that the compilation step has more information about where and how the code is executed. For example, while the program is platform-independent, it can be compiled to the exact architecture where the application is running to take advantage of specific OS or hardware features. An even more interesting feat is that the JVM collects usage statistics of the application. It tracks what kind of parameters are passed to each method, or which parts of the code are utilized the most. Based on this data, a number of further optimizations are possible, making the program even more performant. We write articles like this regularly. Join our mailing list and let's keep in touch. These profile-based optimizations are called optimistic or speculative techniques because rather than mathematical proofs, they are based on the runtime characteristics of the application. For example, the JVM might assume that code in a branch is never going to execute because so far the condition has always been evaluated to false. In this case, it might safely remove the unused branch. If the runtime behavior changes dramatically, for example, by supplying unusual input, some optimizations made by the JVM may cause problems. If the condition in the example above ever becomes true the optimized code would not work as expected, so the code has to be deoptimized before continuing. Later, the compiler might attempt to optimize the related code again, taking the new information into account. To get some idea about what profile-based optimizations are supported by the JVM, check this wiki page and these posts . Last year I crafted several small programs to trigger and measure certain optimizations, but I only got so far to create test cases for optimizations that do not depend on statistical data. While those can make huge performance boost as well, it’s not a specialty of the JIT compilers, many of those optimizations can be performed beforehand by many modern Ahead-of-Time (AOT) compilers, so I decided to do some experiments with the optimistic optimizations as well. In the test cases I aimed to The measurements were done with JMH (Java Microbenchmark Harness), a library that aids building and running microbenchmarks. The exact numbers in this post are just for illustration purposes, the measurements were made on simple code snippets, not on real productions systems. Contents of this post are mostly based on my observations and the linked resource I could find. The code of the performance tests are available on Github . Inlining can eliminate the overhead of a method call by substituting the call (the call site ) with the body of the method. More importantly, inlining makes additional optimizations possible , by moving code together and removing method boundaries. Because of inlining increases code size, it has to be applied carefully. The compiled code is stored in the Code Cache , which is a special heap with a finite size. If it runs out of space, no further code can be compiled. The cost of the inlining depends on the size of the inlined method and the number of call sites that are inlined. So blindly inlining everything is not an option because large functions could bloat the callers. Many classic compilers can inline methods and functions, but the JIT can do it more aggressively , thanks to its access to runtime data that highlights the call sites worth inlining. The JVM uses multiple heuristics to determine if a method should be inlined or not. For example, it tries to inline trivial methods, like getters and setters as well as frequently called methods to a certain size limit. The exact parameters of the inlining policy can be changed via runtime arguments, but I’d advise against it. The defaults are tuned for optimal performance by JDK engineers, so except for a few really special cases, it’s unlikely that somebody will come up with any better. Instead, if you suspect that there is an inlining related issue in a critical code path, just enable the following command line flags, to see what the JVM does under the hood. (Looking at the diagnostic log of a larger application can be disturbing, but there are tools to visualize it, such as jitwatch . Here is an excellent presentation about it.) In the following example, some calculations are required for every user interaction. Every user enters two integers, and the calculation just adds them and reports the result back to the user. However, when the second number is zero, a special error handling logic is executed. Measuring the application with inputs evenly distributed between 0 and 50000 reveals that it can only serve 10,900,000 calculations per second on average. Running the application with the diagnostic parameters reveals that the JVM can’t inline the doSomeCalculation method to its caller because it’s too big: This is a problem because this method is executed for every single user request. Moving the edge case handling logic to a separate method allows the JIT to inline the call, significantly improving the performance. This is a rare case where improving performance also improves readability. Investigating the diagnostic output shows that the doSomeCalculation method is now inlined, but the JVM could not do anything with the complicated handleEdgeCase . That seems OK for now because the edge case handling only affects a small portion of the user requests. Running the microbenchmark again reveals that the new throughput is 2,256,900,000 calculation per second. (This extreme difference in performance is largely due to the simple “business logic”, the gain would be proportionally smaller if it were more complex.) Watching the diagnostic logs it seems that the JVM usually undoes the inlining whenever the inlined code has to be deoptimized, but I couldn’t measure temporary regression in performance. In this case, I’ve changed the test to pass only positive integers in the first few minutes, then call the method with numbers from 0 to 50000 as before. When the method was called with the first zero parameter, it had to be temporarily deoptimized because the JVM optimistically eliminated the error handling branch (it was a victim of an optimization called untaken branch pruning - more on that later). Inlining gets tricky when polymorphism is involved because the call site might refer to different methods depending on the type of the receiver object. Consider for example that the previous calculation has multiple implementations: Inlining the following hot calculator.doSomeCalculation(a,b) call would be beneficial. Luckily the JVM collects type profiles for the polymorphic call sites, and given enough time, it will try to figure out how it can inline them effectively. When the handleRequest is called with only one type of a calculator, then the call site is monomorphic , so it can be easily inlined. The JVM can detect and inline bimorphic cases as well, making the code contain a type check to determine which inlined code should be executed. When more than two types are occurring regularly in the call site, the JVM does not inline it. I was hoping that I can see some optimistic inlining when the call site is dominated by one type but occasionally members of 2 other types are passed as well. Shortly after the startup, the JVM inlines the method, because it has seen only one type so far. But after supplying a new type, it quickly deoptimizes. After a while it concludes that it’s a bimorphic call site, so optimizes it accordingly. But soon after, another yet unseen type is supplied, so the code is deoptimized. After the JVM determines that the call site is dominated by one type, the inlining happens again. The inlined code is prepared to handle the known rare types, but it’s optimized to handle the dominant one, possibly by inlining the most frequent case. So, to summarize so far the JVM optimizes based on the concrete types supplied as arguments. If only one type of an object is supplied to a method at a call site, it will be specialized to handle that case. At this point I got curious, what happens if I drastically change the input: The first two cases worked out as I expected. After the first change, the code was reoptimized to handle the bimorphic call site. When I increased the number of types to 5, the doSomeCalculation method was marked as not inlineable as the call site became megamorphic. But after the third change, the JVM behaved interestingly. Looking solely at the diagnostic logs it did not inline the method call again. I tried to run the test for a while to give some time for the type profiles to accommodate, but the inlining still did not happen. It seems that the JVM concluded that the call site is megamorphic after the second change, and did not dare to poke it further. To avoid issues with branch prediction (more on that later), I measured the performance after each experiment by calling the method with a single type again. The chart shows, that the monomorphic and bimorphic call site was inlined as expected, and had the better performance than the megamorphic call site. In general, a CPU has to do many things when it tries to execute an instruction: decode it, fetch the associated data, process the instruction, and write the results. A modern processor has separate hardware for these stages, so one instruction does not have to wait for the previous one to fully finish, they can execute in parallel. For example, as soon as an instruction is decoded, the decoding of the next one can start immediately, there is no need to wait for the previous instruction to finish completely. This technique is called instruction pipelining. However, when the execution gets to a branch, the next instruction is decided by the result of the preceding one, so this parallelism can not work. What happens is that the CPU tries to guess which branch will be taken, and executes instructions accordingly. If it guessed correctly, then there is no performance hit, but if it’s wrong, the pipeline has to be flushed, wasting the previous effort and starting over with new instructions. (For an example, check this Stack Overflow thread to find out why is it faster to process a sorted array than an unsorted one.) The JVM monitors the branch execution frequencies in the code and tries to compile it accordingly, to minimize the overhead of the branching, taking the characteristics of different hardware into account. I was thinking that it might add some hints to the generated machine code to aid branch prediction, but this mail thread found in this Stack Overflow answer indicates otherwise. So, the following experiment is to measure optimizations by both the JVM and modern hardware. The following code does something when the given number is greater or equals to zero. It’s is already warmed up, both branches are taken at least a few thousand times. As it can be seen on the chart above, the more predictable which branch will be taken, the better it can be optimized. The system quickly follows the changes in the trends, for example, changing from only positive numbers to only negative barely impacts the average performance. On top of that, the JVM can detect if a branch is never taken, and in that case, it can omit it from the compiled code to reduce its size and help improve other optimizations. The compiled code contains an uncommon trap for the event of the pruned branch would be necessary. When such trap is hit, it triggers the JVM to deoptimize the code. A common class of errors is the null pointer dereference . Trying to read the value of a null pointer causes a segmentation fault. In general, processes can handle this, or allow the OS to handle it for them, which usually results in the termination of the process. The abnormal termination is usually not desirable, so the JVM is generously turning these signals into NullPointerExceptions . So, even if we are not explicitly checking for it, the following code is handling the case when the input String is null, and if that so, it throws an exception. The implicit null-checking has a cost, but the JVM can optimistically omit it if the collected runtime profile shows that no nulls are passed to this method. The following diagram illustrates what happens to the performance characteristic of the sample program after deliberately injecting a few nulls. The chart above illustrate what happens to the performance profile when we pass a null to a method that is generally called with non-null values. The first time the mix method was called with a null, there was no explicit check in place because the JVM optimistically removed it after the first few thousand calls. Dereferencing that null pointer caused a segmentation fault in the compiled native code. It was expected and handled by the JVM to recover, optimize the code, and to throw a java.lang.NullPointerException . For more info about the JVM signal handling, check this guide. It was exciting to poke the JVM and watch various optimizations altering the performance characteristics of the test programs. I’ve learned some important lessons along the way from these experiments. The first is that there is a huge gap between the code you write and the code that will execute. It’s always worth to measure before and after optimizing something, and we should not solely rely on our instincts, especially when we just look the static code structure for example on a code review. Knowing the potential gain in performance might help to justify the required additional effort and code complexity. However, measuring performance with microbenchmarks can be challenging. I’ve done it a couple of times, yet I vastly underestimated the effort required for this post. There is an article with some guiding principles about this topic, I recommend everyone who wants to write microbenchmarks to read it. In real scenarios, where the network, database etc. comes into play the factors measured in this post count, but there are much more going on. Make sure to not only measure performance at the micro level but throughout the whole application, in production-like settings. I like to think to microbenchmarks as low-level units test with fuzzy results. It’s good if they indicate good performance, but the end-to-end performance still needs to be tested.", "date": "2017-03-01"},
{"website": "Advanced-Web", "title": "JSPM basics and review", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/jspm-basics-and-review/", "abstract": "JSPM is a package manager on top of the SystemJS loader. Advertised as frictionless , it gives you all the comfort of modern JS development with ES6 modules, Babel transpilation, packaging, and proper dependency management. To get started, the easiest way is to use NPM: (you might need to add sudo in some cases) The next step is to initialize JSPM. Theoretically, you could do it by hand, but it’s far easier to use the built-in next-next-finish-style wizard. We write articles like this regularly. Join our mailing list and let's keep in touch. This creates (or updates) a package.json with a jspm section. This lists the jspm dependencies, separated from the other ones. Also creates a config.js , which is the main configuration file. The first part reflects what you answered at the wizard, and there is a map property which maps the dependencies’ names to paths. JSPM can configure a transpiler that will be used for the imported scripts. You can choose Babel, Traceur, and Typescript at the moment. This makes possible to use all the modern goodness of Javascript without much configuration. For a simple hello world example, create a main.js with the content: Then add an index.html: The important parts are the three script tags. The system.js is the SystemJS loader, which is the core element. The config.js is the configuration we’ve just generated; it specifies how to resolve the dependencies and also some other configurations. Then the System.import loads the script, which in turn prints the Hello world! to the console. To run it, simply host it on a web server and open in a browser. There is no compilation step, every change is reflected in the page after a reload. The easiest way is to use npm install -g http-server , followed by http-server , which hosts the current directory at port 8080. All we’ve added so far is boilerplate. Let’s do something useful with JSPM and add an external dependency! To add a dependency, the easiest way is to use the JSPM CLI, which takes care of not only adding it to the package.json, but also updates the config.js accordingly. To install lodash, simply issue: Then it’s ready to use in your scripts. Update the main.js to use the newly introduced dependency: As you can see, you can use ES6 imports out-of-the-box. The modules themselves can use ES6, AMD, CommonJS, and global. As most of the modules use UMD, which is a combination of formats for compatibility, JSPM should work with just about anything. Jspm install uses the JSPM registry by default. You can use GitHub or NPM if you’d like by adding the github: or the npm: prefix to the package name. To uninstall something, again, use the JSPM CLI: This removes the package from the package.json as well as from the config.js . Since CSS is just a static file that does not need any preprocessing, you can simply put it next to your app and reference it from the html. This works, but this approach is not modular. After a while, you’ll end up with a bunch of link tags, and the link between the component and its styling is lost. Fortunately, plugins can be added that control how resources are processed. One such plugin is plugin-css , which, as you can probably guess, loads css files. Add a style.css , with some easily-recognisable content: Then install the plugin with JSPM CLI: And finally, add the reference to the main.js : Note the bang after the filename. After reloading the page, the background is now red, which indicates that CSS imports are working. While CSS is good for some quick styling, any moderately-sized project would benefit from a CSS preprocessor. Some people prefer LESS over SCSS, but I tend to use the latter. To add SCSS support, first rename the style.css to style.scss . Then add some SCSS-only stuff, just to verify it’s indeed preprocessed: The next step is to install the Sass plugin : And finally, update the reference in the main.js to point to the renamed file: After a page reload, the background is now blue. One other thing that you might notice is that it takes a few seconds to load the page. This is because everything runs inside the browser, and it needs to download like 7 MBs of libraries just to do the SCSS compilation. This sluggishness is gone in production, as packaging runs these compilations ahead of time. Bootstrap integration is a particularly good challenge for build tools, as it contains static files (fonts) as well as js and css, and it’s also trivial to check visually. The first step is to install the dependencies: Then update the main.js to include both the js and the css from Bootstrap: Then add some visuals to the page, both some Bootstrap classes, and an icon to test-drive the fonts too: After a refresh, the magnifier icon is shown with the bootstrap margin. Up until this point, the user experience is questionable. To say the least, loading SystemJS adds an extra roundtrip, let alone various plugins might load a lot of libraries. JSPM supports packaging the app, which not only compiles everything ahead of time, but also packages the app into a single script. This renders the bundle on par with other tools in regard to performance. To bundle the main.js into bundle.js, use: After that, instead of all the other script tags, just include the bundle : The app works as before, but is considerably faster. To further save a few bytes, JSPM can minify too. To enable it, use the --minify switch: Notice that it will also mangle the variables. As some libraries rely on names to work (most notably Angular 1.2 for dependency injection), it may break your app. To disable it, use --no-mangle . This increases the bundle size by a few bytes, but it’s hardly noticeable after gzip. At first sight, it seems your app is contained entirely in the bundle.js, and that’s the only file to deploy to production. But unfortunately, this is not true. If you look at the bootstrap example, you’ll notice that the fonts are still served from the jspm_packages folder. In effect, the jspm_packages folder needs to be served along with your app, with all the dependencies, making the package unnecessarily hefty. Another problem is that URL rewriting is done by the individual plugins (for example, the icons in bootstrap are located by the css plugin). This means that the configuration is likely to be different depending on which plugins you use. As a challenge, try to work your way with the Sass plugin to integrate Bootstrap. Also, you might have a hard time referencing static files directly from js. For example, there is an images plugin , but that does not do any packaging. The first thing I’ve noticed is that it’s very easy to get started with JSPM. With just a few commands and a few lines of code, you have a project up and running with proper dependency management, Babel transpilation, and ES6 modules. I also liked that the development runs entirely inside the browser , without any server-side tools. Just modify and reload, only a basic http server is needed. The config.js gives control over how plugins and the loader work. There are many subtle options that can be set up here. And finally, there are plugins for the basic things . If you are just getting started with JSPM, it’s likely that you’ll find a plugin for your needs. What I immediately noticed is that the package.json and the config.js must be in sync at all times, otherwise the dependencies are likely to break. The CLI tools, like jspm install and jspm uninstall take care of updating everything, but keep in mind that you have to use them. As the CLI tools manage parts of the package.json and the config.js , it makes them fragile . You edit some parts of them by hand, but you manage other parts via tools. This also makes merging more difficult , in case of a conflict. Handling static resources seems unsolved. There is no central way of doing this (except to reference them by hand to the jspm_packages ), and it’s controlled by the plugins. This makes the whole process inconsistent; for some use cases, it works like a charm, but for others, it requires googling around and diving into various configurations. The biggest drawback I’ve seen is that you need to serve the jspm_packages folder along with the “self-executing” bundle, which defeats the notion of self-executability. On most cases, you can alleviate this by using a different build tool along with JSPM, like Grunt or Gulp, but those need different configurations. Finally, after I added the Sass plugin, the developer experience deteriorated. It took more than 4 seconds to load all the files, which is simply too long. As far as I know, there is a dev server which supports live reloading, that might solve this with some config. JSPM is a well-engineered technology and I really like the notion that it runs everything in the browser, a unique feature of its kind. But given the alternatives and its clumsy way of handling some basic web-dev tasks, I’m unlikely to choose it for my next projects. As I’ve seen from various places, some people are using it successfully; I’m sure that after you’ve figured out the workarounds for your particular project, it does a good job. But unless you have a good reason, consider choosing something else.", "date": "2017-01-17"},
{"website": "Advanced-Web", "title": "Managing Scala projects in Vim with Ag and Ctags", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/managing-scala-projects-in-vim-with-ag-and-ctags/", "abstract": "Last year I wrote a short summary about the Vim plugins I was using at that time for Scala development. Those were the low-hanging fruits I could easily configure to solve my initial comfort problems with Vim. Since then I’ve attempted to tackle larger projects. There were some useful plugins in my original setup, for example: I still use the two terminal setup: one Vim session to edit the source files, and one SBT console to compile and test the code. This setup is really handy, especially because SBT can watch file changes and run the tasks accordingly. I’ve switched from KDE to xmonad , the tiling window manager, because its keyboard-oriented philosophy is better suited to this configuration. We write articles like this regularly. Join our mailing list and let's keep in touch. But many essentials were missing. For example, when I needed to search for a piece of text in the project, I usually opened up a third terminal to use find or mc . This is very uncomfortable, and a really clumsy way of looking up method and class definitions. Another problem is the lack of API discovery mechanisms, such as code-aware autocomplete or in-place documentation. The only help besides the compiler error messages are the ScalaDoc available online and code snippets from Stack Overflow. In this post I briefly present some tools I’ve come across to mitigate these problems: Silver Searcher and Ctags . The :grep command is a convenient way to search for text occurrences in multiple files from Vim. With the Silver Searcher , the grep can be faster, while producing much saner results by respecting the .gitignore files. CtrlP can be configured to use Silver Searcher too, so it can enjoy its benefits. As it’s described in this post , it is really easy to configure, all you have to do is install the Silver Searcher: and configure your .vimrc to use it: By default in Vim, one can search for the word under the cursor with the asterisk key (*) in the current file. Going further on this line, I mapped the <leader>* to search for the selected word in the whole project. This is a convenient and fast way to look up usages without requiring any language-awareness from the editor. Setting it up is a one-time activity, then you can forget about it and enjoy its benefits. Ctags is a tool that generates a tag file by indexing the source files looking for important language constructs. The indexing always depends on the language of the language of the source file, for example it can tag method, class, and member declarations. The tag file allows Vim (and other compatible editors and utilities) to easily navigate to the tagged constructs while browsing the source code. Exuberant Ctags is a multilanguage implementation of Ctags that was originally distributed with Vim. It supports many languages out of the box, but unfortunately Scala is not one of them. Luckily, it can be extended to support more with custom regular expressions defined in its dotfile. 3 steps are needed to use Ctags with Vim: Then, you can trigger Ctags at any time to generate or refresh the tag file. To install Exuberant Ctags, issue the following command: Ctags can be customized with the .ctags file, located in the home folder. To enable indexing of Scala sources just add the Scala language definitons to it, as it is described in this post : Also, Ctags indexes everything by default, but it provides an exclude parameter to opt-out of tagging source files in certain paths. The exclude patterns can be defined in the .ctags file as well. Currently, I use the following: Or, if you’d simply like to ignore everything listed in the .gitignore file, add –exclude=@.gitignore , but keep in mind that the indexing will fail if the file doesn’t exist. The final step to set up Ctags is to add the following to the .vimrc file to let Vim know where the tags are. Tag files can be generated by invoking Ctags . A nice tip from this post suggest that you should generate the tag file to the .git folder, since it is normally ignored by the VCS and most search engines, but Vim can still pick it up. Therefore, generate the index file with the following command in the root directory of the project you’d like to index: The tag file generation is really fast, it shouldn’t take too long even for larger projects. The tag file is reloaded by Vim every time a file is opened. (Or when the current buffer is reloaded, for example with :e! .) Because the tag file generation is a frequently needed task, it can be handy to create a mapping for it: The -f parameter ensures that the tag file can not be generated anywhere by accidentally pressing Ctrl+F12 , but only in the root of a project. There are many ways to use the tags in Vim. The most obvious is to find the definition of the method under the cursor by pressing Ctrl+] . This of course works with class definitions and other language constructs too. To jump back where you left off press Ctrl+t . With :tag <tag-name> you can jump directly to a definition, or browse them with :CtrlPTag . It’s useful to list for tags in the current file when looking for method and field declarations, and for this I recommend the Tagbar plugin. An important characteristic of Ctags is that it “just” indexes the source files based on a set of regular expressions, so it’s not fully aware of all the rules of the language. Ocassionally it shows a few false positive matches that you have to check, but generally it’s not a big deal. For example, if you have two classes, each containing a getName method and try to navigate to the definition of one of the methods, then Vim - at best - will list both definitions that you have to choose from. Because it’s really common to browse between similar tags, I’ve remapped my navigation to be a bit more helpful: Although it requires some initial setup, navigating by tags provides really great developer experience. Having control over the tag file generation can be both a powerful feature and a footgun, so you might consider automating it for example with vim-autotag . The setup described above is not tied to Scala, it can be used with other languages as well. Although the Ctags setup above is really powerful it has a shortcoming: it can’t navigate to methods and classes defined in dependencies or in the standard library. The ctags-sbt plugin aims to solve exactly this problem. It downloads the source of every dependency (both Scala and Java) and indexes them with Ctags. With a little configuration the previous setup can be augmented with sbt-ctags , as it is described in its Github page . Just create the ~/.sbt/0.13/plugins/plugins.sbt file with the following contents: Then the tag file can be generated with sbt gen-ctags . Beware though, depending the size of your dependencies this tool can create enormous tag files. I’ve tested it on Time-admin , a small time-tracking web application consisting of about 50 classes. While the tags of the Time-admin sources are below 2 MB, the tags of its dependencies are more than 100 MB. The file generation can be slow (especially the first time as the sources have to be downloaded), but the main problem with this size file is that it kills CtrlPTag on older machines. In this case, instead of fuzzy tag finding use :tag <tag-name> . With the tab key the tag name can be auto-completed, so while it’s a bit less powerful, it provides some browsing capabilities. To ease these problems, I’ve configured sbt-ctags to use a separate file for the index, so it doesn’t write the previously generated tag file over. To achieve this, create a file to ~/.sbt/0.13/sbt-ctags.sbt with the following contents: And modify your .vimrc to use this additional tagfile: This is convenient as the tags for the project sources can be generated independently from the dependencies that change only once in a while. Silver Searcher and Ctags are powerful tools that can be integrated nicely with Vim, providing simple and fast means to navigate effectively in larger projects. I’m using them for a while, and the difference they made is really considerable. Despite their simplicity and the fact, that none of them has complete understanding of the semantics of the programming languages, they work surprisingly well. They can be used with many programming languages, so I recommend giving them a try even if you are not programming in Scala. With a minimal setup they can be great enhancements to anyone’s Scala toolbox who likes to program in Vim.", "date": "2017-01-10"},
{"website": "Advanced-Web", "title": "Why Bower is still relevant", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/why-bower-is-still-relevant/", "abstract": "Many developers consider Bower to be a thing of the past that is superseded by npm. If you are using a bundler, like Webpack or Rollup, it’s certainly true. But if your webapp is missing this compilation step, using Bower is a big improvement over the traditional methods of managing dependencies. In this article, we’ll look into how to get started with Bower, then the potential issues you might encounter. This overview should give enough context so that you can hit the ground running if there is a need to introduce it to a project. This is the written version of a screencast I’ve done for SitePoint. If you have a subscription, be sure to check out that too at this link . We write articles like this regularly. Join our mailing list and let's keep in touch. To install it, only npm and git are needed. After that, simply issue to install it globally (might need sudo in some cases). After installation, use it with bower from the command line. To install something, use bower install <packagename> ; as an illustration, bower install lodash installs Lodash. Bower will download the latest version into the bower_components directory. Likewise for uninstalling , use bower uninstall lodash to remove it, and as an alternative, you can simply delete the lodash folder. To see what packages are available , the easiest way is to use a browser and search the repository at bower.io , under Search packages . Just start typing to see the potential candidates. Since you usually don’t want to install everything by hand, Bower supports listing the dependencies along with their versions in a configuration file . In this bower.json , you can put all the libraries and install them in a batch later with a simple bower install . To bootstrap the config file, use bower init , which is a simple next-next-finish-style wizard. After you have it, specify --save when you install something, and Bower will update the configuration file with the new dependency. For example, bower install lodash --save will download lodash, same as before, but will also add it to the config. Using bower.json renders the bower_components folder redundant and disposable. In case you don’t have all the dependencies installed, just run bower install and everything will be downloaded. Using this, you don’t need to check in the libraries to the version control , which is a bad practice anyway. Similarly, in order to update a library, just modify the bower.json, run bower install , and push just a single line of change to the VCS. Basically, Bower requires a bower.json file at the root of the library that specifies which files are production artifacts that should be downloaded. This is the reason why many people say package maintainers should not support Bower, as it puts additional burden on them. Firstly, maintaining a separate configuration file, and secondly, to put the production artifacts into the repository. Bower support is waning, as new libraries are unlikely to support it. But fortunately, Bower supports different types to download. If you specify a GitHub repository that is missing bower.json , Bower will download the whole repository. In this case, if the production artifacts are in the repo, they will be downloaded too, without explicit support to Bower. For example, if you issue bower install https://github.com/sashee/gentoo.git , bower_components/gentoo will contain all the files. Bower can download any file , just like you would manually. If you specify a URL that points to a single file, Bower will download it. For example, bower install https://unpkg.com/lodash@4.17.2/lodash.min.js will create bower_components/lodash.min/index.js with lodash. This allows you to use any CDN. If you want to reference the production artifact from the npm repo, use unpkg , if you need files from GitHub, use RawGit . See the previous article where we’ve talked about these in detail. Bower can also download and unpack zip files . With bower install https://jqueryui.com/resources/download/jquery-ui-1.12.1.zip , the bower_components/jquery-ui-1.12.1 will contain the contents of the package. It’s likely that if you have a node.js backend you are also using npm to manage its dependencies. Using bower side-by-side is tedious, and it would be nice if a simple npm install would fetch the frontend dependencies too. Also, it’s usually a burden for fellow developers to manually install a new tool; ideally, npm could manage it on a per-project basis. For this to work, list bower as a dev dependency . This is as simple as adding a new entry to the devDependencies section: This will make bower itself managed by npm, but a separate bower install step is still needed. Fortunately, you can use a postinstall script in the package.json, that npm will run after every install . This runs bower install after every npm install . This works fine for installing new packages, but you might notice that it won’t remove unused ones . If you try out a new library, then change your mind, it’s not enough to simply remove it from bower.json. If you do just that, you’ll still have the lib at your bower_components , but others might not. Bower provides a prune command just for that. If a dependency is installed, but missing from the json, it will uninstall it. Since it’s idempotent, it’s safe to run every time. This takes care of cleaning up with every install. One downside of Bower is that it downloads a plethora of unneeded files, and gives you no control over this. If a library is missing the bower.json file - which is likely - the whole repository will be downloaded. Bower-installer is a project with the aim to remedy this. It can override which files will be downloaded and you can specify a different path and directory structure. For several months, it seemed like the project is abandoned. But just recently Richard Quadling stepped up to maintain the library, therefore its future seems bright for now. To get started with bower-installer, simply install it with npm install -g bower-installer and replace every call to bower install with bower-installer . Finally, define an install path in the bower.json, like this: If you forget this last step, running bower-installer will raise a cryptic TypeError: Cannot read property ‘ignore’ of undefined . Just add the install path and the error will be gone. If you run bower-installer, you can see that only the main files are moved to the dependencies folder. For example, if you depend on lodash, bower_components/lodash will contain the whole repository, but dependencies will contain only lodash/lodash.js . For a dependency that supports bower, like underscore, there are still extraneous files by default. Bower-installer cuts down everything but the main file. The best thing about bower-installer is that it’s configurable , in case the default does not fit. For example, if you install bootstrap, you’ll end up with a bootstrap.js and a bootstrap.less , which is definitely not enough. But with a few lines of config, you can fix it: Removing unused dependencies can be done with bower-installer -rp . This removes the dependencies directory before running, and the bower_components after. This is a clean state, as no extra files are retained, and provides a clean output. The drawback is it’s slow. Removing the bower_components mean that bower needs to initialize everything, every time. A better way is to combine prune with bower-installer -p : This takes care of the leftover libraries in the bower_components as well as having a clean state at the dependencies . Just remove the bower_components from the VCS, and you are good to go. We’ve already covered SRI in detail in the previous article , so you should read that first if you haven’t already. At first sight, it seems like it’s only useful to tame corrupted CDNs, and it has nothing to do with Bower. But since Bower does not check the downloaded files in any way, you still can not be sure that those are the expected files. Using SRI adds an extra layer of security , and as it’s just a minor inconvenience, you should use it. There are a few ways to generate the hashes. If you use a library that is available to the public, simply generate the hash for the upstream version and use that. For example, if you use Lodash from CDNJS, you can install it with: Then go to srihash.org , input the URL ( https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.2/lodash.js ), and copy-paste the hash. Remember that this works only for the files that are publicly accessible. Another approach is to use the snippet , and generate it yourself: Just don’t forget to add the sha384- prefix. This works for all your dependencies, public or private, as the hash is generated locally. Or, if you want to be super lazy, temporarily host the files and point srihash.org to that URL. One way to do this is to install localtunnel and http-server to host the files. This gives back an URL. Open it in a browser, find the files you’d like the hash for, open srihash.org , and generate for each file. Finally, copy-paste the hash and you are done. You can use this method for any dependency. Just keep in mind that you are exposing all the library files for a short period of time, therefore it should not be done for anything secret. Bower does what you would do by hand, only better. It provides a single place to define dependencies , and does everything else automatically. This way, you don’t need to check libraries into VCS , as all the files can be regenerated from the bower.json. Also, you don’t need any other tools to manage dependencies. For a simple project without any modern transpilers or bundlers, it’s the way to go. Using Bower separates the front-end dependencies from the others. If you have a backend and use npm , the dependencies will be listed at the package.json and at the bower.json , nicely separated. The biggest drawback is that it’s hardly enough to issue a bower install --save and hope that all and only the needed files will be there. It’s more likely that you end up with entire repos downloaded or missing files. Luckily, with bower-installer , you can fine-tune this, but it’s still a tedious process. If you don’t use a bundler and want to avoid relying on CDNs, definitely use Bower. Avoid managing dependencies by hand and also checking them to the VCS. Bower does a good job on automation, and while you might spend some time tinkering with the configurations, you will reach a point where a simple npm install will set everything up for the project.", "date": "2016-12-21"},
{"website": "Advanced-Web", "title": "One year with Vim", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/one-year-with-vim/", "abstract": "I’m using Vim for a little more than one year. I switched in last August, and it’s my main editor since then; I’m writing these lines in it. To celebrate this anniversary, I’m writing a summary of my motivations, experiences, and the learning curve so steep it’s legendary. I won’t reach hard conclusions whether or not you should switch. It’s a story, and I’m sure if using Vim ever crossed your mind, you’ll find it interesting. Around last year February, I had a bit of spare time to kill, and I was thinking about learning something new. I was developing with AngularJS that time, Ruby before that, while I initially started as a hardcore Java dev. These technologies are quite different and learning yet another shiny framework did not intrigue me at that time. We write articles like this regularly. Join our mailing list and let's keep in touch. Then came the idea, that typing is a common part in almost everything I do. I read a lot of articles and posts about experiences of others, and it seemed like a good challenge. Vim was not my primary target that time, just wanted to type better. I mostly used two fingers and the mouse extensively; it wasn’t actually slow, and while programming speed is not limited by typing speed, but I wanted to type “properly”. So my first milestone was touch typing. I googled around a bit, and found many online sources. I realized that it’s less important how I learn it; it’s a matter of hours I put into practice. So I chose one online trainer, and started learning the different keys. I practiced every day; I believe it’s very important in order to learn new things. It was hard at first. I still feel how exhausting it was to stare at the screen, thinking about where the letter “g” is located on the keyboard. I still mistype “k” and “l” occasionally. It took me like two weeks to build up a basic muscle memory for the essential keys. After the initial period, it became fun. At last, I wasn’t thinking about each and every character and it’s a good feeling to be able to type a whole word with strokes in rapid succession. The progress was still slow, but I observed improvements every week. It needed months of practice to reach an adequate level. I still can’t type fast, but at least I don’t need to constantly look at the keyboard. Also, I’m able to just “feel” when I mistype something. Did it worth it? Absolutely! It took a long time, but it’s like promoted to a pro after being an amateur. I know it does not improve my coding abilities much, but I still feel it made me better. I learned the standard qwerty layout back then. I read about Dvorak and Coleman, but as I heard they don’t offer that much benefit. Maybe at 100 WPM they are sensibly better, but I’m not aiming for that. Also, I’ve found out that good posture and a little warm up goes a long way; my fingers do not hurt, and my accuracy is definitely better. Dávid was using Vim for a while then, and always told me about his good experiences. It’s heavily geared towards touch typing; even escape is remapped to CTRL - [, to accommodate the fingers at the home row. Since even the basics are quite complicated, I started by reading the Practical Vim book, trying out the stuff I learned along the way. It took like two months; yep, its learning curve is that steep. People all over the internet are saying that you should disable the arrow keys when you learn Vim. This is because it’s so much easier to navigate using them when you are starting out; but it’s an antipattern, and you should build up muscle memory the hard way. Since I was touch typing, it wasn’t an issue for me. I got accustomed to hjkl almost from day one. What helped me was a script that prevented using them without a count. It made moving by one character/line much tedious, and eventually I started to use the word/search movements more. But still until today, I find myself pressing 1l1l1l every now and then. The most obvious advantage is that I use the mouse a lot less . Before I started using Vim, I used the mouse and the keyboard together; pointing where I want to edit something, then grabbed the keyboard and press a few buttons, then back to the mouse. I didn’t realize back then how slow and tedious it was. Using only the keyboard without the need to switch back and forth is something that transformed my typing. On the other hand, I rarely type for long uninterrupted intervals. I usually write some code, then go back to the browser and see if it works. There are some plugins to ease that, but I usually need to use the mouse then. Vim supports touch typing to the fullest. I usually do not need to move my fingers to reach far away keys, and I can just keep looking at the monitor while I type. This is an important feature I’m yet to see in other editors. A definitive advantage is that typing in Vim is like coding . You can record complex macros, and it has many features to make repetitious actions easy. For example, one day I needed to insert a lot of data from an HTML table. In the old days, I would write some Javascript to extract the data I’d need; but now I simply inserted the table, recorded a macro to format one row, then I could extract the whole table. No more repetitions. Plugins are quite essential, as the basic configuration is hardly productive. Fortunately, many people share their .vimrc , so that you can start from a nicely configured editor. It’s a great way to get up to speed, and once you get comfortable, you can personalize it to your own needs. Another advantage is the abundance of plugins . You can find snippets and repos for just about every problem you’ll have. Also, it’s highly configurable . In theory, you can add motions, shortcuts, snippets, and just about anything to ease your work. Once you get to know what your specific needs are, chances are you can define a shortcut for that. One great advantage is the community . If you have a problem, it’s likely that someone else had that earlier. Just Google it, insert into your .vimrc, and you have a very new feature, that solves your particular problem. If you take a look at the available plugins and scripts, you can easily find something useful. This is how I stumbled into vim-surround , rainbow parenthesis , ctrlp , and easymotion . It’s like Christmas, every time I need it. What I particularly like in Vim during text editing is the multiple clipboards and the markers . During refactoring, inserting the same text to many places is often needed. It took me some time to get used to the multiple clipboards, but it really speeds things up. The same is true for the markers. Oftentimes I find myself jumping between a few places, and markers come quite handy. One interesting advantage is that coding in Vim is low-level . I can edit text, then I need to know what command to use in order to build the app, how to test it and so on. When I was using an IDE, there were several automated things that just worked, but I had no idea how. For example, I know where to click to package my android app, but I have no idea how to do it manually. If you are using Vim, you get an intimate knowledge about the code; you’ll know every tiny bit, and you always know what’s going on. One advantage people say all the time is that Vim works just about everywhere . It might be true, but I hardly ever use it outside my main computer. And finally, its requirements are like nothing . It uses virtually no memory or CPU. I’ve used IdeaJ before that, and I got used to running out of RAM every now and then. Since the switch, Chrome is the #1 memory killer instead of the IDE. Since I tend to doubt an article if it’s all about the positives, I’m writing about the downsides too; there are quite a few. Learning Vim is a heavy investment . People all over the internet are writing about the steep learning curve, and I can attest. It took me months to reach the level of productivity I had with a modern IDE, and even then I’m waiting for my speed to skyrocket. Since text editing is much more complex than just typing in text, I occasionally experience brain fog when I’m tired. It hinders my ability to navigate the files and the current buffer more than using a mouse. To use Vim effectively, you need plugins . Without them, the editor is a lot less welcoming. This renders starting out a bit harder. On the bright side, you can start with someone else’s .vimrc, hoping that they already made it more comfortable. The biggest drawback of Vim is Vimscript . I’m working as a software dev for a decade now, and decided to give it a try. I made my way through an excellent book on the subject, and I must say it’s horrible. I’m pretty sure that if someone invests many hours mastering it, then they can bend Vim to their will, but the language itself feels it’s a remnant from the past. My problem with this is not that I want to develop new plugins; but it will discourage people who would. Compared with an IDE, Vim has less language support . It’s a generic text editor, and not geared towards a specific programming language. It has many features like text completion and ways to quickly navigate through the codebase, but I miss deeper integration. I used to ctrl-click into a Maven dependency which is something you’ll still need an IDE for. Did all the hassle of getting started worth it? Yes, it transformed how I look on text editing. It’s a refreshing change, and I’m glad I went all the way through. Will I stick with it? Probably not. It might come as a surprise, but I feel that writing and, especially, understanding code is much easier with an IDE. But in order to change, I’m looking for a proper replacement. I tried Atom, but couldn’t make it fully compatible with touch typing. It has a Vim plugin, but it does not support a few features I’m using, and also had to reach for the esc key quite often. The next time I feel like experimenting, I’ll give Sublime Text a go, then IdeaJ. After a quick Google search, seems like they have Vim-like plugins, and might be suitable to replace it. Until then, I’ll be happily using Vim.", "date": "2016-09-14"},
{"website": "Advanced-Web", "title": "Parallel Processing in JS", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/parallel-processing-in-js/", "abstract": "For a long time, Javascript was missing any kind of processing threads. While the single-threaded model added to developer comfort, it also made the platform unable to do serious and time consuming calculations, and the only way to circumvent it was to do it on a remote server. Luckily, with the introduction and widespread adoption of Web Workers, we can now do resource-intensive calculations on background threads. On the down side, the specification had to fit into the current ecosystem, and it feels quite awkward at times. If you came from languages where threading is supported from the beginning, you might find the amount of restrictions surprising. It’s far from just instantiating a new Thread and everything you write into that will be processed in parallel. This post is an introduction to Web Workers, how and when you can use them, and their peculiarities. I’ll also cover how to use them in WebPack, and the possible pitfalls. Code ( html , js ) and demo are available. Web Workers are probably the only way to achieve true multi-processing in Javascript. To create a worker, you need to instantiate it with the code it will run. This will create a Worker instance with the desired code. You can then communicate with the worker using postMessage, same as you would with IFrames. Since there are no cross-origin problems in play, there is no need to verify origins. And in the worker’s code, you can listen to these events. This works both ways, so you can postMessage some data back from the worker’s code into your main program. These are the basics to get started with workers. You have multiple methods to handle errors in the worker’s code. You can catch and pass them via the postMessage channel. That would require some extra coding, but this is the most versatile and safest way. The other way is to use the onerror handler. This catches all exceptions that are not handled inside the worker itself and lets the caller code decide how to proceed. To set up error handling, all you need is to attach a handler. To ease debugging, there are some extra fields in the exception object. There are filename , lineno , and colno properties that indicate where things went wrong. Cleaning up workers after they are not needed is crucial. They spawn real OS-level threads, and they can easily kill the browser process if you spawn too many of them simultaneously. You have two ways to kill a worker process: inside the worker, or from outside. I think it’s best to handle the lifecycle from the main page, but there might be certain situations you might think otherwise. To kill a worker, simply call its terminate() method. This will abruptly kill it, freeing all resources it is using. If it was processing something, that would halt too. If you want the worker to manage its lifecycle, just call the stop() method from the worker code. Either way, the worker is stopped, and should not leave anything behind. If you are using one-shot workers, that are doing some computations and then get discarded, make sure you are terminating them in the onerror handler too. Failing to do so will introduce hard-to-find leaks in your code. Code and demo are available. Moving the worker code out to a separate file is making some simple cases more difficult than they should be. Luckily, the workers can be instantiated using a Blob, and you can make them however you’d like. To make an inline worker, just create a Blob with the desired code, make an Object URL from it, and you can give it the worker constructor. Since you are creating a global ObjectURL, don’t forget to get rid of it when it’s not needed. Generally, you’d revoke it when you terminate the worker instance. In theory, you can spawn subworkers inside a worker, and it should work the same as you create one from the main thread. There is even an example in the spec on how to do it. But unfortunately there is a long-standing Chrome bug , that prevents this use case. It might get fixed at some point, but this ticket was opened back in 2010, and there has been little progress since. You should not rely on this feature. Update : It should work now just fine. Code and demo are available. There are a few edge cases that you should be aware of when passing data to and from the worker. Passing simple values like numbers, strings, and arrays work as you’d expect. You can pass simple structures and they get serialized/deserialized properly. In effect, you should not resort to serializing objects into JSON just to keep the structure; in fact, postMessage uses a structured clone algorithm , which can process a few more types, like RegExps and Blobs, and circular references. That said, you should still limit what you pass to the simplest types as possible. There is no way you could pass functions, and even the supported types have some limitations; those would easily manifest themselves as hard-to-debug bugs. If you could define your API to handle only strings, numbers, arrays, and objects, you are less likely to face these kinds of problems. If you have a complex object, there can be circular references in it. If you try to serialize it into JSON, you’ll get a TypeError: Converting circular structure to JSON . But you can safely pass the same object to the postMessage, and you can use it inside the worker. To prevent concurrent modification, everything you pass to postMessage is copied to the other side. This makes sure that you can not modify the same object from two places in parallel. But if you want to pass large amounts of data around, you’ll quickly experience how slow these copy operations are. For example, if you are doing image-related calculations, you’re likely to pass whole images; making the copies might easily be the bottleneck. Fortunately, there are transferable object, and you can, well, transfer them instead of copy them. One such transferable object is an ArrayBuffer , which can contain just about any raw data. If you transfer an object, the thread that originally owned it loses access. It makes sure that while the data is not copied, no concurrent modifications can happen. The postMessage syntax is quite awkward regarding transferables. You need to pass the data as previous as the first argument, but you need to pass an array of transferables as the second one. Make sure you pass the transferable in the second argument; if you forget, the data will be copied over. Code and demo are available. To use Web Workers with Webpack, you can use the worker-loader . Just add it to the devDependencies section at your package.json, run npm install , and it’s all set up. To use a worker, simply require it. This will instantiate the worker, and you can use it the same way as without Webpack. To instantiate an inline worker, all you need to do is add the inline query parameter to the loader. Code and demo are available. You can even import and use any modules inside the workers. Getting the workers up and running in Webpack is quite easy. But there are a few obstacles you need to be aware of when you use this approach. First, there seems to be no way to move out the common parts of the code. If you have a worker that depends on a piece of code, then it will be included no matter whether other parts of your codebase also use it. And if you have multiple workers using the same library, then it will be included in all of them. You might think that if you dump worker-loader and specify a new entry point then use the CommonsChunkPlugin , it will take care of this. But unfortunately workers are not like a browser window, and some features are not available that the resulting code would require. Also, using inline workers do no better in this regard. The shared code is still present in multiple places inside the bundle. And second, inline workers leak ObjectURLs . They are created, but never freed. This might not seem a big problem, but if you use a lot of one-shot workers, it may affect performance. Based on the above observations, my advice is to use normal workers, and look out what you import into them. And also make sure you send appropriate cache headers, so that the browser does not need to download the code more than once. Web Workers are very similar to IFrames, and they might give the impression that using them would also result in parallel processing. But since IFrames have access to non-threadsafe APIs, like the DOM, the browser cannot spawn new threads for them. Click here for a demonstration. Cross-origin IFrames are quite different. They do not have access to most of the APIs, and they can communicate only via postMessage, same as Web Workers. This, in theory, allows browsers to run these IFrames on different threads and that would result in parallel processing. But in practice, they are still single-threaded, and the browser doesn’t give them any special handling. To see it in action, click here . Web Workers solve the long-standing issue in Javascript on parallel processing. Albeit its syntax is a bit awkward and it poses quite a few limitations, the support is solid and does its job well. On the other hand, while there are basic tooling, they are not mature enough to be reliable. Seems like this technology is far from being mainstream, as there is only niche use-case. That said, if you need parallel processing, you don’t need to wait for something else. Learn its rough edges, use the tools you need, and you can drastically improve the user experience.", "date": "2016-08-09"},
{"website": "Advanced-Web", "title": "JVM JIT optimization techniques - part 2", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/jvm-jit-optimization-techniques-part-2/", "abstract": "In a previous post I wrote a short intro about the high level overview of the JVM optimizations, followed by the results of some measurements, where I crafted small programs to trigger certain optimizations. It gained some attention on some forums, and as I watched the discussions unfold on Reddit or Hacker News, I was truly amazed how many people are enthusiastic about this interesting topic. I also realized that most of you are more precise than I am. At the time of writing I did not think that anybody will notice or care about the randomly copy-pasted assembly code on the first illustration, but many of you did. :) I hope to have some time in the future to play with JITWatch and the XX:+PrintAssembly flag to be able to correctly address my mistake. We write articles like this regularly. Join our mailing list and let's keep in touch. For now, I’d like to continue with the measurements part of the previous post with more optimizations, and share my recent experiences with You. Code motion is a technique to relocate parts of the code from hot paths, attempting to make them execute fewer times. Expression hoisting is a form of such optimization, where an expression is pulled up in the execution graph, for example by moving the body outside of a loop without changing the semantics of the code. In the following program, the expression related to the result variable is loop invariant , as it’s guaranteed to have the same outcome for every loop iteration. The JVM can recognize this and move the calculation out of the loop, making it look like this: Measuring the two code samples above, in terms of speed they perform almost identically due to this optimization. Let’s break the loop invariance, by assigning the loop variable to y instead of a constant! This is a lot slower than the first version, as all computation has to be executed in every iteration. Similarly to hoisting, there is an optimization strategy that moves the affected code down the execution graph, called Expression sinking . This is useful when a precomputed value is not used in all of the subsequent branches. For example, consider the following code: In the else branch the previously calculated result variable is not used. The JVM can recognize this pattern as well, and moves the calculation to the branch where it is needed, making the code above performance-wise similar to the code below: Measuring the version where the result is used in both branches shows the speedup potential of this optimization. A neat trick the JVM uses is to check if there are any assignments thats result will be overwritten before the stored value is accessed. In the code above the first assignment to the variable result has no effect on the program semantics, because nobody can see the results of that store operation. In such cases the JVM removes the useless stores, and the unnecessary computations related to them. So technically it’s equivalent with a simple constant assignment, both semantically and performance-wise. Measuring the two code snippets turns out that the JVM can indeed optimize this code, resulting in similar performance characteristics. Removing the constant assignment from the first snippet completely changes the outcome of the program, and of course makes the optimization impossible, thus resulting in slower code. Loop unswitching can be considered a special case of code motion, where the compiler moves a conditional out of the loop, by duplicating it’s body. For example in this case, because the myCondition is loop invariant, it’s unnecessary to calculate which branch to take in every iteration. The compiler can transform the code to this, without changing it’s meaning: The two code snippets above perform similarly, despite the former includes more steps if you decide to execute it step-by-step. If there are any special cases need to be handled while looping through an array, it’s usually about it’s first element. Loop peeling is a strategy to optimize such cases, by moving the first or the last few iterations outside the loop body, and changing the iteration range of the loop. Consider this: This optimization (and some subsequent dead code elimination) can transform it to something more performant: The benefit is, of course, that the special case does not need to be considered in every iteration. The two code snippets perform almost identically thanks to this optimization, but if we change the condition to be something else, the performance gain evaporates. It’s really fun to dig into the details of the underlying mechanics, but it’s a bit tiresome as well. Although many presentation has long lists about what optimizations are implemented in modern JVMs, for certain ones there are surprisingly low amount of documentation available. Besides, sometimes it’s really hard to squeeze the desired behaviour out of the JVM due to it’s complexity, so if you are planning to discover them yourself, be sure to be prepared for challenges. Taking all these advanced and sometimes magical optimizations into account, I think it’s important to note that:", "date": "2016-06-28"},
{"website": "Advanced-Web", "title": "Simulating movement with ES6 generators", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/simulating-movement-with-es6-generators/", "abstract": "One particularly good example to infinite collections and ES6 generators is movement simulation. This requires a possibly complex calculation of the points along the path, and you need to take care of a few additional cases like collision detection and eventual termination. In a traditional way, you might use a while-loop that takes care of all the necessary parts. This results in an intermingled function that does many things. Using ES6 generators, you can nicely separate the different concerns, making the code easier to understand and maintain. This post is intended only to give an example on how to use infinite collections for a more complex use case. It is not meant to give a thorough guide on every aspect of movement simulation. There are a few typical building blocks when you want to calculate movement. You might want to Live demo and full source are available. For an example to this post, I’ll use a simple gravitational simulation. The user can hover over the canvas to set the initial velocity of a rock, then the path is calculated based on the gravitational pull of three planets. If the rock crashes into a planet or the infinite loop protection kicks in (after 3000 points) then the simulation terminates. Also because there is no need to have subpixel-level precision, only the points that moves at least a pixel are emitted. Without using an infinite collection, you might use a while-loop. In this function, all concerns are mixed, and it is quite hard to reuse this function. If you are using an infinite collection, then the different parts can be separated. The path calculation does not need to know about neither stop conditions nor display resolution. It just emits the path infinitely. Using the generator function from above, we can then fit the other parts in: The first part uses the limit operation. The order is important; if you put it to a later stage, it can easily result in an infinite loop, which we aim to prevent. 3000 is an arbitrary number, the exact value is not that important. The stop condition is implemented using takeWhile . It is somewhat similar to the filter , but terminates once the predicate is false. It ensures that the simulation finishes when the trajectile is crashed into a planet. Resolution conversion is done with a simple filter . It stores the last emitted point internally, and compare the stream to see if is displaced by at least a pixel. If not, then the point is filtered out. This example uses the gentoo library, but you can use any others you like that provides these functions. Infinite collections are useful only in a handful of situations and I’ve found that path calculation is a great example. They allow better separation, readability, and cleaner code overall over the traditional while-loop way, all without sacrificing performance.", "date": "2016-06-14"},
{"website": "Advanced-Web", "title": "Infinite collections with ES6 generators", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/infinite-collections-with-es6-generators/", "abstract": "Infinite and lazy collections are commonplace in many languages, and they are beginning to find their ways to mainstream Javascript too. With the new iterators and generators spec getting widespread adoption, you can now use them in your projects. They have some very specific use cases which may not come up in everyday coding, but are quite useful in certain situations. The specs are quite new, but libraries are starting to pop up to provide the most useful operations. In this post, you can learn the basics of the specs as well as a particular use case where you’ll likely to use the new techniques. Also you’ll learn about one available library which provides most of the basic operations to work with these kinds of collections effectively. Arrays are inherently finite, as they store all the elements in memory. There is no way to construct them dynamically and they also don’t support lazy evaluation. So a construct like this will result in an infinite loop and thus infeasible: ES6 Proxies might change this, as they add support to dynamic getters. You might think that a construct like this would result in an array containing all the natural numbers: It indeed creates an array-like object that returns all the natural numbers, but unfortunately in practice it’s hardly usable. It is missing essential Array functions like splice; it makes them unsupported by libraries like Underscore.js. In theory, you can write utility functions like filter and map, but it’s definitely not mainstream. Then iterators came to the rescue. They allow infinite collections and even have a built-in language construct to iterate them: the for-of loop. To construct an iterable, you need to return an iterator for the Symbol.iterator key. The iterator only needs a next() method that returns an object with a done and a value keys. The former indicates whether there are more elements, and the latter contains the actual element. You can create an iterator like this: And you can iterate over it using the for-of loop (just don’t forget to terminate it, because it’s an infinite collection!): Generators are just syntactic sugar over iterators. Instead of writing all the boilerplate, you can concentrate at the logic. The same iterable can be created using a generator: And you need to call it when you are iterating over it: Why would you need infinite collections? They came handy when you don’t know how many elements you’ll need in advance. For example, calculating the sum of the first 100 positive numbers is pretty straightforward (this example uses Underscore.js ): But calculating the first 100 primes are a bit harder: The widely used libraries, like Underscore.js, do not support iterators. They are based on arrays and array-likes. Fortunately there are already a few projects filling the gap. It’s still early days, but they are slowly becoming mainstream. The one I’ve found quite usable is called Gentoo and it has the basic utility functions you’d need when you are working with collections, like filter, map, and reduce. The original repo seems abandoned, but feel free to use my fork, as it has some additional features like takeWhile and chaining. Just drop in the library and the babel polyfill for the generators and you’re good to go. Despite being a relatively new and still little known technology, browser support is quite good. Chrome, Firefox, and Edge all have proper support, only Safari is lagging behind. But with compilers like Babel, you can transpile your code to ES5; just include the polyfill, as that’s required in runtime. When you are working with infinite collections, always make sure you use an operator that limits the output. It is quite easy to make an infinite loop and break your app. Using the gentoo library, the previous example can be written in a more effective and robust way: This solution does not have any magic numbers that are error-prone, making it a more robust way. It is also effective, as there are no wasted operations. Generators are already supported by the major browsers, and you can compile your code with Babel to use them in older ones. You can use them today without any hassle, and while their use cases are quite limited, they will certainly make your code more readable if you use them effectively.", "date": "2016-05-31"},
{"website": "Advanced-Web", "title": "More readable Javascript without variables", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/more-readable-javascript-without-variables/", "abstract": "The single biggest obstacle of understanding a piece of code is a lot of variables. Variables introduce state, which in turn increase complexity exponentially. Every single bit of variable information makes reasoning and understanding the code harder. A single boolean can have two states. Two booleans have four. If you have ten inside your function, they can have 1024 different states. This is far beyond what people can understand. Properly scoping your code and using constants instead of vars greatly increase the readability. And that’s what matters in the long run in almost all situations. Imperative programming is still the mainstream way of coding today. It is how computers work naturally - they execute operations. Imperative code is optimized for the computers , and not for the people. Coding in an imperative way is also quite easy in most cases. There are only a few constraints, and the architecture mostly dictates how you structure the code. It is easier to write than to read . But taking the full lifecycle into account, code is like a book - write once, read many. Surprisingly, lines of code does not matter much; as long as the code is readable and easily modifiable, it’s fine if it’s longer than necessary. Javascipt is inherently an imperative language. It also has a dynamic typing system. In statically typed languages, the type system offers some guidance about the variables. But we have a lot less safety here. This is why people are finding ways to remedy this. First, there was JSDoc annotations , then Flow , and now TypeScript . Their purpose is to give developers the comfort they got used to in other languages. Linting also helps to some extent, as it can spot some well-known bogus structures. Functional languages does not have the notion of variables. All they have are values, which are effectively constants. All collections are also immutable. This sounds counterintuitive, but it allows structure reuse, making operations more effective without sacrificing the nice properties of immutability. The common counter argument from people coming from imperative languages is that functional-style coding is less effective and wastes a lot of computer resources. If a collection can be modified in-place with a simple for loop, copying and recreating the whole structure in every pass increases the running complexity. But in practice, 99% of the time, you just can’t notice the difference. If you fire up a profiler and watch close enough, you might see a particular part of the code to run 3 ms instead of 1, but you won’t notice the lag after a button click. There are certain situations, like for complex mathematical calculations, where there is a big difference. But keep in mind that immutable structures might come handy. For example if you use the React framework, you can safely ignore a subtree if nothing is changed, and won’t encounter those nasty bugs where things don’t update when they should. Focus on those parts that you see as slow, and don’t prematurely optimize. Every optimization comes with a cost, as it is basically a shift in readability from people to computers. If your implementation is already fast, then keep the code clean and readable. The first and foremost thing you can do is to use const every place you would use var . With the help of a suitable linter (like ESLint , with the no-const-assign rule ), you can spot invalid reassigns in compile time. This makes spotting bugs easier. Let’s consider the following example. We have a bunch of penguins, and we are interested in the average age of the males. Written in an imperative style, it would look like this: In contrast, a more functional style solution for the same problem would be something like this: Apart from being a bit shorter than the first solution, it has a definitive advantage that if you delete any line (except for the returns), the linter would immediately let you know. A side effect is that the code is easily copy-pasteable. If something is missing, there will be a warning, and you would immediately know. Also it’s easy to spot what is not needed. If a const is declared but never used, you are safe to delete it. You can use the no-unused-vars rule of ESLint to detect this. In contrast with imperative code, where in most situations it’s quite hard to detect this. Let’s consider the following code, where a variable is read and set, but never actually used: In this code, the lastDigit variable is set and read, so that the linter won’t detect it as useless. But if you don’t use it ever after, you could safely delete it. The downside is that you most likely can’t write purely functional programs in Javascript. Side effects are here to stay, and they can easily ruin all the benefits of programming in a functional style. As a general rule, I like to keep the following best practices: Using consts promotes proper scoping of your values. Since you can’t reassign a variable, you’ll find yourself using IIFEs (immediately-invoked function expression) a lot. The most important step is to learn the functional methods for collections. These include filter, map, reduce, some, every, and some others. Their effective use will make your code shorter and easier to understand for people who are using them too. Using them promotes a programming pattern called collection pipeline . It’s basically a series of operations on a collection and return the result. As an example, let’s group the male penguins’ name by age: In contrast, using the collection pipeline pattern, it would be something like this: Collection pipeline properly scopes each operation. You don’t need to inspect the whole code to see what criteria you are filtering on, or how each groups are formed. They are in their easily-identifiable position. When you are reducing an array and need to keep multiple values in the memo, you can use the spread operator. For example to make an aggregated shopping list for an array of cars, you can write: As arrays are inherently mutable, you need to make sure not to mutate them. Assignments are easier to spot, but push/pop are two mutate functions you might use every now and then. For push, you can use the spread operator, like const b = […a, 3]; . For pop, there is no alternative, but many libraries provide this function. To make sure your arrays are immutable, you can use Object.freeze() . Despite the name, it also freezes arrays, and prevents any modifications. But keep in mind that it only throws an exception in strict mode, otherwise it silently ignores the change. But most likely you are already using strict mode, and if not, you should. Immutable objects are a bit harder topic, but they are very well doable. The first thing is to use the bracket notation, so that you’ll be able to create objects with variable keys: Then object concatenation can be done with Object.assign . It creates a shallow copy, but it doesn’t matter, as you are only using immutable values. As an example on how to create dynamic objects, let’s see the different styles on a shopping list for a car. The same in a more functional style: Albeit the first solution is shorter, the functional style is properly scoped. With this approach, you always know what part you should look to figure out how the wheels calculations are done. With the imperative approach, you need to look at the whole code to make sure nothing else changes that particular property. For small codebases the difference is negligible, but as the components begin to grow, it really makes a difference. Using IIFEs are a lot like refactoring to functions. For example, you could write a getNeededWheels() and getNeededTransmissions() function and call them; it’s the same, but if you only use them once it’s better to keep related things close together. The methods I used in the examples above are using standard ES6 functions. There are many libraries that provide similar (and more) functionality in a more supported way. You should check out Underscore.js , lodash , or ImmutableJs . The spread operators and fat arrow functions are also supported in older browsers if you use a suitable compiler, like Babel . There are many ways to write readable code. My coding style is heavily influenced by functional languages, but I’ve found these principles to be the cornerstones of clean code. Following them is not trivial and certainly feels odd at first, but your codebase will be more readable in the long run.", "date": "2016-05-17"},
{"website": "Advanced-Web", "title": "Fully automated dockerized Let's Encrypt reverse proxy", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/fully-automated-dockerized-lets-encrypt-reverse-proxy/", "abstract": "While most people agree that securing web sites with SSL is a good thing, many sites still lack the necessary certificates. There are free options available when it comes to trusted certificates, but setting them up is a tedious process. The domain validation still requires much human work that can not be automated. The Let’s Encrypt project aims to remedy this. It’s built on a protocol called ACME (Automated Certificate Management Environment) that automates the domain validation. On the down side, certificate registration and renewal is still a mostly manual process. One possible solution is to use a Docker container that automates all the tasks regarding the certificate and also provide a reverse proxy for the application. This approach moves the SSL part into a separate component. We write articles like this regularly. Join our mailing list and let's keep in touch. In effect, you’ll need only a few lines of configuration and your app is secured with a valid and evergreen certificate. The easiest way is to use Docker Compose and define the bridged network there. This compose file defines a network called proxy that provides a connection between the reverse proxy and the business app (this case it’s called angular and it’s a simple AngularJS app). The app must be accessible with the proxied name, and the target port is configurable. Most likely you’ll use 80 or 8080. The most important configuration is the HOST . It defines the URL for the certificate; make sure it’s valid, as the validation process will fail otherwise. You should use your own registered domain here, if you have one. For testing purposes, you can use nip.io ; it provides a domain for public IPs. Let’s Encrypt has a blacklist of domains you can’t use. For example you can’t use amazonaws.com as it would be a security risk. Keep this in mind if you are using it from AWS. If you don’t use Docker Compose, you can manually set up a Docker Network and add an alias to your application. The default option is to use the staging servers, which generates a not trusted certificate. Let’s Encrypt limits the number of certificates that can be issued, so that it’s best practice to test with dummy ones. If you request too many live certs in a short period of time, you can easily find yourself limited and have to wait a week or two to get back on track. Only uncomment the MODE variable when you are deploying to production and you’ve already tested your setup. The container utilises Supervisor to start and manage the different services. First and foremost, there is an Apache web server that provides the reverse proxy. Then the certificate registration and the renewal is separated, as the former is only needed during initialization, and the latter is persistent. The initialization process is as follow: This solution provides a fully automated, configure-and-forget way to have HTTPS for your website. It is suitable to provide SSL termination in any environment that uses Docker networks without much configuration.", "date": "2016-05-10"},
{"website": "Advanced-Web", "title": "Lessons learned from using xmonad for a year", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/lessons-learned-from-using-xmonad-for-a-year/", "abstract": "Last year I switched to xmonad in the strive for a configurable yet minimalist environment. So far I am pretty satisfied with it. I’ve never experienced any crashes or slowdowns related to it, works easily for most of the tasks, and supports multi monitor setup. It is highly configurable and well documented, so it’s easy and fun to customize the whole environment to suit your unique workflows. In fact, it’s so minimal by default, that my first task was to figure out how I would use my system and configure it accordingly. I think it’s great to stop sometimes and rethink our tools and processes, explore different means to solve day-to-day problems and identify what could be improved. I like tinkering and seeking new stuff in my free time anyway, so starting with a minimalistic environment was very inspiring, because it forced me to rethink even some of the basic aspects of my workflows. We write articles like this regularly. Join our mailing list and let's keep in touch. To illustrate how minimalistic it is by default, I show you a screenshot: By default it provides almost all features you might need to get comfortably around without being in the way. Let’s see what you can do in this seemingly empty environment and how to get there. Previously I observed that I use virtual desktops in a way that most of them has one application opened all the time, and in a few I have 2-3 terminals and SQL clients side-by-side, so I can read contents from all of them. Because I always arranged my windows the same way, I’ve set out to find a tiling window manager that automates it for me. This is how I bumped into xmonad. From the docs and posts I’ve found it seemed that it’s design is less focused on initial intuitiveness, while putting more emphasis on effectiveness and minimalism. It requires familiarity with some basics, but after the initial period you can get around much quicker. I decided to keep my KDE setup, and gradually move to xmonad and continue using KDE software that I like. Luckily, it’s easily possible, if I remember correctly installing xmonad did not harm my existing DE in any way. (Some xmonad tutorials encourage to change the xsession.rc file. When you do that, be careful, because it might interfere with your other desktop environments.) The blank screen that I’ve shown you might be startling at first, but at this point you are only one mod+shift+enter away from opening a terminal, and with mod+p you can run any program. (The mod key is bound to alt by default, but it can be changed. I remapped it to the Windows key, because there are many programs that offer shortcuts with alt.) The default xmonad configuration is pretty user friendly, as it is described here , with a few keystrokes you can select, move or close windows that are automatically arranged by the selected layout strategy. Each workspace can have different layout strategy which can be dynamically selected and configured. With multiple monitors each screen has one workspace visible that can be chosen independently from the workspaces on the other displays. To my surprise it supports floating windows as well, so if you open a Save File dialog, it won’t mess with the layout, just presents the dialog gently floating above the other windows. (You can easily sink a floating window to the grid if you need to .) If you are convinced, getting xmonad is easy. It only takes to install the xmonad package and the suckless-tools for some essentials. The default configuration allows to do anything in this minimalist world, but some old habits die hard and I wanted to keep some parts of my old environment, for example a good screen locker, toolbar or tray with current date and volume etc. Xmonad can be easily configured and extended. There is an excellent tutorial about installing some essentials that is worth checking out, but a quick Google search will usually lead you to a friendly forum where most of the typical questions are already answered. In this post I try to cover the most interesting stuff, but all my xmonad related configurations can be found in my dotfiles repository . With these and the user friendly defaults, I am using exclusively xmonad for a while now, and it is a breeze. It’s fast and everything I need is at my fingertips, or even better: I can easily augment the environment according to my needs. The thing that I missed the most is a tray or status bar where all the ambient information is presented. There are many viable alternatives, but xmobar was the closest to my liking. It “is a minimalistic, mostly text based, status bar” which can be easily extended with plugins and output templates. There are plugins for the most common tasks, such as date and time display and to show cpu utilization, and it can run any arbitrary program for you and show its standard output on the status bar. Somewhere I found a script that visualizes the current volume level with dashes and greater than symbols. Based on this idea, I created another script that displays the signal strength for the wifi connection and included both of them on my status bar. It’s incredibly fun to customize the status bar with simple shell scripts. The status bar elements can be configured to respond to various user actions, such as mouse clicks. Because I frequently need a calendar, I enhanced the displayed date. Now when I click on it, a gsimplecal pops up. It’s a lightweight calendar tool that nicely fits into this customizable world, because it can run any program when the a date is selected. I configured mine to open the corresponding week in Google Calendar, so I can see my events. I used KOrganizer before, but it was too slow to startup and too heavy-weight for my needs as it contains not just a calendar, but everything else related to scheduling events. Finally, there was only one status bar related thing left unsolved: finding a way to hide it. I’ve already set up my layout strategies (I usually either arrange windows vertically, or select one to take the full screen), but the permanent status bar can really demolish the movie watching experience. Fortunately there is an easy way to toggle the statusbar. The next thing that bothered me was that by default, xmonad indicates window focus with a thin border. It mostly bothered me because it makes a bit harder to grab scrollbars at the edges of the screen. I modified my config to use padding rather than border to separate the windows, and to indicate which window is focused, I modified my config to apply some transparency on the unfocused windows. In this setup a nice wallpaper can be shown in the space between the windows, and faintly through the unfocused ones as well. This setting is really appealing in a simple screenshot, but as it quickly turned out not too practical. If the windows are not opaque enough then it’s really hard to tell where is the focus. To make things worse, this depends on not only the opaqueness, but the displayed contents as well. However, if the windows are too transparent, you can’t read from them, which is bad in a single monitor setup, but totally defeats having multiple monitors. I have tried many different arrangements and spent incredibly long time to find the perfect wallpaper hoping that it will make everything much better, but finally, I reverted to the bordered windows, but using smartBorders instead of simple borders. This solved my original problem because smart border does not appear until there are two or more windows present. The only problem I couldn’t figure out yet is that for some reason xmonad requires a special setting to display Java Swing applications correctly. I don’t know why it is necessary or why it isn’t an issue in other desktop environments. It’s a bit annoying, because this setting turns Chrome’s tab bar a bit bigger. Because I rarely use Swing based applications I keep this setting in a commented form in my configuration. At first xmonad may seem oversimplified and hostile, but it’s quite the opposite. On top of that, it’s extremely reliable and performant. I’ve been using xmonad at home and at work, both desktop and laptop, and I’ve never encountered strange crashes. I think it’s important to continuously watch and improve our workflows, and from the experience of the previous year, I think xmonad can be easily bent to one’s personal needs. If some frequent task bothers you or involves too much manual work, there is a great chance that it can be automated or greatly simplified. Starting with xmonad doesn’t mean you should ditch our loved tools. I am using many excellent tools from the KDE ecosystem, like Dolphin , Konsole or Krita , and xmonad plays nicely with them.", "date": "2016-08-24"},
{"website": "Advanced-Web", "title": "JVM JIT optimization techniques", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/jvm-jit-optimization-techniques/", "abstract": "There’s a lot of buzz about JVM optimizations and how it makes production code perform better thanks to the Just-In-Time (JIT) compilation and various optimization techniques. A lots of excellent research materials are available, but I wanted to see for myself how these apply in practice, so I decided to dig deeper and play around with some measurements. There can be differences between different JVM implementations and architectures that the measurements are done. Different JVM implementations and architectures might yield different results, so in this post I don’t intend to give exact measurements, just a bird’s-eye view on the possibilities of the platform. We write articles like this regularly. Join our mailing list and let's keep in touch. The Java source compiler (javac) reads Java source files and compiles them into class files . Unlike many other compilers, like gcc or ghci it does not produce performant native code for a given target architecture, but it creates bytecode . The bytecode is composed from a portable instruction set, where all operation codes are represented in a single byte. The instructions are executed on a virtual machine. Thanks to this, the bytecode is architecture agnostic , but it’s also a lot less performant in itself than the optimized native code. The other difference from the compilers mentioned above is that the bytecode compilation does not involve much optimization just a few, like inlining referred constants . (This can bite you if you dare to patch class files directly.) This means that the produced bytecode resembles the original characteristics of the source code. This similarity opens up the possibility of decompiling applications with ease, and simplifies bytecode manipulation, which is great, for example, if you decide to collect the variable assignments for failing tests . Early JVM versions did not have any further optimizations, they worked with the bytecode by interpreting the opcodes directly. To make things more performant, HotSpot was introduced in JDK 1.3 to provide Just-In-Time compilation , enabling the runtime to execute optimized code rather than slowly interpreting bytecode instructions. The JVM takes the time to carefully watch the code as it executes on the virtual machine. For example it analyses what parts of the code are executed often to decide what is worth the optimization, and it also analyses how the code is executed to decide what optimizations can be applied. The JIT Compiler produces optimized code tailored to the target architecture that replaces the interpreted version on the fly . The compilation happens on a background thread, so it doesn’t really interrupt the program execution. Some optimizations require a more detailed profile, some of them not, so some optimizations can be applied earlier than others, and some of them only worth the effort if the application will run for a long time. There are two compilers in the JVM: the client (c1) and the server (c2) compiler. The client compiler performs simpler optimizations, so it requires less time to analyse the running code. It’s best used for relatively short running client applications, providing faster startup, but less optimization. The server compiler targets applications that run for a long time, collects more data for profiling. It starts slower, but offers more advanced optimization techniques. With Java 6, one had to choose between c1 and c2 compilers. From Java 7, there is an option to use Tiered compilation , and from Java 8 this is the default behaviour of the JVM. This approach uses both the c1 and the c2 compilers. The VM uses the interpreter to execute the code (without optimizations) immediately after starting and profiles it, to decide what optimizations to apply. It keeps track of the number of invocations for each method, and if it exceeds the threshold of the c1 compiler, it makes the method eligible for optimizations by the c1 compiler, so it’s queues the method for compilation. Similarly for the c2, if the number of invocations reaches a certain threshold, eventually it will be compiled by it. The optimizations can be applied at loop back edges too. The VM counts how many times a loop completes an execution, and similarly for the number of method invocations, it queues the loop body for compilation after a number of executions. This technique is called On Stack Replacement (OSR) , and it allows the VM to switch between different implementations (interpreted or optimized) of the running code. Many optimizations are based on educated guesses shaped by previously collected runtime data. Later, if a new information come up that proves such an assumption to be wrong, the VM deoptimizes the related code to prevent incorrect behaviour. The JVM verifies the assumptions made for optimizations with uncommon traps . This triggers VM to fall back to interpreted mode, and recompile the affected code without the incorrect assumption. The compiled, optimized code is stored in a special heap, called the Code Cache . It is stored there until the related class is unloaded or while the code is not deoptimized. The code cache has a finite size, so if it runs out of space no further code can be compiled, which can cause performance problems. In my experience it rarely happens even for larger applications, but if the default proves to be too small for your app, or you deliberately shrink the Code Cache size, the JVM starts to complain about it: So, to quickly summarize, JVM defers the compilation to the point where more information is available about the target architecture and the dynamic behaviour of the code. Contrasting with the static Ahead-Of-Time compilation, the dynamic Just-In-Time compilation After the lengthy intro about optimizations in JVM lets have some fun with the different techniques that JVM provides. First, I’d like to point out that my measurements are only to highlight what’s possible in the JVM, and not to produce exact numbers. Some optimizations can be easily measured, while others are not. Some measurements might be distorted by other optimizations that I did not take into account, or by some clever JVM strategy that provides graceful degradation if some techniques are not fully available. Moreover, optimizations typically work together, and in reality, they serve larger applications, not by tweaked microbenchmarks like the ones I’ve created. Finally, there are many optimizations in the JVM, and they are deep under the hood. They are not always that well documented, and may vary from JVM to JVM. So, below are a non-exhaustive list of optimizations with measurements I adjusted in the hope to trigger a specific optimization. The measurements are done with JMH (Java Microbenchmark Harness), a library that aids building and running various benchmarks correctly. Inlining eliminates the cost of the method calls by substituting a method call with the body of called method. Moving related code together also enables further optimizations, that would be impossible for separate methods, like Subexpression Elimination or Heap Allocation Elimination . Of course, it would be impractical to inline everything. By default it works on methods where the compiled size is less than 35 bytes, or for hot methods smaller than 325 bytes. (This thresholds can be changed via the -XX:MaxInlineSize and -XX:FreqInlineSize parameters.) For example, in the following example the add method will be inlined eventually: The size limit takes assertion statements into account, even if they are disabled. (Which is the case by default.) I’ve ran a series of experiments where I measured the performance of the inliningExample method with bigger and bigger add methods. (I made the method bigger by adding simple assert expressions.) The figure above clearly shows a break in performance after a certain method size. Inlining is also bound by polymorphism. When dispatching a method call, the JVM checks how many different implementations of that method has. Monomorphic and bimorphic method calls can be inlined, while megamorphic calls can not. I ran a second experiment to verify this. I checked it with one, three and five Animal implementations. The code with the monomorphic dispatch was almost twice as fast than the other two. The scope analysis performed by the JVM to determine if an object is able to escape from the scope of the current method or thread is called the Escape Analysis . If the object is only used by a single thread, then the locks for this object can be elided, so unnecessary synchronization will not cause performance penalty. If the object stays in the context of one method, its heap allocation can be avoided as well. Inlining from the previous chapter can help increase the effectiveness of this optimization. If an object is only accessed by a few methods, then inlining those opens the possibility of eliminating its heap allocation. To see it in action consider the following example. In the code below the synchronized section has no real effect, because the lock can only be accessed by the current thread. The JVM recognises this, and removes the lock-related parts from the executed code, so it’s as performant as there was no synchronized block at all. This works for more complex cases too. A legacy codebase might utilize StringBuffers instead of StringBuilders . In a typical scenario, one usually don’t need the thread safety provided by the former one, yet it stuck in the code for historical reasons. Luckily the JVM can optimise the locking in this case for us. Measuring the method above and a slightly altered version where sb is a StringBuilder I got almost the same performance characteristics. So if the StringBuffer does not escape this method, it’s as performant as a StringBuilder. However, if I changed them to deliberately leak the sb reference, the version with the StringBuffer got a lot slower, while the one with the StringBuilder performed just as good as before. Where Lock Elision is not applicable, Lock Coarsening might kick in to improve the performance. This optimization merges adjacent synchronization blocks where the same lock object is used, thus reducing synchronization overhead. Consider the following examples: These examples only differ in the order of the synchronization blocks. In the first example, the first two synchronization block can be merged, because they lock on the same object, so performance-wise the first and the second methods are the same. The last of the three methods, however, can not be optimized this way, the three separate blocks must be preserved. Dead Code is code that has no effect on the outcome of the program. In many cases, such code can be detected and removed without any noticeable behaviour change, except performance. Consider the following code: If we forget to do anything with the computed result in the end of the deadCodeElim method, then it’s futile to initialize and increment it with every value. But if we don’t use the result for anything, then it means that we also don’t use the values array for anything, so the call to the createValues method can be skipped as well. Which makes the size variable completely useless. The JVM removes all these unnecessary code as part of this optimization making the implementation much faster. JVM is a brilliant piece of software that makes your code run faster without having to worry about optimizations that might come at the expense of maintainable codebase (e.g. inlining methods in the source code). These optimizations are affected by many factors, so it’s not worth optimizing anything before the need for that is well supported by measurements. Gut feelings about code performance can be misleading, because there is a lot more under the hood of the JVM that it seems at first. And exactly this is the great thing about it: for most of the time, many performance considerations are invisible from the higher abstraction levels, so you can concentrate writing simple, elegant and maintainable applications in Java, Scala, Kotlin or anything that runs on the JVM. If you enjoyed this post, make sure to check out the follow-up with more JVM magic.", "date": "2016-05-27"},
{"website": "Advanced-Web", "title": "Some thoughts on managing small development teams", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/some-thoughts-on-managing-small-development-teams/", "abstract": "In the last few years I worked on quite a few different projects with a variety of teams and I observed some trends and common pain points that affected most of them. This post is about some of these, some relevant stories and a few suggestions how to make more from a team of talented developers. As my projects were mostly with smaller and remote teams, I focus on these. Larger teams require very different management and have other problems, so it’s a different story. These are purely my observations and based on my experiences. Please keep it in mind while reading. One of the more prominent trends today is automated testing. Most of the technically minded management people have a romantic attitude on this subject, as like a holy grail of software quality. It is indeed a hot topic today and with reason, but it requires a great commitment too. Tests often require more lines of code than the actual system, contribute more to the build time than compiling, and also makes dirty quick fixes harder. These drawback are often overlooked during project planning. We write articles like this regularly. Join our mailing list and let's keep in touch. Several years ago working on a project long overdue, the management sent a mail to the dev team asking to write unit tests because having them will greatly ease the work of everyone. On the other hand the specs were fragile with cursory changes, and the burn down charts were detached from reality. There were absolutely no effect of the call to testing, because no one wanted to pay the cost of doing it in an effectual way. The takeaway here is that testing needs serious commitment and only consider it if the team is embracing it. On another instance I was working on a quite complex but isolated part of a system and decided to go the test-driven way. It was a great process, and I was able to progress at a good pace. When I finished the module, I moved on to another one for a few months. When I again needed to work on the tested code, I realized that some ‘quick fixes’ were made and my tests were mostly broken as there were no process in place that made sure that they got actually ran. So there should be some documentation how to run the tests that already exist. One other aspect of testing is that they have to be included in the build process. One consequence is that breaking the tests should be treated like uncompilable code. This means that the HEAD commit must always be green or fixed ASAP. This is because if someone begins working on a feature, she must start from a clean state, otherwise she must also fix the bugs to make her work pass. This results in unnecessary rebasing and merging. To summarize: Communication is vital in the success of projects, and working with remote teams only amplify this. The key concept here is managing expectations between team members. Periodic status reports and setting clear deadlines on all tasks are effective ways to manage this. Another common symptom is information scarcity, when some people does not know vital infos. Using email as the primary channel can easily result in this, as people tend to misuse reply-all and it is generally not structured. One of the most important thing is to have a clear workflow. Technically it can be anything that suits the team, but it must be clear to everyone who has to work on each task. I experienced countless times that there was a question on a ticket and it was waiting for ages before got answered, or tasks were stalled ‘in review’, just because the respondent did not thought it is her responsibility. These must be clearly indicated in the ticket, making sure nothing is stuck indefinitely. Questions on tickets is a tricky part of the workflow, as it is often a borderline between the business and the development. First, the question should be addressed to someone, be it the ticket requester or someone else. Countless tickets got stalled with a simple question that no one felt being the one that should answer it. On the other hand, not all questions are blockers. It is essential to choose the right color for the background, but it can be changed with a single line of code, but deciding whether pages are predefined or can be added later has a fundamental effect on the architecture. From the business perspective, the two tasks does not seem to be that different , so the developer should make clear if a question is blocking the work now or at a foreseeable future, or just something that should be decided at some point of time. On a final note on this subject, tickets from developers should be taken seriously, as they can signal more pressing problems early. At one point the client found a critical bug in their systems and the management had the whole team on exigent fixing, but that very same issue was discovered and announced by one of the developers weeks before, only to be neglected by everyone. The same holds for security findings, do not wait until the system gets breached. To summarize: Documentation or at least the lack of is pretty much understood in most projects. I’d cover only 3 area that took my attention for most of my projects. The first is to have a ‘how to start’ and ‘how to start developing’ docs for the codebase. Onboarding new developers are often a manual process because the lack of these docs, and others are generally afraid to migrate to a new dev environment. These docs should contain the exact steps that are needed to start the codebase and also how to effectively develop it. The other thing I often see missing is a place where random documentations and how-tos can be placed. I often find myself in a situation when I am sure that a particular piece of information will be needed in the future, but have nowhere to write it. That said, even an unstructured central document would be a solution, as the contents can be refactored if it gets too lengthy and illegible. Following the previous point, there should be an ideas docs too. This should be a place where random thoughts are recorded for later about what should be improved. This is just a collection of ideas that may be discarded later, but they might be a good starting point when planning the next or a refactored version of the system. Ideas are often discarded because there is no such place to record them. To summarize: The last topic I cover here is about dependencies. Recently I saw more and more projects that have an awful lot of 3rd party libraries included, even when the system is still in it’s infancy and have only a few features. There is a blurred line between a well justified and a superfluous dependency, and this makes this matter hard to reason about. Also every developer has favorite libraries that are auto-included (for ex. Underscore.js and Guava to name mine). On the other hand, a library that ‘might be needed in the future’ should be postponed when it’s actually needed, as these can easily result in bloatware. For a quick takeaway:", "date": "2015-10-15"},
{"website": "Advanced-Web", "title": "Walkthrough for a TDD Kata in Eclipse", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/walkthrough-for-a-tdd-kata-in-eclipse/", "abstract": "I always liked the idea of Test Driven Development. Previously I gave it a try in different settings, but I wasn’t sure where to start it with Eclipse and how to do it efficiently. So I decided to work through a TDD Kata using the latest version of Eclipse, Mars. Because it’s based on quick iterations, the application of this technique requires solid knowledge of the tools used for development. For example it’s essential to easily change between test and production code or run the tests many times a minute. If one has to constantly grab the mouse and browse the menus for such actions, the process can become unbearably slow and tiresome. Anyway, knowing the ins and the outs of the tools related to the job at hand is beneficial not just for TDD, so I decided to work through a TDD Kata to see how my current workflow could be improved. We write articles like this regularly. Join our mailing list and let's keep in touch. I did the exercise a couple of times, and in this post I’d like to present a brief walkthrough for the Kata in Eclipse and summarize the things I learned from it. Before diving into the Kata, below are the customizations and shortcuts that I think are needed for an enjoyable TDD flow. MoreUnit is an Eclipse plug-in that adds important shortcuts to allow easy navigation and creation of tests. This plug-in is an absolutely must have. Static imports are generally used in tests to improve readability and provide a compact DSL-like experience. With Favorites the IDE can give suggestions for static members even if the corresponding import is missing. For example, after typing assertEquals and triggering the autocomplete Eclipse will automatically add org.junit.Assert.assertEquals as a static import. Favorites for static imports can be defined in the preferences under the Window > Preferences > Java > Editor > Content Assist > Favorites . Here is my list of favorites: This can greatly speed things up, because certain types of static imports will come up in almost every test, and this way you don’t have to remember the related class names just phrases like assertThat and equalTo . Typing List and triggering the autocomplete usually brings up some totally irrelevant suggestions. When developing a web application probably the last thing you need is java.awt.List or something from the javax.swing package. Luckily the autocompletion list can be filtered in Window > Preferences > Java > Appearance > Type Filters . Eclipse orders the suggestions based on their usage, so if you select java.util.List , the next time that will be the first option presented even without the use the Type Filter. However, I still think it’s better to remove these packages from the suggestions if you don’t need them. Under Window > Preferences > Java > Templates there is a list of predefined code templates that can save you from a lot of typing. Related to tests, there is a test method template bound to the test keyword by default. Typing test and hitting Ctrl+Space gives an easy option for generating blank test cases. Below are the list of shortcuts I found useful during the Kata. The only reason I mention Content Assist in the list is because it has a less popular cousin, the Word Completion . While the former one looks for a lot of sources to provide suggestions, the latter one just looks for the words in the edited file and completes based on that, so it’s simpler and faster. There are a lot of ways to navigate between recently edited files, find the most convenient for you. For example: I rarely put my Editor to full screen as I needed the feedback from the JUnit reporter, and I also like to keep a Package Explorer on the left hand side of the IDE, linked with the Editor. Because the focus was almost always on the Editor, I did not use shortcuts to Focus Editor or Package Explorer, but in more real life tasks these can come in handy. Don’t forget to customize the shortcuts if you feel that the default ones are uncomfortable for you in the General > Keys section. For example, I always bind Rerun JUnit Test to Shift+Alt+x t , because with this I can easily rerun integration tests when I am editing related files. The necessary setup for the Kata in this case is to create a Maven project and add JUnit as a dependency to the pom file. My goal was to find potentially time consuming repetitive tasks, and try to finish the exercise without using a mouse. The next section contains a detailed walkthrough of the first subtask of the Kata. It describes how elementary operations can be carried out with Eclipse and what shortcuts I found useful for them. The subsequent sections contains the less detailed description of how I finished the rest of the Kata. The first goal is to create the Calculator that can receive a String with a comma delimited list of numbers and return the sum of those numbers. For now, the input string can contain 0, 1 or 2 numbers. For example, if the input is “1,2”, the result is 3, or for an empty string the result should be 0. Step 1. To create a new test class press Ctrl+n and select the Class wizard. Type CalculatorTest for the name then press Alt-k to define a package, and press Alt-d to change the folder to /src/test/java . Note: if there is an open file, the wizard populates the folder and the package based on its properties, which greatly helps creating new classes in random places. Try to use this method to create classes rather than browsing for the package in the Package or Project Explorer. Step 2. Create the production class simply by pressing Ctrl+j . The package and the name of the class is populated automatically. Note: In the first step there is an option to create the production class first. Pressing Ctrl+j on it will create the corresponding test class, so it works both ways. Use this to create pairs of test/production classes. Step 3. Navigate back to the test class by pressing Ctrl+j again. Step 4 . Type test then hit Ctrl+Space , and select JUnit test method to insert a blank test case. Step 5 . Fill the method body by writing the trivial test case for the calculator that checks for the simplest case. Step 6 . Press Ctrl+r to run the test. It fails, as it does not compile. Go to the error then press Ctrl+1 and in the Quick Fix menu, select “Create method…”. Note: Eclipse can compile and run the test even if there is a compile error in one or more test methods. In that case it marks those tests as failed. Note 2: To easily check the compile error, just navigate to the underlined text, and see the error message on the status bar. Step 7 . Leave the generated implementation as it is sufficient to pass the test. Run the related tests with Ctrl+r , see it pass. Note: Make sure to tick “Always save resources” before running the tests. Save with Ctrl+r rather than Ctrl+s. Step 8. Now for the refactoring, just remove the IDE generated comment. At this point it is simply not possible to polish the one-liner solution further. Step 9. Repeat steps 3-8 to add support for inputs that contain exactly one number. Create a new test case for it. Now, there is at least 3 convenient ways to do it: Write the test case as before, then run the test suite with Ctrl+r to see it fail. Change the production code to pass the test, then run the tests again. Aim for a minimal effort implementation. I added simple if-else statement to support the new functionality, and skipped the refactoring phase because that probably can’t be made any better without knowing the further requirements. This could be done more generally, but by choosing the simplest way and introducing a new execution branch I can extend the functionality in the easiest way without attempting to produce the ultimate solution in one go. Running the tests proves that the functionalities are intact. In fear of overengineering I almost convinced myself to skip refactoring the solution when I saw a code smell. The comma used to parse the string is a constant, but it’s copy pasted in the code. After refactoring, it’s important to rerun the tests to see everything is OK. The second task is to improve the calculator to handle unknown amount of numbers in the input string. I added a new test that exercises the calculator with three numbers. Don’t aim for the full coverage of the specification, just provide some relevant tests as working examples for the features. The more complex the tests are, the harder they are to maintain. After seeing my test fail, I changed the implementation of the second branch to add the values of all numbers. The test suite confirmed that the calculator is still what it needs to be. Inspecting the three branches it is obvious that the second one holds the implementation of the general solution, while the other two contains guards for special cases. The third branch is doing what the second branch would do if there is only one number in the list, so this two can be merged. Running the tests verifies that all expectations are matched. After this success, driven by a wild thought I quickly removed the first branch without thinking. Running the test suite immediately reminds me that splitting an empty string results in an array of strings that contain only one empty string, and that can not be parsed as a number. I quickly rolled back to the last working calculator. For a last cleanup I changed the for-loop to a Stream-based solution as I think it’s more readable, so the final version looks like the following. The next task is to upgrade the calculator to handle new lines and custom strings as delimiters. The first job is to support new lines and commas as separator characters. In the current state of the calculator this improvement can be done easily. I just added a new test case for the new behaviour and updated the regular expression to match new line characters too. Next job is to support input strings that optionally specify an arbitrary separator. Apart from supporting all previous use-cases, the user of the calculator can specify the custom separator with the following syntax: ”//[delimiter]\\n[numbers]”, for example “//;\\n1;2” . First, I created the test that specifies the new behaviour. Then I realized that in order to be able to support this functionality, my current solution needs some refactoring. The problem with the current version is that it does not separate the input processing from the calculation logic. The test I wrote acts as an acceptance test for the whole calculator, but I needed to develop one tiny piece in the middle of it. Previously this was not a problem, as this part was really simple (it used a constant regexp), but with the new requirement it’s going to get a little more complicated. So I decided to create two helper methods in the Calculator class in TDD fashion: In the scope of the refactor all I needed was to support all existing functionalities in the new structure. So the getDelimiter I ended up with just returns the DELIMITER constant, and the getNumberList returns the whole input. The final step of the refactoring was to wire the helper methods up in the add method. After the tests confirmed that I didn’t break the calculator, I could start implementing the new functionality, and for that I just needed to slightly modify the new helper methods (and of course write some test cases before that). In the last part of the Kata, the calculator gets some validation to guard against negative numbers. I followed the strategy from the previous section: I’ve found TDD on Mars. The overall experience with the tooling was pleasant, on the final try I was able to work it through without constantly looking for the mouse. TDD is really fun, but it needs a lot of practice and discipline to do it right. I learned that for me the hardest part was to decide when and what to refactor, because it’s part of every iteration and at first I felt wrong when I did almost nothing to it. To avoid overengineering, it’s important to set up rules on refactoring. I did not refactor unless In both cases it’s important to consider possible further implications of the refactoring (or the absence of it) and the required effort to do it. Aim for easy wins and keep it simple. The other important thing that I’ve learned about TDD is that the actual development and the refactoring happens separately. It means that there is only one thing to keep in mind. There is only one goal to achieve at once. See Martin Fowler’s great presentation in the topic. Note: Credit for the headline image background photography: ESA/DLR/FU Berlin (G.Neukum)", "date": "2015-10-27"},
{"website": "Advanced-Web", "title": "Automated tests with Eclipse using MoreUnit", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/automated-tests-with-eclipse-using-moreunit/", "abstract": "Eclipse has some shortcomings in it’s support for managing automated tests. For example, creating test class in the right place, or jumping between the production and the test code isn’t that easy as it should be. Features like these are essential if you regularly write test cases, but it’s a must have if you would like to do TDD, as unnecessary obstacles can easily disrupt the flow of development. Luckily, there are many plug-ins created by the community to extend the functionality of vanilla Eclipse. MoreUnit does a nice job improving testing experience. The first thing that I bumped into with Eclipse is that creating a test case for a source class can be painful in some cases. For example in a Maven project there is an option to create a new JUnit Test Case, but by default it creates the test right beside the source file in the src/main directory. That has to be changed manually every time. We write articles like this regularly. Join our mailing list and let's keep in touch. MoreUnit is aware of the Maven project structure and by default with Ctrl+j it creates the test case in the right directory. If the test file exists, it just jumps to that file with this command. Moreover, if a method is selected, it jumps to the related test method directly, or gives a list of choices if there are more possibilities. (It works when initiated from the test methods too.) After selecting a source method, one can easily create a test case for it with Ctrl+u . If there is none, it jumps to the test file and creates a test case, with a name generated by the method’s name. Finishing the test case, press Ctrl+u again on the test method to create a new test case quickly. As a bonus, the plug-in keeps the names used in the production and the test code synchronized, so you don’t have to manually rename the test classes or methods manually after refactoring. Another really neat feature of this plug-in that it makes possible to run tests related to a source file quickly. Pressing Ctrl+r on the source or the test file both results in running the appropriate tests. It’s also capable of running a subset of the test methods, just the ones that are related to the selected method. I am not a big fan of it, it’s usually much easier to rerun the whole test class without thinking, but it can come handy if there are many slow and unrelated test methods for a class. It’s a vanilla Eclipse feature, but I’d like to mention here that I usually bind Rerun JUnit Test to Shift+Alt+x r . This is really useful when working with integration tests that have a larger scope, as with this setup I can run the last test regardless what file I am editing to check the results. With this setup it’s easy to create and run the appropriate tests without continuously browsing the project hierarchy. I think Eclipse should have these features by default. Hopefully one day it will happen, but until then there are plug-ins to do the job. MoreUnit is a mature, stable tool that does these kind of things very well.", "date": "2015-09-17"},
{"website": "Advanced-Web", "title": "Revisiting webapp performance on HTTP/2", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/revisiting-webapp-performance-on-http-2/", "abstract": "It’s well known and documented that webapp speed is an important aspect when designing a build system. Slow loading can cost visitors and in turn money; many people investigated the issues which resulted in a number of techniques to maximize performance. Now that HTTP/2 is just around the corner and browsers are already supporting it , we should revisit former best practices and assess whether to incorporate them in the build or measure the diminished results in the new transport. In this post, I will cover the common techniques, introduce the testing system, make some measurements and finally draw some results. I am focusing only on the loading times. First, let’s iterate over the most commonly used techniques that are aimed to speed up client-side loading times! We write articles like this regularly. Join our mailing list and let's keep in touch. This is pretty basic and universally supported. It just means that the content is compressed, effectively making it fewer bytes, hence more speed. There are virtually no use cases when gzipping should be disabled, so it should be used in each and every setup. Traditionally the number of round trips is the most important bottleneck on loading times; unlike bandwidth, it won’t become better with newer technologies. Concatenating the sources is copying them in a single (or at least, fewer) file so that the browser can download it in a single round. As current browsers have a connection limit, this compaction has a huge effect on loading times. However, there are a few drawbacks. The most important is that it requires HTML rewriting too, making the build more complex and is somewhat error-prone. The other scenario being to keep an eye on is when multiple output files are produced and contain an overlapping set of input files. This setup can render them bulky. As client side code is written for reading, it contains much unneeded parts, such as indentation. Minification should keep the functionality intact while reducing the footprint. As an added benefit, it makes copying the code without possessing the source harder. A drawback is that it makes debugging harder, but using Source Maps should mitigate this. Mangling is part of the minification, it is the renaming of the variables and functions. It should save quite a few bytes, and if done properly, it should not affect the functionality. The drawback is that it can easily break the app. When building a library, renaming the public API makes consuming it impossible. Exports and externs should solve this, but it’s cumbersome to write them. Libraries like AngularJs depend on argument naming , and mangling possibly breaks them. Revving is the renaming of referenced files to contain the hash of their content. This makes perfect caching possible. The drawback is that as the concatenation it also requires rewriting the HTML which might be error-prone. The complete source and information can be found in this Github repository . The following performance optimizations can be tested with it: The effect of gzipping can also be observed with the HTTP/2 and HTTP/1.x serving. The first test I carried out is inspecting the sizes of each case for both the gzipped and the raw contents. The following table summarizes the results: Measuring times are trickier than measuring size, as there are more aspects that can affect the outcome. First, I am only considering the gzipped versions for both protocols. Second, I only take the js download times into consideration, as the HTML and the HTML processing is the same for both protocols. I prime the caches before every request, simulating a new visitor every time. The tests are performed on an AWS micro instance to have a realistic RTT and each measurement is performed multiple times to filter out some random noise. The following table summarizes the results: This comparison is trickier than comparing speeds. The question that I was particularly interested in is whether revving is really worth it’s price or not. The main benefit of perfect caching is that it cannot become stale, so there is no need to validate it. This means that if a visitor downloaded it once, there is no additional round trip. The effect is nicely presented in the network timeline: The green line represents the HTML that must be downloaded/validated, but after that there is absolutely no network usage. This greatly speeds up the website. Let’s compare how the two protocols differ when the assets must be validated, and they are not concatenated (because then there is only 1 round trip)! For HTTP/1.1 I got the following timeline: And the same setup for HTTP/2: Now that we have the hard data, let’s draw some conclusions. Regarding size, gzipping is a must and it has the greatest effect on minimizing bytes transmitted. Using minification we can reduce it even further, about half. Mangling has some effect too, but on a much smaller scale: it reduces the size by about 15%. Interpreting the timings is a more interesting topic. HTTP/2 is considerably faster in all the cases than the old standard. The second noticeable thing is HTTP/2 load times are only correlated with the amount of data transferred, and does not depend on the number of files, thanks to the multiplexing feature. On the other hand, HTTP/1 depends on the quantity of the files, but on a much lower level than anticipated. Seems like the browser does a surprisingly good job of scheduling downloads. Although, when testing the cached case, the time needed to only validate all assets is almost the same as downloading them. If you are writing a library that will be used by a massive amount of people, like an analytics snippet , then you should apply every possible optimizations and save every byte that you can. On the other hand, for most of the webapps out there, this zealous approach may results in a more complex build than it should be and the savings would be marginal. My recommendation for HTTP/1.1 is concatenation + minification + revving, without mangling. This setup mitigates the lack of multiplexing and allows perfect caching, while sacrifices only about 15% of size optimizations. As mangling can easily break the functionality, it should be used sporadically and with extra care. For HTTP/2, I recommend using only the minification, without mangling, concatenation or revving. This has the same ~15% penalty in size, without effect on timing. Without revving, the client must validate the assets, and it means 1 extra round trip, which in turn adds ~80 ms to the cached download time, a small footprint. On the other hand, it basically eliminates the need for a build system for the frontend. As there is no need to rewrite the paths in the HTMLs, each file can be minified and then gzipped separately. It can be done even with a simple bash script that is run during the deployment. This means there is nothing that would break the functionality of the deployed app, and the whole script can be written in a generic and reusable way.", "date": "2015-09-01"},
{"website": "Advanced-Web", "title": "Static HTTP preview server using Docker", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/static-http-preview-server-using-docker/", "abstract": "A preview server for static files can be used to showcase frontend works and it’s an effective way to show and communicate progress. It should be easily updated, password (or at least a secret) protected and should be reachable by the client. For some time I’ve used AWS S3 for this purpose, as it has a definite advantage that it does not require a running server, but I find the updating tedious and overall inflexible. My solution is utilizing a Docker container to host an SSHD for file upload along with a Node.js Express server for authentication and content serving. If you want to quick-start your very own preview server, just go to the Github repository and follow the instructions to configure and fire up the container. The architecture is fairly easy. There is a Dockerfile that ties everything together, copying and installing the necessary files and software, then fires up a Supervisor . That will start up a simple SSHD that is listening on port 22 and is accessible with the keys present in the authorized_keys . The Node.js server’s dependencies are installed by Docker, then the Supervisor starts the server.js. We write articles like this regularly. Join our mailing list and let's keep in touch. This is the place where most of the magic happens. First, we need a session store, which is now configured as a cookie-based one. It has the advantage over server-based ones that there is no need to store application state, hence less overhead. It’s configuration is pretty straightforward. The next thing is to check whether the current user is authenticated or not. Fortunately Express is providing an easy way to add filters to the request chain that can tap into the processing. The contents of the login form is extracted to the form.html and is processed with Handlebars for readability. Basically it is a form that POSTs to /login and sends a redirecturl , a username and a password back. The redirecturl is where the server will redirect after a login, so if someone is landing to a sub resource other than index.html, she won’t be redirected to the root path. The login handler is also pretty straightforward. It just stores the username and the password in the session and sends a redirect to the redirecturl . Note: In case you are wondering why the cookie session needs to be secured when the username and the password is also stored there, you are right. In this particular case, there is no need for secure sessions, but it’s a best practice nevertheless. Last but not least, the static preview directory is needed. While the above implementation is pretty straightforward and might seem trivial to experienced web developers, most of the time providing a simple preview is still an unsolved challenge. This project provides an easy way to start one while maintaining the versatility. Using Docker has several advantages:", "date": "2015-09-29"},
{"website": "Advanced-Web", "title": "Scott: detailed failure reports and hassle free assertions for Java tests", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/scott-detailed-failure-reports-and-hassle-free-assertions-for-java-tests/", "abstract": "Today I am excited to present Scott . The goal of this project is to provide detailed failure messages for tests written in Java, without the use of complex assertion libraries to aid developers in rapid development, troubleshooting and debugging of tests. Lately I tried the Spock Framework , and found it very pleasant to write tests with it. The main thing that got me was I had to worry much less about expressing assertions than before, and still have meaningful failure messages. (Spock has a lot of other features, but this is the main reason I think it’s awesome. For example, many commenters mentioned it’s also great for mocking, but I try to avoid mocking as much as I can, it’s just not my cup of tea.) But the thing is that I need to test Java, not Groovy applications. The Java/Groovy integration is really smooth, but it’s still a different language, framework, editor-plugin to use and job to incorporate to the build flow. Not to mention that the projects I’d like to use it already has a lot of tests with different tools, so introducing another testing framework would make the stack more complex, and the tests more diverse. We write articles like this regularly. Join our mailing list and let's keep in touch. So I really wanted a small, non-intrusive tool that help achieve a similar effect without the need to use another framework or language. So it works as follows. So there is no need to use a new API or change anything in the tests, the only task to do is to include Scott to the pom.xml . The error message contains the source code of the test case with some runtime information printed on it as comments. For example, the printed source has extra comment for every line that has a variable assignment that contains the new value for the variable. (Much like Chrome’s debugger.) If a call to a method changes a local variable, the new value is printed too. Detecting such changes are done simply by checking the string representations of the objects referenced by variables that are attached to the detailed report. Using Scott, even simple assertions produce meaningful failure messages and a lot of extra information to reduce the need to debug a test. To demonstrate Scott, let’s see a simple test case that exercises java.util.List a bit. This test fails. It says Let’s add Scott to the mix and see what happens. The List created by Arrays.aslist is backed by the array passed as an argument, so sorting the List affects the array too. Note for the curious: add an element to the list before sorting it, and see what happens. Data about variables has to be collected at runtime. To achieve this Scott instruments the bytecode of the test methods on the fly during class loading with a Java Agent , and manipulates it with with ASM . The instrumentation happens really fast, many other tool use them in the industry, for example the JaCoCo Java Code Coverage Library . Scott inserts code to the test methods that has no effect but to record the interesting stuff happens at runtime (line number, variable name, new value, etc.). These events are saved in a store object, and queried by a JUnit RunListener. Before every test it clears the event store, and after a failing test it constructs the report based on the runtime information and the source code of the test if it’s available. This is usually the case when running unit tests. I think there are many features that would be great to implement in the future: Scott loves contributions, just visit the contribution guide for some notes on how to get started. If you are looking for issues that can get you started the development, check out the Issues marked with the help-wanted tag . You can find Scott in this Github repository.", "date": "2015-08-26"},
{"website": "Advanced-Web", "title": "A solution to Codility Kalium", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/a-solution-to-codility-kalium/", "abstract": "I recently stumbled on Codility and it’s latest open challenge, Kalium . At first glance, it is a very simple problem. It turned out that my initial intuition to approach this problem was plain wrong and it took me about two weeks to come up with a proper solution . In this post I give a detailed overview and my way of thinking behind it. The challenge in a nutshell is that given a database table with segments on the number line, write an SQL select that results the covered length. For example, for the following data: The solution would be 3 , because the segments are overlapping, and the covered length is 0-3 . If one of it would reach to 4, that would modify the solution to 4. We write articles like this regularly. Join our mailing list and let's keep in touch. My first intuition was to simply sum up all the lengths and then subtract all common intervals. The idea might came from how I’d approach this in an imperative language, which SQL is obviously not. The summing part is pretty easy, just a SUM of the difference of the r and l , as they are ordered this way. Getting the intersections is a bit trickier, but not much. The table can be joined with itself on a condition that the joined segments are right to the original one (this sets up an ordering between the segments, so there will be no repetition in the result). The intersection’s length is also easily calculated, then summed, finally the solution is ready. The only problem is that it is wrong. Let’s see the following example: For these segments, we have a total length of 3+2+1=6, and since the join processes all pairs of segments , for red-green, it subtracts 2, for red-gold 1, and for green-gold another one, totaling 4, which yields the final result of 2, which is clearly wrong. The problem is that we subtracted the gold segment twice. To fix this, we could introduce a 3-way join that adds back all these mistakenly subtracted parts. This would fix this scenario, but if there was a 4-level stack of intervals, it adds back the same interval multiple times, which is also bad. The thinking goes on, we can subtract the all intervals common in 4 segments, add back all 5, subtract all 6, but it will never yield to a general solution. The main idea is that the problem would be easy if all segments are either fully cover each other or disjoint. If we could transform the original data to this form, then a SELECT DISTINCT and a simple summing would yield the correct result. The good news is that it is not even hard. First, let’s introduce a notion of point of interest : Let’s call it a number if it is either the left or the right of a segment. Then the first thing is to define the right points of the new segments. This is a simple join on the pois, where the new right is less than equal to the original one (so it will include the original right too) and greater than the left: Then when we have all the rights, the lefts will be the greatest poi that is less than the right: Now that we have the new intervals, a simple SELECT DISTINCT and a SUM is all is needed. To handle the edge case where there is no segments at all, an Ifnull is needed. The complete source for the solution : (This is a refactored version, for the original blunt one, check this ) This passes all the required tests and scores 100% in the challenge.", "date": "2015-08-18"},
{"website": "Advanced-Web", "title": "Isolating scripts on a page", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/isolating-scripts-on-a-page/", "abstract": "Recently I had multiple projects where I needed some kind of widget-like code to be injected into different pages. I came up with multiple solutions, but they were flawed one way or another. The trivial solution would be to write a js file and inject it to the target page, just like most libraries work. This comes with almost no technical problems, but I soon found out that host pages tend to redefine basic objects, making even vanilla js unreliable and it makes the whole widget unreliable. The other extreme solution would be to put everything into an IFrame as it would properly sandbox the code and the visuals. This approach would fall short in two aspects: For the first point, there is a long forgotten initiative called seamless IFrame , but browsers already deserted it. And the other point is even more hopeless. So I needed to find some other way to achieve it. My solution is to incorporate an IFrame to isolate the scripts but otherwise use the original DOM to display the contents. This reduces the IFrame to a simple container of scripts, and there is no need to display it. The first problem is to insert the code into an IFrame. The easy solution would be to simply add a new HTML file to the server and reference it, but I wanted a simply copy-pasteable and self-contained piece of code. There is an attribute called srcdoc , but it is not completely supported. Luckily it can be achieved with Javascript, with a little hack. It turned out that inserting a <script> into another script is not allowed by the browsers, the end tag needs to be escaped, and unescaped during the insert. The resulting template would be like this: And it makes the IFrame insertion to this: This makes the injected.js script to be inserted to the page, has access to the parent window, and as an added bonus it works cross-domain too. For a simple example, let’s kick off a simple AngularJs app! First, the required library script needs to be inserted to the template: And the HTML: The last thing is to define the controller and bootstrap the app: Visiting with a browser we can not tell that it is not a regular Angular app, but it’s code is fully isolated from the rest of the page. Oftentimes the host page would want to customize the appearances with custom templates. This makes the logic fully isolated, while retains the ability to alter the visuals. As the IFrame can access the host, it is just a matter of parameter passing. This is most easily done at the IFrame template part: One of the advantages of this method is that it allows the visuals to be the same as the host. Sometimes it is not wanted, and the content should refrain from inherit styling. There are many css reset libraries, and there is an interesting library to provide style resetting called cleanslate . This tutorial should give a good overview and a basis for a solution that is likely to work in a wide variety of use cases. If you’d prefer a more drop-in solution, then there is a project called sandie for this purpose. Using isolation techniques should be used scarcely as it prevents script reuse. If you have a page with a lots of parts each loading the same libraries makes it sluggish and bulky. That said, there are valid use cases when it comes handy.", "date": "2015-08-04"},
{"website": "Advanced-Web", "title": "A simple proxy to circumvent the SOP", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/a-simple-proxy-to-circumvent-the-sop/", "abstract": "Often during frontend-only development there is a backend server accessible without setting the CORS headers. The common solution would be to fire up the backend on my computer too and it makes the API to be on the same host as the code I’m working. The bad solution would be to set the CORS headers in production even they are not needed, effectively opening an attack vector. The solution I came up with is a simple proxy that serves static files from the file system and proxies some paths to the remote API server. Since Same Origin Policy only affects browsers, making the call from the server-side possible, and it effectively brings everything to the same domain. For this proxy, I’ll be using Node.js, as it provides very easy dependency management with a simple file. These dependencies are Express and Request . The other important part in package.json is the main property that sets the main script. Then with a simple npm install , both dependencies are downloaded and ready to roll. We write articles like this regularly. Join our mailing list and let's keep in touch. The script is very simple. It fires up an Express server that serves the static files from the fs, and adds a route that forwards to the predefined API URL. The complete source is as follows: For the paths multiple values can be added, delimited with a pipe (|). Start it with a simple npm start and it will be listening on http://localhost:8080 . Also you don’t need to restart it when you make changes to the files, making front-end development easier. For an added benefit, it supports Heroku, so just create a new app there and push your code, and it makes an instant preview server. This proxy script proved to be very useful in some situations. It is in my ever accumulating development tools collection. I hope you find it useful and saves some coding next time.", "date": "2015-07-28"},
{"website": "Advanced-Web", "title": "Calm assertions with Spock", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/calm-assertions-with-spock/", "abstract": "Expressing assertions are usually the toughest part of writing a test-case. The rest of the test exercises code that made to be used in production, but assertions are usually made of custom logic that lives only in tests to ensure the correct behaviour. So, while I think it’s natural to write the given and the when in most tests (not counting mocks), the then part require some attention to get it right. To express assertions for different kinds of tests in Java I mostly use Hamcrest , which is an excellent library that provides an easily extensible DSL that can be used to define complex assertion rules. The basic idea is really simple: one can use or even define small building blocks called Matchers. The Matchers are essentially functions that can work with the subject of the test, implemented by immutable objects that does not manage any state. They can be combined, and the behaviour of the assertion is essentially determined by the combination of these objects. We write articles like this regularly. Join our mailing list and let's keep in touch. It’s really flexible and extensible, but for me this kind of functional composition always felt a bit unnatural to Java. While the core concepts are simple, it has a steep learning curve, and it’s not always trivial to please the Java type checker while specifying the test assertions with Matchers. For me at least, usually there is a non-trivial mental transition between thinking about what to do and how to express it. There are a lot of solutions for these problems. I thought I give the Spock Framework a try for testing Java applications, mainly seeking some relief from writing complicated assertions. Setting up Spock is pretty easy in almost any build environments as the official example project provides guidance for many tools. To make it work with Maven one has to declare org.spockframework:spock-core as test dependency to the project, and set up the appropriate plugin to compile the groovy code. In theory, the other plugins seen in the example pom.xml are not mandatory, but one better set the useFile configuration parameter to false for maven-surefire-plugin , as the default setting prevents Spock’s detailed error messages from showing. So, a minimalist configuration looks as follows: To get started, one has to extend the development environment to support Groovy. Most IDEs has plugins for the language that integrates nicely to the existing ecosystem. I tried Eclipse Mars with the Groovy feature installed, and its almost as natural as editing Java with JDT. There are a few glitches, like navigating back and forth in the last edited locations can go haywire sometimes when jumping between Java and Groovy source codes, but usually it works fine, and most of the essential functions are in place, like code completion, jump to method definition and refactoring, even if the other side of the call is implemented in Java. Although I am just started experimenting with Spock, I’d like to share my findings with you in this post. The most obvious benefit that one immediately notices is that most assertions can be expressed without any kind of DSL. With no extra testing API involved, test are more straightforward to write and easier to read. For example, let’s assume that regarding our custom implementation we always expect the next random value to be different from the previous one, and we’d like to write a test for it. (I know, testing a random generator is usually a bad idea but bear with me for the example.) The required code in Java with Hamcrest: While the corresponding test case in Groovy with Spock: As you can see in the example above, Spock encourages BDD style tests written in given-when-then fashion, providing an opportunity to naturally divide these parts. Integrating with Java code, it’s also useful that Groovy can be written in a very similar syntax. This way the tests can be used as living examples of using the API under test as if they were written in Java. In the first example, one has to look up the documentation for available matchers as it’s functional-composition based nature makes hard times for IDEs when it comes to suggestions. This is not the case in the Spock version, it’s easy to write and read. While in many cases it’s not necessary, in complicated situations the DSL provided by Hamcrest matchers can make the test easier to read. With Spock, you can still use them if you’d like. Although it’s hard to achieve for all cases, I think it’s better to strive for asserts made from calls just to regular APIs: Keeping it simple can save one from trying to use the wrong word in the DSL, (for example, Matchers.hasItem vs. Matchers.contains) or from trying to force an expression down the throat of the compiler. Another clear benefit is that Spock produces detailed error messages when a test breaks. This is also true for Hamcrest matchers, but I think they are a bit harder to decode. Suppose that the MyRandom in the first example is done by someone who value a classic joke more than a faithful random implementation. The assertion error generated by Hamcrest and Spock tests, respectively: Both framework tells the important stuff, but the output produced by Spock is much clearer, containing almost no noise. The output contains the string representation of all objects affected by the assertion, even if it contains multiple logical parts. With Java/Hamcrest, one would have to make assertions for every interesting piece, or have to create a custom matcher to achieve similar effect. Using Hamcrest matchers with Spock produces both error outputs, which is really useful. One can use complex matchers without having to give up the benefits of Spock’s detailed error reporting. I think Hamcrest is a really good library, and the issues mentioned about it can be mitigated in many ways, and there is a chance that these things bother me more than others. The goal of this post was not to discourage it’s usage. I am really in the beginning of piloting Spock, so everything is unicorns and rainbows now, so I think there will be a follow-up post about it, when things get settled. So far, it seems that using it has a lot of clear advantages, and it is really easy to get started with, I recommend trying it if you haven’t already. There are a few things to watch out for. Groovy might be designed to cope well with Java, but it’s still a different language. It requires it’s own tools and expertise, which might make it an unsuitable choice in some cases. Also keep the key differences between Groovy and Java in mind. Spock and Groovy makes writing assertions fun and simple, which is a really important factor to motivate people to write tests.", "date": "2015-07-21"},
{"website": "Advanced-Web", "title": "Historical monitoring on AWS", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/historical-monitoring-on-aws/", "abstract": "Running several production services, there are many point-in-time metrics of interest. While these can be calculated for the present instant, their dynamic properties are hard to measure this way. For some metrics there are easy-to-use tools like Google Analytics for page visits, for some other metrics are much harder to graph. For example, counting Facebook comments or G+ +1s requires several HTTP calls. Amazon Web Services CloudWatch provides a way to store arbitrary monitoring data and an API to retrieve them historically. In this post I’ll cover an effective and portable way to collect point-in-time data, and a way to retrieve and graph it using Javascript. AWS provides several APIs to access CloudWatch, supporting a wide range of technology stack. For my use case I used a simple Bash script to collect and upload the data, packaged into a Docker container. I’ve used a simple Dockerfile which installs some dependencies and the AWS CLI bundle, do some configuration, then it runs the monitoring script at every hour. These are the boilerplate stuff. My aws_config/config is like: And the aws_config/credentials is: Don’t forget to generate an AWS Access Key, and attach a policy that allows putting monitoring data: This is the part where things get a bit more interesting. The monitoring uploading command is a simple one-liner, but how you calculate the metric value can be quite tricky. For a basic reference, this is a sample and not-so-useful script that uploads the current time: For some practical examples, let’s count some social metrics from a site with an URL-list sitemap. To count the sum of the Facebook shares: The same for Twitter shares: And lastly for G+ +1s, which is a bit more tricky: As with the data collection, AWS provides many SDKs to retrieve the data, so you’ll probably find a suitable library for the language of your choice. For this example, I’ll be using the Javascript one. In the example code which you can find on GitHub I’ve used D3.js for data plotting, and hash parameters for all the config including the AWS Access and the Secret keys in order to make it a general purpose (and IFrame embeddable) tool. Retrieving monitoring data is pretty straightforward using the official library, there aer only a handful of required parameters. First, the library must be included in the page: Then a few global configuration is needed, as the region and the keys are needed to be set on a global object. And then the actual call: Also don’t forget that you’ll need to use an AWS key that has the required permission to read the metric statistics: At this point, you have all the data you need, and you can plot/use it however you’d like. In my example I’ve plotted them to a very simple line chart. During this experiment, I’ve encountered several shortcomings in the AWS APIs that severely limits the usefulness of this monitoring approach. These are: The effect of the first two is that you can’t publicly disclose your AWS keys, as you might inadverently disclose information of other metric statistics (#1), and you also open an attack surface to your account balance (#2). Of course these can be mitigated by hosting an API and enclose the keys inside it, but then you need to care about scaling it (for the data collection you already need a server, but it does not need to be scaled). Architecturally it would be far better if you could just distribute the keys and it would just work. The current data retention limits (#3), severely impact the usefulness of historical graphing. While it allows the examination of short term dynamic properties, it makes it less useful as an analytical platform. It should be possible to retrieve older data, even for a fair price. I started to examine CloudWatch hoping that it can be made to a universal and architecturally clean analytics solution. While it lives up to this promise in some ways like use of use, it eventually falls short in both aspects. Its main use is to monitor instances and service health, and while it is very good at this, there are some important features missing before it can be used for historical monitoring. That said, for visualization of short term dynamics it is still an usable, easily scriptable and versatile solution.", "date": "2015-06-23"},
{"website": "Advanced-Web", "title": "Scala development in Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/scala-development-in-vim/", "abstract": "Truth to be told I always was quite an IDE guy. Mainly developing Java I praised the power of these tools that helped to get my job done. Sometimes I scratched my head when some indexing blocked the whole thing for a few seconds (or minutes), and I did not mind if some magic gone off-tracks, and had to refresh/restart the whole thing a few times to make the correct value appear. Many times it felt sluggish, but it always did the job. Java development always seemed inevitably IDE oriented, so I did not thought much about other approaches. But many other languages, such as Scala seems different in this respect, so I started to experiment with simple, small tools, and relying extensively on the shell. Using Unix as an IDE has some rough edges, some parts are not as nicely integrated as in a real IDE, but there are lots of little components awaiting for those who are not afraid of the power of the shell. We write articles like this regularly. Join our mailing list and let's keep in touch. I think it’s more of a philosophical difference rather than technological. The tools we use shape our thinking. With a full-fledged IDE you get a nice default behaviour that that you can customize, but It’s rather the opposite with small programs that do only one thing. You have the freedom to assemble/hack things together from a wast library of reusable pieces according to your needs. Of course its not black and white, for example nothing stops you from using an IDE for coding while managing versions and building from the command line. But enough of this babbling. The main reason I started to look around, is that I felt that the IDE I use is a bit slower and heavier than it should be, and it made me feel uncomfortable. So the idea is to use a lightweight and fast text editor - yes, Vim - for most of the coding, and use other command line tools and the shell for the rest of the job. In this post I’d like to share my findings about the Vim plugins that I found useful for coding in general, and a bit about my half-baked Scala development workflow from Vim perspective that I am trying to evolve. Vim has a bunch of ways to manage plugins. Although it can be done manually, it’s important to choose a method that helps automate installs and uninstalls, as these are really common tasks, especially when experimenting. Personally I use Vundle for this, it can download plugins directly from Github, which is nice. At first for navigation I started to use The NERD Tree . As the name suggests, it represents the directories and files in a comforting tree structure, much like things are usually represented in an IDE - feels right at home. One can easily open files from it even in a new tab or pane. Another great feature I like is that it can show the currently opened file in the hierarchy, which is a great way to explore files inside a package. I set up key bindings to the common operations I tend to use. The following configuration enables to Browsing the structure is great in a tree, but it can be slow to manually find a file in the project. CtrlP is a fuzzy file finder for Vim, that solves this problem. By default, if you press ctrl+p (cool name, you don’t even need a manual to use it) it searches not only from the current relative directory, but it’s also able to detect the root of a project based on the Version Control files and perform a search from there. It’s important to set up the appropriate file and directory exclusions for the search, as by default it finds anything, including all the derived files the compiler generates. For Maven (or SBT) projects, simply add the following line to your .vimrc. I mainly use the terminal for Git related jobs, but there are several tasks that can be aided by the editor. Fugitive is a Git wrapper that augments it’s functionality with great features. I use it for merging and patching, but there is a lot more potential in it. For visual indicators, I use Gitgutter and Airline . Gitgutter marks the lines added, removed or changed since the last commit, while Airline enhances the statusbar with lots of useful information in an aesthetic way. Bonus, the latter can integrate with the former to display summarized information related to the numbers of the modified lines. To enhance highlighting, I use the Rainbow Parentheses plugin which mark parentheses in different colors according to their depths. This can be really useful, as one can determine matching brackets in a flash. And it’s also nicely highlights scopes. Neocomplete is a powerful suggestion system that enhances Vim’s built in keyword completion functionality. For example it provides smart-casing, and it can be configured to automatically display suggestions as you type, which is incredibly helpful. Of course, by default it does not provide suggestions based on the analysis of the edited source file, but I find that most of the time this simple approach is good enough, and it’s really fast. Finally, when programming it’s useful to be able to comment and uncomment sections of code. Nerdcommenter provides commenting commands that integrates nicely to Vim’s vocabulary. The plugins above can provide help with almost any programming language, let’s see the ones that are specific to the Scala language. One must decide about the programming environment what are the frequent tasks that arise during development, and adjust the tools and the workflow accordingly. This is strongly related to the question of what features are important, and what are the ones that one can live without. Nowadays, I use two terminal windows when I code, one for Vim to edit the source files, and one for the SBT console, that compiles and tests the code continuously. The only Scala related enhancement to Vim I used is syntax highlighting for the language (vim-scala). Write some code or test, switch terminal, see if everything is OK, switch back, do some more coding. It’s easy to get into the flow, as this approach is really lightweight, nothing slows down the editor and thus the coding. Also, thanks to the SBT, the incremental compilation and unit testing can happen automatically in the background. The frequency of terminal switching would be much less, if the editor could report basic problems, such as syntax errors or even compile errors. I thought the former one is probably easier to achieve, so I tried Syntastic , which is a powerful tool to detect and report basic syntactic errors. It can even check files on save automatically. However, I found two problems with it when I started using it with Scala. First, it was really slow. Upon checking, it compiled the current file with scalac, and every time that happened, it stopped Vim for a bit. The other problem is, that it turns out Scala’s syntax is really lenient, and without the type checks, there are not many potential errors that can be reported. I could have thought about that earlier, but it was really surprising to see that I was almost unable to squeeze out a meaningful error. So I dismissed the idea of pure syntax checking, and started searching for something that can report all compile errors when I stumbled upon this post , and tried the sbt-quickfix plugin described therein. This way, the two terminal solution essentially remains the same, and all the other necessary build activities can be done in the SBT console. In the meantime, when the SBT continuously compiles and tests the code, it also generates a quickfix file with actual errors. This file can be read automatically or on demand by Vim for reporting. (I use vim-hier to visually indicate the location of the quickfix messages.) Contrary to the syntax checking solution, it does not have performance impact on the editor, as the compilation continue to happen in a separate process, not invoked directly by the editor. So it’s nicely extends the previous workflow. There are a lot of stuff that does not feel 100% right in this setup. For example, the lack of integrated API discovery solutions (like intelligent code analysis based completion) are a mayor source of pain, not to mention refactoring tools, code templates, code formatting and linting. It’s a long road ahead, but I think some of these problems are already solved, and some features are simply not required (for me, at least), and it’s important to keep the benefits of the different approach in mind.", "date": "2015-06-11"},
{"website": "Advanced-Web", "title": "Accessing Google Drive in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/accessing-google-drive-in-javascript/", "abstract": "Google Drive gives a convenient and free way to store files in the users’ cloud accounts. It can be used for private files as well as publicly available ones. As an application developer, it gives a major easement as it removes the burden of scaling the backend to support larger binaries (like images). On the other hand, Google imposes some usage limits , so keep it in mind when you choose to rely on it. For this tutorial, I’m continuing the previous tutorial , so I assume that the gapi and Google auth is already set up. There is two things needed to set up: The objective of this tutorial is to have a service with a method that takes a file and returns a promise which resolves with the publicly accessible link. In practice, it would be used like this: The basic building blocks are the following: The first step is already covered in the previous article , so it is a simple gapiAuthService.login() . For the second step, we need to create a new directory in Drive. Folders are just special files with mime type set to application/vnd.google-apps.folder . This effectively makes creating a directory a one-step process: This returns a promise with the result. One thing we should do is to first check if the folder is present, and only create it if it is not. This is a simple listing with filters to the directory mime type and not trashed, then check if there is a result. This will return a promise with the upload directory, creating it if needed. This part is a bit longer, as we need to construct a multipart POST request as per the sample code . Basically we need to read the file data, construct the metadata, construct the multipart POST, make the request, and wrap it all into a deferred, so that we get the promise with the inserted file in the end. The next thing is that we need to check if the file is inserted or not. In a few occurrences I experienced some delays between the insertion and the availability, so it’s best to make sure it’s ready before we pass it back. The checking part is to get the file, and if it’s successful, then we know it’s ready, otherwise we wait a little and check again. The last part is to insert a permission for the newly uploaded file so that everyone with the link can view it. It is a simple insert permission operation, nothing special. It returns a promise that will be resolved when the permission is successfully inserted. Now we have all the basic building blocks we need to insert a file, we just need to invoke them in the right order. The very last part is to return the link to the file, and it’s available via the webContentLink field. In this tutorial we’ve covered most of the basic building blocks of using the Drive API. While keeping the limitations in mind it is a viable alternative to file storages. It’s API needs some time to getting used to, but after a bit of groundwork to hide the details behind a service, it can be made easy-to-use. Also there are many more functionality not covered in this tutorial, you can always refer to the documentation .", "date": "2015-05-26"},
{"website": "Advanced-Web", "title": "Using Google auth in Javascript", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/using-google-auth-in-javascript/", "abstract": "Using a third party login provider yields many benefits. You do not need to worry about all the sensitive user data and secure password storage, and you can integrate with the provider’s other services too. You might also don’t need any server side for managing users if you want that, as client-side libraries often support this use case too. Although you can implement the full OAuth handshake, for some use cases it is not necessary. For a login like this we generally want to implement two methods. The first is to check whether the user is currently logged in or not, and the second to actually do the login. In case of Google auth, we also need to load and initialize the client library. For demonstration, I will use AngularJS and it’s promise library, $q , as most of the scripts are asynchronous. First, we need to include the script in the page. It uses the callback pattern, so we need to pass a function that will be called when the library is loaded. So first, we need to include the script at the bottom of the page: The tricky part is that the client library is loaded asynchronously, we would want to use it before it happens. That means we need to implement a queue that stores the callbacks and makes sure that they will be called after the library is initialized. Google Analytics provides a nice way to work around this issue, and it is detailed in this post . Basically it initializes an array that other components can push their callbacks, and during the initialization it replaces it with an object thats push method is redefined, and lastly calls all existing callbacks. It can be implemented like this: Just make sure it is defined before the above script tag (so that the callback function exists). Also you should take care of loading additional client library interfaces you would like to use. It can then be used like this: The good thing is that from now on, we don’t need to worry about the asynchronicity of the client library, when the callback is run the client library will be up and running. The next part is to check if the user is already logged in or not. This is a 3 step process: A token is either missing, in case there were no previous auth attempts, expired or valid. Currently the tokens have a 1 hour expiration time, and it has an expires_at field we can check. For safety, we should treat nearly expired tokens as expired ones and ask for a new one. Let’s define our checker method! Note : For pain-free date handling I’m using the excellent moment library. The gapi.auth.authorize needs a config object ( reference ) that we need to populate. There are 3 important components we need to provide: Our config method will be like this: The last missing part is to actually put it all together and define a method that can be called: Calling this method will return a promise that will be resolved if the user is logged in and rejected otherwise. The success handler will also get the access token that we can use from there on. We can then use the function like this: We can easily construct our login method using the above techniques. First, we do a silent check to see if the user is logged in, and if not, then try to log in with the immediate parameter set to false. The full method would look like this: And we can implement a click handler like this: Note : The login method should always be called from a click handler, otherwise the browser would block the popup Integrating Google auth is somewhat tricky in some places, but after one gets the hang of it, it can be easily integrated and requires very little effort compared to a full blown authentication and user handling solution. Hopefully this article gives the necessary insight to help you get started. Also check out the sample project at GitHub.", "date": "2015-05-12"},
{"website": "Advanced-Web", "title": "Animated failure reports with Selenium and Cucumber", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/animated-failure-reports-with-selenium-and-cucumber/", "abstract": "Automated acceptance tests usually consist of multiple steps that describes complex scenarios of user interaction. Sometimes it can be hard to determine the exact cause of the failure after the execution, when one such test fails. If a picture is worth a thousand words, then motion pictures are surely able to enhance the error reports. With end-to-end tests that document features with Cucumber and simulate user interactions in the browser via the WebDriver API with Selenium, the simplest solution is to take a screenshot after a scenario is failed. Although in many cases this approach provides sufficient information to reproduce the bug that caught by the test, in complex situations the problem might surface in an earlier step, causing the scenario to fail much later. This can be the case if the setup goes wrong undetected, or the actions described in the when steps do something really surprising that the author of the test did not anticipate. One way to collect the missing pieces of information is to rerun the tests in the development environment, which is really time consuming, especially if the said test is not fully deterministic, and the incorrect behavior cannot be reproduced in a single run. Finally, it does not help the communication between the development and the tester team as much as an animation would. And it’s much less cooler. We write articles like this regularly. Join our mailing list and let's keep in touch. A possible solution for these problems can be if a screenshot is taken after the execution of every step, rather than just for the failing one. If a scenario fails, then the captured images can be concatenated to an animated GIF that can be attached to the test report. For a test suite built on Selenium and Cucumber, this can be achieved with a few extra components. At the time of writing Cucumber-JVM does not support step hooks , so to be able to do something after every step the first step is to implement a JUnit RunListener. With Maven Surefire or Failsafe plugins it can be registered as follows: Then in case of a failure a Cucumber Hook can generate the animation from the screenshots and attach it to the report. I think there are a few things that can enhance the animations: The screenshot transformation and GIF generation can be aided with the tools in the javax.imageio and java.awt packages. This code can be quite lengthy, at least my version is, as you can see here in the related repository. To increase the fun factor QA people might even consider including portrait images from the committer who broke the build as the last or first frame of the animation. The result is something like this: Although the solution is easy to implement, creating informative animations with low performance impact on the test execution is not simple, and there are some drawbacks of the described technique that may or may not be an issue, depending on what kind of suite you have. One potential problem is, that if the animation frames are created immediately after the step execution, therefore in certain cases the effect of the step may not be visible on the animation. For example, if a step induces an AJAX call that populates a dynamic list, and the second step is to select an element in the list, then the items of the list may not appear in the animation. The screenshot creation happens before the call finishes, because it does not benefit from the implicit and explicit waits that take effect before the actions in the subsequent step. Besides, at least two performance trade-offs should be considered using this technique. Regardless that the scenario fails or not, a screenshot is created after every step for every scenario in the suite. Although creating a screenshot is not really expensive, this can make the execution of large test suites a bit slower, so in some cases one might have to look for alternative solutions, like rerunning the failed tests and only then take the screenshots. Another possible problem is that the image manipulation can be measured in precious seconds, so one might decide carefully what transformations to apply on the collected screenshots. These transformations only necessary when a scenario fails, so as long as only a few scenario fails at a time, it’s not a big problem. But when a core component is changed that can affect multiple tests, the report generation can really slow things down if many test fail. This can be mitigated if the animation is only created for the first few failures, and for the rest of the failing tests only a single screenshot is attached to the report. If a single modification causes a lot of failures, possibly enough information can be acquired by investigating some of the caused problems anyway. If this threshold can be parameterized easily, then the developers doing ATDD or testing something locally after fixing a bug won’t be annoyed by the unnecessary GIF generation as they can switch it off. Monitoring performance of the test suite in the Continuous Integration environment is crucial, as it can help detecting potential performance problems that arise when tweaking the test suite, or just upgrading the dependencies of the application. So as it stands so far, I think for most real-life test suites the disadvantages of this approach outweigh it’s advantages, so I’ll look for alternatives.", "date": "2015-04-28"},
{"website": "Advanced-Web", "title": "Deploying Docker images via SSH", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/deploying-docker-images-via-ssh/", "abstract": "When we began Dockerizing this blog for good, I began to look for ways to automate the build and deployment process. It turned out that although Docker is an excellent container for running applications, there is no standard way to update a server. Luckily with a moderately complex shell script it is indeed doable, and can be fully automated. Our current architecture is made of several dependent Docker images and a Linux box as a production server. Luckily we have no dynamic data, but the deploy script can easily be modified to handle those too. In that case we would use the data only container approach. So, let’s build our deployment script! We write articles like this regularly. Join our mailing list and let's keep in touch. The basic idea of the build script is simple: Build the images, upload them to the server, then restart the containers with the new versions. These are the building blocks, just need some tricks to mix in order to be able to fully automate the process. For the sake of example, let’s say we have an apache app in the apache/ subdirectory and a monitoring app residing in monitoring/ . Let’s name our script deploy.sh and add some bootstrapping: The last variable will be the repository name for the Docker images. It is important to add one, as we will use it for the up-to-date check later. The first thing to do is to build the images. It has nothing interesting, just a standard Docker build: It builds the image to the predefined repository and adds a tag for easy retrieval. Fortunately Docker can save and load an image to/from the standard input stream, so it makes piping possible. It effectively makes the whole uploading a one-liner . What we should implement is to first check whether the image was actually modified or not. As Docker images tends to weight several hundred megabytes, it can save a lot of bandwidth, especially when there are several images and only a few of them are changing. The idea is that we can list the images for a given repository, then extract the image ID, then do the same on the remote machine, and if the IDs are the same, then the images are the same. The finished part looks like this: The last step is to restart the containers using the new images. The good thing is that we can embed remote bash commands in our deploy script and treat them just like the local ones: The first thing is to kill the current containers if they exist: The || true is needed, because if the container does not exists, then docker rm gives back an error. Since we just want to have the containers killed, in case they did not exist in the first place, doing nothing is perfectly fine. The second step is to start the containers, the standard way: For a better overview, here is the complete script: Docker is a fascinating container technology that allows building works-everywhere apps, and it comes with good CLI support. It currently lacks the features to make deploying easy, but with some scripting magic, we can work around these issues. I hope the above script gives some insight and possibly a solution to people facing the same problem.", "date": "2015-04-14"},
{"website": "Advanced-Web", "title": "Why you should care about Vim", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/why-you-should-care-about-vim/", "abstract": "Recently I have started to use Vim for general text editing tasks, such as blogging and in certain cases, programming. For the latter, as most of the time I spend looking at Java code, previously I have used Eclipse for almost every task, which is a really powerful editor, but sometimes felt sluggish for small, non-Java related tasks. So I started looking for lightweight alternatives to find a suitable general text editing tool. I am still new to Vim, but I’d like to share my initial experiences on why would someone should try this editor. Another post about Vim… Why should anybody even care about Vim? Probably everyone is using a somewhat comfortable text editor already, and text editing might not seem like a really challenging task, or something that could or should be further optimized. But if someone edit text files a lot, there is a huge chance that parts of this work can be speed up by automation. We write articles like this regularly. Join our mailing list and let's keep in touch. Imagine something that you are doing frequently if a given common task arises, and it requires huge amount of perception and accuracy with the keyboard and the mouse to achieve, repeatedly. There are multiple ways to handle this situation: The first version is the simplest but the slowest in the long term. The second one is good, but it usually requires a lot of testing to make the script produce the good results, and it’s hard to use again on a similar problem. With the third approach one have to check if commands or plug-ins can help in the situation. Most of the time a command will not solve the entire problem, but some subset of it, so you either need to combine these functions or do some manual work to achieve the desired result. Small functions can be combined and easily applied to other problems too, and excessive testing is usually not required, as you can try everything on the fly, line by line if it’s necessary. This third approach is where Vim can help the user to automatize boring, error prone and repetitive tasks. Vim is modal. By default it accepts commands in Normal mode to execute functions, and the user can enter text in Insert mode . Users typically navigate and edit in Normal mode, and enter to Insert mode periodically to type chunks of text. It has powerful functions and modifiers bound to almost every key, and these can be composed as desired making them even more powerful. If you learn a new function, you can not only use it on it’s own, but in composition with the other functions. Steve Losh wrote a post about that using these commands feels like Vim is a programmable text editor with it’s own (extendable) language that has different types of elements that allow to compose meaningful commands. The language has verbs, adjectives and nouns like d elete, a round and w ord. (Vim calls them motions and operators.) Placing a cursor over a word, and pressing daw deletes the word with the whitespace around it, making the word disappear without leaving a scar in the sentence. Another noun is the s entence. The command das does the same thing with the currently selected sentence. The verb c hange indicates to not only delete the referred noun, but immediately switch to insert mode to let the user enter a new version of it. Combining it with the adjective i nside, one is easily able to fix typos in a text with ciw . In the previous commands in the place of the w or s the user can specify delimiter characters too. For example, placing a cursor in the body of a Javascript function and issuing di{ means to delete the whole function body. It’s like speaking in abbreviations to the editor. By adding new elements to the language one can use these elements in combination with the previous ones. This is really powerful and makes learning new things much easier. If I say v is responsible to indicate v isual selection, you can easily guess that vas creates a visual selection around the sentence that the cursor is in. Of course, custom elements to the language can be added by plug-ins, for example, the Ruby plug-in allows to express Ruby related terms with the language, like methods and classes. Custom key bindings and macros can join multiple things to make issuing complicated commands easily accessible, while the dot key makes really simple to issue the last complete command again. One can certainly live without these features, but doing everything manually is usually much slower. If you stumble into a new problem, there is probably a way to help in the situation with the use of functions and custom key bindings. ##Turning a text editor into an IDE Also, Vim is not just a thing from the past: it has a vibrant and active community. It has continuously growing user base and actively developed plug-ins to extend its functionality. There are plug-in managers like Vundle or Pathogen to ease the organization of these extensions. With the right set of plug-ins, Vim can be language-aware, like someone would expect it from an IDE, providing not only the above mentioned navigation and syntax related functions, but context dependent auto completion, navigation, documentation and more. As I am writing this post in Vim, I really appreciate Markdown syntax highlight and English spell checker to catch errors as I write, features that Vim gives by default. Some other features are handy too, like word highlight to spot unnecessary repetition, or easy way to jump through sentences. Coding usually requires somewhat different set of tools. Copy-pasting or deleting whole lines is really common, as well as doing these operations around code blocks or function-argument lists. Besides the above mentioned context aware navigation and code completion, using version control is also essential. It’s up to the user to find the right features required for the job. Vim has a lot of built in functionality and a large number of plug-ins to manage text and code. This makes Vim to be a great candidate to be someone’s primary text editor. In addition, it can be used via terminal, which is great for using the same editor to manage files on remote servers too. Accessing complex functions in only a few keystrokes in this case can greatly improve productivity, as administering or coding can be annoying on low bandwidth if you have to type a lot. Vim can be challenging as it works differently than most other editors, but although it requires some dedication and effort, finding new ways to use it can always be fun.", "date": "2015-03-31"},
{"website": "Advanced-Web", "title": "MathJax processing on the server-side", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/mathjax-processing-on-the-server-side/", "abstract": "MathJax is an excellent library and makes it very easy to include mathematical expressions to a site, you may have found some in this blog too. But it always bugged me that it requires some client-side Javascript in order to work and it breaks the site in article readers (like Pocket ). Some of MathJax’s simplicity is that you just drop a single line of JS into the head section, some configuration if needed, and it just converts the expressions to nice-looking HTML. Let’s see if we can move this to the server-side! At the very core, MathJax converts MathML to HTML elements, adds some fonts, and the browser shows the expression. The first thought is to make it happen on the server and include the generated HTML in the site. When searching for it I’ve found some existing solution (like this ), although none of them seems mature and reliable. Luckily, there are some tools to move JavaScript to the server-side, and one of them is PhantomJs. It might seem an overkill to use an actual browser to render some stuff meant to be rendered by the client (actually it is), but it gives a very solid and reliable result. First, we need the MathJax files, as a Ruby dependency. There is an excellent gem source called Rails Assets which essentially brings Bower to Ruby. Thus, adding it as a source then rails-assets-MathJax as a dependency, we automatically get all the needed files. We write articles like this regularly. Join our mailing list and let's keep in touch. The next thing we need is a HTTP server that our PhantomJS browser will fetch the site from. For it I’ve used WEBrick, as it comes with standard Ruby. Basically we need to start the server, mount the MathJax assets to a directory, then make an HTML page that contains out MathML expression. We need to do this in a separate thread because we need it running in the background. After that we need to poll and wait for the server to start. And we can have server_started? like this: Lastly, we need to mount an HTML file with the expression, like this: After we have the server ready, we need to actually fetch it, wait for MathJax to render, then grab the HTML. I’ve used Capybara and after the setup, the relevant parts are: This loads the page, waits for MathJax, then extracts the results. The resulting HTML can be directly included into the page, but don’t forget to include the fonts too. You can use the MathJax CDN, but since you already have them as part of the rails-assets-MathJax gem, it’s best to use them instead. For the CDN, you might want to use something similar to this: The described solution works quite well, it really does the job. After everything was set up, I’ve noticed that the expression in a browser looks the same as before (that’s the good part), but Pocket still don’t show anything (and that’s the bad part). So in conclusion it successfully transitioned the processing to the server-side (although it compromises some compatibility of MathJax to older browsers) it did not solve the original problem. The second thought is instead of generating the HTML content, let’s capture an image of the rendered expression. This will surely work in every article reader, and it will look the same, so there will be no cross-browser issues. The first thing I tried it PhantomJs, but sadly it does not support custom fonts ( here’s a GitHub issue ) as of the current version. Version 2 will support them, but it is not available for all platforms at the moment. The stack will then use Chrome in Xvfb, so it will not actually pop up a browser window, but it will behave exactly like one. Also we need some PNG processing to crop the screenshot, Chunky PNG will do the job well. We will reuse most of the code from the first attempt, there are only some additions we need to make. First, we need to start Xvfb. Fortunately there is an excellent gem for this, reducing it to a Headless.ly block. Then we need to reconfigure Capybara to use Chrome instead of PhantomJs. Generating the actual image is quite easy, we just need some Javascript to know the actual dimensions of the expression. The following code shows how to do it: And that’s it, we have the image of the expression and we can substitute all MathML with it. This imaging solution works quite well, you can check at the earlier posts in this blog. I needed some tweaks like a minimal width, some padding and custom CSS, you can find the documentation and the end result at Github. The downside is that it requires some software that is unlikely to be present on every machine, and it’s kinda slow. When PhantomJS 2 will be generally available, I’ll look into it whether it can substitute a real browser.", "date": "2015-03-17"},
{"website": "Advanced-Web", "title": "Quick and dirty way to build PhantomJS2 from source", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/quick-and-dirty-way-to-build-phantomjs2-from-source/", "abstract": "PhantomJS 2 is just around the corner, but official Linux binaries are yet to be made available. In the meantime I needed some of the new functionality so I couldn’t wait until the errors were fixed, so I compiled one from source. Being a Java/Javascript/Ruby developer, compiling C/C++ programs from source was always a black box for me. I needed to install a lot of libraries to my system which I most likely never use again, just to produce a binary that actually does not need any of those stuff. As I recently got familiar with Docker , it seems like there is a solution to these issues. In short, I wanted a script that: The first step is to create a new container based on Ubuntu, execute some commands and remove it. Docker and Bash have powerful support for this: With --name , we can name a container, and then we can reference it without first having to look up the container id. We write articles like this regularly. Join our mailing list and let's keep in touch. The next thing is to actually setting up and the compiling inside the container. I’ve started from the official build docs , just added make , git and libqt5webkit5-dev as they were needed for the base Ubuntu image. Also, the build have a confirmation prompt, but luckily that can be supplied from the CLI with --confirm . The commands are: To make it a one-liner, simply add && between the commands: Docker have several ways to copy a file from the container to the host (there is a COPY command, and the container can also mount a volume of the host), unfortunately they are not portable. Luckily there is a method that works with relative paths, and that is stream redirection. The basic idea is that we redirect all stderr streams to stdout (so there will be no stderr data), and read the binary to the stderr, and finally on the host redirect the stderr to a file. As the binary is located as bin/phantomjs, the resulting commands are: And the redirection to file: The only step now is to combine and compress all the above into a single line, which can be copy-pasted to any system with Docker: The actual compilation will take quite some time, on my machine it was ~ 1 - 1.5 hours. The binary works fine, and it will make a good substitution until the official build arrive. The only thing the process leaves is an Ubuntu image, but that’s something people may generally want to have around. If you don’t need it, delete it with a simple sudo docker rmi ubuntu . The good thing with this approach is that it will work with any other compilation too. Just figure out the exact sequence of commands, add the above tricks, and you have a fully automated build script. Remember, that you can pull out multiple files, just tar them inside the container and untar on the host. For a reference, if you want to start a fresh new Ubuntu as a playground, use:", "date": "2015-03-03"},
{"website": "Advanced-Web", "title": "Log review guided by tests", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/log-review-guided-by-tests/", "abstract": "This is a follow up of the undesired log output testing post . The issue I would like to invite you to think about is not just to check for unexpected events, but to ensure that the important information gets to the log file. In case of an error the people responsible for the operation of a service have to take actions mostly based on the information provided in these files, which is why it’s imperative to reduce the noise in the log by recording the relevant information with the relevant log level. Checking log statements with dedicated tests seems cumbersome and unpractical to maintain. To control the contents of the log, I think it’s a much better approach is the occasional review of the output, and that can be aided by the automatic testing infrastructure in many ways. We write articles like this regularly. Join our mailing list and let's keep in touch. One is to record the log output for every feature test case to see what lines will be present in the log file after a particular scenario is executed. The generated reports can be checked periodically for problems. The report might contain some noise, but it can provide an opportunity to spot if something is really missing. As discussed in the previous post, JUnit provides mechanism to intercept certain phases of the test execution, for example to execute custom assertions or reporting. By creating and registering a similar log event listener as before, we can generate a report based on the log events occurred during the execution of the feature test suite. During the operation of a software usually all log events with INFO or higher level are relevant, so the listener in this example registers the log appender accordingly before the test starts. It produces markdown output that can be easily converted into HTML. One problem with this approach is that the report will contain every log statement occurred during the whole test case, including those that are related to the necessary setup work. (Of course unit tests with mocks are less affected, but integration and feature tests inevitably contains such parts.) With a test structured like the example above, there might be not much that we can do about this, but with differently organized tests there is an opportunity to ignore certain phases of a scenario. For example with Cucumber a typical test looks like the following (with the necessary glue code): This test is organized into multiple steps in a given-when-then (GWT) fashion, where the “when” part describes something that can happen in a typical real life situation, and the “given” and “then” parts are responsible for documenting the appropriate pre and expected post conditions. So usually, while the former one is what the scenario is really about, the log information related to the latter two can be ignored. Cucumber provides Hooks to enable users to execute custom code before and after every test scenario, but executing additional code before or after the steps is more convenient with the JUnit listeners, as logically every step in a Cucumber scenario is a separate test for the JUnit. So, with a slightly modified version of the LogEventListener we can filter the relevant log events by keeping the ones generated for “when” parts of the scenarios. To put the icing on the cake, Cucumber itself is able to present the test results in a nice looking HTML report . Powered with the adequate information collected by the LogEventListener, this report can be enhanced to contain additional information for every scenario. Creating nice, digestible reports opens the opportunity for manual inspection. For example, during a routine checkup the reviewer can load the report from the previous inspection and the current one to a suitable text comparison tool , and be able to oversee differences. As mentioned in the earlier post, there can be difficulties when the test suite runs in a different process. In this case, some additional work has to be done to create the report with all the necessary information. For example, the system under test might be able to generate the report if the identifiers of each test are provided by the testing application. A stricter approach is to programmatically compare the generated log output to a baseline. Based on the comparison various actions can be taken, like generate reports or send emails with the differences or even break the build. If the generated log output is strictly contrasted to a standard, it might cause a lot of false positive alerts, because of the slight differences in the individual log outputs, like seemingly random events from background tasks or insignificant ordering issues of the events. So for practical reasons there is some leniency that needs to be used when dealing with the automatic inspection of the logs, as well as careful planning on designing how and what aspects of the output should be compared. One way to do it might be to create a log norm in a convenient format, that describes the important parts that the tests should emit during the execution. At the end of the test suite, this norm is the baseline to compare the actual log output. It’s also important to decide how the actual check should work. It raises many questions specific to the task at hand, like how to handle Another way might be to store the generated log reports for every build and provide comprehensive information about the change of the report over time, such as log size variation, or automatically generate diff reports that can be easily checked. This can be aided with the continuous integration and inspection infrastructure, that already keeps track many metrics of the software. I think it’s worth thinking about the more automated solutions, but generally the manual review is more feasible, as the requirements about the logs are usually not exactly specified. If a scenario has strong requirements about logging, it might be another option to turn the log checks into real tests, that has documentation value as well.", "date": "2015-02-17"},
{"website": "Advanced-Web", "title": "Detecting possibly duplicate Strings", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/detecting-possibly-duplicate-strings/", "abstract": "A few days ago I was collecting content for an app of mine, and after a while I had quite a lot. I wanted to filter out lines I had found multiple times from different sources, but the data set was beyond the size to do it manually. I’ve googled for a while for some Google Docs extensions that would do that, but none of them was working. I’ve some previous experience with text similarity, so I wrote a simple script to analyze the data set for me. So the problem is: I have a text file with many lines and I want to know the most similar lines in it. Sadly this can not be fully automatized. For example, the world the and teh are very different, although they are just a mistype away, while French King Louis II and French King Louis VII seem very similar, they are actually 258 years away from each other. So an algorithm can only provide possible duplicates. The central part of the algorithm is the Levenshtein distance . It basically calculates a distance of two texts, where every character deleted, inserted or modified means 1 step. So if the distance from 2 strings are small, there is only a few changes needed to make one from the other, whereas if the distance is large, there are many changes needed. This wiki page has a comprehensive set of languages the algorithm is implemented in. As I’m working with a Node.js script, I used the Javascript one: We write articles like this regularly. Join our mailing list and let's keep in touch. Note: I’m using underscore.js when working with collections, as it makes it much clearer and easier. Let’s assume we have read the lines to a variable called lines , and it is an array and contains one element per line. First, we need to construct all the pairs from the texts. For this, we need to map the elements to return an array of the pairs, keeping in mind that we don’t want to include the same pair in a reverse order too. This is done with the following snippet: Note the map function’s arguments: the first is the actual element, the second is the actual index, and the third is the whole list. These all are needed here. Also, we can not compare the strings in order to avoid the reverse pairs to end up in the result, as there can be duplicates. So we need to use the index here. This produces the following: In order to get the pairs in an array, we need to flatten the array 1 level: This gives us: Next, calculate the distance between all pairs and order them in a decreasing way: This gives us: So far so good, it’s almost done. But let’s look at a corner case! Consider the following lines: This gives us the 3 letter words first, and only then the longer lines, although they are very different and the longer lines are very similar. To overcome this, we need to normalize the distance to be proportional to the length of the lines (the longer one in the pair actually). To do this, simply divide the distance with the longer text: This way we get the correct result: The running time is clearly O(n^2) as we need to calculate all pairs because we need to order them. For a few hundred lines it ran in a few seconds, and I think it should be alright for a few thousands at least.", "date": "2015-02-03"},
{"website": "Advanced-Web", "title": "Testing for undesired log output", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/testing-for-undesired-log-output/", "abstract": "Working on projects that went live a long time ago and have tons of features, unsuspecting developers often have the opportunity to encounter some uncertainty and discomfort around the log files. Wild things take root in the dark, hiding in the shadows of the towering piles of data. Sometimes one have to tweak some configuration files or the environment to make them disappear, sometimes the log message is actually indicate an error or an unhandled edge case. But in many cases these error messages are not fatal, the application still works as expected, at least it’s not interrupting the current task at hand, so there is no immediately detectable feature loss. After a while developers accommodate to these error messages, and they become an expected indicator that the application is alive and running. Usually no one has the time to investigate them, at least until the whole system goes haywire, and turns out these ignored messages were the silent screams that supposed to inform us about broken invariants and glitches. With additional features already built around them, the causes are a lot harder to eliminate. This problem is usually overlooked and neither do I want to make a big deal out of it, but I think there is a cheap and easy solution for this. Besides, from an operations perspective, having the right contents in the generated log files is an important feature, but this is not what I’d like to focus on this post. We write articles like this regularly. Join our mailing list and let's keep in touch. Feature tests exercise UI, API and data layers, but who cares about the logs? Probably we can’t solve every mysterious bug by cleaning the logs, but I think we can preserve some of our sanity if we take logging more rigorously in the automated integration testing environment. During the test of normal operation - and while executing feature tests - there should be no unexpected and ignored errors as they indicate something went really wrong. Even if the test case in question passed, the hidden error might cause problems later on. To enforce this, we have to break tests if certain log events happen during the execution - fatal, error, ones with attached exceptions… - depending on the attitude or policy regarding the logs. For example, if the difference between an error and a warning is that the former one means to call the system administrator (possibly in the middle of the night), then it should break the tests. Some testing scenarios and legacy applications might require opt-out switches based on event type or location, because certain test cases might produce expected errors, and there might be events that are inconvenient or not possible to handle differently. These switches also provide an opportunity to gradually improve the expectations, starting only with a subset. One approach is check the log events after every test scenario. For this, we have to create and register a custom logger implementation that stores every log event and provides methods for querying and clearing the recorded log data. With log4j, we might implement an Appender. And to use it in a JUnit test as follows: With multiple appenders and custom assertions many expectations can be expressed. The previous method can be unreliable when things can happen in parallel. If a background service generates undesired events, it’s not deterministic which tests will fail. This problem can be avoided, if it’s enough that one dedicated test at the very end of the test suite informs us to check the log files when something goes wrong. I think test level precision for this kind of checks is usually not necessary. In this case special care has to be taken to run this dedicated test after every other test. This can be done easily with Maven Surefire or Failsafe plugins: The implementation of the RunListener might be as follows: Testing separate process can be challenging, because some transport mechanism is needed to transfer log events from the system under test to the test application. This can be achieved with a simple file logger dedicated to log undesired events so the test can check if the log file exists or it’s size growth during the execution. Checking logs once at the end of the test suite seems more easily achievable. Checking undesired log events might cause some trouble when introduced to a mature application, but it can provide early detection of bugs and random behavior. What do you think, is it worth paying attention to?", "date": "2015-01-20"},
{"website": "Advanced-Web", "title": "Attached Maven tests with Cucumber", "author": ["Dávid Csákvári"], "link": "https://advancedweb.hu/attached-maven-tests-with-cucumber/", "abstract": "It’s common practice to build some kind of extensible, customizable application that can be deployed with a variety of customizations for different clients. When the base set of functionality is covered by integration and end-to-end tests, it is desirable to run these common tests on the different customized versions of the product. This was the challenge we faced with my colleagues on a recent project, where we developed a Java based web application that was intended to be deployed with different configurations and customized look and feel. The base version of the application was functional on it’s own, implemented in a Maven war project, had some sensible defaults and the core functionality. The client specific versions, also Maven war projects, provided custom design, configuration and additional functionality. The base project already had quite a lot of Cucumber feature specification that drove end-to-end tests to verify the application through the browser via the WebDriver API. We intended to find an elegant way to run this test suite on the client specific projects, and extend it to cover the additional functionality. For the sake of simplicity, here is a reduced similar problem that lacks the complexity of a web application, but hopefully able to demonstrate the need for shared integration tests. The sample project is available on Github . We write articles like this regularly. Join our mailing list and let's keep in touch. Suppose we have Calculator that can be configured to support many different operations. It has two public methods: The Calculator reads a configuration file at startup to determine which operations to support, and what implementation do the operations have. Implementations of the Operation interface must provide the perform method, for example, our basic feature set might contain an addition: To register an Operation in the Calculator, one must declare it in the config.properties: To extend this system, in the case of Maven, we simply need to declare the Calculator as a dependency, provide additional Operations and our custom configuration file. The Calculator project is documented and tested by Cucumber feature specifications and some glue code. The feature files are run with the JUnit Cucumber runner, as defined below. The feature directory contains the Cucumber feature files in all projects. It’s referenced via the classpath , not with direct file path. It is necessary to ensure that the test runner takes all feature files into account, even if they are in a dependency in some jar file. The specification for the base Addition feature is the following: To make this specification living, we have to provide the appropriate glue code. The first and the last sentence in this scenario is pretty general, not specific to the addition feature. In order to make the glue code more extensible we implement the general step definitions in a separate class. And the glue code for the addition operation: At this point we can easily test and extend our project with the built-in facilities of Maven. For example, we might create a new project that simply depend on the core module, and extend it with a Multiplication operation. To do that, we just have to provide one implementation class, and a modified configuration. To run the test suite in the extended project, it must depend on the required test dependencies. One way to do that with Maven is to provide a common parent project that declares the necessary test dependencies. There are a few things to be done to run the integration test suite on the new project. First, the core module have to export it’s test code and feature files. We can create a test-jar artifact with the maven-jar-plugin . The exported test-jar artifact contains the compiled tests for the project and the test resources of the project. The second step is to use these tests in the new project. There are two things to it. In it’s test scope the extended project has to depend on the test-jar artifact of the core project (as well as to the main artifact on it’s compile scope of course). Then we have to tell Maven to run these included tests. From the maven-surefire-plugin version 2.15 there is a way to scan dependencies for tests to run. (Note: it works the same way with the maven-failsafe-plugin.) This way the core module’s test classes and feature files are part of the test suite of the extended project. We might extend this test suite easily, we only have to provide the feature file and the step definitions for the new functionality: If we build the extended project, all the tests for the core module and the new functionality should run. I think attached tests are the most suitable to share integration tests, but in some cases it might be beneficial for sharing tests with smaller scope, such as unit tests to catch some weird bugs. For example if someone creates a class with a FQN that exist in a dependency might violate some of it’s invariants. In this case these tests demonstrates that their subject works fine in integration with it’s current environment.", "date": "2014-12-23"},
{"website": "Advanced-Web", "title": "Efficient SQLite backup on Android", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/efficient-sqlite-backup-on-android/", "abstract": "In a previous article we’ve seen that the Backup Manager on Android can be a viable option for automagically backing up data. Although it is a bit of a black box, it is easy to use and should meet most needs. The platform offers helper classes for handling Shared Preferences and files, but there is no out of the box support for data stored in the SQLite database. In this post, I’ll cover an efficient way to back up and restore data, and I’ll also provide some timing and size measurements. Let’s use a simple database schema! Basically there are Locations that can contain zero or more Stuffs and each Locations and Stuff has a name: It is a simple schema, but it provides a variable amount of entities and also a relation between them. We write articles like this regularly. Join our mailing list and let's keep in touch. The main idea behind the backup is simple: We’ll use a Cursor to fetch all the entities from the database one by one, convert them to JSON by a streaming architecture using GSON and finally GZIP the whole stream. Except the last step, the whole process is streaming, so it does not require a proportional amount of RAM to the number of entities. Only the GZIPed data must fit in the memory, but we’ll see that it won’t be a problem either (also, as the Backup Manager has a practical limit of 2MB, producing larger dump would make the saving problematic). First, let’s define a helper method. I needed to iterate over Cursors, and this introduced bloat and reduced readability. So, use this method to simplify this: Next, we need an OutputStream we write the data to, and a writer: Now, everything we write into writer will get GZIPed and will end up in bufStream. Next, we’ll define the methods that actually write the data. The resulting JSON will be like this: The export should be separated by objects/arrays, as it makes the streaming more readable and also help catch unclosed object/arrays more easily. Note: You might be using an ORM, it does not make any difference. Just read the data and serialize it somehow into JSON. And thats it, it can be written into the BackupDataOutput : The restore is very similar to the backup, just the opposite direction. First, we construct the reader: Then we read the entities one by one, and save them to the database: Note: Notice the explicit beginTransaction() and endTransaction() . This greatly increases the speed of the inserts. See this blog post about this. … And so on, same for the Stuffs. And we construct the dataBuf like this in the onRestore() method: Also, do not forget to clear all the previous data from the database! Running a backup and then a restore indeed worked fine. To check it, I’ve introduced a database hash which is a consistently ordered read of all entity name hashed with SHA-1. This produces a string that can be used to check the data without manually examining the database. I’ve ran the backup: Then, I’ve modified the database, effectively changed the hash, then ran a restore: The hash is the same, so the data is the same. Note: The hashing algorithm I use does not take the IDs, only the names. It is probable that the autogenerated IDs are different after the restore than before the backup. I’ve ran multiple tests with different amount of entities ranging from a small database to a relatively big one. I’ve also ran a couple of tests for each run: I’ve made the following runs: The findings are summarized in this table: We can conclude several things from the table above. First, restoring from the GZIPed stream did not have any overhead as generating the entities randomly (in fact, it was faster. It may happen because of increased heap size when the last step is running, or some caching, or the random is slow). Also, GZIPing the stream also had no effect on the running time. All the tests shown <1sec differences to the raw stream. But the size requirements are significantly lower, with GZIPing we’ll see unacceptably long backups/restores before reaching the 2MB limit. On the other side, serializing into JSON does add a considerable overhead. On smaller data sets it is still a little (4 sec vs 2 sec), on larger sets it is more considerable. For the largest set I’ve tested it added a 16% computation to the raw read. The method described above is a valid solution to backing up SQLite databases on Android. It can be used for a sizeable amount data, and is also compatible with the Backup Manager. It is expected that the 2MB limit will be enough for most apps. Also, the JSON serializing gives the freedom to freely choose the schema, and it can be also processed with other apps easily. Also, with dump versioning, apps can be made to restore any previously made backups, no matter the concrete database schema.", "date": "2015-01-06"},
{"website": "Advanced-Web", "title": "Practical measurement of the Android Backup Manager", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/practical-measurement-of-the-android-backup-manager/", "abstract": "The Android Backup Manager offers an easy way to add backup capabilities to an Android app. It has extensive documentation how to do a backup and a restore, and also offers some helpers for easy SharedPreferences and File backuping. It also has a great advantage as a developer does not need to know the underlying architecture, as it is integrated into the platform. Also the app does not need any special permissions to be backupable, just a quick (and automatically approved) registration is needed. The downside is that the process is a black box, there is no insight how it works and also there are only a few guarantees. The documentation says that when the app requests a backup, it will be run in an appropriate time, but there is no indication when it will be. Also there is no info about the maximum amount of data that can be stored. These makes the Backup Manager kind of unreliable, as it might work well most of the time, but it’s near impossible to figure out what’s wrong when it breaks. One of the most important unknown aspect is the frequency of the backup. Let’s make a test for it! Fortunately the Backup Agent has access to the app’s Context thus it can read/write the SharedProperties, and that makes a two-way communication with the backup process and the app possible. Also, if we call dataChanged() from the onBackup() method, it gets registered just as it would have been when called from the app. We write articles like this regularly. Join our mailing list and let's keep in touch. So we just need to trigger dataChanged() from the app, then also call it from the Agent, and write a log with the current times with every call to onBackup() . I’ve ran it for a while, and here are the results: Based on these, the backup runs roughly every hour. Also I saw that it does not matter if the device is on WiFi or mobile data, the backup runs either way. If the updating is turned off for the device, then the backups are ceased too. Also if I called the dataChanged() , the first backup was not ran immediately. The other important factor is the amount of bytes we can reliably store in a backup. If an app has a moderate database, it might be important to know whether it can use the built-in option, or it must use an external solution. The first thing I noticed is that there is absolutely no error reporting . An app can write any amount of data to the BackupDataOutput , the only error symptom is that the next restore does not gets the updated data. There is no Exception, or logging of any kind. This is a major drawback, as to check whether a backup is successful or not you need to call a restore and see into the data manually. The next surprising thing is the previous data is not erased in case of a backup. If there is a key key1 with some data, then the next backup writes key2 , then both will be available during the next restore. If you need to remove a key, you can send -1 as the size. Also, as the BackupDataOutput does not contain the keys available on the remote backup, you need to know which keys you want to delete. Third, the BackupDataInput does not preserve the ordering of the keys. When I added some data with a for-cycle, I got back these: You can see the keys are ordered lexicographically, and not in the order I put them in. The good thing is that it seems like there is no difference writing many entities to few ones as long as the overall size is the same. So practically a whole database dump can be fit to a single entity, and no need to fragment it. Regarding the size, it seems like both the current amount of data and the data already backed up takes part in the limit. I could store 3MB data, but then another 3MB failed, but 1MB succeeded. After that, another 3MB was fine. In my tests, 2MB could be stored consistently . You can find the tester app on Github with the link below. The usage is simple: To force a backup, just call: Based on the above observations, the Backup Manager can be a good solution to provide an easy-to-implement backup for your apps. If you have only a small amount of data, it would run reliably, and it results in happy users. If you need to store more data or provide historical or named backups, then it would fall short on features and reliability.", "date": "2014-12-09"},
{"website": "Advanced-Web", "title": "Integrating Facebook Comment Count Sum", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/integrating-facebook-comment-count-sum/", "abstract": "In the last article, we covered how to get the comments count for a given Facebook comment box. It is a simple GET request to a Graph API URL, and it’s limits are scaling well with visitors, so we don’t have to worry about getting capped. In this article, we cover how to display the sum of the comments for all boxes present on the site. The simplest method is to issue a GET request for each comment box (we know all the boxes’ URL in advance), then we aggregate it with Javascript. The drawback with this is that if we have n boxes, then we need to issue n requests for a pageview. We write articles like this regularly. Join our mailing list and let's keep in touch. Luckily, Facebook offers support to batch requests in packs of 50, and that seriously cuts off the needed requests. This works by issuing a multipart POST with a batch that contains a JSON array of the required operations. Then the response will be the aggregated responses for each request. This should be a correct solution, sadly it does not work in our case. The batch API requires an access token to present in the request, even though none of the part operations require it. Since we don’t login users yet, we don’t have an access token, so we can’t issue a batch request. That would be another good solution, just fetch the counts on the server, update it periodically and the clients won’t need to handle it. This might be a preferred solution to some, but it is less scalable then solving it on the client-side. In summary, we are left with the first method, issuing a GET request for every comment box and summing client-side. But we are taught to minimize the amount of HTTP requests, as they have many round trips, and have much overhead as each requires a TCP connection to be built, right? Actually, no. Recent HTTP standards (and most of the browsers are supporting it too) have persistent connections , and tunneling . These two makes sure that issuing many requests to the same host will result in minimal overhead. HTTP persistent connection allows a TCP connection to remain open after the HTTP request-response cycle. It means that if we made a request, then later we make another one, there is no need for a TCP SYN SYN,ACK ACK handshake, thus greatly reducing the RTT. HTTP tunneling allows a single TCP connection to simultaneously have multiple HTTP connections. It means that if we have a connection to a host, we can issue several requests to the same host, there will be only 1 TCP connection. This minimizes he overhead, also takes care of the 2 connection/host limit. In 2009 the fine folks at Google started working on a new protocol to enhance HTTP, called SPDY. It came with a promise to speed up the web and implemented several features to this effort. The important here is true tunneling and header compression . True tunneling means that instead of a FCFS nature of the HTTP tunneling, it gives true multiplexing, so a given request does not need to wait for all the previous to finish. If we issue numerous requests, we’ll see the benefit of this. Header compression is another neat thing. HTTP does have GZIP compression, but it is only for the content of a request. If we have many requests with little payload, the overhead becomes greater. With SPDY, the headers are also compressed, making requests even smaller. If we run a check, we find that Facebook’s Graph API does very well support SPDY . It means we have all the goodness we have discussed in the previous topics. Let’s see the benefits! One way to see the difference between HTTP and SPDY is to disable Chrome’s support for SPDY and compare the results. But it would yield suboptimal results, as it would really compare HTTPS vs SPDY. The way I compared them is with a little NodeJs server that listens on a HTTP port and pipes the requests to the Graph API. The server’s code is just an express server: Then, with chrome, I used these scripts: Then I monitored the Network tab for timings, and used Wireshark to analyze the amount of bytes transmitted on the wire. The first thing I saw is the difference in timings. With SPDY, the requests were dispatched in parallel, so all round trips lengths almost the same: On the other hand, piping through HTTP gave an ever increasing blocking time for each consecutive request: Based on these, if we dispatch numerous requests over SPDY simultaneously, it does not slows down the processing, as it scales well. Chrome Dev Tools does not help much with the actual bytes transferred, as it does not take into consideration the header compressions, nor the TCP overhead. Luckily, we have Wireshark that does counts the actual bytes transferred. There are many unknown factors that shape the exact bandwidth needed, so I’ve run a few tests and averaged. For HTTP, it varied between 107 - 111K , that gives an average of 1.82K / request . For SPDY, it varied between 69 - 74K , that gives an average of 1.19K / request . That gives a reduction of 34% in request sizes when using SPDY. This analysis gives an interesting result: from the transport layer, there is no need for a batch request to sum the comment counts. The naive method has some drawbacks but it is actually far less worse than one might think it is, thanks to modern standards and browsers. The drawback is the scaling. Using this method increases the amount of bandwidth needed to show every single page by 1.19K * the number of posts . If you have hundreds of posts, this is an issue, and you need another way to have the sums. But as long as it is only on the tens scale, this should be no problem. Also, keep an eye on the Graph API limits. If there is people who navigates a lot on your site, they can reach the limits, and then they ( but only they ) won’t see the counts nor the sums.", "date": "2014-10-14"},
{"website": "Advanced-Web", "title": "Plotting charts with SVG", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/plotting-charts-with-svg/", "abstract": "Some time ago I needed to draw a simple chart. I took the hard path not to use any third party library, although there are some with this already implemented, like D3.js . Also I didn’t want to use Canvas, as it would have brought it’s own problems. Soon I realized that SVG does not support curves that crosses all the control points, making them unsuitable for plotting charts. There is a spline, called Catmull-Rom, which does cross the control points and provide a nice-looking plot. After some googling, I’ve found this excellent article . It has a converter that parses the SVG path d attribute for a special curve, labelled R and converts it’s control points to Bezier curve and replaces the path. It is exactly what I wanted, but I more like that each stuff do one thing, so I’ve separated his code into more reusable parts. The soul of this is the converter. It gets a couple of points and return the control points for the Bezier curve. We write articles like this regularly. Join our mailing list and let's keep in touch. It gives back an array where each item is an array of 3 points. These are the control points we’ll use to construct the path. We need to construct the path using the control points we got back from the converter. First, we need to move to the start point, then we just draw the curves. This function gives back the string for the path, ready to use. First, we need an index.html: And a test.js: This draws a nice looking chart, like this:", "date": "2014-10-28"},
{"website": "Advanced-Web", "title": "Integrating Facebook Comments", "author": ["Tamás Sallai"], "link": "https://advancedweb.hu/integrating-facebook-comments/", "abstract": "This one is fairly easy. Just register an app and grab a code from the Facebook Comments site , and paste the code to a suitable part of your page. There is a data-href parameter which has a sensible default, although it is best to set it manually. It might come handy when you are migrating to a new domain or reordering your structure, it also makes sure you see the comments in localhost. This part, things are getting a little interesting. We need a bit of Javascript to get back the comments count for a given post. We can use this URL: A simple GET request gives back the comments count nested in a JSON object. No OAuth, no access keys needed, just a plain old XHR. Lets see a little example: We write articles like this regularly. Join our mailing list and let's keep in touch. Since we are using the Graph API here, we should take a look at the limits. There are several layers of limiting, and most are not detailed in the official site, the closest is 600 calls per 600 seconds, per token & per IP . In this case, there is no application, since we did not include our appid. Also we do not use client access tokens, partly because we didn’t have one (you can if you ise Facebook login). So the effective limiting here is per IP , so it scales with the number of visitors. It is easy to test too: Just make a for-cycle to reach limiting, switch IP, and try again. With the new IP, the requests will be successful. In conclusion, we should not fear running out of limiting using this method of getting the counts. This is the part where things are getting interesting, but this is covered in a follow up article .", "date": "2014-09-30"}
]