[
{"website": "Etsy", "title": "Google Safe Browsing without The Browser", "author": ["Nick Galbreath"], "link": "https://codeascraft.com/2012/03/04/google-safe-browsing/", "abstract": "Google Safe Browsing without The Browser Posted by Nick Galbreath on March 4, 2012 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “allowlist/blocklist” in our Code as Craft entries. At Etsy, we are constantly evaluating the security and safety of our members as they use the site.  One way we do this is by analyzing user generated content (UGC) for possible problems.  As part of the process we integrate results from the Google Safe Browsing (GSB) service. Typically this is client-side technology used by web browsers to protect the end-user from visiting dangerous websites that might serve malware or be part of a phishing scam. The Security and Defensive Systems group here at Etsy have flipped this model around.  Rather than warn the user when a malicious link is followed, we block the link (or the whole page) from displaying in the first place. There are a few ways to use the Google Safe Browsing service. For lower volume queries, there is a very simple REST API .  For high volume, high performance systems, the GSB V2 protocol is more appropriate as it mirrors the entire GSB database locally. It’s designed to scale to an extremely large number of clients while minimizing network traffic.  To do so, it uses a complicated protocol involving multiple blocklists and allowlists sent as a series of distributed binary diffs. While many implementations of the GSB protocols are available, for a variety of reasons they were not appropriate for use in Etsy’s operational environment (e.g. use of autoincrement ids, designed to run under a web server, etc), and so we created our own.  We have open sourced our version and made it available in our gsb4ugc git repository. It’s in PHP, but it should be straightforward to port to other languages, as it’s really more of a toolkit than a standalone product. To use, you’ll need to create and assemble resources to create your own API. First you need to set up some boilerplate for both the GSB updater and client: // Set up a db connection.\n$dbh = new PDO('mysql:host=127.0.0.1; dbname=gsb', 'user', ‘password’);\n$dbh->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n// Create storage; works with mysql, sqlite.\n// No auto-increment IDs, so it's safe with master-master replication.\n// Etsy subclasses this and adds StatsD calls. http://etsy.me/dQwVXi $storage = new GSB_StoreDB($dbh);\n\n// Create network access. Pass in your GSB API key . Uses PHP curl .\n$network = new GSB_Request($api);\n\n// Logger. Subclass to use your logging infrastructure (or not).\n$logger = new GSB_Logger(5); Then one needs to setup a cron job that runs every 30 minutes to start mirroring the GSB database. $updater = new GSB_Updater($storage, $network, $logger);\n$updater->downloadData($gsblists, FALSE); It takes about 24 hours to full sync up. Finally, you are able to start checking URLs: $client = new GSB_Client($storage, $network, $logger);\n$url = \"http://malware.testing.google.test/testing/malware/”;\nprint_r($client->doLookup($url)); should return something similar to: [list_id] => 1\n[add_chunk_num] => 70219\n[host_key] => b2ae8c6f\n[prefix] => 51864045\n[match] => malware.testing.google.test/testing/malware/\n[hash] => 518640453f8b2a5f0d43bc2251....\n[host] => testing.google.test/\n[url] => http://malware.testing.google.test/testing/malware/\n[listname] => goog-malware-shavar More details are in the bin/samples directory of our repository. We are currently scanning a few types of user generated content in production. This is done asynchronously from the website so we don’t block the user experience, however we still care about performance. Almost all performance metrics here at Etsy measure maximum and minimum times, as well as 90th percentile and mean, and this is no exception. The peak times occur when a network call is required, otherwise, it’s typically 5ms. Since this is security-related code, another goal of gsb4ucg is testability.  The protocol-parsing code is separated out from database and networking code, so it’s very easy to write unit tests . This also helps to explain how the code works. As you see below, we have some more work to do: In addition to expanding test coverage and improving performance, we’d like to add MAC support, and to use it for more content types on Etsy.  We’d also like to add the results from PhishTank for completeness and redundancy.  Comments, bug reports, patches and pull requests are all welcome, but if this type of work interests you, consider doing it full time . Now, go forth and browse and consume content safely! Posted by Nick Galbreath on March 4, 2012 Category: engineering , security", "date": "2012-03-4,"},
{"website": "Etsy", "title": "Code as Craft Speaker Series: Rasmus Lerdorf, this Thursday", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2012/02/01/code-as-craft-speaker-series-rasmus-lerdorf-this-thursday/", "abstract": "Code as Craft Speaker Series: Rasmus Lerdorf, this Thursday Posted by Kellan Elliott-McCrea on February 1, 2012 A look at the state of PHP in 2012. Where are we, how did we get here and how does PHP fit into the current infrastructure ecosystem of the Web? Plus, a quick tour of what is new and cool in PHP 5.4. Reserve your free ticket , and subscribe to our list to find out about upcoming speakers. Update: A recording of this talk and the slides are now available online. Posted by Kellan Elliott-McCrea on February 1, 2012 Category: Uncategorized", "date": "2012-02-1,"},
{"website": "Etsy", "title": "Upcoming, Etsy Engineering Near You", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2012/02/01/upcoming-etsy-engineering-near-your/", "abstract": "Upcoming, Etsy Engineering Near You Posted by Kellan Elliott-McCrea on February 1, 2012 A few places to look for us in the next few months. Michelle D’Netto and Lindsey Baron, February 23, Selenium 101 Workshop . Brooklyn, NY. Laura Beth Denker, February 24, Scaling Communication via Continuous Deployment .  London. We’re sponsoring Devopsdays Austin , April 2nd and 3rd.  Austin, TX.  Look for us. John Goulah, April 11th, Starts with S and Ends With Hard: The Etsy Shard Architecture . Santa Clara, CA Michelle D’Netto, Stephen Hardisty and Noah Sussman, April 16-18, Handmade Etsy Tests and Selenium In the Enterprise: What Went Right, What Went Wrong (So Far) .  London. Laura Beth Denker, May 22nd, Developer Testing 201: When to Mock and When to Integrate and It’s More Than Just Style .  Chicago, IL Posted by Kellan Elliott-McCrea on February 1, 2012 Category: engineering , events", "date": "2012-02-1,"},
{"website": "Etsy", "title": "The Product Hacking Ecosystem", "author": ["Andrew Morrisson"], "link": "https://codeascraft.com/2012/01/04/the-product-hacking-ecosystem/", "abstract": "The Product Hacking Ecosystem Posted by Andrew Morrisson on January 4, 2012 Most product ideas are shitty, yet we spend the majority of our lives working on them. As a product hacker, you’ll be working on a constant stream of ideas that excite you to the point of obsession; staying up late writing code, thinking about it every waking and non-waking minute. We’ve all admitted that a minority of our ideas will turn into something that will have the impact we dream of, but we don’t let that truth prevent us from being excited that this next thing might be the one. Some have admitted this and accepted that they’re a junky who’s only going to get that fix from a great feature once in a long while. Although I admit that I’m a junky, I haven’t yet become a fatalist. Web Operations people speak about measuring their work by the Mean Time Between Failures (MTBF). For product hackers, we should be thinking in terms of minimizing Mean Time Between Wins (MTBW). Because it’s difficult to know which ideas are going to blossom into that great feature, a nice proxy for MTBW is Mean Time to Bad Idea Detection (MTTBID). By building out an ecosystem for you and your team that allows bad ideas to be detected quickly, you can spend your time iterating on the great ideas and shipping your wins quickly while the shitty ideas die a meaningless death somewhere in a pile of other shitty ideas. The best hackers I know are impatient. As soon as you get an exciting result, you’re going to be talking about it with whoever will listen. An ecosystem of tools that are just there providing a source of truth that everyone can understand and agree with is like having a posse of hardened thugs at your back at all times. Instead of excitement going sour when people who haven’t seen the light are doubting you, you can all agree on whats actually going on. If the numbers you care about are getting better, then great. If your product isn’t something that can be measured easily, or is a long term bet, you can show that the numbers you care about aren’t getting worse and show that its safe to push on into the wilderness. Here are some things we’ve learned about how to build that ecosystem. Make Tools for Failing Fast Ideas can fail at any level of scrutiny. Some ideas don’t pan out when looked at under a microscope. Others don’t work out when talking about it over a drink. If it survives to the point of being shown to users, it can fail when you’re looking at it through a telescope and you’re just not seeing the response you hoped for. We spent some time trying to improve the quality and performance of our relevance sorting algorithm for search results before we made relevance-ordering the site-wide default. During the four month period where we did this work, we were able to get thirty experiments completed. Of those, eleven were real wins that made it into the final product. At Etsy, the birth of every idea is the simplest possible implementation that permits experimentation. To give ourselves immediate feedback on the effects of search algorithm changes we created a tool that let us see the new ranking and all of the information we need to understand why a listing is ranked the way it is. The tool let us see this new ranking the moment our search server finished compiling, allowing for rapid iteration on tricky edge-cases, and the ability to quickly detect and kill bad components. We created a tool that runs a sample of popular and long-tail queries through a new algorithm and displays as much information as can be determined without real people being involved; an estimated percent of changed search results over the universe of all queries, a list of the most strongly affected queries, a list of the most strongly affected Etsy shops, etc.. We created tooling for running side-by-side studies where real users were asked to rate which set of search results they preferred for a given query. When a feature was ready to be launched as an A-B test, we were able to see a set of visualizations explaining how our change was performing relative to the standard algorithm. What a Search AB Test Looks Like What a site-wide AB test looks like The best part is that we don’t think about these tools while building new products and running experiments. We come up with ideas, implement them, and if they do well we ship them. Our conversations are about the product, the code we write is for the product and our shitty ideas are executed on the spot and sloppily buried in shallow graves, as they deserve and as is our wont. Make Tools that Make Process Disappear Edward Tufte introduced the concept of “ chart junk ”; the distracting stuff on a visualization that isn’t saying anything about the data. Marshall McLuhan made a compelling case that “ The medium is the message ” implying that the vehicle through which you perceive something impacts your understanding of it. Just because your paying clients won’t see your internal tooling doesn’t give you license to slap together an ill considered tool. The medium is the message, and your tools are your medium. Working Memory is limited and people are busy. Decisions are worse when getting the answer to a question about your product requires that you lose track of what you asked or why it’s important. Decisions are even worse if you never get a chance to ask questions and get answers. Products designed with fewer poor decisions are less shitty than products designed with more poor decisions. GNU wouldn’t exist without GDB . Our Non-Shitty Search Query Analysis Tool Solr’s Shitty Query Analysis Tool It’s really important to our business that we return great results when people are doing searches on Etsy. It turns out we’re super lazy and if there are any barriers in the way of us asking “why is this item showing up for this query”, we’re just not going to ask the question and it’s not going to get fixed. Our query analysis tool (pictured on the left) helps reduce that barrier to getting an answer. The best information about your product is going to come from real users. Unfortunately, its often painful to get your products out in to the real world. Having completed an iteration of a product, you’re filled with excitement and fear. You’re hoping you got it all right, but if you didn’t, you’re ready to fix it because you know every intimate detail of your new creation. This state of excitement and readiness is the last thing you want to let go of. Continuous deployment, the practice of pushing your code live the moment its ready, is absolutely essential for product hackers. If you need to wait any non-trivial amount of time between completing something and seeing how well it’s performing, you’re not going to be working on that project by the time you get your answer. When you do get your answer, you’re not only going to have to refresh your memory on what you had been working on, but you’re going to have to do the same on whatever else you had started working on. Asking your team to work with patience and discipline has never worked and never will work. Build an ecosystem where doing the right thing is the easiest thing. Build an ecosystem where making great decisions is the easiest thing. Build an ecosystem where the lazy, excitable and impatient really shine. Posted by Andrew Morrisson on January 4, 2012 Category: engineering , people , philosophy Tags: bad idea , mean time between failures , meaningless death", "date": "2012-01-4,"},
{"website": "Etsy", "title": "Turbocharging Solr Index Replication with BitTorrent", "author": ["David Giffin"], "link": "https://codeascraft.com/2012/01/23/solr-bittorrent-index-replication/", "abstract": "Turbocharging Solr Index Replication with BitTorrent Posted by David Giffin on January 23, 2012 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “primary/replica” in our Code as Craft entries. Many of you probably use BitTorrent to download your favorite ebooks, MP3s, and movies.  At Etsy, we use BitTorrent in our production systems for search replication. Search at Etsy Search at Etsy has grown significantly over the years. In January of 2009 we started using Solr for search. We used the standard primary-replica configuration for our search servers with replication. All of the changes to the search index are written to the primary server. The replicas are read-only copies of the primary which serve production traffic. The search index is replicated by copying files from the primary server to the replica servers. The replica servers poll the primary server for updates, and when there are changes to the search index the replica servers will download the changes via HTTP. Our search indexes have grown from 2 GB to over 28 GB over the past 2 years, and copying the index from the primary to the replica nodes became a problem. The Index Replication Issue To keep all of the searches on our site working fast we optimize our indexes nightly. Index optimization creates a completely new copy of the index. As we added new boxes we started to notice a disturbing trend: Solr’s HTTP replication was taking longer and longer to replicate after our nightly index optimization. After some benchmarking we determined that Solr’s HTTP replication was only allowing us to transfer between 2 MB and 8 MB of data per second. We tried various tweaks to HTTP replication adjusting compression and chunk size, but nothing helped. This problem was only going to get worse as we scaled search. When deploying a new replica server we experienced similar issues, only 8 MB per second transfer pulling all of our indexes at once and it could take over 4 hours, with our 3 large indexes consuming most of the transfer time. Our 4 GB optimized listings index was taking over an hour to replicate to 11 search replicas. Even if we made HTTP replication go faster, we were still bound by our server’s network interface card.  We tested netcat from the primary to a replica server and the results were as expected, the network interface was flooded. The problem had to be related to Solr’s HTTP replication. The fundamental limitation with HTTP replication is that replication time increases linearly with the number of replicas. The primary must talk to each replica separately, instead of all at once. If 10 boxes take 4 hours, scaling to 40 boxes would take over half a day! We started looking around for a better way to gets bits across our network. Multicast Rsync? If we need to get the same bits to all of the boxes, why not send the index via multicast to the replicas? It sure would be nice to only send the data once. We found an implementation of rsync which used multicast UDP to transfer the bits. The mrsync tool looked very promising: we could transfer the entire index in our development environment in under 3 minutes. So we thought we would give it a shot in production. [15:25]  <gio> patrick: i'm gonna test multi-rsyncing some indexes\n          from host1 to host2 and host3 in prod. I'll be watching the\n          graphs and what not, but let me know if you see anything\n          funky with the network\n [15:26]  <patrick> ok\n ....\n [15:31]  <keyur> is the site down? Multicast rsync caused an epic failure for our network, killing the entire site for several minutes. The multicast traffic saturated the CPU on our core switches causing all of Etsy to be unreachable. BitTorrent? For those folks who have never heard of BitTorrent, it’s a peer-to-peer file sharing protocol used for transferring data across Internet. BitTorrent is a very popular protocol for transferring large files. It’s been estimated that 43% to 70% of all Internet traffic is BitTorrent peer-to-peer sharing. Our Ops team started experimenting with a BitTorrent package herd , which sits on top of BitTornado . Using herd they transferred our largest search index in 15 minutes. They spent 8 hours tweaking all the variables and making the transfer faster and faster. Using pigz for compression and herd for transfer, they cut the replication time for the biggest index from 60 minutes to just 6 minutes! Our Ops experiments were great for the one time each day when we need to get the index out to all the replica servers, but it would also require coordination with Solr’s HTTP replication. We would need to stop replication, stop indexing, and run an external process to push the index out to the boxes. BitTorrent and Solr Together By integrating BitTorrent protocol into Solr we could replace HTTP replication. BitTorrent supports updating and continuation of downloads, which works well for incremental index updates. When we use BitTorrent for replication, all of the replica servers seed index files allowing us to bring up new replicas (or update stale replicas) very quickly. Selecting a BitTorrent Library We looked into various Java implementations of the BitTorrent protocol and unfortunately none of these fit our needs: The BitTorrent component of Vuze was very hard to extract from their code base torrent4j was largely incomplete and not usable Snark is old, and unfortunately unstable bitext was also unstable, and extremely slow Eventually we came upon ttorrent which fit most of the requirements that we had for integrating BitTorrent into the Solr stack. We needed to make a few changes to ttorrent to handle Solr indexes. We added support for multi-file torrents, which allowed us to hash and replicate the index files in place. We also fixed some issues with large file (> 2 GB) support. All of these changes can be found our fork of the ttorrent code ; most of these changes have already been merged back to the main project. How it Works BitTorrent replication relies on Lucene to give us the names of the files that need to be replicated. When a commit occurs the steps taken on the primary server are as follows: All index files are hashed, a Torrent file is created and written to disk. The Torrent is loaded into the BitTorrent tracker on the primary Solr server. Any other Torrents being tracked are stopped to ensure that we only replicate the latest version of the index. All of the replicas are then notified that a new version of the index is available. The primary server then launches a BitTorrent client locally which seeds the index. Once a replica server has been notified of a new version of the index, or the replica polls the primary server and finds a newer version of the index, the steps taken on the replica servers are as follows: The replica server requests the latest version number from the primary server. The Torrent file for the latest index is downloaded from primary over HTTP. All of the current index files are hash verified based on the contents of the Torrent file. The missing parts of the index are downloaded using the BitTorrent protocol. The replica server then issues a commit to bring the new index online. When new files need to be downloaded, partial (“.part”) files are created. This allows for us to continue downloading if replication gets interrupted. After downloading is completed the replica server continues to seed the index via BitTorrent. This is great for bringing on new servers, or updating servers that have been offline for a period of time. HTTP replication doesn’t allow for the transfer of older versions of a given index. This causes issues with some of our static indexes. When we bring up new replicas, Solr creates a blank index whose version is greater than the static index. We either have to optimize the static indexes or force a commit before replication will take place. With BitTorrent replication all index files are hash verified ensuring replica indexes are consistent with the primary index. It also ensures the index version on the replica servers match the primary server, fixing the static index issue. User Interface The HTTP replication UI is very clunky: you must visit each replica to understand which version of the index it has. Its transfer progress is pretty simple, and towards the end of the transfer is misleading because the index is actually being warmed, but the transfer rate keeps changing. Wouldn’t it be nice to look in one place and understand what’s happening with replication? With BitTorrent replication the primary server keeps a list of replicas in memory. The list of replicas is populated by the replicas polling the primary for the index version. By keeping this list we can create an overview of replication across all of the replicas. Not to mention the juicy BitTorrent transfer details and a fancy progress bar to keep you occupied while waiting for bits to flow through the network. The Results Pictures are worth a few thousand words. Lets look again at the picture from the start of this post, where we had 11 replica servers pull 4 GB of index. Today we have 23 replica servers pulling 9 GB of indexes. You can see it no longer takes over an hour to get the index out to the replicas despite more than doubling the number of replicas and the index size. The second largest triangle on the graph represents our incremental indexer playing catch up after the index optimization. This shows the replicas are helping to share the index as well. The last few red blobs are indexes that haven’t been switch to BitTorrent replication. Drawbacks One of the BitTorrent features is hash verification of the bits on disk. This creates a side effect when dealing with large indexes. The primary server must hash all of the index files to generate the Torrent file. Once the Torrent file is generated all of the replica servers must compare the hashes to the current set of index files. When hashing 9 GB of index it can take roughly 60 seconds to perform the SHA1 calculations. Java’s SHA1 implementation is not thread safe making it impossible to do this process in parallel. This means there is a 2 minute lag before the BitTorrent transfer begins. To get around this issue we created a thread safe version of SHA1 and a DigestPool interface to allow for parallel hashing. This allows us to tune the lag time before the transfer begins, at the expense of increased CPU usage. It’s possible to hash the entire 9 GB in 16 seconds when running in parallel, making the lag to transfer around 32 seconds total. Improvements To better deal with the transfer lag we are looking at creating a Torrent file per index segment. Lucene indexes are made up of various segments. Each commit creates an index segment. By creating a new Torrent file per segment we can reduce the lag before transfer to milliseconds, because new segments are generally small. We are also going to be adding support for transfer of arbitrary files via replication. We use external file fields and custom index time stamp files for keeping track of incremental indexing. It makes sense to have Solr manage replication of these files. We will follow HTTP replication’s lead on confFiles, adding dataFiles and indexFiles to handle the rest of the index related files. Conclusion Our search infrastructure is mission critical at Etsy. Integrating BitTorrent into Solr allows us to scale search without adding lag, keeping our sellers happy ! Posted by David Giffin on January 23, 2012 Category: data , engineering , infrastructure , operations , search", "date": "2012-01-23,"},
{"website": "Etsy", "title": "Etsy at LISA Conference", "author": ["John Goulah"], "link": "https://codeascraft.com/2011/11/30/etsy-at-lisa-conference/", "abstract": "Etsy at LISA Conference Posted by John Goulah on November 30, 2011 A few of us at Etsy will be speaking at the LISA Conference next week which runs from December 4–9, in Boston, MA. Avleen Vig is speaking on Dec. 7th about the operational impact of continuous deployment. Erik Kastner and John Goulah will be talking on Dec. 8th about the tools and culture around our deployment process. Check here for a full schedule of technical sessions. If you happen to be there, please come say hello! Posted by John Goulah on November 30, 2011 Category: events Tags: boston , conferences , continuous deployment , deployinator , lisa", "date": "2011-11-30,"},
{"website": "Etsy", "title": "Translation Memory", "author": ["Diego Alonso"], "link": "https://codeascraft.com/2011/12/05/translation-memory/", "abstract": "Translation Memory Posted by Diego Alonso on December 5, 2011 By: Diego Alonso As we mentioned in Teaching Etsy to Speak a Second Language , developers need to tag English content so it can be extracted and then translated. Since we are a company with a continuous deployment development process, we do this on a daily basis and as an result get a significant number of new messages to be translated along with changes or deletions of existing ones that have already been translated. Therefore we needed some kind of recollection system to easily reuse or follow the style of existing translations. A translation memory is an organized collection of text extracted from a source language with one or more matching translations. A translation memory system stores this data and makes it easily accessible to human translators in order to assist with their tasks. There’s a variety of translation memory systems and related standards in the language industry. Yet, the nature of our extracted messages (containing relevant PHP, Smarty, and JavaScript placeholders) and our desire to maintain a translation style curated by a human language manager made us develop an in-house solution. In short, we needed a system to suggest translations for the extracted messages. Etsy’s Search Team has integrated Lucene / Solr into our deployment infrastructure allowing for Solr configuration, Java-based indexers, and query parsing logic to go to production code in minutes. We decided to take advantage of Lucene’s MoreLikeThis functionality to index “similar” documents, in this case similar English messages with existing translations. The process turned out to be pretty straightforward: we query the requested English message using a ContentStream to the MoreLikeThisHandler and get as a result similar messages with scores. This is done through our Translator’s UI via Thrift . We’ve determined a threshold to filter the messages by score in order to only provide relevant translations after getting similar English messages from the query results. It’s worth mentioning that we need to use a ContentStream to send the source message because most of the time we’ll be requesting translation suggestions for new messages. In other words, messages without translations are not present in our index to match as documents. When sending a ContentSream to the MoreLikeThisHandler , it will extract the “interesting” terms to perform the similarity search. Here’s a simple diagram of the main parts of this process: We could easily test and optimize our results on the search environment through Solr queries before wiring the service in the Translator’s UI. As you can see in the following query we send the content ( stream.body ) of the English message, play with the minimum document frequency ( mindf ) and term frequency ( mintf ) of the terms and even filter the query ( fq ) for translations in a certain language. http://localhost:8393/solr/translationmemory/mlt? stream.body=Join%20Now &mlt.fl=content& mlt.mindf=1 & mlt.mintf=1 &mlt.interestingTerms=list &fl=id,md5,content,type,score&fq= language:de And since we know you love to read some code, here’s how we defined our translation memory data types and service interface in Thrift: struct TranslationMemoryResult {\n 1: string md5\n 2: double score\n}\n\nstruct TranslationMemorySearchResults {\n 1: i32 count,\n 2: list matchedMessages\n}\n\nservice TranslationMemorySearch extends fb303.FacebookService {\n /**\n  * Search for translation memory\n  *\n  * @param content of the message to match\n  * @param language code of the existing translations\n  * @return a TranslationMemorySearchResults instance - never \"null\"\n  */\n TranslationMemorySearchResults search(1: string content,\n                                       2: i32 type,\n                                       3: string language)\n} Let’s look at some common use cases where translation memory comes in handy. It’s pretty common that a new feature is released where we want to attract new members by adding some kind of registration button. In this case the extracted English message has the following data: Description: A call to action to join etsy.com Content: Join Now When displaying this message in our Translator’s UI we get the following results after looking for its content in our Translation Memory . Match Source Translation 100%   Join Now Jetzt teilnehmen 80%    Join now. It's free! Jetzt anmelden. Kostenlos! Another case is when we have a whole feature translated in the site, but we try some different copy in our English version. In the following example the translators can base their translations in their following suggestions. Description: Text for when an experimental feature has no requirements Content: This prototype has no specific requirements. Welcome! Match Source / Translation 86%    This team has no specific requirements. Welcome! / Dieses Team stellt keine speziellen Bedingungen. Willkommen! 86%    This experiment has no specific requirements. Welcome! / Für dieses Experiment gibt es keine speziellen Bedingungen. Willkommen! Here’s a screenshot from our Translator’s UI in action: Having a translation memory system like this has proven to be really useful for our translators who stumble upon new, edited, and deleted messages each day. We also update our index of extracted messages every few minutes with translations, providing resh suggestions. In addition, we have created a translation glossary manager to maintain a common style when translating. When viewing an English message, we stem the content of the message and match the terms with our glossary. A few examples from our German version of the site are “Search” into “Suche”, “Circles” into “Zirkel”, and “Shop” into –surprisingly, the English word– “Shop”. So that’s a glimpse of how deal with translations at Etsy. Check back soon for more posts on how we handle internationalization at Etsy. Posted by Diego Alonso on December 5, 2011 Category: data , engineering , internationalization", "date": "2011-12-5,"},
{"website": "Etsy", "title": "Engineering Social Commerce", "author": ["Jason Davis"], "link": "https://codeascraft.com/2011/11/09/engineering-social-commerce/", "abstract": "Engineering Social Commerce Posted by Jason Davis on November 9, 2011 This holiday season we launched a redesigned version of a product we call “Gift Ideas for Your Friends”. The product works by connecting with your Facebook account, analyzing thousands or more of your friends’ likes and interests, and then making recommendations across millions of items in Etsy’s marketplace. Social commerce has been somewhat of a hot topic lately, and the gift recommender is a social commerce feature in that it provides a new and unique shopping experience to buy gifts for your friends and family. In this post we explore some of the engineering challenges we faced in building a social commerce feature like the gift recommender. The gift recommender is social in that it brings your friends to Etsy. We all know shopping for friends and family is hard, particularly around the holidays when shopping lists grow quite large. Building a responsive experience here that allows navigation across your friends and their recommendations requires a tight coupling between client and server components as well as with Facebook’s API. On the commerce side of things, the product is powered by data mining algorithms that analyze contexts in both Facebook’s social setting and Etsy’s marketplace to make relevant recommendations. While developing these algorithms represents a challenge within itself, the coupling between these algorithms and the end design and user interaction is equally critical. The end result is a product that requires integration among components across our entire stack, including: frontend html, css, and javascript, middle tier application logic and libraries, backend database interfaces and job queues, and hadoop driven recommendations. Let’s dive into some of the application’s core components, related system couplings, and some of the challenges we faced in building the product. Tight integration with Facebook The gift ideas product works by analyzing each of your friends. For each friend, we request various attributes, including name, education history, likes, interests, and activities. Facebook has a limit of 5,000 friends, but having friend counts above 1,000 is not uncommon. Furthermore, it is also not uncommon for people to have upwards of thousands of likes and interests. So, for a typical Facebook power user (read: your average graduate college student) requesting upwards of 100,000 attributes is not uncommon. So how do we pull this amount data back from Facebook? First, you may have noticed that each friend is featured in a separated UI component which allows us to compute recommendations independently. When creating recommendations, we split up friends into groups of 50 and use our asynchronous job queueing system (powered by Gearman) to create recommendations in parallel. Each Facebook request is constructed using a series of fairly complex Facebook Query Language queries, a SQL like language supported by Facebook’s API. Some of these queries are extremely complex. For example, the query to fetch a user’s page likes looks something like this: select page_id, name, type\nfrom page\nwhere page_id in (\n  select page_id\n  from page_fan\n  where uid in (\n    select uid2 from friend where uid1 = me() limit 50;\n  )\n) Requesting data from Facebook is the slowest component of the recommendation creation process: some of our larger Facebook queries take multiple seconds to respond. Performance: Caching, Caching, and Caching The new design for the product displays many friends and their recommendations on the primary splash page. This is in contrast to the old design which only allowed for viewing of only one friend’s recommendations at a time. This presented several performance challenges. Each Facebook attribute triggers a recommendation, and each recommendation shows items from the marketplace by issuing a search query. The new product displays four recommendations per friend in batches of 20 friends, so each batch can require as many as 80 search queries. Assuming an average response time of ~200ms per search, this could add up to load times in excess of 15 seconds. (!) Luckily, the distribution of Facebook likes (and corresponding gift recommendations) is very sharp: the most popular 5,000 recommendations represent over 90% of all recommendations made by the product. Therefore, caching listing results at a per-recommendation level granularity provides us with tremendous speedups: 200ms search requests optimize to ~2ms memcache requests. Client and Server-side Facebook API We make heavy usage of the Facebook client API to authenticate users during initial Facebook connection phases: we did not want to recreate the javascript authentication flow supported by Facebook’s javascript SDK. However, we also needed server side API access for deeper queries required by the core recommender algorithm. Complicating matters, we also recently released a feature that allows you to connect your Etsy account with your Facebook account. Managing tokens and authentication across the two systems while also allowing users to shop for gift ideas without an Etsy account presents several technical challenges. Dealing with Backend Latency on the Frontend Perhaps the biggest improvements made this year stem from a tighter coupling between the backend recommendation generation process and the frontend display. The initial creation process can take 3 or more seconds, and providing user feedback and context throughout is critical. You may have noticed that your recommendations fill in “on the fly” as they’re created. As each of the asynchronous Gearman job workers completes its recommendation task, we stream results back to the client, which then renders them immediately via ajax. The end goal here is to enable the user to see recommendations appear as soon as possible, providing a more immediate shopping experience. Data Mining Of course, the core of the recommender system is the recommender algorithm and the supporting data. The core algorithm is responsible for understanding the meaning of a given Facebook attribute in an Etsy context. For example, the artist “Pink” is a popular musician on Facebook. However, a query for “pink” returns substantially different results on Etsy. The core gift recommendation algorithm is overviewed in a post from earlier this year. We’ve also made several improvements since then. We’re smarter in analyzing gender when retrieving appropriate listing suggestions, and we’ve also taken another pass and removed bad listings based on data from the first year of the product. Precision vs Recall, and the End Experience “Gift Ideas for Your Friends” provides a different experience compared to other traditional recommender algorithms. For example, Netflix’s algorithms take a collaborative approach in which your entire profile is analyzed in aggregate, and recommendations are created by comparing your favorite movies compared to others. In contrast, “Gift Ideas for Your Friends” makes point-based recommendations off of a single attribute of your friend’s Facebook profile. Jim likes burning man. Kurt likes video games. Chad likes Brooklyn. In informational retrieval terms, the goal of the gift recommender is to optimize on precision: to make a handful of good recommendations based on a given set of attributes. There are lots of things that your mother likes on Etsy that aren’t represented in her Facebook profile, and the gift recommender will “miss” these recommendation opportunities. This is compared to Netflix style recommendations where the goal is to optimize for recall: given your entire movie history, provide recommendations that capture your taste as a whole. In fact, for the general gift giving problem, optimizing for precision is a more natural objective: you generally buy your mother only a one or two gifts each year. Your mother might appreciate gifts like vintage glassware, amethyst jewelry, raku pottery, etc. A successful holiday gift really only requires buying her one of these items. Netflix style recommendations are aimed at capturing your various aspects of your taste and have stronger expectations for movie recommendations across all genres / styles that you may like. The ultimate goal of the product is to provide a glimpse of Etsy through your friends and their existence on Facebook. We view the recommendations and sample results not as the final word in what to buy, but rather as a landing pad to dive into the marketplace. Diving into Chad’s recommendations for “Brooklyn” could then lead to a search for “brooklyn bridge” and purchasing an 8×10 photo. At Etsy, we build our system in a continuously deployed environment which allows us to quickly iterate and experiment. We view everything we build as somewhat of an experiment, and the Facebook gifter is no exception. We look forward to the future of “Gift Ideas for Your Friends” and social commerce in general on Etsy. Posted by Jason Davis on November 9, 2011 Category: data", "date": "2011-11-9,"},
{"website": "Etsy", "title": "Etsy at Hadoop World", "author": ["Aaron Beppu"], "link": "https://codeascraft.com/2011/11/07/etsy-at-hadoop-world/", "abstract": "Etsy at Hadoop World Posted by Aaron Beppu on November 7, 2011 Tomorrow afternoon, I will be speaking at Hadoop World about Data Mining for Product Search Ranking . The talk will cover recent attempts to improve search at Etsy using clickstream analysis in Hadoop. If you’re attending Hadoop World, come say hello! Posted by Aaron Beppu on November 7, 2011 Category: events", "date": "2011-11-7,"},
{"website": "Etsy", "title": "Grace Hopper Celebration of Women in Computing 2011", "author": ["Laura Beth Denker"], "link": "https://codeascraft.com/2011/11/04/grace-hopper-celebration-of-women-in-computing-2011/", "abstract": "Grace Hopper Celebration of Women in Computing 2011 Posted by Laura Beth Denker on November 4, 2011 We are proud to announce that we are a sponsor of this year’s Grace Hopper Celebration of Women in Computing .  The conference is being held next week, November 9-11, at the Oregon Convention Center in Portland, OR. Grace Hopper Celebration of Women in Computing is the largest technical conference for women in computing.  Michelle D’Netto, Rachel Vecchitto, and myself are so excited to be attending the conference.  We cannot wait to catch up with women leaders in industry, academia and government, and have the most wonderful opportunity to meet the future of women in computing. Please come meet us at the Etsy booth in the exhibit hall, or at least bump into us if you will be in Portland. P.S.  Thank you, Grace Hopper, for your words of wisdom: “It’s easier to ask for forgiveness than it is to get permission.” Posted by Laura Beth Denker on November 4, 2011 Category: events Tags: conferences Related Posts Posted by John Goulah on 30 Nov, 2011 Etsy at LISA Conference Posted by John Goulah on 13 Oct, 2011 Removing Barriers to Going Fast", "date": "2011-11-4,"},
{"website": "Etsy", "title": "Localizing Logically for a Global Marketplace", "author": ["Eric Bogs"], "link": "https://codeascraft.com/2011/10/21/localizing-logically-for-a-global-marketplace/", "abstract": "Localizing Logically for a Global Marketplace Posted by Eric Bogs on October 21, 2011 An often-overlooked (or underestimated) aspect of internationalizing a website is determining how to localize for a given visitor.  You might detect that a visitor is “German” because their Geo IP locates them as connecting from Germany, or because their browser accept language is German.  But what if your detection is faulty… they’re using a shared computer, or on vacation, or the Geo IP is just plain wrong? At Etsy, we wanted to use a tiny bit of magic to help provide the best localized experience for visitors, while still allowing users control over their experience.  Websites often use too much magic (e.g. automatically setting incorrect localization settings with a difficult-to-find escape hatch) or use too little (e.g. forcing visitors to a lame splash page where they must choose their home country/language from dropdowns). For a global marketplace like Etsy, localization breaks down into three components: Language —what language should we use to display the site (UI chrome, shops, listings, help content, emails, etc.)? Region —what region (country) should we assume the user is from, when showing regional content (e.g. blog posts) or during shopping (e.g. search, checkout, shipping)? Currency —what currency should we display prices in? For a given visitor, localizing to the correct language, region and currency provides the best experience on Etsy.  We spent time understanding our visitors and members, determining the best cues for our detection logic, and created an EtsyLocale helper class to encapsulate all localization-related functionality on the site. Understand your visitors We started by examining primary and secondary use cases. Primary use cases: English-speaking American user (en/US/USD) English-speaking Canadian user (en/US/CAD) British English-speaking UK user (en-GB/UK/GBP) British English-speaking Australian user (en-GB/AU/AUD) German-speaking German user (de/DE/EUR) French-speaking French user (fr/FR/EUR) Secondary use cases: Curious American user wanting to see German translations (de/DE/EUR with easy route back to en/US/USD) German user on vacation in America (de/DE/EUR) Spanish-speaking American (es/US/USD) French-speaking Canadian (fr/CA/CAD) British ex-pat living in Japan (en-GB/JP/JPY) English-speaking user using friend’s German computer (en/DE/EUR) Some countries have clear majority preferences for language, but some don’t!  And there are plenty of edge case exceptions to think through for any userbase.  For that reason, we allow language, region & currency to be set independently. Cues At Etsy, we use a series of cues to determine which language, region & currency to show a user.  In decreasing order of “signal strength”: User preference (language, currency and/or region preference set manually by signed-in member) Cookie preference (language, currency and/or region preference set manually by signed-out visitor) Primary browser accept language (e.g. en-US or de-DE or de or kr) User profile address country (e.g. DE or US) GeoIP region (e.g. DE or US) We also have ccTLDs (e.g. etsy.de) and subdomains (e.g. de.etsy.com) that we use for marketing efforts and for search indexing (SEO) purposes, which we’ll discuss in a later blog post. Magic When a visitor comes to Etsy without preferences set, we iterate through the above cues in order, and for each cue we see if there’s a clear mapping between the cue and our set of supported languages, regions and currencies.  Once we’ve computed our best guess for language, region & currency, we show a gentle nag at the bottom of the page: The nag is persistent, and is in both the detected language and English.  We always nag instead of auto-setting to minimize confusion/surprises ( magic ). We don’t nag our primary market users (English-speaking users in the US).  All visitors/members have the ability to change their language, region & currency preferences by clicking links in the footer of any page: We store these preferences as cookie preferences for signed-out visitors.  When a user registers, we migrate these cookie preferences to user preferences.  When a user signs out, we don’t write their language/region/currency preferences back out as a cookie, providing a “clean slate” experience for other users of that browser.  When we add support for new languages/regions/currencies, we treat that as a new “version” of the preferences, re-nagging where appropriate. Encapsulate We encapsulate all of this logic in an EtsyLocale() object, which is available across our stack, for easy access to the current visitor/member’s language, region and currency, e.g. if (EtsyLocale::getInstance()->getRegion() == “DE”) { include “hello_etsy_berlin_meetup.tpl”; } We make use of Smarty modifiers to format and display prices based on EtsyLocale->getCurrency() .  Our translation tools (specifically, translateMsg() ) make use of EtsyLocale->getLanguage() to determine which translations to use. We use PHP’s built-in setlocale() methods for date formatting (including month name translations), number formatting, string alphabetization and so on.  PHP’s setlocale() function has varying support for locale formats.  For example, if an Etsy visitor has German language preferences but French region preferences, we might represent that locale string as “de_FR” in PHP.   However, setlocale() doesn’t understand that we should use German month names and number formatting for “de_FR”.  So, to be safe, we pass in a list of locale strings that setlocale() should attempt to use including a more generic language-only locale—in this case (“de_FR”, “de_DE”). You get a lot for free with PHP locales, but there’s still a lot of holes to plug.  At Etsy, we needed to develop date formats for each region, e.g. short dates (“Dec 1, 2010” for en-US, “01. Dez. 2010” for de) and long dates (“December 1, 2010” for en-US, “01 décembre 2010” for fr).  Using setlocale() too aggressively for number formatting can cause SQL-incompatible float writing (e.g. “1.234,56” instead of “1,234.56“).  And keep in mind you often need to use multibyte-aware functions in PHP to take advantage of locale settings. Na, was sagt ihr? We’d love to hear from you… any examples of well-localized sites, problems you’ve come across, PHP tricks/solutions?  Share with us below.  Stay tuned for more about Etsy’s internationalization . Posted by Eric Bogs on October 21, 2011 Category: engineering , internationalization Tags: currency , language , locale , localization , region", "date": "2011-10-21,"},
{"website": "Etsy", "title": "Teaching Etsy to Speak a Second Language", "author": ["Corey Losenegger"], "link": "https://codeascraft.com/2011/10/19/teaching-etsy-to-speak-a-second-language/", "abstract": "Teaching Etsy to Speak a Second Language Posted by Corey Losenegger on October 19, 2011 By: JM Imbrescia & Corey Losenegger As an Engineer, it can be quite daunting planning how to translate a website from English into another language. We just finished teaching the Etsy website to speak German , and wanted to share some tips. When preparing to translate the site, there are a few challenges to think through: How to annotate English content that needs to be translated How to handle translation of English content into other languages How to efficiently serve translated content to the end user In this post we’ll be focusing on how we solved #1 and #3, saving #2 for a separate post later.  Also, note that user-generated content (e.g. shops & item listings) are translated by our sellers through a separate interface. Although there many great translation solutions for platforms such as Rails and Java, there aren’t that many for PHP and Smarty, which we use at Etsy.  We evaluated a variety of existing PHP solutions (SmartyMultiLanguage, Yahoo R3) as well as looking at how other sites (Twitter, Facebook) are translated.  In the end, we decided to create our own translation infrastructure. Here’s an overview of the workflow: The key components are the Message Extractor, the Translation Bundles, and the Substitution functions, which we’ll discuss below.  First, let’s talk about tagging… Tagging The key to our translation workflow is message tagging, which is a way for developers to annotate English content for translation.  Once messages are tagged, they can then be extracted for translation, and later substituted with translated content. We use a custom message HTML tag to wrap all English phrases: <p><msg desc=\"Footer link\">Click <a href=\"/\">here</a> to return to Etsy.</msg></p> This msg tag is a cue to the Message Extractor that this is an English string which needs to be translated.  It is also used later by the substitution functions, which swap in translated content into the template.  The desc attribute is used to give human translator some context. What about PHP, Javascript, and images? We strive to keep all English content in the presentation layer (in Smarty templates).  In the case of English strings which live in databases (e.g. item category names), we create custom extraction methods.  For images, we strive to move stylized text into HTML & CSS (again, in Smarty).  For Javascript strings, we strive to keep text in Smarty templates (either as inline Javascript, or as hidden DOM nodes which can be accessed via Javascript).  For strings which live in PHP files (e.g. error messages shared by several Smarty templates), we have an additional “tag” which looks like this: $translator->translateMsg(“You must enter your username”, “Error message”); Tagging Etsy At Etsy, it tooks us 3 months to tag 1,200 templates containing 13,000 strings, plus 4,000 database strings (e.g. item categories).  The responsibility for tagging all of these templates falls on all Engineers, Product Managers and Designers at Etsy that contribute to our codebase.  The mandate is that all English strings must be tagged for translation, which took us some time to adapt to but now that we’ve launched in a few languages it’s easy for everyone to understand the importance of tagging for translation.  Anyone who inadvertently forgets to tag a string gets a sticker surreptitiously attached to their laptop: We see around 50-100 new English strings come in each day for our translators to translate.  Tagging bite-sized phrases no larger than a few sentences has worked well for our translators.  We’ll talk more about some tagging gotchas (e.g. plurals, possessives), as well as our translation workflow (and how it affects continuous deployment) in later blog posts. Extraction The Message Extractor is a (PHP, like most of our tools) script which scans the codebase, extracts tagged messages, and stores them for translation.  It is a dumb file parser which uses regexes to match messages based on the <msg> and translateMsg() “tags” above.  We use the MD5 of content hashed with description as a way to track unique/changed messages.  Messages with the exact same content and description will always be translated to the same translation. Pre-commit hooks help developers by checking for invalid tags. A nightly cron runs the Message Extractor and writes messages to a database, and sends out an email like this that allows everyone to monitor translation status: Translators use custom translation tools built in-house to add/edit translations. Bundling Once our translators have translated English strings into the relevant languages, we then bundle (dump) these translations into JSON files to be deployed alongside our PHP & Smarty codebase using Deployinator .  Use a static translation bundle file removes any dependency on databases, and allows for easy versioning, rollbacks, and allows us to test translations in our usual QA -> Princess -> Production deployment flow. Substitution How does a Smarty template littered with tagged (<msg>) English strings get translations swapped in?  Fortunately, Smarty provides pre- and post-filters to apply functions to template contents.  We make use of a straightforward Smarty prefilter, which runs a regex against Smarty template contents, looking for <msg> tags.  For each <msg> tag, it computes an MD5 hash (again, based on content and description), and then checks the translation bundle for a relevant translation to swap in.  We use Smarty’s compilation functions to precompile all templates across all supported languages during deployment. The function it uses to do this MD5 hashing & swapping is a PHP function called translateMsg(), which we mentioned above.  This same translateMsg() function is also available throughout the PHP codebase to translate one-off messages that aren’t able to be moved into Smarty templates. Translation is just one piece of the puzzle There’s a great deal of other localization that needs to be handled—from language- and region-specific features, to little details such as date, currency, and number formatting.  For these cases we usually crate custom Smarty modifiers and wrappers which take localization logic into account. That’s our translation stack from top to bottom—please chime in with any questions or comments you’ve got.  Stay tuned for additional posts about how we’ve internationalized Etsy. Posted by Corey Losenegger on October 19, 2011 Category: engineering , internationalization Tags: German , localization , translation Related Posts Posted by Eric Bogs on 08 Oct, 2011 Internationalizing Etsy: Fostering a Global Marketplace", "date": "2011-10-19,"},
{"website": "Etsy", "title": "Removing Barriers to Going Fast", "author": ["John Goulah"], "link": "https://codeascraft.com/2011/10/13/removing-barriers-to-going-fast/", "abstract": "Removing Barriers to Going Fast Posted by John Goulah on October 13, 2011 Erik Kastner and I will be speaking at EscConf on October 27th in Boston. The talk is a redux of our OSCON talk about overcoming cultural barriers to moving fast and the tools we use at Etsy to push code. We hope to see you there! Posted by John Goulah on October 13, 2011 Category: events Tags: conferences , deployinator , tools Related Posts Posted by John Goulah on 30 Nov, 2011 Etsy at LISA Conference Posted by Laura Beth Denker on 04 Nov, 2011 Grace Hopper Celebration of Women in Computing 2011", "date": "2011-10-13,"},
{"website": "Etsy", "title": "Internationalizing Etsy: Fostering a Global Marketplace", "author": ["Eric Bogs"], "link": "https://codeascraft.com/2011/10/08/internationalizing-etsy-fostering-a-global-marketplace/", "abstract": "Internationalizing Etsy: Fostering a Global Marketplace Posted by Eric Bogs on October 8, 2011 There’s nothing more frustrating than not understanding what’s right in front of you, as the non-German speakers among us might surmise after watching the clip from comedy programme Loriot above.  At Etsy, we’ve been working to build the world’s first truly global marketplace, allowing buyers and sellers to connect and have meaningful exchanges, regardless of which languages they speak.  As a first step, we launched Etsy in German in September. As Chad points out , Etsy has always been a global marketplace, but it’s only recently that we’ve really focused on better supporting our international (non-US, non-English) members. Internationalization at Etsy means preparing our business to support international members and transactions, and also preparing the website—everything from registration, to searching and discovering, to buying and selling, to support. We typically think of two broad categories when thinking of the technical side of internationalization: localization and translation. Over the coming weeks, we’ll be sharing some of the engineering work we’ve done to internationalize Etsy.  Check back here on Code as Craft as we dig into some of the interesting bits of our internationalization stack (on both the localization and translation sides), discuss some of the challenges we’ve faced and share the solutions we’ve developed. Some of the topics we plan on covering: Localization logic : Combining cues such as Geo IP, browser language preferences along with cookie/user preferences to display the appropriately-localized language, region and currency content. Translation stack : How we tag templates and database content for translation, then translate and serve to our visitors.  Will include our PHP-based message extractor, Smarty prefilter for precompiling in translated content, and SQLite translation bundler. Translator tools , used by translators to translate and test the translated site. In Translation Memory , we’ll discuss one of the key pieces of our Translation Tools: our translation memory built on Lucene, and how we extended CodeMirror to handle HTML and Smarty entities. Translating user-generated content : allowing for multilanguage user-generated content (such as shops and item listings) comes with data model challenges. Search query translation to allow for performant cross-language search and discovery of shops and listings, using a translated taxonomy and machine translation services. Multilingual User Generated Content and SEO : cues for search engines to understand translated content, translated site UI, and how to play nice with sitemaps and robots.txt. Continuous deployment = continuous translation .  We’ll talk about our translation workflow, and how we’re dealing with common translation and linguistic QA problems. Localization gotchas : addresses, date formats, possessives, plurals, gender, cache keys, stemming, compound words. If you haven’t already, take a test drive of Etsy in German, and comment below with any questions or feedback. Posted by Eric Bogs on October 8, 2011 Category: engineering , infrastructure , internationalization Tags: German , localization , translation Related Posts Posted by Corey Losenegger on 19 Oct, 2011 Teaching Etsy to Speak a Second Language", "date": "2011-10-8,"},
{"website": "Etsy", "title": "Scaling Etsy: What Went Wrong, What Went Right", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2011/09/28/scaling-etsy-what-went-wrong-what-went-right/", "abstract": "Scaling Etsy: What Went Wrong, What Went Right Posted by Kellan Elliott-McCrea on September 28, 2011 Ross Snyder will be speaking at Surge this Thursday/Friday in Balitmore, on Scaling Etsy: What Went Wrong, What Went Right . Posted by Kellan Elliott-McCrea on September 28, 2011 Category: Uncategorized", "date": "2011-09-28,"},
{"website": "Etsy", "title": "How Etsy Prepared for Historic Volumes of Holiday Traffic in 2020", "author": ["Mike Adler"], "link": "https://codeascraft.com/2021/02/25/how-etsy-prepared-for-historic-volumes-of-holiday-traffic-in-2020/", "abstract": "How Etsy Prepared for Historic Volumes of Holiday Traffic in 2020 Posted by Mike Adler on February 25, 2021 The Challenge For Etsy, 2020 was a year of unprecedented and volatile growth. (Simply on a human level it was also a profoundly tragic year, but that’s a different article.) Our site traffic leapt up in the second quarter, when lockdowns went into widespread effect, by an amount it normally would have taken several years to achieve. When we looked ahead to the holiday season, we knew we faced real uncertainty. It was difficult to predict how large-scale economic, social and political changes would affect buyer behavior. Would social distancing accelerate the trends toward online shopping? Even if our traffic maintained a normal month-over-month growth rate from August to December, given the plateau we had reached, that would put us on course deep into uncharted territory. Graph of weekly traffic for 2016-2020. This was intimidating! If traffic exceeds our preparation, would we have enough compute resources? Would we hit some hidden scalability bottlenecks? If we over-scaled, there was a risk of wasting money and spoiling environmental resources . If we under-scaled, it could have been much worse for our sellers. For context about Etsy, as of 2020 Q4 we had 81 million active buyers and over 85 million items for sale. Modulating Our Pace of Change When we talk about capacity , we have to recognize that ultimately Etsy’s community of sellers and employees are the source of it. The holiday shopping season is always high-stakes so we adapt our normal working order each year so that we don’t over-burden that capacity. Many sellers operate their businesses at full stretch for weeks during the season. It’s unfair to ask them to adapt to changes at a time when they’re already maxed out. Our Member Services support staff are busy when our sellers are busy. If we push changes that increase the demand for support, we undermine our own efforts to provide excellent care. Our infrastructure teams are on alert to scale our systems against novel peaks and to maintain overall system health. Last-minute changes could easily compromise their efforts. In short, we discourage, for a few weeks, deploying changes that might be expected to disrupt sellers, support, or infrastructure. Necessary changes can still get pushed, but the standards are higher for preparation and communication. We call this period “Slush” because it’s not a total freeze and we’ve written about it before . For more than a decade, we have maintained Slush as an Etsy holiday tradition. Each year, however, we iterate and adapt the guidance, trying to strike the right balance of constraints. It’s a living tradition. Modeling History To Inform Capacity Planning Even though moving to Google Cloud Platform ( GCP ) in 2018 vastly streamlined our preparations for the holidays, we still rely on good old-fashioned capacity planning. We look at historical trends and system resource usage and communicate our infrastructure projections to GCP many weeks ahead of time to ensure they will have the right types of machines in the right locations. For this purpose, we share upper-bound estimates, because this may become the maximum available to us later. We err on the side of over-communicating with GCP. As we approached the final weeks of preparation and the ramp up to Cyber Monday, we wanted to understand whether we were growing faster or slower than a normal year. Because US Thanksgiving is celebrated on the fourth Thursday of November rather than a set date, it moves around in the calendar and it takes a little effort to perform year-to-year comparisons. My teammate Dany Daya built a simple model that looked at daily traffic but normalized the date using “days until Cyber Monday.” This would come in very handy as a stable benchmark of normal trends when customer patterns shifted unusually. Adapting Our “Macro” Load Testing Though we occasionally write synthetic load tests to better understand Etsy’s scalability, I’m generally skeptical about what we can learn using macro (site-wide) synthetic tests. When you take a very complex system and make it safe to test at scale, you’re often looking at something quite different from normal operation in production. We usually get the most learning per effort by testing discrete components of our site, such as we do with search. Having experienced effectively multiple years of growth during Q2, and knowing consumer patterns could change unexpectedly, performance at scale was now an even bigger concern. We decided to try a multi-team game day with macro load testing and see what we could learn. We wanted an open-ended test that could help expose bottlenecks as early as possible. We gathered together the 15-plus teams that are primarily responsible for scaling systems. We talked about how much traffic we wanted to simulate, asked about how the proposed load tests might affect their systems and whether there were any significant risks. The many technical insights of these teams deserve their own articles. Many of their systems can be scaled horizontally without much fuss, but the work is a craft: predicting resource requirements, anticipating bottlenecks, and safely increasing/decreasing capacity without disrupting production. All teams had at least one common deadline: request quota increases from GCP for their projects by early September. We were ready to practice together as part of a multi-team “scale day” in early October. We asked everyone to increase their capacity to handle 3x the traffic of August and then we ran load tests by replaying requests (our systems served production traffic plus synthetic replays). We gradually ramped up requests, looking for signs of increased latency, errors, or system saturation. While there are limitations to what we can learn from a general site-wide load test, scale day helped us build confidence. We confirmed all projects had enough quota from GCP We confirmed our many scaling tools worked as intended (Terraform, GKE , instance group managers, Chef, etc.) We exposed bottlenecks in the configuration of some components, for example some Memcache clusters and our StatsD relays, both of which were quickly addressed But crucially, we confirmed many systems looked like they could handle scale beyond what we expected at the peak of 2020. Cresting The Peak Let’s skip ahead to Cyber Monday, which is typically our busiest day of the year. Throughput on our sharded MySQL infrastructure peaked around 1.5 million queries per second. Memcache throughput peaked over 20M requests per second. Our internal http API cluster served over 300k requests per second. Normally, no one deploys on Cyber Monday. Our focus is on responding to any emergent issues as quickly as possible. But 2020 threw us another curve: postal service interruptions meant that our customers were facing widespread package delivery delays . It only needed a small code change to better inform our buyers about the issue, but we’d be deploying it at the peak hour of our busiest day. And since it would be the first deployment of that day, the entire codebase would need to be compiled from scratch, in production, on more than 1000 hosts. We debated waiting till morning to push the change, but that wouldn’t have served our customers, and we were confident we could push at any time. Still, as Todd Mazierski and Anu Gulati began the deploy, we started nominating each other for hypothetical Three Armed Sweaters Awards . But the change turned out to be sublimely uneventful. We have been practicing continuous deployment for more than a decade. We have invested in making deployments safe and easy. We know our tools pretty well and we have confidence in them. Gratitude We have long maintained a focus on scalability at Etsy, but we all expected to double traffic over a period of years, not just a few months. We certainly did not expect to face these challenges while working entirely distributed during a pandemic. We made it to Christmas with fewer operational issues than we’ve experienced in recent memory. I think our success in 2020 underscored some important things about Etsy’s culture and technical practices. We take pride in operational excellence , meaning that every engineer takes responsibility not just for their own code, but for how it actually operates in production for our users. When there is an outage, we always have more than enough experts on hand to mitigate the issue quickly. When we hold a Blameless Postmortem , everyone shares their story candidly. When we discover a technical or organization liability, we try to acknowledge it openly rather than hide it.  All of this helps to keep our incidents small. Our approach to systems architecture values long-term continuity, with a focus on a small number of well-understood tools , and that provided us the ability to scale with confidence. So while 2020 had more than its share of surprising circumstances, we could still count on minimal surprises from our tools. Posted by Mike Adler on February 25, 2021 Category: engineering , infrastructure , operations , outages , performance", "date": "2021-02-25,"},
{"website": "Etsy", "title": "Bringing Personalized Search to Etsy", "author": ["Lucia Yu"], "link": "https://codeascraft.com/2020/10/29/bringing-personalized-search-to-etsy/", "abstract": "Bringing Personalized Search to Etsy Posted by Lucia Yu on October 29, 2020 The Etsy marketplace brings together shoppers and independent sellers from all over the world. Our unconventional inventory presents unique challenges for product search, given that many of our listings fall outside of standard e-commerce categories. With more than 80 million listings and 3.7 million sellers, Etsy relies on machine learning to help users browse creative, handmade goods in their search results. But what if we could make the search experience even better with results tailored to each user? Enter personalized search results. Search results for “tray” (above) default results, (below) a user who recently interacted with leather goods When a user logs into the marketplace and searches for items, they signal their preferences by interacting with listings that pique their interest. In personalization, our algorithms train on these signals and learn to predict, per user, the most relevant listings. The resulting personalized model lets individuals’ taste shine through in their search results without compromising performance. Personalization enhances the underlying search model by customizing the ordering of relevant items according to user preference. Using a combination of historical and contextual features, the search ranking model learns to recognize which items have a greater alignment with an individual’s taste. In the following sections, we describe the Etsy search architecture and pipeline, the features we use to create personalized search results, and the performance of this new model. Finally, we reflect on the challenges from launching our first personalized search model and look ahead to future iterations of personalization. Please note that some sections of the post are more technical and assume knowledge of machine learning basics from the reader. Etsy Search Architecture The search pipeline is separated into two passes: candidate set retrieval and ranking. This ensures that we are returning low-latency results — a crucial component of a good search system. Because the latter ranking step is computationally expensive, we want to use it wisely. So from millions of total listings, the candidate set retrieval step selects the top one thousand items for a query by considering tags, titles, and other seller-provided attributes. This allows us to run the ranking algorithm over less than one percent of all listings. In the end, the ranker will place the most relevant items at the top of the search results page. Our search ranking algorithm is an ensemble model that uses a gradient boosted decision tree with pairwise formulation. For personalization, we introduce a new set of features that allow us to model user preferences. These features are included in addition to existing features, such as listing price and recency. And as much as we try to avoid impacting latency, the introduction of these personalized features creates new challenges in serving online results. Because the features are specific to each user, the cache utilization rate drops. We address this and other challenges in a later section. Personalized User Representations The novelty of personalized search results lies in the new features we pass to the ranker. We categorize personalization features into two groups: historical and contextual features. Historical features refer to singular data points about a user’s profile that can succinctly describe shopping habits and behaviors. Are they modern consumers of digital goods, or are they hunting and gathering vintage pieces? Do they carefully deliberate on each individual purchase, or do they follow their lightning-strike gut instinct? We can gather these insights from the number of digital or vintage items purchased and average number of site visits. Historical user features help us put the “person” in personalization. Search results for “lamp” (above) default results, (below) a user who recently interacted with epoxy resin items In contrast to these numerical features, data can also be represented as a vector, or a list of numbers. For personalization, we refer to these vectored features as contextual features because the listing vector represents a listing with respect to the context of all other listings . In fact, there are many ways to represent a listing as a vector but we use term frequency–inverse document frequency (Tf-Idf), item-interaction embeddings and interaction-based graph embeddings. If you’re unfamiliar with any of these methods, don’t worry! We’ll be diving deeper into the specific vector generation algorithms. So how do we capture a user’s preferences from a bunch of listing vectors? One method is to average all the listings a user has clicked on to represent them. In other words, the user contextual vector is simply the average of all the interacted listings’ contextual vectors. We gather historical and contextual features from across our mobile web, desktop and mobile application platforms. This allows us to maximize the amount of information our model can use to personalize search result rankings. The Many Ways Users Show Us Love In addition to clicks on a listing from search results, a user has a few other ways to connect with sellers’ items on the marketplace. After a user searches for an item, they can favorite items in the search results page and save them to their own curated collections, they can add an item to their cart while they continue to browse, and once they are satisfied with their selection they can purchase the item. Each of these interactions has distinct characteristics which help our model generalize and generate more accurate predictions. Clicks are by far the most popular way for buyers to engage with listings, and through sheer quantity provide for a rich source of material  to model user behaviors. On the other end, purchase interactions occur less frequently than clicks but contain stronger indications of relevance of an item to the user’s search query. The Heart of Personalization Now, let’s get to the crux of personalization and dig deeper into user contextual features. Tf-Idf vectors consider listings from a textual standpoint, where words in the seller-provided attributes are weighted according to their importance. These attributes include listing titles, tags, and others. Each word’s importance is derived with respect to its frequency in the immediate listing text, but also the larger corpus of listing texts. This allows us to distinguish a listing from others and capture its unique qualities. When we average the last few months’ worth of listings a user has interacted with, we are averaging the weights of words in those listings to create a single Tf-Idf vector and represent the user. In other words, in Tf-Idf a listing is represented by its most important listing words and a user is represented as an average of those most important words. Diagram of interaction-based graph embedding In this example of interaction-based graph embeddings, the queries “dollhouse”  and “dolls” resulted in clicks on listing 824770513 on three and five occasions, respectively. Unlike Tf-Idf, an interaction-based graph embedding can capture the larger interaction context of query and listing journeys. Recall that interactions can be as clicks, favorites, add-to-carts or purchases from a user. Let’s say we have a query and some listings that are often clicked with that query. When we match words within the query to the words in the listing texts and weigh the words common to both of them, we can represent listings and queries with the same vocabulary. A common vocabulary is an important quality in textual representations because we can derive degrees of relatedness between queries to listings despite differences in length and purpose. Therefore, if a few different listings are all clicked as a result of the same query, we expect the embeddings for these listings to be similar. And similar to Tf-Idf, we can simply average the weights of words in the sparse vectors over some time frame. Whereas graph embeddings weave behavior from interaction logs into the vector representation, Tf-Idf only uses available listing text. Put more plainly, for graph embeddings users tell us which queries and listings are related and we model this information by finding overlaps between their words. Diagram from Learning Item-Interaction Embeddings for User Recommendations However, focusing on a single interaction type within an embedding can be limiting. In reality, users can have a combination of different interactions within a single shopping session. Item-interaction vectors can learn multiple interactions in the same space. Created by our very own data scientists here at Etsy, item-interaction vectors build upon the methods of word2vec where words occurring in the same context share a higher vector similarity. The implementation of item-interaction vectors is simple but elegant: we replace words and sentences with item-interaction token pairs and sequences of interacted items. A token pair is formulated as (item, interaction-type) to represent how a user interacted with a specific item or listing. And an ordered list of these tokens represents the sequence of what and how a user interacted with various listings in a session. As a result, item-interaction token pairs that appear in the same context will be considered similar. Because these listings embeddings are dense vectors, we can easily find similar listings via distance metrics. To summarize item-interaction vectors, similar to interaction-based graph embeddings we let the users guide us in learning which listings are similar. But rather than deriving listing similarities from query and listing relationships, we infer similarity if listings appear in the same sequences of interactions. Putting It All Together Let’s take stock of what we have to work with: recent or lifetime look back windows, three types of contextual features (Tf-Idf, graph embedding, item-interaction), and four types of user behavior interactions (click, favorite, add-to-cart, purchase). Mixing and matching these together, we have a grand total of 24 contextual vectors to represent a single user in order to rank items for personalized search results. For example, we can combine an overall time window, item-interaction method, and “favorite” interactions to generate an item-interaction vector that represents a user’s all-time favorite listings. Search results for “necklace charms blue” (above) default results, (below) a user who recently interacted with eye charms In personalized search ranking, when a user enters a query we still do a coarse sweep of the inventory and grab top-related items to a query in candidate set retrieval. But in the ranking of items, we now include our new features. Recall that decision trees take input features in the form of integers or decimals. To satisfy this requirement, we can pass user historical features straight through to the tree or create new features by combining them with other features beforehand. To include user contextual features in the ranking, we have to compute similarity metrics between users’ contextual vectors and the candidate listing vectors from the candidate retrieval step. We derive Jaccard similarity and token overlap for sparse vectors and cosine similarity for dense vectors. From these metrics we understand which candidate listings are more similar to listings a user has previously interacted with. However, this metric alone is not sufficient to determine a final ranking. Decision trees take these inputs and learn how each feature impacts whether an item will be purchased. We feed user historical features, similarity measures, and other non-personalized features into the tree so it can learn to rank listings from most relevant to least. The expectation is that the most relevant listings are the items a user is more likely to purchase. Personalized Search Performance In online A/B experiments we compared this personalized model with a control and observed an improvement in ranking performance from a purchase’s normalized discounted cumulative gain (NDCG). NDCG captures the goodness of a ranking. If, on average, users purchase items ranked higher on the page, this ranking would have a high purchase NDCG. In our experiments, we observed that the NDCG for personalization was especially high for users that have recently and/or often interacted with the marketplace. Search results for “print maxi dress” (above) default results, (below) a user who recently interacted with African prints Users also click around less in personalized results, index to fewer pages, and buy more items compared to the control model. This indicates that users are finding what they want faster with the personalized variant. Overall, personalization features play an important role relative to the existing features for our decision tree. Generally speaking, the importance gain of a feature in a decision tree indicates how much a feature contributes to better predictions. In personalization, contextual features prove to be a strong influence in determining a good final ranking. For users with a richer history of interactions, we provide even better personalized results. This is confirmed by a greater number of purchases online and increased NDCG offline for users that have recently purchased items more than once. Vector representations from recent time windows had greater feature importance gain compared to lifetime vectors. This means that users’ recent interactions with listings give a better indication of what the user wants to purchase. Out of the three user contextual feature types, the text-based Tf-Idf vectors tend to have higher feature importance gain. This might suggest that ranking items based on seller-provided attributes given a query is the best way to help users find what they are looking for. We also identify users’ clicked and favorited items as more important signals compared to past cart adds or purchases. This might indicate that if a user purchased an item once, they have less utility for highly similar items later. Challenges & Considerations Mentioned in the beginning of the post, serving personalized results online for individual users introduces latency challenges . Typically we rank items in the second pass in real-time and rely on caching to save ordered rankings per query to reduce latency in online serving. But because personalized results are specific to individual users, we have to rank listings for every user and query pair. Cache utilization decreases due to the exploding amount of information we need to store in order to account for each personalized result, which can impact the user experience. Understanding further the appropriate situations to deploy personalized search results can improve a user’s experience. For example, when shopping for gifts for your grandmother, would we really want to tailor the results to your taste preferences? There are many ways to achieve this, such as learning the degree of personalization with respect to the query but there might be tradeoffs to latency and training time. For new users who haven’t interacted with many listings on our marketplace, we can have an alternative approach for populating default vector values. One possible method would be to set the default vector for new users as the average of all user vectors. However, it’s been shown that personalized results are better when a user’s individual preferences are very different from the group’s average preference. Personalization should take into account users’ privacy choices . This can take many forms, such as removing personalization information over time, providing user preferences as to personalization, anonymizing personalization information, or considering the sensitivity of information in how it is used for personalization. Conclusion In this post, we have covered how Etsy achieves personalized search ranking results. Our models learn which listings should rank higher for a user based on their own history as well as others’ history. These features are encapsulated with user historical features and contextual features. Since launching personalization, users have been able to find items they liked more easily, and they often come back for more. At Etsy, we’re focused on connecting our vibrant marketplace of 3.7 million sellers with shoppers around the world. With the introduction of personalized ranking, we hope to maintain course in our mission to keep commerce human. Posted by Lucia Yu on October 29, 2020 Category: search Tags: data science , machine learning , ranking algorithm Related Posts Posted by Xuan Yin on 03 Aug, 2020 How to Pick a Metric as the North Star for Algorithms to Optimize Business KPI?  A Causal Inference Approach Posted by Xuan Yin and Ercan Yildiz on 24 Feb, 2020 The Causal Analysis of Cannibalization in Online Products Posted by Aakash Sabharwal and Jingyuan Zhou on 02 Aug, 2019 Code as Craft: Understand the role of Style in e-commerce shopping", "date": "2020-10-29,"},
{"website": "Etsy", "title": "Improving our design system through Dark Mode", "author": ["Stephanie Sharp"], "link": "https://codeascraft.com/2020/10/21/improving-our-design-system-through-dark-mode/", "abstract": "Improving our design system through Dark Mode Posted by Stephanie Sharp on October 21, 2020 Etsy recently launched Dark Mode in our iOS and Android buyer apps. Since Dark Mode was introduced system-wide last year in iOS 13 and Android 10, it has quickly become a highly requested feature by our users and an industry standard. Benefits of Dark Mode include reduced eye strain, accessibility improvements, and increased battery life. For the Design Systems team at Etsy, it was the perfect opportunity to test the limits of the new design system in our apps. In 2019, we brought Etsy’s design system, Collage, to our iOS and Android apps. Around the same time, Apple announced Dark Mode in iOS 13. By implementing Dark Mode, the Design Systems team could not only give users the flexibility to customize the appearance of the Etsy app to match their preferences, but also test the new UI components app-wide and increase adoption of Collage in the process. Semantic colors Without semantic colors, Dark Mode wouldn’t have been possible. Semantic colors are colors that are named relative to their purpose in the UI (e.g. primary button color) instead of how they look (e.g. light orange color). Collage used a semantic naming convention for colors from the beginning. This made it relatively easy to support dynamic colors, which are named colors that can have different values when switching between light and dark modes. For example, a dynamic primary text color might be black in light mode and white in Dark Mode. Dynamic semantic colors opened up the possibility for Dark Mode, but they also led to a more accessible app for everyone. On iOS, we also added support for the Increased Contrast accessibility feature which increases the contrast between text and backgrounds to improve legibility. Any color in the Etsy app can now have up to four values for light/dark modes and regular/increased contrast. Color generation To streamline the process for adding new colors, we created a script on iOS that generates all of our color assets and files. With the growing complexity of dynamic colors, having a single source of truth for color definitions is important. On iOS, our source of truth is a property list (a key-value store for app data) of color names and values. We created a script that automatically runs when the app is built and generates all the files we need to represent colors: an asset catalog, convenience variables for accessing colors in code, and a source file for the unit tests. Adding a new color is as simple as adding a line to the property list, and all the relevant files are updated for you. This approach has reduced the time it takes to add a new color and eliminated the risk of inconsistencies across the codebase. On iOS, a script reads from a property list to generate the color asset catalog and convenience variables. Rethinking elevation Another design change we made for Dark Mode was rethinking how we represent elevation in our UI components. In light mode, it’s common to add a shadow around your view or dim the background to show that one view is layered above another. In Dark Mode, these approaches aren’t as effective and the platform convention is to slightly lighten the background of your view instead. The Etsy app uses shadows and borders extensively to indicate levels of elevation. For Dark Mode, we removed shadows entirely and used borders much more sparingly. Instead, we followed iOS and Android platform conventions and introduced elevated background colors into our design system. Semantic colors came to the rescue again and we were easily able to use our regular background color in light mode while applying a lighter color in Dark Mode on views that needed it, such as listing cards. Examples of elevated cards in light and dark modes Choose your theme There is no system-level Dark Mode setting available on older versions of Android, but it can be enabled for specific apps that support Themes (an Android feature that allows for UI customization and provides the underlying structure for Dark Mode). This limitation turned into an opportunity for us to provide more customization options for all our users. In both our iOS and Android apps you can personalize the appearance of the Etsy app to your preferences. So if you want to keep your phone in light mode but the Etsy app needs to be easy on the eyes for late night shopping, we’ve got you covered. Dark Mode in web views Another obstacle to overcome was our use of web views, a webpage that is displayed within a native app. Web views are used in a handful of places in our iOS and Android apps, and we knew that for a great user experience they needed to work seamlessly in Dark Mode as well. Thankfully, the web engineers on the Design Systems team jumped in to help and devised a solution to this problem. Using the Sass !default syntax for variables, we were able to define default color values for light mode. Then we added Dark Mode variable overrides where we defined our Dark Mode colors. If the webpage is being viewed from within the iOS or Android app with Dark Mode enabled, we load the Dark Mode variables first so the default (light mode) color variables aren’t used because they’ve already been defined for Dark Mode. This approach is easy to maintain and performant, avoiding a long list of style overrides for Dark Mode. A better design system Implementing Dark Mode was no small task. It took months of design and engineering effort from the Design Systems team, in collaboration with apps teams across the company. A big thank you to Patrick Montalto , Kate Kelly , Stephanie Sharp , Dennis Kramer , Gabe Kelley , Sam Sherman , Matthew Spencer , Netty Devonshire , Han Cho and Janice Chang . In the end, our users not only got the Dark Mode they’d been asking for, but we also developed a more robust and accessible design system in the process. Posted by Stephanie Sharp on October 21, 2020 Category: mobile Tags: android , Dark mode , Design systems , ios Related Posts Posted by Patrick Cousins on 12 Apr, 2018 Sealed classes opened my mind Posted by Deniz Veli on 13 Jan, 2014 Android Staggered Grid", "date": "2020-10-21,"},
{"website": "Etsy", "title": "Mutation Testing: A Tale of Two Suites", "author": ["Nathan Thompson"], "link": "https://codeascraft.com/2020/08/17/mutation-testing-a-tale-of-two-suites/", "abstract": "Mutation Testing: A Tale of Two Suites Posted by Nathan Thompson on August 17, 2020 In January of 2020 Etsy engineering embarked upon a highly anticipated initiative. For years our frontend engineers had been using a JavaScript test framework that was developed in house. It utilized Jasmine for assertions and syntax, PhantomJS for the test environment, and a custom, built-from-scratch test runner written in PHP. This setup no longer served our needs for a multitude of reasons: It was slow and memory intensive to run It was expensive to maintain, let alone enhance PhantomJS had been archived nearly two years before It was time to reach for an industry standard tool with a strong community and a feature list JavaScript developers have come to expect. We settled on Jest because it met all of those criteria and naturally complemented the areas of our site that are built with React. Within a few months we had all of the necessary groundwork in place to begin a large-scale effort of migrating our legacy tests. This raised an important question — would our test suite be as effective at catching regressions if it was run by Jest as opposed to our legacy test runner? At first this seemed like a simple question. Our legacy test said: expect(a).toEqual(b); And the migrated Jest test said: expect(a).toEqual(b); So weren’t they doing the same thing? Maybe. What if one checked shallow equality and the other checked deep equality? And what about assertions that had no corollaries in Jest? Our legacy suite relied on jasmine.ajax and jasmine-jquery , and we would need to propose alternatives for both modules when migrating our tests. All of this opened the door for subtle variations to creep in and make the difference between catching and missing a bug. We could have spent our time poring through the source code of Jasmine and Jest to figure out if these differences really existed, but instead we decided to use Mutation Testing to find them for us. What is Mutation Testing? Mutation Testing allows developers to score their test suite based on how many potential bugs it can catch. Since we were testing our JavaScript suite we reached for Stryker , which works roughly the same as any other Mutation Testing framework. Stryker analyzes our source code, makes any number of copies of it, then mutates those copies by programmatically inserting bugs into them. It then runs our unit tests against each “mutant” and sees if the suite fails or passes. If all tests pass, then the mutant has survived. If one or more tests fail, then the mutant has been killed. The more mutants that are killed, the more confidence we have that the suite will catch regressions in our code. After testing all of these potential mutations, Stryker generates a score by dividing the number of mutants that were killed by the total number generated: Output from running Stryker on a single file Stryker’s default reporter even displays how it generated the mutants that survived so it’s easy to identify the gaps in the suite. In this case, two Conditional Expression mutants and a Logical Operator mutant survived. All together, Stryker supports roughly thirty possible mutation types, but that list can be whittled down for faster test runs. The Experiment Since our hypothesis was that the implementation differences between Jasmine and Jest could affect the Mutation Score of our legacy and new test suites, we began by cataloging every bit of Jasmine-specific syntax in our legacy suite. We then compiled a list of roughly forty test files that we would target for Mutation Testing in order to cover the full syntax catalog. For each file we generated a Mutation Score for its legacy state, converted it to run in our new Jest setup, and generated a Mutation Score again. Our hope was that the new Jest framework would have a Mutation Score as good as or better than our legacy framework. By limiting the scope of our test to just a few dozen files, we were able to run all mutations Stryker had to offer within a reasonable timeframe. However, the sheer size of our codebase and the sprawling dependency trees in any given feature presented other challenges to this work. As I mentioned before, Stryker copies the source code to be mutated into separate sandbox directories. By default, it copies the entire project into each sandbox, but that was too much for Node.js to handle in our repository: Error when opening too many files Stryker allows users to configure an array of files to copy over instead of the entire codebase, but doing so would require us to know the full dependency tree of each file that we hoped to test ahead of time. Instead of figuring that out by hand, we wrote a custom Jest file resolver specifically for our Stryker testing environment. It would attempt to resolve source files from the local directory structure, but it wouldn’t fail immediately if they weren’t found. Instead, our new resolver would reach outside of the Stryker sandbox to find the file in the original directory structure, copy it into the sandbox, and re-initiate the resolution process. This method saved us time for files that had very expansive dependency trees. With that in hand, we pressed forth with our experiment. The Result Ultimately we found that our new Jest framework had a worse Mutation Score than our legacy framework. …Wait, What? It’s true. On average, tests run by our legacy framework received a 55.28% Mutation Score whereas tests run by our new Jest framework received a 54.35%. In one of our worst cases, the legacy test earned a 35% while the Jest test picked up a measly 16%. Analyzing The Result Once we began seeing lower Mutation Scores on a multitude of files, we put a hold on the migration to investigate what sort of mutants were slipping past our new suite. It turned out that most of what our new Jest suite failed to catch were String Literal mutations in our Asynchronous Module Definitions: Mutant generated by replacing a dependency definition with an empty string We dug into these failures further and discovered that the real culprit was how the different test runners compiled our code. Our legacy test runner was custom built to handle Etsy’s unique codebase and was tightly coupled to the rest of our infrastructure. When we kicked off tests it would locate all relevant source and test files, run them through our actual webpack build process, then load the resulting code into PhantomJS to execute. When webpack encountered empty strings in the dependency definition it would throw an error and halt the test, effectively catching the bug even if there were no tests that actually relied on that dependency. Jest, on the other hand, was able to bypass our build system using its file resolver and a handful of custom mappings and transformers. This was one of the big draws of the migration in the first place; decoupling the tests from the build process meant they could execute in a fraction of the time. However, the module we used in Jest to manage dependencies was much more lenient than our actual build system, and empty strings were simply ignored. This meant that unless a test actually relied on the dependency, our Jest setup had no way to alert the tester if it was accidentally left out. Ultimately we decided that this sort of bug was acceptable to let slide. While it would no longer be caught during the testing phase, the code would still be rejected by the build phase of our CI pipeline, thereby preventing the bug from reaching Production. As we proceeded with the migration we encountered a handful of other cases where the Mutation Scores were markedly different, one of which is particularly notable. We happened upon an asynchronous test that used a done() callback to signify when the test should exit. The test was malformed in that there were two done() callbacks with assertions between them. In Jest this was no big deal; it happily executed the additional assertions before ending the test. Jasmine was much more strict though. It stopped the test immediately when it encountered the first callback. As a result, we saw a significant jump in Mutation Score because mutants were suddenly being caught by the dangling assertions. This validated our suspicion that implementation differences between Jasmine and Jest could affect which bugs were caught and which slipped through. The Future of Mutation Testing at Etsy Over the course of this experiment we learned a ton about our testing frameworks and Mutation Testing in general. Stryker generated more than 3,800 mutations for the forty or so files that were tested, which equates to roughly ninety-five test runs per file. In all transparency, that number is likely to be artificially low as we ruled out some of the files we had initially identified for testing when we realized they generated many hundreds of mutations. If we assume our calculated average is indicative of all files and account for how long it takes to run our entire Jest suite, then we can estimate that a single-threaded, full Mutation Test of our entire JavaScript codebase would take about five and a half years to complete. Granted, Stryker parallelizes test runs out of the box, and we could potentially see even more performance gains using Jest’s findRelatedTests feature to narrow down which tests are run based on which file was mutated. Even so, it’s difficult to imagine running a full Mutation Test on any regular cadence. While it may not be feasible for Etsy to test every possible mutant in the codebase, we can still gain insights about our testing practices by applying Mutation Testing at a more granular level. A manageable approach would be to generate a Mutation Score automatically any time a pull request is opened, and focus the testing on only the files that changed. Posting that information on the pull request could help us understand what conditions will cause our unit tests to fail. It’s easy to write an overly-lenient test that will pass no matter what, and in some ways that’s more dangerous than having no test at all. If we only look at Code Coverage, such a test boosts our numbers giving us a false sense of security that bugs will be caught. Mutation Score forces us to confront the limitations of our suite and encourages us to test as effectively as possible. Posted by Nathan Thompson on August 17, 2020 Category: engineering , infrastructure Tags: javascript , jest , mutation score , mutation testing , stryker , testing , unit testing", "date": "2020-08-17,"},
{"website": "Etsy", "title": "How to Pick a Metric as the North Star for Algorithms to Optimize Business KPI?  A Causal Inference Approach", "author": ["Xuan Yin"], "link": "https://codeascraft.com/2020/08/03/causal-inference-to-pick-north-star-metric-for-algorithms-to-optimize-business-kpi/", "abstract": "How to Pick a Metric as the North Star for Algorithms to Optimize Business KPI?  A Causal Inference Approach Posted by Xuan Yin on August 3, 2020 This article draws on our published paper in KDD 2020 (Oral Presentation, Selection Rate: 5.8%, 44 out of 756) Introduction It is common in the internet industry to develop algorithms that power online products using historical data.  An algorithm that improves evaluation metrics from historical data will be tested against one that has been in production to assess the lift in key performance indicators (KPIs) of the business in online A/B tests.  We refer to metrics calculated using new predictions from an algorithm and historical ground truth as offline evaluation metrics.  In many cases, offline evaluation metrics are different from business KPIs.  For example, a ranking algorithm, which powers search pages on Etsy.com, typically optimizes for relevance by predicting purchase or click probabilities of items.  It could be tested offline (offline A/B tests) for rank-aware evaluation metrics, for example, normalized discounted cumulative gain (NDCG), mean reciprocal rank (MRR) or mean average precision (MAP), which are calculated using predicted ranks of ranking algorithms on the test set of historical purchase or click-through feedback of users.  Most e-commerce platforms, however, deem sitewide gross merchandise sale (GMS) as their business KPI and test for it online.  There could be various reasons not to directly optimize for business KPIs offline or use business KPIs as offline evaluation metrics, such as technical difficulty, business reputation, or user loyalty.  Nonetheless, the discrepancy between offline evaluation metrics and online business KPIs poses a challenge to product owners because it is not clear which offline evaluation metric, among all available ones, is the north star to guide the development of algorithms in order to optimize business KPIs. The challenge essentially asks for the causal effects of increasing offline evaluation metrics on business KPIs, for example how business KPIs would change for a 10% increase in an offline evaluation metric with all other conditions remaining the same ( ceteris paribus ).  The north star should be the offline evaluation metric that has the greatest causal effects on business KPIs.  Note that business KPIs are impacted by numerous factors, internal and external, especially macroeconomic situations and sociopolitical environments.  This means, just from changes in offline evaluation metrics, we have no way to predict future business KPIs.  Because we are only able to optimize for offline evaluation metrics to affect business KPIs, we try to infer the change in business KPIs, given all other factors, internal and external, unchanged, from changes in our offline evaluation metrics, based on historical data.  Our task here is causal inference rather than prediction. Our approach is to introduce online evaluation metrics, the online counterparts of offline evaluation metrics, which measure the performance of online products (see Figure 1).  This allows us to decompose the problem into two parts: the first part is the consistency between changes of offline and online evaluation metrics, the second part is the causality between online products (assessed by online evaluation metrics) and the business (assessed by online business KPIs).  The first part is solved by the offline A/B test literature through counterfactual estimators of offline evaluation metrics.  Our work focuses on the second part.  The north star should be the offline evaluation metric whose online counterpart has the greatest causal effects on business KPIs.  Hence, the question becomes how business KPIs would change for a 10% increase in an online evaluation metric ceteris paribus . Figure 1: The Causal Path from Algorithm Trained Offline to Online Business Note: Offline algorithms powers online products, and online products contribute to the business. Why Causality? Why do we focus on causality?  Before answering this question, let’s think about another interesting question: thirsty crow vs. talking parrot, which one is more intelligent (see Figure 2)? Figure 2. Thirsty Crow vs. Talking Parrot Note: The left painting is from The Aesop for Children , by Aesop, illustrated by Milo Winter , http://www.gutenberg.org/etext/19994 In Aesop’s Fables , a thirsty crow found a pitcher with water at the bottom.  The water is beyond the reach of its beak.  It intentionally dropped pebbles into the pitcher, which caused the water to rise to the top.  A talking parrot cannot really talk.  After being fed a simple phrase tons of times (big data and machine learning), it can only mimic the speech without understanding its meaning. The crow is obviously more intelligent than the parrot.  The crow understood the causality between dropped pebbles and rising water and thus leveraged the causality to get the water.  Beyond big data and machine learning (talking parrot), we want our artificial intelligence (AI) system to be as intelligent as the crow.  After understanding the causality between evaluation metric lift and GMS lift, our system can leverage the causality, by lifting the evaluation metric offline, to achieve GMS lift online (see Figure 3).  Understanding and leveraging causality are key topics in current AI research (see, e.g., Bergstein, 2020 ). Figure 3: Understanding and Leveraging Causality in Artificial Intelligence Causal Meta-Mediation Analysis Online A/B tests are popular to measure the causal effects of online product change on business KPIs.  Unfortunately, they cannot directly tell us the causal effects of increasing offline evaluation metrics on business KPIs.  In online A/B tests, in order to compare the business KPIs caused by different values of an online evaluation metric, we need to fix the metric at its different values for treatment and control groups.  Take the ranking algorithm as an example.  If we could fix online NDCG of the search page at 0.22 and 0.2 for treatment and control groups respectively, then we would know how sitewide GMS would change for a 10% increase in online NDCG at 0.2 ceteris paribus .  However, this experimental design is impossible, because most online evaluation metrics depend on users’ feedback and thus cannot be directly manipulated. We address the question by developing a novel approach: causal meta-mediation analysis (CMMA).  We model the causality between online evaluation metrics and business KPIs by dose-response function (DRF) in potential outcome framework.  DRF originates from medicine and describes the magnitude of the response of an organism given different doses of a stimulus.  Here we use it to depict the value of a business KPI given different values of an online evaluation metric.  Different from doses of stimuli, values of online evaluation metrics cannot be directly manipulated.  However, they could differ between treatment and control groups in experiments of treatments other than algorithms: user interface/user experience (UI/UX) design, marketing, and etc.  This could be due to the “fat hand” nature of online A/B tests that a single intervention can change many causal variables at once.  A change of the tested feature, which is not an algorithm, could induce users to change their engagement with algorithm-powered online products, so that values of online evaluation metrics would change.  For instance, in an experiment of UI design, users might change their search behaviors because of the new UI design, so that values of online NDCG, which depend on search interaction, would change even though the ranking algorithm does not change (see Figure 4).  The evidence suggests that online evaluation metrics could be mediators that partially transmit causal effects of treatments on business KPIs in experiments where treatments are not necessarily algorithm-related.  Hence, we formalize the problem as the identification, estimation, and testing of mediator DRF. Figure 4: Directed Acyclic Graph of Conceptual Framework Our novel approach CMMA combines mediation analysis and meta-analysis to solve for mediator DRF.  It relaxes common assumptions in causal mediation literature: sequential ignorability (in linear structural equation model) or complete mediation (in instrumental variable approach) and extends meta-analysis to solve causal mediation while the meta-analysis literature only learns the distribution of average treatment effects.  We did extensive simulations, which show CMMA’s performance is superior to other methods in the literature in terms of unbiasedness and the coverage of confidence intervals.  CMMA uses only experiment-level summary statistics (i.e., meta-data) of many existing experiments, which makes it easy to implement and to scale up.  It can be applied to all experimental-unit-level evaluation metrics or any combination of them.  Because it solves the causality problem of a product by leveraging experiments of all products, CMMA could be particularly useful in real applications for a new product that has been shipped online but has few A/B tests. Application We apply CMMA on the three most popular rank-aware evaluation metrics: NDCG, MRR, and MAP, to show, for ranking algorithms that power search products, which one has the greatest causal effect on sitewide GMS. User-Level Rank-Aware Metrics We redefine the three rank-aware metrics (NDCG, MAP, MRR) at the user level.  The three metrics are originally defined at the query level in the test collection evaluation of information retrieval (IR) literature.  Because the search engine on Etsy.com is an online product for users, the computation could be adapted to the user level.  We include search sessions of no interaction or no feedback into the metric calculation in accordance with online business KPI calculation in online A/B tests that always includes visits/users of no interaction or no feedback.  Specifically, the three metrics are constructed as follows: Query-level metrics are computed using rank positions on the search page and user conversion status (binary relevance).  Queries of non-conversion have zero values. User-level metric is the average of query-level metrics across all queries the user issues (including non-conversion associated queries).  Users who do not search or convert have zero values. All three metrics are defined at rank position 48, the lowest position of the first page of search results in Etsy.com. Demo To demonstrate CMMA, we randomly selected 190 experiments from 2018 and implemented CMMA on summary results of each experiment (e.g., the average user-level NDCG per user in treatment and control groups).  Figure 5 shows results from CMMA.  The vertical axis indicates elasticity, the percentage change of average GMS per user for a 10% increase in an online rank-aware metric with all other conditions that can affect GMS remaining the same.  Lifts in all the three rank-aware metrics have positive causal effects on the average GMS per user.  These don’t have the same performance; different values of different rank-aware metrics have different elasticities.  For example, suppose current values of NDCG, MAP, and MRR of search page are 0.00210, 0.00156, and 0.00153 respectively, then a 10% increase in MRR will cause higher lifts in GMS than 10% increases in the other two metrics ceteris paribus , which is indicated by red dashed lines, and thus MRR should be the north star to guide our development of algorithms.  Because all the three metrics have the same input data, they are highly correlated and thus the differences are small.  As new IR evaluation metrics are continuously developed in the literature, we will implement CMMA for more comparison in the future. Figure 5: Elasticity from Average Mediator DRF Estimated by CMMA Note: Elasticity means the percentage change of average GMS per user for a 10% increase in an online rank-aware metric with all other conditions that can affect GMS remaining the same. Conclusion We implement CMMA to identify the north star of rank-aware metrics for search-ranking algorithms.  It is easy to implement and to scale up.  It has helped product and engineering teams to achieve efficiency and success in terms of business KPIs in online A/B tests.  We published CMMA on KDD 2020 .  We also made a fun video describing the intuition behind CMMA.  Interested readers can refer to our paper for more details and our GitHub repo for the analysis code. Posted by Xuan Yin on August 3, 2020 Category: data , engineering , search Tags: ab test , business KPI , causal inference , data science , experimentation , mediation analysis , meta-analysis , offline evaluation metric , online evaluation metric , ranking algorithm , search Related Posts Posted by Xuan Yin and Ercan Yildiz on 24 Feb, 2020 The Causal Analysis of Cannibalization in Online Products", "date": "2020-08-3,"},
{"website": "Etsy", "title": "Chaining iOS Machine Learning, Computer Vision, and Augmented Reality to Make the Magical Real", "author": ["Jacob Van Order"], "link": "https://codeascraft.com/2020/06/23/chaining-ios-machine-learning-computer-vision-and-augmented-reality-to-make-the-magical-real/", "abstract": "Chaining iOS Machine Learning, Computer Vision, and Augmented Reality to Make the Magical Real Posted by Jacob Van Order on June 23, 2020 Etsy recently released a feature in our buyer-facing iOS app that allows users to visualize wall art within their environments. Getting the context of a personal piece of art within your space can be a meaningful way to determine whether the artwork will look just as good in your room as it does on your screen. The new feature uses augmented reality to bridge that gap, meshing the virtual and real worlds. Read on to learn how we made this possible using aspects of machine learning and computer vision to present the best version of Etsy sellers’ artwork in augmented reality. It didn’t even require a PhD.-level education or an expensive 3rd party vendor – we did it all with tools provided by iOS. Building a Chain Using Computers to See Early in 2019, I put together a quick proof of concept that allowed for wall art to be displayed on a vertical plane, which required a standalone image of the artwork filling the entire image. Oftentimes, though, Etsy sellers upload images that show their item in context, like on a living room wall, to show scale. This complicates the process because these listing images can’t be placed onto vertical planes in augmented reality as-is; they need to be reformatted and cropped. Two engineers, Chris Morris and Jake Kirshner developed a solution that used computer vision to find a rectangle within an image, perhaps a frame, and crop the image for use.  Using the Vision framework in iOS, they were able to pull out the artwork we needed to place in 3D space.  We found that trying to detect only one rectangle, as opposed to all, created performance wins and gave us the shape with greatest confidence by the system. Afterwards, we used Core Image in order to crop the image, adjusting for any perspective skew that might be present. Apple has an example using a frame buffer but can be applied to any UIImage. To Crop or Not to Crop As I mentioned before, some Etsy sellers upload their artwork as standalone images, while others depict their artwork in different environments. We wanted to present the former as-is, and we needed to crop the latter, but we had no way to automatically categorize the more than 5 million artwork listings available on our marketplace. To solve this, we used on-device machine learning provided by Core ML . The team sifted through more than 1,200 listings and sorted the images by those that should be cropped and those that should not be cropped. To create the machine learning model, we first used an iOS Playground and, later, a Mac application called Create ML . The process was as easy as dropping a directory with two subdirectories filled with correct images, “no_frames” and “frames”, into the application along with a corresponding smaller set of different images used to test the resulting model. Once this model was created and verified, we used VNCoreMLRequest to check a listing’s image and determine whether we should crop it or present it as-is. This type of model is known as image classification . We also investigated a different type of mode called object detection , which finds the existence and coordinates of a frame within an image. This technique had two downsides: training the model required laborious manual object marking for each image provided, and the resulting model, which would be included in our app bundle, would be well over 60mb vs. the 15 kb model for image classification. That’s right, kilobytes. Translating Two Dimensions to Three Once we had the process for determining whether the image needs to be reformatted,  we used a combination of iOS’ SceneKit and ARKit to place the artwork as a material on a rudimentary shape . With Apple focusing heavily on this space, we were able to find plenty of great examples and tutorials to get us started with augmented reality on iOS. We started with the easy-to-use RealityKit framework, but the iOS 13-only restriction was a blocker as we supported back to iOS 11 at the time. The implementation in ARKit was relatively straightforward, technically, but working for the first time in 3D space vs. a flat screen, it was a challenge to develop a vocabulary and way of thinking about the physical space being altered by the virtual. It was difficult putting into words the difference between, for example,  moving on a y-axis and how that differed from making the item scale in size. While this was eventually smoothed out with experience, we knew we had to keep this in mind for Etsy buyers, as augmented reality is not a common experience for most people. For example, how would we coach them through the fact that ARKit needs them to use the camera to scan the room to find the edges of the wall in order to discern the vertical plane? What makes it apparent that they can tap on screen? In order to give our users an inclination of how to use this feature successfully, our designer, Kate Matsumoto , product manager, Han Cho , and copywriter, Jed Baker , designed an onboarding flow, based on user-testing, that walks our buyers through this new experience. Wrapping it All Up Using machine learning to determine if we should crop an image or not, cropping it based on a strong rectangle, and presenting the artwork on a real wall was only part of the picture here. Assisted by Evan Wolf and Nook Harquail , we also dealt with complex problems including parsing item descriptions to gather dimension, raytraced hit-testing, and color averaging to make this feature feel as seamless and lifelike as possible for Etsy buyers. From here, we have plenty of ideas for continuing to improve this experience but in the meantime, I encourage you to consider the fantastic frameworks you have at your disposal, and how you can link them together to create an experience that seemed impossible just years ago. Referenced Frameworks: Vision Core Image Core ML Create ML SceneKit ARKit RealityKit Posted by Jacob Van Order on June 23, 2020 Category: mobile Tags: ARKit , augmented reality , computer vision , CoreML , ios , machine learning", "date": "2020-06-23,"},
{"website": "Etsy", "title": "Keeping IT Support Human during WFH", "author": ["Seth Liber"], "link": "https://codeascraft.com/2020/05/06/keeping-it-support-human-during-wfh/", "abstract": "Keeping IT Support Human during WFH Posted by Seth Liber on May 6, 2020 Image: Human Connection, KatieWillDesign on Etsy Hi! We’re the Etsy Engineering team that supports core IT and AV capabilities for all Etsy employees. Working across geographies has always been part of our company’s DNA; our globally distributed teams use collaboration tools like Google apps, Slack, and video conferencing. As we transitioned to a fully distributed model during COVID-19, we faced both unexpected challenges and opportunities to permanently improve our support infrastructure. In this post we will share some of the actions we took to support our staff as they spread out across the globe. Digging deeper on our core values Keeping Support Human Our team’s core objective is to empower Etsy employees to do their best work. We give them the tools they need and we teach, train, and support them to use those tools as best they can. We also document and share our work in the form of user guides and support run-books. With friendly interactions during support, we strive to embody Etsy’s mission to Keep Commerce Human ® . Despite being further physically distributed, we found ways to increase human connections. We launched daily tips that are delivered in Slack , inspired by ideas from teams across Etsy, and we prioritized items that were most relevant to helping folks navigate their new work setups. In addition to our existing monitored #helpdesk Slack channel, we hosted live, virtual helpdesk hours on video. We added downloadable self-service software in our internal IT tools that reduced the load on our front-line helpdesk team, and enabled employees to quickly resolve some common issues with easily digestible one-page guides. Staying Connected Before Etsy went fully remote, a common meeting setup included teams in multiple office locations connecting through video-conference-enabled conference rooms with additional remote participants dialing in. To better support the volume of video calls we needed with all employees WFH, we accelerated our planned video conferencing solution migration to Google Meet. We also quickly engineered solutions to integrate Google Meet, including making video conference details the default option in calendar invites and enabling add-ons that improve the video call experience. Within a month we had a 1000% increase in Google Meet usage and a ~60% drop off of the old platform. We also adapted our large team events, such as department-wide all hands meetings, to support a full remote experience. We created new “ask me anything” formats and shortened some meetings’ length. To make the meetings run smoothly, we added additional behind-the-scenes prep for speakers, gathered Q&A in advance, and created user guides so teams can self-manage complex events. Continuing Project Progress We reviewed our committed and potential project list and decided where we could prioritize focus, adapting to the new needs of our employees. Standing up additional monitoring tools allowed us to be even more proactive about the health of our end-point fleet.  We also seized opportunities to take advantage of our empty offices to do work that would have otherwise been disruptive. We were able to complete firewall and AV equipment firmware upgrades (remotely, of course) in a fraction of the time it would have taken us with full offices. In summary, some learnings Collaboration is key Our team is very fortunate to have strong partners, buoyed by supportive leadership, operating in an inclusive company culture that highly values innovation. Much of our success in this highly unique situation is a result of multiple departments coming together quickly, sharing information as they received it, and being flexible during rapid change. For example, we partnered with our recruiting, human resources, and people development teams to adjust how we would onboard new employees, contractors, and short term temporary employees, ensuring we properly deployed equipment and smoothly introduced them to Etsy. Respect the diversity of WFH situations We’ve dug deeper into ways to help all our employees work effectively at home. We’re constantly learning, but we continue to build a robust “how to work from home” guide and encourage transparency around each employee’s challenges so that we can help find solutions. Home networks can be a major point of friction and we’ve built out guides to help our employees optimize their network and Wi-Fi setups. Empathy for each other Perhaps most of all, through this experience we’ve gained an increased level of empathy for our peers. We’ve learned that there are big differences between working from home for one day, being a full-time remote employee, and working in isolation during a global crisis. We’re using this awareness to rethink the future of our meeting behaviors, the technology in our conference rooms, and the way we engage with each other throughout the day, whether we’re in or out of the office. Posted by Seth Liber on May 6, 2020 Category: engineering , events , people , philosophy , support", "date": "2020-05-6,"},
{"website": "Etsy", "title": "The journey to fast production asset builds with Webpack", "author": ["Jonathan Lai"], "link": "https://codeascraft.com/2020/02/03/production-webpack-builds/", "abstract": "The journey to fast production asset builds with Webpack Posted by Jonathan Lai on February 3, 2020 Etsy has switched from using a RequireJS-based JavaScript build system to using Webpack . This has been a crucial cornerstone in the process of modernizing Etsy’s JavaScript ecosystem. We’ve learned a lot from addressing our multiple use-cases during the migration, and this post is the first of two parts documenting our learnings. Here, we specifically cover the production use-case — how we set up Webpack to build production assets in a way that meets our needs. The second post can be found here . We’re proud to say that our Webpack-powered build system, responsible for over 13,200 assets and their source maps, finishes in four minutes on average. This fast build time is the result of countless hours of optimizing. What follows is our journey to achieving such speed, and what we’ve discovered along the way. Production Expectations One of the biggest challenges of migrating to Webpack was achieving production parity with our pre-existing JavaScript build system, named Builda. It was built on top of the RequireJS optimizer , a build tool predating Webpack, with extensive customization to speed up builds and support then-nascent standards like JSX. Supporting Builda became more and more untenable, though, with each custom patch we added to support JavaScript’s evolving standards. By early 2018, we consequently decided to switch to Webpack; its community support and tooling offered a sustainable means to keep up with JavaScript modernization. However, we had spent many years optimizing Builda to accommodate our production needs. Assets built by Webpack would need to have 100% functional parity with assets built by Builda in order for us to have confidence around switching. Our primary expectation for any new build system is that it takes less than five minutes on average to build our production assets. Etsy is one of the earliest and biggest proponents of continuous deployment, where our fast build/deploy pipeline allows us to ship changes up to 30 times a day. Builda was already meeting this expectation, and we would negatively impact our deployment speed/frequency if a new system took longer to build our JavaScript. Build times, however, tend to increase as a codebase grows, productionization becomes more complex, and available computing resources are maxed out. At Etsy, our frontend consists of over 12,000 modules that eventually get bundled into over 1,200 static assets. Each asset needs to be localized and minified, both of which are time-consuming tasks. Furthermore, our production asset builds were limited to using 32 CPU cores and 64GB of RAM. Etsy had not yet moved to the cloud when we started migrating to Webpack, and these specs were of the beefiest on-premise hosts available. This meant we couldn’t just add more CPU/RAM to achieve faster builds. So, to recap: Our frontend consists of over 1,200 assets made up of over 12,000 modules. Each asset needs to be localized and minified as part of productionization. We are limited to 32 CPU cores and 64GB of RAM. Production asset builds need to finish in less than five minutes on average. We got this. Localization From the start, we knew that localization would be a major obstacle to achieving sub-five-minute build times. Localization strings are embedded in our JavaScript assets, and at Etsy we officially support eleven locales. This means we need to produce eleven copies of each asset, where each copy contains localization strings of a specific locale. Suddenly, building over 1,200 assets balloons into building over 1,200 × 11 = 13,200 assets. General caching solutions help reduce build times, idempotent of localization’s multiplicative factor. After we solved the essential problems of resolving our module dependencies and loading our custom code with Webpack, we incorporated community solutions like cache-loader and babel-loader ’s caching options. These solutions cache intermediary artifacts of the build process, which can be time-consuming to calculate. As a result, asset builds after the initial one finish much faster. Still though, we needed more than caching to build localized assets quickly. One of the first search results for Webpack localization was the now-deprecated i18n-webpack-plugin . It expects a separate Webpack configuration for each locale, leading to a separate production asset build per locale. Even though Webpack supports multiple configurations via its MultiCompiler mode , the documentation crucially points out that “each configuration is only processed after the previous one has finished processing.” At this stage in our process, we measured that a single production asset build without minification was taking ~3.75 minutes with no change to modules and a hot cache (a no-op build). It would take us ~3.75 × 11 = ~41.25 minutes to process all localized configurations for a no-op build. We also ruled out using this plugin with a common solution like parallel-webpack to process configurations in parallel. Each parallel production asset build requires additional CPU and RAM, and the sum far exceeded the 32 CPU cores and 64GB of RAM available. Even when we limited the parallelism to stay under our resource limits, we were met with overall build times of ~15 minutes for a no-op build. It was clear we need to approach localization differently. Localization inlining To localize our assets, we took advantage of two characteristics about our localization. First, the way we localize our JavaScript code is through a module abstraction. An engineer defines a module that contains only key-value pairs. The value is the US-English version of the text that needs to be localized, and the key is a succinct description of the text. To use the localized strings, the engineer imports the module in their source code. They then have access to a function that, when passed a string corresponding to one of the keys, returns the localized value of the text. example of how we include localizations in our JavaScript For a different locale, the message catalog contains analogous localization strings for the locale. We programmatically handle generating analogous message catalogs with a custom Webpack loader that applies whenever Webpack encounters an import for localizations. If we wanted to build Spanish assets, for example, the loader would look something like this: example of how we would load Spanish localizations into our assets Second, once we build the localized code and output localized assets, the only differing lines between copies of the same asset from different locales are the lines with localization strings; the rest are identical. When we build the above example with English and Spanish localizations, the diff of the resulting assets confirms this: diff of the localized copies of an asset Even when caching intermediary artifacts, our Webpack configuration would spend over 50% of the overall build time constructing the bundled code of an asset. If we provided separate Webpack configurations for each locale, we would repeat this expensive asset construction process eleven times. diagram of running Webpack for each locale We could never finish this amount of work within our build-time constraints, and as we saw before, the resulting localized variants of each asset would be identical except for the few lines with localizations. What if, rather than locking ourselves into loading a specific locale’s localization and repeating an asset build for each locale, we returned a placeholder where the localizations should go? code to load placeholders in place of localizations We tried this placeholder loader approach, and as long as it returned syntactically valid JavaScript, Webpack could continue with no issue and generate assets containing these placeholders, which we call “sentinel assets”. Later on in the build process a custom plugin takes each sentinel asset, finds the placeholders, and replaces them with corresponding message catalogs to generate a localized asset. diagram of our build process with localization inlining We call this approach “localization inlining”, and it was actually how Builda localized its assets too. Although our production asset builds write these sentinel assets to disk, we do not serve them to users. They are only used to derive the localized assets. With localization inlining, we were able to generate all of our localized assets from one production asset build. This allowed us to stay within our resource limits; most of Webpack’s CPU and RAM usage is tied to calculating and generating assets from the modules it has loaded. Adding additional files to be written to disk does not increase resource utilization as much as running an additional production asset build does. Now that a single production asset build was responsible for over 13,200 assets, though, we noticed that simply writing this many assets to disk substantially increased build times. It turns out, Webpack only uses a single thread to write a build’s assets to disk. To address this bottleneck, we included logic to write a new localized asset only if the localizations or the sentinel asset have changed — if neither have changed, then the localized asset hasn’t changed either. This optimization greatly reduced the amount of disk writing after the initial production asset build, allowing subsequent builds with a hot cache to finish up to 1.35 minutes faster. A no-op build without minification consistently finished in ~2.4 minutes. With a comprehensive solution for localization in place, we then focused on adding minification. Minification Out of the box, Webpack includes the terser-webpack-plugin for asset minification . Initially, this plugin seemed to perfectly address our needs. It offered the ability to parallelize minification, cache minified results to speed up subsequent builds, and even extract license comments into a separate file. When we added this plugin to our Webpack configuration, though, our initial asset build suddenly took over 40 minutes and used up to 57GB of RAM at its peak. We expected the initial build to take longer than subsequent builds and that minification would be costly, but this was alarming. Enabling any form of production source maps also dramatically increased the initial build time by a significant amount. Without the terser-webpack-plugin, the initial production asset build with localizations would finish in ~6 minutes. It seemed like the plugin was adding an unknown bottleneck to our builds, and ad hoc monitoring with htop during the initial production asset build seemed to confirmed our suspicions: htop during minification At some points during the minification phase, we appeared to only use a single CPU core. This was surprising to us because we had enabled parallelization in terser-webpack-plugin’s options. To get a better understanding of what was happening, we tried running strace on the main thread to profile the minification phase: strace during minification At the start of minification, the main thread spent a lot of time making memory syscalls (mmap and munmap). Upon closer inspection of terser-webpack-plugin’s source code, we found that the main thread needed to load the contents of every asset to generate parallelizable minification jobs for its worker threads. If source maps were enabled, the main thread also needed to calculate each asset’s corresponding source map . These lines explained the flood of memory syscalls we noticed at the start. Further into minification, the main thread started making recvmsg and write syscalls to communicate between threads. We corroborated these syscalls when we found that the main thread needed to serialize the contents of each asset (and source maps if they were enabled) to send it to a worker thread to be minified. After receiving and deserializing a minification result received from a worker thread, the main thread was also solely responsible for caching the result to disk . This explained the stat, open, and other write syscalls we observed because the Node.js code promises to write the contents. The underlying epoll_wait syscalls then poll to check when the writing finishes so that the promise can be resolved. The main thread can become a bottleneck when it has to perform these tasks for a lot of assets, and considering our production asset build could produce over 13,200 assets, it was no wonder we hit this bottleneck. To minify our assets, we would have to think of a different way. Post-processing We opted to minify our production assets outside of Webpack, in what we call “post-processing”. We split our production asset build into two stages, a Webpack stage and a post-processing stage. The former is responsible for generating and writing localized assets to disk, and the latter is responsible for performing additional processing on these assets, like minification: running Webpack with a post-processing stage diagram of our build process with localization inlining and post-processing For minification, we use the same terser library the terser-webpack-plugin uses. We also baked parallelization and caching into the post-processing stage, albeit in a different way than the plugin. Where Webpack’s plugin reads the file contents on the main thread and sends the whole contents to the worker threads, our parallel-processing jobs send just the file path to the workers. A worker is then responsible for reading the file, minifying it, and writing it to disk. This reduces memory usage and facilitates more efficient parallel-processing. To implement caching, the Webpack stage passes along the list of assets written by the current build to tell the post-processing stage which files are new. Sentinel assets are also excluded from post-processing because they aren’t served to users. Splitting our production asset builds into two stages does have a potential downside: our Webpack configuration is now expected to output un-minified text for assets. Consequently, we need to audit any third-party plugins to ensure they do not transform the outputted assets in a format that breaks post-processing. Nevertheless, post-processing is well worth it because it allows us to achieve the fast build times we expect for production asset builds. Bonus: source maps We don’t just generate assets in under five minutes on average — we also generate corresponding source maps for all of our assets too. Source maps allow engineers to reconstruct the original source code that went into an asset. They do so by maintaining a mapping of the output lines of a transformation, like minification or Webpack bundling, to the input lines. Maintaining this mapping during the transformation process, though, inherently adds time. Coincidentally, the same localization characteristics that enable localization inlining also enable faster source map generation. As we saw earlier, the only differences between localized assets are the lines containing localization strings. Subsequently, these lines with localization strings are the only differences between the source maps for these localized assets. For the rest of the lines, the source map for one localized asset is equivalently accurate for another because each line is at the same line number between localized assets. If we were to generate source maps for each localized asset, we would end up repeating resource-intensive work only to result in nearly identical source maps across locales. Instead, we only generate source maps for the sentinel assets the localized assets are derived from. We then use the sentinel asset’s source map for each localized asset derived from it, and accept that the mapping for the lines with localization strings will be incorrect. This greatly speeds up source map generation because we are able to reuse a single source map that applies to many assets. For the minification transformation that occurs during post-processing, terser accepts a source map alongside the input to be minified. This input source map allows terser to account for prior transformations when generating source mappings for its minification. As a result, the source map for its minified results still maps back to the original source code before Webpack bundling. In our case, we pass terser the sentinel asset’s source map for each localized asset derived from it. This is only possible because we aren’t using terser-webpack-plugin, which (understandably) doesn’t allow mismatched asset/source map pairings. diagram of our complete build process with localization inlining, post-processing, and source map optimizations Through these source map optimizations, we are able to maintain source maps for all assets while only adding ~1.7 minutes to our build time average. Our unique approach can result in up to a 70% speedup in source map generation compared to out-of-the-box options offered by Webpack, a dramatic reduction in the time. Conclusion Our journey to achieving fast production builds can be summed up into three principles: reduce, reuse, recycle. Reduce Reduce the workload on Webpack’s single thread. This goes beyond applying parallelization plugins and implementing better caching. Investigating our builds led us to discover single-threaded bottlenecks like minification, and after implementing our own parallelized post-processing we observed significantly faster build times. Reuse The more existing work our production build can reuse, the less it has to do. Thanks to the convenient circumstances of our production setup, we are able to reuse source maps and apply them to more than one asset each. This avoids a significant amount of unnecessary work when generating source maps, a time-intensive process. Recycle When we can’t reuse existing work, figuring out how to recycle it is equally valuable. Deriving localized assets from sentinel assets allows us to recycle the expensive work of producing an asset from an entrypoint, further speeding up builds. While some implementation details may become obsolete as Webpack and the frontend evolve, these principles will continue to guide us towards faster production builds. This post is the first in a two-part series on our migration to a modern JavaScript build system. The second part can be found here . Posted by Jonathan Lai on February 3, 2020 Category: engineering , infrastructure Tags: frontend , webpack", "date": "2020-02-3,"},
{"website": "Etsy", "title": "Developing in a Monorepo While Still Using Webpack", "author": ["Salem Hilal"], "link": "https://codeascraft.com/2020/04/06/developing-in-a-monorepo-while-still-using-webpack/", "abstract": "Developing in a Monorepo While Still Using Webpack Posted by Salem Hilal on April 6, 2020 When I talk to friends and relatives about what I do at Etsy, I have to come up with an analogy about what Frontend Infrastructure is. It’s a bit tricky to describe because it’s something that you don’t see as an end user; the web pages that people interact with are several steps removed from the actual work that a frontend infrastructure engineer does. The analogy that I usually fall to is that of a restaurant: the meal is a fully formed web page, the chefs are product engineers, and the kitchen is the infrastructure. A good kitchen should make it easy to cook a bunch of different meals quickly and deliciously. Recently, my team and I spent over a year swapping out our home-grown, Require-js-based JavaScript build system for Webpack . Running with this analogy a bit, this project is like trading out our kitchen without customers noticing, and without bothering the chefs too much.  Large projects tend to be full of unique problems and unexpected hurdles, and this one was no exception. This post is the second in a short series on all the things that we learned during the migration, and is adapted in part from a talk I gave at JSConf 2019. The first post can be found here . The state of JavaScript at Etsy last year. At Etsy, we have a whole lot of JavaScript. This alone doesn’t make us very unique, but we have something that not every other company has: a monorepo . When we deploy our web code, we need to build and deploy over 1200 different JavaScript assets made up from over twelve thousand different JavaScript files or modules. Like the rest of the industry, we find ourselves relying more and more on JavaScript, which means that a good bit more of our code base ends in “.js” this year than last. When starting to adopt Webpack, one of the first places we saw an early win was in our development experience. Up to and until this point, our engineers had been using a development server that we had written in-house. We ran a copy of it on every developer machine, where it built files as they were requested. This approach meant that you could reliably navigate around Etsy.com in development without needing to think about a build system at all. It also meant that we could start and restart an instance of the development server without worrying about losing state or interrupting developers much. Conceptually, this made things very simple to maintain. You truly couldn’t have asked for a simpler diagram. In practice, however, developers were asking for more from JavaScript and from their build systems. We started adopting React a few years prior using the then-available JSXTransform tool, which we added to our build system with a fair amount of wailing and gnashing of teeth. The result was a server that successfully, yet sluggishly, supported JSX. Because it wasn’t designed with large applications in mind, our development server didn’t do things like cache transpiled JSX between builds. Building some of our weightier JavaScript code often took the better part of a minute, and most of our developers grew increasingly frustrated with the long iteration cycles it produced. Worse yet, because we were using JSXTransform, rather than something like Babel, our developers could use JSX but weren’t able to use any ES6 syntax like arrow functions or classes. Bending Webpack to our will. Clearly, there was a lot with our development environment that could be improved. To be worth the effort of adopting, any new build system we adopted would at least have to support the ability to transpile syntaxes like JSX, while still allowing for fast rebuild times for developers. Webpack seemed like a pretty safe bet — it was widely adopted ; it was actively developed and funded ; and everyone who had experience with it seemed to like it (in spite of its intimidating configuration). So, we spent a good bit of time configuring Webpack to work with our codebase (and vice versa). This involved writing some custom loaders for things like templates and translations, and it meant updating some of the older parts of our codebase that relied on the specifics of Require.js to work properly. After a lot of planning, testing, and editing, we were able to get Webpack to fully build our entire codebase. It took half an hour, and that was only when it didn’t fill all 16 gigabytes of our development server’s memory. Clearly, we had a lot more work on our plates. This is one of our beefiest machines maxing out all 32 of its processors and eating up over 20 gigs of memory trying to run Webpack once. When Webpack typically runs in development mode, it behaves much differently than our old development server did. It starts by compiling all your code as it would for a production build, leaving out optimizations that don’t make sense in development (like minification and compression). It then switches to “watch mode”, where it listens to your source files for changes and kicks off partial recompilations when any of your source code updates. This keeps it from starting from scratch every time an asset updates, and watching the filesystem lets builds start a few seconds before the assets are requested by the browser. Webpack is very effective at partial rebuilds, which is how it’s able to remain fast and effective, even for larger projects. …and maybe bending our will to Webpack’s. Although Webpack was designed for large projects, it wasn’t designed for a whole company’s worth of large projects. Our monorepo contains JavaScript code from every part of Etsy. Making Webpack try to build everything at once was a fool’s errand, even after playing with plugins like HardSource , CacheLoader , and HappyPack to either speed up the build time or reduce its resource footprint. We ended up admitting to ourselves that building everything at once was impossible. If your solution to a problem just barely works today, it’s not going to be very useful when your problem doubles in size in a few years’ time. A pretty straightforward next step would be to split up our codebase into logical regions and make a webpack config for each one, rather than using one big config to build everything. Splitting things up would allow each individual build to be reasonably sized, cutting back on both build times and resource utilization. Plus, production builds wouldn’t need to change much, since Webpack is perfectly happy accepting either a single configuration or an array of them There was one problem with this approach though: if we only built one slice of the site at a time, we wouldn’t be able to allow developers to easily browse around Etsy.com in development unless they manually started and stopped multiple instances of Webpack. There are a lot of features in Etsy that touch multiple parts of the site; adding a change to how a listing might appear could mean a change for our search page, the seller dashboard, and our internal tools as well. We needed a solution that would both allow us to only build parts of the site that made sense, while maintaining the “it just works!” behavior of our old system. So, we wrote something we’re calling Kevin. This is Kevin. Kevin (technically “kevin-middleware”) is an express-style middleware that manages multiple instances of Webpack for you. Its job is to make it easier to build a monorepo’s worth of JavaScript while maintaining the resource footprint of something much smaller. It was both inspired by and meant as a replacement to webpack-dev-middleware , which is what Webpack’s own development server uses to manage a single instance of Webpack under the hood. If you happen to be using that, Kevin will probably feel a bit familiar. Kevin works by reading in a list of Webpack configurations and determining all of the assets that each one could be responsible for. It then listens for requests for those assets, determines the config that is responsible for that asset, and then starts an instance of Webpack with that config. It’ll keep a few instances around in memory based on a simple frecency algorithm, and will monitor your source files in order to eagerly rebuild any changes. When there are more instances than a configured limit, the least used compiler is shut down and cleaned up. While otherwise being a lot cooler in every respect, Kevin has an objectively more complicated diagram. Webpack’s first build often takes a while. Like I mentioned before, it has to do a first pass of all the assets it needs to build before it’s able to do fast, iterative rebuilds. If a developer requests an asset from a config that isn’t being built by an active compiler, that request might time out before a fresh compiler finishes its first build. Kevin tries to offset this problem by serving some static code that renders an overlay whenever an asset is requested from a compiler that’s still running its first build. The overlay code communicates back with your development server to check on the status of your builds, and automatically reloads the page once everything is complete. Using Kevin is meant to be really straightforward. If you don’t already have a development server of some sort, creating one with Kevin and Express is maybe a dozen lines of code. Here’s a snippet taken from Kevin’s documentation: const express = require(\"express\");\nconst Kevin = require(\"kevin-middleware\");\n\n// This is an array of webpack configs. Each config **must** be named so that we\n// can uniquely identify each one consistently. A regular ol' webpack config\n// should work just fine as well.\nconst webpackConfigs = require(\"path/to/webpack.config.js\");\n\n// Setup your server and configure Kevin\nconst app = express();\n\nconst kevin = new Kevin(webpackConfigs, {\n    kevinPublicPath = \"http://localhost:3000\"\n});\napp.use(kevin.getMiddleware());\n\n// Serve static files as needed. This is required if you generate async chunks;\n// Kevin only knows about the entrypoints in your configs, so it has to assume\n// that everything else is handled by a different middleware.\napp.use(\"/ac/webpack/js\", express.static(webpackConfigs[0].output.path));\n\n// Let 'er rip\napp.listen(9275); We’ve also made a bunch of Kevin’s internals accessible through Webpack’s own tapable plugin system. At Etsy, we use these hooks to integrate with our monitoring system , and to gracefully restart active compilers that have pending updates to their configurations. In this way, we can keep our development server up to date while keeping developer interruptions to a minimum. Sometimes, a little custom code goes a long way. In the end, we were able to greatly improve the development experience. Rebuilding our seller tools, which previously took almost a minute on every request, now takes under 30 seconds when we’re starting a fresh compiler, and subsequent requests take only a second or two. Navigating around Etsy.com in development still takes very little interaction with the build system from our engineers. Plus, we can now support all the other things that Webpack enables for us, like ES6, better asset analysis, and even TypeScript. This is the part where I should mention that Kevin is officially open-source software. Check out the source on Github , and install it from npm as kevin-middleware . If you have any feedback about it, we would welcome an issue on Github. I really hope you get as much use out of it as we did. This post is the second in a two-part series on our migration to a modern JavaScript build system. The first part can be found here . Posted by Salem Hilal on April 6, 2020 Category: Uncategorized", "date": "2020-04-6,"},
{"website": "Etsy", "title": "G-Scout Enterprise and Cloud Security at Etsy", "author": ["Angelo Mellos"], "link": "https://codeascraft.com/2019/11/18/g-scout-enterprise-and-cloud-security-at-etsy/", "abstract": "G-Scout Enterprise and Cloud Security at Etsy Posted by Angelo Mellos on November 18, 2019 As companies are moving to the cloud, they are finding a need for security tooling to audit and analyze their cloud environments. Over the last few years, various tools have been developed for this purpose. We’ll look at some of them and consider the uses for them. Specifically, we’ll take a close look at G-Scout, a tool I developed while working at NCC Group to look for security misconfigurations in Google Cloud Platform (GCP); and G-Scout Enterprise, a new tool with the same purpose, but tailored to the needs of security engineers at Etsy. We’ll also consider G-Scout Enterprise’s role within an ecosystem of other cloud logging and monitoring tools used at Etsy. Cloud environments have a convenient feature which you won’t get from on premise servers: they have APIs. It’s similar for all the major cloud providers. They have a REST API which provides information on what services are being used, what resources exist, and how they are configured. An authorized user can call these APIs through a command line tool, or programmatically through a client library. Those APIs provide information which is useful for security purposes. A classic example is a storage bucket (S3, GCS, etc.) which has been made public. It could be publicly readable, or publicly writable. Since we can use the API to see the permissions on any bucket we own, we can look for misconfigured permissions. So we go through all the API data we have for all our storage buckets, and look for permissions assigned to allUsers, or allAuthenticatedUsers. Here are some other common examples: Firewall rules are too permissive. Unencrypted database connections. Users have excessive permissions. Configuration Scanning Tools Rather than making API calls and processing the data ad hoc, you can create a framework. A tool that will allow you, with a single command, to run various API calls to gather data on diverse resources, and then programmatically look for misconfigurations in that data. And in the end, you can have the tool place the results into a human-readable HTML report which you can browse according to your whims. Scout 2 does all of the above for Amazon Web Services (AWS). G-Scout was created with a similar solution in mind as Scout 2, but for GCP. After Scout 2 there have followed plenty of other examples. Some, like G-Scout, have been open source, and others are available for purchase. These tools continue to evolve. It is becoming increasingly common for companies to use more than one cloud provider. With this trend we’ve seen the creation of multi-cloud tools. Scout Suite has replaced Scout 2. Inspec supports AWS, Azure, and GCP. And some of them have added features. Forseti Inventory stores the data collected in a SQL database (I’ve moved G-Scout in a similar direction, as we’ll see later). Forseti Enforcer will actually make changes to match policies. These features are useful, but not so much to a consultant, since a consultant shouldn’t want any permissions aside from viewer permissions. Scout 2 was designed for consulting. The user can get viewer permissions, run the tool, and leave no trace. Forseti, on the other hand, requires Organization Admin permissions, and creates a database and other resources within the organization that is being audited. Difficulties With G-Scout But the same basic functionality remains at the core of each of these tools. When it came to G-Scout, that core functionality worked well for smaller companies, or those less committed to GCP. But when there are hundreds of projects, thousands of instances, and many other resources, it becomes difficult to go through the results. Adding to this difficulty is the presence of false positives. Any automated tool is going to turn up false positives. Context may make something that seems like a finding at first glance, instead turn out to be acceptable. To return to our public storage bucket example, there are some cases where the content in the bucket is intended to be public. You can even serve a simple HTML website from a storage bucket. So it tends to fall to a human to go through and figure out which are false positives. Since it takes time to fix real findings, and the false positives don’t go away, running the tool frequently to see what’s new becomes untenable. Finally, at Etsy, many of the findings G-Scout would turn up had already been found by other means, which we will explore a bit below. We have a tool called Reactor. There is a stackdriver log sink for the organization, and those logs (with filters applied) go to a PubSub topic. There’s a cloud function that subscribes to that topic, and when it finds logs that match any of a further set of filters (the alerting rules) then it triggers an alert. So for example, if someone makes a storage bucket public, an alert will trigger as soon as the corresponding stackdriver log is generated, rather than waiting for someone to run G-Scout at some point. Here’s a partial example of a stackdriver log. As an API call to check IAM permissions would, it has all the information we need to trigger an alert. We see the user that granted the permission (in this case a service account). And below the fold we would see which role was assigned and which user it was assigned to. Another point where we are alerting on misconfigurations is resource creation. We use Terraform for infrastructure as code. Before a Terraform apply is run, we have a series of unit tests that will be run by the pipeline. The unit tester runs tests for many of the same events which we alert on with the stackdriver logs. This includes the common example of a bucket being made public. This is another process that is not so useful for a security consultant. But it’s better to catch misconfigurations in this way, than in the way Scout 2 or G-Scout would catch them, since this will prevent them from ever being implemented! So we have what I’ll call a three-pronged approach to catching misconfigurations in GCP. These are the three prongs: Terraform unit testing that is meant to catch misconfigurations before they go into effect. Stackdriver alerting that occurs when the resource is created or changed (whether those changes are made through Terraform or not). And in case anything gets through the first two, we have the point in time audit of all GCP resources provided by G-Scout Enterprise. In summary, G-Scout’s traditional purpose was proving minimally useful. It was difficult to make good use of the G-Scout reports. And as we’ve seen, the first two prongs will usually catch misconfigurations first. So I moved away from G-Scout, and toward a new creation: G-Scout Enterprise. G-Scout Enterprise The fundamental change is to replace the HTML report with a BigQuery data collection. In fact, at its core, G-Scout Enterprise is very simple. It’s mostly just something that takes API data and puts it into BigQuery. Then other systems can do with that data as they please. The rules that will trigger alerts can be written in our alerting system like any other alerts we have (though they can also easily be written in Python within G-Scout Enterprise). We are now putting all of our other data into BigQuery as well, so it’s all connected. Users can query any of the tables, each of which corresponds to one GCP API endpoint. G-Scout Enterprise tables can be joined – and they can be joined to our other data sources as well. And we can be very specific: like looking for all roles where amellos@etsy.com is a member, without enshrining it in our ruleset, because we can run queries through the BigQuery console. Or we can run queries in the command line, with helper functions that allow us to query with Python rather than SQL. We can make comparisons and track changes over time. It can also provide context to other alerts. For example, if we have an IP address from an SSH alert, we can get information about the instance which owns that IP address, such as what service account it has, or what Chef role it has. Or for instance, the following, more complicated scenario: We run Nessus. Nessus is an automated vulnerability scanner. It has a library of vulnerabilities it looks for by making network requests. You give it a list of IPs and it goes through them all. We now have it running daily. With a network of any size the volume of findings will quickly become overwhelming. Many of them are informational or safely ignored. But the rest need to be triaged, and addressed in a systematic way. Not all Nessus findings are created equal. The same vulnerability on two different instances may be much more concerning on one than the other: if one is exposed to the internet and the other is not; if one is working with PII and the other is not; if one is in development and the other in production, and so on. Most of the information which determines how concerned we are with a vulnerability can be found among the collection of data contained in G-Scout Enterprise. This has simplified our scanning workflow. Since we can do network analysis with the data in G-Scout Enterprise, we can identify which instances are accessible from where. That means we don’t have to scan from different perspectives. And it has improved the precision of our vulnerability triaging, since there is so much contextual data available. So we go through the following process: Enumerate all instances in our GCP account. Discard duplicate instances (instances from the same template, e.g. our many identical web server instances). Run the Nessus scan and place the results into BigQuery. Create a joined table of firewall rules and instances which they apply to (matching tags). Take various network ranges (0.0.0.0/0, our corporate range, etc.), and for each firewall rule see if it allows traffic from that source. For instances with firewall rules that allow ingress from 0.0.0.0/0, see if the instance has a NatIP or is behind an external load balancer. Check whether the project the instance lives in is one of the projects classified as sensitive. Compute and assign scores according to the previous steps And then we save the results into BigQuery. That gives us historical data. We can see if we are getting better or worse. We can see if we have certain troublemaker projects. We can empower our patch management strategy with a wealth of data. Conclusion That leaves us with a few main lessons gained from adapting G-Scout to Etsy: It’s useful to store cloud environment info in a database. That makes it easier to work with, and easier to integrate with other data sources. The needs of a consultant are different from the needs of a security engineer. Although there is crossover, different tools may better fit the needs of one or the other. The three pronged alerting approach described above provides a comprehensive solution for catching security misconfigurations in the Cloud. One last note is that we have plans to open source G-Scout Enterprise in the coming months. Posted by Angelo Mellos on November 18, 2019 Category: Uncategorized Tags: GCP , Google Cloud Platform , security Related Posts Posted by Emily Sommer , Mike Adler , John Perkins , Joshua Thiel , Hilary Young , Chelsea Mozen , Dany Daya and Katie Sundstrom on 23 Apr, 2020 Cloud Jewels: Estimating kWh in the Cloud Posted by Angelo Mellos on 25 Sep, 2019 Apotheosis: A GCP Privilege Escalation Tool Posted by Toria Gibbs on 05 Jun, 2018 Deploying to Google Kubernetes Engine", "date": "2019-11-18,"},
{"website": "Etsy", "title": "Engineering Career Development at Etsy", "author": ["Ryan Bateman"], "link": "https://codeascraft.com/2019/10/02/engineering-career-development-at-etsy/", "abstract": "Engineering Career Development at Etsy Posted by Ryan Bateman on October 2, 2019 In late May of 2018, Etsy internally released an Engineering Career Ladder. Today, we’re sharing that ladder publicly and detailing why we decided to build it, why the content is what it is, and how it’s been put into use since its release. Take a look Defining a Career Ladder A career ladder is a tool to help outline an engineer’s path for growth within a company. It should provide guidance to engineers on how to best take on new responsibilities, and allow their managers to assess and monitor performance and behavior. A successful career ladder should align career progression with a company’s culture, business goals, and guiding principles and act as a resource to guide recruiting, training, and performance assessments. Etsy has had several forms of a career ladder before this iteration. The prior career ladders applied to all Etsy employees, and had a set of expectations for every employee in the same level across all disciplines. Overall, these previous ladders worked well for Etsy as a smaller company, but as the engineering team continued to grow we found the content needed updating to meet practical expectations, as the content in the ladder started to feel too broad and unactionable. As a result, we developed this career ladder, specific to engineering, to allow us to be more explicit with those expectations and create a unified understanding of what it means to be an engineer at a certain level at Etsy. This ladder has been in place for over a year now, and in that time we’ve gone through performance reviews, promotion cycles, lots of hiring, and one-on-one career development conversations. We’re confident that we’ve made a meaningful improvement to engineering career development at Etsy and hope that releasing this career ladder publicly can help other companies support engineering career growth as well. Designing the Etsy Engineering Career Ladder We formed a working group focused on creating a new iteration of the career ladder comprised of engineers and engineering managers of various levels. The working group included Miriam Lauter, Dan Auerbach, and Jason Wain, and me. We started by exploring our current company-wide career ladder, discussing its merits and limitations, and the impact it had on engineering career development. We knew that any new version needed to be unique to Etsy, but we spent time exploring publicly available ladders of companies who had gone through a similar process in an effort to understand both tactical approaches and possible formats. Many thanks specifically to Spotify , Kickstarter , Riot Games , and Rent the Runway for providing insight into their processes and outcomes. Reviewing their materials was invaluable. We decided our first step was to get on the same page as to what our goals were, and went through a few exercises resulting in a set of tenets that we felt would drive our drafting process and provide a meaningful way to evaluate the efficacy of the content. These tenets provided the foundation to our approach for developing the ladder. The Tenets Support meaningful career growth for engineers Our career ladder should be clear enough, and flexible enough, to provide direction for any engineer at the company. We intended this document to provide actionable steps to advance your career in a way that is demonstrably impactful. Ideally, engineers would use this ladder to reflect on their time at Etsy and say “I’ve developed skills here I’ll use my entire career.” Unify expectations across engineering We needed to build alignment across the entire engineering department about what was required to meet the expectations of a specific level. If our career ladder were too open to interpretation it would cause confusion, particularly as it relates to the promotion process. We wanted to ensure that everyone had a succinct, memorable way to describe our levels, and understand exactly how promotions happen and what is expected of themselves and their peers. Recognize a variety of valid career paths Whether you’re building machine learning models or localizing our products , engineering requires skills across a range of competencies, and every team and project takes individuals with strengths in each. We wanted to be explicit about what we believe about the discipline, that valid and meaningful career paths exist at all levels for engineers who bring differences of perspectives and capabilities, and that not everyone progresses as an engineer in the same way. We intended to codify that we value growth across a range of competencies, and that we don’t expect every person to have the same set of strengths at specific points in their career. Limit room for bias in how we recognize success A career ladder is one in a set of tools that can help an organization mitigate potential bias. We needed to be thoughtful about our language, ensuring that it is inclusive, objective, and action oriented. We knew the career ladder would be used as basis for key career advancement moments, such as hiring and promotions, so developing a clear and consistent ladder was critical for mitigating potential bias in these processes. Developing the Etsy Engineering Career Ladder With these tenets in place, we had the first step towards knowing what was necessary for success. In addition to creating draft ladder formats, we set about determining how we could quantify the improvements that we were making. We outlined key areas where we’d need to directly involve our stakeholders, including engineering leadership, HR, Employee Resource Groups, and of course engineers. We made sure to define multiple perspectives for which the ladder should be a utility; e.g. an engineer looking to get promoted, a manager looking to help guide an engineer to promotion, or a manager who needed to give constructive performance feedback. Implicit biases can be notoriously difficult to acknowledge and remove from these processes, and we knew that in order to do this as best as possible we’d need to directly incorporate feedback from many individuals, both internal and external, across domains and disciplines, and with a range of perspectives, to assure that we were building those perspectives into the ladder. Our tactics for measuring our progress included fielding surveys and requests for open feedback, as well as direct 1:1 in-depth feedback sessions and third party audits to ensure our language was growth-oriented and non-idiomatic. We got feedback on structure and organization of content, comprehension of the details within the ladder, the ladder’s utility when it came to guiding career discussions, and alignment with our tenets. The feedback received was critical in shaping the ladder. It helped us remove duplicative, unnecessary, or confusing content and create a format that we thought best aligned with our stated tenets and conveyed our intent. And finally, the Etsy Engineering Career Ladder You can find our final version of the Etsy Engineering Career Ladder here . The Etsy Engineering Career Ladder is split into two parts: level progression and competency matrix. This structure explicitly allows us to convey how Etsy supports a variety of career paths while maintaining an engineering-wide definition of each level. The level progression is the foundation of the career ladder. For each level, the ladder lays out all requirements including expectations, track record, and competency guidelines. The competency matrix lays out the behaviors and skills that are essential to meeting the goals of one’s role, function, or organization. Level Progression Each section within the level progression provides a succinct definition of the requirements for an engineer with that title. It details a number of factors, including the types of problems an engineer is solving, the impact of their work on organizational goals and priorities and how they influence others that they work with. For levels beyond Engineer I, we outline an expected track record, detailing achievements over a period of time in both scale and complexity. And to set expectations for growth of competencies, we broadly outline what levels of mastery an engineer needs to achieve in order to be successful. Competencies If the level progression details what is required of an engineer at a certain level, competencies detail how we expect they can meet those expectations. We’ve outlined five core competency areas: Delivery Domain Expertise Problem Solving Communication Leadership For each of these five competency areas, the competency matrix provides a list of examples that illustrate what it means to have achieved various levels of mastery. Mastery of a competency is cumulative — someone who is “advanced” in problem solving is expected to retain the skills and characteristics required for an “intermediate” or “beginner” problem solver. Evaluating our Success We internally released this new ladder in May of 2018. We did not immediately make any changes to our performance review processes, as it was critical to not change how we were evaluating success in the middle of a cycle. We merely released it as a reference for engineers and their managers to utilize when discussing career development going forward. When our next performance cycle kicked off, we began incorporating details from the ladder into our documentation and communications, making sure that we were using it to set the standards for evaluation. Today, this career ladder is one of the primary tools we use for guiding engineer career growth at Etsy. Utilizing data from company-wide surveys, we’ve seen meaningful improvement in how engineers see their career opportunities as well as growing capabilities for managers to guide that growth. Reflecting on the tenets outlined at the beginning of the process allows us to look back at the past year and a half and recognize the change that has occurred for engineers at Etsy and evaluate the ladder against the goals we believed would make it a success. Let’s look back through each tenet and see how we accomplished it. Support meaningful career growth for engineers While the content is guided by our culture and Guiding Principles , generally none of the competencies are Etsy-specific. The expectations, track record, and path from “beginner” to “leading expert” in a competency category are designed to show the growth of an engineer’s impact and recognize accomplishments that they can carry throughout their career, agnostic of their role, team, or even company. The competency matrix also allows us to guide engineer career development within a level. While a promotion to a new level is a key milestone that requires demonstration of meeting expectations over time, advancing your level of mastery by focusing on a few key competencies allows engineers to demonstrate continual growth, even within the same level. This encourages engineers and their managers to escape the often insurmountable task of developing a plan to achieve the broader set of requirements for the next promotion, and instead create goals that help them get there incrementally. Compared to our previous ladder, the path to Staff Engineer is no longer gated by the necessity to increase one’s breadth. We recognized that every domain has significantly complex, unscoped problems that need to be solved, and that we were limiting engineer growth by requiring those who were highly successful in their domain to expand beyond it. Having expectations outlined as they are now allows engineers the opportunity to grow by diving more deeply into their current domains. Unify expectations across engineering The definition for each level consists only of a few expectations, a track record, and guidelines for level of mastery of competencies. It is easy to parse, and to refer back to to get a quick understanding of the requirements. With a little reflection, it should be easy to describe how any engineer meets the three to five expectations of their level. Prior to release, we got buy-in from every organizational leader in engineering that these definitions aligned with the reality of the expectations of engineers in their org. Since release we’ve aligned our promotion process to the content in the ladder. We require managers to outline how a candidate has met the expectations over the requisite period stated in the track record for their new level, and qualify examples of how they demonstrate the suggested level of mastery for competencies. Recognize a variety of valid career paths We ask managers to utilize the competencies document with their reports’ specific roles in mind when talking about career progression. Individual examples within the competency matrix may feel more or less applicable to individual roles, such as a Product Engineer or a Security Engineer, and this adaptability allows per-discipline growth while still aligning with the behaviors and outcomes we agree define a level of mastery. A small set of example skills is provided for each competency category that can help to better contextualize the application of the competencies in various domains. Additionally, we intentionally do not detail any competencies for which success is reliant on your team or organization. Allowing managers to embrace the flexibility inherent in the competency matrix and its level of mastery system has allowed us to universally recognize engineer growth as it comes in various forms, building teams that embrace differences and value success in all its shapes. Managers can grow more diverse teams, for instance, by being able to recognize engineering leaders who are skilled domain experts, driving forward technical initiatives, and other engineering leaders who are skilled communicators, doing the glue work and keeping the team aligned on solving the right problems. We recognize that leadership takes many forms, and that is reflected in our competency matrix. Limit room for bias in how we recognize success The career ladder is only a piece of how we can mitigate potential bias as an organization. There are checks and balances built into other parts of Etsy’s human resources processes and career development programs, but since a career ladder plays such a key role in shaping the other processes, we approached this tenet very deliberately. The competencies are not personality based, as we worked to remove anything that could be based on subjective perception of qualities or behaviors, such as “being friendly.” All content is non-idiomatic, in an effort to reduce differences in how individuals will absorb or comprehend the content. We also ensured that the language was consistent between levels by defining categories for each expectation. For instance, defining the expected complexity of the problems engineers solve per level allowed us to make sure we weren’t introducing any leaps in responsibility between levels that couldn’t be tied back to growth in the previous level. We also explicitly avoided any language that reads as quantifiable (e.g. “you’ve spoken at two or more conferences”) as opportunities to achieve a specific quantity of anything can be severely limited by your role, team, or personal situation, and can lead to career advice that doesn’t get at the real intent behind the competency. Additionally, evaluation of an individual against the ladder, for instance as part of a promotion, is not summarized in numbers. There is no score calculation or graphing an individual on a chart, nor is there an explicit number of years in role or projects completed as an expectation. While reducing subjectivity is key to mitigating potential bias, rigid numerical guidelines such as these can actually work against our other tenets by not allowing sufficient flexibility given an individual’s role. Most importantly, the ladder was shaped directly through feedback from Etsy engineers, who have had direct personal experiences with how their individual situations may have helped or hindered their careers to draw on. We’re really passionate about supporting ongoing engineer career growth at Etsy, and doing it in a way that truly supports our mission. We believe there’s a path to Principal Engineer for every intern and that this ladder goes a long way in making that path clear and actionable. We hope this ladder can serve as an example, in addition to those we took guidance from, to help guide the careers of engineers everywhere. If you’re interested in growing your career with us, we’d love to talk, just click here to learn more. Posted by Ryan Bateman on October 2, 2019 Category: engineering , people", "date": "2019-10-2,"},
{"website": "Etsy", "title": "Apotheosis: A GCP Privilege Escalation Tool", "author": ["Angelo Mellos"], "link": "https://codeascraft.com/2019/09/25/apotheosis-a-gcp-privilege-escalation-tool/", "abstract": "Apotheosis: A GCP Privilege Escalation Tool Posted by Angelo Mellos on September 25, 2019 The Principle of Least Privilege One of the most fundamental principles of information security is the principle of least privilege. This principle states that users should only be given the minimal permissions necessary to do their job. A corollary of the principle of least privilege is that users should only have those privileges while they are actively using them. For especially sensitive actions, users should be able to elevate their privileges within established policies, take sensitive actions, and then return their privilege level to normal to resume normal usage patterns. This is sometimes called privilege bracketing when applied to software, but it’s also useful for human users. Following this principle reduces the chance of accidental destructive actions due to typos or misunderstandings. It may also provide some protection in case the user’s credentials are stolen, or if the user is tricked into running malicious code. Furthermore, it can be used as a notice to perform additional logging or monitoring of user actions. In Unix this takes the form of the su command, which allows authorized users to elevate their privileges, take some sensitive actions, and then reduce their permissions. The sudo command is an even more fine-grained approach with the same purpose, as it will elevate privileges for a single command. Some cloud providers have features that allow for temporary escalation of privileges. Authorized users can take actions with a role other than the one which is normally assigned to them. The credentials used to assume a role are temporary, so they will expire after a specified amount of time. However, we did not find a built-in solution to achieve the same functionality in Google Cloud Platform (GCP). Enter Apotheosis Apotheosis is a tool that is meant to address the issues above. The word apotheosis means the elevation of someone to divine status. It’s possible, and convenient, to give users permanent “godlike” permissions, but this is a violation of the principle of least privilege. This tool will allow us to “apotheosize” users, and then return them to a “mortal” level of privilege when their job duties no longer require additional privileges. Users or groups can be given “actual permissions” and “eligible permissions”. For example, a user who currently has the owner role may instead be given only the viewer role, and we will call that their “actual permissions”. Then we can give them “eligible permissions” of owner , which will come in the form of the service account token creator role on a service account with the editor or organization admin role. For this user to elevate their privileges, the Apotheosis command line program will use their GCP credentials to call the REST API to create a short-lived service account token . Then, using that token, Apotheosis will make another REST API call which will grant the requested permissions to the user. Or, alternatively, the permissions may be granted to a specified third party, allowing the Apotheosis user to leverage their eligible permissions to grant actual permissions to another entity. The program will wait for a specified amount of time, remove the requested permissions, and then delete the short-lived service account token. This process has the following advantages: It requires no additional access controls or centralized server actions. There is no possibility of compromising the program since it is local and only capable of escalating to the level of privilege which users are already allowed in the GCP Identity and Access Management (IAM) configuration. The user is only required to enter one command in their terminal. It looks like this: apotheosis -m user:someuser@etsy.com -r roles/editor  -d 600 --resource some-project . Or to use the defaults, just apotheosis . Any permissions will be granted by the designated service account. This allows for logging that service account’s IAM activity, and alerting on any troubling events in regards to that activity. Future Additions Some additional features which may be added to Apotheosis are contingent on the launch of other features, such as conditional IAM. Conditional IAM will allow the use of temporal restrictions on IAM grants, which will make Apotheosis more reliable. With conditional IAM, if Apotheosis is interrupted and does not revoke the granted permissions, they will expire anyway. The ability to allow restricted permissions granting will be a useful IAM feature as well. Right now a user or service account can be given a role like editor or organization admin , and then can grant any other role in existence. But if it were possible to allow granting a predefined list of roles, that would make Apotheosis useful for a larger set of users. As it is now, Apotheosis is useful for users who have the highest level of eligible privilege, since their access to the Apotheosis service account gives them all the privileges of that service account. That is, the scope of those privileges can be limited to a particular project, folder, or organization, but cannot be restricted to a limited set of actions. At the moment that service account must have one of the few permissions which grant the ability to assign any role to any user. Requiring two-factor authentication when using the short-lived service account token feature on a particular service account would be another useful feature. This would require an Apotheosis user to re-authenticate with another factor when escalating privileges. Open Source Apotheosis is open source and can be found on Github. Posted by Angelo Mellos on September 25, 2019 Category: security Tags: GCP , iam , security Related Posts Posted by Emily Sommer , Mike Adler , John Perkins , Joshua Thiel , Hilary Young , Chelsea Mozen , Dany Daya and Katie Sundstrom on 23 Apr, 2020 Cloud Jewels: Estimating kWh in the Cloud Posted by Angelo Mellos on 18 Nov, 2019 G-Scout Enterprise and Cloud Security at Etsy Posted by Toria Gibbs on 05 Jun, 2018 Deploying to Google Kubernetes Engine", "date": "2019-09-25,"},
{"website": "Etsy", "title": "An Introduction to Structured Data at Etsy", "author": ["Yael Kaufman"], "link": "https://codeascraft.com/2019/07/31/an-introduction-to-structured-data-at-etsy/", "abstract": "An Introduction to Structured Data at Etsy Posted by Yael Kaufman on July 31, 2019 Etsy has an uncontrolled inventory ; unlike many marketplaces, we offer an unlimited array of one-of-a-kind items, rather than a defined set of uniform goods. Etsy sellers are free to list any policy-compliant item that falls within the three broad buckets of craft supplies, handmade, and vintage. Our lack of standardization, of course, is what makes Etsy special, but it also makes learning about our inventory challenging. That’s where structured data comes in. Structured vs. Unstructured Data Structured data is data that exists in a defined relationship to other data. The relation can be articulated through a tree, graph, hierarchy, or other standardized schema and vocabulary. Conversely, unstructured data does not exist within a standardized framework and has no formal relationship to other data in a given space. For the purposes of structured data at Etsy, the data are the product listings, and they are structured according to our conception of where in the marketplace they belong. That understanding is expressed through the taxonomy . Etsy’s taxonomy is a collection of hierarchies comprised of 6,000+ categories (ex. Boots), 400+ attributes (ex. Women’s shoe size), 3,500+ values (ex. 7.5), and 90+ scales (ex. US/Canada). These hierarchies form the foundation of 3,500+ filters and countless category-specific shopping experiences on the site. The taxonomy imposes a more controlled view of the uncontrolled inventory — one that engineers can use to help buyers find what they are looking for. Building the Taxonomy The Etsy taxonomy is represented in JSON files , with each category’s JSON containing information about its place in the hierarchy and the attributes, values, and scales for items in that category. Together, these determine what questions will be asked of the seller for listings in that category ( Figure A, Box 1 ), and what filters will be shown to buyers for searches in that category ( Figure A, Box 2 ). Figure A A snippet of the JSON representation of the Jewelry > Rings > Bands category The taxonomists at Etsy are able to alter the taxonomy hierarchies using an internal tool. This tool supports some unique behaviors of our taxonomy, like inheritance. This means that if a category has a particular filter, then all of its subcategories will inherit that filter as well. Figure B Sections of the Jewelry > Rings > Bands category as it appears in our internal taxonomy tool Gathering Structured Data: The Seller Perspective One of the primary ways that we currently collect structured data is through the listing creation process , since that is our best opportunity to learn about each listing from the person who is most familiar with it: the seller! Sellers create new listings using the Shop Manager . The first step in the listing process is to choose a category for the listing from within the taxonomy. Using auto-complete suggestions, sellers can select the most appropriate category from all of the categories available. Figure C Category suggestions for “ring” At this stage in the listing creation process, optional attribute fields appear in the Shop Manager. This is also enabled by the taxonomy JSON, in that the fields correspond with the category selected by the seller (see Figure A, Box 1 ). This behavior ensures that we are only collecting relevant attribute data for each category and simplifies the process for sellers. Promoting this use of standardized data also reduces the need for overloaded listing titles and descriptions by giving sellers a designated space to tell buyers about the details of their products. Data collected during the listing creation process appears on the listing page, highlighting for the buyer some of the key, standardized details of the listing. Figure D Some of the attribute fields that appear for listings in Jewelry > Rings > Bands (see Figure A, Box 1 for the JSON that powers the Occasion attribute) Making Use of Structured Data: The Buyer Perspective Much of the buyer experience is a product of the structured data that has been provided by our sellers. For instance, a given Etsy search yields category-specific filters on the left-hand navigation of the search results page. Figure E Some of the filters that appear upon searching for “Rings” Those filters should look familiar! (see Figure D ) They are functions of the taxonomy. The search query gets classified to a taxonomy category through a big data job, and filters affiliated with that category are displayed to the user (see Figure F below). These filters allow the buyer to narrow down their search more easily and make sense of the listings displayed. Figure F The code that displays category-specific filters upon checking that the classified category has buyer filters defined in its JSON (see Figure A, Box 2 for a sample filter JSON) Structuring Unstructured Data There are countless ways of deriving structured data that go beyond seller input. First, there are ways of converting unstructured data that has already been provided, like listing titles or descriptions, into structured data. Also, we can use machine learning to learn about our listings and reduce our dependence on seller input. We can, for example, learn about the color of a listing through the image provided; we can also infer more nuanced data about a listing, like its seasonality or occasion. We can continue to measure the relevance of our structured data through metrics like the depth of our inventory categorization within our taxonomy hierarchies and the completeness of our inventory’s attribution. All of these efforts allow us to continue to build deeper category-specific shopping experiences powered by structured data. By investing in better understanding our inventory, we create deeper connections between our sellers and our buyers. Posted by Yael Kaufman on July 31, 2019 Category: engineering Tags: structured-data , taxonomy", "date": "2019-07-31,"},
{"website": "Etsy", "title": "What it’s Like to Intern at Etsy? – Part I", "author": ["Kelly Shen"], "link": "https://codeascraft.com/2019/03/18/what-its-like-to-intern-at-etsy-part-i/", "abstract": "What it’s Like to Intern at Etsy? – Part I Posted by Kelly Shen on March 18, 2019 I secretly like seeing people’s surprise when I told them that I chose to intern at Etsy because it was the only company that asked for a cover letter. I enjoyed every second of filling out my Etsy software engineering internship application because I felt like I was really telling my story to a company that cared about my whole self. I interned at Etsy during summer 2016 and started working full-time after I graduated from college in 2017. The human touch embedded in Etsy’s engineering culture, business strategy and company vision is still the number one thing I am proud of. Over the past three years, I have gotten many questions about what it’s like to intern and have my first job out of college at Etsy. It always gives me a warm feeling when students are curious and excited about careers at Etsy, and I think it’s time we give this question answers that will live on the interweb. This past winter, I met five interns that Etsy hosted for WiTNY (Women in Technology and Entrepreneurship in New York)’s Winternship program. At the end of their three-week internships, they were super excited to share their experiences. One of the winterns, Nia Laureano, wrote a fantastic recap of her time at Etsy, and I thought it would be a great way to start sharing the Etsy internship experience! Inventing a Process: Five Interns Navigate a Complex Problem Thanks to Etsy’s Relentlessly Human Touch by Nia Laureano Interning at Etsy is a unique experience because so much about Etsy’s identity has to be understood in order to move forward with any sort of work. For three weeks in January, a team of four girls and I joined the Etsy family as interns and were tasked with solving an issue being faced by a product team that helps the buyers. Coming into our first day, the details of our task were overwhelmingly foreign to us. The subject we were dealing with was Etsy’s listing page. It’s a complicated page, due to the fact that 60 million listings exist on Etsy and they are all vastly different. When engineers make changes to the listing page, it is difficult to test their code against every possible variation of the page that exists. Sometimes, a variation slips from their mind — they forget to account for it, which could potentially cause the page to break when they push code. This is what engineers call an edge case , and our job was to create a tool that allows Etsy engineers to test for edge cases more thoroughly. Specifically, we were asked to create a reference for them to easily find listings that match different criteria and variations to test code against. But solving a project that we barely understood ourselves seemed daunting, if not impossible. The entirety of our first week was spent immersing ourselves in the context of this world we were working in. We strolled through the typical workflow of an Etsy engineer, trying to imagine where our solution would fit neatly into the puzzle. We spoke to engineers about their frustrations to get to the root of their needs. We became engineers by being thrusted into the process of pushing code to Etsy’s repository. We couldn’t commit to our craft without first understanding how these employees live and work; then we had to imagine what we could do to make their world better. After interviewing several engineers, we realized that they each had their own ways of testing for edge cases. “I just have two folders of bookmarks that have some links in them,” said one engineer. “But I’m not sure what other people use.” It was surprising to hear that engineers weren’t sure what other people on their team were doing . We realized, at this point, that the problem wasn’t a faulty process — there was no process to begin with. It was up to us to invent a process, or at least establish a basic standard when it comes to testing for edge cases. In ideation, the solutions we envisioned ranged dramatically. Something as basic as a spreadsheet would have been helpful, but we also dreamed bigger. We thought about creating an automated Etsy shop that auto-generates listings that represent the edge cases that needed to be tested. We wanted to create something ambitious, but it also had to be something we could attain in three weeks. Ultimately, we focused on creating a solution that would deliver on three crucial needs of our engineers: structure , convenience and confidence . Structure. While some engineers relied on their own bookmarks or spreadsheets to keep track of edge cases, some relied on sheer memory, or asking their coworkers via Slack. Testing for something that could potentially break the listing page , we realized, shouldn’t be such a structureless process . Our solution needed to provide an element of uniformity; it needed to eliminate that glaring unawareness about what other teammates were doing. It needed to be a unifier. Convenience . In order to make a tool that was accessible and easy to use, we needed to identify and understand the environment in which engineers complete the bulk of their work, because that’s where we would want our tool to live. We quickly noticed one common thread woven through the workflow of not only Etsy’s engineers, but the company as a whole: our messaging platform, Slack. We observed that so much important work at Etsy is already accomplished via Slack; it’s where employees collaborate and even push code. It made perfect sense for our solution to be integrated within the environment that was already so lived-in . Confidence. Bugs are inevitable, but our engineers deserve to feel confident that the code they are pushing is as clean as it can be. The more edge cases they can test for, the more certain they can feel that their code is quality and fully functional. Therefore, our solution had to be thorough and reliable; it had to be something engineers could trust. After three weeks, our project was completed in two phases. Our first phase was focused on creating a spreadsheet. This was the skeleton of our final product, which mirrored the anatomy of the listing page itself. To build this, we broke down the different components of the listing page and identified all of the variations that could occur within those components. Then, we spent several days creating almost one hundred of our own listings on Etsy that represented each of those variations. We ended up with a thorough, intuitively structured catalog of edge cases which can now be accessed by anyone at Etsy who needs it. The second phase of our project was a Slack-integrated bot. Using our spreadsheet as a backbone, we aimed to design a bot that can retrieve edge cases on command via Slack. Engineers can input commands that return single, multiple, or all edge cases they may be looking for. Due to our time constraint, we were only able to create a bot that utilizes test data, but we hope to see a future iteration that fully integrated with our spreadsheet. A universe of terminology and culture had to be packed into our brains in order to accomplish what we did in three weeks. Yet, we somehow felt so seamlessly integrated into Etsy’s ecosystem from day one, thanks to the friendly and enthusiastic nature of everyone around us. We were never afraid to ask questions, because no one ever talked down to us or made us feel inferior. There are no mechanisms in place at Etsy that make power dynamics apparent, not even from the perspective of an intern. Our project was completed not because of crash courses in PHP or because we overloaded on cold brew; it was thanks to the people who nurtured us along the way. It was the prospect of creating something that could make a lasting impact on a company we loved that motivated us. Etsy’s relentlessly human touch makes even the smallest of projects feel meaningful, and it can turn three weeks into an unforgettable experience that I will never stop feeling passionate about. A note about our internship & our organization: WiTNY (Women in Technology and Entrepreneurship in New York) is a collaborative initiative between Cornell Tech x CUNY designed to inspire young women to pursue careers in technology. WiTNY offers workshops and program that teach important skills and provide real work experience. The Winternship program is a paid, three-week, mini-internship for first and second-year undergraduate students at CUNY schools, during their January academic recess. Etsy is one of many companies who participated in the Winternship program this year, taking a team of five young women and giving them a challenging project to complete while also teaching them about the different roles within a tech company. Posted by Kelly Shen on March 18, 2019 Category: engineering , people , philosophy Tags: careers , internship", "date": "2019-03-18,"},
{"website": "Etsy", "title": "Executing a Sunset", "author": ["Rachana Kumar"], "link": "https://codeascraft.com/2019/02/01/executing-a-sunset/", "abstract": "Executing a Sunset Posted by Rachana Kumar on February 1, 2019 We all know how exciting it is to build new products, the thrill of a pile of new ideas waiting to be tested, new customers to reach, knotty problems to solve, and dreams of upward-sloping graphs.  But what happens when it is no longer aligned with the trajectory of the company. Often, the product, code, and infrastructure become a lower priority, while the team moves on to the next exciting new venture. In 2018, Etsy sunset Etsy Wholesale, Etsy Studio, and Etsy Manufacturing, three customer-facing products. In this blog post, we will explore how we sunset these products at Etsy. This process involves a host of stakeholders including marketing, product, customer support, finance and many other teams, but the focus of this blog post is on engineering and the actual execution of the sunset. Process : Pre-code deletion Use Feature Flags and Turn off Traffic Once the communication had been done through emails, in-product announcements, and posts in the user forums, we started focusing on the execution. Prior to the day of each sunset, we used our feature flagging infrastructure to build a switch to disable access to the interface for Wholesale and Manufacturing. Feature flags are an integral part of the continuous deployment process at Etsy. Feature flags reinforce the benefits of small changes and continuous delivery. On the day of the sunset, all we had to do was deploy a one line configuration change and the product was shut off since there was a feature flag that controlled access to these products. A softer transition is often preferable to a hard turn off. For example, we disabled the ability for buyers to create new orders one month before shutting Etsy Wholesale off. That gave sellers a chance to service the orders that remained on-platform, avoiding a mad-dash at the end. Export Data for Users Once the Etsy Wholesale platform was turned off, we created data export files for each seller and buyer with information about every order they received or placed during the five years that the platform was active. Generating and storing these files in one shot allowed us to clean up the wholesale codebase without fear that parts of it would be needed later for exporting data. Set Up Redirects We highly recommend redirects through feature flags,  but a hard DNS redirect might be required in some circumstances. The sunset of Etsy Studio was complicated by the fact that in the middle of this project, etsy.com was being migrated from on-premise hosting to the cloud . To reduce complexity and risk for the massive cloud migration project, Etsy Studio had to be shut off before the migration began. On the day before the cloud migration, a DNS redirect was made to forward any request on etsystudio.com to a special page on etsy.com that explained that Etsy Studio was being shut down. Once the DNS change went live, it effectively shut off Etsy Studio completely. Code Deletion Methodology: Once we confirmed that all three products were no longer receiving traffic, we kicked off the toughest part of the engineering process: deleting all the code. We tried to phase it in two parts, as tightly and loosely integrated products. Integrations in the most sensitive/dangerous spots were prioritized, and safer deletions were done later as we were heading into the holiday season (our busiest time of the year). For Etsy Wholesale and Etsy Manufacturing, we had to remove the code piece-by-piece because it was tightly integrated with other features on the site. For Etsy Studio, we thought we would be able to delete the code in one massive commit. One benefit of our continuous integration system is that we can try things out, fail, and revert without negatively affecting our users. This proved valuable as when we tried deleting the code one massive commit, some unit tests for the Etsy codebase started failing. We realized that small dependencies between the code had formed over time. We decided to delete the code in smaller, easier to test, chunks. a small example of dependencies creeping in where you least expect them. Challenges: Planning (or lack of it) for slowdowns Interdependencies During the process of sunsetting, we didn’t consider how busy other teams would be heading into the holiday season. This slowed down the process of getting code reviews approved. This became especially crucial for us since we were removing and modifying big chunks of code maintained by other teams. There were also several other big projects in flight while we were trying to delete code across our code base and that slowed us down. One example that I already mentioned was cloud migration: we couldn’t shut off Etsy Studio using a config flag and we had to work around it. Commit Size and Deploys To reduce risk, our intention was to keep our commits small, but when trying to delete so much code at once, it’s hard to keep all your commits small. Testing and deploying was a least 50% of our team’s time. Our team made about 413 commits over five months, deleting 275,000 lines of code. That averages out to 630 lines of code deleted per commit, which were frequently deployed one at a time. Compliance We actively think of compliance when building new things, but it is also important to keep in mind compliance requirements when you delete code. Etsy’s SOX compliance system requires that certain files in our codebase are subject to extra controls. When we deploy changes to such files, we need additional reviews and signoffs. We had to do 44 SOX reviews since we did multiple small commits. Each review requires approvals by multiple people and this added on average a few hours to each bit of deletion we did.  Similarly, we considered user privacy and data protection in how to make retention decisions about sunsetted products, how to make data available for export, and how it impacts our terms and policies. Deleting so much code can be a difficult process. We had to revert changes from production at least five times, which, for the most part was simple. One of these five reverts was complicated by a data corruption issue affecting a small population of sellers, which required several days of work to write, test, and run a script to fix the problem. The Outcome We measured success using the following metrics: Code Deletion : 275,000 Test Coverage : We caused a slight drop in test coverage metrics because the Etsy Studio project was well above average, while Etsy Wholesale and Etsy Manufacturing were just slightly below average. System Complexity: Across 8+ Etsy systems: listing management, listing page, payments, search indexes, authentication, member conversations, analytics, member services and our global header user interface, Operational hours : Saved 152 member support hours a month and about 320 engineering hours a month From 1000s of error logs a day for wholesale, to less than 100 (eventually we got this to zero) The roots that three products had in our systems demonstrated the challenges in building and maintaining a standalone product alongside our core marketplace. The many branching pieces of logic that snuck in made it difficult to reuse lots of existing code. By deleting 275,000 lines of code, we were able to reduce tech debt and remove roadblocks for other engineers. Posted by Rachana Kumar on February 1, 2019 Category: api , data , databases , engineering , events , people", "date": "2019-02-1,"},
{"website": "Etsy", "title": "Why Diversity Is Important to Etsy", "author": ["Mike Fisher"], "link": "https://codeascraft.com/2019/01/07/why-diversity-is-important-to-etsy/", "abstract": "Why Diversity Is Important to Etsy Posted by Mike Fisher on January 7, 2019 We recently published our company’s Guiding Principles . These are five common guideposts that apply to all organizations and departments within Etsy. We spent a great deal of time discussing, brainstorming, and editing these. By one estimate, over 30% of the company had some input at some phase of the process. This was a lot of effort by a lot of people but this was important work. These principles need to not only reflect how we currently act but at the same time they need to be aspirational for how we want to behave. These principles will be used in performance assessments, competency matrices, interview rubrics, career discussions, and in everyday meetings to refocus discussions. One of the five principles is focused on diversity and inclusion. The principle states: We embrace differences. Diverse teams are stronger, and inclusive cultures are more resilient. When we seek out different perspectives, we make better decisions and build better products. Why would we include diversity and inclusion as one of our top five guiding principles? One reason is that Etsy’s mission is to Keep Commerce Human . Etsy is a very mission-driven company. Many of our employees joined and remain with us because they feel so passionate about the mission. Every day, we keep commerce human by helping creative entrepreneurs find buyers who become committed fans of the seller’s art, crafts, and collections. The sellers themselves are a diverse group of individuals from almost every country in the world. We would have a hard time coming to work if the way we work, the way we develop products, the way we provide support, etc. isn’t done in a manner that supports this mission. Failing to be diverse and inclusive would fail that mission. Besides aligning with our mission, there are other reasons that we want to have diverse teams. Complicated systems, which feature unpredictable, surprising, and unexpected behaviors have always existed. Complex systems, however, have gone from something found mainly in large systems, such as cities, to almost everything we interact with today. Complex systems are far more difficult to manage than merely complicated ones as subsystems interact in unexpected ways making it harder to predict what will happen. Our engineers deal with complex systems on a daily basis. Complexity is a bit of an overloaded term, but scholarly literature generally categorizes it into three major groups, determined according to the point of view of the observer: behavioral, structural, and constructive. 1 Between the website, mobile apps, and systems that support development, our engineers interact with highly complex systems from all three perspectives every day. Research has consistently shown that diverse teams are better able to manage complex systems. 2 We recently invited Chris Clearfield and András Tilcsik, the authors of Meltdown (Penguin Canada, 2018), to speak with our engineering teams. The book and their talk contained many interesting topics, most based on Charles Perrow’s book, Normal Accident Theory (Princeton University Press; revised ed. 1999). However, perhaps the most important topic was based on a series of studies performed by Evan Apfelbaum and his colleagues at MIT. This study revealed that as much as we’re predisposed to agree with a group, our willingness to disagree increases dramatically if the group is diverse. 3 According to Clearfield and Tilcsik, homogeneity may facilitate “smooth, effortless interactions,” but diversity drives better decisions. Interestingly, it’s the diversity and not necessarily the specific contributions of the individuals themselves, that causes greater skepticism, more open and active dialogue, and less group-think. This healthy skepticism is incredibly useful in a myriad of situations. One such situation is during pre-mortems , where a project team imagines that a project has failed and works to identify what potentially could lead to such an outcome. This is very different from a postmortem where the failure has already occurred and the team is dissecting the failure. Often individuals who have been working on projects for weeks or more are biased with overconfidence and the planning fallacy . This exercise can help ameliorate these biases and especially when diverse team members participate. We firmly believe that when we seek out different perspectives, we make better decisions, build better products, and manage complex systems better. Etsy Engineering is also incredibly innovative. One measure of that is the number of open source projects on our GitHub page and the continuing flow of contributions from our engineers in the open source community. We are of course big fans of open source as Etsy, like most modern platforms, wouldn’t exist in its current form without the myriad of people who have solved a problem and published their code under an open source license. But we also view this responsibility to give back as part of our culture. Part of everyone’s job at Etsy is making others better. It has at times been referred to as “generosity of spirit”, which to engineers means that we should be mentoring, teaching, contributing, speaking, writing, etc. Another measure of our innovation is our experiment velocity. We often run dozens of simultaneous experiments in order to improve the buyer and seller experiences. Under the mission of keeping commerce human, we strive every day to develop and improve products that enable 37M buyers to search and browse through 50M+ items to find just the right, special piece. As you can imagine, this takes some seriously advanced technologies to work effectively at this scale. And, to get that correct we need to experiment rapidly to see what works and what doesn’t. Fueling this innovation is the diversity of our workforce. Companies with increased diversity unlock innovation by creating an environment where ideas are heard and employees can find senior-level sponsorship for compelling ideas. Leaders are twice as likely to unleash value-driving insights if they give diverse voices equal opportunity. 4 So diversity fits our mission, helps manage complex systems, and drives greater innovation, but how is Etsy doing with respect to diversity? More than 50% of our Executive Team and half of our Board of Directors are women. More than 30% of Etsy Engineers identify as women/non-binary and more than 30% are people of color. 5 These numbers are industry-leading, especially when compared to other tech companies who report “tech roles” and not the more narrow category, “engineering” roles. Even though we’re proud of our progress, we’re not fully satisfied. In October 2017, we announced a diversity impact goal to “meaningfully increase representation of underrepresented groups and ensure equity in Etsy’s workforce.” To advance our goal, we are focused on recruiting, hiring, retention, employee development, mentorship, sponsorship, and building an inclusive culture. We have been working diligently on our recruiting and hiring processes. We’ve rewritten job descriptions, replaced some manual steps in the process with third-party vendors, and changed the order of steps in the interview process, all in an effort to recruit and hire the very best engineers without bias. We have also allocated funding and people in order to sponsor and attend conferences focused on underrepresented groups in tech. We’ll share our 2018 progress in Q1 2019. Once engineers are onboard, we want them to bring their whole selves to work in an inclusive environment that allows them to thrive and be their best. One thing that we do to help with this is to promote and partner directly with employee resource groups (ERGs). Our ERGs include Asian Resource Community, Black Resource and Identity Group at Etsy, Jewish People at Etsy, Hispanic Latinx Network, Parents ERG, Queer@Etsy, and Women and NonBinary People in Tech. If you’re not familiar with ERGs, their mission and goals are to create a positive and inclusive workplace culture where employees from underrepresented backgrounds, lifestyles, and abilities have access to programs that foster a sense of community, contribute to professional development, and amplify diverse voices within our organization. Each of these ERGs has an executive sponsor. This ensures that there is a communication channel with upper management. It also highlights the value that we place upon the support that these groups provide. We are also focused on retaining our engineers. One of the things that we do to help in this area is to monitor for discrepancies that might indicate bias. During our compensation, assessment, and promotion cycles, we evaluate for inconsistencies. We perform this analysis both internally and through the use of third parties. Etsy Engineering has been a leader and innovator in the broader tech industry with regard to technology and process. We also want to be leaders in the industry with regards to diversity and inclusion. It is not only the right thing to do but it’s the right thing to do for our business. If this sounds exciting to you, we’d love to talk, just click here to learn more. Endnotes: 1 Wade, J., & Heydari, B. (2014). Complexity: Definition and reduction techniques. In Proceedings of the Poster Workshop at the 2014 Complex Systems Design & Management International Conference. 2 Sargut, G., & McGrath, R. G. (2011). Learning to live with complexity. Harvard Business Review, 89(9), 68–76 3 Apfelbaum EP, Phillips KW, Richeson JA (2014) Rethinking the baseline in diversity research: Should we be explaining the effects of homogeneity? Perspect Psychol Sci 9(3):235–244. 4 Hewlett, S. A., Marshall, M., & Sherbin, L. (2013). How diversity can drive innovation. Harvard Business Review. 5 Etsy Impact Update (August 2018). https://extfiles.etsy.com/Impact/2017EtsyImpactUpdate.pdf Posted by Mike Fisher on January 7, 2019 Category: Uncategorized", "date": "2019-01-7,"},
{"website": "Etsy", "title": "boundary-layer : Declarative Airflow Workflows", "author": ["Kevin McHale"], "link": "https://codeascraft.com/2018/11/14/boundary-layer%e2%80%89-declarative-airflow-workflows/", "abstract": "boundary-layer : Declarative Airflow Workflows Posted by Kevin McHale on November 14, 2018 When Etsy decided last year to migrate our operations to Google Cloud Platform (GCP), one of our primary motivations was to enable our machine learning teams with scalable resources and the latest big-data and ML technologies. Early in the cloud migration process, we convened a cross-functional team between the Data Engineering and Machine Learning Infrastructure groups in order to design and build a new data platform focused on this goal. One of the first choices our team faced was how to coordinate and schedule jobs across a menagerie of new technologies. Apache Airflow (incubating) was the obvious choice due to its existing integrations with GCP, its customizability, and its strong open-source community; however, we faced a number of open questions that had to be addressed in order to give us confidence in Airflow as a long-term solution. First, Etsy had well over 100 existing Hadoop workflows, all written for the Apache Oozie scheduler. How would we migrate these to Airflow? Furthermore, how would we maintain equivalent copies of Oozie and Airflow workflows in parallel during the development and validation phases of the migration, without requiring our data scientists to pause their development work? Second, writing workflows in Airflow (expressed in python as directed acyclic graphs, or DAGs) is non-trivial, requiring new and specialized knowledge. How would we train our dozens of internal data platform users to write Airflow DAGs? How would we provide automated testing capabilities to ensure that DAGs are valid before pushing them to our Airflow instances? How would we ensure that common best-practices are used by all team members? And how would we maintain and update those DAGs as new practices are adopted and new features made available? Today we are pleased to introduce boundary-layer , the tool that we conceived and built to address these challenges, and that we have released to open-source to share with the Airflow community. Introduction: Declarative Workflows Boundary-layer is a tool that enables data scientists and engineers to write Airflow workflows in a declarative fashion, as YAML files rather than as python. Boundary-layer validates workflows by checking that all of the operators are properly parameterized, all of the parameters have the proper names and types, there are no cyclic dependencies, etc. It then translates the workflows into DAGs in python, for native consumption by Airflow. Here is an example of a very simple boundary-layer workflow: name: my-dag-1\r\n\r\ndefault_task_args:\r\n  start_date: '2018-10-01'\r\n\r\noperators:\r\n- name: print-hello\r\n  type: bash\r\n  properties:\r\n    bash_command: \"echo hello\"\r\n- name: print-world\r\n  type: bash\r\n  upstream_dependencies:\r\n  - print-hello\r\n  properties:\r\n    bash_command: \"echo world\" Boundary-layer translates this into python as a DAG with 2 nodes, each consisting of a BashOperator configured with the provided properties, as well as some auto-inserted parameters: # Auto-generated by boundary-layer\r\n\r\nimport os\r\nfrom airflow import DAG\r\n\r\nimport datetime\r\n\r\nfrom airflow.operators.bash_operator import BashOperator\r\n\r\nDEFAULT_TASK_ARGS = {\r\n        'start_date': '2018-10-01',\r\n    }\r\n\r\ndag = DAG(\r\n        dag_id = 'my_dag_1',\r\n        default_args = DEFAULT_TASK_ARGS,\r\n    )\r\n\r\nprint_hello = BashOperator(\r\n        dag = (dag),\r\n        bash_command = 'echo hello',\r\n        start_date = (datetime.datetime(2018, 10, 1, 0, 0)),\r\n        task_id = 'print_hello',\r\n    )\r\n\r\n\r\nprint_world = BashOperator(\r\n        dag = (dag),\r\n        bash_command = 'echo world',\r\n        start_date = (datetime.datetime(2018, 10, 1, 0, 0)),\r\n        task_id = 'print_world',\r\n    )\r\n\r\nprint_world.set_upstream(print_hello) Note that boundary-layer inserted all of the boilerplate of python class imports and basic DAG and operator configuration. Additionally, it validated parameter names and types according to schemas, and applied type conversions when applicable (in this case, it converted date strings to datetime objects). Generators Moving from python-based to configuration-based workflows naturally imposes a functionality penalty. One particularly valuable feature of python-based DAGs is the ability to construct them dynamically: for example, nodes can be added and customized by iterating over a list of values. We make extensive use of this functionality ourselves, so it was important to build a mechanism into boundary-layer to enable it. Boundary-layer generators are the mechanism we designed for dynamic workflow construction. Generators are complete, distinct sub-workflows that take a single, flexibly-typed parameter as input. Each generator must prescribe a mechanism for generating a list of values: for example, lists of items can be retrieved from an API via an HTTP GET request. The python code written by boundary-layer will iterate over the list of generator parameter values and create one instance of the generator sub-workflow for each value. Below is an example of a workflow that incorporates a generator: name: my-dag-2\r\n\r\ndefault_task_args:\r\n  start_date: '2018-10-01'\r\n\r\ngenerators:\r\n- name: retrieve-and-copy-items\r\n  type: requests_json_generator\r\n  target: sense-and-run\r\n  properties:\r\n    url: http://my-url.com/my/file/list.json\r\n    list_json_key: items\r\n\r\noperators:\r\n- name: print-message\r\n  type: bash\r\n  upstream_dependencies:\r\n  - retrieve-and-copy-items\r\n  properties:\r\n    bash_command: echo \"all done\"\r\n---\r\nname: sense-and-run\r\n\r\noperators:\r\n- name: sensor\r\n  type: gcs_object_sensor\r\n  properties:\r\n    bucket: <<item['bucket']>>\r\n    object: <<item['name']>>\r\n- name: my-job\r\n  type: dataproc_hadoop\r\n  properties:\r\n    cluster_name: my-cluster\r\n    region: us-central1\r\n    main_class: com.etsy.jobs.MyJob\r\n    arguments:\r\n    - <<item['name']>> This workflow retrieves the content of the specified JSON file, extracts the items field from it, and then iterates over the objects in that list, creating one instance of all of the operators in the sense-and-run sub-graph per object. Note the inclusion of several strings of the form << ... >> .  These are boundary-layer verbatim strings , which allow us to insert inline snippets of python into the rendered DAG. The item value is the sub-workflow’s parameter, which is automatically supplied by boundary-layer to each instance of the sub-workflow. Also note that generators can be used in dependency specifications, as indicated by the print-message operator’s upstream_dependencies block. Generators can even be set to depend on other generators, which boundary-layer will encode efficiently, without creating a combinatorially-exploding set of edges in the DAG. Advanced features Under the hood, boundary-layer represents its workflows using the powerful networkx library, and this enables a variety of features that require making computational modifications to the graph, adding usability enhancements that go well beyond the core functionality of Airflow itself. A few of the simpler features that modify the graph include before and after sections of the workflow, which allow us to specify a set of operators that should always be run upstream or downstream of the primary list of operators. For example, one of our most common patterns in workflow construction is to put various sensors in the before block, so that it is not necessary to specify and maintain explicit upstream dependencies between the sensors and the primary operators. Boundary-layer automatically attaches these sensors and adds the necessary dependency rules to make sure that no primary operators execute until all of the sensors have completed. Another feature of boundary-layer is the ability to prune nodes out of workflows, while maintaining all dependency relationships between the nodes that remain. This was especially useful during the migration of our Oozie workflows. It allowed us to isolate portions of those workflows for running in Airflow and gradually add more portions in stages, until the workflows were fully migrated, without ever having to create the portioned workflows as separate entities. One of the most useful advanced features of boundary-layer is its treatment of managed resources . We make extensive use of ephemeral, workflow-scoped Dataproc clusters on the Etsy data platform. These clusters are created by Airflow, shared by various jobs that Airflow schedules, and then deleted by Airflow once those jobs are complete. Airflow itself provides no first-class support for managed resources, which can be tricky to configure properly: we must make sure that the resources are not created before they are needed, and that they are deleted as soon as they are not needed anymore, in order to avoid accruing costs for idle clusters. Boundary-layer handles this automatically, computing the appropriate places in the DAG into which to splice the resource-create and resource-destroy operations. This makes it simple to add new jobs or remove old ones, without having to worry about keeping the cluster-create and cluster-destroy steps always installed in the proper locations in the workflow. Below is an example of a boundary-layer workflow that uses Dataproc resources: name: my-dag-3\r\n\r\ndefault_task_args:\r\n  start_date: '2018-10-01'\r\n  project_id: my-gcp-project\r\n\r\nresources:\r\n- name: dataproc-cluster\r\n  type: dataproc_cluster\r\n  properties:\r\n    cluster_name: my-cluster\r\n    region: us-east1\r\n    num_workers: 128\r\n\r\nbefore:\r\n- name: sensor\r\n  type: gcs_object_sensor\r\n  properties:\r\n    bucket: my-bucket\r\n    object: my-object\r\n\r\noperators:\r\n- name: my-job-1\r\n  type: dataproc_hadoop\r\n  requires_resources:\r\n  - dataproc-cluster\r\n  properties:\r\n    main_class: com.etsy.foo.FooJob\r\n- name: my-job-2\r\n  type: dataproc_hadoop\r\n  requires_resources:\r\n  - dataproc-cluster\r\n  upstream_dependencies:\r\n  - my-job-1\r\n  properties:\r\n    main_class: com.etsy.bar.BarJob\r\n- name: copy-data\r\n  type: gcs_to_gcs\r\n  upstream_dependencies:\r\n  - my-job-2\r\n  properties:\r\n    source_bucket: my-bucket\r\n    source_object: my-object\r\n    dest_bucket: your-bucket In this DAG, the gcs_object_sensor runs first, then the cluster is created, then the two hadoop jobs run in sequence, and then the job’s output is copied while the cluster is simultaneously deleted. Of course, this is just a simple example; we have some complex workflows that manage multiple ephemeral clusters, with rich dependency relationships, all of which are automatically configured by boundary-layer. For example, see the figure below: this is a real workflow that runs some hadoop jobs on one cluster while running some ML training jobs in parallel on an external service, and then finally runs more hadoop jobs on a second cluster. The complexity of the dependencies between the training jobs and downstream jobs required boundary-layer to insert several flow-control operators in order to ensure that the downstream jobs start only once all of the upstream dependencies are met. Conversion from Oozie One of our primary initial concerns was the need to be able to migrate our Oozie workflows to Airflow. This had to be an automated process, because we knew we would have to repeatedly convert workflows in order to keep them in-sync between our on-premise cluster and our GCP resources while we developed and built confidence in the new platform. The boundary-layer workflow format is not difficult to reconcile with Oozie’s native configuration formats, so boundary-layer is distributed with a parser that does this conversion automatically. We built tooling to incorporate the converter into our CI/CD processes, and for the duration of our cloud validation and migration period, we maintained perfect fidelity between on-premise Oozie and cloud-based Airflow DAGs. Extensibility A final requirement that we targeted in the development of boundary-layer is that it must be easy to add new types of operators, generators, or resources. It must not be difficult to modify or add to the operator schemas or the configuration settings for the resource and generator abstractions. After all, Airflow’s huge open-source community (including several Etsy engineers!) ensures that its list of supported operators is growing practically every day. In addition, we have our own proprietary set of operators for Etsy-specific purposes, and we must keep the configurations for these out of the public boundary-layer distribution. We satisfied these requirements via two design choices. First, every operator, generator, or resource is represented by a single configuration file, and these files get packaged up with boundary-layer. Adding a new operator/generator/resource is accomplished simply by adding a new configuration file. Here is an example configuration, in this case for the Airflow BashOperator : name: bash\r\noperator_class: BashOperator\r\noperator_class_module: airflow.operators.bash_operator\r\nschema_extends: base\r\n\r\nparameters_jsonschema:\r\n  properties:\r\n    bash_command:\r\n      type: string\r\n    \r\n    xcom_push:\r\n      type: boolean\r\n    \r\n    env:\r\n      type: object\r\n      additionalProperties:\r\n        type: string\r\n    \r\n    output_encoding:\r\n      type: string\r\n  \r\n  required:\r\n  - bash_command\r\n  \r\n  additionalProperties: false We use standard JSON Schemas to specify the parameters to the operator, and we use a basic single-inheritance model to centralize the specification of common parameters in the BaseOperator , as is done in the Airflow code itself. Second, we implemented a plugin mechanism based on python’s setuptools entrypoints . All of our internal configurations are integrated into boundary-layer via plugins. We package a single default plugin with boundary layer that contains configurations for common open-source Airflow operators. Other plugins can be added by packaging them into separate python packages, as we have done internally with our Etsy-customized plugin. The plugin mechanism has grown to enable quite extensive workflow customizations, which we use at Etsy in order to enable the full suite of proprietary modifications used on our platform. Conclusion The boundary-layer project has been a big success for us. All of the nearly-100 workflows that we deploy to our production Airflow instances are written as boundary-layer configurations, and our deployment tools no longer even support python-based DAGs. Boundary-layer’s ability to validate workflow configurations and abstract away implementation details has enabled us to provide a self-service Airflow solution to our data scientists and engineers, without requiring much specialized knowledge of Airflow itself. Over 30 people have contributed to our internal Airflow workflow repository, with minimal process overhead ( Jenkins is the only “person” who must approve pull requests), and without having deployed a single invalid DAG. We are excited to release boundary-layer to the public, in hopes that other teams find it similarly useful. We are committed to supporting it and continuing to add new functionality, so drop us a github issue if you have any requests. And of course, we welcome community contributions as well! Posted by Kevin McHale on November 14, 2018 Category: data , engineering , infrastructure", "date": "2018-11-14,"},
{"website": "Etsy", "title": "Double-bucketing in A/B Testing", "author": ["David Schott"], "link": "https://codeascraft.com/2018/11/07/double-bucketing-in-ab-testing/", "abstract": "Double-bucketing in A/B Testing Posted by David Schott on November 7, 2018 Previously, we’ve posted about the importance we put in Etsy’s experimentation systems for our decision-making process. In a continuation of that theme, this post will dive deep into an interesting edge case we discovered. We ran an A/B test which required a 5% control variant and 95% treatment variant rather than the typical split of 50% for control and treatment variants.  Based on the nature of this particular A/B test, we expected a positive change for conversion rate, which is the percent of users that make a purchase. At the conclusion of the A/B test, we had some unexpected results. Our A/B testing tool, Catapult, showed the treatment variant “losing” to the control variant. Catapult was showing a negative change in conversion rate when we’d expect a positive rate of change. Due to these unexpected negative results, the Data Analyst team investigated why this was happening. This quote summarizes their findings The control variant “benefited” from double-bucketing because given its small size (5% of traffic), receiving an infusion of highly engaged browsers from the treatment provided an outsized lift on its aggregate performance. With the double-bucketed browsers excluded, the true conversion rate of change is positive which is the results that we expected from the A/B test.  Just 0.02% of the total browsers in the A/B test were double-bucketed. This small percentage of the total browsers had a large impact on the A/B test results.  This post will cover the details of why that occurred. Definition of Double-bucketing So what exactly is double-bucketing? In an A/B test, a user is shown either the control or treatment experience. The process to determine which variant the user falls into is called ‘bucketing’. Normally, a user experiences only the control or only the treatment; however in this A/B test, there was a tiny percentage of users who experienced both variants. We call this error in bucketing ‘double-bucketing’. Typical user 50/50 bucketing for an A/B test puts ½ of the users into the control variant and ½ into the treatment variant. Those users stay in their bucketed variant. We calculate metrics and run statistical tests by summing all the data for the users in each variant. However, the double-bucketing error we discovered would place the last 2 users in both control and treatment variants, as shown below. Now those users’ data is counted in both variants for statistics on all metrics in the experiment. How browsers are bucketed Before discussing the cases of double-bucketing that we found, it helps to have a high-level understanding of how A/B test bucketing works at Etsy. For etsy.com web requests, we use an unique identifier from the user’s browser cookie which we refer to as “browser id”. Using the string value from the cookie, our clickstream data logic, named EventPipe, sets the browser id property on each event. Bucketing is determined by a hash. First we concatenate the name of the A/B test and the browser id.  The name of the A/B test is referred to as the “configuration flag”. That string is hashed using SHA-256 and then converted to an integer between 0 and 99. For a 50% A/B test, if the value is < 50, the browser is bucketed into the treatment variant. Otherwise, the browser is in the control variant.  Because the hashing function is deterministic, the user should be bucketed into the same variant of an experiment as long as the browser cookie remains the same. EventPipe adds the configuration flag and bucketed variant information to the “ab” property on events. For an A/B test’s statistics in Catapult, we filter by the configuration flag and then group by the variant. This bucketing logic is consistent and has worked well for our A/B testing for years.  Although occasionally some experiments wound up with small numbers of double-bucketed users, we didn’t detect a significant impact until this particular A/B test with a 5% control. Some Example Numbers (fuzzy math) We’ll use some example numbers with some fuzzy math to understand how the conversion rate was effected so much by only 0.02% double-bucketed browsers. For most A/B tests, we do 50/50 bucketing between the control variant and treatment variants. For this A/B test, we did a 5% control which puts 95% in the treatment. If we start with 1M browsers, our 50% A/B test has 500K browsers in both control and treatment variants. Our 5% control A/B test has 50K browsers in the control variant and 950K in the treatment variant. Let’s assume a 10% conversion rate for easy math. For the 50% A/B test, we have 50K converted browsers in both the control and treatment variant. Our 5% control A/B test has 5K converted browsers in the control variant and 95K in the treatment variant. For the next step, let’s assume 1% of the converting browsers are double-bucketed. When we add the double-bucketed browsers from the opposite variant to both the numerator and denominator, we get a new conversion rate. For our 50% A/B test, that is 50,500 converted browsers in both the control and treatment variants. The new conversion rate is slightly off from the expected conversion rate but only by 0.1%. For our 5% control A/B test, the treatment variant’s number of converted browsers only increased by 50 browsers from 95,000 to 95,050. The treatment variant’s new conversion rate still rounds to the expected 10%. But for our 5% control A/B test, the control variant’s number of converted browsers jumps from 5000 to 5950 browsers. This causes a huge change in the control variant’s conversion rate – from 10% to 12% – while the treatment variant’s conversion rate was unchanged. Cases of Double-bucketing Once we understood that double-bucketing was causing these unexpected results, we started digging into what cases led to double-bucketing of individual browsers. We found two main cases. Since conversion rates were being affected, unsurprisingly both cases involved checkout. Checkout from new device Checkout from Pattern (individual seller sites hosted by Etsy on a different domain) Checkout from new device When browsing etsy.com while signed out, you can add listings to your cart. Once you click the “Proceed to checkout” button, you are prompted to sign in. You get a sign in screen similar to this. After you sign in, if we have never seen your browser before, then we email you a security alert that you’ve been signed in from a new device. This is a wise security practice and pretty standard across the internet. Many years ago, we were doing A/B testing on emails which were all sent from offline jobs. Gearman is our framework for running offline jobs based on http://gearman.org . In Gearman, we have no access to cookies and thus cannot get the browser id, but we do have the email address. So override logic was added deep in email template logic to bucket by email address rather than by browser id. This worked perfectly. But the security email isn’t sent from Gearman; it is coming from the sign in request. So now our bucketing for the same browser id has this different bucketing based on email address rather than browser id. This worked perfectly for A/B testing in emails sent by Gearman, but the logic applied to all emails, not just those sent by Gearman. Even though the security email is sent by the sign in request (not Gearman), the logic updated the bucketing ID to be the user’s email address rather than the browser id so that the browser might be bucketed into two different variants (once using the browser id and once using the email address). Since we are no longer using that email system for A/B testing, we were able to simply remove the override call. Pattern Checkout Pattern is Etsy’s tool that sellers use to create personalized, separate websites for their businesses.  Pattern shops allow listings to be added to your cart while on the shop’s patternbyetsy.com domain. The checkout occurs on etsy.com domain instead of the patternbyetsy.com domain. Since the value from the user’s browser cookie is what we bucket on and we cannot share cookies across domains, we have two different hashes used for bucketing. In order to attribute conversions to Pattern, we have logic to override the browser id with the value from the patternbyetsy.com cookie during the checkout process on etsy.com. This override logic works for attributing conversions; however during sign in some bucketing happens prior to the execution of the override logic by the controllers. For this case, we chose to remove bucketing data for Pattern visits as this override caused the bucketing logic to put the same user into both the control and treatment variants. Conclusions Here is a dashboard of double-bucketed browsers per day that helped us track our fixes of double-bucketing. Dashboards are good Use dashboards to show the problem, track progress and monitor that problems don’t reoccur. Overrides bite Both of the cases involved use of override functions. Often, overrides are a quick and convenient way to solve a local problem, but in a complex system, they can have unintended consequences that manifest in unexpected ways that may not be immediately apparent. Cross-domain is a difficult problem set Cookies cannot be shared across domains. As a result, it’s a challenge to maintain consistency across domains, both in terms of user experience and in how data are joined and aggregated in downstream processing. Data quality issues are tricky to debug Data quality issues are not uncommon in complex data systems, and it can be a challenge to fully understand the impact. An issue that may seem benign in one case can become significant for a slightly different case. In addition, finding the cause of a data quality issue can be tricky and require significant investigation. As mentioned above, we used data analysis and monitoring to identify patterns and narrow in on the diagnostic . Posted by David Schott on November 7, 2018 Category: data Tags: abtesting , analytics , experimentation Related Posts Posted by Callie McRee and Kelly Shen on 03 Oct, 2018 How Etsy Handles Peeking in A/B Testing", "date": "2018-11-7,"},
{"website": "Etsy", "title": "Capacity planning for Etsy’s web and API clusters", "author": ["Daniel Schauenberg"], "link": "https://codeascraft.com/2018/10/23/capacity-planning-for-etsys-web-and-api-tiers/", "abstract": "Capacity planning for Etsy’s web and API clusters Posted by Daniel Schauenberg on October 23, 2018 Capacity planning for the web and API clusters powering etsy.com has historically been a once per year event for us.The purpose is to gain an understanding of the capacity of the heterogeneous mix of hardware in our datacenters that make up the clusters. This was usually done a couple of weeks before the time we call Slush. Slush (a word play on code freeze) is the time range of approximately 2 months around the holidays where we deliberately decide to slow down our rate of change on the site but not actually stop all development. We do this to recognize the fact that the time leading up to the holidays is the busiest and most important time for a lot of our sellers and any breakage results in higher than usual impact on their revenue. This also means it’s the most important time for us to get capacity estimates right and make sure we are in a good place to serve traffic throughout the busy holiday season. During this exercise of forecasting and planning capacity someone would collect all relevant core metrics (the most important one being requests per second on our Apache httpd infrastructure) from our Ganglia instances and export them to csv. Those timeseries data would then be imported into something that would give us a forecasting model. Excel, R, and python scripts are examples of tools that have been used in previous years for this exercise. After a reorg of our systems engineering department in 2017, Slush that year was the first time the newly formed Compute team was tasked with capacity planning for our web and api tiers. And as we set out to do this, we had three goals: Determine capacity needs Make it an easily repeatable process Do it more frequently than once per year First we started with a spreadsheet to track everything that we would be capacity planning for. Then we got an overview of what we had in terms of hardware serving those tiers. We got this from running a knife search like this: knife search node \"roles:WebBase\" -a cpu.0.model_name -a cpu.cores -F json and turning it into CSV via a ruby script, so we could have it in the spreadsheet as well. Now that we had the hardware distribution of our clusters, we gave each model a score so we could rank them and derive performance differences and loadbalancer weighting scores if needed. These performance scores are a rough heuristic to allow relative comparison of different CPUs. It takes into account core count, clock speed and generational improvements (assuming a 20% improvement between processors for the same clock speed and core count). It’s not an exact science at this point but a good enough measure to get us a useful idea of how to compare different hardware generations against each other. Then we assigned each server a performance score, based on that heuristic. Next up was the so called “squeeze testing”. The performance scores weren’t particularly helpful without knowing what they mean in actual work a server with that score can do on different cluster types. Request work on our frontend web servers is very different than the work on our component api tier for example. So a performance score of 50 means something very different depending on which cluster we are talking about. Squeeze testing is the capacity planning exercise of trying to see how much performance you can squeeze out of a service, usually by gradually increasing the amount of traffic it receives and how much it can handle before exhausting its resources. In the scenario of an established cluster this is often hard to do as we can’t arbitrarily add more traffic to the site. That’s why we turned the opposite dial and removed resources (i.e. servers) from a cluster until the cluster (almost) started to not serve in an appropriate manner anymore. So for our web and api clusters this meant removing nodes from the serving pools until they drop to about 25% idle CPU and noting the number of requests per second they are serving at this point. 20% idle CPU is a threshold on those tiers where we start to see performance decrease due to the rest of the CPU time being used for tasks like context switching and other non application workloads. That means stopping at 25% gives us headroom for some variance in this type of testing and also means we weren’t hurting actual site performance while doing the squeeze testing. Now that we got the number of requests per second we could process based on the performance score, the only thing we were missing was knowing how much traffic we expect to see in the coming months. This meant in the past – as mentioned above – that we would download timeseries data from Ganglia for requests per second for each cluster for every datacenter we had nodes in. Then that data needed to be combined to get the total sum of requests we have been serving. Then we would take that data and stick it into Excel and try a couple of Excel’s curve fitting algorithms, see which looked best and take the forecasting results based on fit. We have also used R or python for that task in previous years. But it was always a very handcrafted and manual process. So this time around we wrote a tool to do all this work for us called “Ausblick”. It’s based on Facebook’s prophet and automatically pulls in data from Ganglia based on host and metric regexes, combines the data for each datacenter and then runs forecasting on the timeseries and shows us a nice plot for it. We can also give it a base value and list of hosts with perfscores and ausblick will draw the current capacity of the cluster into the plot as a horizontal red line. Ausblick runs in our Kubernetes cluster and all interactions with the tool are happening through its REST API and an example request looks like this: % cat conapi.json\r\n{ \"title\": \"conapi cluster\",\r\n  \"hostregex\": \"^conapi*\",\r\n  \"metricsregex\": \"^apache_requests_per_second\",\r\n  \"datacenters\": [\"dc1\",\"dc2\"],\r\n  \"rpsscore\": 9.5,\r\n  \"hosts\": [\r\n    [\"conapi-server01.dc1.etsy.com\", 46.4],\r\n    [\"conapi-server02.dc1.etsy.com\", 46.4],\r\n    [\"conapi-server03.dc2.etsy.com\", 46.4],\r\n    [\"conapi-server04.dc1.etsy.com\", 27.6],\r\n    [\"conapi-server05.dc2.etsy.com\", 46.4],\r\n    [\"conapi-server06.dc2.etsy.com\", 27.6],\r\n    [\"conapi-server06.dc1.etsy.com\", 46.4]\r\n  ]\r\n}\r\n% curl -X POST http://ausblick.etsycorp.com/plan -d @conapi.json --header \"Content-Type: application/json\"\r\n{\"plot_url\": \"/static/conapi_cluster.png\"}% In addition to this API we wrote an integration for our Slack bot to easily generate a new forecast based on current data. Ausblick Slack integration And to finish this off with a bunch of graphs, here is what the current forecasting looks like for some of our internal api tiers, that are backing etsy.com: Ausblick forecast for conapi cluster Ausblick forecast for compapi cluster Ausblick has allowed us to democratize the process of capacity forecasting to a large extent and given us the ability to redo forecasting estimates at will. We used this process successfully for last year’s Slush and are in the process of adapting it to our cloud infrastructure after our recent migration of the main etsy.com components to GCP . Posted by Daniel Schauenberg on October 23, 2018 Category: engineering , infrastructure , monitoring , performance", "date": "2018-10-23,"},
{"website": "Etsy", "title": "Etsy’s experiment with immutable documentation", "author": ["Paul-Jean Letourneau"], "link": "https://codeascraft.com/2018/10/10/etsys-experiment-with-immutable-documentation/", "abstract": "Etsy’s experiment with immutable documentation Posted by Paul-Jean Letourneau on October 10, 2018 Introduction Writing documentation is like trying to hit a moving target. The way a system works changes constantly, so as soon as you write a piece of documentation for it, it starts to get stale. And the systems that need docs the most are the ones being actively used and worked on, which are changing the fastest. So the most important docs go stale the fastest! 1 Etsy has been experimenting with a radical new approach: immutable documentation . Woah, you just got finished talking about how documentation goes stale! So doesn’t that mean you have to update it all the time? How could you make documentation read-only? How docs go stale Let’s back up for a sec. When a bit of a documentation page becomes outdated or incorrect, it typically doesn’t invalidate the entire doc (unless the system itself is deprecated). It’s just a part of the doc with a code snippet, say, which is maybe using an outdated syntax for an API. For example, we have a command-line tool called dbconnect that lets us query the dev and prod databases from our VMs. Our internal wiki has a doc page that discusses various tools that we use to query the dbs. The part that discusses ‘dbconnect’ goes something like: Querying the database via dbconnect ...\r\n\r\n((section 1))\r\ndbconnect is a script to connect to our databases and query them. [...]\r\n\r\n((section 2))\r\nThe syntax is:\r\n\r\n% dbconnect <shard> Section 1 gives context about dbconnect and why it exists, and section 2 gives tactical details of how to use it. Now say a switch is added so that dbconnect --dev <shard> queries the dev db, and dbconnect --prod <shard> queries the prod db. Section 2 above now needs to be updated, because it’s using outdated syntax for the dbconnect command. But the contextual description in section 1 is still completely valid. So this doc page is now technically stale as a whole because of section 2, but the narrative in section 1 is still very helpful! In other words, the parts of the doc that’s most likely to go stale are the tactical, operational details of the system. How to use the system is constantly changing. But the narrative of why the system exists and the context around it is less likely to change quite so quickly. How to use the system is constantly changing. But the narrative of why the system exists and the context around it is less likely to change quite so quickly. Docs can be separated into how-docs and why-docs Put another way: ‘code tells how, docs tell why’ 2 . Code is constantly changing, so the more code you put into your docs, the faster they’ll go stale. To codify this further, let’s use the term “ how-doc ” for operational details like code snippets, and “ why-doc ” for narrative, contextual descriptions 3 . We can mitigate staleness by limiting the amount we mix the how-docs with the why-docs. We can mitigate staleness by limiting the amount we mix the how-docs with the why-docs. Documenting a command using Etsy’s FYI system At Etsy we’ve developed a system for adding how-docs directly from Slack . It’s called “FYI”. The purpose of FYI is to make documenting tactical details — commands to run, syntax details, little helpful tidbits — as frictionless as possible. FYI is a system for adding how-docs directly from Slack. Here’s how we’d approach documenting dbconnect using FYIs 4 : Kaley was searching the wiki for how to connect to the dbs from her VM, to no avail. So she asks about it in a Slack channel: When she finds the answer, she adds an FYI using the ?fyi command (using our irccat integration in Slack 5 ): Jason sees Kaley add the FYI and mentions you can also use dbconnect to list the databases: Kaley then adds the :fyi: Slack reaction (reacji) to his comment to save it as an FYI: A few weeks later, Paul-Jean uses the FYI query command ?how to search for info on connecting to the databases, and finds Kaley’s FYI 6 : He then looks up FYIs mentioning dbconnect specifically to discover Jason’s follow-up comment: But he notices that the dbconnect command has been changed since Jason’s FYI was added: there is now a switch to specify whether you want dev or prod databases. So he adds another FYI to supplement Jason’s: Now ?how dbconnect returns Paul-Jean’s FYI first, and Jason’s second: FYIs trade completeness for freshness Whenever you do a ?how query, matching FYIs are always returned most recent first. So you can always update how-docs for dbconnect by adding an FYI with the keyword “dbconnect” in it. This is crucial, because it means the freshest docs always rise to the top of search results . FYIs are immutable , so Paul-Jean doesn’t have to worry about changing any FYIs created by Jason. He just adds them as he thinks of them, and the timestamps determine the priority of the results. How-docs change so quickly, it’s easier to just replace them than try to edit them. So they might as well be immutable. How-docs change so quickly, it’s easier to just replace them than try to edit them. So they might as well be immutable. Since every FYI has an explicit timestamp , it’s easy to gauge how current they are relative to API versions, OS updates, and other internal milestones. How-docs are inherently stale, so they might as well have a timestamp showing exactly how stale they are . How-docs are inherently stale, so they might as well have a timestamp showing exactly how stale they are. The tradeoff is that FYIs are just short snippets. There’s no room in an FYI to add much context. In other words, FYIs mitigate staleness by trading completeness for freshness . FYIs mitigate staleness by trading completeness for freshness Since FYIs lack context, there’s still a need for why-docs (eg a wiki page) about connecting to dev/prod dbs, which mentions the dbconnect command along with other relevant resources. But if the how-docs are largely left in FYIs, those why-docs are less likely to go stale. So FYIs allow us to decouple how-docs from why-docs . The tactical details are probably what you want in a hurry. The narrative around them is something you sit back and read on a wiki page. FYIs allow us to decouple how-docs from why-docs What FYIs are To summarize, FYIs are: How-docs : code snippets, API details, or helpful tips that explain how to use a system Fresh : searching FYIs gives most recent matches first, and adding them is easy Time-stamped : every FYI has an explicit timestamp, so you know exactly how stale it is Immutable : instead of editing an FYI you just add another one with more info Discoverable : using the ?how command Short : about the length of a sentence Unstructured : just freeform text Collaborative : FYIs quickly share knowledge within or across teams Immediate : use ?fyi or just tag a slack message with the :fyi: reaction What FYIs are NOT Similarly, FYIs are NOT: Why-docs : FYIs are short, helpful tidbits, not overviews, prose or narratives Wiki pages or READMEs: why-docs belong on the wiki or in READMEs Code comments : a code comment explains what your code does better than any doc Conclusions Etsy has recognized that technical documentation is a mixture of two distinct types: a narrative that explains why a system exists (“why-docs”), and operational details that describe how to use the system (“how-docs”). In trying to overcome the problem of staleness, the crucial observation is that how-docs typically change faster than why-docs do. Therefore the more how-docs are mixed in with why-docs in a doc page, the more likely the page is to go stale. We’ve leveraged this observation by creating an entirely separate system to hold our how-docs. The FYI system simply allows us to save Slack messages to a persistent data store. When someone posts a useful bit of documentation in a Slack channel, we tag it with the :fyi: reacji to save it as a how-doc. We then search our how-docs directly from Slack using a bot command called ?how . FYIs are immutable: to update them, we simply add another FYI that is more timely and correct. Since FYIs don’t need to contain narrative, they’re easy to add, and easy to update. The ?how command always returns more recent FYIs first, so fresher matches always have higher priority. In this way, the FYI system combats documentation staleness by trading completeness for freshness. We believe the separation of operational details from contextual narrative is a useful idea that can be used for documenting all kinds of systems. We’d love to hear how you feel about it! And we’re excited to hear about what tooling you’ve built to make documentation better in your organization. Please get in touch and share what you’ve learned. Documentation is hard! Let’s make it better! Acknowledgements The FYI system was designed and implemented by Etsy’s FYI Working Group: Paul-Jean Letourneau, Brad Greenlee, Eleonora Zorzi, Rachel Hsiung, Keyur Govande, and Alec Malstrom. Special thanks to Mike Lang, Rafe Colburn, Sarah Marx, Doug Hudson, and Allison McKnight for their valuable feedback on this post. References From “The Golden Rules of Code Documentation” : “It is almost impossible without an extreme amount of discipline, to keep external documentation in-sync with the actual code and/or API.” Derived from “code tells what, docs tell why” in this HackerNoon post . The similarity of the terms “how-doc” and “why-doc” to the term here-doc is intentional. For any given command, a here-doc is used to send data into the command in-place, how-docs are a way to document how to use the command, and why-docs are a description of why the command exists to begin with. You can replicate the FYI system with any method that allows you save Slack messages to a predefined, searchable location. So for example, one could simply install the Reacji Channeler bot , which lets you assign a Slack reacji of your choosing to cause the message to be copied to a given channel. So you could assign an “fyi” reacji to a new channel called “#fyi”, for example. Then to search your FYIs, you would simply go to the #fyi channel and search the messages there using the Slack search box. When the :fyi: reacji is added to a Slack message (or the ?fyi irccat command is used), an outgoing webhook sends a POST request to irccat.etsy.com with the message details. This triggers a PHP script to save the message text to a SQLite database, and sends an acknowledgement back to the Slack incoming webhook endpoint. The acknowledgement says “OK! Added your FYI”, so the user knows their FYI has been successfully added to the database. Searching FYIs using the ?how command uses the same architecture as for adding an FYI, except the PHP script queries the SQLite table, which supports full-text search via the FTS plugin . Posted by Paul-Jean Letourneau on October 10, 2018 Category: engineering , infrastructure , philosophy", "date": "2018-10-10,"},
{"website": "Etsy", "title": "How Etsy Localizes Addresses", "author": ["Danielle Grenier"], "link": "https://codeascraft.com/2018/09/26/how-etsy-localizes-addresses/", "abstract": "How Etsy Localizes Addresses Posted by Danielle Grenier on September 26, 2018 Imagine you’re browsing the web from your overpriced California apartment one day and you find a neat new website with some really cool stuff. You pick out a few items, add them to your cart, and start the checkout process. You get to the part where they ask for your shipping address and this is the form you see: It starts off easy – you fill in your name, your street, your apartment number, and then you reach the field labelled “Post Town”. What is a “Post Town”? Huh. Next you see “County”. Well, you know what a county is, but since when do you list it in your address? Then there’s “Postal code”. You might recognize it as what the US calls a “zip code”, but it’s still confusing to see. So now you don’t really know what to do, right? Where do you put your city, or your state? Do you just cram your address into the form however you can and hope that you get your order? Or do you abandon your cart and decide not to buy anything from this site? This is in fact a fake form I put together for this exercise, but it demonstrates exactly how a lot of our members outside the United States felt when we showed them  a US-centric address form. Etsy does business in more than 200 countries, and it’s important that our members feel comfortable and confident when providing their shipping address and placing orders. They need to know that their orders are going to reach them. Furthermore, we ask for several addresses when new sellers join Etsy, like billing and banking addresses, and we want them to feel comfortable providing this information, and confident that we will know how to support them and their needs. In this post I’ll cover: Where we started: Generic Address Forms What are international addresses supposed to look like? Building a Localized Address Experience Where we started: Generic Address Forms When we first started this project, our address forms were designed in a variety of technologies, with minimal localization. Most address forms worked well for US members, and we displayed the appropriate forms for Canadian and Australian members, but members in every other country were shown a generic, unlocalized address form. United States Our address form for US members looks just fine – all the fields we expect to see are there, and there aren’t any unexpected fields. Germany (et al) This is the form we showed for Germany (and most other countries). We asked for a postal code, and a state, province, or region. For someone unfamiliar with German addresses, this might seem fine at first. But what if I told you that German addresses don’t have states? Even worse, in this form, state is a required field! This form confused a lot of our German members, and they ended up putting any number of things in that field, just to be able to move forward. This led us to saved addresses like: Ets Y. Crafter 123 Main Street Berlin, Berlin 12435 Germany In this case, the member just entered the city in the state field. This wasn’t the worst situation, and anything shipped to this address would probably arrive just fine. But what about this address? Ets Y. Crafter 123 Main Street Berlin, California 12435 Germany Sometimes the member entered a US state in the state field. This confused sellers and postal workers alike – we had real life examples of packages being shipped to the US because a state was included, even though the country listed was something totally different! Ets Y. Crafter 123 Main Street Berlin, dasdsaklklg Germany Members could even enter gibberish in the state field. Again, this was a bit confusing for sellers and the postal service. What are non-US addresses supposed to look like? Here’s an example of a German address: Ets Y. Crafter 123 Main Street 12435 BERLIN Germany If you wanted to mail something to this address, you’d need to specify the recipient, the street name and number, the postal code and city, and the country. We could have used this address to determine an address format for Germany, but what about the almost 200 other countries Etsy supports? We didn’t really want look up sample addresses for each country and guess at what the address format should be. Thankfully, we didn’t have to do that. We drew on 3 different sources when compiling a list of address formats for different countries. The majority of our data came from Google’s Address Data Service . We also did a little sanity checking using the Universal Postal Union (or UPU) , an international organization based out of Switzerland that basically tries to make sure all the different postal carriers in the world can work together. We also consulted our international staff to make sure the data for certain markets, like Germany and Japan, was correct. So what kind of data did we get? The most important piece of information we got is a format string that tells us: Which fields are part of the address The order they should be displayed in Any delimiters (spaces, commas, newlines) between each field We also got other formatting data, including: The ISO standard country code An array indicating which fields are required for a complete address Administrative area (state) data Regular expression (regex) patterns for postal code validation Detail on what each field is called (e.g. State, Province, Region, or Prefecture) Here’s what the formatting data looks like for a couple of different countries, and how that data is used to assemble the localized address form for that country. United States $format = [\r\n  209 => [\r\n      'format' => '%name\\n%first_line\\n%second_line\\n%city, %state %zip\\n%country_name',\r\n      'required_fields' => [\r\n          'name',\r\n          'first_line',\r\n          'city',\r\n          'state',\r\n          'zip',\r\n      ],\r\n      'uppercase_fields' => [\r\n          'city',\r\n          'state',\r\n      ],\r\n      'name' => 'UNITED STATES',\r\n      'administrative_area_type' => 'state',\r\n      'locality_type' => 'city',\r\n      'postal_code_type' => 'zip',\r\n      'postal_code_pattern' => '(\\\\d{5})(?:[ \\\\-](\\\\d{4}))?',\r\n      'administrative_areas' => [\r\n          'AL' => 'Alabama',\r\n          'AK' => 'Alaska',\r\n          ...\r\n          'WI' => 'Wisconsin',\r\n          'WY' => 'Wyoming',\r\n      ],\r\n      'iso_code' => 'US',\r\n  ]\r\n]; Germany $format = [\r\n  91 => [\r\n    'format' => '%name\\n%first_line\\n%second_line\\n%zip %city\\n%country_name',\r\n    'required_fields' => [\r\n        'name',\r\n        'first_line',\r\n        'city',\r\n        'zip',\r\n    ],\r\n    'uppercase_fields' => [\r\n        'city',\r\n    ],\r\n    'name' => 'GERMANY',\r\n    'locality_type' => 'city',\r\n    'postal_code_type' => 'postal',\r\n    'postal_code_pattern' => '\\\\d{5}',\r\n    'input_format' => '%name\\\\n%first_line\\\\n%second_line\\\\n%zip\\\\n%city\\\\n%country_name',\r\n    'iso_code' => 'DE',\r\n  ],\r\n]; So, now we had all this great information on what addresses were supposed to look like for almost 200 countries. How did we take this data and turn it into a localized address experience? Building a Localized Address Experience A complete localized address experience requires two components: address input and address display. In other words, our members need to be able to add and edit their addresses using a form that makes sense to them, and they need to see their address displayed in a format that they’re familiar with. Address Input You’ve already seen what our unlocalized address form looked like, but here’s a quick reminder of what German members were seeing when they were entering their addresses. This was a static form, meaning we had a big template with a bunch of <input> tags, and a little bit of JavaScript to handle interactions with the form. For a few select countries, like Canada and Australia, we added conditional statements to the template, swapping in different state or province lists as necessary. It made for a pretty messy template. When deciding how we wanted to handle address forms, we knew that we didn’t want to have a template with enough conditional statements to handle hundreds of different address formats. Instead, we decided on a compositional approach. Every address form starts with a country <select> input. This prompts the member to select their country first, so we can render the localized form before they start entering their address. We identified all the possible fields that could be in an address form: first_line, second_line, city, state, zip, and country, and recognized that all these fields could be rendered using just a few generic templates. These templates would allow us to specify custom labels, indicate whether or not the field was required, display validation errors, and render other custom content by providing different data when we render the template for each field. Text Input A pretty basic text input can be used for the first line, second line, city, and zip address fields, as well as the state field, depending on the country. Here’s what our text input template looks like: State Select Input For the countries for which we have state (aka administrative area) data, we created a select input template: With these templates, and the appropriate address formatting data, we can generate address input forms for almost 200 countries. Address Display Displaying localized addresses was also handled by a static template before this project. It was based on the US address format, and was written with the assumption that all addresses had the same fields as US addresses. It looked something like this: <p>{{name}}</p>\r\n<p>{{first_line}}</p>\r\n<p>{{second_line}}</p>\r\n<p>{{city}}, {{state}} {{zip}}</p>\r\n<p>{{country_name}}</p> While this wasn’t as problematic as the way we were handling address input, it was still not ideal. Addresses for international members would be displayed incorrectly, causing varying levels of confusion. For German members, the difference wasn’t too bad: But for members in countries like Japan, the difference was pretty significant: To localize address display, we went with a compositional approach again, treating each field as a separate piece, and then combining them in the order specified, and using the delimiters indicated by the format string. <span class=\"name\">Ets Y. Crafter</span><br>\r\n<span class=\"first-line\">123 Main Street</span><br/>\r\n<span class=\"zip\">12345</span> <span class=\"city\">BERLIN</span><br/>\r\n<span class=\"country-name\">Germany</span><br/> We further enhanced our address display library by creating a PHP class that could render a localized address in plaintext, or fully customizable HTML, to support the numerous ways addresses are displayed throughout Etsy and our internal tools. Conclusion No more confusing address forms! While we’re nowhere near finished with localized addresses, we’ve made really great progress so far. We’re hopeful that our members will enjoy their experience just a little bit more now that they have fewer concerns when it comes to addresses. There is a lot more that we learned from this project (like how we replaced the unlocalized address forms with the localized address form on the entire site!), so keep an eye out for future blog posts. Thanks for reading! Posted by Danielle Grenier on September 26, 2018 Category: Uncategorized", "date": "2018-09-26,"},
{"website": "Etsy", "title": "Modeling User Journeys via Semantic Embeddings", "author": ["Nishan Subedi"], "link": "https://codeascraft.com/2018/07/12/modeling-user-journey-via-semantic-embeddings/", "abstract": "Modeling User Journeys via Semantic Embeddings Posted by Nishan Subedi on July 12, 2018 Etsy is a global marketplace for unique goods. This means that as soon as an item becomes popular, it runs the risk of selling out. Machine learning solutions that simply memorize the popular items are not as effective, and crafting features that generalize well across items in our inventory is important. In addition, some content features such as titles are sometimes not as informative for us since these are seller provided, and can be noisy. In this blog post, I will cover a machine learning technique we are using at Etsy that allows us to extract meaning from our data without the use of content features like titles, modeling only the user journeys across the site. This post assumes understanding of machine learning concepts,  specifically word2vec. What are embeddings? Word2vec is a popular method in natural language processing for learning a semi-supervised model from unsupervised data to discover similarity across words in a corpus using an unlabelled body of text. This is done by relating co-occurrence of words and relies on the assumption that words that appear together are more related than words that are far apart. This same method can be used to model user interactions on Etsy by modeling users journeys in aggregate as a sequence of user actions. Each user session is analogous to a sentence, and each user action (clicking on an item, visiting a shop’s home page, issuing a search query) is analogous to a word in NLP word2vec parlance. This method of modeling interactions allows us to represent items or other entities (shops, locations, users, queries) as low dimensional continuous vectors (semantic embeddings), where the similarity across two different vectors represents their co-relatedness. This method can be used without knowing anything about any particular user. Semantic embeddings are agnostic to the content of items such as their titles, tags, descriptions, and allow us to leverage aggregate user interactions on the site to extract items that are semantically similar. In addition, they give us the ability to embed our search queries, items, shops, categories, and locations in the same vector space. This leads to better featurization and candidate selection across multiple machine learning problems, and provides compression, which drastically improves inference speeds compared to representing them as one-hot encodings. Modeling user journeys as a sequence of actions gives us information that is different from content-based methods that leverage descriptions and titles of items, and so these methods can be used in conjunction. We have already productionized use of these embeddings across product recommendations, guided search experiences and they show great promise on our ranking algorithms as well. External to Etsy, similar semantic embeddings have been used to successfully learn representations for delivering ads as product recommendations via email and matching relevant ads to queries at Yahoo; and to improve their search ranking and derive similar listings for recommendations at AirBnB. Approach Etsy has over 50 million active items listed on the site from over 2 million sellers, and tens of millions of unique search queries each month. This amounts to billions of tokens (items or user actions – equivalent to word in NLP word2vec) for training. We were able to train embeddings on a single box, but we quickly ran into some limitations when modeling a sequence of user interactions as a naive word2vec model. The output embedding we constructed did not give us satisfactory performance. This gave us further assurance that some extensions to the standard word2vec implementation were necessary, and so extended the model with additional signals that are discussed below. Skip-gram model and extensions We initially started training the embeddings as a Skip-gram model with negative sampling ( NEG as outlined in the original word2vec paper ) method. The Skip-gram model performs better than the Continuous Bag Of Words (CBOW) model for larger vocabularies. It models the context given a target token and attempts to maximize the average likelihood of seeing any of the context tokens given a target token. The negative sampling draws a negative token from the entire corpus with a frequency that is directly proportional to the frequency of the token appearing in the corpus. Training a Skip-gram model on only randomly selected negatives, however, ignores implicit contextual signals that we have found to be indicative of user preference in other contexts. For example, if a user clicks on the second item for a search query, the user most likely saw, but did not like, the first item that showed up in the search results. We extend the Skip-gram loss function by appending these implicit negative signals to the Skip-gram loss directly. Similarly, we consider the purchased item in a particular session to be a global contextual token that applies to the entire sequence of user interactions. The intuition behind this is that there are many touch points on the user’s journey that help them come to the final purchase decision, and so we want to share the purchase intent across all the different actions that they took. This is also referred to as the linear multi-touch attribution model. In addition, we want to be able to give a user journey that ended in a purchase more importance in the model. We define an importance weight per user interaction (click, dwell, add to cart, and purchase) and incorporate this to our loss function as well. The details of how we extended Skip-gram are outside the scope of this post but can be found in detail in the Scalable Semantic Matching paper. Training We aim to learn a vector representation for each unique token, where a token can be listing id, shop id, query, category, or anything else that is part of a user’s  interaction. We were able to train embeddings up to 100 dimensions on a single box. Our final models take in billions of tokens and are able to produce embeddings for tens of millions of unique tokens. User action can be broadly defined to any sort of explicit or implicit engagement of the user with the product. We extract user interactions from multiple sources such as the search, category, market, and shop home pages, where these interactions are aggregated and not tied to a particular user. The model performed significantly better when we thresholded tokens based on their type. For example, the frequency count and distribution for queries tend to be very different from that of items, or shops. User queries are unbounded and have a very long tail, and order of magnitudes larger than the number of shops. So we want to capture all the shops in the embeddings vector space whereas limit queries or items based on a cutoff. We also found a significant improvement in performance by training the model on the past year’s data for the current and upcoming month to add some forecasting capabilities, eg. for a model serving production in the month of December, last month December and January data was added, so our model would see more interactions related to Christmas during this time. Training application specific models gave us better performance. For example, if we are interested in capturing shop level embeddings, training on the shops for an item instead of just the items directly yields better performance than averaging the embeddings for all items from a particular shop. We are actively experimenting with these models and plan to incorporate user and session specific data in the future. Results These are some interesting highlights of what the semantic embeddings are able to capture: Note that all these relations are created without the model being fed any content features. These are results of the embeddings filtered to just search queries and projected onto tensorboard. This first set of query similarities captures many different animals for the query jaguar.  The second set shows the model also able to relate across different languages. Montre is watch in French, armbanduhr is wristwatch in German, horloge is clock in French, orologio da polso is wristwatch in Italian, uhren is again watch in German, and relog in Spanish. Estate pipe signifies tobacco pipes that are previously owned. Here, we find the the model able to identity different items the pipe is made from (briar, corn cob, meerschaum), different brands of manufacturers (Dunhill and Peterson), and identifies accessories that are relevant to this particular type of pipe (pipe tamper) while not showing correlation with glass pipes that are not valid in this context. Content based methods have not been very effective in dealing with this. The embeddings are able to capture different styles, with boho, gypsy, hippie, gypsysoul all being related styles to bohemian. We found semantic embeddings to also provide better similar items to a particular item compared to a candidate set generation model that is based on content. This example comes from a model we released recently to generate similar items across shops. For an item that is a cookie of steer and cacti design, we see the previous method latch onto content from the term ‘steer’ and ignore ‘cactus’, whereas the semantic embeddings place significance on cookies. We find that this has the advantage of not having to guess the importance of a particular item, and just rely on user engagement to guide us. These candidates are generated based on a k-nn search across the semantic representations of items. We were able to run state of the art recall algorithms, unconstrained by memory on our training boxes themselves. We are excited about the variety of different applications of this model ranging from personalization to ranking to candidate set selection. Stay tuned! This work is a collaboration between Xiaoting Zhao and Nishan Subedi from the Search Ranking team. We would like to thank our manager, Liangjie Hong for insightful discussions and support, the Recommendation Systems and Search Ranking teams for their input during the project, specially Raphael Louca and Adam Henderson for launching products based on models, Stan Rozenraukh, Allison McKnight and Mohit Nayyar for reviewing this post, and Mihajlo Grbovic , leading author of the semantic embeddings paper for detailed responses to our questions. Posted by Nishan Subedi on July 12, 2018 Category: data , engineering , search Tags: artificial intelligence , data science , embeddings , machine learning , word2vec", "date": "2018-07-12,"},
{"website": "Etsy", "title": "Deploying to Google Kubernetes Engine", "author": ["Toria Gibbs"], "link": "https://codeascraft.com/2018/06/05/deploying-to-google-kubernetes-engine/", "abstract": "Deploying to Google Kubernetes Engine Posted by Toria Gibbs on June 5, 2018 Late last year, Etsy announced that we’ll be migrating our services out of self-managed data centers and into the cloud. We selected Google Cloud Platform (GCP) as our cloud provider and have been working diligently to migrate our services. Safely and securely migrating services to the cloud requires them to live in two places at once (on-premises and in the cloud) for some period of time. In this article, I’ll describe our strategy specifically for deploying to a pair of Kubernetes clusters: one running in the Google Kubernetes Engine (GKE) and the other on-premises in our data center. We’ll see how Etsy uses Jenkins to do secure Kubernetes deploys using authentication tokens and GCP service accounts. We’ll learn about the challenge of granting fine-grained GKE access to your service accounts and how Etsy solves this problem using Terraform and Helm. Deploying to On-Premises Kubernetes Etsy, while new to the Google Cloud Platform, is no stranger to Kubernetes. We have been running our own Kubernetes cluster inside our data center for well over a year now, so we already have a partial solution for deploying to GKE, given that we have a system for deploying to our on-premises Kubernetes. Our existing deployment system is quite simple from the perspective of the developer currently trying to deploy: simply open up Deployinator and press a series of buttons! Each button is labeled with its associated deploy action, such as “build and test” or “deploy to staging environment.” Under the hood, each button is performing some action, such as calling out to a bash script or kicking off a Jenkins integration test, or some combination of several such actions. For example, the Kubernetes portion of a Search deploy calls out to a Jenkins pipeline, which subsequently calls out to a bash script to perform a series of “docker build”, “docker tag”, “docker push”, and “kubectl apply” steps. Why Jenkins, then? Couldn’t we perform the docker/kubectl actions directly from Deployinator? The key is in… the keys! In order to deploy to our on-premises Kubernetes cluster, we need a secret access token . We load the token into Jenkins as a “ credential ” such that it is stored securely (not visible to Jenkins users), but we can easily access it from inside Jenkins code. Now, deploying to Kubernetes is a simple matter of looking up our secret token via Jenkins credentials and overriding the “kubectl” command to always use the token. Our Jenkinsfile for deploying search services looks something like this: All of the deploy.sh scripts above use environment variable $KUBECTL in place of standard calls to kubectl, and so by wrapping everything in our withKubernetesEnvs closure, we have ensured that all kubectl actions are using our secret token to authenticate with Kubernetes. Declarative Infrastructure via Terraform Deploying to GKE is a little different than deploying to our on-premises Kubernetes cluster and one of the major reasons is our requirement that everything in GCP be provisioned via Terraform . We want to be able to declare each GCP project and all its resources in one place so that it is automatable and reproducible. We want it to be easy—almost trivial—to recreate our entire GCP setup again from scratch. Terraform allows us to do just that. We use Terraform to declare every possible aspect of our GCP infrastructure. Keyword: possible. While Terraform can create our GKE clusters for us, it cannot (currently) create certain types of resources inside of those clusters. This includes Kubernetes resources which might be considered fundamental parts of the cluster’s infrastructure, such as roles and rolebindings. Access Control via Service Accounts Among the objects that are currently Terraformable: GCP service accounts! A service account is a special type of Google account which can be granted permissions like any other user, but is not mapped to an actual user. We typically use these “robot accounts” to grant permissions to a service so that it doesn’t have to run as any particular user (or as root!). At Etsy, we already have “robot deployer” user accounts for building and deploying services to our data center. Now we need a GCP service account which can act in the same capacity. Unfortunately, GCP service accounts only (currently) provide us with the ability to grant complete read/write access to all GKE clusters within the same project. We’d like to avoid that! We want to grant our deployer only the permissions that it needs to perform the deploy to a single cluster. For example, a deployer doesn’t need the ability to delete Kubernetes services—only to create or update them. Kubernetes provides the ability to grant more fine-grained permissions via role-based access control (RBAC). But how do we grant that kind of permission to a GCP service account? We start by giving the service account very minimal read-only access to the cluster. The service account section of the Terraform configuration for the search cluster looks like this: We have now created a service account with read-only access to the GKE cluster. Now how do we associate it with the more advanced RBAC inside GKE? We need some way to grant additional permissions to our deployer by using a RoleBinding to associate the service account with a specific Role or ClusterRole. Solving RBAC with Helm While Terraform can’t (yet) create the RBAC Kubernetes objects inside our GKE cluster, it can be configured to call a script (either locally or remotely ) after a resource is created. Problem solved! We can have Terraform create our GKE cluster and the minimal deployer service account, then simply call a bash script which creates all the Namespaces, ClusterRoles, and RoleBindings we need inside that cluster. We can bind a role using the service account’s email address, thus mapping the service account to the desired GKE RBAC role. However, as Etsy has multiple GKE clusters which all require very similar sets of objects to be created, I think we can do better. In particular, each cluster will require service accounts with various types of roles, such as “cluster admin” or “deployer”. If we want to add or remove a permission from the deployer accounts across all clusters, we’d prefer to do so by making the change in one place, rather than modifying multiple scripts for each cluster. Good news: there is already a powerful open source tool for templating Kubernetes objects! Helm is a project designed to manage configured packages of Kubernetes resources called “charts”. We created a Helm chart and wrote templates for all of the common resources that we need inside GKE. For each GKE cluster, we have a yaml file which declares the specific configuration for that cluster using the Helm chart’s templates. For example, here is the yaml configuration file for our production search cluster: And here are the templates for some of the resources used by the search cluster, as declared in the yaml file above (or by nested references inside other templates)… When we are ready to apply a change to the Helm chart—or Terraform is applying the chart to an updated GKE cluster—the script which applies the configuration to the GKE cluster does a simple “helm upgrade” to apply the new template values (and only the new values! Helm won’t do anything where it detects that no changes are needed). Integrating our New System into the Pipeline Now that we have created a service account which has exactly the permissions we require to deploy to GKE, we only have to make a few simple changes to our Jenkinsfile in order to put our new system to use. Recall that we had previously wrapped all our on-premises Kubernetes deployment scripts in a closure which ensured that all kubectl commands use our on-premises cluster token. For GKE, we use the same closure-wrapping style, but instead of overriding kubectl to use a token, we give it a special kube config which has been authenticated with the GKE cluster using our new deployer service account. As with our secret on-premises cluster token, we can store our GCP service account key in Jenkins as a credential and then access it using Jenkins’ withCredentials function. Here is our modified Jenkinsfile for deploying search services: And there you have it, folks! A Jenkins deployment pipeline which can simultaneously deploy services to our on-premises Kubernetes cluster and to our new GKE cluster by associating a GCP service account with GKE RBAC roles. Migrating a service from on-premises Kubernetes to GKE is now (in simple cases) as easy as shuffling a few lines in the Jenkinsfile. Typically we would deploy the service to both clusters for a period of time and send a percentage of traffic to the new GKE version of the service under an A/B test. After concluding that the new service is good and stable, we can stop deploying it on-premises, although it’s trivial to switch back in an emergency. Best of all: absolutely nothing has changed from the perspective of the average developer looking to deploy their code. The new logic for deploying to GKE remains hidden behind the Deployinator UI and they press the same series of buttons as always. — Thanks to Ben Burry, Jim Gedarovich, and Mike Adler who formulated and developed the Helm-RBAC solution with me. Posted by Toria Gibbs on June 5, 2018 Category: engineering , infrastructure , search Tags: cloud , deployment , GCP , GKE , helm , jenkins , kubernetes , rbac Related Posts Posted by Emily Sommer , Mike Adler , John Perkins , Joshua Thiel , Hilary Young , Chelsea Mozen , Dany Daya and Katie Sundstrom on 23 Apr, 2020 Cloud Jewels: Estimating kWh in the Cloud", "date": "2018-06-5,"},
{"website": "Etsy", "title": "The EventHorizon Saga", "author": ["Brad Greenlee"], "link": "https://codeascraft.com/2018/05/29/the-eventhorizon-saga/", "abstract": "The EventHorizon Saga Posted by Brad Greenlee on May 29, 2018 This is an epic tale of EventHorizon, and how we finally got it to a happy place. EventHorizon is a tool we use to watch events streaming into our system. Events (also known as beacons) are basically clickstream data—a record of actions visitors take on our site, what content they saw, what experiments they were bucketed into, etc. Events are sent primarily from our web & API servers (backend events) and web browsers (frontend events), and logged in Kafka . EventHorizon is primarily used as a debugging tool, to make sure that a new event you’ve added is working as expected, but also serves to monitor the health of our event system. EventHorizon UI EventHorizon is pretty simple; it’s only around 200 lines of Go code. It consumes messages from our main event topic on Kafka (“beacon-main”) and forwards them via WebSockets to any connected browsers. Ideally, the time between when an event is fired on the web site and when it appears in the EventHorizon UI is on the order of milliseconds. EventHorizon has been around for years, and in the early days, everything was hunky-dory. But then, the lagging started. Nobody Likes a Laggard As with most services at Etsy, we have lots of metrics we monitor for EventHorizon. One of the critical ones is consumer lag , which is the age of the last message we’ve read from the beacon-main Kafka topic. This would normally be milliseconds, but occasionally it would start lagging into minutes or even hours. EventHorizon Consumer Lag Sometimes it would recover on its own, but if not, restarting EventHorizon would often fix the problem, but only temporarily. Within anywhere from a few hours to a few weeks we’d notice lag time beginning to grow again. We spent a lot of time pouring over EventHorizon’s code, thinking we had a bug somewhere. It makes use of Go’s channels —perhaps there was a subtle concurrency bug that was causing a deadlock? We fiddled with that, but it didn’t help. We noticed that we could sometimes trigger the lag if two users connected to EventHorizon at the same time. This clue led us to think that there was a bug somewhere in the code that sent events to the browsers. Something with Websockets? We considered rewriting it to use Server-sent Events , but never got around to that. We also wondered if the sheer quantity of events we were sending to browsers was causing the problem. We updated EventHorizon to only send events from Etsy employees to browsers in production. Alas, the lag didn’t go away—although seemed to have gotten a little better. We eventually moved onto other things. We set up a Nagios alert for when EventHorizon started lagging, and just restarted it when it got bad. Since it would often be fine for 2-3 weeks before lagging, spending more time trying to fix it wasn’t a top priority. Orloj Joins the Fray In September 2017 EventHorizon lag had gotten really bad. We would restart it and it would just start lagging again immediately. At some point we even turned off the Nagios alert. However, another system, Orloj (pronounced “OR-loy”, named after the Prague astronomical clock ), had started experiencing lag as well. Orloj is another Kafka consumer, responsible for updating the Recently Viewed Listings that are shown on the homepage when are you are signed in. As Orloj is a production system, figuring out what was happening became much more urgent. Orloj’s lag was a little different: lag would spike once an hour, whenever the Hadoop job that pulls beacons down from Kafka into HDFS ran, and at certain times of the day it would be quite significant. Orloj Periodic Lag It turned out that due to a misconfiguration, KafkaPullJob, which was only supposed to launch one mapper per Kafka partition (of which we have 144 for beacon-main), was actually launching 400 mappers, which was swamping the network. We fixed this, and Orloj was happy again. For about a week. Trouble with NICs Orloj continued to have issues with lag. While digging into this, I realized that the machines in the Kafka clusters only had 1G network interfaces (NICs), whereas 10G NICs were standard in most of our infrastructure. I talked to our networking operations team to ask about upgrading the cluster and one of the engineers asked what was going on with one particular machine, kafkautil01. The network graph showed that its bandwidth was pegged at 100%, and had been for a while. kafkautil01 also had a 1G NIC. And that’s where EventHorizon ran. A light bulb exploded over my head. Relaying this info to Kevin Gessner , the engineer who wrote Orloj, he said “Oh yeah, consuming beacon-main requires at least 1.5 Gbps.” Suddenly it all made sense. Beacon traffic fluctuates in proportion to Etsy site traffic, which is cyclical. Parts of the day were under 1 Gbps, parts over, and when it went over, EventHorizon couldn’t keep up and would start lagging. And we were going over more and more often as Etsy grew. And remember the bit about two browsers connecting at once triggering lag? With EventHorizon forwarding the firehose of events to each browser, that was also a good way to push the network bandwidth over 1 Gbps, triggering lag. We upgraded the Kafka clusters and the KafkaUtil boxes to 10G NICs and everything was fixed. No more lag! Ha, just kidding. Exploding Events We did think it was fixed for a while, but EventHorizon and Orloj would both occasionally lag a bit, and it seemed to be happening more frequently. While digging into the continuing lag, we discovered that the size of events had grown considerably. Looking at a graph of event sizes, there was a noticeable uptick around the end of August. Event Beacon Size Increase This tied into problems we were having with capacity in our Hadoop cluster—larger events mean longer processing times for nearly every job. Inspecting event sizes showed some clear standouts. Four search events were responsible for a significant portion of all event bandwidth. The events were on the order of 50KB each, about 10x the size of a “normal” event. The culprit was some debugging information that had been added to the events. The problem was compounded by something that has been part of our event pipeline since the beginning: we generate complementary frontend events for each backend primary event (a “primary event” is akin to a page view) to capture browser-specific data that is only available on the frontend, and we do it by first making a copy of the entire event and then adding the frontend attributes. Later, when we added events for tracking page performance metrics, we did the same thing. These complementary events don’t need all the custom attributes of the original, so this is a lot of wasted bandwidth. So we stopped doing that. Between slimming down the search events, not copying attributes unnecessarily, and finding a few more events that could be trimmed down, we managed to bring down the average event size, as well as event volume, considerably. Event Beacon Size Decrease Nevertheless, the lag persisted. The Mysterious Partition 20 Orloj was still having problems, but this time it was a little different. The lag seemed to be happening only on a single partition, 20. We looked to see if the broker that was the leader for that partition was having any problems, and couldn’t see anything. We did see that it was serving a bit more traffic than other brokers, though. The first thing that came to mind was a hot key. Beacons are partitioned by a randomly-generated string called a “browser_id” that is unique to a client (browser, native device, etc.) hitting our site. If there’s no browser_id, as is the case with internal API calls, it gets assigned to a random partition. I used a command-line Kafka consumer to try to diagnose. It has an option for only reading from a single partition. Here I sampled 100,000 events from partitions 20 and 19: Partition 20 $ go run cmds/consumer/consumer.go -ini-files config.ini,config-prod.ini -topic beacon-main -partition 20 -value-only -max 100000 | jq -r '[.browser_id[0:6],.user_agent] | @tsv' | sort | uniq -c | sort -rn | head -5\r\n    558 orcIq5  Dalvik/2.1.0 (Linux; U; Android 7.0; SAMSUNG-SM-G935A Build/NRD90M) Mobile/1 EtsyInc/4.77.0 Android/1\r\n    540 null    Api_Client_V3/Bespoke_Member_Neu_Orchestrator\r\n    400 ArDkKf  Dalvik/2.1.0 (Linux; U; Android 8.0.0; Pixel XL Build/OPR3.170623.008) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    367 hK8GHc  Dalvik/2.1.0 (Linux; U; Android 7.0; SM-G950U Build/NRD90M) Mobile/1 EtsyInc/4.75.0 Android/1\r\n    366 EYuogd  Dalvik/2.1.0 (Linux; U; Android 7.0; SM-G930V Build/NRD90M) Mobile/1 EtsyInc/4.77.0 Android/1 Partition 19 $ go run cmds/consumer/consumer.go -ini-files config.ini,config-prod.ini -topic beacon-main -partition 19 -value-only -max 100000 | jq -r '[.browser_id[0:6],.user_agent] | @tsv' | sort | uniq -c | sort -rn | head -5\r\n    570 null    Api_Client_V3/Bespoke_Member_Neu_Orchestrator\r\n    506 SkHj7N  Dalvik/2.1.0 (Linux; U; Android 7.0; LG-LS993 Build/NRD90U) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    421 Jc36zw  Dalvik/2.1.0 (Linux; U; Android 7.0; SM-G930V Build/NRD90M) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    390 A586SI  Dalvik/2.1.0 (Linux; U; Android 8.0.0; Pixel Build/OPR3.170623.008) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    385 _rD1Uj  Dalvik/2.1.0 (Linux; U; Android 7.0; SM-G935P Build/NRD90M) Mobile/1 EtsyInc/4.77.0 Android/1 I couldn’t see any pattern, but did notice we were getting a lot of events from the API with a null browser_id. These appeared to be distributed evenly across partitions, though. We were seeing odd drops and spikes in the number of events going to partition 20, so I thought I’d see if I could just dump events around that time, so I started digging into our beacon consumer command-line tool to try to do that. In this process, I came across the big discovery: the -partition flag I had been relying on wasn’t actually hooked up to anything . So I was never consuming from a particular partition, but from all partitions. Once I fixed this, the problem was obvious: Partition 20 $ go run cmds/consumer/consumer.go -ini-files config.ini,config-prod.ini -topic beacon-main -q -value-only -max 10000 -partition 20 | jq -r '[.browser_id[0:6],.user_agent] | @tsv' | sort | uniq -c | sort -nr | head -5 8268 null Api_Client_V3/Bespoke_Member_Neu_Orchestrator 335 null Api_Client_V3/BespokeEtsyApps_Public_Listings_Offerings_FindByVariations\r\n    137 B70AD9  Mozilla/5.0 (iPhone; CPU iPhone OS 11_1_1 like Mac OS X) AppleWebKit/604.3.5 (KHTML, like Gecko) Mobile/15B150 EtsyInc/4.78 rv:47800.37.0\r\n     95 C23BB0  Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_3 like Mac OS X) AppleWebKit/603.3.8 (KHTML, like Gecko) Mobile/14G60 EtsyInc/4.78 rv:47800.37.0 83 null Api_Client_V3/Member_Carts_ApplyBestPromotions and another partition for comparison: $ go run cmds/consumer/consumer.go -ini-files config.ini,config-prod.ini -topic beacon-main -q -value-only -max 10000 -partition 0 | jq -r '[.browser_id[0:6],.user_agent] | @tsv' | sort | uniq -c | sort -nr | head -5\r\n   1074 dtdTyz  Dalvik/2.1.0 (Linux; U; Android 7.0; VS987 Build/NRD90U) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    858 gFXUcb  Dalvik/2.1.0 (Linux; U; Android 7.0; XT1585 Build/NCK25.118-10.2) Mobile/1 EtsyInc/4.78.1 Android/1\r\n    281 C380E3  Mozilla/5.0 (iPhone; CPU iPhone OS 11_0_3 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Mobile/15A432 EtsyInc/4.77 rv:47700.64.0\r\n    245 E0464A  Mozilla/5.0 (iPhone; CPU iPhone OS 11_0_3 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Mobile/15A432 EtsyInc/4.78 rv:47800.37.0\r\n    235 BAA599  Mozilla/5.0 (iPhone; CPU iPhone OS 11_0_3 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Mobile/15A432 EtsyInc/4.78 rv:47800.37.0 All the null browser_ids were going to partition 20. But how could this be? They’re supposed to be random. I bet a number of you are slapping your foreheads right now, just like I did. I wrote this test of the hashing algorithm: https://play.golang.org/p/MJpmoPATvO Yes, the browser_ids I was thinking were null were actually the string “null”, which is what was getting sent in the JSON event. I put in a fix for this, and: Orloj Partition 20 Baumgartner Lessons I’m not going to attempt to draw any deep conclusions from this sordid affair, but I’ll close with some advice for when you find yourself with a persistent confounding problem like this one. Graph everything . Then add some more graphs. Think about what new metrics might be helpful in diagnosing your issue. Kevin added the above graph showing Orloj events/sec by partition, which was critical to realizing there was a hot key issue. If something makes no sense, think about what assumptions you’ve been making in diagnosing it, and check those assumptions. Kevin’s graph didn’t line up with what I was seeing, so I dug deeper into the command-line consumer and found the problem with the -partition flag. Talk to people. Almost every small victory along the way came after talking with someone about the problem, and getting some crucial insight and perspective I was missing. Keep at it. As much as it can seem otherwise at times, computers are deterministic, and with persistence, smart colleagues, and maybe a bit of luck, you’ll figure it out. Epilogue On November 13 Cloudflare published a blog post on tuning garbage collection in Go . EventHorizon and Orloj both spent a considerable percentage of time (nearly 10%) doing garbage collection. By upping the GC threshold for both, we saw a massive performance improvement: Except for a couple brief spikes during our last Kafka upgrade, EventHorizon hasn’t lagged for more than a second since that change, and the current average lag is 2 ms. Thanks to Doug Hudson, Kevin Gessner, Patrick Cousins, Rebecca Sliter, Russ Taylor, and Sarah Marx for their feedback on this post. You can follow me on Twitter at @bgreenlee . Posted by Brad Greenlee on May 29, 2018 Category: engineering , events , infrastructure , monitoring , operations , performance", "date": "2018-05-29,"},
{"website": "Etsy", "title": "Sealed classes opened my mind", "author": ["Patrick Cousins"], "link": "https://codeascraft.com/2018/04/12/sealed-classes-opened-my-mind/", "abstract": "Sealed classes opened my mind Posted by Patrick Cousins on April 12, 2018 How we use Kotlin to tame state at Etsy Bubbly the Baby Seal by SmartappleCreations Etsy’s apps have a lot of state.  Listings, tags, attributes, materials, variations, production partners, shipping profiles, payment methods, conversations, messages, snippets, lions, tigers, and even stuffed bears. All of these data points form a matrix of possibilities that we need to track.  If we get things wrong, then our buyers might fail to find a cat bed they wanted for Whiskers.  And that’s a world I don’t want to live in. There are numerous techniques out there that help manage state.  Some are quite good, but none of them fundamentally changed our world quite like Kotlin’s sealed classes .  As a bonus, most any state management architecture can leverage the safety of Kotlin’s sealed classes. What are sealed classes? The simplest definition for sealed class is that they are “like enums with actual types.”  Inspired by Scala’s sealed types, Kotlin’s official documentation describes them thus: Sealed classes are used for representing restricted class hierarchies, when a value can have one of the types from a limited set, but cannot have any other type. They are, in a sense, an extension of enum classes: the set of values for an enum type is also restricted, but each enum constant exists only as a single instance, whereas a subclass of a sealed class can have multiple instances which can contain state. A precise, if a bit wordy, definition.  Most importantly: they are restricted hierarchies, they represent a set , they are types, and they can contain values. Let’s say we have a class Result to represent the value returned from a network call to an API.  In our simplified example, we have two possibilities: a list of strings, or an error. Now we can use Kotlin’s when expression to handle the result. The when expression is a bit like pattern matching . While not as powerful as pattern matching in some other languages, Kotlin’s when expression is one of the language’s most important features. Let’s try parsing the result based on whether it is was a success or an error. We’ve done a lot here in a just a few lines so let’s go over each aspect.  First we were able to check the type of result using Kotlin’s is operator, which is equivalent to Java’s instanceof operator.  By checking the type, Kotlin was able to smartcast the value of result for us for each case.  So if result is a success, we can access the value as if it is typed Result.Success .  Now we can can pass items without any type casting to showItems(items: List<String>) .  If the result was an error, just print the stack trace to the console. The next thing we did with the when expression is to exhaust all the possibilities for the Result sealed class type.  Typically a when expression must have an else clause.  However, in the above example, there are no other possible types for Result , so the compiler, and IDE, know we don’t need an else clause.  This is important as it is exactly the kind of safety we’ve been longing for.  Look at what happens when we add a Cancelled type to the sealed class: Now the IDE would show an error on our when statement because we haven’t handled the Cancelled branch of Result . ‘when’ expression must be exhaustive, add necessary ‘is Cancelled’ branch or ‘else’ branch instead. The IDE knows we didn’t cover all our bases here.  It even knows which possible types result could be based on the Result sealed class.  Helpfully, the IDE offers us a quickfix to add the missing branches. There is one subtle difference here though.  Notice we assigned the when expression to a val . The requirement that a when expression must be exhaustive only kicks in if you use when as a value, a return type, or call a function on it.  This is important to watch out for as it can be a bit of a gotcha.  If you want to utilize the power of sealed classes and when , be sure to use the when expression in some way. So that’s the basic power of sealed classes, but let’s dig a little deeper and see what else they can do. Weaving state Let’s take a new example of a sealed class Yarn . Then let’s create a new class call Merino and have it extend from Wool What would be the effect of this on our when expression if we were branching on Yarn ?  Have we actually created a new possibility that we must have a branch to accommodate?  In this case we have not, because a Merino is still considered part of Wool .  There are still only two branches for Yarn : So that didn’t really help us represent the hierarchy of wool properly.  But there is something else we can do instead. Let’s expand the example of Wool to include Merino and Alpaca types of wool and make Wool into a sealed class. Now our when expression will see each Wool subclass as unique branches that must be exhausted. Only wool socks please As our state grows in complexity however, there are times in which we may only be concerned about a subset of that state.  In Android it is common to have a custom view that perhaps only cares about the loading state. What can we do about that? If we use the when expression on Yarn , don’t we need to handle all cases?  Luckily Kotlin’s stdlib comes to the rescue with a helpful extension function. Say we are processing a sequence of events. Let’s create a fake sequence of all our possible states. Now let’s say that we’re getting ready to knit a really warm sweater . Maybe our KnitView is only interested in the Wool.Merino and Wool.Alpaca states . How do we handle only those branches in our when expression?  Kotlin’s Iterable extension function filterIsInstance to the rescue!  Thankfully we can filter the tree with a simple single line of code. And now, like magic, our when expression only needs to handle Wool states.  So now if we want to iterate through the sequence we can simply write the following: Meanwhile at Etsy Like a lot of apps these days we support letting our buyers and sellers login via Facebook or Google in addition to email and password.  To help protect the security of our buyers and sellers, we also offer the option of Two Factor Authentication. Adding additional complexity, a lot of these steps have to pause and wait for user input.  Let’s look at the diagram of the flow: So how do we model this with sealed classes?  There are 3 places where we make a call out to the network/API and await a result.  This is a logical place to start. We can model the responses as sealed classes. First, we reach out to the server for the social sign in request itself: Next, is the request to actually sign in: Finally, we have the two factor authentication for those that have enabled it: This is a good start but let’s look at what we have here.  The first thing we notice is that some of the states are the same.  This is when we must resist our urges and instincts to combine them.  While they look the same, they represent discrete states that a user can be in at different times. It’s safer, and more correct to think of each state as different.  We also want to prefer duplication over the wrong abstraction . The next thing we notice here is that these states are actually all connected. In some ways we’re starting to approach a Finite State Machine (FSM).  While an FSM here might make sense, what we’ve really done is to define a very readable, safe way of modeling the state and that model could be used with any type of architecture we want. The above sealed classes are logical and match our 3 distinct API requests — ultimately however, this is an imperfect representation.  In reality, there should be multiple instances of SignInResult for each branch of SocialSignInResult ,  and multiple instances of TwoFactorResult for each branch of SocialSignInResult . For us three steps proved to be the right level of abstraction for the refactoring effort we were undertaking at the time.  In the future we may very well connect each and every branch in a single sealed class hierarchy. Instead of three separate classes, we’d use a single class that represented the complete set of possible states. Let’s take a look at what that would have looked like: Note: to keep things simple I omitted the parameters  and used only regular subclasses Our sealed class is now beginning to look a bit more busy, but we have successfully modeled all the possible states for an Etsy buyer or seller during login.  And now we have a discrete set of types to represent each state, and a type safe way to filter them using Iterable.filterIsInstance<Type>(iterable) . This is pretty powerful stuff.  As you can imagine we could connect a custom view to a stream of events that filtered only on 2FA states.  Or we could connect a progress “spinner” that would hide itself on some other subset of the states. These ways of representing state opened up a clean way to react with business logic or changes on the UI.  Moreover, we’ve done it in a type-safe, expressive, and fluent way. So with a little luck, now you can login and get that cat bed for Whiskers! Special thanks to Cameron Ketcham who wrote most of this code! Posted by Patrick Cousins on April 12, 2018 Category: engineering , mobile Tags: android , kotlin , state Related Posts Posted by Stephanie Sharp on 21 Oct, 2020 Improving our design system through Dark Mode Posted by Deniz Veli on 13 Jan, 2014 Android Staggered Grid", "date": "2018-04-12,"},
{"website": "Etsy", "title": "Culture of Quality: Measuring Code Coverage at Etsy", "author": ["Jeffery Campbell"], "link": "https://codeascraft.com/2018/02/15/culture-of-quality-measuring-code-coverage-at-etsy/", "abstract": "Culture of Quality: Measuring Code Coverage at Etsy Posted by Jeffery Campbell on February 15, 2018 In the summer of 2017, Etsy created the Test Engineering Strategy Team to define, measure, and develop practices that would help product engineering teams ship quality code. One of our first goals was to find a way to establish a quantifiable quality “baseline” and track our progress against this baseline over time. We settled on code coverage because it provides both a single percentage number as well as line-by-line detailed information for engineers to act on. With over 30,000 unit tests in our test suite, this became a difficult challenge. Code coverage is the measure of how much code is being executed by your test suite. This blog post will walk you through the process of implementing coverage across our various platforms. We are not going to weigh in on the debate over the value of code coverage in this blog post. We approached measuring code coverage with 5 requirements: Generate code coverage metrics by file for PHP, Android, and iOS code bases Where possible, use existing tools to minimize impact on existing test and deploy infrastructure Create an automated and reliable hands-off process for measurement and reporting Surface an accurate measurement as a KPI for code owners Promote wide dissemination of the results to increase awareness Setup and Setbacks Our first step was collecting information on which tools were already being utilized in our deployment pipeline. A surprise discovery was that Android code coverage had already been implemented in our CI pipeline using JaCoCo , but had been disabled earlier in the year because it was impacting deploy times. We re-enabled the Jenkins job and set it to run on a weekly schedule instead of each deploy to the master trunk. Impact on deployment would be a recurring theme throughout coverage implementation. There was some early concern about the accuracy of the coverage report generated by JaCoCo. We noticed discrepancies in the reports when viewed through the Jenkins JaCoCo plugin and the code coverage window in Android Studio. We already have a weekly “Testing Workshop” scheduled where these kinds of problems are discussed. Our solution was to sit down with engineers during the workshops and review the coverage reports for differences. A thorough review found no flaws in the reports, so we moved forward with relying on JaCoCo as our coverage generation tool. For iOS, xcodebuild has built-in code coverage measurement. Setting up xcodebuild’s code coverage is a one-step process of enabling coverage in the target scheme using Xcode . This provides an immediate benefit to our engineers, as Xcode supplies instant feedback in the editor on which code is covered by tests. We measured the performance hit that our unit test build times would take from enabling coverage in CI and determined that it would be negligible. The jobs have a high variance build time with a mean(μ) swing of about 40 seconds. When jobs with coverage enabled were added to the sample, it did not have a noticeable effect on the build time trend. The trend is continually monitored and will be adjusted if the impact becomes detrimental to our deployment process. Our PHP unit tests were being executed by PHPUnit , an industry standard test runner, which includes an option to run tests with coverage. This was a great start. Running the tests with coverage required us to enable Xdebug , which can severely affect the performance of PHP. The solution was to employ an existing set of containers created by our Test Deployment Infrastructure team for smaller tasks (updating dashboards, running build metric crons, etc.). By enabling Xdebug on this small subset of containers we could run coverage without affecting the entire deployment suite. After this setup was completed, we attempted to run our unit tests with coverage on a sample of tests. We quickly found that executing tests with coverage would utilize a lot of RAM, which would cause the jobs to fail. Our initial attempt to solve the problem was to modify the “memory_limit” directive in the PHP.ini file to grant more RAM to the process. We were allowing up to 1024mb of RAM for the process, but this was still unsuccessful. Our eventual solution was to prepend our shell execution with php -d \"memory_limit=-1\" to free up all available memory for the process. Even after giving the process all of the memory available to the container, we were still running into job failures. Checking coverage for ~30,000 tests in a single execution was too problematic. We needed a way to break up our coverage tests into multiple executions. Our PHP unit tests are organized by directory (cart, checkout, etc.). In order to break up the coverage job, we wrote a simple shell script that iterates over each directory and creates a coverage report. for dir in */ ; do\r\nif [[ -d \"$dir\" && ! -L \"$dir\" ]]; then\r\nCoverage Command $dir;\r\nfi;\r\ndone Once we had the individual coverage reports, we needed a way to merge them. Thankfully, there is a second program called PHPCov that will combine reports generated by PHPunit. A job of this size can take upwards of four hours in total, so checking coverage on every deploy was out of the question. We set the job to run on a cron alongside our Android and iOS coverage jobs, establishing a pattern of checking code coverage on Sunday nights. Make It Accessible After we started gathering code coverage metrics, we needed to find a way to share this data with the owners of the code. For PHP, the combined report generated by PHPCov is a hefty 100mb XML file in the Clover format. We use a script written in PHP to parse the XML into an array and then output that array as a CSV file. With that done, we copy the data to our Vertica data warehouse for easy consumption by our engineering teams. For iOS, our first solution was to use a ruby gem called Slather to generate HTML reports. We followed this path for many weeks and implemented automated Slather reporting using Jenkins. Initially this seemed like a viable solution, but when reviewed by our engineers we found some discrepancies between the coverage report generated by Slather and the coverage report generated by Xcode. We had to go back to the drawing board and find another solution. We found another ruby gem, xcov , that directly analyzed the .xccoverage file generated by xcodebuild when running unit tests. We could parse the results of this coverage report into a JSON data object, then convert it into CSV and upload it to Vertica. For Android, the JaCoCo gradle plugin is supposed to be able to generate reports in multiple formats, including CSV. However, we could not find a working solution for generating reports using the gradle plugin. We spent a considerable amount of time debugging this problem and eventually realized that we were yak shaving . We discarded base assumptions and looked for other solutions. Eventually we decided to use the built-in REST api provided by Jenkins. We created a downstream job, passed it the build number for the JaCoCo report, then used a simple wget command to retrieve and save the JSON response. This was converted into a CSV file and (once again) uploaded to Vertica. Once we had the coverage data flowing into Vertica we wanted to get it into the hands of our engineering teams. We used Superbit, our in-house business intelligence reporting tool, to make template queries and dashboards that provided examples of how to surface relevant information for each team. We also began sending a weekly email newsletter to the engineering and project organizations highlighting notable changes to our coverage reports. To be continued… Measuring code coverage is just one of many methods the Test Engineering Strategy Team is using to improve the culture of quality in Etsy Engineering. In a future blog post we will be discussing the other ways in which we measure our code base and how we use this reporting to gain confidence in the code that we ship. Posted by Jeffery Campbell on February 15, 2018 Category: engineering , infrastructure", "date": "2018-02-15,"},
{"website": "Etsy", "title": "Selecting a Cloud Provider", "author": ["Mike Fisher"], "link": "https://codeascraft.com/2018/01/04/selecting-a-cloud-provider/", "abstract": "Selecting a Cloud Provider Posted by Mike Fisher on January 4, 2018 Etsy.com and most of our related services have been hosted in self-managed data centers since the first Etsy site was launched in 2005. Earlier this year, we decided to evaluate migrating everything to a cloud hosting solution. The decision to run our own hardware in data centers was the right decision at the time, but infrastructure as a service (IaaS) and platform as a service (PaaS) offerings have changed dramatically in the intervening years. It was time to reevaluate our decisions. We recently announced that we have selected Google Cloud Platform (GCP) as our cloud provider and are incredibly excited about this decision. This marks a shift for Etsy from infrastructure self-reliance to a best-in-class service provider. This shift allows us to spend less time maintaining our own infrastructure and more time on strategic features and services that make the Etsy marketplace great. Although we use the term ‘vendor’ when referring to a cloud provider, we viewed this as much more than a simple vendor selection process. We are entering into a partnership and a long-term relationship. The provider that we have chosen is going to be a major part of our successful initial migration, as well as a partner in the long-term scalability and availability of our site and services. This was not a decision that we wanted to enter into without careful consideration and deliberate analysis. This article will walk you through the process by which we vetted and ultimately selected a partner. We are not going to cover why we chose to migrate to a cloud hosting provider nor are we going to cover the business goals that we have established to measure the success of this project. From One, Many While the migration to a cloud hosting provider can be thought of as a single project, it really is one very large project made up of many smaller projects. In order to properly evaluate each cloud provider accurately, we needed to identify all of the sub-projects, determine the specific requirements of each sub-project, and then use these requirements to evaluate the various providers. Also, to scope the entire project, we needed to determine the sequence, effort, dependencies, priority, and timing of each sub-project. We started by identifying eight major projects, including the production render path for the site, the site’s search services, the production support systems such as logging, and the Tier 1 business systems like Jira. We then divided these projects further into their component projects—MySQL and Memcached as part of the production render path, for example. By the end of this exercise, we had identified over 30 sub-projects. To determine the requirements for each of these sub-projects, we needed to gather expertise from around the organization. No one person or project team could accurately, or in a timely enough manner, gather all of these requirements. For example, we not only needed to know the latency tolerance of our MySQL databases but also our data warehousing requirement for an API to create and delete data. To help gather all of these requirements, we used a RACI model to identify subproject ownership. RACI The RACI model is used to identify the responsible, accountable, consulted, and informed people for each sub-project. We used the following definitions: Responsible – This is the person ultimately responsible for completing the project or initiative. Accountable – This is the person to whom the R is accountable, and who must approve the work before it is okay to complete. Consulted – These people might have data that is useful in completing the project. Informed – These are people that should be kept informed or notified about the progress of the project but who do not need to approve of any step or artifact. Each Responsible person owned the gathering of requirements and the mapping of dependencies for their sub-project. The accountable person ensured the responsible person had the time, resources, and information they needed to complete the project and ultimately signed off that it was done. Architectural Review Etsy has long used an architectural review process, whereby any significant change in our environment, whether a technology, architecture, or design, undergoes a peer review. As these require a significant contribution of time from senior engineers, the preparation for these is not taken lightly. Experts across the organization collaborated to solicit diverse viewpoints and frequently produced 30+ page documents for architectural review. We determined that properly evaluating various cloud providers required an understanding of the desired end state of various parts of our system. For example, our current provisioning is done in our colocation centers using a custom toolset as automation to build bare-metal servers and virtual machines (VMs). We also use Chef roles and recipes applied on top of provisioned bare-metal servers and VMs. We identified a few key goals for choosing a tool for infrastructure creation in the cloud including: greater flexibility, accountability, security, and centralized access control. Our provisioning team evaluated several tools against these goals, discussed them, and then proposed new workflows in an architecture review. The team concluded by proposing we use Terraform in combination with Packer to build the base OS images. Ultimately, we held 25 architectural reviews for major components of our system and environments. We also held eight additional workshops for certain components we felt required more in-depth review. In particular, we reviewed the backend systems involved in generating pages of etsy.com (a.k.a. the production render path) with a greater focus on latency constraints and failure modes. These architectural reviews and workshops resulted in a set of requirements that we could use to evaluate the different cloud providers. How it Fits Together Once we had a firm-ish set of requirements for the major components of the system, we began outlining the order of migration. In order to do this we needed to determine how the components were all interrelated. This required us to graph dependencies, which involved teams of engineers huddled around whiteboards discussing how systems and subsystems interact. The dependency graphing exercises helped us identify and document all of the supporting parts of each major component, such as the scheduling and monitoring tools, caching pools, and streaming services. These sessions ultimately resulting in high-level estimates of project effort and timing, mapped in Gantt-style project plans. Experimentation Earlier in the year, we ran some of our Hadoop jobs on one of the cloud providers’ services, which gave us a very good understanding of the effort required to migrate and the challenges that we would face in doing so at scale. For this initial experiment, however, we did not use GCP, so we didn’t have the same level of understanding of the cloud provider we ultimately chose. We therefore undertook an experiment to enable batch jobs to run on GCP utilizing Dataproc and Dataflow. We learned a number of lessons from this exercise including that some of the GCP services were still in alpha release and not suitable for the workloads and SLAs that we required. This was the first of many similar decisions we’ll need to make: use a cloud service or build our own tool. In this case, we opted to implement Airflow on GCP VMs. To help us make these decisions we evaluated the priority of various criteria, such as vendor support of the service, vendor independence, and impact of this decision on other teams. There is no right answer to these questions. We believe that these questions and criteria need to be identified for each team to consider them and make the best decision possible. We are also not averse to revisiting these decisions in the future when we have more information or alpha and beta projects roll out into general availability (GA). Meetings Over the course of five months, we met with the Google team multiple times. Each of these meetings had clearly defined goals, ranging from short general introductions of team members to full day deep dives on various topics such as using container services in the cloud. In addition to providing key information, these meetings also reinforced a shared engineering culture between Etsy and Google. We also met with reference customers to discuss what they learned migrating to the cloud and what to look out for on our journey. The time spent with these companies was unbelievably worthwhile and demonstrated the amazing open culture that is pervasive among technology companies in NY. We also met with key stakeholders within Etsy to keep them informed of our progress and to invite their input on key directional decisions such as how to balance the mitigation of financial risk with the time-discounted value of cost commitments. Doing so provided decision makers with a shared familiarity and comfort with the process and eliminated the possibility of surprise at the final decision point. The Decision By this point we had thousands of data points from stakeholders, vendors, and engineering teams. We leveraged a tool called a decision matrix that is used to evaluate multiple-criteria decision problems. This tool helped organize and prioritize this data into something that could be used to impartially evaluate each vendor’s offering and proposal. Our decision matrix contained over 200 factors prioritized by 1,400 weights and evaluated more than 400 scores. This process began with identifying the overarching functional requirements. We identified relationship, cost, ease of use, value-added services, security, locations, and connectivity as the seven top-level functional requirement areas. We then listed every one of the 200+ customer requirements (customers referring to engineering teams and stakeholders) and weighted them by how well each supported the overall functional requirements. We used a 0, 1, 3, 9 scale to indicate the level of support. For example, the customer requirement of “autoscaling support” was weighted as a 9 for cost (as it would help reduce cost by dynamically scaling our compute clusters up and down), a 9 for ease of use (as it would keep us from having to manually spin up and down VMs), a 3 for value-added services (as it is an additional service offered beyond just basic compute and storage but it isn’t terribly sophisticated), and a 0 for supporting other functional requirements. Multiple engineering teams performed an in-depth evaluation and weighting of these factors. Clear priorities began to emerge as a result of the nonlinear weighting scale, which forces overly conservative scorers to make tough decisions on what really matters. We then used these weighted requirements to rank each vendor’s offering. We again used a 0, 1, 3, 9 scale for how well each cloud vendor met the requirement. Continuing with our “autoscaling support” example, we scored each cloud vendor a 9 for meeting this requirement completely in that all the vendors we evaluated provided support for autoscaling compute resources as native functionality. The total scores for each vendor reached over 50,000 points with GCP exceeding the others by more than 10%. As is likely obvious from the context, it should be noted that Etsy’s decision matrix (filled with Etsy’s requirements, ranked with Etsy’s weights by Etsy’s engineers) is applicable only to Etsy. We are not endorsing this decision as right for you or your organization, but rather we are attempting to provide you with insight into how we approach decisions such as these that are strategic and important to us. Just the beginning Now, the fun and work begin. This process took us the better part of five months with a full-time technical project manager, dozens of engineers and engineering managers, as well as several lawyers, finance personnel, and sourcing experts working on this part or in some cases full time. Needless to say, this was not an insignificant effort but really just the beginning of a multi-year project to migrate to GCP. We have an aggressive timeline to achieve our migration in about two years. We will do this while continuing to focus on innovative product features and minimizing risk during the transition period. We look forward to the opportunities that moving to GCP provides us and are particularly excited about this transformational change allowing us to focus more on core, strategic services for the Etsy marketplace by partnering with a best-in-class service provider. Posted by Mike Fisher on January 4, 2018 Category: engineering , infrastructure", "date": "2018-01-4,"},
{"website": "Etsy", "title": "VIPER on iOS at Etsy", "author": ["Michael MacDougall"], "link": "https://codeascraft.com/2017/12/11/viper-on-ios-at-etsy/", "abstract": "VIPER on iOS at Etsy Posted by Michael MacDougall on December 11, 2017 Background Consistent design patterns help Etsy move fast and deliver better features to our sellers and buyers. Typically we built features in a variety of ways, but found great benefits from the consistency VIPER brings. Inconsistent development practices can make jumping between projects and features fairly difficult. When starting a new project, developers would have to understand the way a feature was implemented with very little shared base knowledge. Just before we began working on the new Shop Settings in the Sell on Etsy app, VIPER was gaining steam amongst the iOS community and given the complexity of Shop Settings we decided to give this new pattern a try. While the learning curve was steep and the overhead was significant (more on that later) we appreciated the flexibility it had over other architectures allowing it to be more consistently used throughout our codebase. We decided to to apply the pattern elsewhere in our apps while investigating solutions to some of VIPER’s drawbacks. What is VIPER? VIPER is a Clean Architecture that aims to separate out the concerns of a complicated view controller into a set of objects with distinct responsibilities. VIPER stands for View, Interactor, Presenter, Entity, and Router, each with its own responsibility: View: What is displayed Interactor : Handles the interactions for each use case. Presenter: Sets up the logic for what is displayed in the view Entity: Model objects that are consumed by the Interactor Router: Handles the navigation/data passing from one screen to the next Example Implementation As a contrived example let’s say we want a view that displays a list of people. Our VIPER implementation could be setup as follows View: A simple UITableView set up to display UITableViewCells Interactor: The logic for fetching Person objects Presenter: Takes a list of Person objects and returns a list of only the names for consumption by the view Entity: Person objects Router: Presents a detail screen about a given Person The interaction would look something like this: The main view controller would tell the Interactor to fetch the Person entities The Interactor would fetch the Person entities and tell the Presenter to present these entities The Presenter would consume these entities and generate a list of strings The Presenter would then tell the UITableView to reload with the given set of strings Additionally if the user were to tap on a given Person object the Interactor would call to the Router to present the detail screen. Overhead & the issues with VIPER VIPER is not without issues. As we began to develop new features we ran into a few stumbling blocks. The boilerplate for setting up the interactions between all the classes can be quite tedious: // The Router needs a reference to the interactor for // any possible delegation that needs to be set up. // A good example would be after a save on an edit // screen the interactor should be told to reload the data. router.interactor = interactor // This is needed for the presentation logic router.controller = navigationController // If the view has any UI interaction the presenter // should be responsible for passing this to the interactor presenter.interactor = interactor // Some actions that the Interactor may do may require // presentation/navigation logic interactor.router = router // The interactor will need to update the presenter when // a UI update is needed interactor.presenter = presenter As a result of all the classes involved and the interaction between them developing a mental model of how VIPER works is difficult. Also, the numerous references required between classes can easily lead to retain cycles if weak references are not used in the appropriate places. VIPERBuilder and beyond In an attempt to reduce the amount of boilerplate and lower the barrier for using VIPER we developed VIPERBuilder . VIPERBuilder consists of a simple set of base classes for the Interactor, Presenter and Router (IPR) as well as a Builder class. By creating an instance of the VIPERBuilder object with your IPR subclasses specified via Swift generics (as needed) the necessary connections are made to allow for consistent VIPER implementation. Example within a UIViewController: lazy var viperBuilder: VIPERBuilder<NewInteractor, NewPresenter, NewRouter> = { return VIPERBuilder(controller: self) }() The VIPERBuilder classes intended to be subclassed are written in Objective C to allow subclassing in both Swift & Objective C. There is also an alternative builder object ( VIPERBuilderObjC ) that uses Objective C generics. Since the boilerplate is fairly limited the only thing left to do is write your functionality and view code. The part of VIPERBuilder that we chose not to architect around is what code should actually go in the IPR subclasses. Instead we broke down our ideal use-cases for IPR subclasses into the Readme . Additionally, we have an example project in the repo to give a better idea of how the separation of concerns work with VIPERBuilder. The iOS team at Etsy has built many features with VIPER & VIPERBuilder over the past year: shop stats, multi-shop checkout, search and the new seller dashboard just to name a few. Following these guidelines in our codebase has lessened the impact of context switching between features implemented in VIPER. We are super excited to see how the community uses VIPERBuilder and how it changes over time. Posted by Michael MacDougall on December 11, 2017 Category: engineering , mobile Tags: design pattern , ios , MVC , MVVM , native , VIPER", "date": "2017-12-11,"},
{"website": "Etsy", "title": "How Etsy caches: hashing, Ketama, and cache smearing", "author": ["Kevin Gessner"], "link": "https://codeascraft.com/2017/11/30/how-etsy-caches/", "abstract": "How Etsy caches: hashing, Ketama, and cache smearing Posted by Kevin Gessner on November 30, 2017 At Etsy, we rely heavily on memcached and Varnish as caching tiers to improve performance and reduce load. Database and search index query results, expensive calculations, and more are stored in memcached as a look-aside cache read from PHP and Java. Internal HTTP requests between API servers are sent through and cached in Varnish. As traffic has grown, so have our caching pools. Today, our caching clusters store terabytes of cached data and handle millions of lookups per second. At this scale, a single cache host can’t handle the volume of reads and writes that we require.  Cache lookups come from thousands of hosts, and the total volume of lookups would saturate the network and CPU of even the beefiest hardware—not to mention the expense of a server with terabytes of RAM.  Instead, we scale our caches horizontally, by building a pool of servers, each running memcached or Varnish. In memcached, the cache key can be (almost) any string, which will be looked up in the memcached server’s memory.  As an HTTP cache, Varnish is a bit more complex: requests for cacheable resources are sent directly to Varnish, which constructs a cache key from the HTTP request.  It uses the URL, query parameters, and certain headers as the key.  The headers to use in the key are specified with the Vary header ; for instance, we include the user’s locale header in the cache key to make sure cached results have the correct language and currency.  If Varnish can’t find the matching value in its memory, it forwards the request to a downstream server, then caches and returns the response. With large demands for both memcached and varnish, we want to make efficient use of the cache pool’s space.  A simple way to distribute your data across the cache pool is to hash the key to a number, then take it modulo the size of the cache pool to index to a specific server.  This makes efficient use of your cache space, because each cached value is stored on a single host—the same host is always calculated for each key. Consistent hashing for scalability A major drawback of modulo hashing is that the size of the cache pool needs to be stable over time.  Changing the size of the cache pool will cause most cache keys to hash to a new server.  Even though the values are still in the cache, if the key is distributed to a different server, the lookup will be a miss.  That makes changing the size of the cache pool—to make it larger or for maintenance—an expensive and inefficient operation, as performance will suffer under tons of spurious cache misses. For instance, if you have a pool of 4 hosts, a key that hashes to 500 will be stored on pool member 500 % 4 == 0 , while a key that hashes to 1299 will be stored on pool member 1299 % 4 == 3 .  If you grow your cache by adding a fifth host, the cache pool calculated for each key may change. The key that hashed to 500 will still be found on pool member 500 % 5 == 0 , but the key that hashed to 1299 be on pool member 1299 % 5 == 4 . Until the new pool member is warmed up, your cache hit rate will suffer, as the cache data will suddenly be on the ‘wrong’ host. In some cases, pool changes can cause more than half of your cached data to be assigned to a different host, slashing the efficiency of the cache temporarily. In the case of going from 4 to 5 hosts, only 20% of cache keys will be on the same host as before! A fixed-size pool, with changes causing the hit rate to suffer, isn’t acceptable. At Etsy, we make our memcached pools more flexible with consistent hashing , implemented with Ketama . Instead of using the modulo operation to map the hash output to the cache pool, Ketama divides the entire hash space into large contiguous buckets, one (or more) per cache pool host. Cache keys that hash to each range are looked up on the matching host.  This consistent hashing maps keys to hosts in such a way that changing the pool size only moves a small number of keys—“consistently” over time and pool changes. As a simplified example, with 4 hosts and hash values between 0 and 2^32-1 , Ketama might map all values in [0, 2^30) to pool member 0, [2^30, 2^31) to pool member 1, and so on. Like modulo, this distributes cache keys evenly and consistently among the cache hosts. But when the pool size changes, Ketama results in fewer keys moving to a different host. In the case of going from four hosts to five, Ketama would now divide the hash outputs into 5 groups. 50% of the cache keys will now be on the same host as previously. This leads to fewer cache misses and more efficiency during pool changes. In reality, Ketama maps each host to dozens or hundreds of buckets, not just one, which increases the stability and evenness of hashing. Ketama hashing is the default in PHP’s memcached library, and has served Etsy well for years as our cache has grown. The increased efficiency of consistently finding keys across pool changes avoids expensive load spikes. Hot keys and cache smearing While consistent hashing is efficient there is a drawback to having each key exist on only one server. Different keys are used for different purposes, and certain cache items will be read and written more than others. In our experience, cache key read rates follow a power law distribution : a handful of keys are used for a majority of reads, while the majority of keys are read a small number of times. With a large number of keys and a large pool, a good hash function will distribute the “hot keys”—the most active keys—among many servers, smoothing out the load. However, it is possible for a key to be so hot—read so many times—that it overloads its cache server. At Etsy, we’ve seen keys that are hit often enough, and store a large enough value, that they saturate the network interface of their cache host. The large number of clients are collectively generating a higher read rate than the cache server can provide.  We’ve seen this problem many times over the years , using mctop, memkeys , and our distributed tracing tools to track down hot keys. Further horizontal scaling by adding more cache hosts doesn’t help in this case, because it only changes the distribution of keys to hosts—at best, it would only move the problem key and saturate a different host. Faster network cards in the cache pool also help, but can have a substantial hardware cost. In general, we try to use large numbers of lower-cost hardware, instead of relying a small number of super-powered hosts. We use a technique I call cache smearing to help with hot keys. We continue to use consistent hashing, but add a small amount of entropy to certain hot cache keys for all reads and writes. This effectively turns one key into several, each storing the same data and letting the hash function distribute the read and write volume for the new keys among several hosts. Let’s take an example of a hot key popular_user_data . This key is read often (since the user is popular) and is hashed to pool member 3. Cache smearing appends a random number in a small range (say, [0, 8) ) to the key before each read or write. For instance, successive reads might look up popular_user_data3 , popular_user_data1 , and popular_user_data6 . Because the keys are different, they will be hashed to different hosts. One or more may still be on pool member 3, but not all of them will be, sharing the load among the pool. Cache smearing (named after the technique of smearing leap seconds over time ) trades efficiency for throughput, making a classic space-vs-time tradeoff . The same value is stored on multiple hosts, instead of once per pool, which is not maximally efficient. But it allows multiple hosts to serve requests for that key, preventing any one host from being overloaded and increasing the overall read rate of the cache pool. We manually add cache smearing to only our hottest keys, leaving keys that are less read-heavy efficiently stored on a single host. We ‘smear’ keys with just a few bits of entropy, that is, with a random number in a small range like 0 to 8 or 0 to 16. Choosing the size of that range is its own tradeoff. A too-large range duplicates the cached data more than necessary to reduce load, and can lead to data inconsistencies if a single user sees smeared values that are out-of-sync between multiple requests. A too-small range doesn’t spread the load effectively among the cache pool (and could even result in all the smeared keys still being hashed to the same pool member). We’ve successfully smeared both memcached keys (by appending entropy directly to the key) and Varnish keys (with a new query parameter for the random value). While we could improve the manual process to track down and smear hot keys, the work we’ve done so far has been integral to scaling our cache pools to their current sizes and beyond. The combination of consistent hashing and cache smearing has been a great combination of efficiency and practicality, using our cache space well while ameliorating the problems caused by hot keys. Keep these approaches in mind as you scale your caches! You can follow me on Twitter at @kevingessner Posted by Kevin Gessner on November 30, 2017 Category: engineering , infrastructure , performance", "date": "2017-11-30,"},
{"website": "Etsy", "title": "Reducing Image File Size at Etsy", "author": ["Will Gallego"], "link": "https://codeascraft.com/2017/05/30/reducing-image-file-size-at-etsy/", "abstract": "Reducing Image File Size at Etsy Posted by Will Gallego on May 30, 2017 Serving images is a critical part of Etsy’s infrastructure. Our sellers depend on displaying clean, visually appealing pictures to showcase their products. Buyers can learn about an item by reading its description, but there’s a tangibility to seeing a picture of that custom necklace, perfectly-styled shirt, or one-of-a-kind handbag you’ve found. Our image infrastructure at its core is simple. Users will upload images, we convert them to jpegs on dedicated processing machines (ImgWriters) and store them in large datastores (Mediastores). Upon request, we have specialized hosts (ImgCaches) that fetch the images from the Mediastores and send them to our users. Of course, there are many smaller pieces that make up our Photos infrastructure. For example, there’s load balancing involved with our ImgWriters to spread requests across the cluster, and our ImgCaches have a level of local in-memory caching for images that are frequently requested, coupled with a hashing scheme to reduce duplication of work. In front of that, there’s also caching on our CDN side. Behind the writers and caches, there’s some redundancy in our mediastores both for the sake of avoiding single point of failure scenarios and for scale. Throughout this architecture, though, one of our biggest constraints is the size of the data we pass around. Our CDN’s will only be able to cache so much before they begin to evict images, at which point requests are routed back to our origin. Larger images will require heavier use of our CDN’s and drive up costs. For our users, as the size of images increases so too will the download time for a page, impacting performance and resulting in a suboptimal user experience. Likewise, any post-processing of images internally is more bandwidth we’re using on our networks and increases our storage needs. Smaller images help us scale further and faster. As we’re using jpegs as our format, an easy way to reduce the data size of our images is to lower the jpeg quality value, compressing the contents of the image. Of course, as we compress further we’ll continue to reduce the visual quality of an image—edges will grow blurry and artifacts will appear. Below is an example of such. The image on the left has a jpeg quality of 92 while that on the right has one of 25. In this extreme case, we can see the picture of a bird on the right begins to show signs of compression – the background becomes “blocky” and there are “artifacts” around edges, such as around the bird and the blockiness of the background behind. The left with jpeg quality 92, the right jpeg quality 25. For most of Etsy’s history processing images, we’ve kept to a generally held constant that setting the quality of a jpeg image to 85 is assumed to be a “good balance”. Any larger and we probably won’t see enough of a difference to warrant all those extra bytes. But could we get away with lowering it to 80? Perhaps the golden number is 75? This is all very dependent upon the image a user uploads. A sunset with various gradients may begin to show the negative effects of lowering the jpeg quality before a picture of a ring on a white background. What we’d really like is to be able to determine what is a good jpeg quality to set per image. To do so, we added an image comparison tool called dssim to our stack. Dssim computes how dissimilar two images are using the SSIM algorithm for estimating human vision. Given two (or more) images, it will compare the structural similarity and produce a value from 0 (exactly the same) to unbounded. Note that while the README states that it only compares two pngs, recent updates to the library have also allowed the use of comparison between jpegs. With the user’s uploaded image as a base, we can do a binary search on the jpeg quality to find a balance between the visual quality (how good it looks) to the jpeg quality (the value that can help determine compression size). If we’ve assumed that 85 is a good ceiling for images, we can try lowering the quality to 75. Comparing the image at 85 and 75, if the value returned is higher than a known threshold of what is considered “good” for a dssim score, we can raise the jpeg quality back up to half way between, 80. If it’s below a threshold, maybe we can try push the quality a little lower at say 70. Either way, we continue to compare until we get to a happy medium. This algorithm is adapted from Tobias Baldauf’s work in compressing jpegs. We first tested compression using this bash script before running into some issues with speed. While this would do well for offline asynchronous optimizations of images, we wanted to keep ours synchronous so that users could see the same image on upload as they would when it was displayed to users. We were breaking 15-20 seconds on larger images by shelling out from PHP and so dug in a bit to look for some performance wins. Part of the slowdown was that cjpeg-dssim was converting from jpeg to png to compare with dssim. Since the time the original bash script was written, though, there had been an update to allow dssim to compare jpegs. We cut off half the time or more by not having to do the conversions each way. We also ported the code to PHP, as we already had the image blob in memory and could avoid some of the calls reading and writing the image to disk each time. We also adjusted to be a bit more conservative with some of the resizing, limiting it to a range of jpeg quality between 85 and 70 to short circuit the comparisons by starting at an image quality of 78 and breaking out if we went above 85 or below 70. As we cut numerous sizes for an image, we could in theory perform this for each resized image, but we rarely saw the jpeg quality value differ and so use the calculated jpeg quality for one size on all cropped and resized images of a given upload. As you’d expect, we’re adding work to our image processing and so had to take that into account. We believed that the additional upfront cost in time during the upload would pay for itself many times over. The speed savings in downloading images means an improvement in the time to render the important content on a page. We were adding 500ms to our processing time on average and about 800ms for the upper 90th percentile, which seemed within acceptable limits. This will vary, though, on the dimensions of the image you’re processing. We’re choosing an image we display often, which is 570px in width and a variable height (capped at 1500px). Larger images will take longer to process. As we strive to Graph All the Things™, we kept track of how the jpeg quality changed from the standard 85 to vary from image to image, settling at roughly 73 on average. The corresponding graphs for bytes stored, as expected, dropped as well. We saw reductions often ranging from 25% to 30% in the size of images stored, comparing the file sizes of images in April of 2016 with those in April of 2017 on the same day: Total file size of images in bytes in 1 week in April, 2016 vs 2017 Page load performance So how does that stack up to page load on Etsy? As an example, we examined the listing page on Etsy, which displays items in several different sizes: a medium size when the page first loads, a large size when zoomed in, and several tiny thumbnails in the carousel as well as up to 8 on the right that show other items from the shop. Here is a breakdown of content downloaded (with some basic variations depending upon the listing fetched): MIME Type Bytes js 6,119,491 image 1,022,820 font 129,496 css 122,731 html 86,744 other 3,838 Total 7,485,120 At Etsy, we frontload our static assets (js, fonts, css) and share it throughout the site, though, so that is often cached when a user reaches the listings page. Removing those three, which total 6,371,718 bytes, we’re left with 1,113,402 bytes, of which roughly 92% is comprised of image data. This means once a user has loaded the site’s assets, the vast majority of their download is then the images we serve on a given page as they navigate to each listing. If we apply our estimate of 23% savings to images, reducing image data to 787,571 and the page data minus assets to 878,153, on a 2G mobile device (roughly 14.4 kbps) it’s the difference in downloading a page in 77.3 seconds vs 60.8 seconds. For areas without 4G service, that’s a big difference! This of course is just one example page and the ratio of image size to page size will vary depending upon the listing. Images that suffer more Not all images are created equal. What might work for a handful of images is not going to be universal. We saw that some images in particular have a greater drop in jpeg quality while also suffering visual quality in our testing. Investigating, there are a few cases where this system could use improvements. In particular, jpeg quality typically suffers more when images contain line art or thin lines, text, and gradients, as the lossy compression algorithms for jpegs general don’t play well in these contexts. Likewise, images that have a large background area of a solid color and a small focus of the product would suffer, as the compression would run heavily on large portions of empty space in the image and then over compress the focus of the image. This helped inform our lower threshold of 70 for jpeg quality, at which we decided further compression wasn’t worth the loss in visual quality. The first uncompressed, the second compressed to jpeg quality 40. Other wins Page load speed isn’t the only win for minifying images, as our infrastructure also sees benefits from reducing the file size of images. CDN’s are a significant piece of our infrastructure, allowing for points of presence in various places in the world to serve static assets closer to the location of request and to offload some of the hits to our origin servers. Small images means we can cache more files before running into eviction limits and reduce the amount we’re spending on CDN’s. Our internal network infrastructure also scales further as we’re not pushing as many bytes around the line. Availability of the tool and future work We’re in the process of abstracting the code behind this into an open source tool to plug into your PHP code (or external service). One goal of writing this tool was to make sure that there was flexibility in determining what “good” is, as that can be very subjective to users and their needs. Perhaps you’re working with very large images and are willing to handle more compression. The minimum and maximum jpeg quality values allow you to fine tune the amount of compression that you’re willing to accept. On top of that, the threshold constants we use to narrow in when finding the “sweet spot” with dssim are configurable, giving you leverage to adjust in another direction. In the meantime, following the above guidelines on using a binary search approach combined with dssim should get you started. Closing Not all of the challenges behind these adjustments are specifically technical. Setting the jpeg quality of an image with most software applications is straightforward, as it is often an additional parameter to include when converting images. Deploying such a change, though, requires extensive testing over many different kinds of images. Knowing your use cases and the spectrum of images that make up your site is critical to such a change. With such a shift, knowing how to roll back, even if it requires a backfill, can also be useful as a safety net. Lastly, there’s merit in knowing that tweaking images is rarely “finished”. Some users like an image that heavily relies on sharpening images, while another might want some soft blending when resizing. Keeping this in mind was important for us to find out how we can continue to improve upon the experience our users have, both in terms of overall page speed and display of their products. Posted by Will Gallego on May 30, 2017 Category: Uncategorized", "date": "2017-05-30,"},
{"website": "Etsy", "title": "How Etsy Ships Apps", "author": ["Sasha Friedenberg"], "link": "https://codeascraft.com/2017/05/15/how-etsy-ships-apps/", "abstract": "How Etsy Ships Apps Posted by Sasha Friedenberg on May 15, 2017 In which Etsy transforms its app release process by aligning it with its philosophy for web deploys Anchors Aweigh Deploying code should be easy. It should happen often, and it should involve its engineers. For Etsyweb, this looks like continuous deployment. A group of engineers (which we call a push train ) and a designated driver all shepherd their changes to a staging environment, and then to production. At each checkpoint along that journey, the members of the push train are responsible for testing their changes, sharing that they’re ready to ship, and making sure nothing broke. Everyone in that train must work together for the safe completion of their deployment. And this happens very frequently: up to 50 times a day. TOPIC: clear\r\n mittens> .join                                TOPIC: mittens\r\n   sasha> .join with mittens                   TOPIC: mittens + sasha\r\n pushbot> mittens, sasha: You're up            TOPIC: mittens + sasha\r\n   sasha> .good                                TOPIC: mittens + sasha*\r\n mittens> .good                                TOPIC: mittens* + sasha*\r\n pushbot> mittens, sasha: Everyone is ready    TOPIC: mittens* + sasha*\r\n  nassim> .join                                TOPIC: mittens* + sasha* | nassim\r\n mittens> .at preprod                          TOPIC: <preprod> mittens + sasha | nassim\r\n mittens> .good                                TOPIC: <preprod> mittens* + sasha | nassim\r\n   sasha> .good                                TOPIC: <preprod> mittens* + sasha* | nassim\r\n pushbot> mittens, sasha: Everyone is ready    TOPIC: <preprod> mittens* + sasha* | nassim\r\n mittens> .at prod                             TOPIC: <prod> mittens + sasha | nassim\r\n mittens> .good                                TOPIC: <prod> mittens* + sasha | nassim\r\n     asm> .join                                TOPIC: <prod> mittens* + sasha | nassim + asm\r\n   sasha> .good                                TOPIC: <prod> mittens* + sasha* | nassim + asm\r\n     asm> .nm                                  TOPIC: <prod> mittens* + sasha* | nassim\r\n pushbot> mittens, sasha: Everyone is ready    TOPIC: <prod> mittens* + sasha* | nassim\r\n mittens> .done                                TOPIC: nassim\r\n pushbot> nassim: You're up                    TOPIC: nassim\r\n    lily> .join                                TOPIC: nassim | lily This strategy has been successful for a lot of reasons, but especially because each deploy is handled by the people most familiar with the changes that are shipping. Those that wrote the code are in the best position to recognize it breaking, and then fix it. Because of that, developers should be empowered to deploy code as needed, and remain close to its rollout. App releases are a different beast. They don’t easily adapt to that philosophy of deploying code. For one, they have versions and need to be compiled. And since they’re distributed via app stores, those versions can take time to reach end users. Traditionally, these traits have led to strategies involving release branches and release managers. Our app releases started out this way, but we learned quickly that they didn’t feel very Etsy. And so we set out to change them. Jen and Sasha We were the release managers. Jen managed the Sell on Etsy apps, and I managed the Etsy apps. We were responsible for all release stage transitions, maintaining the schedule, and managing all the communications around releases. We were also responsible for resolving conflicts and coordinating cross-team resources in cases of bugs and urgent blockers to release. Ready to Ship A key part of our job was making sure everyone knew what they’re supposed to do and when they’re supposed to do it. The biggest such checkpoint is when a release branches — this is when we create a dedicated branch for the release off master, and master becomes the next release. This is scheduled and determines what changes make it into production for a given release. It’s very important to make sure that those changes are expected, and that they have been tested. For Jen and me, it would’ve been impossible to keep track of the many changes in a release ourselves, and so it was our job to coordinate with the engineers that made the actual changes and make sure those changes were expected and tested. In practice, this meant sending emails or messaging folks when approaching certain checkpoints like branching. And likewise, if there were any storm warnings (such as show-stopping bugs), it was our responsibility to raise the flag to notify others. Then Jen left Etsy for another opportunity, and I became a single-point-of-failure and a gatekeeper. Every release decision was funneled through me, and I was the only person able to make and execute those decisions. I was overwhelmed. Frustrated. I was worried I’d be stuck navigating iTunes Connect and Google Play, and sending emails. And frankly, I didn’t want to be doing those things. I wanted those things to be automated. Give me a button to upload to iTunes Connect, and another to begin staged rollout on Google Play. Thinking about the ease of deploying on web just filled me with envy. This time wasn’t easy for engineers either. Even back when we had two release managers, from an engineer’s perspective, this period of app releases wasn’t transparent. It was difficult to know what phase of release we were in. A large number of emails was sent, but few of them were targeted to those that actually needed them. We would generically send emails to one big list that included all four of our apps. And all kinds of emails would get sent there. Things that were FYI-only, and also things that required urgent attention. We were on the path to alert-fatigue . All of this meant that engineers felt more like they were in the cargo hold, rather than in the cockpit. But that just didn’t fit with how we do things for web. It didn’t fit with our philosophy for deployment. We didn’t like it. We wanted something better, something that placed engineers in front of the tiller. Ship So we built a vessel that coordinates the status, schedule, communications, and deploy tools for app releases. Here’s how Ship helps: Keeps track of who committed changes to a release Sends Slack messages and emails to the right people about the relevant events Manages the state and schedule of all releases It’s hard to imagine all of that abstractly, so here’s an example: Captain’s Log Alicia makes her first commit to the iOS app for v4.64.0. Ship gets notified of this and sends Alicia an email welcoming her to v4.64.0 . Monday A cron moves the release into “Testing” and generates testing build v4.64.0.52. Ship is notified of this and sends an email to Alicia with the build . Alicia installs the build, verifies her changes, and tells Ship she’s ready. Tuesday Everyone has tested their changes and reported themselves as ready . A cron branches the release and creates a release candidate. Ship is notified and sends an email to coordinate final testing of the release . Wednesday The final testing finds no show-stopping issues A cron submits v4.64.0 to iTunes Connect for review. Friday A cron checks iTunes Connect for the review status of this release, and updates Ship that it’s been approved. Ship emails Alicia and others letting them know the release is approved. Tuesday A cron releases v4.64.0. (Had Alicia committed to our Android app, a cron would instead begin staged rollout on Google Play.) Ship emails Alicia and others letting them know the release is out in production. Wednesday Ship emails a report of top crashes to all the engineers in the release (including Alicia) Before Ship, all of these components above would’ve been performed manually. But you’ll notice that release managers are missing from the above script; have we replaced release managers with all the automations in Ship? Release Drivers Partially. Ship has a feature where each release is assigned a driver. This driver is responsible for a bunch of things that we couldn’t or shouldn’t automate . Here’s what they’re responsible for: Schedule changes Shepherding ‘ready to ships’ from other engineers Investigating showstopping bugs before release Everything else? That’s automated. Branching, release candidate generation, submission to iTunes Connect — even staged rollout on Google Play! But, we’ve learned from automation going awry before. By default, some things are set to manual. There are others for which Ship explicitly does not allow automation, such as continuing staged rollout on Google Play. Things like this should involve and require human interaction. For everything else that is automated, we added a failsafe: at any time, a driver can disable all the crons and take over driving from autopilot: When a driver wants to do something manually, they don’t need access to iTunes Connect or Google Play, as each of these things is made accessible as a button. A really nice side effect of this is that we don’t have to worry about provisioning folks for either app store, and we have a clear log of every release-related action taken by drivers. Drivers are assigned once a release moves onto master, and are semi-randomly selected based on previous drivers and engineers that have committed to previous releases. Once assigned, we send them an onboarding email letting them know what their responsibilities are: Ready to Ship Again The driver can remain mostly dormant until the day of branching. A couple hours before we branch, it’s the driver’s responsibility to make sure that all the impacting engineers are ready to ship, and to orchestrate efforts when they’re not. After we’re ready, the driver’s responsibility is to remain available as a point-of-contact while final testing takes place. If an issue comes up, the driver may be consulted for steps to resolve. And then, assuming all goes well, comes release day. The driver can opt to manually release, or let the cron do this for them — they’ll get notified if something goes wrong, either way. Then a day after we release, the driver looks at all of our dashboards, logs, and graphs to confirm the health of the release. Bugfixes But not all releases are planned. Things fail, and that’s expected . It’s naïve to assume some serious bug won’t ship with an app release. There’s plenty of things that can and will be the subject of a post-mortem . When one of those things happens, any engineer can spawn a bugfix release off the most-recently-released mainline release. The engineer that requests this bugfix gets assigned as the driver for that release. Once they branch the release, they make the necessary bugfixes (others can join in to add bugfixes too, if they coordinate with the driver) in the release’s branch, build a release candidate, test it, and get it ready for production. The driver can then release it at will. State Machine Releases are actually quite complicated. It starts off as an abstract thing that will occur in the future. Then becomes a concrete thing actively collecting changes via commits on master in git. After this period of collecting commits, the release is considered complete and moves into its own dedicated branch. The release candidate is then built from this dedicated branch, which then gets thoroughly tested, and moved into production. The release itself then concludes as an unmerged branch. Once a release branches, the next future release moves onto master. Each release is its own state machine, where the development and branching states overlap between successive releases. Notifications: Slack and Email Plugged into the output of Ship are notifications. Because there are so many points of interest en route to production, it’s really important that the right people are notified at the right times. So we use the state machine of Ship to send out notifications to engineers (and other subscribers) based on how much they asked to know, and how they impacted the release. We also allow anyone to sign up for notifications around a release. This is used by product managers, designers, support teams, engineering managers, and more. Our communications are very targeted to those that need or want them. In terms of what they asked to know, we made it very simple to get detailed emails about state changes to a release: In terms of how they impacted the release, we need to get that data from somewhere else. Git We mentioned data Ship receives from outside sources. At Etsy, we use GitHub for our source control. Our apps have repos per-platform (Android and iOS). In order to keep Ship’s knowledge of releases up-to-date, we set up GitHub Webhooks to notify Ship whenever changes are pushed to the repo. We listen for two changes in particular: pushes to master, and pushes to any release branch. When Ship gets notified, it iterates through the commits and uses the author, changed paths, and commit message to determine which app (buyer or seller) the commit affects, and which release we should attribute this change to. Ship then takes all of that and combines it into a state that represents every engineer’s impact on a given release. Is that engineer “user-impacting” or “dark” ( our term for changes that aren’t live )? Ship then uses this state to determine who is a member of what release, and who should get notified about what events. Additionally, at any point during a release, an engineer can change their status. They may want to do this if they want to receive more information about a release, or if Ship misunderstood one of their commits as being impacting to the release. Deployinator Everything up until has explained how Ship keeps track of things. But there’s been no explanation for how some of the automated actions affecting the app repo or things outside Etsy occur. We have a home-grown tool for managing deploys called Deployinator , and we added app support. It can now perform mutating interactions with the app repos, as well as all the deploy actions related to Google Play and iTunes Connect. This is where we build the testing candidates, release candidate, branch the release, submit to iTunes Connect, and much more. We opted to use Deployinator for a number of reasons: Etsy engineers are already familiar with it It’s our go-to environment for wrapping up a build process into a button Good for things that need individual run logs, and clear failures In our custom stack, we have crons. This is how we branch on Tuesday evening (assuming everyone is ready). This is where we interface with Google Play and iTunes Connect. We make use of Google Play’s official API in a custom python module we wrote, and for iTunes Connect we use Spaceship to interface with the unofficial API. Seaworthy The end result of Ship is that we’ve distributed release management. Etsy no longer has any dedicated release managers. But it does have an engineer who used to be one — and I even get to drive a release every now and then. People cannot be fully automated away. That applies to our web deploys, and is equally true for app releases. Our new process works within that reality. It’s unique because it pushes the limit of what we thought could be automated. Yet, at the same time, it empowers our app engineers more than ever before. Engineers control when a release goes to prod. Engineers decide if we’re ready to branch. Engineers hit the buttons. And that’s what Ship is really about. It empowers our engineers to deliver the best apps for our users. Ship puts engineers at the helm. Posted by Sasha Friedenberg on May 15, 2017 Category: infrastructure , mobile Tags: continuous deployment , deployinator , deployment , mobile Related Posts Posted by Jayson Paul on 20 Feb, 2015 Re-Introducing Deployinator, now as a gem! Posted by Rasmus Lerdorf on 01 Jul, 2013 Atomic deploys at Etsy Posted by Laura Beth Denker on 12 Mar, 2012 Scaling CI at Etsy: Divide and Concur, Revisited", "date": "2017-05-15,"},
{"website": "Etsy", "title": "Modeling Spelling Correction for Search at Etsy", "author": ["Mohit Nayyar"], "link": "https://codeascraft.com/2017/05/01/modeling-spelling-correction-for-search-at-etsy/", "abstract": "Modeling Spelling Correction for Search at Etsy Posted by Mohit Nayyar on May 1, 2017 Introduction When a user searches for an item on Etsy, they don’t always type what they mean. Sometimes they type the query jewlery when they’re looking for jewelry ; sometimes they just accidentally hit an extra key and type dresss instead of dress . To make their online shopping experience successful, we need to identify and fix these mistakes, and display search results for the canonical spelling of the actual intended query. With this motivation in mind, and through the efforts of the Data Science, Linguistic Tools, and the Search Infrastructure teams, we overhauled the way we do spelling correction at Etsy late last year. The older service was based on a static mapping of misspelled words to their respective corrections, which was updated only infrequently. It would split a search query into its tokens, and replace any token that was identified as a misspelling with its correction. Although the service allowed a very fast way of retrieving a correction, it was not an ideal solution for a few reasons: It could only correct misspellings that had already been identified and added to the map; it would leave previously unseen misspellings uncorrected. There was no systematic process in place to update the misspelling-correction pairs in the map. There was no mechanism of inferring the word from the context of surrounding words. For instance, if someone intended to type butter dish , but instead typed buttor dish , the older system would correct that to button dish since buttor is a more commonly observed misspelling for button at Etsy. Additionally, the older system would not allow one-to-many mappings from misspellings to corrections. Storing both buttor -> button and buttor -> butter mappings simultaneously would not be possible without modifying the underlying data structure. We, therefore, decided to upgrade the spelling correction service to one based on a statistical model. This model learns from historical user data on Etsy’s website and uses the context of surrounding words to offer the most probable correction. Although this blog post outlines some aspects of the infrastructure components of the service, its main focus is on describing the statistical model used by it. Model We use a model that is based upon the Noisy Channel Model , which was historically used to infer telegraph messages that got distorted over the line. In the context of a user typing an incorrectly spelled word on Etsy, the “distortion” could be from accidental typos or a result of the user not knowing the correct spelling. In terms of probabilities, our goal is to determine the probability of the correct word, conditional on the word typed by the user. That probability, as per Bayes’ rule, is proportional to the product of two probabilities: We are able to estimate the probabilities on the right-hand side of the relation above, using historical searches performed by our users. We describe this in more detail below. To extend this idea to multi-word phrases, we use a simple Hidden Markov Model (HMM) which determines the most probable sequence of tokens to suggest as a spelling correction. Markov refers to being able to predict the future state of the system based solely on its present state. Hidden refers to the system having hidden states that we are trying to discover. In our case, the hidden states are the correct spellings of the words the user actually typed. The main components of an HMM are explained via the figure below. Assume, for the purpose of this post, that the user searched for Flower Girl Baske when they intended to search for Flower Girl Basket . The main components of an HMM are then as follows: Observed states : the words explicitly typed by the user, Flower Girl Baske (represented as circles). Hidden states : the correct spellings of the words the user intended to type, Flower Girl Basket (represented as squares). Emission probability : the conditional probability of observing a state given the hidden state. Transition probability : the probability of observing a state conditional upon the probability of the immediately previous observed state, like, for instance, in the figure below, the probability of transitioning from Girl to Baske , i.e., P(Baske|Girl) . The fact that this probability does not depend on the probability of having observed Flower , the state that precedes Girl, illustrates the Markov property of the model. Once the spelling correction service receives the query, it first splits the query into three tokens, and then suggests possible corrections for each of the tokens as shown in the figure below. In each column, one of the correction possibilities represents the true hidden state of the token that was typed (emitted) by the user. The most probable sequence can be thought of as identifying the true hidden state for each token given the context of the previous token. To accomplish this, we need to know probabilities from two distributions: first, the probability of typing a misspelling given the correct spelling of the word, the emission probability, and, second, the probability of transitioning from one observed word to another, the transition probability. Once we are able to calculate all the emission probabilities, and the transition probabilities needed, as described in the sections below, we can determine the most probable sequence using a common dynamic programming algorithm known as the Viterbi Algorithm . Error Model The emission probability is estimated through an Error Model, which is a statistical model created by inferring historical corrections provided by our own users while making searches on Etsy. The heuristic we used is based on two conditions: a user’s search being followed immediately by another search similar to the first, and with the second search then leading to a listing click. We align these tokens to each other in a way that minimizes the number of edits needed on the misspelled query to transform it into the corrected second query. We then make aligned splits on the characters, and calculate counts and associated conditional probabilities . We generate character level probabilities for four types of operations: substitution, insertion, deletion, and transposition. Of these, insertion and deletion also require us to also keep track of the context of neighboring letters — the probability of adding a letter after another, or removing a letter appearing after another, respectively. To continue with the earlier example, when the user typed baske , the emitted token, they were probably trying to type basket , the hidden state for baske, which corresponds to the probability P(Baske|Basket) . Error probabilities for tokens, such as these, are calculated by multiplying the character-level probabilities which are assumed to be independent. For instance, the probability of correcting the letter e by appending the letter t to it is given by: Here, the numerator is the number of times in our dataset where we see an e being corrected to an et , and the denominator is the number of corrections where any letter was appended after e . Since we assume that the probability associated with a character remaining unchanged is 1, in our case, the probability P(Baske|Basket) is solely dependent on the insertion probability P(et|e) . Language Model Transition probabilities are estimated through a language model which is determined by calculating the unigram and bigram token frequencies seen in our historical search queries. A specific instance of that, from the chosen example, would be the probability of going from one token in the Flower (first) column, say Flow , to a token, say Girl , in the Girl (second) column, which is represented as P(Girl|Flow) . We are able to determine that probability from the following ratio of bigram token counts to unigram token counts: The error models and language models are generated through Scalding -powered jobs that run on our production hadoop cluster. Viterbi Algorithm Heuristic To generate inferences using these models, we employ the Viterbi Algorithm to determine the optimal query, i.e., sequence of tokens to serve as a spelling correction. The main idea is to present the most probable sequence as the spelling correction. The iterative process goes, as per the figure above, from the first column of tokens to the last. At the n th iteration, we have the sequence with the maximum probability available from the previous iteration, and we choose the token from the nth column which increases the maximum probability from the previous iteration the most. Let’s explain this more concretely by describing the third iteration for our main example: assume that we already have Flower girl as the most probable phrase at the second iteration. Now we pick the token from the third column that corresponds to the maximum of the following transition probability and emission probability products: We can see from the figure above that the transition probability from Girl to Basket is high enough to overcome the lower emission probability of someone typing Baske when they mean to type Basket , and that, consequently, Basket is the word we want to add to the existing Flower Girl phrase. We now know that Flower Girl Basket is the most probable correction as predicted by our models. The decision about whether we suggest this correction to the user, and how we suggest it, is made on the basis of its confidence score. We describe that process a little later in the post. Hyperparameters and Model Training We added a few hyperparameters to the bare bones model we have described so far. They are as follows: We added an exponent to the emission probability because we want the model to have some flexibility in weighting the error model component in relation to the language model component. Our threshold for offering a correction is a linear function of the number of tokens in the query. We, therefore, have a slope and the intercept of the threshold as a pair of hyperparameters. Finally, we also have a parameter corresponding to the probability of words our language model does not know about. Our training data set consists of two types of examples: the first kind are of the form misspelling -> correct spelling , like jewlery -> jewelry , while the second kind are of the form correct spelling -> _ , like dress -> _. The second type corresponds to queries that are already correct, and therefore don’t need a correction. This setup enables our model to distinguish between the populations of both correct and incorrect spellings, and to offer corrections for the latter. We tune our hyperparameters via 10-fold cross-validation using a hill-climbing algorithm that maximizes the f-score on our training data set. The f-score is the harmonic mean of the precision and the recall . Precision measures how many of the corrections offered by the system are correct and, for our data set, is defined as: Recall is a measure of coverage — it tries to answer the question, “how many of the spelling corrections that we should be offering are we offering?”. It is defined as: Optimizing on the f-score is allows us to increase coverage of the spelling corrections we offer while keeping the number of invalid corrections offered in check. Serving the Correction We serve three types of corrections at Etsy based on the confidence we have in the correction. We use the following odds-ratio as our confidence score: If the confidence is greater than a certain threshold, we display search results corresponding to the correction, along with a “Search instead of” link to see the results for the original query instead. If the confidence is below a certain threshold, we offer results of the original query, with a “Did you mean” option to the suggested correction. If we don’t have any results for the original query, but do have results for the corrected query, we display results for the correction independent of the confidence score. Spelling Correction Service With the new service in place, when a search is made on Etsy, we fire off requests to both the new Java-based spelling service and the existing search service. The response from the spelling service includes both the correction and its confidence score, and how we display the correction, based on the confidence score, is described in the previous section. For high-confidence corrections, we make an additional request for search results with the corrected query, and display those results if their count is greater than a certain threshold. When the spelling service receives a query, it first splits it into its constituent tokens. It then makes independent suggestions for each of the tokens, and, finally, strings together from those suggestions a sequence of the most probable tokens. This corrected sequence is sent back to the web stack as a response to the spelling service request. The correction suggestions for each token are generated by Lucene’s DirectSpellChecker class which queries an index generated from tokens that are generated through a Hadoop job. The Hadoop job counts tokens from historical queries, rejecting any tokens that appear too infrequently or those that appear only in search queries with very few search results. We have a daily cron that ships the tokens, along with the generated Error and Language model files, from HDFS to the boxes that host the spelling correction service. A periodic rolling restart of the service across all boxes ensures that the freshest models and index tokens are picked up by the service. Follow-up Work The original implementation of the model was limited, in the sense, that it could not suggest corrections that were, at the token level, more than two edits away from the original token. We have subsequently implemented an auxiliary framework of suggesting correction tokens that fixes this issue. Another limitation was related to splitting and compounding of tokens in the original query to make suggestions. For instance, we were not able to suggest earring as a suggestion for ear ring . Some of our coworkers are working on modifying the service to accommodate corrections of this type. Although we do use supervised learning to train the hyperparameters of our models, since launching the service we have additional inputs from our users which we can improve our models with. Specifically, users clicking the “Did you mean” link on our low-confidence corrections results page provides us with explicit positive feedback, while clicks on the “Search instead for” link on our high-confidence corrections results page provides us with negative feedback. The next major evolution of the model would be to explicitly use this feedback to improve corrections. (This project was a collaboration between Melanie Gin and Benjamin Russell on the Linguistic Tools team who built out the infrastructure and front-end; Caitlin Cellier and Mohit Nayyar on the Data Science team who worked on implementing the statistical model; and Zhi-Da Zhong on the Search Infrastructure team who consulted on infrastructure design. A special thanks to Caitlin whose presentation some of these figures are copied from.) Posted by Mohit Nayyar on May 1, 2017 Category: data , search Tags: data , data science Related Posts Posted by Emily Sommer , Mike Adler , John Perkins , Joshua Thiel , Hilary Young , Chelsea Mozen , Dany Daya and Katie Sundstrom on 23 Apr, 2020 Cloud Jewels: Estimating kWh in the Cloud Posted by Ken Lee and Kai Zhong on 15 Sep, 2016 Introducing 411: A new open source framework for handling alerting Posted by Diego Alonso on 03 Apr, 2014 Web Experimentation with New Visitors", "date": "2017-05-1,"},
{"website": "Etsy", "title": "Etsy’s Debriefing Facilitation Guide for Blameless Postmortems", "author": ["John Allspaw"], "link": "https://codeascraft.com/2016/11/17/debriefing-facilitation-guide/", "abstract": "Etsy’s Debriefing Facilitation Guide for Blameless Postmortems Posted by John Allspaw on November 17, 2016 In 2012, I wrote a post for the Code As Craft blog about how we approach learning from accidents and mistakes at Etsy. I wrote about the perspectives and concepts behind what is known (in the world of Systems Safety and Human Factors) as the New View on “human error.” I also wrote about what it means for an organization to take a different approach, philosophically, to learn from accidents, and that Etsy was such an organization. That post’s purpose was to conceptually point in a new direction, and was, necessarily, void of pragmatic guidance, advice, or suggestions on how to operationalize this perspective. Since then, we at Etsy have continued to explore and evolve our understanding of what taking this approach means, practically. For many organizations engaged in software engineering, the group “post-mortem” debriefing meeting (and accompanying documentation) is where the rubber meets the road. Many responded to that original post with a question: “Ok, you’ve convinced me that we have to take a different view on mistakes and accidents. Now: how do you do that ?” As a first step to answer that question, we’ve developed a new Debriefing Facilitation Guide which we are open-sourcing and publishing. We wrote this document for two reasons. The first is to state emphatically that we believe a “post-mortem” debriefing should be considered first and foremost a learning opportunity, not a fixing one. All too often, when teams get together to discuss an event, they walk into the room with a story they’ve already rationalized about what happened. This urge to point to a “root” cause is seductive — it allows us to believe that we’ve understood the event well enough, and can move on towards fixing things. We believe this view is insufficient at best, and harmful at worst, and that gathering multiple diverse perspectives provides valuable context, without which you are only creating an illusion of understanding. Systems safety researcher Nancy Leveson, in her excellent book Engineering A Safer World , has this to say on the topic: “An accident model should encourage a broad view of accident mechanisms that expands the investigation beyond the proximate events: A narrow focus on operator actions, physical component failures, and technology may lead to ignoring some of the most important factors in terms of preventing future accidents. The whole concept of ‘root cause’ needs to be reconsidered.” In other words, if we don’t pay attention to where and how we “look” to understand an event by considering a debriefing a true exploration with open minds, we can easily miss out on truly valuable understanding. How and where we pay attention to this learning opportunity begins with the debriefing facilitator. The second reason is to help develop debriefing facilitation skills in our field. We wanted to provide practical guidance for debriefing facilitators as they think about preparing for, conducting, and navigating a post-event debriefing. We believe that organizational learning can only happen when objective data about the event (the type of data that you might put into a template or form) is placed into context with the subjective data that can only be gleaned by the skillful constructing of dialogue with the multiple, diverse perspectives in the room. The Questions Are As Important As The Answers In his book “Pre-Accident Investigations: Better Questions,” Todd Conklin sheds light on the idea that the focus on understanding complex work is not in the answers we assert, but in the questions we ask: “The skill is not in knowing the right answer. Right answers are pretty easy. The skill is in asking the right question. The question is everything.” What we learn from an event depends on the questions we ask as facilitators, not just the objective data you gather and put into a document. It is very easy to assume that the narrative of an accident can be drawn up from one person’s singular perspective, and that the challenge is what to do about it moving forward. We do not believe that to be true. Here’s a narrative taken from real debriefing notes, generalized for this post: “I don’t know,” the engineer said, when asked what happened. “I just wasn’t paying attention, I guess. This is on me. I’m sorry, everyone.” The outage had lasted only 9 minutes, but to the engineer it felt like a lifetime. The group felt a strange but comforting relief, ready to fill in the incident report with ‘engineer error’ and continue on with their morning work. The facilitator was not ready to call it a ‘closed case.’ “Take us back to before you deployed, Phil…what do you remember? This looks like the change you prepped to deploy…” The facilitator displayed the code diff on the big monitor in the conference room. Phil looked closely to the red and green lines on the screen and replied “Yep, that’s it. I asked Steve and Lisa for a code review, and they both said it looked good.” Steve and Lisa nod their heads, sheepishly. “So after you got the thumbs-up from Steve and Lisa…what happened next?” the facilitator continued. “Well, I checked it in, like I always do,” Phil replied. “The tests automatically run, so I waited for them to finish.” He paused for a moment. “I looked on the page that shows the test results, like this…” Phil brought up the page in his browser, on the large screen. “Is that a new dashboard?” Lisa asked from the back of the room. “Yeah, after we upgraded the Jenkins install, we redesigned the default test results page to the previous colors because the new one was hard to read,” replied Sam, the team lead for the automated testing team. “The page says eight tests failed.” Lisa replied. Everyone in the room squinted. “No, it says zero tests failed, see…?” Phil said, moving closer to the monitor. Phil hit control-+ on his laptop, increasing the size of the text on the screen. “Oh my. I swear that it said zero tests failed when I deployed.” The facilitator looked at the rest of the group in the conference room. “How many folks in the room saw this number as a zero when Phil first put it up on the screen?” Most of the group’s hands went up. Lisa smiled. “It looked like a zero to me too,” the facilitator said. “Huh. I think because this small font puts slashes in its zeros and it’s in italics, an eight looks a lot like a zero,” Sam said, taking notes. “We should change that.” As a facilitator, it would be easy to stop asking questions at the mea culpa given by Phil. Without asking him to describe how he normally does his work, by bringing us back to what he was doing at the time, what he was focused on, what led him to believe that deploying was going to happen without issue, we might never have considered that the automated test results page could use some design changes to make the number of failed tests clearer, visually. In another case, an outage involved a very complicated set of non-obvious interactions between multiple components. During the debriefing, the facilitator asked each of the engineers who designed the systems and were familiar with the architecture to draw on a whiteboard the picture that they had in their minds when they think about how it all is expected to work. When seen together, each of the diagrams from the different engineers painted a fuller picture of how the components worked together than if there was only one engineer attempting to draw out a “comprehensive” and “canonical” diagram. The process of drawing this diagram together also brought the engineers to say aloud what they were and were not sure about, and that enabled others in the room to repair those uncertainties and misunderstandings. Both of these cases support the perspective we take at Etsy, which is that debriefings can (and should!) serve multiple purposes, not just a simplistic hunt for remediation items. By placing the focus explicitly on learning first, a well-run debriefing has the potential to educate the organization on not just what didn’t work well on the day of the event in question, but a whole lot more. The key, then, to having well-run debriefings is to start treating facilitation as a skill in which to invest time and effort. Influencing the Industry We believe that publishing this guide will help other organizations think about the way that they train and nurture the skill of debriefing facilitation. Moreover, we also hope that it can continue the greater dialogue in the industry about learning from accidents in software-rich environments. In 2014, knowing how critical I believe this topic to be, someone alerted me to the United States Digital Service Play Books repo on Github, and an issue opened about incident review and post-mortems . I commented there that this was relevant to our interests at Etsy, and that at some point I would reach back out with work we’ve done in this area. It took two years, but now we’re following up and hoping to reopen the dialogue. Our Debriefing Facilitation Guide repo is here , and we hope that it is useful for other organizations. Posted by John Allspaw on November 17, 2016 Category: engineering , people , philosophy , Uncategorized Tags: blameless postmortems , debriefing facilitation guide , human error", "date": "2016-11-17,"},
{"website": "Etsy", "title": "API First Transformation at Etsy – Human aspects & developer adoption", "author": ["Stefanie Schirmer"], "link": "https://codeascraft.com/2016/11/08/api-first-transformation-human-aspects-developer-adoption/", "abstract": "API First Transformation at Etsy – Human aspects & developer adoption Posted by Stefanie Schirmer on November 8, 2016 This is the third post in a series of three about Etsy’s API, the abstract interface to our logic and data. In the last posts we covered how we built a new API framework, and we clearly identified the gains in terms of performance and shared abstraction layer between languages and devices. But how did we make an entire engineering organization switch to the new framework? How did we achieve the cultural transformation to API first? How do we avoid this being the new thing that everyone knows about, but no one has the time to try? How did we “sell” it? In our case, we had multiple strategies that worked together. The first one was communication: It was simple to write new endpoints, and the gains were very clear: Both the performance gain through concurrent data fetch, and the possibility to share an endpoint with the apps, were huge selling points. We partnered with the Product organisation to make clear that they needed to include a standard question in their project templates “Is this being built on APIv3? If not, why not?”, which enforced the company strategy to adopt Mobile First development. We had pilot groups that tried out the new API framework for new product features, we partnered closely with the Activity Feed team to help them adopt APIv3. This resulted in early converts that were strong advocates for wider adoption. The benefits were clear to communicate but also so compelling that we didn’t need to do much to sell teams, they could see what the activity feed had accomplished. We had evolving documentation about the architecture and how to use the framework, in the API handbook. We had a codelab, which engineers could use for self study. In the lab you learn to build endpoints by building a little library app. We had workshops in which people did the code lab together with experienced API developers. We had tooling to learn about the system, such as the distributed tracing tool CrossStitch, the fanout manager informing about the complexity of the fetch, and datafindr for general information about API endpoints and example calls and results. Once it’s going, too fast to keep up After the word was spreading, so many people were making new endpoints that it was almost impossible for us to keep up with it. Initially, we alerted on each new endpoint, but had to switch this off due to the rate at which new endpoints were being built. The biggest motivator for adoption was that we communicated the gains. Everyone became interested in the performance gains and the cross-device-sharing through abstraction, which was a motivating incentive to switch. Also, there was a potential speedup through caching of endpoints. White space gets filled with code Immediately after adoption started, we saw some misunderstandings in the code that we did not foresee. Such as magic hiding in traits, inheritance between endpoints, while they contain just declarative static functions, or really complex code in bespoke endpoints, which should be library code. The minimalist code required to write an endpoint didn’t make explicit what is missing by accidental omission vs. deliberate design decisions. This also lead to confusion about which building blocks of endpoints are mandatory, about how to opt into a service, naming conventions, the location in the file system, and the complete route of an endpoint. This was a caused some cleanup work for the API team, but in fact it was a good thing, caused by fast and wide adoption of a new system, and is probably inevitable in that case. Documentation is critical We addressed the problems by improving our documentation, and creating an interactive endpoint generator that creates a file in the right place, with stub functions, and outputs the route. Also, we added format checks for endpoints in Jenkins. It was helpful to have the API code lab as a narrative introduction to gain practical experience fast, while also developing the API handbook as a reference manual. Two goals, two documents. Pockets of Patterns When we internally announced the project in which we unified the format for how an API endpoint is written, we started getting emails about what developers wanted from the API. And also: Mails about patterns that had emerged within the API framework without our knowledge. People had found and implemented solutions for their specific use cases. Our discussions had opened up a space to evaluate those patterns, and share them with all API endpoint developers! We’ve learned that we need to stay on top of new patterns as they emerge, and keep talking to developers about their needs. A subtle, but powerful paradigm shift: We, as the API framework developers, listen to endpoint developers and address everyone’s needs by evolving the framework. Instead of having developers using our API framework as a service, we are serving them. Also, we learned to trust our fellow developers even more! We underestimated their curiosity and willingness to try out new things, we underestimated the adoption rate, and we also underestimated how creative they would be with finding solutions for their specific use cases that we did not plan for. What an awesome surprise. 😀 Types too late + types too loose D: Also, we underestimated everyone’s willingness to do the work of typing their endpoint result. In the beginning, we made specifying a resultType optional, because we feared making it mandatory would slow the adoption. And if a type was specified, we only sampled the type errors, to not make correct typing a hurdle during API endpoint development, but rather a “nice to have” hint for when things go wrong. Not a guarantee. In retrospect, we could have saved ourselves a lot of extra work if we had made the resultType mandatory, and if we had made the type errors more prominent from the beginning. Etsy’s developers are generally happy about result types, they helped to implement a coarse grained gradual typing, and actively pushed us towards making the result types mandatory, to rely on them as a guarantee. Work in progress There remain four open problems we all touched upon: Versioning for the Apps – we got off the hook for JavaScript & PHP, but not the rest Active Cache Invalidation – it’s hard. Caching the API at different geographic locations (Edge) opening up v3 to 3rd party developers A question that comes with the developer adoption question is: What are we thinking in terms of preparing for third parties? This is a valid question, and we did not fully answer it yet, because we switched to generated client code in the meantime. Even though we started announcing the new API v3 on our developer mailing lists, all third party apps are still on version 2. The platform for v3 is ready to open up to 3rd parties, as soon as Etsy as a whole decides to make the switch. What did we learn? How were we transformed? This story is a case study of how the API first approach transformed the architecture, and also transformed how we work with and think about our API at Etsy. It covers a lot of ground and is influenced by our infrastructure and history. The story of architectural decisions and adoption is transferable to other systems though. So what did we learn from the decisions? Or from the decisions not being made yet? Were there surprises? We learned that we can seamlessly grow a system from a hack day project to a live system. Over time, a domain specific language of endpoints evolved, and the system grew according to the endpoint developers needs. Also, we learned a great deal about cURL or the according php extension! Not only does it allow to make parallel requests, but it also let’s us check on, control and modify the in-flight requests in a non-blocking way. Another realization is that we should have thought about caching early on. We added it in a hurry and are still working on active cache invalidation. So far, we can only use timeouts, which limits the class of endpoints that can be cached. Also, we have to think some more about variations due to different locales, which might only vary for parts of the response. A huge positive surprise was the HHVM experiment. By teeing traffic and trying out a completely new system, we solved our performance problems to this day. The textbook approach says: design APIs contract first. As we have seen, we circumvent that part by using automated client generation. Components help us with abstraction, but even the bespoke layer is very malleable. This is an interesting trick. Another surprising learning is that we should trust our developers not only to adopt and adapt to the new API framework, but also to go all the way and make typing of endpoints mandatory from the start. Also, we should keep the conversation ongoing to find out how the API framework can serve developers and their needs, instead of just designing it as a rigid service. We even found solutions that they had created themselves, and the framework could officially adopt them for everyone’s benefit. Did it work? How did it end? Did we succeed? Let’s assess this with a quote from Etsy’s Andrew Morrison , from before all this work started: “ We desperately need to figure out a scheme for allowing concurrency or else we’re going to have performance problems forever. ” YUP! We solved this, and a few unexpected things along the way. Despite initial problems with the extra layer, we figured out unconventional solutions by experimenting and organically growing the system towards the developers needs. Almost the entirety of etsy.com is powered by API v3 now. We are at a point where it is very easy to share new web features also with JavaScript and with the Etsy apps. Onwards? What we are discussing now is how to shift some of the complexity back towards the client. Maybe GraphQL is an interesting approach? Right now, the clients don’t know how the queries will be executed. This makes sense if you have services and teams and clear cut boundaries and interfaces, and if you have a contract in form of an API. Our approach is currently not structured like that. But could we compile an alternative, more knowledgeable PHP client, that lifts the composability from the HTTP layer into the API consumer code, in cases where we are our own consumer and create a website of the same tree structure? It’s clear in this case how many endpoints we are calling, and as long as it’s us, it’s safe to lift this control beyond the client, into the view. This is the third post in a series of three about Etsy’s API , the abstract interface to our logic and data. Posted by Stefanie Schirmer on November 8, 2016 Category: Uncategorized", "date": "2016-11-8,"},
{"website": "Etsy", "title": "Building a Better Mobile Crash Analytics Platform", "author": ["Russ Taylor"], "link": "https://codeascraft.com/2016/11/04/building-a-better-mobile-crash-analytics-platform/", "abstract": "Building a Better Mobile Crash Analytics Platform Posted by Russ Taylor on November 4, 2016 ‘Crashcan’ (think trashcan, but for crashes) is Etsy’s internal application for mobile crash analytics. We use an external partner to collect and store crash and exception data from our mobile apps, which we then ingest via their API. Crashcan is a single-page web app that refines and better exposes the crash data we receive from our external partner. Crashcan gives us extra analysis of our crashes on top of what our partner offers. We can make less cautious assumptions about our stack traces internally, which allows us to group more of them together. It connects crashes to our user data and internal tooling. It allows us to search for crashes in a range of versions. It’s provided a good balance between building our own solution from scratch and completely relying on an external partner. We get the ability to customize our mobile crash reporting without having to maintain an entire crash reporting infrastructure. Error Reporting – The Web vs. Mobile Unfortunately, collecting mobile crashes and exceptions is (of necessity) quite different from most error reporting on the web, in several ways: On the web (especially the desktop web), we can be fairly confident that a user is online – they’re less prone to flaky or slow connections. Plus, users don’t expect to be able to access the web when they don’t have a connection. ‘Crashes’ are very different on the web, so many exceptions and errors are less severe. Sure, users may need to refresh a page, but it’s rare that a web page will crash their browser. We can watch graphs and logs live as we deploy on the web (hooray, continuous deployment!) – and it’s clear if hundreds or thousands of exceptions start pouring in. With our mobile apps, however, we have to wait for users to install new versions after a release makes it to the App Store. Only then do we get to see exceptions. With mobile, when a crash occurs, we normally can’t send the crash until the app is launched again (with a data connection) – in some cases, this can be days or weeks. App crashes are costly to the user, as the app crashing loses the user’s state. On the web, even if a page breaks for some reason, the user keeps their browser history. With the web, there’s one version of Etsy at any point in time. It’s updated continuously, and every user is running the latest version, always. With the apps, we have to deal with multiple versions at once, and we have to wait for users to update. With these differences, it’s been important to approach the analysis of crashes and exceptions differently than we do on the web. A Crash Analytics Wish List Many of the issues mentioned above were handled by our external partner. But while this external partner provides a good overview of our mobile crashes, there were still some bits of functionality that would make analyzing crashes significantly easier for us. Some of the functionality we really wanted was: An easy way to filter broad ranges of app versions – like being able to specify 4.0-4.4 to find all crashes for versions between 4.0.0.0 and 4.4.999.999. Links between users’ accounts and specific crashes – like “This user reported they experienced this crash… Let’s find it.” This coupling with our user database allows us to better determine who is experiencing a crash – is it just sellers? Just buyers? Better crash de-duplication, specifically handling different versions of our apps and different versions of mobile operating systems. For example, crash traces may be almost identical, but with different line numbers or method names depending on the app version. But if they originate in the same place, we want to group them all together. Crash categorization – such as NullPointerException s versus OutOfMemory errors on Android – because some types of crashes are fairly easy to fix, while others (like OutOfMemory errors) are often systemic and unactionable. Custom alerting with various criteria – like when we experience a new crash with this keyword, or when an old version of our app suddenly experiences new API errors. It seemed like it’d be fairly straightforward to build our own application, using the data we were already collecting, to implement this functionality. We wanted to augment the data we receive from our external partner with data and couple it with our own internal tooling. We also wanted to provide any interested Etsy employees with a simple way to view the overall health of our apps. So that’s exactly what we chose to do. Building Crashcan Crashcan’s structure was a pretty wide-open space. All it really needed to do was provide crash ingestion from an API, process the crash a bit, and expose it via a simple user interface (it sounds a lot like many technologies, actually). So while the options for technologies and methodologies were open, we ultimately decided to keep it simple. By using PHP, Etsy’s primary development language, we keep the barrier to entry for developers at Etsy low . We used as much modern PHP as possible, with Composer handling our dependency management. MySQL handles the data storage, with Doctrine ORM providing the interface to the database. Data Ingestion Ingesting the data was the first hurdle. Before handling anything else, we needed to make sure that we could actually (1) get the data we wanted and (2) keep up with the number of crashes that we wanted to ingest, without breaking down our system. After all, if you can’t get the data you want and you can’t do so reliably, there’s really no point. After analyzing the API endpoints we had at our fingertips (yay, documentation!), we determined that we could get all the data we wanted. The architecture needed to allow us to: Determine whether we already have a crash (regardless of whether it has been deduplicated on our end) Keep track of deduplicated crashes, and link them to the originating crash from the external provider Run complex queries to combine data Analyze whether crashes are meeting specific thresholds, like whether a new crash has occurred at least n times Count crashes by category Filter everything by version range and other criteria In the end, we developed a schema that allowed us to fulfill all those needs while remaining quick in response to queries: To actually ingest the data from our external provider, we run a cron job every minute that checks for new crashes. This cron runs a simple loop – it loads new crashes from a paginated endpoint, looping through each page and each crash in turn. Each crash is added to a queue so that we can update it asynchronously. We run a series of workers that run continuously, monitoring the queue for incoming crashes. As these workers run, they each pick a crash off the queue and processes it. This includes several steps, first checking whether we have the crash already, then updating it if we have it or creating a new crash if we don’t. We also go through each crash’s occurrences to make sure that we’re recording each one and tying it to an existing user if one exists. The flowchart below demonstrates how these workers process crashes. Monitoring & Alerting After building Crashcan’s initial design and getting crashes ingesting correctly, we quickly realized that we needed utilities to monitor the data ingestion and to alert us when something went wrong. Initially, we had to manually compare crash counts in Crashcan with those that our external provider offered in their user interface. Obviously, this was neither convenient nor sustainable, so we began integrating StatsD and Nagios . To check that we were still ingesting all our crashes, we also wrote a script to perform a sort of ‘spot-check’ of our data against our external provider’s – which fails if our data differs too much from theirs. We created a simple dashboard, linked to StatsD, that allows us to see at-a-glance if the ingestion is going well – or if we’re encountering errors, slowness, or hitting our API limit. While we plan to improve our alerting infrastructure over time, this has been serving us well for now – though before we got our monitoring in a good state, we hit some big snags that kept us from being able to use Crashcan for weeks at a time. There’s an important lesson there: plan for monitoring and alerting from the beginning . Application Structure When deciding on Crashcan’s structure, we decided to focus first on building a stable, straightforward API . This would enable us to expose our crash data to both users and other applications – with one interface for accessing the data. This meant that it was simple for us to build Crashcan’s user interface as a Single Page Application . Very few of the disadvantages of single page applications applied in Crashcan’s case, since our users would only be other Etsy employees. Building a robust API also enabled us to share the data easily with other applications inside Etsy – most especially with our internal app release platform. When an Etsy engineer accesses Crashcan, we aim to present them with the most broadly applicable information first – the overall health of an app. This is presented through an overview of frequent crashes, common crash categories, and new crashes, along with a graph showing all crashes for the app split out by version. This makes it much easier to spot big new crashes or problematic releases. The engineer then has the option to narrow the scope of their search and view the details of specific crashes. Ongoing Work While we’ve finished Crashcan v1 with much of the core functionality and gotten it in a stable enough state that we can depend on its data, there’s still quite a bit that we’d like to improve. For example, we haven’t even begun to implement a couple of the items we mentioned in our wish list, like custom alerting. Second, the user interface could do with some bugfixes and refinement. Right now, it’s in a mostly-usable state that other Etsy engineers can at least tolerate, but it’s not stable or refined enough that we’d be comfortable releasing it to a wider audience. Additionally, our crash deduplication is still rudimentary. It only performs simple (and expensive) string comparisons to find similar crashes. We’d like to implement more advanced and more efficient crash deduplication using crash signature generation. This would give us a much more reliable way of determining when crashes are related, therefore providing a more accurate picture of how common each crash is. Lessons Learned Most of the pain points in Crashcan’s development weren’t new or especially unexpected, but they serve as a valuable reminder of some important considerations when building new applications. Build with monitoring and alerting in mind from the beginning. We could’ve avoided a several-week-long lapse in functionality had we focused on building in monitoring from the beginning. Don’t be afraid to consult with others on structural or technical decisions, and then just make a decision . It’s something that I’ve always struggled with – but getting blocked on making decisions or digging too deep into the minutiae of every decision is a great way to waste time. Document your assumptions – especially when dealing with APIs – as small assumptions can turn into big deals later on. This is what led to our biggest failure – we mistakenly assumed that crash timestamps were accurate. When a crash said it had occurred 4 days in the future, our app stopped updating, because it was checking only crashes occurring after that crash. Posted by Russ Taylor on November 4, 2016 Category: mobile Tags: analytics , mobile Related Posts Posted by David Schott on 07 Nov, 2018 Double-bucketing in A/B Testing Posted by Callie McRee and Kelly Shen on 03 Oct, 2018 How Etsy Handles Peeking in A/B Testing Posted by Daniel Marks , Bill Ulammandakh and Sungwan Jo on 25 Jan, 2017 Optimizing Meta Descriptions, H1s and Title Tags: Lessons from Multivariate SEO Testing at Etsy", "date": "2016-11-4,"},
{"website": "Etsy", "title": "How Etsy Uses Code “Slush” to Manage Development During the Holidays", "author": ["Jason Shen"], "link": "https://codeascraft.com/2016/11/16/code-slush-holidays/", "abstract": "How Etsy Uses Code “Slush” to Manage Development During the Holidays Posted by Jason Shen on November 16, 2016 Note: This article was adapted from an internal Etsy newsletter earlier this year. As the holidays roll around, it seemed like a timely opportunity to share what we do with a larger audience. As the calendar year draws to a close, people’s thoughts often turn to fun activities like spending time with family and friends and enjoying pumpkin or peppermint flavored treats. But for retailers, the holiday season is an intense and critically important period for the business. The months of November and December compose nearly a fifth of all US retail sales and pretty much every retailer needs to undertake special measures during the holidays, from big sales promotions to stocking up on popular items, to hiring additional staff to stock inventory and reduce wait times at checkout. A lot of these measures apply as well to digital retailers, with the added risk of the entire site running slowly or not at all. In 2015, Neiman Marcus experienced an extended outage on Black Friday and Target and PayPal were intermittently down on Cyber Monday. Etsy is no stranger to this holiday intensity. This is the biggest shopping season of the year for us and we typically receive more site visits than at other times, which translate into more orders. Over the years, our product, engineering, and member-facing organizations have developed practices and approaches to support our community during the intensity of the holidays. How Etsy Handles the Holidays The increase in site traffic and transactions impacts many areas of the business. Inbound support emails and Non-Delivery Cases reach a peak in December, and the Trust and Safety team ramps down outside project work and hiring efforts to focus on providing exceptional support. “Emotions tend to be more heightened around the holidays” says Corinne Haxton Pavlovic, head of the trust and safety team at Etsy, “There’s a lot on the line for everyone this time of year – highs can feel higher and lows can feel lower. We have to really dig into our EQ and make sure that we’re staying neutral and empathetic.” For our sellers, the holidays can be an exciting but scary time: “the Etsy sales equivalent of Laird Hamilton surfing a 70-ft wave off Oahu’s North Shore” says Joseph Stanek, seller account manager. Stanek works with a portfolio of top Etsy sellers to advise and support their business growth. He’s found that many sellers spend an enormous amount of effort on holiday sales promotion, and are then hit with a record number of orders. They’re pushed to increase their shipping and fulfillment capabilities, which can serve “as a kind of graduation” as they level up to a new tier of business. With huge numbers of buyers browsing for the perfect gift, and sellers working hard to manage orders, it’s critically important for Etsy’s platform to be as clear and reliable as possible. That’s why the period between mid-October through December 31st is a time to be exceptionally careful and more conservative than usual about how we make changes to the site. We affectionately refer to this period as “Slush.” The Origins of Slush The actual term “Slush” is a play on the phrase “code freeze,” which is when a piece of software is deemed finished and thus no new changes are being made to the code base. “Code freezes help to ensure that the system will continue to operate without disruptions,” says Robert Tekiela at CTO Insights . It’s a way to prevent new bugs from being created and and are “commonly used in the retail industry during holiday shopping season when systems load is at a peak.” Since at Etsy, we still push code during the holidays, just more carefully, it’s not a true code freeze, but a cold, melty mixture of water and ice. Hence, Slush. According to Jason Wong, Slush at Etsy got started sometime after current CEO Chad Dickerson became CTO in the fall of 2008. As an engineering director of infrastructure, Jason has been a key part of Etsy’s platform stability since joining in 2010. Back then, Etsy’s infrastructure was less robust and the team was still figuring out how to effectively support the already high levels of traffic on Etsy.com. There was not yet a process for managing code deployments during the holidays and the site experienced more crashes than it does today. . Said Wong, “the question was: during the holiday season, a high traffic, high visibility time when we made a significant portion of our [gross merchandise sales], how do we stabilize the site? That’s where Slush got started.” Here’s the slightly redacted email from Chad that kicked off the idea of Slush. From: Chad Dickerson Date: Fri, Oct 31, 2008 at 4:08 PM Subject: holiday “slush” (i.e. freeze) — need your input To: adminName and adminName Cc: adminName, adminName, and adminName adminName / adminName, adminName, adminName, and I met recently to discuss a holiday “freeze” beginning at end of day on November 14.  We’re calling it a “slush” because there are certain types of projects that we can still do without making critical changes to the database or introducing bugs.  The goal with setting this freeze is to eliminate the distractions of any projects not on the must-do list so we can focus on the most important projects exclusively. adminName, adminName, adminName, and I met earlier this week to mutually agree on what projects we need to complete before the freeze/slush beginning at the end of the day on 11/14.   We came up with a list of “must do” projects that need to get done by 11/14, and “nice to have” projects if we complete the must-dos: [ Link to Document Detailing Slush Plan ] There are couple of projects that we’ve discussed already on the list, like Super Etsy Mini and BuyHandmade blog.  I wanted to make sure that any “must do” projects in your worlds are reflected and prioritized.  On Monday, we’re going to start doing daily standups at 11:30am to track progress against the agreed-upon list of projects leading up to 11/14.   Since there will be 9.5 working days to execute, we need to freeze the list itself by Monday.  Can you review the list and let us know if you have additions that we are missing that we should discuss?  Thanks. Chad Learning to Build Safely In the early days, Slush was far more strict, in part because Etsy’s infrastructure was not as robust and mature as it is today. We operated off of a federated database model, which in theory was meant to prevent one database crash from affecting another, but in practice, it was hard to keep clusters from affecting one another and site stability suffered as a result. This technical approach also made it hard to understand what went wrong and how the team could fix it. Engineers went from deploying five times a day to once a day. Feature flags were tested thoroughly so that major features like convos or add to cart could be turned off without shutting down the site. Over the past few years, a major effort was made to get Etsy’s one really big box called master onto a sharded database model. With a sharded system, data is distributed across a series of smaller active-active pairs so that if single database goes down, there is an exact replica with the data intact. This system is faster, more scalable, and resilient compared to the prior method of simply storing all the data in one really big box. In 2016, we successfully migrated all of our key databases, including receipts, transactions, and many others, “to the shards” and decommissioned the old master database. Developing continuous deployment was also a major feat which allowed Etsy to develop A/B testing and feature flagging. These technical efforts, in conjunction with our culture of examining failures through blameless postmortems, have allowed allowed Etsy to get better at building safely. Today our engineering staff and systems are studied by organizations around the world. Slush Today and Tomorrow Within the engineering organization, there’s often a senior staff member who helps organize Slush. The role is an informal one, meant to share best practices and encourage the product org to be mindful of the higher stakes of the holidays. Tim Falzone, an engineering manager on Infrastructure, took on this role in 2015 and presented a few slides at the September Engineering All-Hands which highlight the way we handle Slush today. Today, Slush means that major buyer and seller facing feature changes are put on hold, or pushed “dark,” where they are hidden behind config flags and not shown publicly. Additionally, engineers get more people to review their pull requests of code changes. These extra precautions are taken to ensure that the site runs quickly and with minimal errors or downtime even with the increased traffic. Falzone says that now, Slush is less about not breaking the site and more about preventing disruptions for members. “You could easily make something that works flawlessly but tanks conversion or otherwise sends buyers away, or is really slow,” he explained. For sellers, managing a huge wave of orders means relying on muscle memory of how Etsy works, which means that the holidays is a bad time to change the workflow or otherwise add friction for our sellers, who often become profitable on their business for the year during this time. As Etsy grows, Slush will continue to evolve. A more powerful platform also means more points of integration. More traffic means more pressure on parts of the platform. Even as we work to secure more headroom for our infrastructure and develop tooling to stress-test our systems, we will always be challenged in new ways. Though we’ve come a long way, Slush will continue to be a helpful reminder to move safely during a critical time of the year for our members and our organization. Posted by Jason Shen on November 16, 2016 Category: operations , philosophy Tags: culture , holidays , safety , slush Related Posts Posted by Toria Gibbs and Ian Malpass on 10 Aug, 2016 Recommended Reading for Allies Posted by John Goulah on 15 Sep, 2015 Assisted Serendipity – Fostering Peer to Peer Connections in Your Organization", "date": "2016-11-16,"},
{"website": "Etsy", "title": "SEO Title Tag Optimization at Etsy: Experimental Design and Causal Inference", "author": ["Bill Ulammandakh"], "link": "https://codeascraft.com/2016/10/25/seo-title-tag-optimization/", "abstract": "SEO Title Tag Optimization at Etsy: Experimental Design and Causal Inference Posted by Bill Ulammandakh on October 25, 2016 External search engines like Google and Bing are a major source of traffic for Etsy, especially for our longer-tail, harder to find items, and thus Search Engine Optimization (SEO) is important in driving efficient listing discovery on our platform. We want to make sure that our SEO strategy is data-driven and that we can be highly confident that whatever changes we implement will bring about positive results. At Etsy, we constantly run experiments to optimize the user experience and discovery across our platform, and we therefore naturally turned to experimentation for improving our SEO performance. While it is relatively simple to set up an experiment on-site on our own pages and apps, running experiments with SEO required changing how Etsy’s pages appeared in search engine results, over which we did not have direct control. To overcome this limitation, we designed a slightly modified experimental design framework that allows us to effectively test how changes to our pages affect our SEO performance. This post explains the methodology behind our SEO testing, the challenges we have come across, and how we have resolved them. Experiment Methodology For one of our experiments, we hypothesized that changing the titles our pages displayed in search results (a.k.a. ‘title tags’) could increase their clickthrough rate. Etsy has millions of pages generated off of user generated content that were suitable for a test. Many of these pages also receive the majority of their traffic through SEO. Below is an example of a template we used when setting up a recent SEO title tag experiment. We were inspired by SEO tests at Pinterest and Thumbtack and decided to set up a similar experiment where we randomly assigned our pages into different groups and applied different title tag phrasings shown above. We would measure the success of each test group by how much traffic it drove relative to the control groups. In this experiment, we also set up two control groups to have a higher degree of confidence in our results and to be able to quality check our randomized sampling once the experiment began. Sampling We took a small sample of pages of a similar type while ensuring that our sample was large enough  to allow us to reach statistical significance within a reasonable amount of time. Because visits to individual pages are highly volatile, with many outliers and fluctuations from day to day, we had to create relatively large groups of 1000 pages each to expect to reach significance quickly. Furthermore, because of the high degree of variance across our pages, simple random sampling of our pages into test groups was creating test groups different from each other in a statistically significant way even before the experiment began. To ensure our test groups were more comparable to each other, we used stratified sampling , where we first ranked the the pages to be a part of the test by visits, broke them down into ntile groups and then randomly assigned the pages from each ntile group into one of the test groups, ensuring to take a page from each ntile group. This ensured that our test groups were consistently representative of the overall sample and more reliably similar to each other. We then looked at the statistical metrics for each test group over the preceding time period, calculating the mean and standard deviation values by month and running t-tests to ensure the groups were not different from each other in a statistically significant way. All test groups passed this test. Estimating Causal Impact Although the test groups in our experiment were not different from each other at a statistically significant level before the experiment, there were small differences that prevented the estimation of the exact causal impact post treatment. For example, test group XYZ might see an increase relative to control B, but if Control B was slightly better than test groups XYZ even before the experiment began, simply taking the difference between of the two groups would not be the best estimate of the difference the treatment had effected. One common approach to resolve this problem is to calculate the difference of differences between the test and control groups pre- and post-treatment. While this approach would have worked well, it might have created two different estimated treatment effect sizes when comparing the test groups against the two different control groups. We decided that, instead, using Bayesian structural time series analysis to create a synthetic control group incorporating information from both the control groups would provide a cleaner analysis of the results. In this approach, a machine learning model is trained using pre-treatment data to predict the performance of each test group based on its covariance relative to its predictors — in our case, the two control groups. Once the model is trained, it is used to generate the counterfactual, synthetic control groups for each of the test groups, simulating what would have happened had the treatment not been applied. The causal impact analysis in this experiment was implemented using the CausalImpact package by Google. Results We started seeing the effects of our test treatments as soon as a few days after the experiment start date. Even seemingly very subtle title tag changes resulted in large and statistically significant changes in traffic to our pages. In some test groups, we saw significant gains in traffic. While in others, we saw no change. And in some others, we even saw a strong negative change in traffic. A-A Testing The two control groups in this test showed no statistically significant difference compared to each other after the experiment. Although a slight change was detected, the effect did not reach significance. Post-experiment rollout validation Once we identified the best performing title tag, the treatment was rolled out across all test groups. The other groups experienced similar lifts in traffic and the variance across buckets disappeared, further validating our results. The fact that our two control groups saw no change when compared to each other, and also the fact that the other buckets experienced the same improvement in performance once the best performing treatment was applied to them gave us strong basis for confidence in the validity of our results. Discussion It appeared in our results that shorter title tags performed better than longer ones. This might be because for shorter, better targeted title tags, there is a higher probability of a percentage match (that could be calculated using a metric like the Levenshtein Distance between the search query and the title tag) against any given user’s search query on Google. In a similar hypothesis, it might be that using well-targeted title tags that are more textually similar to common search terms helps to increase percentage match to Google search terms and therefore improves ranking. However, it is likely that different strategies work well for different websites, and we would recommend rigorous testing to uncover the best SEO strategy tailored for each individual case. Takeaways Have two control groups for A-A testing. This allowed us to have much greater confidence in our results. The CausalImpact package can be used to easily account for small differences in test vs. control groups and estimate the differences of treatments more accurately. For title tags, it is most likely a best practice to use phrasing and wording that would maximize the probability of a low Levenshtein distance match from popular target search queries on Google Image Credits: Visualization of Stratified Sampling Posted by Bill Ulammandakh on October 25, 2016 Category: data , engineering Tags: analytics , data science , experimentation , marketing , marketing analytics , seo Related Posts Posted by David Schott on 07 Nov, 2018 Double-bucketing in A/B Testing Posted by Callie McRee and Kelly Shen on 03 Oct, 2018 How Etsy Handles Peeking in A/B Testing Posted by Daniel Marks , Bill Ulammandakh and Sungwan Jo on 25 Jan, 2017 Optimizing Meta Descriptions, H1s and Title Tags: Lessons from Multivariate SEO Testing at Etsy", "date": "2016-10-25,"},
{"website": "Etsy", "title": "API First Transformation at Etsy – Operations", "author": ["Stefanie Schirmer"], "link": "https://codeascraft.com/2016/09/26/api-first-transformation-at-etsy-operations/", "abstract": "API First Transformation at Etsy – Operations Posted by Stefanie Schirmer on September 26, 2016 This is the second post in a series of three about Etsy’s API, the abstract interface to our logic and data. The previous post is about concurrency in Etsy’s API infrastructure . This post covers the operational side of the API infrastructure. Operations: Architecture Implications How do the decisions for developing Etsy’s API that we discussed in the first post relate to Etsy’s general architecture? We’re all about Do It Yourself at Etsy. A cloud is just other people’s computers, and not in the spirit of DIY; that’s why we rather run our own datacenter with our own hardware. Also, Etsy kicks it old school and runs on a LAMP stack. Linux, Apache, MySQL, PHP. We’ve already talked about PHP being a strictly sequential, single-threaded, shared-nothing environment, leading to our choice of parallel cURL. In the PHP world, everything runs through a front controller, for example index.php. In that file, we have to include other PHP files if we need them, and to make that easier, we usually use an autoloader to help with dependencies. Every web request gets a new PHP environment in its own instance of the PHP interpreter. The process of setting up that environment is called bootstrap. This bootstrapping process is a fixed cost in terms of CPU work, regardless of the actual work required by the request. By enabling multiple, concurrent HTTP sub-requests to fetch data for a single client request, this fixed cost was multiplied. Additionally, this concurrency encouraged more work to be done within the same wall clock time. Developers built more diverse features and experiences, but at the cost of using more back-end resources. We had a problem. Problem: PHP time to request + racking servers D: As more teams adopted the new approach to build features in our apps and on the web, more and more backend resources were being consumed, primarily in terms of CPU usage from PHP. In response, we added more compute capacity, over time growing the API to four times the number of servers prior to API v3. Continuing down this path we would have exhausted space and power in our datacenters. This was not a long term solution. To solve this, we tried several strategies at once. First, we skipped some work by allowing to mark some subrequests as optional. This approach was abandoned because people used it as a graceful error recovery mechanism, triggering an alternate subrequest, rather than for optional data fetches. This didn’t help us reduce the overall work required for a given client request. Also, we spent some time optimizing the bootstrap process. The bootstrap tax is paid by all requests and subrequests, making it a good place to focus our efforts. This initially showed benefit with some low hanging fruit, but it was a moving target in a changing codebase, requiring constant work to maintain a low bootstrap tax. We needed other ways of doing less work. A big step forward was the introduction of HTTP response caching. We had to add caching quickly, and first tried the same cache we use for image serving, Apache Traffic Server. While being great for caching large image files, it didn’t work as well for smaller, latency sensitive API responses. We settled on Varnish, which is fast and easy to configure for our needs. Not all endpoints are being cached, but for cached ones, Varnish will serve the same response many times. We accept staleness for a small 10 – 15 minute period, drastically reducing the amount of work required for these requests. For the cacheable case, Varnish handles thousands of requests per second with a 80% hit rate. Because the API framework requires input parameters to be explicit in the HTTP request, this meshed well with introducing the caching tier. The framework also transparently handles locale, passing the user’s language, currency and region with every subrequest, which Varnish uses to manage variants. The biggest step forward came from a courageous experiment. Dan from the core team looked at bigger organizations that faced the same problem, and tried out facebook’s hhvm on our API cluster. And got a rocketship. We could do the same work, but faster, solving this issue for us entirely. The performance gain from hhvm was a catalyst for performance improvements that made it into PHP7. We are now completely switched over to PHP7 everywhere, but it’s unclear what we would have done without hhvm back in the day. In conclusion, concurrency proved to be great for logical aggregation of components, and not so great for performance optimization. Better database access would be better for that. Problem: Balancing the load If we have a tree-like request with sub-requests, a simple solution would be to route this initial request via a load balancer into a pool, and then run all subrequests on the same machine. This leads to a lumpy distribution of work. The next step from here is one uniform pool, and routing the subrequests back into that pool again, to have a good balance across the cluster. Over time (and because we experimented with hhvm), we created three pools that correspond to the three logical tiers of endpoints. In this way, we can monitor and scale each class of endpoints separately, even though each node in all three clusters works the same way. Where would this not work? If we sit back and think about this for a bit – how is this architecture specific to Etsy’s ecosystem? Where wouldn’t it work? What are the known problems? The most obvious gaping hole is that we have no API versioning. How do we even get away with that? We solve this by keeping our public API small and our internal API very very fluid. Since we control both ends of the internal API via client generation and meta-endpoints, the intermediate language of domain objects is free to evolve. It’s tied into our continuous deployment system, moving along with up to 60 deploys per day for etsy.com. And the client is constantly in flux for the internal API. And as long as it’s internal at Etsy, even the outside layer of bespoke AJAX endpoints is very malleable and matures over time. Of course this is different for the Apps and the third party API, but those branch off after maturing on the internal API service over several years. Software development companies who focus on an extensive public API or even have that as the main service could not work in this way. They would need an internal place to let the API endpoints mature, which we do on the internal API service that is powering Etsy. We know there are very technical solutions to version changes being used in our industry, such as ESPN having a JSON schema , and publishing just a schema change, like a diff, which can be smaller than 100k. That’s really exciting, but we’re just not at the point where this is our most important priority, since we don’t have too many API consumers at Etsy yet. We ourselves are our biggest consumer, and generated clients shield us from the versioning problem for now, while giving us the benefit of a monorepo-like ecosystem, in which we can refactor without boundaries between PHP and JavaScript. Operations: Tooling Let’s talk about tooling that we built to learn more about the behavior of our code in practice. Most of the tools that we developed for API v3 are around monitoring the new distributed system. CrossStitch: Distributed tracing As we know, with API v2, we had the problem that almost an arbitrary amount of single threaded work could be generated based on the query parameters, and this was really hard to monitor. Moving from the single-threaded execution model to a concurrent model triggering multiple API requests was even more challenging to monitor. You can still profile individual requests with the usual logging and metrics, but it’s hard to get the entire picture. Child requests need to be tied to back to the original request that triggered them, but they themselves might be executed elsewhere on the cluster. To visualize this, we built a tool for distributed tracing of requests, called CrossStitch. It’s a waterfall diagram of the time spent on different tasks when loading a page, such as HTTP requests, cache queries, database queries, and so on. In darker purple, you can see different HTTP requests being kicked off for a shop’s homepage, for example you see the request for the shop’s about page is running in parallel with requests for other API components. Fanout Manager: Feedback on fanout limit exhaustion for developers Bespoke API calls can create HTTP request fanout to concurrent components, which in turn can create fanout to atomic component endpoints. This can create a strain on the API and database servers that is not easy for an endpoint developer to be aware of when building something in the development environment or rolling out a change to a small percentage of users. The fanout manager aims to put a ceiling on the overall resource requests that are in flight, and we are doing this in the scheduler part of the curl callback orchestrator by keeping track of sub-requests in memcached. When a new request hits the API server, a key based on the unique user ID of that root request is put into memcached. This key works as a counter of parallel in-flight requests for that specific root request. The key is being passed on to the concurrent and component endpoint subrequests. When the scheduler runs a subrequest, it increments the counter for that key. When the request got a response and it’s slot is freed in the scheduler, the counter for the key is decremented. So we always know how many total subrequests are in-flight for one root request at the same time. In a distributed system like this, multiple requests can be competing for the same slot. We have a problem that requires a lock. To avoid the lock overhead, we circumvent the distributed locking problem by relying on memcached’s atomic increment and decrement operation. We optimistically first increment the memcached key counter, and then check whether the operation was valid and we actually got the slot. Sometimes we have to decrement again because this optimistic assumption is wrong, but in that case we are waiting for other requests to finish anyway and the extra operation makes no difference. If an endpoint has too many sub-requests in flight, it just waits before being able to make the next request. This provides a good feedback for our developers about the complexity of the work before the endpoint goes into production. Also, the fanout limit can be hand-tweaked for specific cases in production, where we absolutely need to fetch a lot of data, and a higher number of parallel requests speeds up that fetching process. Automated documentation of endpoints: datafindr We also have a tool for automated documentation of new endpoints. It is called datafindr. It shows endpoints and typed resources, and example calls to them, based on a nightly snapshot of the API landscape. Wanted: Endpoint decommission tool Writing new endpoints is easy in our framework, but decommissioning existing endpoints is hard. How can we find out whether an existing endpoint is still being used? Right now we don’t have such a tool, and to decommission an existing endpoint, we have to explicitly log whether that specific endpoint is called, and wait for an uncertain period of time, until we feel confident enough to decide that no one is using it any more. However, in theory it should be possible to develop a tool that monitors which endpoints become inactive, and how long we have to wait to gain a statistically significant confidence of it being out of use and safe to remove. This is the second post in a series of three about Etsy’s API , the abstract interface to our logic and data. The next post will be published in two weeks, and will cover the adoption process of the new API platform among Etsy’s developers. How do you make an organization switch to a new technology and how did this work in the case of Etsy’s API transformation? Posted by Stefanie Schirmer on September 26, 2016 Category: api , engineering , infrastructure , mobile , operations , philosophy Tags: API , mobile , operations , PHP Related Posts Posted by Stefanie Schirmer on 06 Sep, 2016 API First Transformation at Etsy – Concurrency Posted by Liz Wald on 21 Sep, 2010 Etsy API Handmade Code Contest: Update #2 – Judging is happening!", "date": "2016-09-26,"},
{"website": "Etsy", "title": "API First Transformation at Etsy – Concurrency", "author": ["Stefanie Schirmer"], "link": "https://codeascraft.com/2016/09/06/api-first-transformation-at-etsy-concurrency/", "abstract": "API First Transformation at Etsy – Concurrency Posted by Stefanie Schirmer on September 6, 2016 At Etsy we have been doing some pioneering work with our Web APIs . We switched to API-first design, have experimented with concurrency handling in our composition layer, introduced strong typing into our API design, experimented with code generation, and built distributed tracing tools for API as part of this project. We faced a common challenge: much of our logic was implemented twice. All of the code that was built for the website then had to be rebuilt in our API to be used by our iOS and Android apps. Problem: repeated logic between platforms We wanted an approach where we built everything on reusable API components that could be shared between the web and apps. Unfortunately our existing API framework couldn’t support this shared approach. The solution we settled on was to abandon the existing framework and rebuild it from scratch. Follow along this case study of building an API First architecture, in which functional changes are expressed on the API level before integrating them into the website. Hear what problems prompted this drastic change. Learn which new tools we had to build to be able to work with the new system and what mistakes we made along the way. Finally, how did it end? How did the team adopt the new system and have we succeeded in our goals of API First? This post will be the first post in a series about our current API infrastructure, which we call version 3. The series is based on a talk at QCon New York . The first post will cover concurrency, the second post will cover operations and the third post the human aspects of our API transition. First problem: More devices & platforms (also: JavaScript) If we look into the future, it comes with lots of devices. Mainframes became desktop computers, which became portable laptops and tablets, smart phones and watches. This trend has been going on for a while, and in order to not reinvent the world on each different device, we started sharing data via an internal API years ago. The first version of Etsy’s API was a gateway for flash widgets. And the second one was a JSON RESTful API for 3rd parties and internal use. It was tightly coupled to the underlying database schema, and it empowered clients to make customized complex requests. It was so powerful that when we introduced our first iPad App, we did not need to write any new endpoints, and could build it solely on existing ones. Clients could request multiple resources at once, for example request shop data and also include listing data from that shop, and they could specify fields to trim down the response to just the required data. Very powerful. Second Problem: Performance & complexity control With great power comes great responsibility, and this approach had some drawbacks. The server code was simple, but we did not know the incoming parameters. We gave the clients control over the complexity of the request via the request parameters. This obviously had implications on server-side performance. And measuring the performance was difficult, because it was not clear if an increased response time was due to the performance of our backend, or because the client requested more resources. Third Problem: Repetition & inconsistency Years of changing patterns and an evolving complex codebase with MVC architecture led to bad habits: data fetch during template rendering, and logic in the templates. Our API was for AJAX, whereas the backend code was in PHP.  We did not have the logic in one place that was reusable for both the Web and API. This lead to inconsistencies between API and pre-API web. The schema of the API resource was a snapshot of the data model at the time of exposing it via the endpoint. This one-to-one mapping caused problems with data migrations, as the API resource was “frozen in time”. Should it change with the model? How long should the old resource structure be supported? Requirements for API-first We re-discussed the requirements for our API. If performance, manifesting for the user as latency from request to response, was a problem, what was the bottleneck? First, the time to glass , the time until we see something on our device’s screen, as Ilya Grigorik calls it in his talk “breaking the 1000 milliseconds time to glass”, and he states that due to mobile network speed, we have only 100 milliseconds on the server side if we want to stay in budget. The second problem is that we, at Etsy, come from a sequential-shared-nothing-php-world. No built-in concurrency. How can we parallelize and reuse our work , while still keeping the network footprint low? API v2: repeated logic between platforms          API v3: reusable components Other requirements were how to think about caching . The previous version of the API was memcached only, caching calls including parameters, which lead to a granularity problem. And one last requirement was to solve the problem starting from what we know and what we’re good at – building our own solutions in PHP. Shaping our mental model Based on these learnings, we piece-by-piece architected a new version, called API Version 3. REST resources worked well for both mobile apps and traditional web, so that was a keeper. A new idea was to decouple the endpoints from the framework that hosts them. Minimize the endpoints’ responsibilities to: declaring the route declaring the input expectations and the output guarantees implementing what happens in the endpoint. .. and that’s about it. We have one very simple, declarative file for each endpoint. Everything else is architected away on purpose: StatsD error monitoring, endpoint input and output type checks, and the compilation of the full routes — all of this is handled by the framework. Authentication and access control is also handled there, based on the class of endpoint that the developer has chosen. Enter the meta-endpoint We picked up the industry ideas from Netflix and eBay’s ql.io of server side composition of resources into device-view-specific resources. Or in other words: allowing a second layer of endpoints that are consumers of our own API, requesting and aggregating other endpoints. This means the server itself is also a client of the API, making the server more complex, while giving it more control with an extra layer for code execution. This improves performance of the client, because it only needs to make one single request – the biggest bottleneck if we want to have a responsive mobile interface! These requests used our generated PHP client, and they used cURL. cURL? Let’s talk about this for a bit. And let’s take a step back. The interesting question is how to bring concurrency into the single-threaded world of PHP. cURL is cool We’re in an HTTP context, so what about making additional HTTP requests for concurrency? We examined whether this could be done with cURL. Some time in 2013, Paul tweeted “curl_multi_info_read() is my new event loop.” In a hack week project, Paul and Matt from Etsy’s core team figured out that we could in fact achieve concurrency in the HTTP layer, through parallel cURL calls with curl_multi_info read. The HTTP layer is an interesting layer for this, since there are many existing solutions for routing, load balancing and caching. In addition to cURL, we added logic to establish dependencies on requests to other endpoints, which we call proxies. We are running the requests when the corresponding proxy becomes unblocked, similar to an event loop, which you might know from NodeJS. The whole concurrency dependency analysis and scheduling is encapsulated within one piece of software, which we call the curl callback orchestrator. This is great, because from the endpoint author’s point of view the code looks sequential and single-threaded and is just a list of proxy calls to other endpoints. We’re getting closer to a declarative style, expressing our intent, and the orchestrator figures out how to schedule the calls that are necessary for the complete result. You Wouldn’t Reimplement an API … Ok, so we had some good observations about the previous versions of our API, and we have a working prototype for concurrency via cURL. How did we grow an entire new API framework from here? Perspectives and Services Two concepts are special about Etsy’s API v3: perspectives and services. Perspectives clarify data access rules and give us security hints on what code is permitted for each perspective. They express on whose behalf an API call is being made. So, for example, the Public perspective shows data that a logged-out user would be able to see on Etsy.com. The Member perspective is for calls made on behalf of a particular Etsy member. The user ID is determined via the user cookie or OAuth token, dependent on the Service, which we will talk about below. The Shop perspective is similar to the member perspective but is for a shop. The framework will verify that the given shop is owned by the authenticated user. The Admin perspective is like the member perspective but for Etsy Admin. We occasionally want to take actions from our own servers that may not fit the other perspectives. For this we have the Infrastructure perspective. It is only available on the private internal API and can be used for things such as dataset loading. The application perspective is for calls made on behalf of a particular API application. It contains the application data for the verified API key. While perspectives express on whose behalf a call is being made, the service indicates from where the call is being made. A service can also be thought of as the entry point into the API framework. Each service has its own requirements regarding authentication. Endpoints are included in some services by default. Other services are opt-in, and each endpoint has to declare whether it wants to be exposed on those opt-in services. The Ajax service is accessible from pages that run JavaScript on etsy.com. The Admin service is accessible from pages that run JavaScript on our internal admin tools platform. The internal service is used by other API services that are already inside of our API cluster network. The Apps service is accessible from our native apps in iOS and Android. The 3rd party service is for 3rd party app developers. The services separate different application domains. An example API call Let’s look at an example request to the etsy.com homepage. We know what the homepage looks like: sections of information that might be interesting for me, as a potential buyer. Up at the top are the listings that I favorited, then some picks that Etsy’s recommendation algorithms picked for me, new items from my favorite shops, activity from my friends, and so on. I think about it as something like this. If we look at the data in more detail, we see even more structure. It’s like a tree, growing from left to right. Our setup of network and servers is mirroring the structure of the API call. It starts with an HTTP request from my browser to Etsy’s web server. From there, a bespoke API request is being made to our API server, requesting a personalized version of the homepage data. Internally, this request consists of multiple concurrent components. They themselves are fetched via API requests. Such as my favorites, which are a concurrent component, because they are a large number of listing cards that can be fetched in parallel. So we can imagine an API request as a multi-level tree, kicking off other API requests and constructing an overall result from the results of those subrequests. Domain specific language of API endpoints The project that got me started diving deep into Etsy’s API v3 framework was striving to unify the syntax of API endpoints. This was really fun and involved big, automated changes to unify the API codebase. In the past, there were multiple styles in which endpoints could be written. To unify them, we carved out a language of endpoint building blocks. Some building blocks are mandatory for each endpoint. Each endpoint needs to declare its route, so we know where it should be found on the web. Also, it needs a human readable description, and a resultType. The result type describes what type of data the endpoint returns. All data we return is JSON encoded, but here we can say that we return a primitive data type, such as a string or a boolean inside that encoding. Or we could return what we call “a typed resource” – a compound type that refers to a specific component of the Etsy application domain, such as a ListingCard. And then there is the handle function. In there, every endpoint runs the code that it needs to run, to build its response. Optional building blocks of an API endpoint are also possible. declareInput is only necessary if the endpoint does actually need input parameters. If it doesn’t, the function can be left out. The includedServices function allows an endpoint to opt into specific services. The EtsyApps service is opt-in for example, so if you want to make your endpoint available on the apps, you have to opt into the EtsyApps service via this function. And then there is the cacheTtlSeconds function, which allows you to specify whether an endpoint should be cached, and what should be it’s time to live. Input and output: Typed parameters, typed result The first step when a request is being routed to the endpoint, is the setup of the input parameters. We create an input object based on the request’s URL and the endpoint’s declareInput function. The input declaration tells us how to check for optional or mandatory input parameters, which are parsed according to a pattern in the route. If a parameter is missing or of the wrong type, the framework returns an HTTP error code and message. The input declaration specifies a type for each parameter, such as a string or a user ID. The types are Etsy-specific, and each one comes with its own validation function which is being run by the framework. According to the perspective, information about the logged in user, the logged in admin, shop, or authenticated app is being checked as well, and added to the input object. Each endpoint specifies its own output type via the resultType function. Currently, those types are optional and of different level of detail. We encourage developers to either return a primitive datatype, or to build a compound type, called typed resource, corresponding to the shape of the data that their endpoint returns. Type guarantees are useful for the API clients, and bring us one step closer to having guarantees on our data from the browser input field to the the database record. To make our framework complete, we’re still missing some action on both ends. How does an API request get routed to an endpoint? And how can we make an API request from our code, for example inside a meta-endpoint or in JavaScript when our site uses AJAX? Tooling: API compiler We need two more pieces of software, which we can automatically compile based on the endpoint declaration files. This is the job of the API compiler. Initially, this was a script that took the routes from the endpoint declarations, together with the service and perspective information, and compiled these into full routes for apache by modifying the .htaccess files. Performance concerns were alleviated by splitting up the work and files by perspective. Over time, we also added a second part: the generation of API client code in PHP and in JavaScript. The code is being generated using a mustache template, which is a template language for websites, but works well in this context, too. Before we deploy code to Etsy.com, we check if the compiled routes and client code are up to date via Jenkins. In this way, we control both ends of the API stack from the database access code to the outer shape of the endpoint landscape, which is reflected in changes to the client. And we neatly tie this into our continuous deployment process. This is the first post in a series of three about Etsy’s API, the abstract interface to our logic and data. The next post covers the operational side of Etsy’s API . Posted by Stefanie Schirmer on September 6, 2016 Category: api , engineering , infrastructure , mobile , philosophy Tags: API , concurrency , mobile , PHP Related Posts Posted by Stefanie Schirmer on 26 Sep, 2016 API First Transformation at Etsy – Operations Posted by Liz Wald on 21 Sep, 2010 Etsy API Handmade Code Contest: Update #2 – Judging is happening!", "date": "2016-09-6,"},
{"website": "Etsy", "title": "How Etsy Formats Currency", "author": ["Aditya Bhargava"], "link": "https://codeascraft.com/2016/04/19/how-etsy-formats-currency/", "abstract": "How Etsy Formats Currency Posted by Aditya Bhargava on April 19, 2016 Imagine how you would feel if you went into a grocery store, and the prices were gibberish (“1,00.21 $” or “$100.A”). Would you feel confident buying from this store? Etsy does business in more than 200 regions and 9 languages. It’s important that our member experience is consistent and credible in all regions, which means we have to format prices correctly for all members. In this post, I’ll cover: Examples of bad currency formatting How you can format currency correctly Practical implementation decisions we made along the way In order to follow along, you need to know one important thing: Currency formatting depends on three attributes: the currency, the member’s location, and the member’s language. Examples of bad currency formatting Here are some examples of bad currency formatting: A member browsing in German goes to your site and sees an item for sale for “1,000.21 €”. A Japanese member sees an item selling for “¥ 847,809.34” A Canadian member sees “$1.00”. If you don’t know why the examples above are confusing, read on. What’s wrong with: A member browsing in German goes to your site and sees an item for sale for “1,000.21 €”? The first example is the easiest. If a member is browsing in German, the commas and decimals in a price should be flipped. So “1,000.21 €” should really be formatted as “1.000,21 €”. This isn’t very confusing (as a German member, you can figure out what the price is *supposed* to be), but it is a bad experience. By the way, if you are in Germany, using Euros, but browsing in English, what would you expect to see? Answer: “€1,000.21”. The separators and symbol position are based on language here, not region. What’s wrong with: A Japanese member sees an item selling for “¥ 847,809.34”? Japanese Yen doesn’t have a fractional part. There’s no such thing as half a Yen. So “¥ 847,809.34” could mean “¥ 847,809”, or “¥ 84,780,934” or something else entirely. What’s wrong with: A Canadian member sees “$1.00”? If your site is US-based, this can be confusing. Does “$” mean Canadian dollar or US dollar here? A simple fix is to add the currency code at the end: “$1.00 USD”. How to format currency correctly Etsy’s locale settings picker Formatting currency for international members is hard. Etsy supports browsing in 9 languages, 23 currencies, and hundreds of regions. Luckily, we don’t have to figure out the right way to format in all of these combinations, because the nice folks at CLDR have done it for us. CLDR is a massive database of formatting styles that gets updated twice a year. The data gets packaged up into a portable library called libicu . Libicu is available everywhere, including mobile phones. If you want to format currency, you can use CLDR data to do it. For each language + region + currency combination, CLDR gives you: The currency symbol The currency code The decimal and grouping separators The format pattern. A typical pattern looks like this: A cldr pattern This is the pattern for German + Germany + Euros. It tells you: The currency symbol goes at the end There’s a space between the value and the currency symbol Euros are grouped in sets of three, like $1,000,000 (vs something like rupees, that are grouped in sets of 2: $1,00,00,000. The pattern for Hindi + India + Rupees is “¤ #,##,##0.###”) this is a fractional currency, formatted up to a precision of 2 (vs Japanese Yen, which is not a fractional currency, and uses the format “¤ #,##0”). NOTE: the pattern does *not* tell you what the decimal and grouping separators are. CLDR gives you those separately, they are not a part of the pattern. Now you can use this information to format a value: If you want to format prices using CLDR, your language might have libraries to do it for you already. PHP has NumberFormatter , for example. JavaScript has Intl.NumberFormat . Practical implementation decisions CLDR is great, but it is not the ultimate authority. It is a collaborative project, which means that anyone can add currency data to CLDR, and then everyone votes on whether the data looks correct or not. People can also vote to change existing currency data. CLDR data is not a precise thing, it is fluid and changing. Sometimes you need to customize CLDR for your use case. Here are the customizations we made. The problem with currencies that use a dollar sign ($) We use CLDR to format currency at Etsy, but we’ve made some changes to it. One issue in particular has really bugged us. Dollar currencies are really hard to work with. The symbol for CAD (Canadian dollars) is “$” in Canada, but it is “CA$” in the US and everywhere else to avoid confusion with US Dollars. So if we followed CLDR, Canadian members would see “$1.00”. But our Canadian members might know that Etsy is a US-based company, in which case “$” would be ambiguous to them — it could mean either Canadian dollars or US dollars . Here is how we choose a currency symbol to avoid confusion while still meeting member expectations: What symbol does Etsy use for dollar-based currencies? Here is the value “1000.21” formatted in different currency + region combinations: You might be wondering, why not just add the currency code to the end of the price? For example, it could be “$1,000.21 USD” for US dollars, and “$1,000.21 CAD” for Canadian dollars. This is also explicit but we don’t need to have complicated logic to change the currency symbol. But this approach has another issue: redundancy. Suppose we did add the currency code at the end everywhere to address the CAD problem. Euros would get formatted as “1.000,21 € EUR”, but the “€ EUR” is redundant. Even worse, Swiss Francs doesn’t have a currency symbol, so CLDR recommends using the currency code as the currency symbol. Which means they would see “1.000,21 CHF CHF” , which is definitely redundant: Adding the currency code at the end is explicit, but doesn’t meet member expectations. Our German members said they didn’t like how “1.000,21 € EUR” looked. In the end Etsy decided not to show the currency code. Instead, we change the currency symbol as needed to avoid confusion. Listing price with settings English / Canada / Canadian dollars Overriding CLDR data Here’s a simple case where we overrode CLDR formatting. We are a website, so of course we want our prices to be wrapped in html tags so that they can be styled appropriately. For example, on our listings manager, we want to format price input boxes correctly based on locale: vs It’s hard to wrap a price in html tags *after* you have done the formatting: sometimes the symbol is at the end, sometimes there’s a space between the symbol and value, and sometimes there isn’t, etc etc. To make this work, the html tags need to be a part of the pattern, so we need to be able to override the CLDR patterns directly. Ultimately we ended up overriding a lot of the default CLDR data: symbols patterns pattern for negative formatting adding html tags Different libraries offered different levels of support for this. PHP’s NumberFormatter lets you override the pattern and symbol. JavaScript’s Intl.NumberFormat lets you override neither. None of the libraries had support for wrapping html tags around the output. In the end, we wrote our own JavaScript library and added wrappers for the rest. Consistent formatting across platforms We had to format currency in PHP, JavaScript, and in our iOS and Android apps. PHP, JavaScript, iOS and Android all had different versions of libicu, and so they had different CLDR data. How do we format consistently across these platforms? We went with a dual plan of attack: write tests that are the same across platforms, and make sure all CLDR overrides get shared between platforms. We wrote a script that would export all our CLDR overrides as JSON / XML / plist. Every time the overrides change, we run the script to generate new data for all platforms. Here’s what our JSON file looks like right now (excerpt): {\r\n    \"de_AU\": {\r\n        \"symbol\": {\r\n            \"AUD\": \"AU$\",\r\n            \"BRL\": \"R$\",\r\n            \"CAD\": \"CA$\"\r\n        },\r\n        \"decimal_separator\": \",\",\r\n        \"grouping_separator\": \".\",\r\n        \"pattern\": {\r\n            \"AUD\": \"#,##0.00 \\u00a4\",\r\n            \"BRL\": \"#,##0.00 \\u00a4\",\r\n            \"CAD\": \"#,##0.00 \\u00a4\"\r\n... We wrote another script to generate test fixtures, which look like this (excerpt): \"test_symbol&&!code&&!html\": {\r\n    \"de\": {\r\n        \"DE\": {\r\n            \"EUR\": {\r\n                \"100000\": \"1.000,00 \\u20ac\",\r\n                \"100021\": \"1.000,21 \\u20ac\"\r\n            }\r\n        },\r\n        \"US\": {\r\n            \"EUR\": {\r\n                \"100000\": \"1.000,00 \\u20ac\",\r\n                \"100021\": \"1.000,21 \\u20ac\"\r\n            },\r\n            \"USD\": {\r\n                \"100000\": \"1.000,00 $\",\r\n                \"100021\": \"1.000,21 $\"\r\n            }\r\n        }\r\n    }\r\n} This test says that given these settings: Show the currency symbol Hide the currency code Format as plain text, not html For the settings de/US/USD The value 100021 should be formatted as “1.000,21 $” We have hundreds of tests in total to check every combination of language/region/currency code with symbol shown vs. hidden, formatted as text vs. html, etc. These expected values get checked against the output of the currency formatters on all platforms, so we know that they all format currency correctly and consistently. Any time an override changes (for example, changing the symbol for CAD to be “CA$” in all regions), we update the CLDR data file so that the new override gets spread to all platforms. Then we update the test fixtures and re-run the tests to make sure the override worked on all platforms. Conclusion No more “¥ 847,809.34”! Formatting currency is hard. If you want to do it correctly, use the CLDR data, but make sure that you override it when necessary based on your unique circumstances. I hope our changes lead to a better experience for international members. Thanks for reading! Posted by Aditya Bhargava on April 19, 2016 Category: engineering , internationalization Tags: currency formatting , localization", "date": "2016-04-19,"},
{"website": "Etsy", "title": "Putting the Dev in Devops: Bringing Software Engineering to Operations Infrastructure Tooling", "author": ["Ryn Daniels"], "link": "https://codeascraft.com/2016/02/22/putting-the-dev-in-devops-bringing-software-engineering-to-operations-infrastructure-tooling/", "abstract": "Putting the Dev in Devops: Bringing Software Engineering to Operations Infrastructure Tooling Posted by Ryn Daniels on February 22, 2016 At Etsy, the vast majority of our computing happens on physical servers that live in our own data centers. Since we don’t do much in the cloud, we’ve developed tools to automate away some of the most tedious aspects of managing physical infrastructure. This tooling helps us take new hardware from initial power on to being production-ready in a manner of minutes, saving time and energy for both data center technicians racking hardware and engineers who need to bring up new servers. It was only recently, however, that this toolset started getting the love and attention that really exemplifies the idea of code as craft. The Indigo Tool Suite The original idea for this set of tools came from a presentation on Scalable System Operations that a few members of the ops team saw at Velocity in 2012. Inspired by the Collins system that Tumblr had developed but disappointed that it wasn’t yet (at the time) open source or able to work out of the box with our particular stack of infrastructure tools, the Etsy ops team started writing our own. In homage to Tumblr’s Phil Collins tribute, we named the first ruby script of our own operations toolset after his bandmate Peter Gabriel. As that one script grew into many, that naming scheme continued, with the full suite and all its components eventually being named after Gabriel and his songs. While many of the technical details of the architecture and design of the tool suite as it exists today are beyond the scope of this post, here is a brief overview of the different components that currently exist. These tools can be broken up into two general categories based on who uses them. The first are components used by our data center team, who handle things like unboxing and racking new servers as well as hardware maintenance and upgrades: sledgehammer : a command-line tool for getting a machine set up in RackTables (our datacenter asset management system) and formatting/partitioning disks as needed, such as creating RAID arrays, and configuring the out-of-band management interface. It consists of a default Cobbler profile that machines will boot to if they don’t already have an operating system, so the data center team can power on boxes and have them start installing automatically – we call this sledgehammer’s unattended mode . This default Cobbler profile loads a bootable disk image that runs what we call the sledgehammer executor , which performs some setup steps such as loading various configuration files and then installs and runs another software package we create that takes over for the rest of the setup steps. This package is called the sledgehammer payload , which consists of some shared Indigo libraries and executable code that actually handles setting up the out-of-band management interface, configuring networking, and saving hardware details back to RackTables. This is built as a separate software package to avoid the friction of rebuilding the entire disk image as often. The other set of tools are primarily used by engineers working in the office, enabling them to take boxes that have already been set up by the data center team and sledgehammer and get them ready to be used for specific tasks: gabriel : a command-line tool for installing an operating system on a machine and getting it installed and configured with Chef zaar : a command-line tool for decommissioning boxes and removing them from production indigo-sweeper : a daemon that makes sure out-of-band management commands sent to servers and Cobbler syncs aren’t duplicated between multiple users and multiple runs of the tools indigo-tailer : a daemon that allows for live-tailing of remote server build logs on the command line and on the web indigo-web : a web frontend and API. The web frontend provides a friendlier interface for non-ops engineers who might not know the ins and outs of the command line tool, by providing easier-to-understand form fields rather than relying on a series of command line arguments. The API provides various functionality including an endpoint with a mutex lock to prevent multiple simultaneous builds getting the same IP address assigned to them – this allows multiple boxes to be powered on and provisioned at once without worrying about race conditions. While many of the details of the inner workings of this automation tooling could be a blog post in and of themselves, the key aspect of the system for this post is how interconnected it is. Sledgehammer’s unattended mode, which has saved our data center team hundreds—if not thousands—of hours of adding server information to RackTables by hand, depends on the sledgehammer payload, sledgehammer executor, API, and the shared libraries that all these tools use all working together perfectly. If any one part of that combination isn’t working with the others, the whole thing breaks, which gets in the way of people, especially our awesome data center team, getting their work done. The Problem Over the years, many many features have been added to Indigo, and as members of the operations team worked to add those features, they tried to avoid breaking things in the process. But testing had never been high on Indigo’s list of priorities – when people started working on it, they thought of it more as a collection of ops scripts that “just work” rather than a software engineering project. Time constraints sometimes played a role as well – for example, sledgehammer’s unattended mode in all its complex glory was rolled out in one afternoon, because a large portion of our recent data center move was scheduled for the next day and it was more important at that point to get that feature out for the DC team to use than it was to write tests. For years, the only way of testing Indigo’s functionality was to push changes to production and see what broke—certainly not an ideal process! A lack of visibility into what was being changed compounded the frustration with this process. When I started working on Indigo, I was one of the first people to have touched that code that has a formal computer science background, so one of the first things I thought of was adding unit tests, like we have for so much else of the code we write at Etsy. I soon discovered that, because the majority of the Indigo code had been written without testability in mind, I was going to have to do some significant refactoring to even get to the point where I could start writing unit tests, which meant we had to first lay some groundwork in order to be able to refactor without being too disruptive to other users of these tools. Refactoring first without any way to test the impact of my changes on the data center team was just asking for everyone involved to have a bad time. Adding Tests (and Testability) Some of the most impactful changes we’ve made recently have been around finding ways to test the previously untestable unattended sledgehammer components. Our biggest wins in this area have been: Adding a test mode for unattended sledgehammer . The command-line sledgehammer tool has always created a configuration file specific to the server being built with that run of the command, and if the config file wasn’t present, the sledgehammer payload would run in unattended mode. By adding a flag to the command-line version, I was able to force the code to run in this unattended mode easily. This also allowed me to set up other options for testing with our existing command line tools, such as… Adding versioning to the sledgehammer payload . Previously, there was only ever one version of the payload rpm—the build script was hard-coded to just always use 0.1. I added an option to build a different version number so that people could test that it worked before creating a new production version. Running command-line sledgehammer in unattended mode could then be told to use this new version instead. Adding a test version of the sledgehammer disk image/Cobbler profile . By creating a new cobbler profile and making a few minor changes to the script that we use to build the sledgehammer executor, disk image, and payload, we added the ability to test changes to this part of the process as well. Though the executor code only changed very infrequently, if it broke it would significantly get in the way of the data center team getting their work done, so being able to test it makes life much better for them. Adding an option to run against a different API URL . Using the server-specific configuration files mentioned previously, I was able to allow people to specify the URL for the Indigo API they wanted to use. This allowed people to run a local version of the API to test changes to it and then test all the build tools against that—a much better solution than just deploying to production and hoping things worked! For example: testhost.yml\n\npayload: \"sledgehammer-payload-0.5-test-1.x86_64.rpm\"\nunattended: \"true\"\nunattended_run_recipient: \"testuser@etsy.com\"\nindigo_url: \"http://testindigo.etsy.com:12345/api/v1\" With changes like these in place, we are able to have much more confidence that our changes won’t break the unattended sledgehammer tool that is so critical for our data center team. This enables us to more effectively refactor the Indigo codebase, whether that be to improve it in general or to make it more testable. I gave a presentation at OpsSchool, our internal series of lectures designed to educate people on a variety of operations-related topics inspired by opsschool.org , on how to change the Indigo code to make it better suited to unit testing. Unit testing itself is beyond the scope of this post, but for us, this has meant things like changing method signatures so that objects that might be mocked or stubbed out can be passed in during tests, or splitting up large gnarly methods that grew organically along with the Indigo codebase over the past few years into smaller, more testable pieces. This way, other people on the team are able to help write unit tests for all of Indigo’s shared library code as well. Deploying, Monitoring, and Planning As mentioned previously, one of the biggest headaches with this tooling had been keeping all the different moving pieces in sync when people were making changes. To fix this, we decided to leverage the work that had already been put into Deployinator by our dev tools team. We created an Indigo deployinator stack that, among other things, ensures that the shared libraries, API, command line tools, and sledgehammer payload are all deployed at the same time. It keeps these deploys in sync, handles the building of the payload RPM, and restarts all the Indigo services to make sure that we never again run into issues where the payload stops working because it didn’t get updated when one of its shared library files did or vice versa. Additionally, it automatically emails release notes to everyone who uses the Indigo toolset, including our data center team. These release notes, generated from the git commit logs for all the commits being pushed out with a given deploy, provide some much-needed visibility into how the tools are changing. Of course, this meant making sure everyone was on the same page with writing commit messages that will be useful in this context! This way the data center folks, geographically removed from the ops team making these changes, have a heads up when things might be changing with the tools they use. Finally, we’re changing how we approach the continued development and maintenance of this software going forward. Indigo started out as a single ruby script and evolved into a complex interconnected set of tools, but for a while the in-depth knowledge of all the tools and their interconnections existed solely in the heads of a couple people. Going forward, we’re documenting not only how to use the tools but how to develop and test them, and encouraging more members of the team to get involved with this work to avoid having any individuals be single points of knowledge. We’re keeping testability in mind as we write more code, so that we don’t end up with any more code that has to be refactored before it can even be tested. And we’re developing with an eye for the future, planning what features will be added and which bugs are highest priority to fix, and always keeping in mind how the work we do will impact the people who use these tools the most. Conclusion Operations engineers don’t think of ourselves as developers, but there’s a lot we can learn from our friends in the development world. Instead of always writing code willy-nilly as needed, we should be planning how to best develop the tooling we use, making sure to be considerate of future-us who will have to maintain and debug this code months or even years down the line. Tools to provision hardware in a data center need tests and documentation just as much as consumer-facing product code. I’m excited to show that operations engineers can embrace the craftsmanship of software engineering to make our tools more robust and scalable. Posted by Ryn Daniels on February 22, 2016 Category: engineering , infrastructure , operations", "date": "2016-02-22,"},
{"website": "Etsy", "title": "Q4 2015 Site Performance Report", "author": ["Kristyn Reith"], "link": "https://codeascraft.com/2016/02/12/q4-2015-site-performance-report/", "abstract": "Q4 2015 Site Performance Report Posted by Kristyn Reith on February 12, 2016 Happy New Year! It may be 2016, but we’re here to give you a full review of the site performance highlights for the fourth quarter of 2015. For this report, we’ve collected data from a full week in December that we will be comparing to the full week of data from September we gathered for our last report. Unlike the Q3 report , we did not uncover any prevailing trends that spanned every section of this report. In the fourth quarter, we saw a few improvements and a handful of regressions, but the majority of pages remained stable. As in the past, this report is a collaborative effort. Mike Adler will discuss our server side performance, Kristyn Reith will report on the synthetic front-end data, and the real user monitoring update will come from Allison McKnight and Moishe Lettvin. So without further delay, let’s take a look at the numbers. Server-Side Performance We begin our report with the server-side latencies, which is how long it takes for our servers to build pages. This metric does not include any browser-side time. We calculate it by taking a random sample of our web server logs. One reason we start here is that changes in server-side metrics can explain some changes in synthetic and RUM metrics. As you can see below, most pages are executing on the server at about the same speed, though the cart page did get about 27% faster on average. The fourth quarter of the year obviously includes holiday shopping, which is the busiest time of year for our site. Each year our ops team plans to add capacity in anticipation of the rush, but still, it’s not unusual to discover new performance bottlenecks as traffic ramps up. Each year we have new code, new hardware and new usage patterns. To quote a translation of Heraclitus, “Ever-newer waters flow on those who step into the same rivers.” In this quarter we discovered that we could improve our efficiency by running numad on our web servers. Our web servers do not use any virtualization, so one kernel is scheduling across 24 physical cores (hyper-threading disabled, for now). We noticed that some cores were significantly busier than others and that was effectively limiting throughput and increasing latency. An Etsy engineer learned that by simply running numad, the cpu workload was more balanced, leading to better efficiency. In short, our server-side metrics no longer slowed down during busy times of the day. Today’s servers are built with NUMA (Non-uniform memory access) architectures, which creates an incentive to schedule a tasks on CPUs that are “close” to the memory they need. Depending on many factors (hardware, workload, other settings), this scheduling challenge can result in suboptimal efficiency. We found that numad, a userland daemon that assigns processes to numa zones, is a simple and effective way to optimize for our current conditions. We saw that our search page performance got a little slower on average, but we expected this due to launching some more computationally expensive products (such as our Improved Category Pages). Synthetic Start Render For our synthetic testing, we’ve set up tests using a third party service which simulates actions taken by a user and then automatically reloads the test pages every ten minutes to generate the performance metrics.  As mentioned in the last report, due to recent product improvements, we have decided to retire “Webpage Response” as a metric used for this report, so we will be focusing on the “Start Render” metric in the boxplots below. Overall, we did not see any major changes in start render times this quarter that would have impacted user experience. You may notice that the metrics for Q3 differ from the last report and that the start render times have significantly wider ranges. In the last two reports the plotted data points have represented median measurements, which limited our analysis to the median metrics. In this report, the boxplots are constructed using raw data, thereby providing a more accurate and holistic representation of each page’s performance. Since the raw data captures the full distribution  of start render times, we are seeing more outliers than we have in the past. As you can see above, the start render time has remained fairly stable this quarter. Though nearly all the pages experienced a very minor increase in median start render time, the homepage saw the greatest impact. This slowdown can be attributed to the introduction of a new font. Though this change added additional font bytes to all the pages, we discovered that the scope of the regression significantly varied depending on the browser. The data displayed in the boxplots is from tests run in IE9. The graphs below show the days surrounding the font change. While the font change resulted in a noticeable jump in start render time in IE, start render performance in Chrome remained relatively unaffected . The difference in font file formats displayed by the browsers is partially responsible for the disparity in performance. The font format selected by Chrome (woff2) uses a compression algorithm that reduces the font file by roughly 30%, resulting in a substantially smaller file size when compared to other formats. Additionally, the IE browser running the synthetic test has the compatibility view enabled, meaning that although it’s effectively using IE9, the browser is rendering the pages with an even older version of IE. Therefore, the browser is downloading all the font files included in the pages corresponding CSS file regardless of whether or not they are used on the page. Since Chrome is more commonly used among Etsy users than IE9, we have set up new synthetic tests in Chrome. For all future reports we will use the data generated by the new Chrome tests to populate the synthetic boxplots. We feel that this change, coupled with the continued use of raw data will provide a more realistic representation of what our users experience. Real User Page Load Time “Real User” data can show us variations in the actual speed that our users experience, depending on their geographic location, what browser they’re using, the internet provider they’re using, and so on. Sometimes the richness of this data lets us see trends that we couldn’t see in our other two monitoring methods; other times, the variety of data makes trends harder to see. This quarter, we mostly saw small or no change in our metrics. The only delta that’s noticeable is the fifth percentile in the Shop Home page, which increased from 2.3 seconds to 2.6 seconds. A time series graph of the fifth percentile shows an uptick corresponding to our new page font. This parallels what we found in the synthetic tests, as discussed above. Because shop_home is one of our most-consistently fast pages, it tends to be more sensitive to changes in overall site load time. That is, it shows deltas that might get “lost in the noise” on pages with higher variance. With this context, it can be interesting to look at the data day-by-day in addition to the week vs. week comparison that the box plot above shows us too. You can see below that even with the fairly large difference in the fifth percentile seen in the box plot, on the last day of the comparison weeks the slower and faster lines actually trade positions. Conclusion Despite the fourth quarter being the busiest time of the year, users did not experience degraded performance and most of the fluctuations in page speed were negligible. For this report we improved the data quality in the synthetic section by switching from median measurements to raw data. In the coming 2016 reports, we will strive to make more improvements to our methodology. If you have any thoughts or questions, please leave a comment! Posted by Kristyn Reith on February 12, 2016 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report Posted by Kristyn Reith on 13 Jul, 2015 Q2 2015 Site Performance Report", "date": "2016-02-12,"},
{"website": "Etsy", "title": "Quality Matters: The Benefits of QA-Focused Retros", "author": ["Arylee McSweaney"], "link": "https://codeascraft.com/2016/02/08/quality-matters-the-benefits-of-qa-focused-retros/", "abstract": "Quality Matters: The Benefits of QA-Focused Retros Posted by Arylee McSweaney on February 8, 2016 Retrospectives (retros) are meetings that take place after every major project milestone to help product teams at Etsy improve processes and outcomes in the next iteration of a project. The insights gained from project retros are invaluable to proactively mitigating problems in future projects while promoting continuous learning. I am one of the managers on the Product Quality (PQ) team, which is Etsy’s centralized resource for manual QA. When manual QA was first introduced at Etsy, testers joined a team for the duration of the project but had limited opportunity  to get objective feedback. Consequently, testers were kept from seeing the importance of their contributions and from viewing themselves as true members of the product team they worked with. This lack of communication and continuous learning left our testers with less job satisfaction and feelings of empowerment than their peers in product engineering. We decided to try QA-focused retros to surface feedback that would help us identify repeatable behaviors that contribute to consistently successful product launches. We were also interested in empowering Product Quality team members to understand how their specific contributions impact product launches and allow them to take more ownership of their responsibilities. Regularly scheduled QA retros have helped to promote mindfulness and accountability on the PQ team. Over time, they have solidified relationships with product owners, designers, and engineers and reinforced the sense that we are all working toward the same goal. Here’s some information to help you run a QA-focused retro: Meeting Goals Identify tacit, repeatable behaviors that contributed to increased confidence before launch Promote accountability within the team Foster self efficacy among team members Identify fruitless behaviors and actions that can be removed from the QA process Sample Agenda What went well? What could have gone better? Takeaways? What patterns do we want to avoid in the future? What patterns do we want to repeat in other projects? How Does It Work? The retro should be scheduled after the initial product launch or any milestone that required a significant QA effort. Participants should receive the agenda ahead of time and be asked to come prepared with their thoughts on the main questions.  The QA engineer who led the testing effort should facilitate the retro by prompting attendees, in a round robin fashion, to weigh in on each agenda item. The facilitator also participates by giving insights to the product team they partnered with for launch. This 30-minute activity is best if scheduled near the end of the day and can be augmented with snacks and beverages to create a relaxed atmosphere. The facilitator should record insights and report any interesting findings back to the QA team and the product team they worked with on the project. Who Should Attend? Participants should be limited to those who directly interacted with QA during development. These are usually: Product Managers Software Engineers QA Testers Product Designers What Happens Next? Everyone on the Product Quality team reviews the takeaways from the retro and clarifies any questions with those who participated in the retro. We then make the applicable adjustments to processes prior to the next iteration of a project. All changes made to the PQ process are communicated to team members and product teams as needed. Conclusions QA-focused retros have empowered our team to approach testing as a craft that is constantly honed and developed. The meetings help product managers learn how to get the most out of working with the QA team and provide opportunities for product teams to participate in streamlining the QA process. Posted by Arylee McSweaney on February 8, 2016 Category: operations , people , Uncategorized", "date": "2016-02-8,"},
{"website": "Etsy", "title": "Leveling Up Your Organization With System Reviews", "author": ["John Goulah"], "link": "https://codeascraft.com/2015/12/21/leveling-up-with-system-reviews/", "abstract": "Leveling Up Your Organization With System Reviews Posted by John Goulah on December 21, 2015 One of Etsy’s core engineering philosophies has been to push decisions to the edges wherever possible. Rather than making dictatorial style decisions we enable people to make their own choices given the feedback of a group. We use the system review format to tackle organizational challenges that surface from a quickly growing company. A system review is a model that enables a large organization to take action on organizational and cultural issues before they become larger problems. They’re modeled after shared governance meetings – a disciplined yet inclusive format that enables organizational progress and high levels of transparency. This form of leadership values continued hard work, open communication, trust and respect. This idea was introduced by our Learning and Development team, who among other things run our manager training programs, our feedback system, and our dens program (a vehicle for confidential, small group discussions about leadership and challenges at work). A few years ago, we started bringing the engineering leadership group together on a recurring basis, but the agenda and outcome of these meetings were unclear. We always had something to talk about and discuss, but it was difficult to move forward and address issues. We were looking for something that was better facilitated, and for ways to bring our engineering leadership team together to provide the clear outcome of helping solve some of our organizational challenges. L&D provided us with facilitation training and an overview of different meeting formats to use in different situations. Over time we’ve tested out some of these new meeting types, and the system review is one of the many formats we’ve learned to apply. They coached us through facilitating the first series of these new formats in our meetings over the the first several months.  We’re extremely fortunate to have a team of smart, focused individuals that have the background in providing solutions for these types of problems. We are sharing these insights here for the benefit of anyone interested in the topic. These meetings can work well for anything from small groups of 20 up to large groups of 300 people. They may take a few times to get the hang of, but once you get into a rhythm it becomes an efficient format to survey a large group and take action on important issues. They should be held at a regular cadence, such as monthly or quarterly such that it creates a feedback loop of proactively raising new problems to address, while reporting findings and potential solutions to previously discussed topics. When we are reviewing system issues we are looking into the following things: Where is there lack of clarity? Where are people feeling frustrated? Where are there glitches? System review meetings are based around a specific prompt taken from these areas, for example “In what area of the engineering org do you feel there is currently a high degree of confusion or frustration?”. What does the agenda look like? This type of meeting needs to be timed and facilitated. The agenda is pretty straightforward, it’s important that the group observe the timer that the facilitator maintains and respect that they are moving the meeting along within the confines of a one-hour timeframe. Facilitation is really an art in itself, and there are a lot of resources out there to help with improving the technique. Generally the facilitator should not be emotionally invested in the topic so they can remove themselves from the conversation and focus on moving the meeting forward. They should set and communicate the agenda, and make sure the room is set up to support the format and that the appropriate materials are provided. They should set the stage for the meeting – let people know why they are there, an overview of what they’ll be doing, and why it’s important. They should manage the agenda and timing. The timing is somewhat flexible within the greater time frame and should be adjusted as necessary based on the discussions taking place. It’s possible that a conversation is deeper than a time slot allows but the facilitator decides on the fly that it is important enough to cut time from another part of the meeting. However, every time the timer is ignored, the group slides away from healthy order and towards bad meeting anarchy, so it’s the facilitator’s job to keep this in check. To do this effectively, the facilitator needs to be comfortable interrupting the group to keep the meeting on track. And lastly the facilitator should close the meeting on time, summarize any key actions, agreements and next steps. Below is the agenda for the high level format of the meeting. After presenting the prompt chosen for the meeting, the facilitator should divide the attendees into groups of approximately five people. Each member of the group individually generates ideas about it (sticky notes work great to collect these, one idea per sticky). Within these small groups, everyone shares their top two issues and the impact they feel each has. After everyone has shared theirs, the group should vote and tally the top three issues. The facilitator will then have everyone come back together as the larger group. Each of the subgroups will share the three things that they upvoted. After each round the larger group can ask clarifying questions. It’s a good idea if the facilitator maintains a spreadsheet with all of the ideas so that everyone can refer back to it. It comes in handy because the next phase is for everyone to vote on the issues. Take the top 3 – 5 issues as something to move forward on investigating. Sample Agenda Prompt: In what area of the engineering org do you feel there is currently a high degree of confusion or frustration? Small Groups (20 mins timed by facilitator) Solo brainstorm (2 mins full group) Round-robin: share top 2 issues + their impact (2 mins per person) Group vote: vote and tally top three (5 mins) Full Group (30 mins) Each group shares their three upvoted (2 mins per group) Clarifying Questions asked (3 mins per round) Full vote: Write 3 votes on post-its (3 mins) Drivers volunteer Next Steps After we have settled on the top issues, we need people in the group that are interested in working on investigating and bringing information back to the group at a later date. Hopefully at least one person is passionate enough about each topic to look into it further, or else it would not have been voted a top issue. Create a spreadsheet to maintain each topic, driver and the due date they propose to bring information back to the group. Each of the drivers should report back on these questions to help the organization begin to understand the issue, report back on the answers they’ve acquired and decide on the next steps. This follow up can happen at the beginning of future meetings. Is anyone already on this? If so, where are they at with it — what roadblocks and small wins have they experienced? How wide and deep is the issue? In what ways are people and systems impacted? In what ways are people and systems impacted – good and bad? Where does the issue seem to originate or hit the hardest? If there are communication gaps, what factors seem to be leading to them? What helps or makes the communication gap worse? What’s next? Are there clear next steps coming up? Or are there things already in the works that could be duplicated or expanded? What options are out there? (You don’t have to propose one, just report back what seems to be the options.) Conclusion System reviews are just one format that we can use to build communication, respect and trust across our team and organization. The purpose can be to surface possible glitches in the system, but also to achieve alignment on what the most important problems the group should spend their energy solving and to reach clarity around them. They can also be used to feed a topic into another format called the Decisions Roundtable, which is a similar type of meeting used to drive forward a proposal to make a change. Similar to post mortems and retrospectives, system reviews can be used to level up the organization and foster a learning culture. Some topics that we’ve explored in the past have been around how we think about hiring managers, why we deviated to using different tools to plan work, how we document things and where that information should live, clarity around career paths, and how we can address diversity and inclusivity in tech management. System reviews are used to help explore topics that may be difficult for some of the people in the group, but as long as the process is handled sensitively, we all come out with better understanding, more empathy for the experiences of others and a stronger organization as a whole. Posted by John Goulah on December 21, 2015 Category: people , philosophy Tags: learning culture , problem solving , system reviews", "date": "2015-12-21,"},
{"website": "Etsy", "title": "Introducing Arbiter: A Utility for Generating Oozie Workflows", "author": ["Andrew Johnson"], "link": "https://codeascraft.com/2015/12/16/introducing-arbiter-a-utility-for-generating-oozie-workflows/", "abstract": "Introducing Arbiter: A Utility for Generating Oozie Workflows Posted by Andrew Johnson on December 16, 2015 At Etsy we have been using Apache Oozie for managing our production workflows on Hadoop for several years. We’ve even recently started using Oozie for managing our ad hoc Hadoop jobs as well. Oozie has worked very well for us and we currently have several dozen distinct workflows running in production. However, writing these workflows by hand has been a pain point for us. To address this, we have created Arbiter , a utility for generating Oozie workflows. The Problem Oozie workflows are specified in XML. The Oozie documentation has an extensive overview of writing workflows, but there are a few things that are helpful to know. A workflow begins with the start node: <start to=\"fork-2\"/> Each job or other task in the workflow is an action node within a workflow. There are some built-in actions for running MapReduce jobs, standard Java main classes, etc. and you can also define custom action types. This is an example action: <action name=\"transactional_lifecycle_email_stats\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>transactional_lifecycle_email_stats.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-2\"/>\r\n    <error to=\"join-2\"/>\r\n</action> Each action defines a transition to take upon success and a (possibly) different transition to take upon failure: <ok to=\"join-2\"/>\r\n<error to=\"join-2\"/> To have actions run in parallel, a fork node can be used. All the actions specified in the fork will be run in parallel: <fork name=\"fork-2\">\r\n    <path start=\"fork-0\"/>\r\n    <path start=\"transactional_lifecycle_email_stats\"/>\r\n</fork> After these actions there must be a join node to wait for all the forked actions to finish: <join name=\"join-2\" to=\"screamapillar\"/> Finally, a workflow ends by transitioning to either the end or kill nodes, for a successful or unsuccessful result, respectively: <kill name=\"kill\">\r\n    <message>Workflow email-rollups has failed with msg: [${wf:errorMessage(wf:lastErrorNode())}]</message>\r\n</kill>\r\n<end name=\"end\"/> Here is a complete example of one of our shorter workflows: <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n<workflow-app xmlns=\"uri:oozie:workflow:0.2\" name=\"email-rollups\">\r\n  <start to=\"fork-2\"/>\r\n  <fork name=\"fork-2\">\r\n    <path start=\"fork-0\"/>\r\n    <path start=\"transactional_lifecycle_email_stats\"/>\r\n  </fork>\r\n  <action name=\"transactional_lifecycle_email_stats\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>transactional_lifecycle_email_stats.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-2\"/>\r\n    <error to=\"join-2\"/>\r\n  </action>\r\n  <join name=\"join-2\" to=\"screamapillar\"/>\r\n  <action name=\"screamapillar\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>${queueName}</value>\r\n        </property>\r\n        <property>\r\n          <name>mapreduce.map.output.compress</name>\r\n          <value>true</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.oozie.Screamapillar</main-class>\r\n      <arg>--workflow-id</arg>\r\n      <arg>${wf:id()}</arg>\r\n      <arg>--recipient</arg>\r\n      <arg>fake_email</arg>\r\n      <arg>--sender</arg>\r\n      <arg>fake_email</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"end\"/>\r\n    <error to=\"kill\"/>\r\n  </action>\r\n  <fork name=\"fork-0\">\r\n    <path start=\"email_campaign_stats\"/>\r\n    <path start=\"user_language\"/>\r\n  </fork>\r\n  <action name=\"user_language\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>user_language.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-0\"/>\r\n    <error to=\"join-0\"/>\r\n  </action>\r\n  <join name=\"join-0\" to=\"fork-1\"/>\r\n  <fork name=\"fork-1\">\r\n    <path start=\"email_overview\"/>\r\n    <path start=\"trans_email_overview\"/>\r\n  </fork>\r\n  <action name=\"trans_email_overview\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>trans_email_overview.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-1\"/>\r\n    <error to=\"join-1\"/>\r\n  </action>\r\n  <join name=\"join-1\" to=\"join-2\"/>\r\n  <action name=\"email_overview\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>zz_email_overview.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-1\"/>\r\n    <error to=\"join-1\"/>\r\n  </action>\r\n  <action name=\"email_campaign_stats\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>zz_email_campaign_stats.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-0\"/>\r\n    <error to=\"join-0\"/>\r\n  </action>\r\n  <kill name=\"kill\">\r\n    <message>Workflow email-rollups has failed with msg: [${wf:errorMessage(wf:lastErrorNode())}]</message>\r\n  </kill>\r\n  <end name=\"end\"/>\r\n</workflow-app> Having the workflows be defined in XML has been very helpful. We have several validation and visualization tools in multiple languages that can parse the XML and produce useful results without being tightly coupled to Oozie itself. However, the XML is not as useful for the people that work with it. First, it is very verbose. Each new action adds about 20 lines of XML to the workflow, much of which is boilerplate. As a result, our workflows average around 200 lines and the largest is almost 1800 lines long. This also makes it hard for someone to read the workflow and understand what the workflow does and the flow of execution. Next, defining the flow of execution can be tricky. It is natural to think about the dependencies between actions. Oozie workflows, however, are not specified in terms of these dependencies. The workflow author must satisfy these dependencies by configuring the workflow to run the actions in the proper order. For simple workflows this may not be a problem, but can quickly become complex. Moreover, the author must manually manage parallelism by inserting forks and joins. This makes modifying the workflow more complex. We have found that it’s easy to miss adding an action to a fork, resulting in an orphaned action that doesn’t get run. Another common problem we’ve had with forks is that a single-action fork is considered invalid by Oozie, which means removing the second-last action from a fork requires removing the fork and join entirely. Introducing Arbiter Arbiter was created to solve these problems. XML is very amenable to being produced automatically, so there is the opportunity to write the workflows in another format and produce the final workflow definition. We considered several options, but ultimately settled on YAML. There are robust YAML parsers in many languages and we considered it easier for people to read than JSON. We also considered a Scala-based DSL, but we wanted to stick with a markup language for language-agnostic parsing. Writing Workflows Here is the same example workflow from above written in Arbiter’s YAML format: ---\r\nname: email-rollups\r\nerrorHandler:\r\n  name: screamapillar\r\n  type: screamapillar\r\n  recipients: fake_email\r\n  sender: fake_email\r\nactions:\r\n  - name: email_campaign_stats\r\n    type: rollup\r\n    rollup_file: zz_email_campaign_stats.sql\r\n    category: regular\r\n    dependencies: []\r\n  - name: trans_email_overview\r\n    type: rollup\r\n    rollup_file: trans_email_overview.sql\r\n    category: regular\r\n    dependencies: [email_campaign_stats, user_language]\r\n  - name: email_overview\r\n    type: rollup\r\n    rollup_file: zz_email_overview.sql\r\n    category: regular\r\n    dependencies: [email_campaign_stats, user_language]\r\n  - name: user_language\r\n    type: rollup\r\n    rollup_file: user_language.sql\r\n    category: regular\r\n    dependencies: []\r\n  - name: transactional_lifecycle_email_stats\r\n    type: rollup\r\n    rollup_file: transactional_lifecycle_email_stats.sql\r\n    category: regular\r\n    dependencies: [] The translation of the YAML to XML is highly dependent on the configuration given to Arbiter, which we will cover in the next section. However, there are several points to consider now. First, the YAML definition is only about 20% of the length of the XML. Since the workflow definition is much shorter, it’s easier for someone to read it and understand what the workflow does. In addition, none of the flow control nodes need to be manually specified. Arbiter will insert the start, end, and kill nodes in the correct locations. Forks and joins will also be inserted when actions can be run in parallel. Most importantly, however, the workflow author can directly specify the dependencies between actions, instead of the order of execution. Arbiter will handle ordering the actions in such a way to satisfy all the given dependencies. In addition to the standard workflow actions, Arbiter allows you to define an “error handler” action. It will automatically insert this action before any transitions to the end or kill nodes in the workflow. We use this to send an email alert with details about the success and failure of the workflow actions. If these are omitted the workflow will transition directly to the end or kill nodes as appropriate. Configuration The mapping between a YAML workflow definition and the final XML is controlled by configuration files. These are also specified in YAML. Here is an example configuration file to accompany the example workflow given above: ---\r\nkillName: kill\r\nkillMessage: \"Workflow $$name$$ has failed with msg: [${wf:errorMessage(wf:lastErrorNode())}]\"\r\nactionTypes:\r\n  - tag: java\r\n    name: rollup\r\n    configurationPosition: 2\r\n    properties: {\"mapreduce.job.queuename\": \"rollups\"}\r\n    defaultArgs: {\r\n      job-tracker: [\"${jobTracker}\"],\r\n      name-node: [\"${nameNode}\"],\r\n      main-class: [\"com.etsy.db.VerticaRollupRunner\"],\r\n      arg: [\"--file\", \"$$rollup_file$$\", \"--frequency\", \"daily\", \"--category\", \"$$category$$\", \"--env\", \"${cluster_env}\"]\r\n    }\r\n  - tag: sub-workflow\r\n    name: sub-workflow\r\n    defaultArgs: {\r\n      app-path: [\"$$workflowPath$$\"],\r\n      propagate-configuration: []\r\n    }\r\n  - tag: java\r\n    name: screamapillar\r\n    configurationPosition: 2\r\n    properties: {\"mapreduce.job.queuename\": \"${queueName}\", \"mapreduce.map.output.compress\": \"true\"}\r\n    defaultArgs: {\r\n      job-tracker: [\"${jobTracker}\"],\r\n      name-node: [\"${nameNode}\"],\r\n      main-class: [\"com.etsy.oozie.Screamapillar\"],\r\n      arg: [\"--workflow-id\", \"${wf:id()}\", \"--recipient\", \"$$recipients$$\", \"--sender\", \"$$sender$$\", \"--env\", \"${cluster_env}\"]\r\n    } The key part of the configuration file is the actionTypes setting. Each action type will map to a certain action type in the XML workflow. However, multiple Arbiter action types can map to the same Oozie action type, such as the screamapillar and rollup action types both mapping to the Oozie java action type. This allows you to have meaningful action types in the YAML workflow definitions without the overhead of actually creating custom Oozie action types. Let’s review the parts of an action type definition: - tag: java\r\n    name: rollup\r\n    configurationPosition: 2\r\n    properties: {\"mapreduce.job.queuename\": \"rollups\"}\r\n    defaultArgs: {\r\n      job-tracker: [\"${jobTracker}\"],\r\n      name-node: [\"${nameNode}\"],\r\n      main-class: [\"com.etsy.db.VerticaRollupRunner\"],\r\n      arg: [\"--file\", \"$$rollup_file$$\", \"--frequency\", \"daily\", \"--category\", \"$$category$$\", \"--env\", \"${cluster_env}\"]\r\n    } The tag key defines the action type tag in the workflow XML. This can be one of the built-in action types like java, or a custom Oozie action type. Arbiter does not need to be made aware of custom Oozie action types. The name key defines the name of this action type, which will be used to set the type of actions in the workflow definition. If the Oozie action type accepts configuration properties from the workflow XML, these are controlled by the configurationPosition and properties keys. properties defines the actual configuration properties that will be applied to every action of this type, and configurationPosition defines where in the generated XML for the action the configuration tag should be placed. The defaultArgs key defines the default elements of the generated XML for actions of this type. The keys are the names of the XML tags, and the values are lists of the values for that tag. Even tags that can appear only once must be specified as a list. You can also define properties to be populated from values set in the workflow definition. Any string surrounded by $$ will be interpolated in this way. $$rollup_file$$ and $$category$$ are examples of doing so in this configuration file. These will be populated with the values of the rollup_file and category keys from a rollup action in the workflow definition. Using this configuration file, we could write an action like the following in the YAML workflow definition: - name: email_campaign_stats\r\n    type: rollup\r\n    rollup_file: zz_email_campaign_stats.sql\r\n    category: regular\r\n    dependencies: [] Arbiter would then translate this action to the following XML: <action name=\"email_campaign_stats\">\r\n    <java>\r\n      <job-tracker>${jobTracker}</job-tracker>\r\n      <name-node>${nameNode}</name-node>\r\n      <configuration>\r\n        <property>\r\n          <name>mapreduce.job.queuename</name>\r\n          <value>rollups</value>\r\n        </property>\r\n      </configuration>\r\n      <main-class>com.etsy.db.VerticaRollupRunner</main-class>\r\n      <arg>--file</arg>\r\n      <arg>zz_email_campaign_stats.sql</arg>\r\n      <arg>--frequency</arg>\r\n      <arg>daily</arg>\r\n      <arg>--category</arg>\r\n      <arg>regular</arg>\r\n      <arg>--env</arg>\r\n      <arg>${cluster_env}</arg>\r\n    </java>\r\n    <ok to=\"join-0\"/>\r\n    <error to=\"join-0\"/>\r\n</action> Arbiter also allows you to specify the name of the kill node and the message it logs with the killName and killMessage properties. How Arbiter Generates Workflows Arbiter builds a directed graph of all the actions from the workflow definition it is processing. The vertices of the graph are the actions and the edges are dependencies. The direction of the edge is from the dependency to the dependent action to represent the desired flow of execution. Oozie workflows are required to be acyclic, so if a cycle is detected Arbiter will throw an exception. The directed graph that Arbiter builds will be made up of one or more weakly connected components. This is the graph from the example workflow above, which has two such components: Each of these components is processed independently. First, any vertices with no incoming edges are removed from the graph and inserted into a new result graph. If there is more than one vertex removed Arbiter will also insert a fork/join pair to run them in parallel. Having removed those vertices, the original component will now have been split into one or more new weakly connected components. Each of these components is then recursively processed in this same way. Once every component has been processed, Arbiter then combines these independent components until it has produced a complete graph. Since these components were initially not connected, they can be run in parallel. If there is more than component, Arbiter will insert a fork/join pair. This results in the following graph for the example workflow, showing the ok transitions between nodes: This algorithm biases Arbiter towards satisfying the dependencies between actions over achieving optimal parallelism. In general this algorithm still produces good parallelism, but in certain cases (such as a workflow with one action that depends on every other action), it can degenerate to a fairly linear flow. While it is a conservative choice, this algorithm has still worked out well for most of our workflows and has the advantage of being straightforward to follow in case the generated workflow is incorrect or unusual. Once this process has finished all the flow control nodes will be present in the workflow graph. Arbiter can then translate this into the XML using the provided configuration files. Get Arbiter Arbiter is now available on Github ! We’ve been using Arbiter internally already and it’s been very useful for us. If you’re using Oozie we hope Arbiter will be similarly useful for you and welcome any feedback or contributions you have! Posted by Andrew Johnson on December 16, 2015 Category: data , engineering , infrastructure Tags: hadoop , oozie Related Posts Posted by Andrew Johnson on 24 Sep, 2015 Managing Hadoop Job Submission to Multiple Clusters Posted by Andrew Johnson on 12 May, 2015 Four Months of statsd-jvm-profiler: A Retrospective Posted by Andrew Johnson on 14 Jan, 2015 Introducing statsd-jvm-profiler: A JVM Profiler for Hadoop", "date": "2015-12-16,"},
{"website": "Etsy", "title": "Crunching Apple Pay tokens in PHP", "author": ["Adam Saponara"], "link": "https://codeascraft.com/2015/11/20/crunching-apple-pay-tokens-in-php/", "abstract": "Crunching Apple Pay tokens in PHP Posted by Adam Saponara on November 20, 2015 Etsy is always trying to make it easier for members to buy and sell unique goods. And with 60% of our traffic now coming from mobile devices, making it easier to buy things on phones and tablets is a top priority. So when Apple Pay launched last year, we knew right away we wanted to offer it for our iOS users, and shipped it in April. Today we’re open sourcing part of our server-side solution, applepay-php , a PHP extension that verifies and decrypts Apple Pay payment tokens. Integrating with Apple Pay comes down to two main areas of development: device side and payment-processing side. On the device side, at a high level, your app uses the PassKit framework to obtain an encrypted payment token which represents a user’s credit card info. On the payment-processing side, the goal is to make funds move between bank accounts. The first step here is to decrypt the payment token. Many payment processors offer APIs to decrypt Apple Pay tokens on your behalf, but in our case, we wanted the flexibility of reading the tokens in-house. It turns out that doing this properly is pretty involved (to get an idea of the complexity, our solution defines 63 unique error codes), so we set out to find a pre-existing solution. Our search yielded a couple of open source projects, but none that fully complied with Apple’s spec . Notably, we couldn’t find any examples of verifying the chain of trust between Apple’s root CA and the payment signature, a critical component in guarding against forged payment tokens. We also couldn’t find any examples written in PHP (our primary language) or C (which could serve as the basis for a PHP extension). To meet our needs, we wrote a custom PHP extension on top of OpenSSL that exposes just two functions: applepay_verify_and_decrypt and applepay_last_error . This solution has worked really well for us over the past six months, so we figured we’d share it to make life easier for anyone else in a similar position. Before releasing the code, we asked Syndis , a security consultancy based out of Iceland, to perform an external code review in addition to our everyday in-house code reviews. Syndis surveyed the code for both design flaws and implementation flaws. They found a few minor bugs but no actual vulnerabilities. Knowing that we wouldn’t be exposing users to undue risk gave us greater confidence to publish the code. We’ve committed to using the open source version internally to avoid divergence , so expect to see future development on Github . Future work includes splitting off a generalized libapplepay (making it easier to write wrapper libraries for other languages), PHP7 compatibility, and an HHVM port. (By the way, if any of this sounds fun to you, we’d love for you to come work with us .) We hope this release provides merchants with a solid solution for handling Apple Pay tokens. We also hope it inspires other organizations to consider open sourcing parts of their payment infrastructure. You can follow Adam on Github @adsr . Special thanks to Stephen Buckley, Keyur Govande, and Rasmus Lerdorf. Posted by Adam Saponara on November 20, 2015 Category: engineering , infrastructure , mobile Tags: Apple , Apple Pay , open source , PHP", "date": "2015-11-20,"},
{"website": "Etsy", "title": "Q3 2015 Site Performance Report", "author": ["Mike Adler"], "link": "https://codeascraft.com/2015/11/10/q3-2015-site-performance-report/", "abstract": "Q3 2015 Site Performance Report Posted by Mike Adler on November 10, 2015 Sadly, the summer has come to an end here in Brooklyn, but the changing of the leaves signifies one thing—it’s time to release our Q3 site performance report! For this report, we’ve collected data from a full week in September that we will be comparing to a full week of data from May. Similar to last quarter’s report, we will be using box plots to better visualize the data and the changes we’ve seen. While we love to share stories of our wins, we find it equally important to report on the challenges we face. The prevailing pattern you will notice across all sections of this report is increased latency. Kristyn Reith will provide an update on backend server-side performance and Mike Adler, one of the newest members to the Performance team, will be reporting the synthetic frontend and the real user monitoring sections of this report. Server-Side Performance The server-side data below reflects the time seen by real users, both signed-in and signed-out. As a reminder, we are randomly sampling our data for all pages during the specified weeks in each quarter. You can see that with the exception of the homepage, all of our pages have gotten slower on the backend. The performance team kicked off this quarter by hosting a post mortem for a site-wide performance degradation that occurred at the end of Q2. At that time, we had migrated a portion of our web servers to new, faster hardware, however the way the workload was initially distributed was overworking the old hardware, leading to poor performance for the 95th percentile. Increasing the weighting of the new hardware in the loadbalancer helped mitigate this. While medians did not see a significant impact over the course of the hardware change, it caused higher highs and lower lows for the 95th percentile. As a heavier page, the signed-in homepage saw the greatest improvement once the weights were adjusted, which contributed to its overall improvement this quarter. Other significant causes for the changes seen on the server side can be attributed to two new initiatives that were launched this quarter, Project Arizona and Category Navigation. Arizona is a read-only key / value system to serve product recommendations and other generated datasets on a massive scale. It replaces a previous system that we had outgrown that stored all data in-memory; Arizona instead uses SSDs to allow for more and varied datasets. This quarter we launched the first phase of the project that resulted in some expected performance regressions compared with the previous memory-backed system. The first phase focused on correctness, ensuring data remained consistent between the two systems. Future phases will focus on optimizing speed of lookups to be comparable to the previous system while offering much greater scalability and availability. In the beginning of August, our checkout team noticed two separate regressions on the cart page that had occurred over the course of the prior month. We had not been alerted on these slowdowns because at the end of Q2, the checkout team had launched cart pagination which improved the performance of the cart page by limiting the number of items loaded and we had not adjusted the thresholds to match this new normal. Luckily, the checkout team noticed the change in performance and we were able to trace the cause back to testing for Arizona. While in the midst of testing for Arizona, we also launched a new site navigation bar that is included under the search bar on every page and features eight of the main shopping categories. Not only does the navigation bar make it easier for shoppers to find items on the site, but we also believe that the new navigation will positively affect Search Engine Optimization, driving more traffic to shops. While testing the feature we noticed some performance impacts so when the feature launched at the end of August, we were closely watching as we expected a performance degradation due to the amount of the HTML being generated. The performance impact was felt across the majority of our pages though it was more noticeable on some pages than others depending on the weight of the page. For example, lighter pages such as baseline appear harder hit because the navigation bar accounts for a significant amount of the page’s overall weight. In an awesome win, in response to the anticipated performance hit, the buyer experience engineering team ramped up client side rendering for this new feature, which cut down the rendering time on buyer side pages by caching the HTML output and shipping less to the client. In addition to the hardware change, Project Arizona and the new site navigation feature, we also have been investigating a slow, gradual regression we noticed across several pages that began in the first half of Q3. Extensive investigation and testing revealed that the regression was the result of limited CPU resources. We are currently adding additional CPU capacity and anticipate the affected pages will get faster in this current quarter. Synthetic Start Render Let’s move on to our synthetic tests where we have instrumented browsers load pages automatically every 10 minutes from several locations. This expands the scope of analysis to include browser-side measurements along with server-side. The strength of synthetic measurements is that we can get consistent, highly-detailed metrics about typical browser scenarios. We can look at “start render” to estimate when most people first see our pages loading. The predominant observation is that our median render-start times across most pages has increased about 300ms compared to last quarter. You might expect a performance team to feel bummed out about a distinctly slower result, but we actually care about more about the overall user experience than just page speed measurements on any given week. The goal of our Performance team is not just to make a fast site, but to encourage discussions that accurately consider performance as one important concern among several. This particular slowdown was caused by broader use of our new css toolkit, which adds 35k of CSS to every page. We expect the toolkit to be a net-win eventually, but we have to pay a temporary penalty while we work on eliminating non-standard styles. Several teams gathered together to discuss the impact of this change, which gave us confidence that Etsy’s culture of performance is continuing to mature, despite this particular batch of measurements. The median render-start time for our search page appears to have increased by 800ms, following a similar degradation in the last quarter, but we found this to be misleading. We isolated this problem to IE browsers versions 10 and older, which actually represents a tiny fraction of Etsy users. The search page renders much faster (around 1100ms) in Chrome (far more popular), which is consistent with all our other pages across IE and Chrome. Synthetic checks are vulnerable to this type of misleading measurement because it’s really difficult to build comprehensive labs that match the true diversity of browsers in the wild. RUM measurements are better suited to that task. We are currently discussing how to improve the browsers we use in our synthetic tests. What was once a convenient metric for estimating experience may eventually become less meaningful as one fundamentally changes the way a site is loaded. We feel it is important to adapt our monitoring to the new realities of our product. We always want to be aligned with our product teams, helping them build the best experience, rather than spending precious time optimizing for metrics that were more useful in the past. As it happens, we recently made a few product improvements around site navigation (mentioned in the above section). As we optimized the new version, we focused on end-user experience and it became clear that ‘Webpage Response’ was becoming less and less connected to end-user experience. WR includes the time for ALL assets loaded on the page, even if these requests are hidden from the end-user, such as deferred beacons. We are evaluating alternative ways to estimate end-user experience in the future. Real User Page Load Time Real user monitoring give us insight into actual page loads experienced by end-users. Notably, it accounts for real-world diversity of network conditions, browser versions, and internationalization. We can see across-the-board increases, which is in line with our other types of measurements. By looking at the daily summaries of these numbers, we confirmed that the RUM metrics regressed when we launched our revamped site navigation (first mentioned in the server-side section). Engineers at Etsy worked to optimize this feature over the next couple weeks and made progress, though one optimization ended up causing a regression on some browsers. This was not exposed except in our RUM data. We have a plan to speed this up during the fourth quarter. Conclusion In the third quarter, we had our ups and downs with site performance, due to both product and infrastructure changes. It is important to remember that performance cannot be reduced merely to page speed; it is a balancing act of many factors. Performance is a piece of the overall user experience and we are constantly improving our ability to evaluate performance and make wiser trade-offs to build the best experience. The slowdowns we saw this quarter have only reinforced our commitment to helping our engineering teams monitor and understand the impact of the new features and infrastructure changes they implement. We have several great optimizations and tools in the pipeline and we look forward to sharing the impact of these in the next report. Posted by Mike Adler on November 10, 2015 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Kristyn Reith on 13 Jul, 2015 Q2 2015 Site Performance Report", "date": "2015-11-10,"},
{"website": "Etsy", "title": "Managing Hadoop Job Submission to Multiple Clusters", "author": ["Andrew Johnson"], "link": "https://codeascraft.com/2015/09/24/managing-hadoop-job-submission-to-multiple-clusters/", "abstract": "Managing Hadoop Job Submission to Multiple Clusters Posted by Andrew Johnson on September 24, 2015 At Etsy we have been running a Hadoop cluster in our datacenter since 2012.  This cluster handled both our scheduled production jobs as well as all ad hoc jobs.  After several years of running our entire workload on this one production Hadoop cluster, we recently built a second.  This has greatly expanded our capacity and ability to manage production and ad hoc workloads, and we got to have fun coming up with names for them (we settled on Pug and Basset!).  However, having more than one cluster has brought new challenges.  One of the more interesting issues that came up was how to manage the submission of ad hoc jobs with multiple clusters. The Problem As part of building out our second cluster we decided to split our current workload between the two clusters.  Our initial plan was to divide the Hadoop workload by having all scheduled production jobs run on one cluster and all ad hoc jobs on the other.  However, we recognized that those roles would change over time.  First, if there were an outage or we were performing maintenance on one of the clusters, we may shift all the workload to the other.  Also, as our workload changes or we introduce new technology, we may balance the workload differently between the two clusters. When we had only one Hadoop cluster, users of Hadoop would not have to think about where to run their jobs.  Our goal was to keep it easy to run an ad hoc job without users needing to continually keep abreast of changes in which cluster to use.  The major obstacle for this goal is that all Hadoop users submit jobs from their developer VMs.  This means we would have to ensure that the changes necessary to switch which cluster should be used for ad hoc jobs propagate to all of the VMs in a timely fashion.  Otherwise some users would still be submitting their jobs to the wrong cluster, which could mean those jobs would fail or otherwise be disrupted. To simplify this and avoid such issues, we wanted a centralized mechanism for determining which cluster to use. Other Issues There were two related issues that we decided to address at the same time as managing the submission of ad hoc jobs to the correct cluster.  First, we wanted the cluster administrators to have the ability to disable ad hoc job submission entirely.  Previously we had relied on asking users via email and IRC to not submit jobs, which is only effective if everyone checks and sees that request before launching a job.  We wanted a more robust mechanism that would truly prevent running ad hoc jobs.  Also, we wanted a centralized location to view the client-side logs from running ad hoc jobs.  These would normally only be available in the user’s terminal, which complicates sharing these logs when getting help with debugging a problem.  We wanted both of these features regardless of having the second Hadoop cluster.  However, as we considered various approaches for managing ad hoc job submission to multiple clusters, we found that we could solve these problems at the same time. Our Approach We chose to use Apache Oozie to manage ad hoc job submission.  Using Oozie had several significant advantages for us.  First, we already were using Oozie for all of our scheduled production workflows.  As such we already understood it well and had it properly operationalized.  It also allowed us to reuse existing infrastructure rather than setting up something new, which greatly reduced the time and effort necessary to complete this project. Next, using Oozie let us distribute the load from the job client processes across the Hadoop cluster.  When ad hoc job submission occurred on users’ VMs, this load was naturally distributed.  Distributing this load across the Hadoop cluster allows this approach to grow with the cluster.  Moreover, using Oozie automatically provided a central location for viewing the client logs from job submission.  Since the clients run on the Hadoop cluster, their logs are available just like the logs from any other Hadoop job.  As such they can be shared and examined without needing to retrieve them from the user’s terminal. There was one downside to using Oozie: it did not support automatically directing ad hoc jobs to the appropriate cluster or disabling the submission of ad hoc jobs.  We had to build this ourselves, but as Oozie was handling everything else it was very lightweight.  To minimize the amount of new infrastructure for this component, we used our existing internal API framework to manage this state.  We call this component the “state service”. The Job Submission Process Previously the process of submitting an ad hoc job looked like this: Now submitting an ad hoc job looks like this instead: From the perspective of users nothing had changed; they would still launch jobs using our run_scalding script on their VM.  Internally, it would request the active ad hoc cluster using the API for the state service.  This API call would also indicate if ad hoc job submission was disabled, allowing the script to terminate.  Administrators can also set a message that would be displayed to users when this happens, which we use to provide information about why ad hoc jobs were disabled and the ETA on re-enabling them. Once the script determined the cluster on which the job should run, it would generate an Oozie workflow from a template that would run the user’s job.  This occurs transparently to the user so that they do not have to be concerned about the details of the workflow definition.  The script then submits this generated workflow to Oozie, and the job runs.  The change most visible to users in this process is that the client logs no longer appear in their terminal as the job executes.  We considered trying to stream them from the cluster during execution, but to minimize complexity the script prints a link to the logs on the cluster after the job completes. Other Options While using Oozie ended up being the best choice for us, there were several other approaches we considered. Apache Knox Apache Knox is a gateway for Hadoop REST APIs.  The project primarily focuses on security, so it’s not an immediate drop-in solution for this problem.  However, it provides a gateway, similar to a reverse proxy, that maps externally exposed URLs to the URLs exposed by the actual Hadoop clusters.  We could have used this functionality to define URLs for an “ad hoc” cluster and change the Knox configuration to point that to the appropriate cluster. Nevertheless, we felt Knox was not a good choice for this problem.  Knox is a complex project with a lot of features, but we would have been using only a small subset of these.  Furthermore, we would be using it outside of its intended use case, which could complicate applying it to solve our problem.  Since we did not have experience operating Knox at scale, we felt it would be better to stick with Oozie, which we already understood and would not have to shoehorn into our use case. Custom Job Submission Server We also considered implementing our own custom process to both manage the state of which cluster was currently active for ad hoc jobs as well as handling centralized job submission.  While this would have provided the most flexibility, it also meant building a lot of new infrastructure.  We would have essentially been reimplementing Oozie, but without any of the community testing or support.  Since we were already using Oozie and it met all our requirements, there was no need to build something custom. Gateway Server The final approach we considered was having a “gateway server” and requiring users to SSH to that server and launch jobs from there instead of from their VM.  This would have simplified the infrastructure components for job submission.  The Hadoop configuration changes to point ad hoc job submissions to the appropriate cluster or disable job submission entirely would only need to be deployed there.  By its very nature it would provide a central location for the client logs.  However, we would have to manage scaling and load balancing for this approach ourselves.  Furthermore, it would represent a significant departure from how development is normally done at Etsy.  Allowing users to write and run Hadoop jobs from their VM is important for keeping Hadoop as accessible as possible.  Adding the additional step of moving changes and SSH-ing to a gateway server compromises that goal. Conclusion Using Oozie to manage ad hoc job submission in this way has worked well for us.  Reusing the Oozie infrastructure we already had let us quickly build this out, and having this new process for running jobs made the transition to having two Hadoop clusters much easier.  Moreover, we were able to keep the process of submitting an ad hoc job almost identical to the previous process, which minimized the disruption for users of Hadoop. As we were developing this, we found that there was only minimal discussion online about how other organizations have managed ad hoc job submission with multiple clusters.  Our hope is that this review of our approach as well as other options we considered is helpful if you are in the same situation and are looking for ideas for your own process of ad hoc job submission. Posted by Andrew Johnson on September 24, 2015 Category: data , engineering , infrastructure Tags: hadoop , oozie Related Posts Posted by Andrew Johnson on 16 Dec, 2015 Introducing Arbiter: A Utility for Generating Oozie Workflows Posted by Andrew Johnson on 12 May, 2015 Four Months of statsd-jvm-profiler: A Retrospective Posted by Andrew Johnson on 14 Jan, 2015 Introducing statsd-jvm-profiler: A JVM Profiler for Hadoop", "date": "2015-09-24,"},
{"website": "Etsy", "title": "Assisted Serendipity – Fostering Peer to Peer Connections in Your Organization", "author": ["John Goulah"], "link": "https://codeascraft.com/2015/09/15/assisted-serendipity/", "abstract": "Assisted Serendipity – Fostering Peer to Peer Connections in Your Organization Posted by John Goulah on September 15, 2015 It happens at every growing company – one day you pass someone in the hallway of your office and have no idea whether they work with you, or if they’re just visiting your office. You used to know just about everyone at your company, but you’re growing so fast and hiring so quickly that it’s hard to keep up.  Even the most extroverted of us have a hard time learning everyone’s name when offices start expanding to different floors, different states, and even different countries. One way to combat this problem is to give employees a means of being randomly introduced to each other. We’ve already written a bit about culture hacking using a staff database , and the tool we’re open sourcing today takes advantage of this employee data that we make available within the company. The tool that we’re releasing is called Mixer . It’s a simple web app that allows people to join a group and then get randomly paired with another member of that group. It then prompts you to meet each other for a coffee, lunch, or a drink after work.  If the person you get paired up with is working remotely, that’s not a problem — just hop on a video chat.  This encourages people who may not work in the same place to stay in touch and find out what’s going on in each other’s day to day.  The tool keeps a history of the pairings and attempts to match you with someone unique each week; it’s possible to opt in or out of the program at any time. A lot of managers believe in the value of regular one-on-one meetings with their reports, but it is less common to do so with peers. At Etsy, these meetings between peers have resulted in cross-departmental partnerships that might not have otherwise surfaced, on top of providing an avenue of support for folks to work through difficult situations. These conversations also generally strengthen our culture by introducing people to their co-workers. Other benefits include learning more about what others are working on, brainstorming new collaborative projects that utilize strengths from a diverse set of core skillsets, and getting help with a challenge from someone who is distanced from the situation. Mixer meetings both introduce people who have never met and give folks who know each other a chance to connect in a way they might not have otherwise made time for. As your company grows, it’s important to facilitate the person-to-person connections that happened naturally when everyone fit in the same small room. These interactions create the fabric of your company’s community and are crucial opportunities for building culture and fostering innovation. Our hope is that the Mixer tool can help you scale those genuine connections as you continue to see new faces in the hallway. Find the Mixer code on Github Posted by John Goulah on September 15, 2015 Category: people Tags: communication , culture , golang , hacking , mixer , staff", "date": "2015-09-15,"},
{"website": "Etsy", "title": "How Etsy Uses Thermodynamics to Help You Search for “Geeky”", "author": ["Fiona Condon"], "link": "https://codeascraft.com/2015/08/31/how-etsy-uses-thermodynamics-to-help-you-search-for-geeky/", "abstract": "How Etsy Uses Thermodynamics to Help You Search for “Geeky” Posted by Fiona Condon on August 31, 2015 Etsy shoppers love the large and diverse selection of our marketplace. But, for those who don’t know exactly what they’re looking for, the sheer number and variety of items available can be more frustrating than delightful. In July, we introduced a new user interface which surfaces the top categories for a search request to help users explore the results for queries like “gift.” Searchers who issue broad queries like this often don’t have a specific type of item in mind, and are especially likely to finish their visit empty-handed. Our team lead, Gio, described our motivations and process in an (excellent) blog post last month, which gives more background on the project. In this post, I’ll focus on how we developed and iterated on our heuristic for classifying queries as “broad.” Our navigable interface, shown for a query for “geeky gift” Quantifying “broadness” When I describe what I’m working on to people outside the team, they often jump in with a guess about how we use machine learning techniques to determine which queries are broad in code. While we could have used complex, offline signals like click or purchasing behavior to learn which queries should trigger the category interface, we actually base the decision on a single calculation, evaluated entirely at runtime, which uses very basic statistics about the search result set. There have been several advantages to sticking with a simpler metric. By avoiding query-specific behavioral signals, our approach works for all languages and long-tail queries out of the gate. It’s performant and relatively easy to debug. It’s also (knock on wood) stable and easy to maintain, with very few external dependencies or moving parts. I’ll explain how we do it, and arguably justify the title of this post in the process. Let’s take “geeky” as an example of a broad query, one that tells us very little about what type of item the user is looking for. Jewelry is the top category for “geeky,” but there are many items in all of the top-level categories. Compare to the distribution of results for “geeky mug,” which are predictably concentrated in the Home & Living category. In plainspeak, the calculation we use measures how spread out across the marketplace the items returned for the query are. The distribution of results for “geeky” suggests that the user might benefit from seeing the top categories, which demonstrate the breadth of geeky paraphernalia available on the site, from a periodic table-patterned bow tie to a “cutie pi ” mug . The distribution for “geeky mug” is dominated by one category, and shouldn’t trigger the category interface. The categories shown for a query for “geeky” Doing the math In order to quantify how “spread out” items are, we start by taking the number of results returned for the query in each of the top-level categories and deriving the probability that an item is in each category. Since 20% of the items returned are in the Jewelry category and 15% of items are in the Accessories category, the probability values for Jewelry and Accessories would be .2 and .15 respectively. We use these values as the inputs to the Shannon entropy formula: Shannon entropy formula This formula is a measure of the disorder of a probability distribution. It’s essentially equivalent to the formula used to calculate the thermodynamic entropy of a physical system, which models a similar concept . For our purposes, let r t be the total number of results and r i be the number of results for category i. Then the probability value in the above equation would be (r i /r t ) and the entropy of the distribution of a search result set across its categories can be expressed as: Entropy of a search result set In this way, we can determine when to show categories without using any offline signals. This is not to say that we didn’t use data in our development process at all. To determine the entropy threshold above which we should show categories, we looked at the entropies for a large sample of queries and made a fairly liberal judgement call on a good dividing line (i.e. a low threshold). Once we had results from an AB experiment which showed the new interface to real users, we looked to see how it affected user behavior for queries with lower entropy levels, and refined the cut-off based on the numbers. But this was a one-off analysis; we expect the threshold to be static over time, since the distribution of our marketplace across categories changes slowly. Taking it to the next level A broad query may not necessarily have high entropy at the top level of our taxonomy. Results for “geeky jewelry” are unsurprisingly concentrated in our Jewelry category, but there are still many types of items that are returned. We’d like to guide users into more specific subcategories, like Earrings and Necklaces, so we introduced a secondary round of entropy calculations for queries that don’t qualify as broad at the top level. It works like this: if the result set does not have sufficiently high entropy to trigger the categories at the top level, we determine the entropy within the most populous category (i.e. the entropy of its subcategories) and show those subcategories if that value exceeds our entropy cut-off. The graph above demonstrates the level of spread of results for the query “jewelry” across subcategories of the top-level Jewelry category. This method allows us to dive into top-level categories in cases like this, while sticking to a simple runtime decision based on category counts. Showing subcategories for a query for “geek jewelry” Iterating on entropy While we were testing this approach, we noticed that a query like “shoes,” which we hoped would be high entropy within the Shoes category, was actually high entropy at the top level. Top-level categories for the “shoes” query… doesn’t seem quite right Items returned for “shoes” are apparently sufficiently spread across the whole marketplace to trigger top-level groups, although there are an unusually high number of items in the Shoes category. More generally, items in our marketplace tend to be concentrated in the most popular categories. A result set is likely to have many more Accessories items than Shoes items, because the former category is an order of magnitude larger than the latter. We want to be able to compensate for this uneven global distribution of items when we calculate the probabilities that we use in our entropy calculation. By dividing the number of items in each category that are returned for the active search by the total number of items in that category, we get a number we can think of as the affinity between the category and the search query. Although fewer than 50% of the results that come back for a query for “shoes” are in the Shoes category, 100% of items in the Shoes category are returned for a query for “shoes,” so its category affinity is much higher than its raw share of the result set. Normalizing the affinity values so they sum to one, we use these measurements as the inputs to the same Shannon entropy formula that we used in the first iteration. The normalization step ensures that we can compare entropy values across search result sets of different sizes. Letting r i represent the number of items in category i for the active search query, and t i represent the total number of items in that category, the affinity value for category i, a i , is simply (r i / t i ). Taking s as the sum of all affinity values a 0 …a i , then, the affinity-based entropy is: Affinity-based entropy of a search result set From a Bayesian perspective, both the original result count-based values and the affinity values calculate the probability that a listing is in a category given that it is returned for the search query. The difference is that the affinity formulation corresponds to a flat prior distribution of categories whereas the original formulation corresponds to the observed category distribution of items in our marketplace. By controlling for the uneven distribution of items across categories on Etsy, affinity-based entropy fixed our “shoes” problem, and improved the quality of our system in general. Refining by recipient on a query for “geeky shoes” Keeping it simple Although our iterations on entropy have introduced more complexity than we had at the outset, we still reap the benefits of avoiding opaque offline computations and dependencies on external infrastructure. Big data signals can be incredibly powerful, but they introduce architectural costs that it turns out aren’t necessary for a functional broad query classifier. On the user-facing level, making Etsy easier to explore is something I’ve wanted to work on since before I started working here many years ago. It’s very frustrating for searchers to navigate through the millions of items of all types that we return for many popular queries. If you’ll indulge my thermodynamics metaphor once more, by helping to guide users out of high-entropy result sets, we’re battling the heat death of Etsy search—and that’s literally pretty cool. Couldn’t stomach that “heat death” joke? Leave a comment or let me know on twitter . Huge thanks due to Giovanni Fernandez-Kincade , Stan Rozenraukh , Jaime Delanghe and Rob Hall. Posted by Fiona Condon on August 31, 2015 Category: data , engineering , search Tags: heat death , search , thermodynamics", "date": "2015-08-31,"},
{"website": "Etsy", "title": "Targeting Broad Queries in Search", "author": ["Giovanni Fernandez-Kincade"], "link": "https://codeascraft.com/2015/07/29/targeting-broad-queries-in-search/", "abstract": "Targeting Broad Queries in Search Posted by Giovanni Fernandez-Kincade on July 29, 2015 We’ve just launched some big improvements to the treatment of broad queries like “father’s day,” “upcycled,” or “boho chic” on Etsy. This is the most dramatic change to the search experience since our switch to relevance by default in 2011 . In this post we’d like to give you an introduction to the product and its development process. We think it’s a great example of the values that are at the heart of product engineering at Etsy: leveraging simple techniques, building iteratively, and understanding impact. Motivations Before we make a big investment in an idea, we like to spend some time investigating whether or not that idea represents a reasonable opportunity. The opportunity at the heart of this project is exploratory queries like “silver jewelry” where users don’t have something particular in mind. There are 2.7 MM results for “silver jewelry” on Etsy today. No matter how good we get at ranking results, the universe of silver jewelry is simply so vast that the chances that we will show you something you like are pretty slim. How big of an opportunity is improving the experience for broad queries? How do we even define a broad query? That’s a really difficult question. Going through this exercise can easily turn into doing the hardest parts of the “real work.” Instead of doing something clever, we time-boxed our analysis and looked at a handful of heuristics for different levels of user intent. Here’s a sample: Number of Tokens Result Set Size Number of Distinct Categories Represented in the Results For each heuristic, we looked at the distribution across a week’s worth of search queries, and chose a threshold that generally separated the broad from the specific queries. We looked at the size of that population and their engagement rates (the green arrow is our target audience): None of the heuristics were independently sufficient, but by looking at several we were able to generate a rough estimate: it turns out that a sizable portion of searches on Etsy are broad queries. That matches our intuitions. Etsy is a marketplace of unique goods so it’s hard for consumers to know precisely what to look for. Having some evidence that this was a worthwhile endeavor, we packed our bags and set off to meet the wizard. Crafting an Experience What can we do to improve the experience for users that issue a broad query? What about grouping the results into discrete buckets so users can get a better sense of what types of things are present? Grouping items into their respective categories seemed like an obvious starting place, but we could also group the items by any number of dimensions like style, color, and material. We started with a few quick-and-dirty iterations of design and user-testing. Our designer fashioned a ton of static mocks that he turned into clickable prototypes using Flinto : We followed this up with an unshippable prototype of result grouping on mobile web. We did the simplest possible thing: always show result groupings, regardless of how specific the query is. We even simulated a native version using JPEG technology: People responded really well to these treatments. Many even expressed a desire for the feature before they saw it: “I wish I could just see what types of jewelry there are.” But the user tests also made it painfully clear how problematic false positives (showing groups when search is definitely not broad) were. There were moments of frustration where users clearly just wanted to see some results and the groups were getting in the way. On the other hand, showing too many groups didn’t seem as costly. If random or questionably relevant groups appeared towards the end of the list, users often thought they were interesting  or highlighted what made Etsy unique (“I didn’t know you had those!”), adding a serendipitous flavor to the experience. What’s a broad query? Armed with a binder full of reasonable UX treatments, it was time to start tackling the algorithmic challenge. The heuristics we used at the beginning of this journey were sufficient for ballpark estimation, but they were fairly imprecise and it was clear that minimizing false positives was a priority. We quickly settled on using entropy , which you can think of as a measure of the uncertainty in a probability distribution. In this case, we’re looking at the probability that a result belongs to a particular category. As the probabilities get more concentrated around a handful of categories, the entropy approaches zero. For example, this is the probability distribution for the query “shoes” amidst the top-level categories: As the distribution gets more dispersed, entropy increases. Here is the same distribution for “father’s day”: We looked at samples of queries at different entropy levels to manually decide on a reasonable threshold. Could we have trained a more sophisticated model with some supervised learning algorithms? Probably, but there are a host of challenges with that approach: getting hand-labeled data or dealing with the noise of using behavioral signals for training data, data sparsity/coverage, etc. Ultimately, we already had what we thought was the most discriminating factor, the resulting algorithm had an intuitive explanation that was easy to reason about, and we felt confident that it would scale to cover the long tail. Conclusions and Coming Next After a series of A/B experiments, we’re happy to report that result grouping has resulted in a dramatic increase in user engagement and we’re launching it. But this is only the beginning for this feature and for this story. Henceforth, result grouping will be another lever in the search product toolbox. The work that we’ve been doing for the past year has really been about building a foundation. We’re going to be aggressively iterating on offline evaluation, new treatments, new grouping dimensions,  classification algorithms, and group ordering strategies. We’re in this for the long haul and we’re excited about the many doors this work has opened for us. I hope this post gave you a taste for what went into this effort. In the coming months, we’re going to have many members of the Etsy Search family diving deeper into some of the meatier details on subjects like result grouping performance, iterating on the entropy-based algorithm, and how our new product categories laid the groundwork for these improvements. Oh yeah, and we’re hiring . Posted by Giovanni Fernandez-Kincade on July 29, 2015 Category: data , engineering , search", "date": "2015-07-29,"},
{"website": "Etsy", "title": "Q2 2015 Site Performance Report", "author": ["Kristyn Reith"], "link": "https://codeascraft.com/2015/07/13/q2-2015-site-performance-report/", "abstract": "Q2 2015 Site Performance Report Posted by Kristyn Reith on July 13, 2015 We are kicking off the third quarter of 2015, which means it’s time to update you on how Etsy’s performance changed in Q2. Like in our last report, we’ve taken data from across an entire week in May and are comparing it with the data from an entire week in March. We’ve mixed things up in this report to better visualize our data and the changes in site speed: For backend performance, we randomly sampled 1% of our data for all pages except the baseline page and we will be looking at that sampled data for the aforementioned weeks from each quarter. We are using the full set of data for each week for the baseline page as it gets very little traffic. We are using box plots to represent the data in each section. For the synthetic and real user monitoring portions, the data points plotted are median measurements. For listing and shop pages in the synthetic section, we are using a slightly different date range (the first week of April). This is to compare apples to apples; as we noted in our last report , we had an issue in which the shop we were monitoring changed state. We added the cart page to this quarter’s report. Since we have only just started measuring cart this quarter, we do not have data from March to measure it against; however, moving forward cart will continue to appear in all our quarterly reports. As in the past, we’ve split up the sections of this report among members of our performance team. Allison McKnight will be reporting on the server-side portion, Kristyn Reith will be covering the synthetic front-end section and Natalya Hoota will be providing an update on the real user monitoring section. We have to give a special shout out to our bootcamper Emily Smith, who spent a week working with us and digging into the synthetic changes that we saw. So without further ado, let’s take a look at the numbers. Server-Side Performance Taking a look at our backend performance, we see that the quartile boundaries for home, listing, shop, and baseline pages haven’t changed much between Q1 and Q2. We see a change in the outliers for the shop and baseline pages – the outliers are more spread out (and the largest outlier is higher) in this quarter compared to the last quarter. For this report, we are going to focus on analyzing only changes in the quartile boundaries while we work on honing our outlier analysis skills and tools for future reports. On the cart page, we see the top whisker and outliers move down. During the week in May when we pulled this data, we were running an experiment that added pagination to the cart. Some users have many items in their carts; these items take a long time to load on the backend. By limiting the number of items that we load on each cart page, we improve the backend load time for these users especially. If we were to look at the visit data in another format, we might see a bimodal distribution where users exposed to this experiment would have clearly different performance than users who didn’t see the experiment. Unfortunately, box plots limit our view on whether user experience could be statistically divided into two separate categories (i.e. multimodal distribution ). We’re happy to say that we launched this feature in full earlier this week! This quarter, the Search team experimented with new infrastructure that should make desktop and mobile experience more streamlined. On the backend, this translated into a slightly higher median time with an improvement for the slower end of users: the top whisker moved down from 511 ms to 447 ms, and the outliers moved down with it. The bottom whisker and the third quartile also moved down slightly while the first quartile moved up. Taking a look at our timeseries record of search performance across the quarter, we see that a change was made that greatly impacted slower loads and had a smaller impact on median loads: Synthetic Start Render and Webpage Response Most things look very stable quarter over quarter for synthetic measurements of our site’s performance. As we only started our synthetic measurements for the cart page in May, we do not have quarter-over-quarter data. You can see that the start render time of the search page has gotten slower this quarter but that the webpage response time for search sped up. The regression in start render was caused by experiments being run by our search team, while the improvement in the webpage response time for search resulted from the implementation of the Etsy styleguide toolkit. The toolkit is a set of fully responsive components and utility classes that make layout fast and consistent. Switching to the new toolkit decreased the amount of custom CSS that we deliver on search pages by 85%. As noted above, we are using a slightly different date range for the listing and shop data so that we can compare apples to apples. Taking a look at the webpage response time box plots, we see improvements to both the listing and shop pages. The faster webpage response time for the listing page can be attributed to an experiment running that reduced the page weight by altering the font-weights. The improvement to shop’s webpage response time is the result of migrating to a new tag manager that is used to track the performance of outside advertising campaigns. This migration allowed us to fully integrate third party platforms in new master tags which reduced the number of JS files for campaigns. Real User Page Load Time The software we use to measure our real user measurements, mPulse, was updated in the middle of this quarter, leading to a number of improvements in timer calculation and data collection and validation. Expectedly, we saw a much more comprehensive pattern in data outliers (i.e., values falling far above and below the average) on all pages, and are excited for this cleaner data set. Since Q1 and Q2 data was collected with different versions of the real user monitoring software, it would not be scientifically accurate to make any conclusions about our user experiences this quarter relative to the previous one. It definitely looks like an overall, though slight, improvement sitewide, a trend which we hope to keep throughout next quarter. Conclusion Although we saw a few noteworthy changes to individual pages, things remained fairly stable in Q2. Using box plots for this report helped us provide a more holistic representation of the data distribution, range and quality by looking at the quartile ranges and the outliers. For next quarter’s report we are really excited about the opportunity to continue exploring new, more efficient ways to visualize the quarterly data. Posted by Kristyn Reith on July 13, 2015 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2015-07-13,"},
{"website": "Etsy", "title": "Open Source Spring Cleaning", "author": ["Daniel Schauenberg"], "link": "https://codeascraft.com/2015/07/09/open-source-spring-cleaning/", "abstract": "Open Source Spring Cleaning Posted by Daniel Schauenberg on July 9, 2015 At Etsy, we are big fans of Open Source. Etsy as it is wouldn’t exist without the myriad of people who have solved a problem and published their code under an open source license. We serve etsy.com through the Apache web server running on Linux, our server-side code is mostly written in PHP, we store our data in MySQL, we track metrics using graphs from from Ganglia and Graphite to keep us up to date, and use Nagios to monitor the stability of our systems. And these are only the big examples. In every nook and cranny of our technology stack you can find Open Source code. Part of everyone’s job at Etsy is what we call “Generosity of Spirit”, which means giving back to the industry. For engineers that means that we strive to give a talk at a conference, write a blog post on this blog or contribute to open source at least once every year. We love to give back to the Open Source community when we’ve created a solution to a problem that we think others might benefit from. Maintenance and Divergence This has led to many open sourced projects on our GitHub page and a continuing flow of contributions from our engineers and the Open Source community. We are not shy about open sourcing core parts of our technology stack. We are publicly developing our deployment system , metrics collector , team on-call management tool and our code search tool . We even open sourced the crucial parts of our atomic deployment system. And it has been very rewarding to receive bug fixes and features from the wider community that make our software more mature and stable. As we open sourced more projects, it became tempting to run an internal fork of the project when we wanted to add new features quickly. These projects with internal forks quickly diverged from the open sourced versions. This meant the work to maintain the project was doubled. Anything fixed or changed internally had to be fixed or changed externally, and vice versa. In a busy engineering organization, the internal version usually was a priority over the public one. Looking at our GitHub page, it wasn’t clear – even to an Etsy engineer – whether or not we were actively maintaining a given project. We end up with public projects that hadn’t been committed to in years. Open sourcers who were taking the time to file a bug report and didn’t get an answer on the issue, sometimes for years, which didn’t instill confidence in potential users. No one could tell whether the project is a maintained piece of software or a proof of concept that won’t get any updates. Going forward We want to do better by the Open Source community, since we’ve benefited so much from existing Open Source Software. We did a bit of Open Source spring cleaning to bring more clarity to the state of our open source projects. Going forward our projects will be clearly labeled as either maintained, not maintained, or archived. Maintained Maintained projects are the default and are not specifically labeled as such. For maintained projects, we’re either running the open source version internally or currently working on getting our internal version back in sync with the public version. We already did this for our deployment tool in the past. We are actively working on any maintained projects: merging or commenting on pull requests, answering bug reports, and adding new features. Not Maintained We also have a few of projects that haven’t seen public updates in years. Usually this is because we haven’t found a way to make the project configurable in a way such that we can run the public version internally without slowing down our development cycles. However the code as it is available serves as a great proof of concept and illustrates how we approach the problem. Or it might have been a research project that we have abandoned because it turned out to not really solve our problem in the long run but still wanted to share what we tried. Those projects will just stay the way they are and likely will rarely receive any updates. We will turn off issues and pull requests on those and make it very clear in the README that this is a proof of concept only. Archived We also have a number of projects that we have open sourced because we were using them at one time but have since abandoned altogether. We have likely found that there exists a better solution to the problem or that the solution hasn’t proven useful in the long run. In those cases we will push a commit to the master branch that removes all code and only leaves the README with a description of the project and its status. The README will link to the last commit containing actual code. This way the code doesn’t just vanish, but the project is clearly not active. Those projects will also have issues and pull requests turned off. In addition to the archival of those projects we will also start to delete forks of other Open Source projects that we’ve made at some point, but aren’t actively maintaining. Closing thoughts We have learned a lot about maintaining Open Source projects over the last couple of years. The main lesson we want to share is that it’s essential to use the Open Source version internally to provide a good experience for other Open Source developers who want to use our software. We strive to always learn and get better at everything we do. If you’ve been waiting for us to respond to an issue or merge a pull request, hopefully this will give you more insight into what has been going on and why it took so long for us to respond, and we hope that our new project labeling system will also give you more clarity about the state of our open source projects. In order to be good open source citizens we want to always do our best to give back in a way that is helpful for everyone. And a little spring cleaning is always a good thing. Even if it’s technically summer already. You can follow Jared on Twitter here and Daniel here . Posted by Daniel Schauenberg on July 9, 2015 Category: engineering , people", "date": "2015-07-9,"},
{"website": "Etsy", "title": "Four Months of statsd-jvm-profiler: A Retrospective", "author": ["Andrew Johnson"], "link": "https://codeascraft.com/2015/05/12/four-months-of-statsd-jvm-profiler-a-retrospective/", "abstract": "Four Months of statsd-jvm-profiler: A Retrospective Posted by Andrew Johnson on May 12, 2015 It has been about four months since the initial open source release of statsd-jvm-profiler .  There has been a lot of development on it in that time, including the addition of several major new features.  Rather than just announcing exciting new things, this is a good opportunity to reflect on what has come of the project since open-sourcing it and how these new features came to be. External Adoption It has been very exciting to see statsd-jvm-profiler being adopted outside of Etsy, and we’ve learned a lot from talking to these new users.  It was initially built for Scalding , and many of the people who’ve tried it out have been profiling Scalding jobs.  However, I have spoken to people who are using it to profile jobs written in other MapReduce APIs, such as Scrunch , as well as pure MapReduce jobs.  Moreover, others have used it with tools in the broader Hadoop ecosystem, such as Spark or Storm .  Most interestingly, however, there have been a few people using statsd-jvm-profiler outside of Hadoop entirely, on enterprise Java applications.  There was never anything Hadoop-specific about the profiling functionality, but it was very gratifying to see that they were able to apply it unchanged to a domain so far from the initial use case. Contributions One of the major benefits of open-sourcing a project is the ability to accept contributions from the community.  This has definitely been helpful for statsd-jvm-profiler.  There have been several pull requests accepted, both fixing bugs and adding new features.  Also, there are some active forks that the authors hopefully decide to contribute back.  The community of contributors is small, but the contributions have been valuable.  Questions about how to contribute were common, however, so the project now has contribution guidelines . An unexpected aspect of community involvement in the project has been the amount of questions and suggestions that have come via email instead of through Github.  In hindsight setting up a mailing list for the project would have been a good idea; at the time of the initial release I had thought the utility of a mailing list for the project was low.  I have since created a mailing list for the project , but it would have been useful to have those original emails be publically available.  Nevertheless, the suggestions have been very helpful.  It would be amazing if everyone who had suggested improvements also sent pull requests, but I recognize that not everyone is willing or able to do so.  Even so I am grateful that people have been willing to contribute to the project in this way. Internal Use The use of statsd-jvm-profiler within Etsy has been less successful than it was externally.  We use Graphite as the backend for StatsD and as we started to use the profiler more, we began to have problems with Graphite.  Someone would start to profile a job, thus creating a fairly large number of new metrics.  This would sometimes cause Graphite to lock up and become unresponsive.  We put in some workarounds, including rate limiting the metric creation and configurable filtering of the metrics produced by CPU profiling, but these were ultimately only beneficial for smaller jobs.  Graphite is an important part of our infrastructure beyond statsd-jvm-profiler, so this was a bad situation.  Being able to profile and improve the performance of our Hadoop jobs is important, but not breaking critical pieces of infrastructure is more important.  The issues with Graphite meant that the ability to use the profiler was heavily restricted.  This was the exact opposite of the goal of easy to use, accessible profiling that motivated the creation of statsd-jvm-profiler.  Finally after breaking Graphite yet again the profiler was disabled entirely.  The project admittedly languished for about a month.  Since we weren’t using it internally, there was less incentive to continue improving it. New Features statsd-jvm-profiler was in an interesting state at this point.  There were still external users and internal interest, but it was too risky for us to actually use it.  Rather than abandon the project, I set out to bring to a better state, one where we could use it without risk to other parts of our production infrastructure.  The contributions from the community were incredibly helpful at this point.  Ultimately the new features were all developed internally, but the suggestions and feedback from the community provided lots of ideas for what to change that would both meet our internal needs as well as providing value externally.  As a result we’re able to use it internally again without DDOSing our Graphite infrastructure. Multiple Metrics Backends The idea of supporting multiple backends for metrics collection instead of just StatsD was considered during initial development, but was discarded to keep the profiling data flowing through StatsD and Graphite.  We use these extensively at Etsy, and the theory was that by keeping the profiling data in a familiar tool would make it more accessible.  In practice, however, the sheer volume of data produced from all the jobs we wanted to profile tended to overwhelm our production infrastructure. Also, supporting different backends for metric collections was the most commonly requested feature from the community, and there were a lot of different suggestions for which to use.  StatsD is still the default backend, but it is configurable through the reporter argument to the profiler.  We are trying out InfluxDB as the first new backend.  There are a couple of reasons why it was selected.  First, statsd-jvm-profiler produces very bursty metrics in a very deep hierarchy.  This is fairly different than the normal use case for Graphite and we came to realise that Graphite was not the right tool for the job.  InfluxDB was very easy to set up and had better support for such metrics without needing any configuration.  Also, InfluxDB has a much richer, SQL-like query language .  With Graphite we had been dumping all of the metrics to a file and processing that, but InfluxDB’s query language allows for more complex visualization and analysis of the profiling data without needing the intermediate step.  So far InfluxDB has been working well.  Moreover, since it is independent from the rest of our production infrastructure only statsd-jvm-profiler will be affected if problems do arise. Furthermore, the refactoring done to support InfluxDB in addition to StatsD has created a framework for supporting any number of backends.  This provides a great avenue for community contributions to support some other metric collection service. New Dashboard Better tooling for visualizing the data produced by profiling was another common feature request.  The initial release included a script for producing flame graphs , but it was somewhat hard to use.  Also, we had otherwise been using our internal framework for dashboards to get data from Graphite.  With the move to InfluxDB this wouldn’t be possible anymore.  As such we also needed a better visualization tool internally. To that end statsd-jvm-profiler now includes a simple dashboard .  It is a Node.js application and pulls data from InfluxDB, leveraging its powerful query language.  It expects the metric prefix configured for the profiler to follow a certain pattern , but then you can select a particular process for which to view the profiling data: From there it will display memory usage over the course of profiling: And it will also display the count of garbage collections and the total time spent in GC: It can also produce an interactive flame graph: Embedded HTTP Server Finally, the ability to disable CPU profiling after execution had started was the other most common feature request.  There was an option to disable it from the start, but not after the profiler was already running.  Both this and the ability to inspect some of the profiler state would have been useful for us while debugging the issues that arose with Graphite initially.  To support both of these features, statsd-jvm-profiler now has an embedded HTTP server .  By default this is accessible from port 5005 on the machine the application being profiled is running on, but this choice of port can be configured with the httpPort option to the profiler.  At present this both exposes some simple information about the profiler’s state and allows disabling collection of CPU or memory metrics.  Adding additional features here is another great place for community contributions. Conclusions Unequivocally statsd-jvm-profiler is better for having been open-sourced.  There has been a lot of activity on the project in the months since its initial public release.  It has seen adoption in a variety of use cases, including some quite different from those for which it was initially designed.  There has been a small but helpful community of contributors, both through code and through feedback and suggestions for the project.  When we hit issues using the project internally, the feedback from the community aligned very well with what we needed to get the project back on track and gave us momentum to keep going.. Going forward keeping up contributions from the community is definitely important to the success of the project.  There is a mailing list now, contribution guidelines , as well as some suggestions for how to contribute.  If you’d like to get involved or just try out statsd-jvm-profiler, it is available on Github ! Posted by Andrew Johnson on May 12, 2015 Category: engineering , infrastructure Tags: hadoop , jvm , profile , profiler , statsd Related Posts Posted by Andrew Johnson on 16 Dec, 2015 Introducing Arbiter: A Utility for Generating Oozie Workflows Posted by Andrew Johnson on 24 Sep, 2015 Managing Hadoop Job Submission to Multiple Clusters Posted by Andrew Johnson on 14 Jan, 2015 Introducing statsd-jvm-profiler: A JVM Profiler for Hadoop", "date": "2015-05-12,"},
{"website": "Etsy", "title": "Experimenting with HHVM at Etsy", "author": ["Dan Miller"], "link": "https://codeascraft.com/2015/04/06/experimenting-with-hhvm-at-etsy/", "abstract": "Experimenting with HHVM at Etsy Posted by Dan Miller on April 6, 2015 In 2014 Etsy’s infrastructure group took on a big challenge: scale Etsy’s API traffic capacity 20X. We launched many efforts simultaneously to meet the challenge, including a migration to HHVM after it showed a promising increase in throughput. Getting our code to run on HHVM was relatively easy, but we encountered many surprises as we gained confidence in the new architecture. What is HHVM? Etsy Engineering loves performance , so when Facebook announced the availability of the HipHop Virtual Machine for PHP, its reported leap in performance over current PHP implementations got us really excited. HipHop Virtual Machine (HHVM) is an open-source virtual machine designed for executing programs written in PHP. HHVM uses a just-in-time (JIT) compilation approach to achieve superior performance while maintaining the development flexibility that PHP provides. This post focuses on why we became interested in HHVM, how we gained confidence in it as a platform, the problems we encountered and the additional tools that HHVM provides. For more details on HHVM, including information on the JIT compiler, watch Sara Golemon and Paul Tarjan’s presentation from OSCON 2014. Why HHVM? In 2014 engineers at Etsy noticed two major problems with how we were building mobile products. First, we found ourselves having to rewrite logic that was designed for being executed in a web context to be executed in an API context. This led to feature drift between the mobile and web platforms as the amount of shared code decreased. The second problem was how tempting it became for engineers to build lots of general API endpoints that could be called from many different mobile views. If you use too many of these endpoints to generate a single view on mobile you end up degrading that view’s performance. Ilya Grigorik’s “ Breaking the 1000ms Time to Glass Mobile Barrier ” presentation explains the pitfalls of this approach for mobile devices. To improve performance on mobile, we decided to create API endpoints that were custom to their view. Making one large API request is much more efficient than making many smaller requests. This efficiency cost us some reusability, though. Endpoints designed for Android listing views may not have all the data needed for a new design in iOS. The two platforms necessitate different designs in order to create a product that feels native to the platform. We needed to reconcile performance and reusability. To do this, we developed “bespoke endpoints”. Bespoke endpoints aggregate smaller, reusable, cacheable REST endpoints. One request from the client triggers many requests on the server side for the reusable components. Each bespoke endpoint is specific to a view. Consider this example listing view. The client makes one single request to a bespoke endpoint. That bespoke endpoint then makes many requests on behalf of the client. It aggregates the smaller REST endpoints and returns all of the data in one response to the client. Bespoke endpoints don’t just fetch data on behalf of the client, they can also do it concurrently . In the example above, the bespoke endpoint for the web view of a listing will fetch the listing, its overview, and the related listings simultaneously. It can do this thanks to curl_multi . Matt Graham’s talk “ Concurrent PHP in the Etsy API ” from phpDay 2014 goes into more detail on how we use curl_multi. In a future post we’ll share more details about bespoke endpoints and how they’ve changed both our native app and web development. This method of building views became popular internally. Unfortunately, it also came with some drawbacks. Now that web pages had the potential to hit dozens of API endpoints, traffic on our API cluster grew more quickly than we anticipated. But that wasn’t the only problem. This graph represents all the concurrent requests that take place when loading the Etsy homepage. Between the red bars is work that is duplicated across all of the fanned out requests. This duplicate work is necessary because of the shared-nothing process architecture of PHP. For every request, we need to build the world: fetch the signed-in user, their settings, sanitize globals and so on. Although much of this duplicated work is carried out in parallel, the fan-out model still causes unnecessary work for our API cluster. But it does improve the observed response time for the user. After considering many potential solutions to this problem, we concluded that trying to share state between processes in a shared-nothing architecture would inevitably end in tears. Instead, we decided to try speeding up all of our requests significantly, including the duplicated bootstrap work. HHVM seemed well-suited to the task. If this worked, we’d increase throughput on our API cluster and be able to scale much more efficiently. Following months of iterations, improvements and bug fixes, HHVM now serves all of the fan-out requests for our bespoke endpoints. We used a variety of experiments to gain confidence in HHVM and to discover any bugs prior to deploying it in production. The Experiments Minimum Viable Product The first experiment was simple: how many lines of PHP code do we have to comment out before HHVM will execute an Etsy API endpoint? The results surprised us. We only encountered one language incompatibility . All of the other problems we ran into were with HHVM extensions. There were several incompatibilities with the HHVM memcached extension, all of which we have since submitted pull requests for. Does it solve our problem? We then installed both PHP 5.4 and HHVM on a physical server and ran a synthetic benchmark. This benchmark randomly splayed requests across three API endpoints that were verified to work in HHVM, beginning at a rate of 10 requests per second and ramping up to 280 requests per second. The throughput results were promising. The little green line at the bottom is HHVM response time Our PHP 5.4 configuration began to experience degraded performance at about 190 requests per second, while the same didn’t happen to HHVM until about 270 requests per second. This validated our assumption that HHVM could lead to higher throughput which would go a long way towards alleviating the load we had placed on our API cluster. Gaining Confidence So far we had validated that HHVM could run the Etsy API (at least with a certain amount of work) and that doing so would likely lead to increase in throughput. Now we had to become confident that HHVM could run etsy.com correctly . We wanted to verify that responses returned from HHVM were identical to those returned by PHP. In addition our API’s full automated test suite and good old-fashioned manual testing we also turned to another technique: teeing traffic. You can think of “tee” in this sense like tee on the command line. We wrote an iRule on our f5 load balancer to clone HTTP traffic destined for one pool and send it to another. This allowed us to take production traffic that was being sent to our API cluster and also send it onto our experimental HHVM cluster, as well as an isolated PHP cluster for comparison. This proved to be a powerful tool. It allowed us to compare performance between two different configurations on the exact same traffic profile. 140 rps peak. Note that this is on powerful hardware. On the same traffic profile HHVM required about half as much CPU as PHP did. While this wasn’t the reduction seen by the HHVM team, who claimed a third as much CPU should be expected, we were happy with it. Different applications will perform differently on HHVM. We suspect the reason we didn’t see a bigger win is that our internal API was designed to be as lightweight as possible. Internal API endpoints are primarily responsible for fetching data, and as a result tend to be more IO bound than others. HHVM optimizes CPU time, not IO time. While teeing boosted our confidence in HHVM there were a couple hacks we had to put in place to get it to work. We didn’t want teed HTTP requests generating writes in our backend services. To that end we wrote read-only mysql, memcached and redis interfaces to prevent writes. As a result however, we weren’t yet confident that HHVM would write data correctly, or write the correct data. Employee Only Traffic In order to gain confidence in that area we configured our bespoke endpoints to send all requests to the HHVM cluster if the user requesting the page was an employee. This put almost no load on the cluster, but allowed us to ensure that HHVM could communicate with backend services correctly. At this point we encountered some more incompatibilities with the memcached extension. We noticed that our API rate limiter was never able to find keys to decrement. This was caused by the decrement function being implemented incorrectly in the HHVM extension. In the process of debugging this we noticed that memcached was always returning false for every request HHVM made to it. This turned out to be a bug in the client-side hashing function present in HHVM. What we learned from this is that while the HHVM runtime is rock-solid, a lot of the included extensions aren’t. Facebook thoughtfully wrote a lot of the extensions specifically for the open source release of HHVM. However, many of them are not used internally because Facebook has their own clients for memcached and MySQL , and as a result have not seen nearly as much production traffic as the rest of the runtime. This is important to keep in mind when working with HHVM. We expect this situation will improve as more and more teams test it out and contribute patches back to the project, as we at Etsy will continue to do. After resolving these issues it came time to slowly move production traffic from the PHP API cluster to the HHVM API cluster. Slow Ramp Up As we began the slow ramp in production we noticed some strange timestamps in the logs: [23/janv./2015:22:40:32 +0000] We even saw timestamps that looked like this: [23/ 1月/2015:23:37:56] At first we thought we had encountered a bug with HHVM’s logging system. As we investigated we realized the problem was more fundamental than that. At Etsy we use the PHP function setlocale() to assist in localization. During a request, after we load a user we call setlocale() to set their locale preferences accordingly. The PHP function setlocale() is implemented using the system call setlocale(3) . This system call is process-wide, affecting all the threads in a process. Most PHP SAPI s are implemented such that each request is handled by exactly one process, with many processes simultaneously handling many requests. HHVM is a threaded SAPI. HHVM runs as a single process with multiple threads where each thread is only handling exactly one request. When you call setlocale(3) in this context it affects the locale for all threads in that process. As a result, requests can come in and trample the locales set by other requests as illustrated in this animation. We have submitted a pull request re-implementing the PHP setlocale() function using thread-local equivalents. When migrating to HHVM it’s important to remember that HHVM is threaded, and different from most other SAPIs in common use. Do an audit of extensions you’re including and ensure that none of them cause side effects that could affect the state of other threads. Release! After rolling HHVM out to just the internal API cluster we saw a noticeable improvement in performance across several endpoints. It’s Not Just Speed In the process of experimenting with HHVM we discovered a few under-documented features that are useful when running large PHP deployments. Warming up HHVM The HHVM team recommends that you warm up your HHVM process before having it serve production traffic: “The cache locality of the JITted code is very important, and you want all your important endpoints code to be located close to each other in memory. The best way to accomplish this is to pick your most important requests (say 5) and cycle through them all serially until you’ve done them all 12 times. “ They show this being accomplished with a simple bash script paired with curl. There is a more robust method in the form of “warmup documents”. You specify a warmup document in an HDF file like this: cmd = 1\r\nurl = /var/etsy/current/bin/hhvm/warmup.php // script to execute\r\nremote_host = 127.0.0.1\r\nremote_port = 35100\r\nheaders { // headers to pass into HHVM\r\n0 {\r\nname = Accept\r\nvalue = */*\r\n}\r\n1 {\r\nname = Host\r\nvalue = www.etsy.com\r\n}\r\n2 {\r\nname = User-Agent\r\nvalue = Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.46 Safari/535.11\r\n}\r\n} To tell HHVM to execute that warmup document on startup, simply reference it like so: Server {\r\nWarmupRequests {\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n}\r\n} This will execute /var/etsy/current/bin/hhvm/warmup.php between when the HHVM binary is executed and when the process accepts connections. It will only execute it once however, and HHVM will not JIT any code until after the twelfth request. To execute a warmup document 12 times simply reference it 12 times from the config file, like so: Server {\r\nWarmupRequests {\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n* = /var/etsy/current/bin/hhvm/warmup.hdf\r\n}\r\n} Profiling HHVM with perf(1) HHVM makes it really easy to profile PHP code. One of the most interesting ways is with Linux’s perf tool. HHVM is a JIT that converts PHP code into machine instructions. Because these instructions, or symbols, are not in the HHVM binary itself, perf cannot automatically translate these symbols into functions names. HHVM creates an interface to aid in this translation. It takes the form of a file in /tmp/ named according to this template: /tmp/perf-<pid of process>.map The first column in the file is the address of the start of that function in memory. The second column is the length of the function in memory. And the third column is the function to print in perf. Perf looks up processes it has recorded by their pid in /tmp to find and load these files. (The pid map file needs to be owned by the user running perf report, regardless of the permissions set on the file.) If you run sudo perf record -p <pid> -ag -e instructions -o /tmp/perf.data -- sleep 20 perf will record all of the symbols being executed for the given pid and the amount of CPU time that symbol was responsible for on the CPU over a period of 20 seconds. It stores that data in /tmp/perf.data. Once you have gathered data from perf with a command such as the above, you can display that data interactively in the terminal using `perf report`. Click to embiggen This show us a list of the most expensive functions (in terms of instructions executed on the CPU) being executed. Functions prefixed with HPHP:: are functions built into the language runtime. For example HPHP::f_sort accounts for all calls the PHP code makes to sort(). Functions prefixed with PHP:: are programmer-defined PHP functions. Here we can see that 36% of all CPU time occurred in Api_Handler::respond(), for example. Using perf() to profile PHP code is powerful on its own, but having the ability to jump from a PHP function into an HPHP function allows you to see what parts of your codebase HHVM doesn’t handle efficiently. Using this process we were able to determine that sort() calls were slow when enable_zend_sorting was enabled. After patching it to be more efficient, we realized a significant CPU and performance win: This change resulted in an additional increase in throughput across our API cluster as well as improved response times. HHVM Interactive Debugger HHVM provides an interactive debugger called “hphpd”. hphpd works similarly to gdb: it is a command line based interactive debugger. $ hhvm -c /etc/php.d/etsy.ini -m debug bin/test.php\r\nWelcome to HipHop Debugger!\r\nType \"help\" or \"?\" for a complete list of commands. Program bin/test.php loaded. Type '[r]un' or '[c]ontinue' to go. Here we set a breakpoint on a function: hphpd> b Shutdown::registerApiFunctions()\r\nBreakpoint 1 set upon entering Shutdown::registerApiFunctions()\r\nBut wont break until class Shutdown has been loaded.\r\nCommence execution until we encounter a breakpoint:\r\nhphpd> continue\r\nBreakpoint 1 reached at Shutdown::registerApiFunctions() on line 101 of /home/dmiller/development/Etsyweb/phplib/Shutdown.php\r\n100     public static function registerApiFunctions() {\r\n101*        self::registerFunction(['Shutdown', 'apiShutdown']);\r\n102     } Step into that function: hphpd> step\r\nBreak at Shutdown::registerFunction() on line 74 of /home/dmiller/development/Etsyweb/phplib/Shutdown.php\r\n73     public static function registerFunction() {\r\n74*        $callback = func_get_args();\r\n75 Step over that function: hphpd> next\r\nBreak at Shutdown::registerFunction() on line 76 of /home/dmiller/development/Etsyweb/phplib/Shutdown.php\r\n75\r\n76*        if (empty($callback)) {\r\n77             $bt = new Dev_Backtrace(); hphpd> next\r\nBreak at Shutdown::registerFunction() on line 82 of /home/dmiller/development/Etsyweb/phplib/Shutdown.php\r\n81         }\r\n82*        if (!is_callable($callback[0])) {\r\n83             $bt = new Dev_Backtrace(); After adding a few lines to your configuration file you can use this debugger on any code that executes in HHVM. Lessons Learned from the Experiment The process of migrating our API cluster to HHVM taught us a lot about HHVM as well as how to better perform such migrations in the future. The ability to clone HTTP traffic and tee it to a read-only test cluster allowed us to gain confidence in HHVM much more quickly than we could have otherwise. While HHVM proved to be rock-solid as a language runtime, extensions proved to be less battle-tested. We frequently encountered bugs and missing features in the MySQL, Memcached and OAuth extensions, among others. Finally it’s important to remember that HHVM is threaded, which can result in a weird interplay between the runtime and system calls. The resulting behavior can be very surprising. HHVM met our expectations. We were able to realize a greater throughput on our API cluster, as well as improved performance. Buying fewer servers also means less waste and less power consumption in our data centers, which is important to Etsy as a Benefit Corporation . You can follow Dan on Twitter @jazzdan . Special thanks Sara Golemon, Paul Tarjan and Josh Watzman at Facebook. Extra special thanks to Keyur Govande and Adam Saponara at Etsy. Posted by Dan Miller on April 6, 2015 Category: Uncategorized", "date": "2015-04-6,"},
{"website": "Etsy", "title": "Q1 2015 Site Performance Report", "author": ["Kristyn Reith"], "link": "https://codeascraft.com/2015/03/30/q1-2015-site-performance-report/", "abstract": "Q1 2015 Site Performance Report Posted by Kristyn Reith on March 30, 2015 Spring has finally arrived, which means it’s time to share our Site Performance Report for the first quarter of 2015. Like last quarter, in this report, we’ve taken data from across an entire week in March and are comparing it with data from an entire week in December. Since we are constantly trying to improve our data reporting, we will be shaking things up with our methodology for the Q2 report. For backend performance, we plan to randomly sample the data throughout the quarter so it is more statistically sound and a more accurate representation of the full period of time. We’ve split up the sections of this report among the Performance team and different members will be authoring each section. Allison McKnight is updating us on the server-side performance section, Natalya Hoota is covering the real user monitoring portion, and as a performance bootcamper, I will have the honor of reporting the synthetic front-end monitoring. Over the last three months, front-end and backend performance have remained relatively stable with some variations to specific pages such as, baseline, home, and profile. Now without further ado, let’s dive into the numbers. Server-Side Performance – from Allison McKnight Let’s take a look at the server-side performance for the quarter. These are times seen by real users (signed-in and signed-out). The baseline page includes code that is used by all of our pages but has no additional content. Check that out! None of our pages got significantly slower (slower by at least 10% of their values from last quarter). We do see a 50 ms speedup in the homepage median and a 30 ms speedup in the baseline 95th percentile. Let’s take a look at what happened. First, we see about a 25 ms speedup in the 95th percentile of the baseline backend time. The baseline page has three components: loading bootstrap/web.php, a bootstrap file for all web requests; creating a controller to render the page; and rendering the baseline page from the controller. We use StatsD , a tool that aggregates data and records it in Graphite, to graph each step we take to load the baseline page. Since we have a timer for each step, I was able to drill down to see that the upper bound for the bootstrap/web step dropped significantly in the end of January: We haven’t been able to pin down the change that caused this speedup. The bootstrap file performs a number of tasks to set up infrastructure – for example, setting up logging and security checks – and it seems likely that an optimization to one of these processes resulted in a faster bootstrap. We also see a 50 ms drop in the homepage median backend time. This improvement is from rolling out HHVM for our internal API traffic. HHVM is a virtual machine developed at Facebook to run PHP and Hack , a PHP-like language developed by Facebook and designed to give PHP programmers access to language features that are unavailable in PHP. HHVM uses just-in-time (JIT) compilation to compile PHP and Hack into bytecode during runtime, allowing for optimizations such as code caching. Both HHVM and Hack are open-source. This quarter we started sending all of our internal API v3 requests to six servers running HHVM and saw some pretty sweet performance wins. Overall CPU usage in our API cluster dropped by about 20% as the majority of our API v3 traffic was directed to the HHVM boxes; we expect we’ll see an even larger speedup when we move the rest of our API traffic to HHVM. Time spent in API endpoints dropped. Most notably, we saw a speedup in the search listings endpoint (200 ms faster on the median and 100 ms faster on the 90th percentile) and the fetch listing endpoint (100 ms faster on the median and 50 ms faster on the 90th percentile). Since these endpoints are used mainly in our native apps, mobile users will have seen a speed boost when searching and viewing listings. Desktop users also saw some benefits: namely, the median homepage backend time for signed-in users, whose homepages we personalize with listings that they might like, dropped by 95 ms. This is what caused the 50 ms drop in the median backend time for all homepage views this quarter. The transition to using HHVM for our internal API requests was headed by Dan Miller on our Core Platform team. At Etsy, we like to celebrate the work done on different teams to improve Performance by naming a Performance Hero when exciting improvements are made. Dan was named the first Performance Hero of 2015 for his work on HHVM. Go, Dan! To learn more about how we use HHVM at Etsy and the benefits that it’s brought us, you can see the slides from his talk HHVM at Etsy , which he gave at PHP UK 2015 Conference. A Code as Craft post about HHVM at Etsy will appear from him in the future, so keep checking back! Synthetic Front-End Performance – from Kristyn Reith Below is the synthetic front-end performance data for Q1. For synthetic testing, a third party simulates actions taken by a user and then continuously monitors these actions to generate performance metrics. For this report, the data was collected by Catchpoint, which runs tests every ten minutes on IE9 in New York, London, Chicago, Seattle and Miami. Catchpoint defines the webpage response metric as the time it takes from the request being issued until the last byte of the final element of the page is received. These numbers are all medians and here is the data for the week of March 8-15th 2015 compared to the week of December 15-22nd 2014. To calculate error ranges for our median values, we use Catchpoint’s standard deviation. Based on these standard deviations, the only statistically significant performance regression we saw was for the homepage for both the start render and webpage response times. Looking further into this, we dug into the homepage’s waterfall charts and discovered that Catchpoint’s “response time” metrics are including page elements that load asynchronously. The webpage response time should not account for elements loaded after the document is considered complete. Therefore, this regression is actually no more than a measurement tooling problem and not representative of a real slowdown. Based on these standard deviations, we saw several improvements. The most noteworthy of these are the start render and webpage response times for the listing page. After investigating potential causes for this performance win, we discovered that this was no more than an error in data collection on our end. The Etsy shop that owns the listing page that we use to collect data in Catchpoint had been put on vacation mode, which temporarily puts the shop “on hold” and hides listings, prior to us pulling the Q1 data. While on vacation mode, the listing for the listing page in question expired on March 7th. So all the data pulled for the week we measured in March does not represent the same version of the listing page that was measured in our previous report, since the expired listing page includes additional suggested items. To avoid having an error like this occur in the future, the performance team will be creating a new shop with a collection of listings, specifically designated for performance testing. Although the synthetic data for this quarter may seem to suggest that there were major changes, it turned out that the biggest of these were merely errors in our data collection. As we note in the conclusion, we’re going to be overhauling a number of ways we gather data for these reports. Real User Front-End Performance – from Natalya Hoota As in our past reports, we are using real user monitoring (RUM) data from mPulse. Real user data, as opposed to synthetic measurements, is sent from users’ browsers in real time. It does look like the overall trend is global increase in page load time. After a few examinations it appears that most of the slowdown is coming from the front end. A few things to note here – the difference is not significant (less than 10%) with an exception for homepage and profile page. Homepage load time was affected slightly more than the rest due to two experiments with real time recommendations and page content grouping, both of which are currently ramped down. Profile page showed no outstanding increase in time for the median values; as for the long tail (95 percentile), however, there was a greater change for the worse. Another interesting nugget that we found was that devices send a different set of metrics to mPulse based on whether their browsers support navigation timing. The navigation timing API was proposed by W3C on 2012, leading to major browsers gradually rolling in support for them. Notably, Apple added it to Safari last July , allowing RUM vendors better insight into users experience. For our data analysis it means the following: we should examine each navigation and resource timing metrics separately, since the underlying data sets are not identical. In order to make a definitive conclusion, we would need to test statistical validity of that data. In the next quarter we are hoping to incorporate changes that will include better precision in our data collection, analysis and visualization. Conclusion – from Kristyn Reith The first quarter of 2015 has included some exciting infrastructure changes. We’ve already begun to see the benefits that have resulted from the introduction of HHVM and we are looking forward to seeing how this continues to impact performance as we transition the rest of our API traffic over. Keeping with the spirit of exciting changes, and acknowledging the data collection issues we’ve discovered, we will be rolling out a whole new approach to this report next quarter. We will partner with our data engineering team to revamp the way we collect our backend data for better statistical analysis. We will also experiment with different methods of evaluation and visualization to better-represent the speed findings in the data. We’ve also submitted a feature request to Catchpoint to add an alert that’s only triggered if bytes *before* document complete have regressed. With these changes, we look forward to bringing you a more accurate representation of the data across the quarter, so please check back with us in Q2. Posted by Kristyn Reith on March 30, 2015 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2015-03-30,"},
{"website": "Etsy", "title": "Re-Introducing Deployinator, now as a gem!", "author": ["Jayson Paul"], "link": "https://codeascraft.com/2015/02/20/re-introducing-deployinator-now-as-a-gem/", "abstract": "Re-Introducing Deployinator, now as a gem! Posted by Jayson Paul on February 20, 2015 If you aren’t familiar with Deployinator, it’s a tool we wrote to deploy code to Etsy.com . We deploy code about 40 times per day. This allows us to push smaller changes we are confident about and experiment at a fast rate. Deployinator does a lot of heavy lifting for us. This includes updating source repositories on build machines, minifying/building javascript and css dependencies, kicking off automated tests and updating our staging environment before launching live. But Deployinator doesn’t just deploy Etsy.com, it also manages deploys for a myriad of internal tools, such as our Virtual Machine provisioning system, and can even deploy itself. Within Deployinator, we call each of these independent deployments “stacks”. Deployinator includes a number of helper modules that make writing deployment stacks easy. Our current modules provide helpers for versioning, git operations, and for utilizing DSH . Deployinator works so well for us we thought it best to share. Four years ago we open sourced deployinator for OSCON . At the time we created a new project on github with the Etsy related code removed and forked it internally. This diverged and was difficult to maintain for a few reasons. The original public release of Deployinator mixed core and stack related code, creating a tightly coupled codebase; Configuration and code that was committed to our internal fork could not be pushed to public github. Naturally, every internal commit that included private data invariably included changes to the Deployinator core as well. Untangling the public and private bits made merging back into the public fork difficult and over time impossible. If (for educational reasons) you are interested the old code it is still available here . Today we’d like to announce our re-release of Deployinator as an open source ruby gem ( rubygems link ).  We built this release with open-source in mind from the start by changing our internal deployinator repository (renamed to DeployinatorStacks for clarity) to include an empty gem created on our public github. Each piece of core deployinator code was then individually untangled and moved into the gem. Since we now depend on the same public Deployinator core we should no longer have problems keeping everything in sync. While in the process of migrating Deployinator core into the gem it became apparent that we needed a way to hook into common functionality to extend it for our specific implementations. For example, we use graphite to record duration of deploys and the steps within. An example of some of the steps we track are template compilations, javascript and css asset building and rsync times. Since the methods to complete these steps are entirely within the gem, implementing a plugin architecture allows everyone to extend core gem functionality without needing a pull request merged. Our README explains how to create deployment stacks using the gem and includes an example to help you get up and running. (Example of how deployinator looks with many stacks) Major Changes Deployinator now comes bundled with a simple service to tail running deploys logs to the front end. This replaces some overly complicated streaming middleware that was known to have problems. Deploys are now separate unix processes with descriptive proc titles. Before they were hard to discern requests running under your web server. The combination of these two things decouples deploys from the web request allowing uninterrupted flow in the case of network failures or accidental browser closings. Having separate processes also enables operators to monitor and manipulate deploys using traditional command line unix tools like ps and kill. This gem release also introduces some helpful namespacing. This means we’re doing the right thing now.  In the previous open source release all helper and stack methods were mixed into every deploy stack and view. This caused name collisions and made it hard to share code between deployment stacks. Now helpers are only mixed in when needed and stacks are actual classes extending from a base class. We think this new release makes Deployinator more intuitive to use and contribute to and encourage everyone interested to try out the new gem. Please submit feedback as github issues and pull requests. The new code is available on our github . Deployinator is at the core of Etsy’s development and deployment model and how it keeps these fast. Bringing you this release embodies our generosity of spirit in engineering principle. If this sort of work interests you, our team is hiring . Posted by Jayson Paul on February 20, 2015 Category: engineering , infrastructure Tags: continuous deployment , deployinator , tools Related Posts Posted by Sasha Friedenberg on 15 May, 2017 How Etsy Ships Apps Posted by Rasmus Lerdorf on 01 Jul, 2013 Atomic deploys at Etsy Posted by Laura Beth Denker on 12 Mar, 2012 Scaling CI at Etsy: Divide and Concur, Revisited", "date": "2015-02-20,"},
{"website": "Etsy", "title": "Sahale: Visualizing Cascading Workflows at Etsy", "author": ["Eli Reisman"], "link": "https://codeascraft.com/2015/02/11/sahale-visualizing-cascading-workflows-at-etsy/", "abstract": "Sahale: Visualizing Cascading Workflows at Etsy Posted by Eli Reisman on February 11, 2015 The Problem If you know anything about Etsy engineering culture, you know we like to measure things. Frequent readers of Code As Craft have become familiar with Etsy mantras like “If it moves, graph it.” When I arrived at Etsy, our Hadoop infrastructure, like most systems at Etsy, was well instrumented at the operational level via Ganglia and the standard suite of exported Hadoop metrics, but was still quite opaque to our end users, the data analysts. Our visibility problem had to do with a tooling decision made by most Hadoop shops: to abandon tedious and counterintuitive “raw MapReduce” coding in favor of a modern Domain Specific Language like Pig , Hive , or Scalding . At Etsy, DSL programming on Hadoop has been a huge win. A DSL enables developers to construct complex Hadoop workflows in a fraction of the time it takes to write and schedule a DAG of individual MapReduce jobs. DSLs abstract away the complexities of MapReduce and allow new developers to ramp up on Hadoop quickly. Etsy’s DSL of choice, Scalding , is a Scala-based wrapper for the Cascading framework. It features a vibrant community, an expressive syntax, and a growing number of large companies like Twitter scaling it out in production. However, DSLs on Hadoop share a couple of drawbacks that must be addressed with additional tooling. The one that concerned users at Etsy the most was a lack of runtime visibility into the composition and behavior of their Scalding jobs. The core of the issue is this: Hadoop only understands submitted work at the granularity of individual MapReduce jobs. The JobTracker (or ResourceManager on Hadoop2) is blissfully unaware of the relationships among tasks executed on the cluster, and its web interface reflects this ignorance: Figure 1: YARN Resource Manager. It’s 4am – do you know where your workflow is? A bit dense, isn’t it? This is fine if you are running one raw MapReduce job at a time, but the Cascading framework compiles each workflow down into a set of interdependent MapReduce jobs. These jobs are submitted to the Hadoop cluster asynchronously, in parallel, and ordered only by data dependencies between them. The result: Etsy users tracking a Cascading workflow via the JobTracker/ResourceManager interface did not have a clear idea how their source code became a Cascading workflow plan, how their Cascading workflows mapped to jobs submitted to Hadoop, which parts of the workflow were currently executing, or how to determine what happened when things went badly. This situation also meant any attempt to educate users about optimizing workflows, best practices, or MapReduce itself was relegated to a whiteboard, Confluence doc, or (worse) an IRC channel, soon to be forgotten. What Etsy needed was a user-facing tool for tracking cluster activity at the granularity of Cascading workflows, not MapReduce jobs – a tool that would make core concepts tangible and allow for proactive, independent assessment of workflow behavior at runtime. Etsy’s Data Platform team decided to develop some internal tooling to solve these problems. The guidelines we used were: Developers should be empowered to develop an intuitive understanding of their Cascading jobs through workflow visualizations, and charted job metrics. Developers should be able to evaluate for themselves the resource impact their workflows have on the cluster. Metrics presented should be relevant to end users. Developers should be able to easily identify the particular MapReduce job in a Cascading workflow that causes a failure during execution. Developers should be able to quickly locate and access Hadoop job logs that are most relevant to a workflow failure. Developers should be able to visualize both running and completed workflows. Developers should have access to a historical view of completed executions to map changes in job code to runtime behavior over many dev cycles. Fast forward to now: I am pleased to announce Etsy’s open-source release of Sahale , a tool for visualizing Cascading workflows, to you, the Hadooping public! Design Overview Let’s take a look at how it works under the hood. The design we arrived at involves several discrete components which are deployed separately, as illustrated below: Figure 2: Design overview. The components of the tool are: Sahale, a NodeJS server that collects incoming JSON data dispatched by clients (Cascading jobs) and updates database tables accordingly. The same server also exposes the browser-based user interface for end users. FlowTracker, a small Scala project packaged with Sahale, that produces a JAR file you can include in your Cascading jobs to track and report metrics from running jobs. A MySQL database available to the Sahale app. The workflow is as follows: A User launches a Cascading workflow, and a FlowTracker is instantiated. The FlowTracker begins to dispatch periodic job metric updates to Sahale. Sahale commits incoming metrics to the database tables. The user points her browser to the Sahale web app. About FlowTracker That’s all well and good, but how does the FlowTracker capture workflow metrics at runtime? First, some background. In the Cascading framework, each Cascade is a collection of one or more Flows. Each Flow is compiled by Cascading into one or more MapReduce jobs, which the user’s client process submits to the Hadoop cluster. With a reference to the running Flow, we can track various metrics about the Hadoop jobs the Cascading workflow submits. Sahale attaches a FlowTracker instance to each Flow in the Cascade. The FlowTracker can capture its reference to the Flow in a variety of ways depending on your preferred Cascading DSL. By way of an example, let’s take a look at Sahale’s TrackedJob class. Users of the Scalding DSL need only inherit TrackedJob to visualize their own Scalding workflows with Sahale. In the TrackedJob class, Scalding’s Job.run method is overridden to capture a reference to the Flow at runtime, and a FlowTracker is launched in a background thread to handle the tracking: /**\r\n * Your jobs should inherit from this class to inject job tracking functionality.\r\n */\r\nclass TrackedJob(args: Args) extends com.twitter.scalding.Job(args) {\r\n  @transient private val done = new AtomicBoolean(false)\r\n\r\noverride def run(implicit mode: Mode) = {\r\n    mode match {\r\n      // only track Hadoop cluster jobs marked \"--track-job\"\r\n      case Hdfs(_, _) => if (args.boolean(\"track-job\")) runTrackedJob else super.run\r\n      case _ => super.run\r\n    }\r\n  }\r\n\r\n  def runTrackedJob(implicit mode: Mode) = {\r\n    try {\r\n      val flow = buildFlow\r\n      trackThisFlow(flow)\r\n      flow.complete\r\n      flow.getFlowStats.isSuccessful // return Boolean\r\n    } catch {\r\n      case t: Throwable => throw t\r\n    } finally {\r\n      // ensure all threads are cleaned up before we propagate exceptions or complete the run.\r\n      done.set(true)\r\n      Thread.sleep(100)\r\n    }\r\n  }\r\n\r\n  private def trackThisFlow(f: Flow[_]): Unit = { (new Thread(new FlowTracker(f, done))).start }\r\n} The TrackedJob class is specific to the Scalding DSL, but the pattern is easy to extend. We have used the tool internally to track Cascading jobs generated by several different DSLs. Sahale selects a mix of aggregated and fine-grained metrics from the Cascading Flow and the underlying Hadoop MapReduce jobs it manages. The FlowTracker also takes advantage of the Cascading client’s caching of recently-polled job data whenever possible. This approach is simple, and has avoided incurring prohibitive data transfer costs or latency, even when many tracked jobs are executing at once. This has proven critical at Etsy where it is common to run many 10’s or 100’s of jobs simultaneously on the same Hadoop cluster. The data model is also easily extendable if additional metrics are desired. Support for a larger range of default Hadoop counters is forthcoming. About Sahale Figure 3: Workflow overview page. The Sahale web app’s home page consists of two tables. The first enumerates running Cascading workflows. The second displays all recently completed workflows. Each table entry includes a workflow’s name, the submitting user, start time, job duration, the number of MapReduce jobs in the workflow, workflow status, and workflow progress. The navigation bar at the top of the page provides access to the help page to orient new users, and a search feature to expose the execution history for any matching workflows. Each named workflow links to a detail page that exposes a richer set of metrics: Figure 4: Workflow details page, observing a job in progress. The detail page exposes the workflow structure and job metrics for a single run of a single Cascading workflow. As before, the navigation bar provides links to the help page, back to the overview page, and to a history view for all recent runs of the workflow. The detail page consists of five panels: A table-row style record exposing summary stats similar to the overview page. A dual series area chart (left-top panel) displaying various per-step Cascading metrics. A stacked bar chart (right-top panel) depicting per-step running times. A graph view of the Cascading workflow’s job plan. Vertices represent individual MapReduce jobs, edges are data dependencies. Job status is exposed via color. A detailed set of job metrics for a single workflow step, including progress indicators, source and sink field schemas, and handy links to Hadoop logs. Users can leverage these displays to access a lot of actionable information as a workflow progresses, empowering them to quickly answer questions like: Which stages of my workflow are slowest and could benefit from optimization? Which are the most expensive stages of my job in terms of time, cluster resources, volume of data processed, etc.? Which stages of my workflow are benefitting from data locality, and which are not? How much of my workflow can be submitted in parallel, and how much must be serialized due to data dependencies. Could I restructure my source code to help Cascading generate a simpler or more parallelizable workflow plan? Which parts? In the event of a workflow failure, even novice users can identify the offending job stage or stages as easily as locating the red nodes in the graph view. Selecting failed nodes provides easy access to the relevant Hadoop job logs, and the Sources/SInks tab makes it easy to map a single failed Hadoop job back to the user’s source code. Before Sahale this was a frequent pain point for Hadoop users at Etsy. Figure 5: A failed workflow. The offending MapReduce job is easy to identify and drill down into. If a user notices a performance regression during iterative workflow development, the Job History link in the navigation bar will expose a comparative view of all recent runs of the same workflow: Figure 6: History page, including aggregated per-workflow metrics for easy comparisons. Here, users can view a set of bar graphs comparing various aggregated metrics from historical runs of the same workflow over time. As in the workflow graphs, color is used to indicate the final status of each charted run or the magnitude of the I/O involved with a particular run. Hovering on any bar in a chart displays a popup with additional information. Clicking a bar takes users to the detail page for that run, where they can drill down into the fine-grained metrics. The historical view makes mapping changes in source code back to changes in workflow performance a cinch. Sahale at Etsy After running Sahale at Etsy for a year, we’ve seen some exciting changes in the way our users interact with our BigData stack. One of the most gratifying for me is the way new users can ramp up quickly and gain confidence self-assessing their workflow’s performance characteristics and storage impact on the cluster. Here’s one typical example timeline, with workflow and user name redacted to protect the excellent: Around January 4th, one of our new users got a workflow up and running. By January 16th, this user had simplified the work needed to arrive at the desired result, cutting the size of the workflow nearly in half. By removing an unneeded aggregation step, the user further optimized the workflow down to a tight 3 stages by Feb. 9th. All of this occurred without extensive code reviews or expert intervention, just some Q&A in our internal IRC channel. Viewing the graph visualizations for this workflow illustrates its evolution across the timeline much better: Figure 7a: Before (Jan 4th) Figure 7b: During (Jan. 16th) Figure 7c: After (Feb. 9th) It’s one thing for experienced analysts to recommend that new users try to filter unneeded data as early in the workflow as possible, or to minimize the number of aggregation steps in a workflow. It’s another thing for our users to intuitively reason about these best practices themselves. I believe Sahale has been a great resource for bridging that gap. Conclusion Sahale, for me, represents one of many efforts in our company-wide initiative to democratize Etsy’s data. Visualizing workflows at runtime has enabled our developers to iterate faster and with greater confidence in the quality of their results. It has also reduced the time my team spends determining if a workflow is production-ready before deployment. By open sourcing the tool, my hope is that Sahale can offer the same benefits to the rest of the Big Data community. Future plans include richer workflow visualizations, more charts, additional metrics/Hadoop counter collection on the client side, more advanced mappings from workflow to users’ source code (for users of Cascading 2.6+), and out-of-the-box support for more Cascading DSLs. Thanks for reading! As a parting gift, here’s a screenshot of one of our largest workflows in progress: Figure 8: Etsy runs more Hadoop jobs by 7am than most companies do all day. Posted by Eli Reisman on February 11, 2015 Category: data , databases , engineering , infrastructure , monitoring", "date": "2015-02-11,"},
{"website": "Etsy", "title": "The Art of the Dojo", "author": ["Bucky Schwarz"], "link": "https://codeascraft.com/2015/02/17/the-art-of-the-dojo/", "abstract": "The Art of the Dojo Posted by Bucky Schwarz on February 17, 2015 According to the Wikipedia page I read earlier while waiting in line for a latte, dojo literally means place of the way in Japanese, but in a modern context it’s the gathering spot for students of martial arts. At Etsy and elsewhere, dojos refer to collaborative group coding experiences. “Collaborative group coding experience” is a fancy way of saying “we all take turns writing code together” in service of solving a problem. At the beginning of the dojo we set an achievable goal, usually unrelated to our work at Etsy (but often related to a useful skill), and we try to solve that goal in about two hours. What that means is a group of us (usually around five at any one time) goes into a conference room with a TV on the wall, plugs one of our laptops into the TV, and each of us takes turns writing code to solve a problem. We literally take turns sitting at the keyboard writing code. We keep a strict three minute timer at the keyboard; after three minutes are up, the person at the keyboard has to get up and let another person use the keyboard. We pick an editor at least one person knows and stick with it–invariably someone will use an editor that isn’t their first choice and that’s fine. I often end up organizing the dojos and I’m a sucker for video games, so I usually say, “Hey y’all, let’s make a simple video game using JavaScript and HTML5’s Canvas functionality,” and people say, “kay.” HTML5 games work very well in a short dojo environment because there is instantaneous feedback (good for seeing change happen in a three minute period), there are a variety of challenges to tackle (good for when there are multiple skillsets present), the games we decide to make usually have very simple game mechanics (good for completing the task within a couple of hours), and there is an enormous amount of reward in playing the game you just built from scratch. Some examples of games we’ve successfully done include Pong , Flappy Bird , and Snake . That’s it. Easy peasy lemon squeezy. Belying this simplicity is an enormous amount of value to be derived from the dojo experience. Some of the potential applications and benefits of dojos: Onboarding . Hook a couple of experienced engineers up with a batch of new hires and watch the value flow. The newbies get immediate facetime with institutional members, they get to see how coding is done in the organization, and they get a chance to cut loose with the cohort they got hired with. The veterans get to meet the newest members of the engineering team, they get to impart some good practices, and they get to share knowledge and teach. These kinds of experiences have a tendency to form strong bonds that stick with people for their entire time at the company. It’s like a value conduit. Plus it’s fun. In-House Networking. Most of Etsy’s engineers, PMs, and designers work on teams divided along product or infrastructural lines. This is awesome for incremental improvements to the product and for accumulating domain knowledge, but not so great when you need to branch out to solve problems that exist outside your team. Dojos put engineers, PMs or designers from different teams (or different organizations) together and have them solve a problem together. Having a dojo with people you don’t normally interact with also gives you points of contact all over the company. It’s hard to overstate the value of this–knowing I can go talk to people I’ve dojo-ed with about a problem I’m not sure how to solve makes the anxiety about approaching someone for help evaporate. Knowledge Transfer. Sharing knowledge and techniques gained through experience (particularly encoded knowledge ) is invaluable within an organization. Being on the receiving end of knowledge transfer is kind of like getting power leveled in your career. Some of my bread and butter skills I use every day I learned from watching other people do them and wouldn’t have figured them out on my own for a long, long time. The most exciting part of a dojo is when someone watching the coder shouts “whoa! waitwaitwaitwait, how did you do that!? Show me!” Practice Communicating. Dojos allow for tons of practice at the hardest part of the job: communicating effectively. Having a brilliant idea is useless if you can’t communicate it to anyone. Dojo-ing helps hone communication skills because for the majority of the dojo, you’re not on the keyboard. If you want to explain an idea or help someone who’s stuck on the keyboard, you have to be able to communicate with them. Training. I work with a lot of really talented designers who are always looking to improve their skills on the front end (funny how talent and drive to improve seem to correlate). Rather than me sitting over their shoulder telling them what to do, or worse, forcing them to watch me do some coding, a couple of engineers can dojo up with a couple of designers and share techniques and knowledge. This is also applicable across engineering disciplines. We’re currently exploring dojos as a way for us web folks to broaden our iOS and Android skills. The Sheer Fun of it All. I’m a professional software engineer in the sense that I get paid to do what I love, and I take good engineering seriously within the context of engineering and the business it serves. Thinking hard about whether I should name this variable in the past tense or present tense, while important, is exhausting. Kicking back for a couple of hours (often with a beer in hand–but there’s no pressure to drink either) and just hacking together a solution with duct tape and staples, knowing I’m not going to look at it after the dojo is a welcomed break. It reminds me of the chaos and fun of when I first learned to program. It also reinforces the idea that the dojo is less about the code and more about the experience. We play the games after we finish them, sometimes as a competition. Playing the game you just built with the people you built it with is a rewarding and satisfying experience, and also serves to gives a sense of purpose and cohesion to the whole experience. Our process evolved organically. Our first dojo was a small team of us solving an interview question we asked candidates. We realized that in addition to being helpful, the dojo was also really fun. So we went from there. We moved on to typical coding katas before we finally hit our stride on video games. I encourage anyone starting out with dojos to iterate and find a topic or style that works for you. Some other considerations when running a dojo: there will likely be different skill levels involved. Be sure to encourage people who want contribute – designers, PMs, or anyone in between. It’s scary to put yourself out there in front of people whose opinions you respect; a little bit of reassurance will go a long way towards making people comfortable. Negative feedback and cutting down should be actively discouraged as they do nothing in a dojo but make people feel alienated, unwelcome and stupid. Be sure to have a whiteboard; dojos are a great reminder that a lot of the actual problem solving of software engineering happens away from the keyboard. Make sure you have a timer that everyone can see, hear, and use (if it’s a phone, make sure it has enough charge; the screen usually stays on for the whole dojo which drains the battery quick-like). Apply your typical learning to your dojos and see if you can’t find something that works for you. Most importantly, have fun. Posted by Bucky Schwarz on February 17, 2015 Category: engineering", "date": "2015-02-17,"},
{"website": "Etsy", "title": "Transitioning to SCSS at Scale", "author": ["Dan Na"], "link": "https://codeascraft.com/2015/02/02/transitioning-to-scss-at-scale/", "abstract": "Transitioning to SCSS at Scale Posted by Dan Na on February 2, 2015 Naively, CSS appears easy to comprehend — it doesn’t have many programming constructs, and it’s a declarative syntax that describes the appearance of the DOM rather than an executable language. Ironically it’s this lack of functionality that can make CSS difficult to reason about. The inability to add scripting around where and when selectors are executed can make wide-reaching changes to CSS risky. CSS preprocessors introduce advanced features to CSS that the current iteration of the CSS specification does not.  This functionality commonly includes variables, functions, mixins and execution scope, meaning that developers can embed logic that determines how CSS is written and executed.  If correctly applied preprocessors can go a long way towards making CSS more modular and DRY , which in turn result in long-term maintainability wins for a codebase. One of the goals of the Front-end Infrastructure Team for 2014 was to fully transition the CSS codebase at Etsy to SCSS [1] . SCSS is a mature, versatile CSS preprocessor, and Etsy’s designers and developers decided to integrate it into our tech stack.  However, we knew that this effort would be non-trivial with a codebase of our size.  As of October 2014, we had 400,000+ lines of CSS split over 2000+ files. In tandem with a team of designers, the Front-end Infrastructure Team began developing the processes to deploy SCSS support to all development environments and our build pipeline. In this post I’ll cover the logic behind our decisions, the potential pitfalls of a one-time CSS-to-SCSS conversion, and how we set up tooling to optimize for maintainability moving forward. Why SCSS? An Old and New CSS Pipeline Converting Legacy Code SCSS Clean SCSS Diff Going Live Optimizing for Developer Productivity and Code Maintainability Conclusions and Considerations Footnotes and Links Why SCSS? The biggest validation of the potential for SCSS at Etsy was the small team of designers beta-testing it for more than six months before our work began.  Since designers at Etsy actively push code, a single product team led the initial charge to integrate SCSS into their workflow.  They met regularly to discuss what was and was not working for them and began codifying their work into a set of best practices for their own project. It was through the initial work of this product team that the rest of the company began to see the value and viability of introducing a CSS preprocessor.  The input of these designers proved invaluable when the Front-end Infrastructure Team began meeting to hatch a plan for the deployment of SCSS company-wide, and their list of best practices evolved into an SCSS style guide for future front-end development. After evaluating the landscape of CSS preprocessors we decided to move forward with SCSS. SCSS is an extremely popular project with an active developer community , it is feature rich with excellent documentation, and because the SCSS (Sassy CSS) syntax is a superset of CSS3 , developers wouldn’t have to learn a new syntax to start using SCSS immediately. With regards to performance, the Sass team prioritizes the development and feature parity of libsass , a C/C++ port of the Sass engine [2] . We assumed that using libsass via the NodeJS bindings provided by node-sass would enable us to integrate an SCSS compilation step into our builds without sacrificing speed or build times. We were also excited about software released by the larger SCSS community, particularly tools like scss-lint . In order to compile SCSS we knew that a conversion of CSS files to SCSS meant remedying any syntactical bugs within our CSS code base.  Since our existing CSS did not have consistently-applied coding conventions, we took the conversion as an opportunity to create a consistent, enforceable style across our existing CSS.  Coupling this remediation with a well-defined style guide and a robust lint, we could implement tooling to keep our SCSS clean, performant and maintainable moving forward. An Old and New CSS Pipeline Our asset build pipeline is called “builda” (short for “build assets”). It was previously a set of PHP scripts that handled all JS and CSS concatenation/minification/versioning. When using libraries written in other languages (e.g. minification utilities), builda would shell out to those services from PHP. On developer virtual machines (VMs) builda would build CSS dynamically per request, while it would write concatenated, minified and versioned CSS files to disk in production builds. We replaced the CSS component of builda with an SCSS pipeline written in NodeJS. We chose Node for three reasons. First, we had already re-written the JavaScript build component in Node a year ago, so the tooling and strategies for deploying another Node service internally were familiar to us. Second, we’ve found that writing front-end tools in JavaScript opens the door for collaboration and pull requests from developers throughout the organization. Finally, a survey of the front-end software ecosystem reveals a strong preference towards JavaScript, so writing our build tools in JS would allow us to keep a consistent codebase when integrating third-party code. One of our biggest worries in the planning stages was speed. SCSS would add another compilation step to an already extensive build process, and we weren’t willing to lengthen development workflows or build times. Fortunately, we found that by using libsass we could achieve a minimum of 10x faster compilation speeds over the canonical Ruby gem. We were dedicated to ensuring that the SCSS builda service was a seamless upgrade from the old one. We envisioned writing SCSS in your favorite text editor, refreshing the browser, and having the CSS render automatically from a service already running on your VM — just like the previous CSS workflow. In production, the build pipeline would still output properly compiled, minified and versioned CSS to our web servers. Despite a complete rewrite of the CSS service, with a robust conversion process and frequent sanity checking, we were able to replace CSS with SCSS and avoid any disruptions. Workflows were identical to before the rewrite and developers began writing SCSS from day one. Converting Legacy Code In theory, converting CSS to SCSS is as simple as changing the file extension from .css to .scss.  In practice it’s much more complicated. Here’s what’s hard about CSS: It fails quietly.  If selectors are malformed or parameters are written incorrectly (i.e. #0000000 instead of #000000), the browser simply ignores the rule . These errors were a blocker on our conversion because when SCSS is compiled, syntax errors will prevent the file from compiling entirely. But errors were only one part of the problem. What about intentionally malformed selectors in the form of IE-hacks ? Or, what about making changes to legacy CSS in order to conform to new lint rules that we’d impose on our SCSS? For example, we wanted to replace every instance of a CSS color-keyword with its hex value . Our conversion was going to touch a lot of code in a lot of places. Would we break our site by fixing our CSS?  How could we be confident that our changes wouldn’t cause visual regressions? Conventionally there are some patterns to solve this problem. A smaller site might remedy the syntax bugs, iterate every page with a headless browser and create visual diffs for changes. Alternatively, given a certain size it might even be possible to manually regression test each page to make sure the fixes render smoothly. Unfortunately our scale and continuous experimentation infrastructure makes both options impossible, as there are simply far too many different combinations of pages/experiments to test against, all subject to change at a moments notice.  A back of the envelope calculation puts the number of possible variants of Etsy.com at any time at ~1.2M. We needed to clean any incorrect CSS and enforce new lint rules before we performed the SCSS rename, and we needed to confirm that those fixes wouldn’t visually break our site without the option to look at every page. We broke the solution into two distinct steps: the “SCSS Clean” and the “SCSS Diff.” SCSS Clean We evaluated various ways to perform the CSS fixes, initially involving an extensive list of regular expressions to transform incorrect patterns we identified in the code. But that method quickly became untenable as our list of regular expressions was difficult to reason about. Eventually we settled on our final method: using parsers to convert any existing source CSS/SCSS code into Abstract Syntax Trees (AST), which we could then manipulate to transform specific types of nodes.  For the unfamiliar, an AST is a representation of the structure of parsed source code.  We used the Reworkcss CSS parser to generate CSS ASTs and gonzales-pe to generate SCSS ASTs, and wrote a custom adapter between the two formats to streamline our style and syntax changes.  For an example into what a generated AST might look like, here’s a great example from the Reworkcss CSS parser . By parsing our existing CSS/SCSS into ASTs, we could correct errors at a much more granular level by targeting selectors or errors of specific types. Going back to the color-keyword example, this gave us a cleaner way to replace properties that specified color values as color-keywords (“black”) with their equivalent hexadecimal representation (#000000).  By using an AST we could perform the replacement without running the risk of replacing color words in unintended locations (e.g. selectors: “.black-header”) or navigating a jungle of regular expressions. In summary, our cleaning process was: Generate an AST for the existing CSS/SCSS file. Run a script we created to operate over the AST to identify and fix errors/discrepancies on a per-property level. Save the output as .scss. Run the .scss file through the libsass compiler until the successful compilation of all files. Iterate on steps #2-4, including manual remediation efforts on specific files as necessary. SCSS Diff Cleaning our CSS was only half the battle. We also needed a way to confirm that our cleaned CSS wouldn’t break our site in unexpected ways, and to make that determination automatically across thousands of files. Again we turned to ASTs.  ASTs strip away superficial differences in source code to core language constructs.  Thus we could conclude that if two ASTs were deeply equivalent, regardless of superficial differences in their source, they would result in the same rendered CSS. We used our Continuous Integration (Jenkins) server to execute the following process and alert us after each push to production: Run the old builda process with the original, untouched CSS, resulting in the minified, concatenated and versioned CSS that gets deployed to production servers and the live site.  Build an AST from this output. Concurrently to step 1, run the SCSS conversion/clean, generating SCSS files from CSS.  Run these SCSS files through SCSS builda, resulting in minified, concatenated, and versioned CSS from which we could generate an AST. Diff the ASTs from steps 1 and 2. Display and examine the diff.  Iterate on steps 1-3, modifying the cleaning script, the SCSS builda code or manually addressing issues in CSS source until the ASTs are equivalent. With equivalent ASTs we gained confidence that despite touching the thousands of CSS files across Etsy.com, the site would look exactly the same before and after our SCSS conversion.  Integrating the process into CI gave us a quick and safe way to surface the limits of our cleaning/SCSS implementations by using live code but not impacting production. Going Live With sufficient confidence via AST diffing, our next step was to determine how to deploy to production safely. Here was our deployment strategy: Using the original CSS as source, we added the SCSS builda process to our deployment pipeline. On a production push it would take the original CSS, clean it, create SCSS files and then compile them to CSS files in a separate directory on our production servers. We continued to serve all traffic the CSS output of our existing build process and kept the new build flagged off for production users.  This allowed us to safely run a dress rehearsal of the new conversion and build systems during deployments and monitor the system for failures. Once the SCSS builda process ran for several days (with 25-50 pushes per day) without incident, we used our feature flagging infrastructure to ramp up 50% of our production users to use the new SCSS builda output. We monitored graphs for issues. After several days at 50%, we ramped up SCSS builda output to 100% of Etsy.com users and continued to monitor graphs. The final step was to take a few hours to hold production pushes and convert our CSS source to the converted SCSS.  Since our SCSS builda process generated its own cleaned SCSS, transitioning our source was as simple as replacing the contents of our css/ directory with those generated SCSS files. One 1.2M-line deployment later, Etsy.com was running entirely on SCSS source code. Optimizing for Developer Productivity and Code Maintainability We knew that integrating a new technology into the stack such as SCSS would require up-front work on our end with regards to communication, teaching and developer tools. Beyond just the work related to the build pipeline it was important to make sure developers felt confident writing SCSS from day one. Communication and Teaching The style guide and product work by the initial SCSS design team was key in showing the value of adopting SCSS to others throughout the organization. The speed at which new, consistent and beautiful pages could be created with the new style guide was impressive.  We worked with the designers closely on email communication and lunch-and-learn sessions before the official SCSS launch day and crafted documentation within our internal wiki. Developer Tools and Maintainability Beyond syntax differences, there are a couple of core pitfalls/pain points for developers when using SCSS: SCSS is compiled, so syntax errors explode compilation and no CSS hits the page. You can accidentally bloat your CSS by performing seemingly harmless operations (here’s looking at you, @extend ). Nested @import’s within SCSS files can complicate tracing the source files for specific selectors. We found the best way to remedy both was to integrate feedback into development environments. For broken SCSS, a missing/non-compiling CSS file becomes an error message at the top of the document: For maintainability, the integration of a live, in-browser SCSS lint was invaluable: The lint rules defined by our designers help keep our SCSS error-free and consistent and are used within both our pre-deployment test suites and in-browser lint.  Luckily the fantastic open source project scss-lint has a variety of configurable lint rules right out of the box. Lastly, due to nested SCSS file structures, source maps for inspecting file dependencies in browser developer tools were a must. These were straightforward to implement since libsass provides source map support . With the SCSS build processes, live lint, source maps, test suite upgrades and education around the new style guide, our final internal conversion step was pushing environment updates to all developer VMs. Similarly to the SCSS production pipeline the developer environments involved rigorous testing and iteration, and gathering feedback from an opt-in developer test group was key before rolling out the tooling to the entire company. Conclusions and Considerations The key to making any sweeping change within a complex system is building confidence, and transitioning from CSS to SCSS was no different.  We had to be confident that our cleaning process wouldn’t produce SCSS that broke our site, and we had to be confident that we built the right tools to keep our SCSS clean and maintainable moving forward. With proper education, tooling and sanity checks throughout the process, we were able to move Etsy to SCSS with minimal disruption to developer workflows or production users. Footnotes and Links We use SCSS to refer the CSS-syntax version of Sass . For all intents and purposes, SCSS and Sass are interchangeable throughout this post. In order to maintain our old system’s build behavior and prevent redundant CSS imports, we forked libsass to support compass-style import-once behavior . Graphics from Daniel Espeset – Making Maps: The Role of Frontend Infrastructure at Etsy – Fronteers 2014 Presentation ( http://talks.desp.in/fronteers2014/ ) You can follow Dan on Twitter at @dxna . Posted by Dan Na on February 2, 2015 Category: engineering , infrastructure", "date": "2015-02-2,"},
{"website": "Etsy", "title": "Introducing statsd-jvm-profiler: A JVM Profiler for Hadoop", "author": ["Andrew Johnson"], "link": "https://codeascraft.com/2015/01/14/introducing-statsd-jvm-profiler-a-jvm-profiler-for-hadoop/", "abstract": "Introducing statsd-jvm-profiler: A JVM Profiler for Hadoop Posted by Andrew Johnson on January 14, 2015 At Etsy we run thousands of Hadoop jobs over hundreds of terabytes of data every day.  When operating at this scale optimizing jobs is vital: we need to make sure that users get the results they need quickly, while also ensuring we use our cluster’s resources efficiently.  Actually doing that optimizing is the hard part, however.  To make accurate decisions you need measurements , and so we have created statsd-jvm-profiler : a JVM profiler that sends the profiling data to StatsD . Why Create a New Profiler? There are already many profilers for the JVM, including VisualVM , YourKit , and hprof .  Why do we need another one?  Those profilers are all excellent tools, and statsd-jvm-profiler is not intended to entirely supplant them.  Instead, statsd-jvm-profiler, inspired by riemann-jvm-profiler , is designed for a specific use-case: quickly and easily profiling Hadoop jobs. Profiling Hadoop jobs is a complex process.  Each map and reduce task gets a separate JVM, so one job could have hundreds or even thousands of distinct JVMs, running across the many nodes of the Hadoop cluster.  Using frameworks like Scalding complicates it further: one Scalding job will run multiple Hadoop jobs, each with many distinct JVMs.  As such it is not trivial to determine exactly where the code you want to profile is running.  Moreover, storing and transferring the snapshot files produced by some profilers has also been problematic for us due to the large size of the snapshots.  Finally, at Etsy we want our big data stack to be accessible to as many people as possible, and this includes tools for optimizing jobs.  StatsD and Graphite are used extensively throughout Etsy, so by sending data to StatsD, statsd-jvm-profiler enables users to use tools they are already familiar with to explore the profiling data. Writing the Profiler For simplicity, we chose to write statsd-jvm-profiler is a Java agent, which means it runs in the same JVM as the process being instrumented.  The agent code runs before the main method of that process.  Implementing an agent is straightforward: define a class that has a premain method with this signature: package com.etsy.agent;\r\n\r\nimport java.lang.instrument.Instrumentation;\r\n\r\npublic class ExampleAgent {\r\n    public static void premain(String args, Instrumentation instrumentation) {\r\n        // Agent code here\r\n    }\r\n} The agent class should be packaged in a JAR whose manifest specifies the Premain-Class attribute: Premain-Class: com.etsy.agent.ExampleAgent We are using Maven to build statsd-jvm-profiler, so we use the maven-shade-plugin’s ManifestResourceTransformer to set this property, but other build tools have similar facilities. Finally, we used the JVM’s management interface to actually obtain the profiling data.  java.lang.management.ManagementFactory provides a number of MXBeans that expose information about various components of the JVM, including memory usage, the garbage collector, and running threads.  By pushing this data to StatsD, statsd-jvm-profiler removes the need to worry about where the code is running – all the metrics are available in a central location. Issues There were some issues that came up as we developed statsd-jvm-profiler.  First, statsd-jvm-profiler uses a ScheduledExecutorService to periodically run the threads that actually perform the profiling.  However, the default ScheduledExecutorService runs as a non-daemon thread, which means it will keep the JVM alive, even though the main thread may have exited.  This is not ideal for a profiler, as it will keep the JVM alive and continue to report profiling data even though nothing is happening other than the profiler.  Guava has functionality to create a ScheduledExecutorService that will exit when the application is complete , which statsd-jvm-profiler uses to work around this issue. Safepoints are another interesting aspect of profiling the JVM.  A thread is at a safepoint when it is in a known state: all roots for garbage collection are known and all heap contents are consistent.  At a safepoint, a thread’s state can be safely observed or manipulated by other threads.  Garbage collection must occur at a safepoint, but a safepoint is also required to sample the thread state like statsd-jvm-profiler does.  However, the JVM can optimize safepoints out of hot methods.  As such, statsd-jvm-profiler’s sampling can be biased towards cold methods.  This is not a problem unique to statsd-jvm-profiler – any profiler that samples the thread state like statsd-jvm-profiler does would have the same bias.  In practice this bias may not be that meaningful.  It is important to be aware of, but an incomplete view of application performance that still enables you to make improvements is better than no information. How to Use statsd-jvm-profiler statsd-jvm-profiler will profile heap and non-heap memory usage, garbage collection, and the aggregate time spent executing each function.  You will need the statsd-jvm-profiler jar on the host where the JVM you want to profile will run.  Since statsd-jvm-profiler is a Java agent, it is enabled with the -javaagent argument to the JVM.  You are required to provide the hostname and port number for the StatsD instance to which statsd-jvm-profiler should send metrics.  You can also optionally specify a prefix for the metrics emitted by statsd-jvm-profiler as well as filters for the functions to profile. -javaagent:/path/to/statsd-jvm-profiler/statsd-jvm-profiler.jar=server=statsd,port=8125 An example of using statsd-jvm-profiler to profile Scalding jobs is provided with the code. statsd-jvm-profiler will output metrics under the “statsd-jvm-profiler” prefix by default, or you can specify a custom prefix.  Once the application being profiled has finished, all of the data statsd-jvm-profiler produced will be available in whatever backend you are using with StatsD.  What do you do with all that data? Graph it!  We have found flame graphs to be a useful method of visualizing the CPU profiling data, and a script to output data from Graphite into a format suitable for generating a flame graph is included with statsd-jvm-profiler: The memory usage and garbage collection metrics can be visualized directly: Using the Profiler’s Results We’ve already used the data from statsd-jvm-profiler to determine how best to optimize jobs.  For example, we wanted to profile a job after some changes that had made it slower.  The flame graph made it obvious where the job was spending its time.  The wide bars on the left and right of this image are from data serialization/deserialization.  As such we knew that speeding up the job would come from improving the serialization or reducing the amount of data being moved around – not in optimizing the logic of the job itself. We also made a serendipitous discovery while profiling that job: it had been given 3 Gb of heap, but it was not using anywhere near that much.  As such we could reduce its heap size.  Such chance findings are a great advantage of making profiling simple.  You are more likely to to make these chance discoveries if you profile often and make analysis of your profiling data easier.  statsd-jvm-profiler and Graphite solve this problem for us. Get statsd-jvm-profiler Want to try it out yourself?  statsd-jvm-profiler is available on Github now! Posted by Andrew Johnson on January 14, 2015 Category: engineering , infrastructure Tags: hadoop , jvm , profile , profiler , statsd Related Posts Posted by Andrew Johnson on 16 Dec, 2015 Introducing Arbiter: A Utility for Generating Oozie Workflows Posted by Andrew Johnson on 24 Sep, 2015 Managing Hadoop Job Submission to Multiple Clusters Posted by Andrew Johnson on 12 May, 2015 Four Months of statsd-jvm-profiler: A Retrospective", "date": "2015-01-14,"},
{"website": "Etsy", "title": "We Invite Everyone at Etsy to Do an Engineering Rotation: Here’s why", "author": ["Dan Miller"], "link": "https://codeascraft.com/2014/12/22/engineering-rotation/", "abstract": "We Invite Everyone at Etsy to Do an Engineering Rotation: Here’s why Posted by Dan Miller on December 22, 2014 At Etsy, it’s not just engineers who write and deploy code – our designers and product managers regularly do too. And now any Etsy employee can sign up for an “engineering rotation” to get a crash course in how Etsy codes, and ultimately work with an engineer to write and deploy the code that adds their photo to our about page. In the past year, 70 employees have completed engineering rotations. Our engineers have been pushing on day one for a while now, but it took a bit more work to get non-coders prepared to push as soon as their second week. In this post I’ll explain why we started engineering rotations and what an entire rotation entails. What are rotations and why are they important? Since 2010, Etsy employees have participated in “support rotations” every quarter, where they spend about two hours replying to support requests from our members. Even our CEO participates . What started as a way to help our Member Operations team during their busiest time of year has evolved into a program that facilitates cross-team communication, builds company-wide empathy, and provides no shortage of user insights or fun! This got us thinking about starting up engineering rotations, where people outside of the engineering organization spend some time learning how the team works and doing some engineering tasks. Armed with our excellent continuous deployment tools, we put together a program that could have an employee with no technical knowledge deploying code live to the website in three hours. This includes time spent training and the deploy itself. The Engineering Rotation Program The program is split into three parts: homework; an in-person class; then hands-on deployment. The code that participants change and deploy will add their photos to the Etsy about page . It’s a nice visual payoff, and lets new hires publicly declare themselves part of the Etsy team. Before class begins, we assign homework in order to prepare participants for the code change they’ll deploy. We ask them to complete interactive tutorials, including HTML levels 1 and 2 on Code Academy and our in-house Unix Command Line 101. We also ask them to read Marc Cohen’s excellent article “ How the Web Works – In One Easy Lesson ,” followed by a blog post on Etsy.com discussing how the engineering organization deals with outages. These resources help familiarize each participant with the technologies they’ll work with, and introduce them to some of Etsy’s core engineering tenets, such as blameless post-mortems . Next up is a class for all participants. It has five sections. The first section picks up where How The Web Works left off and explains how Etsy works. Then we introduce the standard three-tier architecture and walk through some example requests: viewing, creating, searching for and purchasing a listing. Next we take a deep-dive into database sharding . We explain what it is, why it’s necessary, why we shard by data owner and how we rebalance our shards. We then explain Content Delivery Networks and why we use them. After that, we move away from the hard technical discussion to talk about continuous deployment . We discuss the philosophy behind it, and describe why it’s safe to change the website fifty times per day and how we ensure that each change does exactly what we expect. We wrap up this session by giving an overview of all the engineering teams at Etsy and their responsibilities. At this point we pair each participant with an engineer who will guide them through the process of making and testing the code change, and ultimately pressing the big green Deploy to Production button . These one-on-one sessions can take up to two hours as the pair discuss the different tools that exist at each step – some as simple as IRC or as complex as our Continuous Integration cluster. As the participant begins the process of deploying their code change, they’ll see their name appear atop our dashboards. What have we learned? The benefits of engineering rotations parallel those of our support rotations in many ways. It’s an opportunity for Admin throughout Etsy to work with people they normally wouldn’t, and to learn more about each other personally and professionally. To an outsider, the more technical aspects of Etsy might feel unapproachable – even a bit mysterious – but demystifying them encourages even more collaboration. Here’s what some of the participants of Etsy’s Engineering Rotations have said: “Understanding the work of your colleagues breeds empathy and it’s been great having a better understanding of what working at Etsy means to others.” “The best part about this program is that it pulls back the curtain on how software development works at Etsy. For people who don’t work with code every day, this can appear to be some sort of magic, which it’s not – it’s just a different kind of work, with different kinds of tools. Without this program, we would miss out on a huge opportunity for different groups to empathize with each other, which I think is crucial for a company to feel like a real team.” “The internet goes under the sea in a cable. Whoa.” Participants in both the engineering and support rotations come away with many lessons beyond the curriculum. More than a few times, support rotations have exposed engineers to parts of the site that generate lots of inquiries, and they were able to fix them immediately. And in one engineering rotation, someone pointed out that a lot of IRC tools we’ve built can’t be used by all employees because they don’t have access to our internal code snippet service. So we’re now looking at how we can give everyone that access. I led one session with our International Counsel, and we ended up having a fascinating discussion about the legality of deleting data. That sprang from my explanation of how we do database migrations! But perhaps the biggest thing we’ve learned from the engineering rotations is that everyone involved likes doing them. They get to meet new people, learn new things, and use a tool called “the Deployinator.” What’s not to like? You can follow Dan on Twitter @jazzdan . Posted by Dan Miller on December 22, 2014 Category: people Tags: deploy , deployment , etsy , first day Related Posts Posted by Mike Brittain on 08 Dec, 2010 Tracking Every Release", "date": "2014-12-22,"},
{"website": "Etsy", "title": "Make Performance Part of Your Workflow", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/12/11/make-performance-part-of-your-workflow/", "abstract": "Make Performance Part of Your Workflow Posted by Lara Hogan on December 11, 2014 The following is an excerpt from Chapter 7, “Weighing Aesthetics and Performance”, from Designing for Performance by Lara Callender Hogan (Etsy’s Senior Engineering Manager of Performance), which has just been released by O’Reilly . One way to minimize the operational cost of performance work is to incorporate it into your daily workflow by implementing tools and developing a routine of benchmarking performance. There are a variety of tools mentioned throughout this book that you can incorporate into your daily development workflow: Automate image compression as new images are added to your site. Use an image resizing service and caching by breakpoint so you don’t need to manually create a new image for every screen size. Document copy-and-pasteable design patterns in a style guide for easy reuse. Check your page weight and critical path using browser plug-ins. By making performance work part of your daily routine and automating as much as possible, you’ll be able to minimize the operational costs of this work over time. Your familiarity with tools will increase, the habits you create will allow you to optimize even faster, and you’ll have more time to work on new things and teach others how to do performance right. Your long-term routine should include performance as well. Continually benchmark improvements and any resulting performance gains as part of your project cycle so you can defend the cost of performance work in the future. Find opportunities to repurpose existing design patterns and document them. As your users grow up, so does modern browser technology; routinely check in on your browser-specific stylesheets, hacks, and other outdated techniques to see what you can clean up. All of this work will minimize the operational costs of performance work over time and allow you to find more ways to balance aesthetics and performance. Approach New Designs with a Performance Budget One key to making decisions when weighing aesthetics and page speed is understanding what wiggle room you have. By creating a performance budget early on, you can make performance sacrifices in one area of a page and make up for them in another. In Table 7-3 I’ve illustrated a few measurable performance goals for a site. TABLE 7-3. Example performance budget MEASURE MAXIMUM TOOL NOTES Total page load time 2 seconds WebPagetest, median from five runs on 3G All pages Total page load time 2 seconds Real user monitoring tool, median across geographies All pages Total page weight 800 Kb WebPagetest All pages Speed Index 1,000 WebPagetest using Dulles location in Chrome on 3G All pages except home page Speed Index 600 WebPagetest using Dulles location in Chrome on 3G Home page You can favor aesthetics in one area and favor performance in another by defining your budget up front. That way, it’s not always about making choices that favor page speed; you have an opportunity to favor more complex graphics, for example, if you can find page speed wins elsewhere that keep you within your budget. You can call a few more font weights because you found equivalent savings by removing some image requests. You can negotiate killing a marketing tracking script in order to add a better hero image. By routinely measuring how your site performs against your goals, you can continue to find that balance. To decide on what your performance goals will be, you can conduct a competitive analysis. See how your competitors are performing and make sure your budget is well below their results. You can also use industry standards for your budget: aim for two seconds or less total page time, as you know that’s how fast users expect sites to load. Iterate upon your budget as you start getting better at performance and as industry standards change. Continue to push yourself and your team to make the site even faster. If you have a responsively designed site, determine a budget for your breakpoints as well, like we did in Chapter 5. Your outlined performance goals should always be measureable. Be sure to detail the specific number to beat, the tool you’ll use to measure it, as well as any details of what or whom you’re measuring. Read more about how to measure performance in Chapter 6, and make it easy for anyone on your team to learn about this budget and measure his or her work against it. Designing for Performance by Lara Callender Hogan ISBN 978-1-4919-0251-6 Copyright 2014 O’Reilly Media, Inc. All right reserved. Used with permission. Posted by Lara Hogan on December 11, 2014 Category: performance", "date": "2014-12-11,"},
{"website": "Etsy", "title": "Juggling Multiple Elasticsearch Instances on a Single Host", "author": ["Shikhar Bhushan"], "link": "https://codeascraft.com/2014/12/04/juggling-multiple-elasticsearch-instances-on-a-single-host/", "abstract": "Juggling Multiple Elasticsearch Instances on a Single Host Posted by Shikhar Bhushan on December 4, 2014 Elasticsearch is a distributed search engine built on top of Apache Lucene . At Etsy we use Elasticsearch in a number of different configurations: for Logstash, powering user-facing search on some large indexes, some analytics usage, and many internal applications. Typically, it is assumed that there is a 1:1 relationship between ES instances and machines. This is straightforward and makes sense if your instance requirements line up well with the host – whether physical, virtualized or containerized. We run our clusters on bare metal, and for some of them we have more ES instances than physical hosts. We have good reasons for doing this, and here I’ll share some of the rationale, and the configuration options that we’ve found to be worth tuning. Why? Managing JVMs with large heaps is scary business due to garbage collection run times. 31Gb is the magic threshold above which point you lose the ability to use CompressedOops . In our experience, it is better to have even smaller heaps. Not only do GC pause times stay low, but it’s easier to capture and analyze heap dumps! To get optimal Lucene performance, it is also important to have sufficient RAM available for OS file caching of the index files. At the same time, we are running on server-class hardware with plenty of CPU cores and RAM. Our newest search machines are Ivy Bridge with 20 physical (40 virtual) cores, 128Gb of RAM, and of course SSDs. If we run a single node with a small heap on this hardware we would be wasting both CPU and RAM, because the size of shards such an instance will be able to support will also be smaller. We currently run 4 ES JVMs per machine with 8Gb of heap each. This works out great for us: GC has not been a concern and we are utilizing our hardware effectively. The settings os.processors Elasticsearch uses this setting to configure thread pool and queue sizing. It defaults to Runtime.getRuntime().availableProcessors() . With multiple instances, it is better to spread the CPU resources across them. We set this to ($(nproc) / $nodes_per_host) . So if we are running 4 nodes per host on 40-core machines, each of them will configure thread pools and queues as if there were 10 cores. node.name The default for this setting is to pick a random Marvel comic character at startup. In production, we want something that lets us find the node we want with as little thought and effort as possible. We set this to $hostname-$nodeId (which results in names like “search64-es0” – less whimsical, but far more practical when you’re trying to get to the bottom of an issue). http.port, transport.port If these ports are not specified, ES tries to pick the next available port at startup, starting at a base of 9200 for HTTP and 9300 for its internal transport. We prefer to be explicit and assign ports as $basePort+$nodeIdx from the startup script. This can prevent surprises such as where an instance that you expect to be down is still bound to its port, causing the ‘next available’ one to be higher than expected. cluster.routing.allocation.awareness.attributes A key way to achieve failure tolerance with ES is to use replicas, so that if one host goes down, the affected shards stay available. If you’re running multiple instances on each physical host, it’s entirely possible to automatically allocate all replicas for a shard to the same host, which isn’t going to help you! Thankfully this is avoidable with the use of shard allocation awareness . You can set the hostname as a node attribute on each instance and use that attribute as a factor in shard assignments. ES_JAVA_OPTS=\"$ES_JAVA_OPTS -Des.node.host=${HOSTNAME}\"\r\nES_JAVA_OPTS=\"$ES_JAVA_OPTS -Des.cluster.routing.allocation.awareness.attributes=host\" path.logs Without having a dedicated log directory for each instance, you would end up with multiple JVMs trying to write to the same log files. An alternative, which we rely on, is to prefix the filenames in logging.yml with the property ${node.name} so that each node’s logs are labelled by host and node ID. Another reason to be explicit about node naming! Minimal Viable Configuration Elasticsearch has lots of knobs, and it’s worth trying to minimize configuration . That said, a lot of ES is optimized for cloud environments so we occasionally find things worth adjusting, like allocation and recovery throttling . What do you end up tweaking? You can follow Shikhar on Twitter at @shikhrr Posted by Shikhar Bhushan on December 4, 2014 Category: infrastructure , search", "date": "2014-12-4,"},
{"website": "Etsy", "title": "Personalized Recommendations at Etsy", "author": ["Robert Hall"], "link": "https://codeascraft.com/2014/11/17/personalized-recommendations-at-etsy/", "abstract": "Personalized Recommendations at Etsy Posted by Robert Hall on November 17, 2014 Providing personalized recommendations is important to our online marketplace.  It benefits both buyers and sellers: buyers are shown interesting products that they might not have found on their own, and products get more exposure beyond the seller’s own marketing efforts.  In this post we review some of the methods we use for making recommendations at Etsy.  The MapReduce implementations of all these methods are now included in our open-source machine learning package “ Conjecture ” which was described in a previous post . Computing recommendations basically consists of two stages.  In the first stage we build a model of users’ interests based on a matrix of historic data, for example, their past purchases or their favorite listings (those unfamiliar with matrices and linear algebra see e.g., this  review ).  The models provide vector representations of users and items, and their inner products give an estimate of the level of interest a user will have in the item (higher values denote a greater degree of estimated interest).  In the second stage, we compute recommendations by finding a set of items for each user which approximately maximizes the estimate of the interest. The model of users and items can be also used in other ways, such as finding users with similar interests, items which are similar from a “taste” perspective, items which complement each other and could be purchased together, etc. Matrix Factorization The first stage in producing recommendations is to fit a model of users and items to the data.  At Etsy, we deal with “implicit feedback” data where we observe only the indicators of users’ interactions with items (e.g., favorites or purchases).  This is in contrast to “explicit feedback” where users give ratings (e.g. 3 of 5 stars) to items they’ve experienced. We represent this implicit feedback data as a binary matrix, the elements are ones in the case where the user liked the item (i.e., favorited it) or a zero if they did not.  The zeros do not necessarily indicate that the user is not interested in that item, but only that they have not expressed an interest so far.  This may be due to disinterest or indifference, or due to the user not having seen that item yet while browsing. An implicit feedback dataset in which a set of users have “favorited” various items, note that we do not observe explicit dislikes, but only the presence or absence of favorites The underpinning assumption that matrix factorization models make is that the affinity between a user and an item is explained by a low-dimensional linear model.  This means that each item and user really corresponds to an unobserved real vector of some small dimension.  The coordinates of the space correspond to latent features of the items (these could be things like: whether the item is clothing, whether it has chevrons, whether the background of the picture is brown etc.), the elements for the user vector describe the users preferences for these features.  We may stack these vectors into matrices, one for users and one for items, then the observed data is in theory generated by taking the product of these two unknown matrices and adding noise: The underpinning low dimensional model from which the observed implicit feedback data is generated, “d” is the dimension of the model. We therefore find a vector representation for each user and each item.  We compute these vectors so that the inner product between a user vector and item vector will approximate the observed value in the implicit feedback matrix (i.e., it will be close to one in the case the user favorited that item and close to zero if they didn’t). The results of fitting a two dimensional model to the above dataset, in this small example the the first discovered features roughly corresponds to whether the item is a shelf or not, and the second to whether it is in a “geometric” style. Since the zeros in the matrix do not necessarily indicate disinterest in the item, we don’t want to force the model to fit to them, since the user may actually be interested in some of those items.  Therefore we find the decomposition which minimizes a weighted error function, where the weights for nonzero entries in the data matrix are higher than those of the zero entries.  This follows a paper which suggested this method.  How to set these weights depends on how sparse the matrix is, and could be found through some form of cross validation . What happens when we optimize the weighted loss function described above, is that the reconstructed matrix (the product of the two factors) will often have positive elements where the input matrix has zeros, since we don’t force the model to fit to these as well as to the non-zeros.  These are the items which the user may be interested in but has not seen yet.  The reason this happens is that in order for the model to fit well, users who have shown interest in overlapping sets of items will have similar vectors, and likewise for items.  Therefore the unexplored items which are liked by other users with similar interests will often have a high value in the reconstructed matrix. Alternating Least Squares To optimize the model, we alternate between computing item matrix and user matrix, and at each stage we minimize the weighted squared error, holding the other matrix fixed (hence the name “alternating least squares”).  At each stage, we can compute the exact minimizer of the weighted square error, since an analytic solution is available.  This means that each iteration is guaranteed not to increase the total error, and to decrease it unless the two matrices already constitute a local minimum of the error function.  Therefore the entire procedure gradually decreases the error until a local minimum is reached.  The quality of these minima can vary, so it may be a reasonable idea to repeat the procedure and select the best one, although we do not do this.  A demo of this method in R is available here . This computation lends itself very naturally to implementation in MapReduce, since e.g., when updating a vector for a user, all that is needed are the vectors for the items which he has interacted with, and the small square matrix formed by multiplying the items matrix by its own transpose.  This way the computation for each user typically can be done even with limited amounts of memory available, and each user may be updated in parallel.  Likewise for updating items.  There are some users which favorite huge numbers of items and likewise items favorited by many users, and those computations require more memory.  In these cases we can sub-sample the input matrix, either by filtering out these items, or taking only the most recent favorites for each user. After we are satisfied with the model, we can continue to update it as we observe more information, by repeating a few steps of the alternating least squares every night, as more items, users, and favorites come online.  New items and users can be folded into the model easily, so long as there are sufficiently many interactions between them and existing users and items in the model respectively.  Productionizable MapReduce code for this method is available here . Stochastic SVD The alternating least squares described above gives us an easy way to factorize the matrix of user preferences in MapReduce. However, this technique has the disadvantage of requiring several iterations, sometimes taking a long time to converge to a quality solution. An attractive alternative is the Stochastic SVD .  This is a recent method which approximates the well-known Singular Value Decomposition of a large matrix, and which admits a non iterative MapReduce implementation.  We implement this as a function which can be called from any scalding Hadoop MapReduce job. A fundamental result in linear algebra is that the matrix formed by truncating the singular value decomposition after some number of dimensions is the best approximation to that matrix (in terms of square error) among all matrices of that rank.  However we note that using this method we cannot do the same “weighting” to the error as we did when optimizing via alternating least squares.  Nevertheless for datasets where the zeros do not completely overwhelm the non-zeros then this method is viable.  For example we use it to build a model from the favorites, whereas it fails to provide a useful model from purchases which are much more sparse, and where this weighting is necessary. An advantage of this method is that it produces matrices with a nice orthonormal structure, which makes it easy to construct the vectors for new users on the fly (outside of a nightly recomputation of the whole model), since no matrix inversions are required.  We also use this method to produce vector representations of other lists of items besides those a user favorited, for example treasuries and other user curated lists on Etsy.  This way we may suggest other relevant items for those lists. Producing Recommendations Once we have a model of users and items we use it to build product recommendations.  This is a step which seems to be mostly overlooked in the research literature.  For example, we cannot hope to compute the product of the user and item matrices, and then find the best unexplored items for each user, since this requires time proportional to the product of the number of items and the number of users, both of which are in the hundreds of millions. One research paper suggests using a tree data structure to allow for a non-exhaustive search of the space, by pruning away entire sets of items where the inner products would be too small.  However we observed this method to not work well in practise, possibly due to the curse of dimensionality with the type of models we were using (with hundreds of dimensions). Therefore we use approximate methods to compute the recommendations.  The idea is to first produce a candidate set of items, then to rank them according to the inner products, and take the highest ones.  There are a few ways to produce candidates, for example, the listings from favorite shops of a user, or those textually similar to his existing favorites.  However the main way we use is “locality sensitive hashing” (LSH) where we divide the space of user and item vectors into several hash bins, then take the set of items which are mapped to the same bin as each user. Locality Sensitive Hashing Locality sensitive hashing is a technique used to find approximate nearest neighbors in large datasets.  There are several variants, but we focus on one designed to handle real-valued data and to approximate the nearest neighbors in the Euclidean distance . The idea of the method is to partition the space into a set of hash buckets, so that points which are near to each other in space are likely to fall into the same bucket.  The way we do this is by constructing some number “p” of planes in the space so that they all pass through the origin.  This divides the space up into 2^p convex cones , each of which constitutes a hash bucket. Practically we implement this by representing the planes in terms of their normal vectors.  The side of the plane that a point falls on is then determined by the sign of the inner product between the point and the normal vector (if the planes are random then we have non-zero inner products almost surely, however we could in principle assign those points arbitrarily to one side or the other).  To generate these normal vectors we just need directions uniformly at random in space.  It is well known that draws from an isotropic Gaussian distribution have this property. We number the hash buckets so that the i^th bit of the hash-code is 1 if the inner product between a point and the i^th plane is positive, and 0 otherwise.  This means that each plane is responsible for a bit of the hash code. After we map each point to its respective hash bucket, we can compute approximate nearest neighbors, or equivalently, approximate recommendations, by examining only the vectors in the bucket.  On average the number in each bucket will be 2^{-p} times the total number of points, so using more planes makes the procedure very efficient.  However it also reduces the accuracy of the approximation, since it reduces the chance that nearby points to any target point will be in the same bucket.  Therefore to achieve a good tradeoff between efficiency and quality, we repeat the hashing procedure multiple times, and then combine the outputs.  Finally, to add more control to the computational demands of the procedure, we throw away all the hash bins which are too large to allow efficient computation of the nearest neighbors.  This is implemented in Conjecture here . Other Thoughts Above are the basic techniques for generating personalized recommendations.  Over the course of developing these recommender systems, we found a few modifications we could make to improve the quality of the recommendations. Normalizing the vectors before computation: As stated, the matrix factorization models tend to produce vectors with large norms for the popular items.  A result is that some popular items may get recommended to many users even if they are not necessarily the most aligned with the users tastes.  Therefore, before computing recommendations, we normalized all the item vectors.  This also makes the use of approximate nearest neighbors theoretically sound: since when all vectors have unit norms, maximum inner products to a user vector are achieved by the nearest item vectors. Shop diversity: Etsy is a marketplace consisting of many sellers. So as to be fair to these sellers, we limit the number of recommendations from a single shop that we present to each user.  Since users may click through to the shop anyway, exposing additional items  available from this shop, this does not seem to present a problem in terms of recommendation quality. Item diversity: To make the recommendations more diverse, we take a candidate set of say 100 nearest neighbors to the user, then we filter those out by removing any item which is within some small distance of a higher ranked item, where the distance is measured in the Euclidean distance between the item vectors. Reranked Items from Similar Users: We used the LSH code to find nearest neighbors among the users (so for each user we find users with similar tastes).  Then to produce item recommendations we can take those users favorite listings, and re-rank them according to the inner products between the item vectors and the target users vector.  This lead to seemingly better and more relevant recommendations, although a proper experiment remains to be done. Conclusion In summary we described how we can build recommender systems for e-commerce based on implicit feedback data.  We built a system which computes recommendations on Hadoop, which is now part of our open source machine learning package “Conjecture.”  Finally we shared some additional tweaks that can be made to potentially improve the quality of recommendations. Posted by Robert Hall on November 17, 2014 Category: data , engineering Tags: matrix factorization , personalization , recommendations", "date": "2014-11-17,"},
{"website": "Etsy", "title": "Building A Better Build: Our Transition From Ant To SBT", "author": ["Andrew Johnson"], "link": "https://codeascraft.com/2014/09/30/building-a-better-build-our-transition-from-ant-to-sbt/", "abstract": "Building A Better Build: Our Transition From Ant To SBT Posted by Andrew Johnson on September 30, 2014 A build tool is fundamental to any non-trivial software project.  A good build tool should be as unobtrusive as possible, letting you focus on the code instead of the mechanics of compilation.  At Etsy we had been using Ant to build our big data stack.  While Ant did handle building our projects adequately, it was a common source of questions and frustration for new and experienced users alike.  When we analyzed the problems users were having with the build process, we decided to replace Ant with SBT , as it was a better fit for our use cases.  In this post I’ll discuss the reasons we chose SBT as well as some of the details of the actual process of switching. Why Did We Switch? There were two perspectives we considered when choosing a replacement for Ant.  The first is that of a user of our big data stack.  The build tool should stay out of the way of these users as much as possible.  No one should ever feel it is preventing them from being productive, but instead that it is making them more productive.  SBT has a number of advantages in this regard: Built-in incremental compiler : We used the stand-alone incremental compiler zinc for our Ant build.  However, this required a custom Ant task, and both zinc and that task needed to be installed properly before users could start building.  This was a common source of questions for users new to our big data stack.  SBT ships with the incremental compiler and uses it automatically. Better environment handling : Because of the size of our code base, we need to tune certain JVM options when compiling it.  With Ant, these options had to be set in the ANT_OPTS environment variable.  Not having ANT_OPTS set properly was another common source of problems for users.  There is an existing community-supported SBT wrapper that solves this problem.  The JVM options we need are set in a .jvmopts file that is checked in with the code. Triggered execution : If you prefix any SBT command or sequence of commands with a tilde, it will automatically run that command every time a source file is modified.  This is a very powerful tool for getting immediate feedback on changes.  Users can compile code, run tests, or even launch a Scalding job automatically as they work. Build console : SBT provides an interactive console for running build commands.  Unlike running commands from the shell, this console supports tab-completing arguments to commands and allows temporarily modifying settings. The other perspective is that of someone modifying the build.  It should be straightforward to make changes.  Furthermore, it should be easy to take advantage of existing extensions from the community.  SBT is also compelling in this regard: Build definition in Scala : The majority of the code for our big data stack is Scala.  With Ant, modifying the build requires a context switch to its XML-based definition.  An SBT build is defined using Scala, so no context switching is necessary.  Using Scala also provides much more power when defining build tasks.  We were able to replace several Ant tasks that invoked external shell scripts with pure Scala implementations.  Defining the build in Scala does introduce more opportunities for bugs, but you can use scripted to test parts of your build definition. Plugin system : To extend Ant, you have to give it a JAR file, either on the command line or by placing it in certain directories.  These JAR files then need to be made available to everyone using the build.  With SBT, all you need to do is add a line like addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.11.2\") to the build definition.  SBT will then automatically download that plugin and its dependencies.  By default SBT will download dependencies from Maven Central, but you can configure it to use only an internal repository: resolvers += resolver(\"internal-repo\", \"<repository URL>\")\r\nexternalResolvers <<= resolvers map { rs =>\r\n    Resolver.withDefaultResolvers(rs, mavenCentral = false)\r\n} Ease of inspecting the build : SBT has an “inspect” command that will provide a lot of information about a setting or task: the type, any related settings or tasks, and forward and reverse dependencies.  This is invaluable when trying to understand how some part of the build works. Debugging support : Most SBT tasks produce verbose debug logs that are not normally displayed.  The “last” command will output those logs, making tracking down the source of an error much easier. This is not to say that SBT is perfect.  There are some aspects of SBT that are less than ideal: Terse syntax : The SBT build definition DSL is full of hieroglyphic symbols, such as the various assignment operators: <<=, <+=, and <++=.  This terse syntax can be intimidating for those that are not used to it. Support for compiling a subset of files : The Scala compiler can be slow, so our Ant build supported compiling only a subset of files.  This was very easy to add in Ant.  SBT required a custom plugin that delved into SBT’s internal APIs.  This does work, but we have already had to do some minor refactoring between SBT 0.13.1 and 0.13.2.  We have to accept that that such incompatibility is likely to occur between future minor versions. Less mature : SBT is a relatively new tool, and the plugin ecosystem is still growing.  As such, there were several times we had to write custom plugins for functionality that would likely already be supported by another tool.  We also experienced a couple of bugs in SBT itself during the switch. At this point it’s fair to ask why SBT, and not Maven, Gradle, or some other tool.  You can certainly get features like these in other tools.  We did not do a detailed comparison of multiple tools to replace Ant.  SBT addressed our pain points with the Ant build, and it was already popular with users of our big data stack. The Transition Process It may sound cliché, but the initial hurdle for actually switching to SBT was gaining an in-depth understanding of the tool.  SBT’s terminology and model of a build is very different from Ant. SBT’s documentation is very good and very thorough, however.  Starting the switch without having read through it would have resulted in a lot of wasted time.  It’s a lot easier to find answers online when you know how SBT names things! The primary goal when switching was to have the SBT build be a drop-in replacement for the Ant build.  This removed the need to modify any external processes that use the build artifacts.  It also allowed us to have the Ant and SBT builds in parallel for a short time in case of bugs in the SBT build.  Unfortunately, our project layout did not conform to SBT’s conventions — but SBT is fully configurable in this regard.  SBT’s “inspect” command was helpful here for discovering which settings to tweak.  It was also good that SBT supports using an external ivy.xml file for dependency management.  We were already using Ivy with our Ant build, so we were able to define our dependencies in one place while running both builds in parallel. With the configuration to match our project layout taken care of, actually defining the build with SBT was a straightforward task.  Unlike Ant, SBT has built-in tasks for common operations, like compiling, running tests, and creating a JAR.  SBT’s multi-project build support also cut down on the amount of configuration necessary.  We have multiple projects building together, and every single Ant task had to be defined to build them in the correct order.  Once we configured the dependencies between the projects in SBT, it automatically guaranteed this. We also took advantage of SBT’s aliasing feature to make the switch easier on users.  The names of SBT’s built-in tasks did not always align with the names we had picked when defining the corresponding task in Ant.  It’s very easy to alias commands, however: addCommandAlias(\"jar\", \"package\") With such aliases users were able to start using SBT just as they had used Ant, without needing to learn a whole set of new commands.  Aliases also make it easy to define sequences of commands without needing to create an entirely new task, such as addCommandAlias(\"rebuild\", \";clean; compile; package\") The process of switching was not entirely smooth, however.  We did run into two bugs in SBT itself. The first is triggered when you define an alias that is the prefix of a task in the alias definition.  Tab-completing or executing that alias causes SBT to hang for some time and eventually die with a StackOverflowError.  This bug is easy to avoid if you define your alias names appropriately, and it is fixed in SBT 0.13.2.  The other bug only comes up with multi-module projects.  Even though our modules have many dependencies in common, SBT will re-resolve these dependencies for each module.  There is now a fix for this in SBT 0.13.6, the most recently released version, that can be enabled by adding updateOptions := updateOptions.value.withConsolidatedResolution(true) to your build definition.  We saw about a 25% decrease in time spent in dependency resolution as a result. Custom Plugins As previously mentioned, we had to write several custom plugins during the process of switching to SBT to reproduce all the functionality of our Ant build.  We are now open-sourcing two of these SBT plugins.  The first is sbt-checkstyle-plugin .  This plugin allows running Checkstyle over Java sources with the checkstyle task.  The second is sbt-compile-quick-plugin .  This plugin allows you to compile and package a single file with the compileQuick and packageQuick tasks, respectively.  Both of these plugins are available in Maven Central. Conclusion Switching build tools isn’t a trivial task, but it has paid off.  Switching to SBT has allowed us to address multiple pain points with our previous Ant build.  We’ve been using SBT for several weeks now.  As with any new tool, there was a need for some initial training to get everyone started.  Overall though, the switch has been a success!   The number of issues encountered with the build has dropped.  As users learn SBT they are taking advantage of features like triggered execution to increase their productivity. Posted by Andrew Johnson on September 30, 2014 Category: engineering , infrastructure Tags: ant , java , sbt , scala", "date": "2014-09-30,"},
{"website": "Etsy", "title": "Expanding Diversity Efforts with Hacker School", "author": ["Mike Brittain"], "link": "https://codeascraft.com/2014/09/25/expanding-diversity-efforts-with-hacker-school/", "abstract": "Expanding Diversity Efforts with Hacker School Posted by Mike Brittain on September 25, 2014 Today we’re proud to announce that Etsy will provide $210,000 in Etsy Hacker Grants to Hacker School applicants in the coming year. These grants extend our support of Hacker School’s diversity initiatives, which first focused on the gender imbalance of applicants to their program and in the wider industry, and will now expand to support applicants with racial backgrounds that are underrepresented in software engineering. The grants will provide up to $7,000 in support for at least 35 accepted applicants in the next three batches of Hacker School, and are designed to help with a student’s living expenses during their three-month curriculum in New York City. Diversity and opportunity lie at the heart of what Etsy is about, a place where anyone can start a business for $0.20. Today we wrote a post talking about how we think about diversity at Etsy: More Than Just Numbers . As an engineering team, diversity and opportunity are core values for us as well — we believe a diverse environment is a resilient one. We love what we do and want to extend that opportunity to anyone who wants it. This doesn’t mean the work to support diversity is easy or that we’ve mastered it — but we are committed to continuing to improve. Over the years, we’ve focused on educational programs looking at unconscious bias, bringing in speakers from NCWIT, and building out our internal leadership courses to support a broad swath of new leaders. Hacker School grants have been one of our favorite and most effective programs since sponsoring our first batch of students in summer 2012. We’ve even given talks about how well it went . Hacker School’s selection process and environment combine to create a group of students diverse across a number of axes, including gender, race, experience and technical background, but that are also culturally and technically aligned with Etsy’s engineering team. Hacker School’s welcoming “programming retreat” approach produces the sort of broad, deep, tool-building and curious system engineers that work well in our distributed, iterative, transparent and scaled environment. We have Hacker School alums across almost every team in Etsy engineering and at nearly every level, from just starting their first job to very senior engineers. We know that racial diversity is a complicated issue, and we are by no means the experts. But we believe that together with Hacker School we are making a small step in the right direction. And we need your help. This program only works if qualified applicants hear that it’s happening, and know that we really want them to apply. If you know someone who you think would be great, please let them know, and encourage them to apply to an upcoming batch ! Posted by Mike Brittain on September 25, 2014 Category: people , philosophy", "date": "2014-09-25,"},
{"website": "Etsy", "title": "Come find Etsy at Velocity NY 2014", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2014/09/10/come-find-etsy-at-velocity-ny-2014/", "abstract": "Come find Etsy at Velocity NY 2014 Posted by Kellan Elliott-McCrea on September 10, 2014 Velocity is our kind of conference, and Velocity NY happens in our backyard in that funny little borough on the other side of the river. (Manhattan)  Make sure to come find us, we’ll be there teaching, speaking, and keynoting: Monday 9am “PostMortem Facilitation: Theory and Practice of “New View” Debriefings” – John Allspaw Monday 1:30pm “Building a Device Lab” – Lara Swanson and Destiny Montague Tuesday 1:15pm “A Woman’s Place is at the Command Prompt” – Lara Swanson (moderator), Katherine Daniels (Etsy), Jennifer Davis (Chef), Bridget Kromhout (DramaFever), Samantha Thomas (UMN) Tuesday 3:30pm “It’s 3AM, Do You Know Why You Got Paged?” – Ryan Frantz Tuesday 5pm “Etsy’s Journey to Building a Continuous Integration Infrastructure for Mobile Apps” – Nassim Kammah Wednesday 11:20am “Unpacking the Black Box: Benchmarking JS Parsing and Execution on Mobile Devices” – Daniel Espeset Holding office hours Nassim Kammah of our continuous integration team, the Autobot Ryan Frantz our sleep and alert design obsessed operations engineer John Allspaw, he should look familiar to you by now Signing books Jon Cowie will be signing his book “Customizing Chef” Lara Swanson will be signing galleys of “Designing for Performance” John Allspaw will be signing “Web Operations” and “Capacity Planning” Posted by Kellan Elliott-McCrea on September 10, 2014 Category: events , Uncategorized", "date": "2014-09-10,"},
{"website": "Etsy", "title": "Teaching Testing: Our Testing 101 Materials", "author": ["Yash Parghi"], "link": "https://codeascraft.com/2014/08/20/teaching-testing-our-testing-101-materials/", "abstract": "Teaching Testing: Our Testing 101 Materials Posted by Yash Parghi on August 20, 2014 Etsy engineers have a wide variety of backgrounds, strengths, and weaknesses, so there are no engineering skills we can take for granted. And there are things you can’t just assume engineers will learn for themselves because you throw a codebase and a workflow at them. I work on Etsy’s continuous deployment team, which advises on automated testing of our code, and I felt that we could use some stronger means of teaching (and establishing as a conscious value) the skills of testing and design in code. To that end, I recently wrote two “Testing 101” materials for use by all Etsy engineers. They’re now both on our public Github: the Etsy Testing Best Practices Guide , and our Testing 101 Code Lab for hands-on practice applying its ideas. Both use PHP and PHPUnit. We called it the “Testing Best Practices Guide” because we love misnomers. It’s more about design than testing, it describes few concrete practices, and we don’t call any of them “best” . Within Etsy, we supplement mere documents with activities like team rotations (“bootcamps”) for new hires, technical talks, and dojos (collaborative coding exercises) to practice and have fun with coding topics as a group. And of course, we do code review. Deeper technical considerations are often invisible in code, so you have to find a way, whether by process, tooling, or teaching, to make them visible. Posted by Yash Parghi on August 20, 2014 Category: Uncategorized", "date": "2014-08-20,"},
{"website": "Etsy", "title": "Q2 2014 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/08/01/q2-2014-site-performance-report/", "abstract": "Q2 2014 Site Performance Report Posted by Lara Hogan on August 1, 2014 As the summer really starts to heat up, it’s time to update you on how our site performed in Q2. The methodology for this report is identical to the Q1 report . Overall it’s a mixed bag: some pages are faster and some are slower. We have context for all of it, so let’s take a look. Server Side Performance Here are the median and 95th percentile load times for signed in users on our core pages on Wednesday, July 16th: A few things stand out in this data: Median homepage performance improved, but the 95th percentile got slower. This is due to a specific variant we are testing which is slower than the current page. We made some code changes that improved load time for the majority of pageviews, but the one slower test variant brings up the higher percentiles. The listing page saw a fairly large increase in both median and 95th percentile load time. There isn’t a single smoking gun for this, but rather a number of small changes that caused little increases in performance over the last three months. Search saw a significant decrease across the board. This is due to a dedicated memcached cluster that we rolled out to cache “listing cards” on the search results page. This brought our cache hit rate for listing related data up to 100%, since we automatically refresh the cache when the data changes. This was a nice win that will be sustainable over the long term. The shop page saw a big jump at the 95th percentile. This is again due to experiments we are running on this page. A few of the variants we are testing for a shop redesign are slower than the existing page, which has a big impact on the higher percentiles. It remains to be seen which of these variants will win, and which version of the page we will end up with. Overall we saw more increases than decreases on the backend, but we had a couple of performance wins from code/architecture changes, which is always nice to see. Looking ahead, we are planning on replacing the hardware in our memcached cluster in the next couple of months, and tests show that this should have a positive performance impact across the entire site. Synthetic Front-end Performance As a reminder, these tests are run with Catchpoint. They use IE9, and they run from New York, London, Chicago, Seattle, and Miami every two hours. The “Webpage Response” metric is defined as the time it took from the request being issued to receiving the last byte of the final element on the page. Here is that data: The render start metrics are pretty much the same across the board, with a couple of small decreases that aren’t really worth calling out due to rounding error and network variability. The “webpage response” numbers, on the other hand, are up significantly across the board. This is easily explained: we recently rolled out full site TLS, and changed our synthetic tests to hit https URLs. The added TLS negotiation time for all assets on the page bumped up the overall page load time everywhere. One thing we noticed with this change is that due to most browsers making six TCP connections per domain, we pay this TLS negotiation cost many times per page . We are actively investigating SPDY with the goal of sending all of our assets over one connection and only doing this negotiation once. Real User Front-end Performance As always, these numbers come from mPulse, and are measured via JavaScript running in real users’ browsers: One change here is that we are showing an extra significant figure in the RUM data. We increased the number of beacons that we send to mPulse, and our error margin dropped to 0.00 seconds, so we feel confident showing 10ms resolution. We see the expected drop in search load time because of the backend improvement, and everything else is pretty much neutral. The homepage ticked up slightly, which is expected due to the experiment that I mentioned in the server side load time section. One obvious question is: “Why did the synthetic numbers change so much while the RUM data is pretty much neutral?”. Remember, the synthetic numbers changed primarily because of a change to the tests themselves. The switch to https caused a step change in our synthetic monitoring, but for real users the rollout was gradual. In addition, real users that see more than one page have some site resources in their browser cache, mitigating some of the extra TLS negotiations. Our synthetic tests always operate with an empty cache, which is a bit unrealistic. This is one of the reasons why we have both synthetic and RUM metrics: if one of them looks a little wonky we can verify the difference with other data. Here’s a brief comparison of the two, showing where each one excels: Synthetic Monitoring Real User Monitoring Browser Instrumentation Navigation Timing API Consistent Trending over time Can be highly variable as browsers and networks change Largely in your control Last mile difficulties Great for identifying regressions Great for comparing across geographies/browsers Not super realistic from an absolute number point of view “Real User Monitoring” A/B tests can show outsized results due to empty caches A/B tests will show the real world impact Conclusion This report had some highs and some lows, but at the end of the day our RUM data shows that our members are getting roughly the same experience they were a few months ago performance wise, with faster search results pages. We’re optimistic that upgrading our memcached cluster will put a dent in our backend numbers for the next report, and hopefully some of our experiments will have concluded with positive results as well. Look for another report from us as the holiday season kicks off! Posted by Lara Hogan on August 1, 2014 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2014-08-1,"},
{"website": "Etsy", "title": "Just Culture resources", "author": ["Ian Malpass"], "link": "https://codeascraft.com/2014/07/18/just-culture-resources/", "abstract": "Just Culture resources Posted by Ian Malpass on July 18, 2014 This is a (very) incomplete list of resources that may be useful to those wanting to find out more about the ideas behind Just Culture, blamelessness, complex systems, and related topics. It was created to support my DevOps Days Minneapolis talk Fallible Humans . Human error and sources of failure The Field Guide To Understanding Human Error by Sidney Dekker (book) The ETTO Principle by Erik Hollnagel (book) Just Culture Just Culture by Sidney Dekker (book, and related blog post ) Second Victim by Sidney Dekker (book) Postmortems and blameless reviews Blameless Postmortems and a Just Culture by John Allspaw (blog post) What Adopting Blameless Post-Mortems Has Taught Me About Culture by Mathias Meyer (blog post) What Blameless Really Means by Jessica Harllee (blog post) Complex systems and complex system failure How Complex Systems Fail by Richard Cook (paper, and related talk ) Understanding Complexity by Scott E Page (course) Posted by Ian Malpass on July 18, 2014 Category: engineering , operations , outages , people , philosophy", "date": "2014-07-18,"},
{"website": "Etsy", "title": "Conjecture: Scalable Machine Learning in Hadoop with Scalding", "author": ["jattenberg"], "link": "https://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/", "abstract": "Conjecture: Scalable Machine Learning in Hadoop with Scalding Posted by jattenberg on June 18, 2014 Intro Predictive machine learning models are an important tool for many aspects of e-commerce.  At Etsy, we use machine learning as a component in a diverse set of critical tasks. For instance, we use predictive machine learning models to estimate click rates of items so that we can present high quality and relevant items to potential buyers on the site.  This estimation is particularly important when used for ranking our cost-per-click search ads, a substantial source of revenue. In addition to contributing to on-site experiences, we use machine learning as a component of many internal tools, such as routing and prioritizing our internal support e-mail queue.  By automatically categorizing and estimating an “urgency” for inbound support e-mails, we can assign support requests to the appropriate personnel and ensure that urgent requests are handled by staff more rapidly, helping to ensure a good customer experience. To quickly develop these types of predictive models while making use of our MapReduce cluster, we decided to construct our own machine learning framework, open source under the name “ Conjecture .”  It consists of three main parts: Java classes which define the machine learning models and data types. Scala methods which perform MapReduce training using Scalding. PHP classes which use the produced models to make predictions in real-time on the web site. This article is intended to give a brief introduction to predictive modeling, an overview of Conjecture’s capabilities, as well as a preview of what we are currently developing. Predictive Models The main goal in constructing a predictive model is to make a function which maps an input into a prediction.  The input can be anything – from the text of an email, to the pixels of an image, or the list of users who interacted with an item.  The predictions we produce currently are of two types: either a real value (such as a click rate) or a discrete label (such as the name of an inbox in which to direct an email). The only prerequisite for constructing such a model is a source of training examples: pairs consisting of an input and its observed output.  In the case of click rate prediction these can be constructed by examining historic click rates of items.  For the classification of e-mails as urgent or not, we took the historic e-mails as the inputs, and an indicator of whether the support staff had marked the email as urgent or not after reading its contents. Feature Representation Having gathered the training data, then next step is to convert it into a representation which Conjecture can understand.  As is common in machine learning, we convert the raw input into a feature representation, which involves evaluating several “feature functions” of the input and constructing a feature vector from the results.  For example, to classify emails the feature functions are things like: the indicator of whether the word “account” is in the email, whether the email is from a registered user or not, whether the email is a follow up to an earlier email and so on. Additionally, for e-mail classification we also included subsequences of words which appeared in the email. For example the feature representation of an urgent email which was from a registered user, and had the word “time” in the subject, and the string “how long will it take to receive our item?” in the body may look like: \"label\": {\"value\" : 1.0},\r\n  \"vector\" : {\r\n    \"subject___time\" : 1.0,\r\n    \"body___will::it::take\" : 1.0,\r\n    \"body___long::will\": 1.0,\r\n    \"body___to::receive::our\" : 1.0,\r\n    \"is_registered___true\" : 1.0,\r\n    ...\r\n  } We make use of a sparse feature representation, which is a mapping of string feature names to double values.  We use a modified GNU trove hashmap to store this information while being memory efficient. Many machine learning frameworks store features in a purely numerical format. While storing information as, for instance, an array of doubles is far more compact than our “string-keyed vectors”, model interpretation and introspection becomes much more difficult. The choice to store the names of features along with their numeric value allows us to easily inspect for any weirdness in our input, and quickly iterate on models by finding the causes of problematic predictions. Model Estimation Once a dataset has been assembled, we can estimate the optimal predictive model.  In essence we are trying to find a model that makes predictions which tend to agree with the observed outcomes in the training set.  The statistical theory surrounding “ Empirical Risk Minimization ” tells us that under some mild conditions, we may expect a similar level of accuracy from the model when applied to unseen data. Conjecture, like many current libraries for performing scalable machine learning, leverages a family of techniques known as online learning . In online learning, the model processes the labeled training examples one at a time, making an update to the underlaying prediction function after each observation.  While the online learning paradigm isn’t compatible with every machine learning technique, it is a natural fit for several important  classes of machine learning models such as logistic regression and large margin classification , both of which are implemented in Conjecture. Since we often have datasets with millions of training examples, processing them sequentially on a single machine is unfeasible. Some form of parallelization is required. However, traditionally the online learning framework does not directly lend itself to a parallel method for model training. Recent research into parallelized online learning in Hadoop gives us a way to perform sequential updates of many models in parallel across many machines, each separate process consuming a fraction of the total available training data. These “sub-models” are aggregated into a single model that can be used to make predictions or, optionally, feed back into another “round” of the process for the purpose of further training. Theoretical results tell us, that when performed correctly, this process will result in a reliable predictive model, similar to what would be generated had there been no parallelization. In practice, we find the models converge to an accurate state quickly, after a few iterations. Conjecture provides an implementation for parallelized online optimization of machine learning models in Scalding ,  a Scala wrapper for Cascading — a package which plans workflows consisting of several MapReduce jobs each.  Scalding also abstracts over the key-value pairs required by MapReduce, and permits arbitrary n-ary tuples to be used as the data elements. In Conjecture, we train a separate model on each mapper using online updates, by making use of the map-side aggregation functionality of Cascading. Here, in each mapper process, data is grouped by common keys and a reduce function is applied. Data is then sent across the network to reduce nodes which complete the aggregation. This map-side aggregation is conceptually the same as the combiners of MapReduce, though the work resides in the same process as the map task itself.  The key to our distributed online machine learning algorithm is in the definition of appropriate reduce operations so that map-side aggregators will implement as much of the learning as possible.  The alternative — shuffling the examples to reducers processes across the MapReduce cluster and performing learning there — is slower as it requires much more communication. During training, the mapper processes consume an incoming stream of training instances. The mappers take these examples and emit pairs consisting of the example and an “empty” (untrained) predictive model. This sequence of  pairs is passed to the aggregator, where the actual online learning is performed. The aggregator process implements a reduce function which takes two such pairs and produces a single pair with an updated model.  Call the pairs a and b, each with a member model and a member called example. Pairs “rolled up” by this reduce process always contain a model, and an example which has not yet been used for training.  When the reduce operation is consuming two models which both have some training, they are merged (e.g., by summing or averaging the parameter values) otherwise, we continue to train whichever model has some training already.  Due to the order in which Cascading calls the reduce function in the aggregator, we end up building a single model on each machine, these are then shuffled to a single reducer where they are merged.  Finally we can update the final model on the one labeled example which it has not yet seen.  Note that the reduce function we gave is not associative — the order in which the pairs are processed will affect the output model.  However, this approach is robust in that it will produce a useful model irrespective of the ordering. The logic for the reduce function is: train_reduce(a,b) = {\r\n\r\n  // neither pair has a trained model,\r\n  // update a model on one example,\r\n  // emit that model and the other example \r\n  if(a.model.isEmpty && b.model.isEmpty) {  \r\n    (b.model.train(a.example), b.example)\r\n  } \r\n\r\n  // one model is trained, other isn't.\r\n  // update trained model, emit that\r\n  // and the other example\r\n  if(!a.model.isEmpty && b.model.isEmpty) {\r\n    (a.model.train(b.example), a.example)\r\n  }\r\n\r\n  // mirroring second case\r\n  if(a.model.isEmpty && !b.model.isEmpty) {\r\n    (b.model.train(a.example), b.example)\r\n  }\r\n\r\n  // both models are partially trained. \r\n  // update one model, merge that model\r\n  // with the other. emit with\r\n  // unobserved example\r\n  if(!a.model.isEmpty && !b.model.isEmpty) {\r\n    (b.model.merge(a.model.train(a.example)), b.example)\r\n  }\r\n} In conjecture, we extend this basic approach, consuming one example at a time, so that we can perform “mini batch” training.  This is a variation of online learning where a small set of training examples are all used at once to perform a single update.  This leads to more flexibility in the types of models we can train, and also lends better statistical properties to the resulting models (for example by reducing variance in the gradient estimates in the case of logistic regression).  What’s more it comes at no increase in computational complexity over the standard training method.  We implement mini-batch training by amending the reduce function to construct lists of examples, only performing the training when sufficiently many examples have been aggregated. Model Evaluation We implement a parallelized cross validation so that we can get estimates of the performance of the output models on unseen inputs.  This involves splitting the data up into a few parts (called “folds” in the literature), training multiple models, each of which is not trained on one of the folds, and then testing each model on the fold which it didn’t see during training.  We consider several evaluation metrics for the models, such as accuracy, the area under the ROC curve , and so on.  Since this procedure yields one set of performance metrics for each fold, we take the appropriately weighted means of these, and the observed variance also gives an indication of the reliability of the estimate.  Namely if the accuracy of the classification is similar across all the folds then we may anticipate a similar level of accuracy on unobserved inputs.  On the other hand if there is a great discrepancy between the performance on different folds then it suggests that the mean will be an unreliable estimate of future performance, and possibly that either more data is needed or the model needs some more feature engineering. Inevitably, building useful machine learning models requires iteration– things seldom work very well right out of the box. Often this iteration involves inspecting a model, developing an understanding as to why a model operates in the way that it does, and then fixing any unusual or undesired behavior. Leveraging our detailed data representation, we have developed tools enabling the manual  examination of the learned models.  Such inspection should only be carried out for debugging purposes, or to ensure that features were correctly constructed, and not to draw conclusions about the data.  It is impossible to draw valid conclusions about parameter values without knowing their associated covariance matrix — which we do not handle since it is presumably too large. Applying the Models As part of the model training, we output a JSON-serialized version of the model. This allows us to load the models into any platform we’d like to use. In our case, we want to use our models on our web servers, which use PHP. To accomplish this, we deploy our model file (a single JSON string encoding the internal model data structures) to the servers where we instantiate it using PHP’s json_decode() function. We also provide utility functions in PHP to process model inputs into the same feature representations used in Java/Scala, ensuring that a model is correctly applied. An example of a json-encoded conjecture model is below: {\r\n  \"argString\": \"--zero_class_prob 0.5 --model mira --out_dir contact_seller \r\n                --date 2014_05_24 --folds 5 --iters 10 \r\n                --input contact_seller/instances \r\n                --final_thresholding 0.001 --hdfs\",\r\n  \"exponentialLearningRateBase\": 1.0,\r\n  \"initialLearningRate\": 0.1,\r\n  \"modelType\": \"MIRA\",\r\n  \"param\": {\r\n    \"freezeKeySet\": false,\r\n    \"vector\": {\r\n      \"__bias__\": -0.16469815089457646,\r\n      \"subject___time\": -0.01698080417481483,\r\n      \"body___will::it::take\": 0.05834880357927012,\r\n      \"body___long::will\": 0.0818060174986067991,\r\n      \"is_registered___true\": -0.002215130480164454,\r\n      ...\r\n    }\r\n  }\r\n} Currently the JSON-serialized models are stored in a git repository and deployed to web servers via Deployinator (see the Code as Craft post about it here ). This architecture was chosen so that we could use our established infrastructure to deploy files from git to the web servers to distribute our models. We gain the ability to quickly prototype and iterate on our work, while also gaining the ability to revert to previous versions of a model at will. Our intention is to move to a system of automated nightly updates, rather than continue with the manually controlled Deployinator process. Deployinator broadcasts models to all the servers running code that could reference Conjecture models, including the web hosts, and our cluster of gearman workers that perform asynchronous tasks, as well as utility boxes which are used to run cron jobs and ad hoc jobs. Having the models local to the code that’s referencing them avoids network overhead associated with storing models in databases; the process of reading and deserializing models, then making predictions is extremely fast. Conclusions and Future Work The initial release of Conjecture shares some of Etsy’s tooling for building classification and regression models at massive scale using Hadoop. This infrastructure is well-tested and practical, we’d like to get it in the hands of the community as soon as possible. However, this release represents only a fraction of the machine learning tooling that we use to power many features across the site.  Future releases of Conjecture will include tools for building cluster models and infrastructure for building recommender systems on implicit feedback data in Scalding. Finally, we will release “web code” written in PHP and other languages that can consume Conjecture models and make predictions efficiently in a live production environment. Posted by jattenberg on June 18, 2014 Category: data", "date": "2014-06-18,"},
{"website": "Etsy", "title": "Calendar Hacks", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/07/15/calendar-hacks/", "abstract": "Calendar Hacks Posted by Lara Hogan on July 15, 2014 As an engineering manager, there’s one major realization you have: managers go to lots of meetings. After chatting with a bunch of fellow engineering managers at Etsy, I realized that people have awesome hacks for managing their calendars and time. Here are some of the best ones from a recent poll of Etsy engineering managers! We’ll cover tips on how to: Block out time Change your defaults Rely on apps and automation Do routine cleanup Think ahead (long-term) Create separate calendars To access any of the Labs settings/apps: Block out time Create big unscheduled blocks every week . It allows for flexibility in your schedule. Some block out 3 afternoons/week as “office hours — don’t schedule unless you’re on my team”. It creates uninterrupted time when I’m *not* in meetings and available on IRC. Some book time to work on specific projects, and mark the event as private. They’ll try to strategize the blocking to prevent calendar fragmentation. Automatically decline events (Labs) : Lets you block off times in your calendar when you are unavailable. Invitations sent for any events during this period will be automatically declined. After you enable this feature, you’ll find a “Busy (decline invitations)” option in the “Show me as” field. Office Hours: Blocks for office hours allow you to easily say, “yes, I want to talk to you, but can we schedule it for a time I’ve already set aside?” Better than saying “No, I cannot talk to you, my calendar is too full.” (Which also has to happen from time to time.) When you create a new event on your calendar, choose the “Appointment slots” link in the top of the pop-up: Then follow the instructions to create a bookable slot on your calendar. You’ll be able to share a link to a calendar with your bookable time slots: Declining meetings: Decline meetings with a note. Unless you work with the organizer often, do this automatically if there’s no context for the meeting. One manager has declined 1 hour meetings, apologized for missing them, asked for a follow up, and found that he really wasn’t needed in the first place. Hours saved! Change your defaults Shorter meetings: Don’t end meetings on the hour; use 25/50 minute blocks. You can set this in Calendars>My Calendars>Settings>General>Default Meeting Length. If you set your calendar invites to 25 minutes or 55 minutes, you need to assert that socially at the beginning of the meeting, and then explicitly do a time check at 20 minutes (“we have 5 minutes left in this meeting”). If possible, start with the 25 (or 30) minute defaults rather than the hour-long ones. Visible vs private: Some make their calendar visible, not private by default. This lets other people see what they’re doing so they have a sense of whether they can schedule against something or not–a team meeting–maybe, mad men discussion group, no way. Custom view: Create a custom 5- or 7-day view or use Week view. Color coding: Downward one-on-ones with one color; upward one-on-ones with another Regular meetings get a color – anything else that pops up should be noticeable since it will now be the only stuff with the default color. Color code all your team’s meetings and your 1:1s with them, so you can see what’s non-team-related in your calendar Gentle Reminders (Labs): This feature replaces Calendar’s pop-ups: when you get a reminder, the title of the Google Calendar window or tab will happily blink in the background and you will hear a pleasant sound. Event dimming: dim past events, and/or recurring future events, so you can focus. Editability: Make all meetings modifiable and including this in the invite description, so as to avoid emails about rescheduling (“you have modifying rights — reschedule as needed, my schedule is updated”) and ask reports and your manager to do the same when they send  invites. This can result in fewer emails back and forth to reschedule. Rely on apps and automation Sunrise: Sunrise for iOS does the right thing and doesn’t show declined events, which weaponizes the auto-decline block. World Clock (Labs) helps you keep track of the time around the world. Plus: when you click an event, you’ll see the start time in each time zone as well. You could alternatively add an additional timezone (settings -> general -> Your Time Zone -> Additional Timezone). Free or busy (Labs): See which of your friends are free or busy right now (requires that friends share their Google Calendars with you. Next meeting (Labs): See what’s coming up next in your calendar. Soft timer: Set a timer on your phone by default at most of meetings, group or 1:1s,  and tell folks you’re setting an alarm to notify everyone when you have 5 minutes left. People love this, especially in intense 1:1s, because they don’t have to worry about the time. It can go really well as a reminder to end in “Speedy Meeting” time. Do routine cleanup Calendar review first thing in the morning. Review and clean up next three days. Always delete the “broadcast” invites you’re not going to attend. Block off some empty timeslots first thing in the morning to make sure you have some breaks during the day—using Busy / Auto Decline technology. People don’t seem to notice when you note all day events at the top of your calendar. If you’re going offsite,  book an event that’s 9am-7pm that will note your location. Think ahead (long-term) Book recurring reminders: For instance, do a team comp review every three months. Or if there is a candidate we lost out on, make appointments to follow up with them in a few months. Limit Monday recurring meetings: Holiday weekends always screw you up when you have to reschedule all of those for later in the week. Track high-energy hours: “I tracked my high energy hours against my bright-mind-needed tasks and lo-and-behold, realized that mornings is when I need a lot of time to do low-urgency/high importance things that otherwise I wasn’t making time for or I was doing in a harried state. I thus time-blocked 3 mornings a week where from 8am to 11am I am working on these things (planning ahead, staring at the wall thinking through a system issue, etc). It requires a new 10pm to 6am sleeping habit, but it has been fully worth it, I feel like I gained a day in my way. This means I no longer work past 6pm most days, which was actually what was most draining to me.” Create separate calendars Have a shared calendar for the team for PTO tracking, bootcampers, time out of the office, team standups etc. Subscribe to the Holidays in the United States Calendar so as to not be surprised by holidays: Calendar ID: en.usa#holiday@group.v.calendar.google.com Subscribe to the conference room calendars if that’s something your organization has. Create a secondary calendar for non critical events , so they stay visible but don’t block your calendar. If there’s an event you are interested in, but haven’t decided on going or not, and don’t want other people to feel the need to schedule around it, you can go the event and copy it. Then remove it from your primary calendar. You can toggle the secondary calendar off via the side-panel, and if someone needs to set something up with you, you’ll be available. When using multiple calendars, there may be events that are important to have on multiple calendars, but this takes up a lot of screen real estate . In these cases, we use Etsy engineer Amy Ciavolino’s Event Merge for Google Calendar Chrome extension . It makes it easy to see those events without them taking up precious screen space. And, for a little break from the office, be sure to check out Etsy analyst Hilary Parker’s Sunsets in Google Calendar (using R!). Posted by Lara Hogan on July 15, 2014 Category: people", "date": "2014-07-15,"},
{"website": "Etsy", "title": "Threat Modeling for Marketing Campaigns", "author": ["Gabrielle Gianelli"], "link": "https://codeascraft.com/2014/07/07/threat-modeling-for-marketing-campaigns/", "abstract": "Threat Modeling for Marketing Campaigns Posted by Gabrielle Gianelli on July 7, 2014 Marketing campaigns and referrals programs can reliably drive growth. But whenever you offer something of value on the Internet, some people will take advantage of your campaign. How can you balance driving growth with preventing financial loss from unchecked fraud? In this blog post, I want to share what we learned about how to discourage and respond to fraud when building marketing campaigns for e-commerce websites. I personally found there wasn’t a plethora of resources in the security community focused on the specific challenges faced by marketing campaigns, and that motivated me to try and provide more information on the topic for others — hopefully you find this helpful! Overview Since our experience came from developing a referrals program at Etsy, I want to describe our program and how we created terms of service to discourage fraud. Then rather than getting into very specific details about what we do to respond in real-time to fraud, I want to outline useful questions for anyone building these kinds of programs to ask themselves. Our Referrals Program We encouraged people to invite their friends to shop on the site. When the friend created an account on Etsy, we gave them $5.00 to spend on their first purchase. We also wanted to reward the person who invited them, as a thank-you gift. Of course, we knew that any program that gives out free money is an attractive one to try and hack! Discourage Bad Behavior from the Start First, we wanted to make our program sustainable through proactive defenses. When we designed the program we tried to bake in rules to make the program less attractive to attackers. However, we didn’t want these rules to introduce roadblocks in the product that made the program less valuable from users’ perspectives, or financially unsustainable from a business perspective. In the end, we decided on the following restrictions. We wanted there to be a minimum spend requirement for buyers to apply their discount to a purchase. We hoped that requiring buyers to put in some of their own money to get the promotion would attract more genuine shoppers and discourage fraud. We also put limits on the maximum amount we’d pay out in rewards to the person inviting their friends (though we’re keeping an eye out for any particularly successful inviters, so we can congratulate them personally). And we are currently only disbursing rewards to inviters after two of their invited friends make a purchase. Model All Possible Scenarios A key principle of Etsy’s approach to security is that it’s important to model out all possible approaches of attack, notice the ones that are easiest to accomplish or result in the biggest payouts if successful, and work on making them more difficult, so it becomes less economical for fraudsters to try those attacks. In constructing these plans, we first tried to differentiate the ways our program could be attacked and by what kind of users. Doing this, we quickly realized that we wanted to respond differently to users with a record of good behavior on the site from users who didn’t have a good history or who were redeeming many referrals, possibly with automated scripts. We also wanted to have the ability to adjust the boundaries of the two categories over time. So the second half of our defense against fraud consisted of plans on how to monitor and react to suspected fraud as well as how to prevent attackers in the worst-case scenario from redeeming large amounts of money. Steps to Develop a Mitigation Plan I have to admit upfront, I’m being a little ambiguous about what we’ve actually implemented, but I believe it doesn’t really matter since each situation will differ in the particulars. That being said, here are the questions that guided the development of our program, that could guide your thinking too. # 1. How can you determine whether two user accounts represent the same person in real life? This question is really key. In order to detect fraud on a referrals program, you need to be able to tell if the same person is signing up for multiple accounts. In our program, we kept track of many measures of similarity. One important kind of relatedness was what we called “invite relatedness.” A person looking to get multiple credits is likely to have generated multiple invites off of one original account. To check for this, and other cases, we had to keep a graph data structure of whether one user had invited another via a referral and do a breadth first search to determine related accounts. Another important type of connection between accounts is often called “fingerprint relatedness.” We keep track of fingerprints (unique identifying characteristics of an account) and user to fingerprint relationships, so we can look up what accounts share fingerprints. There are a lot of resources available about how to fingerprint accounts in the security community that I would highly recommend researching! Here’s an example of a very simple invite graph of usernames , colored by fingerprint relatedness. As you can see, the root user SarahJ28 might have invited three people, but two of them are related to her via other identifying characteristics. # 2. At what point in time are all the different signals of identity discussed in the previous question available to you? You don’t know everything about a user from the instant they land on your site. At that point in time, you might only have a little bit of information about their IP and browser.  You start to learn a bit more about them based on their email if they sign up for an account, and you certainly have more substantive information for analysis when they perform other actions on the site, like making purchases. Generally, our ability to detect identity gets stronger the more engaged someone is with the site, or the closer they move towards making a purchase. However, if someone has a credit on their account that you don’t want them to have, you need to identify them before they complete their purchase. The level of control you have over the process of purchasing will depend on how you process credit card transactions on your site. # 3. What are the different actions you could take on user accounts if you discover that one user has invited many related accounts to a referrals program? There’s generally a range of different actions that can be taken against user accounts on a site. Actions that were relevant to us included: banning user accounts, taking away currently earned promotional credits, blocking promotional credits from being applied to a purchase transaction, and taking away the ability to refer more people using our program. # 4. Do any actions need to be reviewed by a person or can they be automatic? What’s the cost of doing a manual review? On the other hand, how would a user feel if an automated action was taken against their account based on a false positive? We knew that in some cases we would feel comfortable programming automated consequences to user accounts, while in other cases we wanted manual review of the suspected account first. It was really helpful for us to work with the teams who would be reviewing the suspected accounts on this from the beginning. They had seen lots of types of fraud in the past and helped us calibrate our expectations around false positives from each type of signal. Luckily for us at Etsy, it’s quite easy to code the creation of tasks for our support team to review from any part of our web stack. I highly recommend architecting this ability if you don’t already have it for your site because it’s useful in many situations besides this one. Of course, we had to be very mindful that the real cost would come from continually reviewing the tasks over time. # 5. How can you combine what you know about identity and user behavior (at each point in time) with your range of possible actions to come up with checks that you feel comfortable with? Do these checks need to block the action a user is trying to take? We talked about each of these points where we evaluated a user’s identity and past behavior as a “check” that could either implement one of these actions I described or create a task for a person to review. This meant we also had to decide whether the check needed to block the user from completing an action, like getting a promotional credit on their account, or applying a promotional credit to a transaction, and how that action was currently implemented on the site. It’s important to note that if the user is trying to accomplish something in the scope of a single web request, there is a tradeoff between how thoroughly you can investigate someone’s identity and how quickly you can return a response to the user. After all, there are over 30 million user accounts on Etsy and we could potentially need to compare fingerprints across all of them. To solve this problem, we had to figure out how to kick off asynchronously running jobs at key points (especially checkout) that could do the analysis offline, but would nevertheless provide the protection we wanted. # 6. How visible are account statuses throughout your internal systems? Once you’ve taken an automated or manual action against an account, is that clearly marked on that user account throughout your company’s internal tools? A last point of attack may be that someone writes in complaining their promo code didn’t work. If this happens, it’s important for the person answering that email to know they’ve been flagged as under investigation or deemed fraudulent. # 7. Do you have reviews of your system scheduled for concrete dates in the future? Can you filter fraudulent accounts from your metrics when you want to? If your referrals campaign doesn’t have a concrete end date, then it’s easy to forget about how it’s performing, not just in terms of meeting business goals but in terms of fraud. It’s important to have an easy way to filter out duplicate user accounts to calculate true growth, as well as how much of an effect unchecked fraud would have had on the program and how much was spent on what was deemed an acceptable, remaining risk. If we had discovered that too much was being spent on allowing good users to get away with a few duplicate referrals, we could have tightened our guidelines and started taking away the ability to send referrals from accounts more frequently. We found that when we opened up tasks for manual review, the team member reviewing them marked them as accurate 75% of the time. This was pretty good relative to other types of tasks we review. We were also pretty generous in the end in trusting that multiple members of a household might be using the same credit card info. Conclusion Our project revealed that some fraud is catastrophic and should absolutely be prevented, while other types of fraud, like a certain level of duplicate redemptions in marketing campaigns, are less dangerous and require a gentler response or even a degree of tolerance. We have found it useful to review what could happen, design the program with rules to discourage all kinds fraud while keeping value to the user in mind, have automated checks as well as manual reviews, and monitoring that includes the ability to segment the performance of our program based on fraud rate. Many thanks to everyone at Etsy, but especially our referrals team, the risk and integrity teams, the payments team, and the security team for lots of awesome collaboration on this project! Posted by Gabrielle Gianelli on July 7, 2014 Category: engineering , monitoring , security", "date": "2014-07-7,"},
{"website": "Etsy", "title": "Device Lab Checkout – RFID Style", "author": ["Corey Benninger"], "link": "https://codeascraft.com/2014/06/24/device-lab-checkout-rfid-style/", "abstract": "Device Lab Checkout – RFID Style Posted by Corey Benninger on June 24, 2014 You may remember reading about our mobile device lab here at Etsy.  Ensuring the site works and looks proper on all devices has been a priority for all our teams. As the percentage of mobile device traffic to the site continues to increase (currently it’s more than 50% of our traffic), so does the number of devices we have in our lab. Since it is a device lab after-all, we thought it was only appropriate to trick it out with a device oriented check-out system. Something to make it easy for designers and developers to get their hands on the device they need quickly and painlessly, and a way to keep track of who had what device. Devices are now checked in and out with just two taps, or “bumps”, to an RFID reader. And before things got too high tech, we hide all the components into a custom made woodland stump created by an amazing local Etsy seller Trish Czech . “Bump the Stump” to check in and out devices from our Lab If you’re able to make it out to Velocity Santa Clara 2014 , you’ll find a full presentation on how to build a device lab . However, I’m going to focus on just the RFID aspect of our system and some of the issues we faced. RFID – What’s the frequency The first step in converting from our old paper checkout system to an RFID based one, was deciding on the correct type of RFID to use. RFID tags can come in a number of different and incompatible frequencies. In most corporate environments, if you are using RFID tags already, they will probably either be high frequency (13.54 mHz) or low frequency (125 kHz) tags. While we had a working prototype with high frequency NFC based RFID tags, we switched to low frequency since all admin already carry a low frequency RFID badge around with them. However, our badges are not compatible with a number of off the shelf RFID readers . Our solution was to basically take one of the readers off a door and wire it up to our new system. You will find that most low frequency RFID readers transmit card data using the Wiegand protocol over their wires. This protocol uses two wires, commonly labeled “DATA0” and “DATA1” to transmit the card data. The number of bits each card will transmit can vary depending on your RFID system, but lets say you had a 11 bit card number which was “00100001000”. If you monitored the Data0 line, you would see that it drops from a high signal to a low signal for 40 microseconds, for each “0” bit in the card number. The same thing happens on the Data1 line for each “1” bit.  Thus if you monitor both lines at the same time, you can read the card number. Wiegand data on the wire We knew we wanted a system that would be low powered and compact.  Thus we wired up our RFID reader to the GPIO pins of a Raspberry Pi . The Raspberry Pi was ideal for this given its small form factor, low power usage, GPIO pins, network connectivity and USB ports (we later ported this to a BeagleBone Black to take advantage of its on-board flash storage).  Besides having GPIO pins to read the Data1 and Data0 lines, the Raspberry Pi also has pins for supplying 3.3 volts and 5 volts of power. Using this, we powered the RFID reader with the 5 volt line directly from the Raspberry Pi. However, the GPIO pins for the Raspberry Pi are 3.3 volt lines, thus the 5 volt Data1 and Data0 lines from the RFID reader could damage them over time. To fix this issue, we used a logic level converter to step down the voltage before connecting Data0 and Data1 to the Raspberry Pi’s GPIO pins. RFID, Line Converter, LCD, and Raspberry Pi wiring A Need for Speed After that, it is fairly easy to write some python code to monitor for transitions on those pins using the RPi.GPIO module. This worked great for us in testing, however, we started to notice a number of incorrect RFID card numbers once we released it. The issue appeared to be that the python code would miss a bit of data from one of the lines or record the transition after a slight delay. Considering a bit is only 40 microseconds long and can happen every 2 milliseconds while a card number is being read, there’s not a lot of time to read a card. While some have reportedly used more hardware to get around this issue, we found that rewriting the GPIO monitoring code in C boosted our accuracy (using a logic analyzer, we confirmed the correct data was coming into the GPIO pins, so it was an issue somewhere after that). Gordon Henderson’s WiringPi made this easy to implement. We also added some logical error checking in the code so we could better inform the user if we happened to detect a bad RFID tag read. This included getting the correct number of bits in a time window and ensuring the most significant bits matched our known values. With python we saw up to a 20% error rate in card reads, and while it’s not perfect, getting a little closer to the hardware with C dropped this to less than 3% (of detectable errors). Dealing with Anti-Metal One other issue we ran into was RFID tags attached to devices with metallic cases. These cases can interfere with reading the RFID tags. There are a number of manufacturers which supply high frequency NFC based tag to deal with this, however, I’ve yet to find low frequency tags which have this support and are in a small form factor. Our solution is a little bit of a Frankenstein, but has worked well so far. We’ve been peeling off the shield from on-metal high frequency tag and then attaching them to the back of our standard low frequency tags. Strong fingernails, an utility knife, and a little two sided tape helps with this process. Removing anti-metal backing from NFC tags to attach to Low Frequency tags Client Code We’ve posted the client-side code for this project on github ( https://github.com/etsy/rfid-checkout ) along with a parts list and wiring diagram. This system checks devices in and out from our internal staff directory on the backend. Look to the README file ways to setup your system for handling these calls. We hope those of you looking to build a device lab, or perhaps an RFID system for checking out yarn , will find this helpful. Posted by Corey Benninger on June 24, 2014 Category: mobile , people", "date": "2014-06-24,"},
{"website": "Etsy", "title": "Opsweekly: Measuring on-call experience with alert classification", "author": ["Laurie Denness"], "link": "https://codeascraft.com/2014/06/19/opsweekly-measuring-on-call-experience-with-alert-classification/", "abstract": "Opsweekly: Measuring on-call experience with alert classification Posted by Laurie Denness on June 19, 2014 The Pager Life On-call is a tricky thing. It’s a necessary evil for employees in every tech company, and the responsibility can weigh down on you. And yet, the most common thing you hear is “monitoring sucks”, “on-call sucks” and so on. At Etsy, we’ve at least come to accept Nagios for what it is , and make it work for us. But what about on-call? And what are we doing about it? You may have heard that at Etsy, we’re kinda big into measuring everything . “If it moves, graph it, if it doesn’t move graph it anyway” is something that many people have heard me mutter quietly to myself in dark rooms for years. And yet, for on call we were ignoring that advice. We just joined up once a week as a team, to have a meeting and talk about how crappy our on-call was, and how much sleep we lost. No quantification, no action items. Shouldn’t we be measuring this? Introducing Opsweekly And so came Opsweekly . Our operations team was growing, and we needed a good place to finally formalise Weekly Reports.. What is everyone doing? And at the same time, we needed to progress and start getting some graphs behind our on-call experiences. So, we disappeared for a few days and came back with some PHP, and Opsweekly was born. What does Opsweekly do? In the simplest form, Opsweekly is a great place for your team to get together, share their weekly updates, and then organise a meeting to discuss those reports (if necessary) and take notes on it. But the real power comes from Opsweekly’s built in on call classification and optional sleep tracking. Every week, your on-call engineer visits Opsweekly, hits a big red “I was on call” button, and Opsweekly pulls in the notifications that they have received in the last week. This can be from whichever data source you desire; Maybe your Nagios instance logs into Logstash or Splunk, or you use Pagerduty for alerting. An example of on-call report mostly filled in The engineer can make a simple decision from a drop down about what category the alert falls into. The list of alert classifications the user can choose from We were very careful when we designed this list to ensure that every alert type was catered for, but also minimising the amount of choices the engineer had to try and decide from. The most important part here is the overall category choice on the left: Action Taken vs No Action Taken One of the biggest complaints about on call was the noise. What is the signal to noise ratio of your alert system? Well, now we’re measuring that using Opsweekly. Percentage of Action vs No Action alerts over the last year This is just one of the many graphs and reports that Opsweekly can generate using the data that was entered, but this is one of the key points for us: We’ve been doing this for a year and we are seeing an increasingly improving signal to noise ratio. Measuring and making changes based on that can work for your on-call too. The value of historical context So how does this magic happen? By having to make conscious choices about whether the alert was meaningful or not, we can start to make improvements based on that. Move alerts to email only if they’re not urgent enough to be dealt with immediately/during the night. If the threshold needs adjusting, this serves as a reminder to actually go and adjust the threshold; you’re unlikely to remember or want to do it when you’ve just woken up, or you’ve been context switched. It’s all about surfacing that information. Alongside the categorisation is a “Notes” field for every alert. A quick line of text in each of these boxes provides invaluable data to other people later on (or maybe yourself!) to gain context about that alert. Opsweekly has search built in that allows you to go back and inspect the alert time(s) that alert fired, gaining that knowledge of what each previous person did to resolve the alert before you. Sleep Tracking A few months in, we were inspired by an Ignite presentation at Velocity Santa Clara about measuring humans. We were taken aback… How was this something we didn’t have? Now we realised we could have graphs of our activity and sleep, we managed to go a whole 2 days before we got to the airport for the flight home to start purchasing the increasingly common off the shelf personal monitoring devices. Ryan Frantz wrote here about getting that data available for all to share on our dashboards, using conveniently accessible APIs, and it wasn’t long until it clicked that we could easily query that data when processing on call notifications to get juicy stats about how often people are woken up. And so we did: Report in Opsweekly showing sleep data for an engineers on-call week Personal Feedback The final step of this is helping your humans understand how they can make their lives better using this data. Opsweekly has that covered too; a personal report for each person Available on Github now For more information on how you too can start to measure real data about your on call experiences, read more and get Opsweekly now on Github Velocity Santa Clara 2014 If you’re attending Velocity in Santa Clara, CA next week, Ryan and I are giving a talk about our Nagios experiences and Opsweekly, entitled “Mean Time to Sleep: Quantifying the On-Call Experience” . Come and find us if you’re in town! Ryan Frantz and Laurie Denness now know their co-workers sleeping patterns a little too well… Posted by Laurie Denness on June 19, 2014 Category: infrastructure , monitoring , operations , people Tags: etsy , Nagios , Opsweekly Related Posts Posted by Duncan Gillespie and Ben Russell on 22 Mar, 2016 Building a Translation Memory to Improve Machine Translation Coverage and Quality Posted by Dan Miller on 22 Dec, 2014 We Invite Everyone at Etsy to Do an Engineering Rotation: Here’s why Posted by John Marc Imbrescia on 26 Mar, 2013 There and Back Again: Migrating Geolocation Data to GeoNames", "date": "2014-06-19,"},
{"website": "Etsy", "title": "Introducing nagios-herald", "author": ["Ryan Frantz"], "link": "https://codeascraft.com/2014/06/06/introducing-nagios-herald/", "abstract": "Introducing nagios-herald Posted by Ryan Frantz on June 6, 2014 Alert Design Alert design is not a solved problem. And it interests me greatly. What makes for a good alert? Which information is most relevant when a host or service is unavailable? While the answer to those, and other, questions depends on a number of factors (including what the check is monitoring, which systems and services are deemed critical, what defines good performance, etc.), at a minimum, alerts should contain some amount of appropriate context to aid an on-call engineer in diagnosing and resolving an alerting event. When writing Nagios checks, I ask the following questions to help suss out what may be appropriate context: How can this alert better define/illustrate what is broken? What threshold was met? What is the measured value versus the threshold? What assumptions were made when the check was written? Could exposing those assumptions aid in resolving the issue? What things can the operator look into, read, or test to better understand what’s happening? Can any of those things be automated or embedded in the alert? On the last point, about automating work, I believe that computers can, and should, do as much work as possible for us before they have to wake us up . To that end, I’m excited to release nagios-herald today! nagios-herald: Rub Some Context on It nagios-herald was created from a desire to supplement an on-call engineer’s awareness of conditions surrounding a notifying event. In other words, if a computer is going to page me at 3AM, I expect it to do some work for me to help me understand what’s failing. At its core, nagios-herald is a Nagios notification script. The power, however, lies in its ability to add context to Nagios alerts via formatters . One of the best examples of nagios-herald in action is comparing the difference between disk space alerts with and without context. Disk Space Alert I’ve got a vague idea of which volume is problematic but I’d love to know more. For example, did disk space suddenly increase? Or did it grow gradually, only tipping the threshold as my head hit the pillow? Disk Space Alert, Now *with* Context! In the example alert above, a stack bar clearly illustrates which volume the alert has fired on. It includes a Ganglia graph showing the gradual increase in disk storage over the last 24 hours. And the output of the df command is highlighted, helping me understand which threshold this check exceeded. For more examples of nagios-herald adding context, see the example alerts page in the GitHub repo. “I Have Great Ideas for Formatters!” I’m willing to bet that at some point, you looked at a Nagios alert and thought to yourself, “Gee, I bet this would be more useful if it had a little more information in it…”  Guess what? Now it can! Clone the nagios-herald repo, write your own custom formatters , and configure nagios-herald to load them. I look forward to feedback from the community and pull requests! Ryan tweets at @Ryan_Frantz and blogs at ryanfrantz.com . Posted by Ryan Frantz on June 6, 2014 Category: engineering , infrastructure , monitoring , operations , people", "date": "2014-06-6,"},
{"website": "Etsy", "title": "Q1 2014 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/05/15/q1-2014-site-performance-report/", "abstract": "Q1 2014 Site Performance Report Posted by Lara Hogan on May 15, 2014 May flowers are blooming, and we’re bringing you the Q1 2014 Site Performance Report. There are two significant changes in this report: the synthetic numbers are from Catchpoint instead of WebPagetest, and we’re going to start labeling our reports by quarter instead of by month going forward. The backend numbers for this report follow the trend from December 2013 – performance is slightly up across the board. The front-end numbers are slightly up as well, primarily due to experiments and redesigns. Let’s dive into the data! Server Side Performance Here are the median and 95th percentile load times for signed in users on our core pages on Wednesday, April 23rd: There was a small increase in both median and 95th percentile load times over the last three months across the board, with a larger jump on the homepage. We are currently running a few experiments on the homepage, one of which is significantly slower than other variants, which is bringing up the 95th percentile. While we understand that this may skew test results, we want to get preliminary results from the experiment before we spend engineering effort on optimizing this variant. As for the small increases everywhere else, this has been a pattern over the last six months, and is largely due to new features adding a few milliseconds here and there, increased usage from other countries (translating the site has a performance cost), and overall added load on our infrastructure.  We expect to see a slow increase in load time for some period of time, followed by a significant dip as we upgrade or revamp pieces of our infrastructure that are suffering. As long as the increases aren’t massive this is a healthy oscillation, and optimizes for time spent on engineering tasks. Synthetic Front-end Performance Because of some implementation details with our private WebPagetest instance, the data we have for Q1 isn’t consistent and clean enough to provide a true comparison between the last report and this one.  The good news is that we also use Catchpoint to collect synthetic data, and we have data going back to well before the last report.  This enabled us to pull the data from mid-December and compare it to data from April, on the same days that we pulled the server side and RUM data. Our Catchpoint tests are run with IE9 only, and they run from New York, London, Chicago, Seattle, and Miami every two hours.  The “Webpage Response” metric is defined as the time it took from the request being issued to receiving the last byte of the final element on the page.  Here is that data: The increase on the homepage is somewhat expected due to the experiments we are running and the increase in the backend time. The search page also saw a large increase both Start Render and Webpage Response, but we are currently testing a completely revamped search results page, so this is also expected.  The listing page also had a modest jump in start render time, and again this is due to differences in experiments that were running in December vs. April. Real User Front-end Performance As always, these numbers come from mPulse , and are measured via JavaScript running in real users’ browsers: No big surprises here, we see the same bump on the homepage and the search results page that we did in the server side and synthetic numbers. Everything else is essentially neutral, and isn’t particularly exciting. In future reports we are going to consider breaking this data out by region, by mobile vs. desktop, or perhaps providing other percentiles outside of the median (which is the 50th percentile). Conclusion We are definitely in a stage of increasing backend load time and front-end volatility due to experiments and general feature development. The performance team has been spending the past few months focusing on some internal tools that we hope to open source soon, as well as running a number of experiments ourselves to try to find some large perf wins. You will be hearing more about these efforts in the coming months, and hopefully some of them will influence future performance reports! Finally, our whole team will be at Velocity Santa Clara this coming June, and Lara , Seth , and I are all giving talks.  Feel free to stop us in the hallways and say hi! Posted by Lara Hogan on May 15, 2014 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2014-05-15,"},
{"website": "Etsy", "title": "Web Experimentation with New Visitors", "author": ["Diego Alonso"], "link": "https://codeascraft.com/2014/04/03/web-experimentation-with-new-visitors/", "abstract": "Web Experimentation with New Visitors Posted by Diego Alonso on April 3, 2014 We strive to build Etsy with science , and therefore love how web experimentation and A/B testing help us drive our product development process . Several months ago we started a series of web experiments in order to improve Etsy’s homepage experience for first-time visitors. Testing against a specific population, like first-time visitors, allowed us to find issues and improve our variants without raising concerns in our community. This is how the page used to look for new visitors: We established both qualitative and quantitative goals to measure improvements for the redesign. On the qualitative side, our main goal was to successfully communicate to new buyers that Etsy is a global marketplace made by people. On the quantitative side, we primarily cared about three metrics: bounce rate, conversion rate, and retention over time. Our aim was to reduce bounce rate (percentage of visits who leave the site after viewing the homepage) without affecting conversion rate (proportion of visits that resulted in a purchase) and visit frequency. After conducting user surveys, usability tests, and analyzing our target web metrics, we have finally reached those goals and launched a better homepage for new visitors. Here’s what the new homepage looks like: Bucketing New Visitors This series of web experiments marked the first time at Etsy where we tried to consistently run an experiment only for first-time visitors over a period of time. While identifying a new visitor is relatively straightforward, the logic to present that user with the same experience on subsequent visits is something less trivial. Bucketing a Visitor At Etsy we use our open source Feature API for A/B testing. Every visitor is assigned a unique ID when they arrive to the website for the first time. In order to determine in which bucket of a test the visitor belongs to, we generate a deterministic hash using the visitor’s unique ID and the experiment identifier. The main advantage of using this hash for bucketing is that we don’t have to worry about creating or managing multiple cookies every time we bucket a visitor into an experiment. Identifying New Visitors One simple way to identify a new visitor is by the absence of etsy.com cookies in the browser. On our first set of experiments we checked for the existence of the __utma cookie from Google Analytics, which we also used to define visits in our internal analytics stack. Returning New Visitors Before we define a returning new visitor, we need first to describe the concept of a visit. We use the Google Analytics visit definition , where a visit is a group of user interactions on our website within a given time frame. One visitor can produce multiple visits on the same day, or over the following days, weeks, or months. In a web experiment, the difference between a returning visitor and a returning new visitor is the relationship between the experiment start time and the visitor’s first landing time on the website. To put it simply, every visitor who landed on the website for the first time after the experiment start date will be treated as a new visitor, and will consistently see the same test variant on their first and subsequent visits. As I mentioned before, we used the __utma cookie to identify visitors. One advantage of this cookie is that it tracks the first time a visitor landed on the website. Since we have access to the first visit start time and the experiment start time, we can determine if a visitor is eligible to see an experiment variant. In the following diagram we show two visitors and their relation with the experiment start time. Feature API We added the logic to compare a visitor’s first landing time against an experiment start time as part of our internal Feature API. This way it’s really simple to set up web experiments targeting new visitors. Here is an example of how we set up an experiment configuration and an API entry point. Configuration Set-up: $server_config['new_homepage'] => [\r\n   'enabled' => 50,\r\n   'eligibility' => [\r\n       'first_visit' => [\r\n           'after' => TIMESTAMP\r\n       ]\r\n   ]\r\n]; API Entry Point: if (Feature::isEnabled('new_homepage')) {\r\n   $controller = new Homepage_Controller();\r\n   $controller->renderNewHomepage();\r\n} Unforeseen Events When we first started analyzing the test results, we found that more than 10% of the visitors in the experiment had first visit landing times prior to our experiment start day. This suggested that old, seasoned Etsy users were being bucketed into this experiment. After investigating, we were able to correlate those visits to a specific browser: Safari 4+. The visits were a result of the browser making requests to generate thumbnail images for the Top Sites feature. These type of requests are generated any time a user is on the browser, even without visiting Etsy. On the web analytics side, this created a visit with a homepage view followed by an exit event. Fortunately, Safari provides a way to identify these requests using the additional HTTP header “X-Purpose: preview”. Finally, after filtering these requests, we were able to correct this anomaly in our data. Below you can see the experiment’s bounce rates significantly decreased after getting rid of these automated visits. Although verifying the existence of cookies to determine whether a visitor is new may seem trivial, it is hard to be completely certain that a visitor has never been to your website before based on this signal alone. One person can use multiple browsers and devices to view the same website: mobile, tablet, work or personal computer, or even borrow any other device from a friend. Here is when more deep analysis can come in handy, like filtering visits using attributes such as user registration and signed-in events. Conclusions We are confident that web experimentation with new visitors is a good way to collect unbiased results and to reduce product development concerns such as disrupting existing users’ experiences with experimental features. Overall, this approach allows us to drive change. Going forward, we will use what we learned from these experiments as we develop new iterations of the homepage for other subsets of our members. Now that all the preparatory work is done, we can ramp-up this experiment, for instance, to all signed-out visitors. You can follow Diego on Twitter at @gofordiego Posted by Diego Alonso on April 3, 2014 Category: data , engineering Tags: analytics , data Related Posts Posted by David Schott on 07 Nov, 2018 Double-bucketing in A/B Testing Posted by Callie McRee and Kelly Shen on 03 Oct, 2018 How Etsy Handles Peeking in A/B Testing Posted by Daniel Marks , Bill Ulammandakh and Sungwan Jo on 25 Jan, 2017 Optimizing Meta Descriptions, H1s and Title Tags: Lessons from Multivariate SEO Testing at Etsy", "date": "2014-04-3,"},
{"website": "Etsy", "title": "Responsive emails that really work", "author": ["Kevin Gessner"], "link": "https://codeascraft.com/2014/03/13/responsive-emails-that-really-work/", "abstract": "Responsive emails that really work Posted by Kevin Gessner on March 13, 2014 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “allowlist/blocklist” in our Code as Craft entries. If you’ve ever written an HTML email, you’ll know that the state of the art is like coding for the web 15 years ago: tables and inline styles are the go-to techniques, CSS support is laughably incomplete , and your options for layout have none of the flexibility that you get on the “real web”. Just like everywhere online, more and more people are using mobile devices to read their email.  At Etsy, more than half of our email opens happen on a mobile device!  Our desktop-oriented, fixed-width designs are beautiful, but mobile readers aren’t getting the best experience. We want our emails to provide a great experience for everyone, so we’re experimenting with new designs that work on every client: iPhone, iPad, Android, Gmail.com, Outlook, Yahoo Mail, and more.  But given the sorry state of CSS and HTML for email, how can we make an email look great in all those places? Thanks to one well-informed blog commenter and tons of testing across many devices we’ve found a way to make HTML emails that work everywhere.  You get a mobile-oriented design on phones, a desktop layout on Gmail, and a responsive, fluid design for tablets and desktop clients.  It’s the best of all worlds—even on clients that don’t support media queries. A New Scaffolding I’m going to walk you through creating a simple design that showcases this new way of designing HTML emails.  It’s a two-column layout that wraps to a single column on mobile: For modern browsers, this would be an easy layout to implement—frameworks like Bootstrap provide layouts like this right out of the box.  But the limitations of HTML email make even this simple layout a challenge. Client Limitations What limitations are we up against? Android’s Gmail.app only supports inline CSS in HTML style attributes—no <style> tags, no media queries, and no external stylesheets. Gmail.com supports a limited subset of HTML, stripping out many tags (including all of the tags in HTML5) and attributes (including classes and IDs), and only allows some inline CSS and a very limited subset in <style> tags (more on this later). The iOS and Mac OS X Mail apps support media queries and a large selection of CSS.  Of all clients, these are most like a modern browser. On to the Code Let’s start with a simple HTML structure. <html>\n <body>\n   <table cellpadding=0 cellspacing=0><tr><td>\n     <table cellpadding=0 cellspacing=0><tr><td>\n       <div>\n         <h1>Header</h1>\n       </div>\n       <div>\n         <h2>Main Content</h2>\n         <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec gravida sem dictum, iaculis magna ornare, dignissim elit.</p>\n         <p>...</p>\n       </div>\n       <div>\n         <h2>Sidebar</h2>\n         <p>Donec tincidunt tincidunt nunc, eget pulvinar risus sodales eu.</p>\n       </div>\n       <div>\n         <p>Footer</p>\n       </div>\n     </td></tr></table>\n   </td></tr></table>\n </body>\n</html> It’s straightforward: a header and footer with two content areas between, main content and a sidebar.  No fancy tags, just divs and tables and paragraphs—we’re still partying like it’s 1999.  (As we apply styling, we’ll see why both wrapping tables are required.) Initial Styling Android is the least common denominator of CSS support, allowing only inline CSS in style attributes and ignoring all other styles.  So let’s add inline CSS that gives us a mobile-friendly layout of a fluid single column: <html>\n <body style=\"margin: 0; padding: 0; background: #ccc;\">\n   <table cellpadding=0 cellspacing=0 style=”width: 100%;” ><tr><td style=”padding: 12px 2%;” >\n     <table cellpadding=0 cellspacing=0 style=”margin: 0 auto; background: #fff; width: 96%;” ><tr><td style=”padding: 12px 2%;” >\n     <div>\n       <h1>Header</h1>\n     </div>\n     <div>\n       <h2 style=”margin-top: 0;” >Main Content</h2>\n       <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec gravida sem dictum, iaculis magna ornare, dignissim elit.</p>\n       <p>...</p>\n     </div>\n     <div>\n       <h2 style=”margin-top: 0;” >Sidebar</h2>\n       <p>Donec tincidunt tincidunt nunc, eget pulvinar risus sodales eu.</p>\n     </div>\n     <div style=”border-top: solid 1px #ccc;” >\n       <p>Footer</p>\n     </div>\n     </td></tr></table>\n   </td></tr></table>\n </body>\n</html> It honestly doesn’t look that different from the unstyled HTML (but the groundwork is there for your beautiful ideas!).  The table-within-a-table wrapping all the content lets us have our content area in a colored background, with a small (2%) gutter on each side.  Don’t forget the cellspacing and cellpadding attributes, too, or you’ll get extra spacing that can’t be removed with CSS! Dealing with Gmail This design is certainly adequate for both mobile and desktop clients, but it’s not the best we can do.  Desktop clients and large tablets have a lot of screen real estate that we’re wasting. Our main target here is Gmail—desktop and laptop screens keep getting bigger, and we want Gmail users to get a full-width experience.  But Gmail doesn’t support media queries, the go-to way of showing different layouts on different-sized clients.  What can we do? I mentioned earlier that Gmail supports a small subset of CSS inside <style> tags.  This is not a widely known feature of Gmail—most resources you’ll find tell you that Gmail only supports inline styles.  Only a handful of blog comments and forum posts mention this support.  I don’t know when Gmail’s CSS support was quietly improved, but I was certainly pleased to learn about this new way of styling my emails. The subset of CSS that Gmail supports is that you are limited to only using tag name selectors—no classes or IDs are supported.  Coupled with Gmail’s limited allowlist of HTML elements, your flexibility in styling different parts of your email differently is severely limited.  Plus, the <style> tag must be in the <head> of your email, not in the <body>. The trick is to make judicious use of CSS’s structural selectors: the descendant, adjacent, and child selectors .  By carefully structuring your HTML and mixing and matching these selectors, you can pinpoint elements for providing styles.  Here are the styles I’ve applied to show a two-column layout in Gmail: <head>\n          <style type=\"text/css\">\n/*  1 */    table table {\n/*  2 */      width: 600px !important;\n/*  3 */    }\n/*  4 */    table div + div { /* main content */\n/*  5 */      width: 65%;\n/*  6 */      float: left;\n/*  7 */    }\n/*  8 */    table div + div + div { /* sidebar */\n/*  9 */      width: 33%;\n/* 10 */      float: right;\n/* 11 */    }\n/* 12 */    table div + div + div + div { /* footer */\n/* 13 */      width: 100%;\n/* 14 */      float: none;\n/* 15 */      clear: both;\n/* 16 */    }\n          </style>\n        </head> In the absence of classes and IDs to tell you what elements are being styled, comments are your friend!  Let’s walk through each of these selectors. Lines 1-3 lock our layout to a fixed width.  Remember, this is our style for Gmail on the desktop, where a fluid design isn’t our goal.  We apply this to the inner wrapping table so that padding on the outer one remains, around our 600-pixel-wide content.  Without having both tables, we’d lose the padding that keeps our content from running into the client’s UI. Next, we style the main content.  The selector on line 4, reading right to left, finds a div immediately following another div, inside a table.  That actually matches the main content, sidebar, and footer divs, but that’s OK for now.  We style it to take up the left two thirds of the content area (minus a small gutter between the columns). The selector on line 8 styles the sidebar, by finding a div following a div following a div, inside a table. This selects both the footer and the sidebar, but not the main content, and overrides the preceding styles, placing the sidebar in the right-hand third of the content. Finally, we select the footer on line 12—the only div that follows three others—and make it full-width.  Since the proceeding selectors and styles also applied to this footer div, we need to reset the float style back to none (on line 14). With that, we have a two-column fixed layout for Gmail, without breaking the one-column view for Android: The styles we applied to the outer wrapping table keep our content centered, and the other inline styles that we didn’t override (such as the line above the footer) are still rendered. For Modern Browsers Finally, let’s consider Mail.app on iOS and Mac OS X.  I’m lumping them together because they have similar rendering engines and CSS support—the media queries and full CSS selectors you know and love all work.  The styles we applied for Gmail will be also applied on iPhones, giving a mobile-unfriendly fixed-width layout.  We want Android’s single-column fluid layout instead.  We can target modern, small-screen clients (like iPhones) with a media query: /* Inside <style> in the <head> */\n@media (max-width: 630px) {\n  table table {\n    width: 96% !important;\n  }\n  table div {\n    float: none !important;\n    width: 100% !important;\n  }\n} These styles override the earlier ones to restore the single-column layout, but only for devices under 630 pixels wide—the point at which our fixed 600-pixel layout would begin to scroll horizontally.  Don’t forget the !important flag, which makes these styles override the earlier ones.  Gmail.com and Android will both ignore this media query.  iPads and Mail.app on the desktop, which are wider than 630 pixels, will also show the desktop style. This is admittedly not the prettiest approach. With multiple levels of overriding selectors, you need to think carefully about the impact of any change to your styles.  As your design grows in complexity, you need to keep a handle on which elements will be selected where, particularly with the tag-only selectors for Gmail.  But it’s nearly the holy grail of HTML email: responsive, flexible layouts even in clients with limited support for CSS. The biggest caveat of this approach (besides the complexity of the code) is the layout on Android tablets: they will display the same single-column layout as Android phones.  For us (and probably for you, too), Android tablets are a vanishingly small percentage of our users.  In any case, the layout isn’t unusable, it’s just not optimal, with wide columns and needlessly large images. Bringing it All Together You can find the complete code for this example in this gist: https://gist.github.com/kevingessner/9509148 You can extend this approach to build all kinds of complex layouts.  Just keep in mind the three places where every item might be styled: inline CSS, tag-only selectors in a <style> tag, and one or more media query blocks.  Apply your styles carefully, and don’t forget to test your layout in every client you can your hands on! I hope that in the future, Gmail on the web and on Android will enter the 21st century and add support for the niceties that CSS has added in recent years.  Until then, creating an HTML email that looks great on every client will continue to be a challenge.  But with a few tricks like these up your sleeve, you can make a beautiful email that gives a great experience for everyone on every client. You can follow me on Twitter at @kevingessner Want to help us make Etsy better, from email to accounting? We’re hiring! Posted by Kevin Gessner on March 13, 2014 Category: engineering , mobile", "date": "2014-03-13,"},
{"website": "Etsy", "title": "Etsy’s Journey to Continuous Integration for Mobile Apps", "author": ["Nassim Kammah"], "link": "https://codeascraft.com/2014/02/28/etsys-journey-to-continuous-integration-for-mobile-apps/", "abstract": "Etsy’s Journey to Continuous Integration for Mobile Apps Posted by Nassim Kammah on February 28, 2014 Positive app reviews can greatly help in user conversion and the image of a brand. on the other hand, bad reviews can have dramatic consequences; as Andy Budd puts it: “Mobile apps live and die by their ratings in an App Store”. The above reviews are actual reviews of the Etsy iOS App. As an Etsy developer, it is sad to read them, but it’s a fact: bugs sometimes sneak through our releases. On the web stack, we use our not so secret weapon of Continuous Delivery as a safety net to quickly address bugs that make it to production. However, releasing mobile apps requires a 3rd party’s approval (the app store) , which takes five days on average; once an app is approved, users decide when to upgrade – so they may be stuck with older versions. Based on our analytics data, we currently have 5 iOS and 10 Android versions currently in use by the public. Through Continuous Integration (CI), we can detect and fix major defects in the development and validation phase of the project, before they negatively impact user experience: this post explores Etsy’s journey to implementing our CI pipeline for our android and iOS applications. “Every commit should build the mainline on an integration machine” This fundamental CI principle is the first step to detecting defects as soon as they are introduced: failing to compile. Building your app in an IDE does not count as Continuous Integration. Thankfully, both iOS and Android are command line friendly: building a release of the iOS app is as simple as running: xcodebuild -scheme \"Etsy\" archive Provisioning integration machines Integration machines are separate from developer machines – they provide a stable, controlled, reproducible environment for builds and tests. Ensuring that all the integration machines are identical is critical – using a provisioning framework to manage all the dependencies is a good solution to ensure uniformity and scalability. At Etsy, we are pretty fond of Chef to manage our infrastructure – we naturally turned to it to provision our growing Mac Mini fleet. Equipped with the homebrew cookbook for installing packages and rbenv cookbook for managing the ruby environment in a relatively sane way, our sysops wizard Jon Cowie sprinkled a few hdiutil incantations (to manage disk images) and our cookbooks were ready. We are now able to programmatically install 95% of Xcode (some steps are still manual), Git, and all the Android packages required to build and run the tests for our apps. Lastly, if you ever had to deal with iOS provisioning profiles, you can relate to how annoying they are to manage and keep up to date; having a centralized system that manages all our profiles saves a lot of time and frustration for our engineers. Building on push and providing daily deploys With our CI machines hooked up to our Jenkins server, setting up a plan to build the app on every git push is trivial. This simple step helps us detect missing files from commits or compilation issues multiple times a week – developers are notified in IRC or by email and build issues are addressed minutes after being detected. Besides building the app on push, we provide a daily build that any Etsy employee can install on their mobile device – the quintessence of dogfooding. An easy way to encourage our coworkers to install pre-release builds is to nag them when they use the app store version of the app. Testing iOS devices come in many flavors, with seven different iPads, five iPhones and a few iPods; when it comes to Android, the plethora of devices becomes overwhelming. Even when focusing on the top tier of devices, the goal of CI is to detect defects as soon as they are introduced: we can’t expect our QA team to validate the same features over and over on every push! Our web stack boasts a pretty extensive collection of test suites and the test driven development culture is palpable. Ultimately, our mobile apps leverage a lot of our web code base to deliver content: data is retrieved from the API and many screens are web views. Most of the core logic of our apps rely on the UI layer – which can be tested with functional tests. As such, our first approach was to focus on some functional tests, given that the API was already tested on the web stack (with unit tests and smoke tests). Functional tests for mobile apps are not new and the landscape of options is still pretty extensive; in our case, we settled down on Calabash and Cucumber . The friendly format and predefined steps of Cucumber + Calabash allows our QA team to write tests themselves without any assistance from our mobile apps engineers. To date, our functional tests run on iPad/iPhone iOS 6 and 7 and Android, and cover our Tier 1 features, including: searching for listings and shops registering new accounts purchasing an item with a credit card or a gift card Because functional tests mimic the steps of an actual user, the tests require that certain assumed resources exist. In the case of the Checkout test, these are the following: a dedicated test buyer account a dedicated seller test account a prepaid credit card associated with the account Our checkout test then consists of: signing in to the app with our test buyer account searching for an item (in the seller test account shop) adding it to the cart paying for the item using the prepaid credit card Once the test is over, an ad-hoc mechanism in our backend triggers an order cancellation and the credit card is refunded. A great example of our functional tests catching bugs is highlighted in the following screenshot from our iPad app: Our registration test navigates to this view, and fills out all the visible fields. Additionally, the test cycles through the “ Female “, “ Male ” and “ Rather Not Say ” options; in this case, the tests failed (since the “ male ” option was missing). By running our test suite every time an engineer pushes code, we not only detect bugs as soon as they are introduced, we detect app crashes. Our developers usually test their work on the latest OS version but Jenkins has their back: our tests run simultaneously across different combinations of devices and OS versions. Testing on physical devices While our developers enjoy our pretty extensive device lab for manual testing, maintaining a set of devices and constantly running automated tests on them is a logistical nightmare and a full time job. After multiple attempts at developing an in-house solution, we decided to use Appthwack to run our tests on physical devices. We run our tests for every push on a set of dedicated devices, and run nightly regression on a broader range of devices by tapping into Appthwack cloud of devices. This integration is still very recent and we’re still working out some kinks related to testing on physical devices and the challenges of aggregating and reporting test status from over 200 devices. Reporting: put a dashboard on it With more than 15 Jenkins jobs to build and run the tests, it can be challenging to quickly surface critical information to the developers. A simple home grown dashboard can go a long way to communicating the current test status across all configurations: Static analysis and code reviews Automated tests cannot catch all bugs and potential crashes – similar to the web stack, developers rely heavily on code reviews prior to pushing their code. Like all code at Etsy, the apps are stored in Github Enterprise repositories, and code reviews consist of a pull request and an issue associated to it. By using the GitHub pull request builder Jenkins plugin , we are able to systematically trigger a build and do some static analysis (see static analysis with OCLint post ) on the review request and post the results to the Github issue: Infrastructure overview summary All in all, our current infrastructure looks like the following: Challenges and next steps Building our continuous integration infrastructure was strenuous and challenges kept appearing one after another, such as the inability to automate the installation of some software dependencies. Once stable, we always have to keep up with new releases (iOS 7, Mavericks) which tend to break the tests and the test harness. Furthermore, functional tests are flaky by nature, requiring constant care and optimization. We are currently at a point where our tests and infrastructure are reliable enough to detect app crashes and tier 1 bugs on a regular basis. Our next step, from an infrastructure point of view, is to expand our testing to physical devices via our test provider Appthwack. The integration has just started but already raises some issues: how can we concurrently run the same checkout test (add an item to cart, buy it using a gift card) across 200 devices – will we create 200 test accounts, one per device? We will post again on our status 6 months from now, with hopefully more lessons learned and success stories – stay tuned! You can follow Nassim on Twitter at @kepioo Posted by Nassim Kammah on February 28, 2014 Category: engineering , infrastructure , mobile", "date": "2014-02-28,"},
{"website": "Etsy", "title": "Reducing Domain Sharding", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/02/19/reducing-domain-sharding/", "abstract": "Reducing Domain Sharding Posted by Lara Hogan on February 19, 2014 This post originally appeared on the Perf Planet Performance Calendar on December 7th, 2013. Domain sharding has long been considered a best practice for pages with lots of images.  The number of domains that you should shard across depends on how many HTTP requests the page makes, how many connections the client makes to each domain, and the available bandwidth.  Since it can be challenging to change this dynamically (and can cause browser caching issues), people typically settle on a fixed number of shards – usually two. An article published earlier this year by Chromium contributor William Chan outlined the risks of sharding across too many domains, and Etsy was called out as an example of a site that was doing this wrong.  To quote the article: “Etsy’s sharding causes so much congestion related spurious retransmissions that it dramatically impacts page load time.”  At Etsy we’re pretty open with our performance work, and we’re always happy to serve as an example.  That said, getting publicly shamed in this manner definitely motivated us to bump the priority of reinvestigating our sharding strategy. Making The Change The code changes to support fewer domains were fairly simple, since we have abstracted away the process that adds a hostname to an image path in our codebase.  Additionally, we had the foresight to exclude the hostname from the cache key at our CDNs, so there was no risk of a massive cache purge as we switched which domain our images were served on.  We were aware that this would expire the cache in browsers, since they do include hostname in their cache key, but this was not a blocker for us because of the improved end result.  To ensure that we ended up with the right final number, we created variants for two, three, and four domains.  We were able to rule out the option to remove domain sharding entirely through synthetic tests.  We activated the experiment in June using our A/B framework , and ran it for about a month. Results After looking at all of the data, the variant that sharded across two domains was the clear winner.  Given how easy this change was to make, the results were impressive: 50-80ms faster page load times for image heavy pages (e.g. search), 30-50ms faster overall. Up to 500ms faster load times on mobile. 0.27% increase in pages per visit. As it turns out, William’s article was spot on – we were sharding across too many domains, and network congestion was hurting page load times.  The new CloudShark graph supported this conclusion as well, showing a peak throughput improvement of 33% and radically reduced spurious retransmissions: Before – Four Shards After – Two Shards Lessons Learned This story had a happy ending, even though in the beginning it was a little embarrassing.  We had a few takeaways from the experience: The recommendation to shard across two domains still holds for us (YMMV depending on page composition). Make sure that your CDN is configured to leave hostname out of the cache key.  This was integral to making this change painless. Abstract away the code that adds a hostname to an image URI in your code. Measure everything , and question assumptions about existing decisions. Tie performance improvements to business metrics – we were able to tell a great story about the win we had with this change, and feel confident that we made the right call. Segment your data across desktop and mobile, and ideally international if you can.  The dramatic impact on mobile was a huge improvement which would have been lost in an aggregate number. Until SPDY/HTTP 2.0 comes along, domain sharding can still be a win for your site , as long as you test and optimize the number of domains to shard across for your content. Posted by Lara Hogan on February 19, 2014 Category: performance", "date": "2014-02-19,"},
{"website": "Etsy", "title": "December 2013 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2014/01/23/december-2013-site-performance-report/", "abstract": "December 2013 Site Performance Report Posted by Lara Hogan on January 23, 2014 It’s a new year, and we want to kick things off by filling you in on site performance for Q4 2013. Over the last three months front-end performance has been pretty stable, and backend load time has increased slightly across the board. Server Side Performance Here are the median and 95th percentile load times for signed in users on our core pages on Wednesday, December 18th: There was an across the board increase in both median and 95th percentile load times over the last three months, with a larger jump on our search results page. There are two main factors that contributed to this increase: higher traffic during the holiday season and an increase in international traffic, which is slower due to translations. On the search page specifically, browsing in US English is significantly faster than any other language. This isn’t a sustainable situation over the long term as our international traffic grows, so we will be devoting significant effort to improving this over the next quarter. Synthetic Front-end Performance As usual, we are using our private instance of WebPagetest to get synthetic measurements of front-end load time. We use a DSL connection and test with IE8, IE9, Firefox, and Chrome. The main difference with this report is that we have switched from measuring Document Complete to measuring Speed Index , since we believe that it provides a better representation of user perceived performance. To make sure that we are comparing with historical data, we pulled Speed Index data from October for the “old” numbers. Here is the data, and all of the numbers are medians over a 24 hour period: Start render didn’t really change at all, and speed index was up on some pages and down on others. Our search results page, which had the biggest increase on the backend, actually saw a 0.2 second decrease in speed index. Since this is a new metric we are tracking, we aren’t sure how stable it will be over time, but we believe that it provides a more accurate picture of what our visitors are really experiencing. One of the downsides of our current wpt-script setup is that we don’t save waterfalls for old tests – we only save the raw numbers. Thus when we see something like a 0.5 second jump in Speed Index for the shop page, it can be difficult to figure out why that jump occurred. Luckily we are Catchpoint customers as well, so we can turn to that data to get granular information about what assets were on the page in October vs. December. The data there shows that all traditional metrics (render start, document complete, total bytes) have gone down over the same period. This suggests that the jump in speed index is due to loading order, or perhaps a change in what’s being shown above the fold. Our inability to reconcile these numbers illustrates a need to have visual diffs, or some other mechanism to track why speed index is changing. Saving the full WebPagetest results would accomplish this goal, but that would require rebuilding our EC2 infrastructure with more storage – something we may end up needing to do. Overall we are happy with the switch to speed index for our synthetic front-end load time numbers, but it exposed a need for better tooling. Real User Front-end Performance These numbers come from mPulse , and are measured via JavaScript running in real users’ browsers: There aren’t any major changes here, just slight movement that is largely within rounding error. The one outlier is search, especially since our synthetic numbers showed that it got faster. This illustrates the difference between measuring onload, which mPulse does, and measuring speed index, which is currently only present in WebPagetest. This is one of the downsides of Real User Monitoring – since you want the overhead of measurement to be low, the data that you can capture is limited. RUM excels at measuring things like redirects, DNS lookup times, and time to first byte, but it doesn’t do a great job of providing a realistic picture of how long the full page took to render from the customer’s point of view. Conclusion We have a backend regression to investigate, and front-end tooling to improve, but overall there weren’t any huge surprises. Etsy’s performance is still pretty good relative to the industry as a whole , and relative to where we were a few years ago. The challenge going forward is going to center around providing a great experience on mobile devices and for international users, as the site grows and becomes more complex. Posted by Lara Hogan on January 23, 2014 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2014-01-23,"},
{"website": "Etsy", "title": "Static Analysis with OCLint", "author": ["Akiva Leffert"], "link": "https://codeascraft.com/2014/01/15/static-analysis-with-oclint/", "abstract": "Static Analysis with OCLint Posted by Akiva Leffert on January 15, 2014 At Etsy, we’re big believers in making tools do our work for us. On the mobile apps team we spend most of our time focused on building new features and thinking about how the features of Etsy fit into an increasingly mobile world. One of the great things about working at Etsy is that we have a designated period called Code Slush around the winter holidays where product development slows down and we can take stock of where we are and do things that we think are important or useful, but that don’t fit into our normal release cycles. Even though our apps team releases significantly less frequently than our web stack , making it easier to continue developing through the holiday season, we still find it valuable to take this time out at the end of the year. This past slush we spent some of that time contributing to the OCLint project and integrating it into our development workflow. OCLint, as the name suggests, is a linter tool for Objective-C. It’s somewhat similar to the static analyzer that comes built into Xcode, and it’s built on the same clang infrastructure. OCLint is a community open source project and all of the changes to it we’re discussing have been contributed back and are available with the rest of OClint on their github page . If you run OCLint on your code it will tell you things like, “This method is suspiciously long” or “The logic on this if statement looks funny”. In general, it’s great at identifying these sorts of code smells. We thought it would be really cool if we could extend it to find definite bugs and to statically enforce contracts in our code base. In the remainder of this post, we’re going to talk about what those checks are and how we take advantage of them, both in our code and in our development process. Rules Objective-C is a statically typed Object Oriented language. Its type system gets the job done, but it’s fairly primitive in certain ways. Often, additional contracts on a method are specified as comments. One thing that comes up sometimes is knowing what methods a subclass is required to implement. Typically this is indicated in a comment above the method. For example, UIActivity.h contains the comment // override methods above a list of several of its methods. This sort of contract is trivial to check at compile time, but it’s not part of the language, making these cases highly error prone. OCLint to the rescue! We added a check for methods that subclasses are required to implement. Furthermore, you can use the magic of Objective-C categories to mark up existing system libraries. To mark declarations, oclint uses clang’s __attribute__((annotate(“”))) feature to pass information from your code to the checker. To make these marks on a system method like the -activityType method in UIActivity , you would stick the following in a header somewhere: @interface UIActivity (StaticChecks)\r\n...\r\n- (NSString *)activityType\r\n__attribute__((annotate(“oclint:enforce[subclass must implement]”)));\r\n...\r\n@end That __attribute__ stuff is ugly and hard to remember so we #define d it away. #define OCLINT_SUBCLASS_MUST_IMPLEMENT \r\n__attribute__((annotate(“oclint:enforce[subclass must implement]”))) Now we can just do: @interface UIActivity (StaticChecks)\r\n...\r\n- (NSString *)activityType OCLINT_SUBCLASS_MUST_IMPLEMENT;\r\n...\r\n@end We’ve contributed back a header file with these sorts of declarations culled from the documentation in UIKit that anyone using oclint can import into their project. We added this file into our project’s .pch file so it’s included in every one one of our classes automatically. Some other checks we’ve added: Protected Methods This is a common feature in OO languages – methods that only a subclass and its children can call. Once again, this is usually indicated in Objective-C by comments or sometimes by sticking the declarations in a category in separate header. Now we can just tack on OCLINT_PROTECTED_METHOD at the end of the declaration. This makes the intent clear, obvious, and statically checked. Prohibited Calls This is another great way to embed institutional knowledge directly into the codebase. You can mark methods as deprecated using clang, but this is an immediate compiler error. We’ll talk more about our workflow later, but doing it through oclint allow us to migrate from old to new methods gradually and easily use things while debugging that we wouldn’t want to commit. We have categories on NSArray and NSDictionary that we use instead of the built in methods, as discussed here . Marking the original library methods as prohibited lets anyone coming into our code base know that they should be using our versions instead of the built in ones. We also have a marker on NSLog , so that people don’t accidentally check in debug logs. Frequently the replacement for the prohibited call calls the prohibited call itself, but with a bunch of checks and error handling logic. We use oclint’s error suppression mechanism to hide the violation that would be generated by making the original call. This is more syntactically convenient than dealing with clang pragmas like you would have to using the deprecated attribute. Ivar Assignment Outside Getters We prefer to use properties whenever possible as opposed to bare ivar accesses. Among other things, this is more syntactically and semantically regular and makes it much easier to set breakpoints on changes to a given property when debugging.  This rule will emit an error when it sees an ivar assigned outside its getter, setter, or the owning class’s init method. -isEquals: without -hash In Cocoa, if you override the -isEquals: method that checks for object equality, it’s important to also override the -hash method. Otherwise you’ll see weird behavior in collections when using the object as a dictionary key. This check finds classes that implement -isEquals: without implementing -hash . This is another great example of getting contracts out of comments and into static checks. Workflow We think that oclint adds a lot of value to our development process, but there were a couple of barriers we had to deal with to make sure our team picked it up. First of all, any codebase written without oclint’s rules strictly enforced for all of development will have tons of minor violations. Sometimes the lower priority things it warns about are actually done deliberately to increase code clarity. To cut down on the noise we went through and disabled a lot of the rules, leaving only the ones we thought added significant value. Even with that, there were still a number of things it complained frequently about – things like not using Objective-C collection literals. We didn’t want to go through and change a huge amount of code all at once to get our violations down to zero, so we needed a way to see only the violations that were relevant to the current change. Thus, we wrote a little script to only run oclint on the changed files. This also allows us to easily mark something as no longer recommended without generating tons of noise, having to remove it entirely from our codebase, or fill up Xcode’s warnings and errors. Finally, we wanted to make it super easy for our developers to start using it. We didn’t want to require them to run it manually before every commit. That would be just one more thing to forget and one more thing anyone joining our team would have to know about. Plus it’s kind of slow to run all of its checks on a large codebase. Instead, we worked together with our terrific testing and automation team to integrate it into our existing github pull request workflow. Now, whenever we make a pull request, it automatically kicks off a jenkins job that runs oclint on the changed files. When the job is done, it posts a summary a comment right to the pull request along with a link to the full report on jenkins. This ended up feeling very natural and similar to how we interact with the php code sniffer on our web stack . Conclusion We think oclint is a great way to add static checks to your Cocoa code. There are some interesting things going on with clang plugins and direct Xcode integration, but for now we’re going to stick with oclint. We like its base of existing rules, the ease of gradually applying its rules to our code base, and its reporting options and jenkins integration. We also want to thank the maintainer and the other contributors for the hard work they’ve been put into the project. If you use these rules in interesting ways, or even boring ones, we’d love to hear about it. Interested in a working at a place that cares about the quality of its software and about solving its own problems instead of just letting them mount? Our team is hiring! Posted by Akiva Leffert on January 15, 2014 Category: mobile Tags: ios , mobile , tools Related Posts Posted by Stephanie Sharp on 21 Oct, 2020 Improving our design system through Dark Mode Posted by Jacob Van Order on 23 Jun, 2020 Chaining iOS Machine Learning, Computer Vision, and Augmented Reality to Make the Magical Real Posted by Michael MacDougall on 11 Dec, 2017 VIPER on iOS at Etsy", "date": "2014-01-15,"},
{"website": "Etsy", "title": "Android Staggered Grid", "author": ["Deniz Veli"], "link": "https://codeascraft.com/2014/01/13/android-staggered-grid/", "abstract": "Android Staggered Grid Posted by Deniz Veli on January 13, 2014 While building the new Etsy for Android app, a key goal for our team was delivering a great user experience regardless of device. From phones to tablets, phablets and even tabphones, we wanted all Android users to have a great shopping experience without compromise. A common element throughout our mobile apps is the “Etsy listing card”. Listing cards are usually the first point of contact users have with our sellers’ unique items, whether they’re casually browsing through categories or searching for something specific. On these screens, when a listing card is shown, we think our users should see the images without cropping. Android Lists and Grids A simple enough requirement but on Android things aren’t always that simple. We wanted these cards in a multi-column grid, with a column count that changes with device orientation while keeping grid position. We needed header and footer support, and scroll listeners for neat tricks like endless loading and quick return items. This wasn’t achievable using a regular old ListView or GridView. Furthermore, both the Android ListView and GridView are not extendable in any meaningful way. A search for existing open libraries didn’t reveal any that met our requirements, including the unfinished StaggeredGridView available in the AOSP source . Considering all of these things we committed to building an Android staggered grid view. The result is a UI component that is built on top of the existing Android AbsListView source for stability, but supports multiple columns of varying row sizes and more. How it Works The StaggeredGridView works much like the existing Android ListView or GridView. The example below shows how to add the view to your XML layout, specifying the margin between items and the number of columns in each orientation. <com.etsy.android.grid.StaggeredGridView\r\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\r\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\r\n    android:id=\"@+id/grid_view\"\r\n    android:layout_width=\"match_parent\"\r\n    android:layout_height=\"match_parent\"\r\n    app:item_margin=\"8dp\"\r\n    app:column_count_portrait=\"2\"\r\n    app:column_count_landscape=\"3\" /> You can of course set the layout margin and padding as you would any other view. To show items in the grid create any regular old ListAdapter and assign it to the grid. Then there’s one last step. You need to ensure that the ListAdapter’s views maintain their aspect ratio. When column widths adjust on rotation, each item’s height should respond. How do you do this? The AndroidStaggeredGrid includes a couple of utility classes including the DynamicHeightImageView which you can use in your adapter. This custom ImageView overrides onMeasure() and ensures the measured height is relative to the width based on the set ratio. Alternatively, you can implement any similar custom view or layout with the same measurement logic. public void setHeightRatio(double ratio) {\r\n    if (ratio != mHeightRatio) {\r\n        mHeightRatio = ratio;\r\n        requestLayout();\r\n    }\r\n}\r\n\r\n@Override\r\nprotected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) {\r\n    if (mHeightRatio > 0.0) {\r\n        int width = MeasureSpec.getSize(widthMeasureSpec);\r\n        int height = (int) (width * mHeightRatio);\r\n        setMeasuredDimension(width, height);\r\n    }\r\n    else {\r\n        super.onMeasure(widthMeasureSpec, heightMeasureSpec);\r\n    }\r\n} And that’s it. The DynamicHeightImageView will maintain the aspect ratio of your items and the grid will take care of recycling views in the same manner as a ListView. You can check out the GitHub project for more details on how it’s used including a sample project. But There’s More Unlike the GridView, you can add header and footer views to the StaggeredGridView. You can also apply internal padding to the grid that doesn’t affect the header and footer views. An example view using these options is shown below. On our search results screen we use a full width header and add a little extra horizontal grid padding for 10 inch tablets. Into the Real World During the development process we fine-tuned the grid’s performance using a variety of real world Android devices available in the Etsy Device Lab . When we released the new Etsy for Android app at the end of November, the AndroidStaggeredGrid was used throughout. Post launch we monitored and fixed some lingering bugs found with the aid of the awesome crash reporting tool Crashlytics . We decided to open source the AndroidStaggeredGrid : a robust, well tested and real world UI component for the Android community to use. It’s available on GitHub or via Maven, and we are accepting pull requests. Finally, a friendly reminder that the bright folks at Etsy mobile are hiring . You can follow Deniz on Twitter at @denizmveli . Posted by Deniz Veli on January 13, 2014 Category: mobile Tags: android , mobile Related Posts Posted by Stephanie Sharp on 21 Oct, 2020 Improving our design system through Dark Mode Posted by Patrick Cousins on 12 Apr, 2018 Sealed classes opened my mind", "date": "2014-01-13,"},
{"website": "Etsy", "title": "Migrating to Chef 11", "author": ["Ryan Frantz"], "link": "https://codeascraft.com/2013/10/16/migrating-to-chef-11/", "abstract": "Migrating to Chef 11 Posted by Ryan Frantz on October 16, 2013 Configuration management is critical to maintaining a stable infrastructure. It helps ensure systems and software are configured in a consistent and deterministic way. For configuration management we use Chef .  Keeping Chef up-to-date means we can take advantage of new features and improvements.  Several months ago, we upgraded from Chef 10 to Chef 11 and we wanted to share our experiences. Prep We started by setting up a new Chef server running version 11.6.0.  This was used to validate our Chef backups and perform testing across our nodes.  The general plan was to upgrade the nodes to Chef 11, then point them at the new Chef 11 server when we were confident that we had addressed any issues.  The first order of business: testing backups.  We’ve written our own backup and restore scripts and we wanted to be sure they’d still work under Chef 11.  Also, these scripts would come in handy to help us quickly iterate during break/fix cycles and keep the Chef 10 and Chef 11 servers in sync.  Given that we can have up to 70 Chef developers hacking on cookbooks, staying in sync during testing was crucial to avoiding time lost to troubleshooting issues related to cookbook drift. Once the backup and restore scripts were validated, we reviewed the known breaking changes present in Chef 11.  We didn’t need much in the way of fixes other than a few attribute precedence issues and updating our knife-lastrun handler to use run_context.loaded_recipes instead of node.run_state() . Unforeseen Breaking Changes After addressing the known breaking changes, we moved on to testing classes of nodes one at a time.  For example, we upgraded a single API node to Chef 11, validated Chef ran cleanly against the Chef 10 server, then proceeded to upgrade the entire API cluster and monitor it before moving on to another cluster.  In the case of the API cluster, we found an unknown breaking change that prevented those nodes from forwarding their logs to our log aggregation hosts.  This episode initially presented a bit of a boondoggle and warrants a little attention as it may help others during their upgrade. The recipe we use to configure syslog-ng sets several node attributes, for various bits and bobs.  The following line in our cookbook is where all the fun started: if !node.default[:syslog][:items].empty? That statement evaluated to false on the API nodes running Chef 11 and resulted in a vanilla syslog-ng.conf file that didn’t direct the service to forward any logs.  Thinking that we could reference those nested attributes via the :default symbol, we updated the cookbook.  The Chef 11 nodes were content but all of the Chef 10 nodes were failing to converge because of the change.  It turns out that accessing default attributes via the node.default() method and node[:default] symbol are not equivalent.  To work around this, we updated the recipe to check for Chef 11 or Chef 10 behavior and assign our variables accordingly.  See below for an example illustrating this: if node[:syslog].respond_to?(:has_key?)\r\n    # Chef 11\r\n    group = node[:syslog][:group] || raise(\"Missing group!\")\r\n    items = node[:syslog][:items]\r\nelse\r\n    # Chef 10\r\n    group = node.default[:syslog][:group] || raise(\"Missing group!\")\r\n    items = node.default[:syslog][:items]\r\nend In Chef 11, the :syslog symbol points to the key in the attribute namespace (it’s an ImmutableHash object) we need and responds to the .has_key?() method; in that case, we pull in the needed attributes Chef 11-style.  If the client is Chef 10, that test fails and we pull in the attributes using the .default() method. Migration Once we had upgraded all of our nodes and addressed any issues, it was time to migrate to the Chef 11 server.  To be certain that we could recreate the build and that our Chef 11 server cookbooks were in good shape, we rebuilt the Chef 11 server before proceeding.  Since we use a CNAME record to refer to our Chef server in the nodes’ client.rb config file, we thought that we could simply update our internal DNS systems and break for an early beer.  To be certain, however, we ran a few tests by pointing a node at the FQDN of the new Chef server.  It failed its Chef run. Chef 10, by default, communicates to the server via HTTP; Chef 11 uses HTTPS.  In general, Chef 11 Server redirects the Chef 11 clients attempting to use HTTP to HTTPS.  However, this breaks down when the client requests cookbook versions from the server.  The client receives an HTTP 405 response.  The reason for this is that the client sends a POST to the following API endpoint to determine which versions of the cookbooks from its run_list need to be downloaded: /environments/production/cookbook_versions If Chef is communicating via HTTP, the POST request is redirected to use HTTPS.  No big deal, right?  Well, RFC 2616 is pretty clear that when a request is redirected, “[t]he action required MAY be carried out by the user agent without interaction with the user if and only if the method used in the second request is GET…”  When the Chef 11 client attempts to hit the /environments/cookbook_versions endpoint via GET, Chef 11 Server will respond with an HTTP 405 as it only allows POST requests to that resource. The fix was to update all of our nodes’ client configuration files to use HTTPS to communicate with the Chef Server. dsh (distributed shell) made this step easy. Just before we finalized the configuration update, we put a freeze on all Chef development and used our backup and restore scripts to populate the new Chef 11 server with all the Chef objects (nodes, clients, cookbooks, data bags, etc) from the Chef 10 server.  After validating the restore operation, we completed the client configuration updates and shut down all Chef-related services on the Chef 10 server.  Our nodes happily picked up where they’d left off and continued to converge on subsequent Chef runs. Post-migration Following the migration, we found two issues with chef-client that required deep dives to understand, and correct, what was happening.  First, we had a few nodes whose chef-client processes were exhausting all available memory.  Initially, we switched to running chef-client in forking mode.  Doing so mitigated this issue to an extent (as the forked child released its allocated memory when it completed and was reaped) but we were still seeing an unusual memory utilization pattern.  Those nodes were running a recipe that included nested searches for nodes.  Instead of returning the node names and searching on those, we were returning whole node objects.  For a long-running chef-client process, this continued to consume available memory.  Once we corrected that issue, memory utilization fell down to acceptable levels. See the following screenshot illustrating the memory consumption for one of these nodes immediately following the migration and after we updated the recipe to return references to the objects instead: Here’s an example of the code in the recipe that created our memory monster: # find nodes by role, the naughty, memory hungry way\r\nroles = search(:role, '*:*')    # NAUGHTY\r\nroles.each do |r|\r\n  nodes_dev = search(:node, \"role:#{r.name} AND fqdn:*dev.*\")    # HUNGRY\r\n  template \"/etc/xanadu/#{r.name.downcase}.cfg\" do\r\n  ...\r\n  variables(\r\n    :nodes => nodes_dev\r\n  )\r\n  end\r\nend Here’s the same code example, returning object references instead: # find nodes by role, the community-friendly, energy-conscious way\r\nsearch(:role, '*:*') do |r|\r\n  fqdns = []\r\n  search(:node, \"role:#{r.name} AND fqdn:*dev.*\") do |n|\r\n    fqdns << n.fqdn\r\n  end\r\n  template \"/etc/xanadu/#{r.name.downcase}.cfg\" do\r\n    ...\r\n    variables(\r\n      :nodes => fqdns\r\n    )\r\n  end\r\nend Second, we found an issue where, in cases where chef-client would fail to connect to the server, it would leave behind its PID file, preventing future instances of chef-client from starting.  This has been fixed in version 11.6.0 of chef-client. Conclusion Despite running into a few issues following the upgrade, thorough testing and Opscode’s documented breaking changes helped make our migration fairly smooth.  Further, the improvements made in Chef 11 have helped us improve our cookbooks.  Finally, because our configuration management system is updated, we can confidently focus our attention on other issues. Posted by Ryan Frantz on October 16, 2013 Category: operations Tags: chef Related Posts Posted by John Goulah on 13 Mar, 2012 Making it Virtually Easy to Deploy on Day One", "date": "2013-10-16,"},
{"website": "Etsy", "title": "September 2013 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2013/10/14/september-2013-site-performance-report/", "abstract": "September 2013 Site Performance Report Posted by Lara Hogan on October 14, 2013 As we enter the fourth quarter of 2013, it’s time for another site performance report about how we did in Q3. Our last report highlighted the big performance boost we saw from upgrading to PHP 5.4, and this report will examine a general front-end slowdown that we saw over the last few months. Server Side Performance Here are the median and 95th percentile load times for signed in users on our core pages on Wednesday, September 18th: On the server side we saw a modest decrease on most pages, with some pages (e.g. the profile page) seeing a slight increase in load time. As we have mentioned in past reports, we are not overly worried about the performance of our core pages, so the main thing we are looking for here is to avoid a regression. We managed to achieve this goal, and bought ourselves a little extra time on a few pages through some minor code changes. This section isn’t very exciting, but in this case no news is good news. Synthetic Front-end Performance The news here is a mixed bag. As usual, we are using our private instance of WebPagetest to get synthetic measurements of front-end load time. We use a DSL connection and test with IE8, IE9, Firefox, and Chrome. Here is the data, and all of the numbers are medians over a 24 hour period: On the plus side, we saw a pretty significant decrease in start render time almost across the board, but document complete time increased everywhere, and increased dramatically on the listing page. Both of these global effects can be explained by rolling out deferred JavaScript everywhere – something we mentioned back in our June report . At that time we had only done it on the shop page, and since then we have put it on all pages by default. This explains the decrease in start render time on all pages except for the shop page. The profile page also had a slight uptick in start render time, and we are planning on investigating that. One of the side effects of deferring JavaScript is that document complete time tends to rise, especially in IE8. This is an acceptable tradeoff for us, since we care more about optimizing start render, and getting content in front of the user as quickly as possible. We’re also not convinced that a rise in document complete time will have a negative impact on business metrics, and we are running tests to figure that out now. The massive increase in document complete time on the listing page is due to the rollout of a page redesign, which is much heavier and includes a large number of web fonts. We are currently setting up a test to measure the impact of web fonts on customer engagement, and looking for ways to reduce page weight on the listing page. While document complete isn’t a perfect metric, 8 seconds is extremely high, so this bears looking into. That said, we A/B tested engagement on the old page and the new, and all of the business metrics we monitor are dramatically better with the new version of the listing page. This puts further doubt on the impact of document complete on customer behavior, and illustrates that performance is not the only thing influencing engagement – design and usability obviously play a big role. Real User Front-end Performance These numbers come from mPulse , and are measured via JavaScript running in real users’ browsers: The effect here mirrors what we saw on the synthetic side – a general upward trend, with a larger spike on the listing page. These numbers are for the “page load” event in mPulse, which is effectively the onload event. As Steve Souders and others have pointed out , onload is not a great metric, so we are looking for better numbers to measure on the real user side of things. Unfortunately there isn’t a clear replacement at this point, so we are stuck with onload for now. Conclusion Things continue to look good on the server side, but we are slipping on the front-end. Partly this has to do with imperfect measurement tools, and partly it has to do with an upward trend in page weight that is occurring all across the web – and Etsy is no exception. Retina images, web fonts, responsive CSS, new JavaScript libraries, and every other byte of content that we serve continue to provide challenges for the performance team. As we continue to get hard data on how much page weight impacts performance (at least on mobile), we can make a more compelling case for justifying every byte that we serve. You can follow Jonathan on Twitter at @jonathanklein Posted by Lara Hogan on October 14, 2013 Category: performance Tags: performance , performance report Related Posts Posted by Natalya Hoota , Allison McKnight and Kristyn Reith on 28 Apr, 2016 Q1 2016 Site Performance Report Posted by Kristyn Reith on 12 Feb, 2016 Q4 2015 Site Performance Report Posted by Mike Adler on 10 Nov, 2015 Q3 2015 Site Performance Report", "date": "2013-10-14,"},
{"website": "Etsy", "title": "Nagios, Sleep Data, and You", "author": ["Ryan Frantz"], "link": "https://codeascraft.com/2013/09/28/nagios-sleep-data-and-you/", "abstract": "Nagios, Sleep Data, and You Posted by Ryan Frantz on September 28, 2013 Gettin’ Shuteye Ian Malpass once commented that “[i]f Engineering at Etsy has a religion, it’s the Church of Graphs.”  And I believe!  Before I lay me down to sleep during an on-call shift, I say a little prayer that should something break, there’s a graph somewhere I can reference.  Lately, a few of us in Operations have begun tracking our sleep data via Jawbone UPs .  After a few months of this we got to wondering how this information could be useful, in the context of Operations.  Sleep is important.  And being on call can lead to interrupted sleep.  Even worse, after being woken up, the amount of time it takes to return to sleep varies by person and situation.  So, we thought, “why not graph the effect of being on call against our sleep data?” Gathering and Visualizing Data We already visualize code deploys against the myriad graphs we generate, to lend context to whatever we’re measuring.  We use Nagios to alert us to system and service issues.  Since Nagios writes consistent entries to a log file, it was a simple matter to write a Logster parser to ship metrics to Graphite when a host or service event pages out to an operations engineer.  Those data points can then be displayed as “deploy lines” against our sleep data. For the sleep data we used, and extended, Aaron Parecki’s ‘ jawbone-up ‘ gem to gather sleep data (summary and detail information) via Jon Cowie’s handy ‘ jawboneup_to_graphite ‘ script on a daily basis.  Those data are then displayed on personal dashboards (using Etsy’s Dashboard project). Results So far, we’ve only just begun to collect and display this information.  As we learn more, we’ll be certain to share our findings.  In the meantime, here are examples from recent on-call shifts. This engineer appeared to get some sleep! Here, the engineer was alerted to a service in the critical state in the wee hours of the morning. From this graph we can tell that he was able to address the issue fairly quickly, and most importantly, get back to sleep fast. NOTE:  Jawbone recently opened up their API .  Join the party and help build awesome apps and tooling around this device! Posted by Ryan Frantz on September 28, 2013 Category: monitoring , operations", "date": "2013-09-28,"},
{"website": "Etsy", "title": "LXC – Automating Containers aka Virtual Madness (Part 2)", "author": ["Jayson Paul"], "link": "https://codeascraft.com/2013/09/23/lxc-automating-containers-aka-virtual-madness-part-2/", "abstract": "LXC – Automating Containers aka Virtual Madness (Part 2) Posted by Jayson Paul on September 23, 2013 Presenting Virtual Madness! This is part 2 of our LXC blog post. If you missed the first half you can read it here . We already have much tooling around making it easy to create a virtual machine for each developer, so it made sense to build our LXC virtualization tools into the same interface. As shown above, our main page gives a quick look into all the LXC Containers that are running and the Jenkins instances to which they are attached. The containers are known colloquially as the Bobs, a reference to Bob the Builder. The interface also separates the Bobs by physical disk for ease of detecting if any of the virtual hosts’ disks are overloaded. Creation of a new Bob follows our one button deploy principle we strive for at Etsy: The next numerically available hostname is automatically populated into the form by referencing all defined hosts in Chef and finding the first gap in numbering. The first physical host with enough free disk space is pre-selected from the drop-down and is determined according to the fact that we can fit 14 Bobs on each physical host (more on that later). Simply clicking the “Make it” button will kick off the process of creating an LXC container, and its progress is streamed via websockets to the browser in real-time. The process of creating a new container is roughly as follows. Which of the disks on any given physical host has room for a new Bob is determined by a simple rule: the first disk holds a maximum of four Bobs, and the second and third disk hold a maximum of five each. Only four are allowed on the first disk in order to save room for the host OS and a base LVM volume which we clone for each Bob. The first numerically available IP address in any of our virtual host subnets is allocated to the Bob by iterating through reverse DNS lookups on the range until an NXDOMAIN is returned. nsupdate is used to dynamically create the DNS entries for the new container. An empty LVM volume is then created by using lvcreate and mkfs.ext3, it is mounted, and rsync is used to clone the base volume to the empty volume. The base volume is created in advance by cloning the physical host’s root filesystem (which is configured almost exactly the same way as the Bobs) and then filtering out some unneeded items. /dev is fixed using mknod because a simple rsync will render it useless. Sed is used to fix all the network settings in /etc/sysconfig. Unneeded services are disabled by creating a small bash script inside the volume and then chrooting to run it. Once the base volume is copied into our new volume, the Chef authorization key is removed so that the container is forced to automatically reregister with our Chef server as a new node with its newly specified hostname. Our default route to the gateway is added based on subnet and datacenter. An LXC config file is then created using the IP address and other options. lxc.cgroup.cpuset.cpus is set to 0 as our containers are not pinned to a specific set of CPU cores. This allows each executor to use as many CPU cores as can be allocated at any given time. Finally, we bootstrap the node using Chef, bringing the node up to date with all our current packages and configurations. This is necessary because the base LVM volume from which the new container is cloned is created in advance and often has stale configurations. There is also a batch UI which executes the exact same process as above but allows for the creation of Bobs in bulk: Once Bobs are created, they are ready to be attached to Jenkins instances and given executor roles: This interface allows for the choice of instance and label (executor), and the number of Bobs to attach. Unattached Bobs are surfaced by use of the Jenkins API . After iterating through all unattached Bobs, taking into account the rule of only one heavy executor per disk, a groovy script is used to attach the Bob to Jenkins. The Jenkins API did not provide this functionality, but it does allow for the execution of groovy scripts, which we leveraged to automate this. For our final bit of automation around this process, we created a one-button interface to wipe and rebuild the base LVM container on all physical hosts. This is necessary when the LVM containers fall very far out of date and start to cause issues bringing Bobs up to date using Chef. This interface simply performs the base container creation task described above across any number of physical hosts. All in all, this virtual madness allowed us to turn our many manual processes of setting up LXC containers into simple one-button deploys. How do you handle this kind of stuff? Let us know in the comments! Co-written by Jayson Paul, Patrick McDonnell and Nassim Kammah. You can follow Jayson on Twitter at @jaysonmpaul You can follow Patrick on Twitter at @mcdonnps Posted by Jayson Paul on September 23, 2013 Category: engineering , infrastructure , operations", "date": "2013-09-23,"},
{"website": "Etsy", "title": "LXC – Running 14,000 tests per day and beyond! (Part 1)", "author": ["Jayson Paul"], "link": "https://codeascraft.com/2013/09/23/lxc-running-14000-tests-per-day-and-beyond-part-1/", "abstract": "LXC – Running 14,000 tests per day and beyond! (Part 1) Posted by Jayson Paul on September 23, 2013 Continuous Integration at Etsy As Etsy continues to grow and hire more developers, we have faced the continuous integration scaling challenge of how to execute multiple concurrent test suites without slowing the pace of our deploy queue. With a deployment rate of up to 65 deploys / day and a total of 30 test suites (unit tests, integration tests, functional tests, smokers…) that run for every deploy, this means running the test suites 1950 times a day. Our core philosophy is to work from the master branch as much as possible; while developers can run the test suites on their own virtual machines, it breaks the principle of testing in a clone of a production environment . Since everyone has root access and can install anything on their VMs, we can’t ensure a sane, standard test environment on developer VMs. Yet, they must ensure their code will not break the tests before they commit, or they will block the deploy queue. To this end, we developed a small library called “ try ” which sends a diff of the developer’s work to a Jenkins server that in turns applies it against the latest copy of the master branch and runs all the test suites (checkout TryLib on github ). Unfortunately, providing this feedback loop to every developer comes at a cost. With up to 10 developers “trying” their changes simultaneously, we now need to run the test suites an additional 13700 times a day (we average 515 try runs a day, with different test suite configurations). Last but not least, the test feedback loop ought to be short to provide any value. Our current “SLA” for running all test suites is 5 minutes. Following the “ divide and concur ” strategy and with the help of the master project Jenkins plugin , we are able to run the test suites in parallel. Our initial setup had to maintain multiple executors per box, which caused connection issues, and required preventing multiple executors from attempting to access the same Jenkins workspace at once. Workload Considerations While parallelization was initially a major victory, it soon became problematic.  Our workload consists of two different classes of tests: one class constrained by CPU (“lightweight executors”, which run our unit tests), and the other bound to disk I/O (“heavyweight executors”, which run our integration tests).  Prior to virtualization, we would find that if multiple heavyweight jobs ran on a single host, the host would slow to crawl, as its disk would be hammered. We also had issues with workspace contention when executing multiple jobs on a single host, and this need for filesystem isolation, in combination with resource contention and issues within Jenkins concerning too many parallel executors on a single host, led us toward the decision that virtualization was the best path forward.  We were, however, concerned with the logistics of managing potentially hundreds of VMs using our standard KVM/QEMU/libvirt environment . Virtualization with LXC Enter LXC, a technology wonderfully suited to virtualizing homogenized workloads.  Knowing that the majority of our executors are constrained on CPU, we realized that using a virtualization technology for which we would need to be conscious about resource allocation did not make sense for our needs.  Instead of concerning ourselves with processor pinning, RAM provisioning, disk imaging, and managing multiple running kernels, LXC allows us to shove a bunch of jailed processes in LVM containers and call it a day. While the workings of LXC are outside the scope of this post, here is a little bit of information about our specific configuration.  Our physical hosts are 2U Supermicro chassis containing four blades each.  Each blade has three attached SSDs for a total of 12 disks per chassis.  In our case, disk I/O is a key constraint; if we run two heavyweight jobs accessing a single disk simultaneously, the run time doubles for each job.  In order to maintain our 5 minute run time SLA, we ensure that only one heavyweight executor runs on each disk at any given time. Previously, we had many executors running on each physical host, but with LXC, we have decided that each container (LXC’s terminology for a virtualized instance) shall host a single executor.  Maintaining a 1:1 container:executor mapping alleviates the connection issues we saw prior to virtualization within Jenkins while trying to maintain a large number of executors per host, and we never have to worry about multiple executors attempting to access the same Jenkins workspace at once.  It also allows for easy provisioning using the Virtual Madness interface discussed below. Continue to Part 2 … Co-written by Nassim Kammah, Patrick McDonnell and Jayson Paul. Be sure to stay tuned for Part 2 of this blog post where Jayson will be discussing automating this process of automating LXC management. You can follow Nassim on Twitter at @kepioo You can follow Patrick on Twitter at @mcdonnps You can follow Jayson on Twitter at @jaysonmpaul Posted by Jayson Paul on September 23, 2013 Category: engineering , infrastructure , operations Tags: continuous integration Related Posts Posted by Rasmus Lerdorf on 01 Jul, 2013 Atomic deploys at Etsy Posted by Laura Beth Denker on 12 Mar, 2012 Scaling CI at Etsy: Divide and Concur, Revisited", "date": "2013-09-23,"},
{"website": "Etsy", "title": "Improving Etsy for iOS with server-based logging", "author": ["Amy Dyer"], "link": "https://codeascraft.com/2013/09/11/improving-etsy-for-ios-with-server-based-logging/", "abstract": "Improving Etsy for iOS with server-based logging Posted by Amy Dyer on September 11, 2013 One of the major challenges the Native Apps group faces at Etsy is maintaining and debugging a large codebase with a small team. Android and iOS are less mature platforms at Etsy and correspondingly lack some of the development tools and safety net that our fellow engineers have built up on the web side. Part of what the apps team does is build our own platform-specific tools. This is a case study for how we applied Etsy’s log-everything ethos to a particular class of bugs in Etsy for iOS. Collection errors in Cocoa Touch Crashing is one of the worst things an app can do. Cocoa has a number of patterns that encourage silent failures over crashing, most notably nil-messaging. But there are some parts of Cocoa that have fail-fast behavior instead. The most common of these cases involve the collection classes, which throw exceptions for cases like array out-of-bound errors or setting a nil keys in a dictionary. Server hiccups can cause problems when we use collections if we’re not careful, since we interpret empty fields returned from the Etsy API as nil objects. These issues were especially common when we were first developing our app, as the server API was constantly changing, but they can also be caused by all sorts of other server errors. We could solve this problem by wrapping every array or dictionary access in an if statement to check for these cases, but that would be so much more verbose it seems likely to cause more bugs than it catches. Instead, we abstracted this pattern out into a series of wrapper methods and added them as a Safe category on each collection class, such as -[NSArray(Safe) safeObjectAtIndex:] and -[NSMutableDictionary(Safe) safeSetObject:forKey:] . These methods handle those exceptional cases, either taking no action or returning nil as appropriate. This way, if we have unexpected missing data, we might do something like display an empty label, but at least the app remains stable. Users can refresh or go back to a screen with good data. Over time, we started using these methods all over the code base. They supported several convenient patterns, such as lazily loading cached objects from an array: for (SomeClass * object in myArray) {\n    NSUInteger index = [myArray indexOfObject:object];\n    //Returns nil when out-of-bounds\n    UIView * view = [cachedViews safeObjectAtIndex:index];\n    if (!view) {\n        view = //... However, when Apple added subscripted collection indexing in Clang 3.4, we moved away from the Safe methods wherever it looked okay. We wanted our code to be more idiomatic as well as aesthetically pleasing; array[0] is much easier to scan than [array safeObjectAtIndex:0] and more consistent with library code. What we didn’t realize is that our liberal use of these Safe categories had been hiding logic bugs: benign-looking changes started causing app instability. For example, we discovered an NSNumberFormatter that had always returned nil due to bad input. When we switched to subscripting syntax, that minor failure turned into a serious crash. Safer refactoring with Assertions and Expectations In a subsequent postmortem , we realized that the intent of our code needed to be explicit. It was impossible to tell which Safe calls were necessary, which were simply convenient, and which were hiding bugs. Removing the category entirely wasn’t possible, since that would expose all the crashes we intended to fix in the first place. Ultimately we came up with a multi-part plan to rationalize our use of these methods: Split our Safe methods into two variants. These variants have identical user visible behavior, but different intended semantics: A variant where we explicitly expect the out-of-bounds case and handle it by returning nil, ignoring the inputs, or whatever is necessary in that case. A variant where we don’t expect the out-of-bounds case and seeing it means something has gone wrong. Log the cases where something is wrong and upload them to our server-based logging system. Use the aggregated logs to identify spots in our code base where we’re catching potentially unexpected behavior. The first part is simple. We added a set of methods, such as -[NSArray(Safe) objectOrNilAtIndex:] , that explicitly indicate that nil is an acceptable value. This is useful for caching patterns and setting or getting optional values: - (id) objectOrNilAtIndex:(int)index {\n    if (index >= [self count])\n        return nil;\n    return [self objectAtIndex:index];\n} We kept our original “Safe” methods as the second variant, but modified to do the necessary logging. For that logging, enter our new assertion macros: EtsyAssert and EtsyExpect . The Etsy logging system has two main logging levels, ERROR and INFO, to separate critical error logs (that should be immediately investigated) from information (that we just want archived). EtsyAssert corresponds to ERROR, while EtsyExpect corresponds to INFO. //EtsyAssert is an alias for NSAssert in debug builds.\n//In release builds, failures will log an error with a stack trace to the server.\n\n#ifdef NS_BLOCK_ASSERTIONS\n    #define EtsyAssert(condition, description, ...)\n        if (!(condition)) {\n            SERVER_LOG_ERROR_WITH_TRACE(@\"EtsyAssert\", description, ##__VA_ARGS__);\n        }\n#else\n    #define EtsyAssert(condition, description, ...)\n        NSAssert(condition, (description), ##__VA_ARGS__)\n#endif\n\n//EtsyExpect will complain if an expectation fails, but won't ever throw an exception.\n#define EtsyExpect(condition, description, ...)\n    if (!(condition)) {\n        SERVER_LOG_INFO(@\"EtsyExpect\", description, ##__VA_ARGS__);\n    }\n#endif EtsyAssert is a wrapper for NSAssert . It evaluates an expression and, when assertions are enabled, throws an exception if that expression evaluates to false. When assertions are disabled (typical in Release builds), it logs an error-level message. EtsyExpect is a little different: it indicates that we expect the code to work a particular way, and if it doesn’t, an error may (or may not) have occurred. We added EtsyExpect clauses to all of our old Safe methods. Since we were dealing with a potentially large number of existing minor issues, we used EtsyExpect so as not to interrupt regular developer workflows. Let’s take a look at EtsyExpect in action: - (id)safeObjectAtIndex:(int)index {\n    EtsyExpect(index < [self count],\n      @\"Retrieving object at index %i from array of length %i\", index, [self count]);\n    return [self objectOrNilAtIndex:index];\n} Server-based logging for mobile clients What’s makes EtsyExpect distinctly useful from the built-in Cocoa logging operations is the server-side logging. SERVER_LOG_INFO and SERVER_LOG_ERROR are macro wrappers around our logging framework: #define SERVER_LOG_ERROR(ns, description, ...) \n    [EtsyAnalytics logError:[NSString stringWithFormat:description, ##__VA_ARGS__] \n                  namespace:ns \n                        line:__LINE__ \n                    function:__PRETTY_FUNCTION__ \n                  attributes:nil] The logError:namespace:line:function:attributes method piggybacks on our existing native analytics framework. The EtsyAnalytics class sends zipped analytics data and log messages to an API server. Analytics data is ultimately used by our Big Data stack , but these log messages are printed directly to a file. From there they can be indexed, grepped, splunked, or watched in (almost) real time, like any other log. These logs look like: api02 : [Fri Aug 09 21:25:43 2013] [iPhone 5 6.1.2/2.7 rv:2.7.0] [redacted UDID] [EtsyExpect] [-[NSArray(Safe) safeObjectAtIndex:]+401] Retrieving object at index 5 from array of length 5 (called by 1 Etsy 0x0029dcc1 Etsy + 2600129) What makes this powerful is the ability to see issues quickly. In real life conditions, users hit edge cases that are rare or difficult to test. By aggregating errors sampled from 1% of app users, we can see where EtsyExpect and EtsyAssert fail. That increased visibility allows us to pick off, one by one, the bugs that our Safe methods were hiding. By leveraging this data, we can be sure of which cases are safe to change. As an example: the log message above was a small logic error in the UIViewController subclass we use to display a listing. When the user swiped through a listing’s images, we preloaded the next image – without checking whether or not more images were available. App users were doing this thousands of times a day, but developers may have only encountered the issue a handful of times (if at all). While the bug was ultimately harmless, logging made it visible, so we were able to clean up this code with very little risk. The future of logging Moving a few logs out of the Xcode console and onto the Etsy servers turns out to be a very powerful tool. We’re still exploring what else we can do with it: for example, it’s also been useful for diagnosing invalid HTTP requests sent by the app. App logs can be correlated with server logs, so we can track certain errors from the client back to the server. Even  watching the logs in splunk helps us see what bugs we’ve fixed in the last version, and what we may have caused. Ultimately, by making Etsy for iOS a little bit like our web stack, we’ve found a way to make it more robust. By the way, we’re hiring ! Posted by Amy Dyer on September 11, 2013 Category: mobile", "date": "2013-09-11,"},
{"website": "Etsy", "title": "Etsy’s Device Lab", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2013/08/09/mobile-device-lab/", "abstract": "Etsy’s Device Lab Posted by Lara Hogan on August 9, 2013 At the end of 2012, Etsy CEO Chad Dickerson noted that mobile visits grew 244% year over year and represented more than 25% of all traffic. Halfway through 2013, this metric continues to steadily increase, with mobile web and app traffic accounting for about 40% of visits to Etsy . As this traffic grows, our designers and developers need to be able to test how Etsy works on more devices, screen sizes, and operating systems. The Mobile and Corporate IT teams built a mobile device testing lab to help with this. Originally a gray cabinet with devices, now a handcrafted shelf with labels and power, we’ve begun encouraging product development teams to come and use it! We’ve learned a ton so far from building this lab, and we’re eager to share these lessons with others as more companies consider creating their own testing suite. Why build a testing lab? Many designers and developers at Etsy have iPhones or very recent Android smartphones. Mobile web work was historically tested on an employee’s phone or in their desktop browser using spoofed user agents, which is great for day-to-day development, but we need to test out work on a wider range of real, physical devices that represent what our visitors are using when they arrive at Etsy.com. Small bugs and edge cases were being missed during development, and we weren’t necessarily testing on devices that our members are actually using. We decided to build the device lab with devices that drive the most traffic to the site, as well as some older devices that we know can be problematic to develop for (particularly older Androids and Blackberries). There are a ton of articles on how to choose devices for your lab; see the end of this article for links! “Testing your designs in the device lab is great. I had no idea how many strange aspect ratios were out there on Android devices. Being an Apple only mobile device user, the device lab was an invaluable resource for testing my designs across unfamiliar operating systems and hardware.” – Jason Huff, Designer This has helped our teams not just test new projects, but also go back and find things to fix in already-shipped projects: “The mobile device lab really is a phenomenal resource. While it’s easy to check features on your own phone or tablet, it’s easy to forget how many other devices are out there and being used by our members. When we went up to the lab for the first time, our team discovered a number of device specific scenarios that we hadn’t come across during our previous testing , and I’m so glad these were surfaced” – Triona Fritsch, Product Manager Unexpectedly, the device lab has also been a brainstorming opportunity for teams who are new to mobile development. As people go through and test their existing projects, they also see how they could have reframed their approach from the beginning, and they’re inspired to do more mobile-first work. “[The device lab] also gave us some new insight into scenarios that we should consider during the initial design phases for future mobile projects.” – Triona Fritsch, Product Manager Setup We currently have 34 devices on this custom-made bookshelf (made by Reade Bryan who works on Etsy’s Facilities team). See the end of this article for more resources on selecting devices. Here’s our breakdown: 12 Android phones (a range of operating system versions and major manufacturers) 3 Android tablets (a range of screen resolutions) 2 Blackberry phones (one older phone, one BB10) 1 Firefox phone (developer preview) 5 iPads (including 1 mini, different generations) 4 iPhones/iPods (one upgraded to iOS7) 3 Kindle Fires (two are HD, and we have a range of screen resolutions) 1 Windows Phone 1 Windows Surface 1 Google Glass 1 Chromebook Pixel The bookshelf was designed to have different-height openings for our range of device sizes. Each bookshelf has a lip on the front so that we have the option of running cables behind it and putting labels on the front of it. The bottom shelf holds all our power setup as well as custom built crates from Etsy seller EmmersonWoodworks . These crates hold cables for testers to borrow if they’d like to take devices back to their desk, as well as screen wipes, labels and other miscellaneous items that we need nearby. We purchased a variety of stands from Etsy sellers, including PhoneStandz , DonorStand , woodworksRD and Fenton Creek WoodWorks . The stands ranged in material, size, and slot width – some older phones need slightly bigger stands since they’re thicker than newer phones. We typically stand phones upside-down in their stands to charge because the cables rarely sit nicely in the stands while charging. We also use library cards for signing out devices that leave the room, which we purchased from jardindepapier . The library cards are color-coded by operating system (as is the washi tape from InTheClear we used to hold them down, label the front of the shelves and the back of each device). On the front of the library card pockets we include: device name and number screen resolution pixel density operating system version This way, testers can make sure they’re checking out a range of devices, screen sizes, and OS versions. We also developed a getting started guide that outlines device traffic, known quirks like overlays and fixed elements, and how to get set up with Adobe Edge Inspect . Each device gets a cable drop to hold its cable in place so they don’t fall behind the shelves, a library card with its information, Adobe Edge Inspect installed, an asset tag, and a stand that fits it. We also install a background image that reminds testers of our two device lab rules: Please sign out devices Don’t upgrade the OS or apps It does take some time to get each device set up in the lab, but the organization is really worth it to keep it easy for testers to use a device, sign it out, and plug it back in. Major Lessons (So Far) Power Power has been the biggest challenge for us. As a Certified B Corporation , we want to track energy consumption and make sure we’ve set up the device lab in an optimal way. We’re measuring the amount of power we use, and we’ve also set the devices up on a power strip with a timer so they only charge for a few hours each day. We’re still finding the right balance. Power was further complicated when we realized that most USB hubs are not set up for devices with lots of power draw (like iPads). We had to learn the hard way that when you plug too many devices into a hub at once, the whole hub can stop working. We are now using a USB hub that can handle up to 32 devices , which maxes out at 16 Amps (at 500mA per port). There is a safety protection device within the circuit board that prevents a higher charge going to each port. This is ideal for lower powered devices (most of the ones we have in the lab) but is not suitable for larger, more power hungry devices like the iPad or other tablets. Tablets generally require a higher charging level so they will charge very slowly, or not power the device at all. This is why we have some of the lab’s devices running directly into a power strip. The other nice thing about this hub is a fault indicator light which helps identify problematic devices; devices which consume excess current are also disabled, leaving non-faulty devices charging. Label all the things Any web content editor will tell you that people don’t read, they scan . When a designer or developer comes to use the device lab, we want to make it so easy to use that they don’t have to think. Need to grab a cable to take back to your desk? Try the crate that says “BORROW THESE”. Need an iPhone 4? We’ve labeled the front of the bookcase where it lives and the back of the device so that it’s easy to match up later. Need to find the Blackberries among the devices? Look for the blue library cards (and blue washi tape to hold the cards down). The color-coding of library cards and tape was intentional to easily group Androids (green), iOS devices (manila), Kindles (yellow), Windows (pink), Blackberries (blue) and a Firefox phone (orange). We wrapped washi tape around each of the USB cables to borrow so that, should they walk away to someone’s desk and forgotten, they’ll be relatively easy to spot later. We’ve also added a sliver of washi tape on or near the power button for each device so that it’s easy to find — this tends to be the most challenging part of using the device lab! Testing on VMs We have an awesome Security Team at Etsy who has been invaluable during the device lab setup. Recently, they helped install security certificates across devices to make it easier to test on VMs. Previously, testers would have to individually accept certificates device-by-device, which really hampered testing and development, especially when they were trying to tackle a lot of testing at once with Adobe Edge Inspect. Adobe Edge Inspect So far, this tool has been really handy for testing on lots of devices at once. We’re using a multi-device subscription (free Adobe Edge allows you to test on only one device at a time) and installed the Adobe Edge app on all Androids, Kindles and iOS devices in the lab. It’s really awesome to feel like a puppet master and control all of those devices at once, but what’s really handy is capturing screenshots from all those devices at the same time so you can check them out on your laptop and audit them later. We’ve come a long way with our device testing lab, and I’m excited to keep iterating on it and making it even easier to test with. In the future, we’ll be adding more automated testing to the devices, easier tracking of devices (including signing them out), and more analysis of use. As mobile traffic continues to increase, we’re eager to make sure that it’s incredibly easy for designers and developers to test their work and experience what our members do when they use Etsy from a mobile device. More reading: Strategies for choosing test devices How to build a device lab, with a thorough first section on choosing and acquiring devices Mobile testing for dummies Establishing an open device lab You can follow Lara, engineering manager for the mobile web team, on Twitter at @laraswanson Posted by Lara Hogan on August 9, 2013 Category: mobile", "date": "2013-08-9,"},
{"website": "Etsy", "title": "Infrastructure upgrades with Chef", "author": ["Daniel Schauenberg"], "link": "https://codeascraft.com/2013/08/02/infrastructure-upgrades-with-chef/", "abstract": "Infrastructure upgrades with Chef Posted by Daniel Schauenberg on August 2, 2013 In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “primary/replica” in our Code as Craft entries. This post in particular names an open-source repository that we need to rename. We are actively working on that and will update this post ASAP. Infrastructure overview As we have written before , at Etsy we run our production stack exclusively on physical hardware. This, albeit being less elastic in terms of bringing new hosts up, gives us the power to serve 1.5 billion page views a month on a relatively small number of machines. In addition to our production infrastructure, every engineer and designer also have their own VM to develop on the Etsy stack (see this post for details). This brings us to over 1000 hosts managed by Chef. They are all connected to one Chef server and we run the same cookbooks on all hosts. We have about 30 engineers regularly making changes to our cookbooks. In order to make this workflow smooth, we have built knife spork , a knife plugin that helps with versioning cookbooks, updating environments and interacting with the chef server. For a full overview of our chef workflow check out the Velocity workshop and this presentation at the Chef NYC meetup . And for some more background on our Chef setup, see this talk from ChefConf 2012. This workflow allows us to promote changes to our production infrastructure about 20 times on an average day. How we roll out changes The rather static nature of our infrastructure means that we don’t usually spin up new hosts to test our changes and then switch over as this would also require a lot of extra infrastructure to do at this frequency. We have two major ways of how we test and roll out changes to the live infrastructure. Testing with knife spork and knife flip In our Chef workflow we have three environments, production , development and testing . The production and development environments contain all of our nodes serving etsy.com and nodes which make up our development infrastructure respectively. Those environments contain explicitly pinned cookbook versions, meaning that all nodes in those environments run exactly the specified version of a cookbook. The third environment – “testing” – doesn’t have any version constraints and has no specified set of nodes it contains. While we test changes to our cookbooks, we move a node that would be affected by those changes to the testing environment. As soon as the cookbook version is updated and the new cookbook is uploaded to the server it will be run on all hosts in the testing environment and thus also the node we just moved there for. So if we want to change something in the apache cookbook we can depool a web node, change it to be in the testing environment instead of production, trigger a Chef run (or wait 10 minutes as this is the interval with which we run chef-client) and test the changes. We have also created a knife plugin called knife flip which automates the environment change with a simple command. After the changes are tested on the web node, we flip it back to production and promote the changes to the cookbook to be used in the development and production environments. While this is useful for short lived testing, it essentially blocks everybody else from working on this cookbook. This is often fine, since not everybody usually has to do work on the same cookbook at the same time. However longer lasting ramp ups (for example rolling out a new version of PHP to all web nodes) would be impossible as it would block the cookbook for weeks. That’s why we use a different approach for rolling out major changes to hosts. Allowlisting hosts The basic pattern of our allowlisting approach is based on data bags which contain a list of hosts that are allowed to receive the update. In the recipe we then test whether or not the current node is in this allowlist and if it is, run a different branch of the recipe. This is very similar to how we do branching in code in our web code via the Feature API . In order to make this easier we created a library cookbook which we are releasing as Open Source today. Introducing chef-allowlist chef-whitelist is a simple library cookbook which we include as a dependency in all cookbooks that run a allowlisted change. Adding and using a allowlist can be accomplished in two simple steps. First create a new data bag in the “whitelist” namespace. This is the default namespace and can be changed to whatever you want. The data bag has to at least contain a “patterns” key (also configurable) with an array of hostnames. To make it easier to whitelist groups of similar nodes, wildcard hostnames are also allowed in there. Then in your recipe you can do something like this: if node.is_in_whitelist? \"new_whitelist\"\n# new hawtness\nelse\n# old way of doing things\nend Now everytime this recipe is run, it checks the node for inclusion in the allowlist “new_whitelist” and then acts accordingly. Verdict For infrastructure changes we have the same continuous deployment mentality with which we develop on the web stack. Small changes are built up and continuously deployed. For this we have two ways to ramp up and test smaller changes as well as more elaborate ones which need to be rolled out more slowly. The whitelist library has lowered the bar to overcome when we want to hide a change behind a rollout flag and makes it easier for engineers that are not working with Chef in a daily manner to safely roll out their changes. How do you handle rolling out changes to your infrastructure? How does it tie into your development workflow? Let us know in the comments. You can follow Daniel on Twitter at @mrtazz Posted by Daniel Schauenberg on August 2, 2013 Category: engineering , infrastructure , operations", "date": "2013-08-2,"},
{"website": "Etsy", "title": "Reducing The Roots of Some Evil", "author": ["Zane Lackey"], "link": "https://codeascraft.com/2013/07/16/reducing-the-roots-of-some-evil/", "abstract": "Reducing The Roots of Some Evil Posted by Zane Lackey on July 16, 2013 High profile compromises of SSL Certificate Authorities have become increasingly common over the past several years, and although there are some interesting advancements in this area coming down the line (such as TACK.io), CAs currently present a unique source of risk for a number of organizations. In May of this year, we decided to see what actions we could take to reduce our exposure to compromised or malicious Certificate Authorities and we hope our research will be of use to other groups as well. Our goal was simple: We wanted to determine what CAs the browsers across our organization actually use, and remove the rest from being trusted. While this wouldn’t prevent a user from clicking through a certificate warning from a now untrusted CA, our goal was simply to reduce the number of CAs that could silently MITM the browser traffic of people who work at Etsy with no certificate warnings at all. We felt this would provide us with a practical way to reduce our exposure to compromised and/or malicious CAs that we didn’t even use in the first place. Which CAs? The first question we had to answer was how do we build a comprehensive list of what CAs our systems actually use? Arguably we could guess that certain CAs shouldn’t be on our systems by name alone (we’re looking at you, “AAA Certificate Services”) however we wanted to gather real data we could analyze and make our decisions from. Enter CAWatch After debating ideas such as pushing browser plugins to record CAs seen by the browser, we decided to implement a simple solution called CAWatch. CAWatch sits at the main network egress point of our Brooklyn HQ and, unsurprisingly, watches for CA certificates crossing the wire. Whenever a CA is observed, CAWatch logs the name of the CA and the time it was seen. In order to ensure there was no privacy impact, no information about the site being visited is recorded. Our first step in creating CAWatch was to obtain a list of CA certificates currently trusted by our standard laptop/desktop endpoints so that we knew what to watch for. On most modern desktops, there are two sources of these certificates: the OS key storage mechanism, and the collection of keys baked in to Firefox. To acquire the CAs built in to OSX one can simply use the Keychain Access utility and export all System Roots. However, Firefox does not natively have support for exporting all the CA certs (although they can be exported manually one by one) so we used a Firefox plugin called Export All Certificates to obtain them all in bulk. Unfortunately, the certificates are then exported in DER format when we needed them in PEM format. Luckily they can be easily converted from DER to PEM with the following command: mkdir output; ls *.der | xargs -L 1 -I {} openssl x509 -in {} -inform DER -out 'output/{}' -outform PEM; cat output/*.der > certs.pem; rm -rf ./output/ Make sure to use a freshly installed Firefox profile separate from normal use, otherwise any CA certificates you have made exceptions for will end up in the output. Our Results During the two months we’ve had CAWatch in operation, we’ve seen only 61 unique CA certificates cross the wire. This accounts for slightly less than 29% of the 212 total CA certificates installed by default in our standard build, and means that we could significantly reduce our exposure by removing a large number of unused CAs. Of the 61 observed, below are the 23 CA’s we saw more than 0.01% of the time: 21.29% EQUIFAX SECURE CERTIFICATE AUTHORITY 10.37% ENTRUST.NET SECURE SERVER CERTIFICATION AUTHORITY 10.07% DIGICERT HIGH ASSURANCE EV ROOT CA 8.97% GO DADDY CLASS 2 CERTIFICATION AUTHORITY 7.91% GEOTRUST GLOBAL CA 7.23% ADDTRUST EXTERNAL CA ROOT 6.48% HTTP://WWW.VALICERT.COM/ 6.04% GTE CYBERTRUST GLOBAL ROOT 4.45% VERISIGN CLASS 3 PUBLIC PRIMARY CERTIFICATION AUTHORITY - G5 4.08% CLASS 3 PUBLIC PRIMARY CERTIFICATION AUTHORITY 3.82% BALTIMORE CYBERTRUST ROOT 3.22% CLASS 3 PUBLIC PRIMARY CERTIFICATION AUTHORITY - G2 1.37% THAWTE PRIMARY ROOT CA 1.36% THAWTE PREMIUM SERVER CA 1.33% ENTRUST.NET CERTIFICATION AUTHORITY (2048) 0.65% GLOBALSIGN ROOT CA 0.47% STARTCOM CERTIFICATION AUTHORITY 0.22% COMODO CERTIFICATION AUTHORITY 0.20% STARFIELD CLASS 2 CERTIFICATION AUTHORITY 0.16% UTN-USERFIRST-HARDWARE 0.11% NETWORK SOLUTIONS CERTIFICATE AUTHORITY 0.09% UTN - DATACORP SGC 0.05% AMERICA ONLINE ROOT CERTIFICATION AUTHORITY 1 [The 38 CAs which had < 0.01% traffic have been edited out for brevity] Removing Unused Certificates Although we’re waiting on a few more months of data to begin removing certificates en masse, we’ve begun the process on a few test machines. Removing unused certificates from the OS X Keychain is quite straightforward and can be done via the Common Name of the certificate or its SHA1 fingerprint with the following commands: /usr/bin/security delete-certificate -Z {SHA1 Fingerprint} /System/Library/Keychains/SystemRootCertificates.keychain or /usr/bin/security delete-certificate -t {Common Name} /System/Library/Keychains/SystemRootCertificates.keychain Conclusions Based on our first two months of data we have removed a number of unused CA certificates from some pilot systems to test the effects, and will run CAWatch for a full six months to build up a more comprehensive view of what CAs are in active use. Over the coming months, as we confirm there are no issues, we’ll reduce the certificates installed on endpoints across the organization. While this approach certainly doesn’t solve all risks associated with the Certificate Authority system, we feel that it provides some practical steps for reducing overall exposure. We hope that by sharing our results and methodologies from this research it will help other organizations interested in similarly limiting their exposure to compromised CAs. Additional information on this and other security research will be discussed in a talk entitled Attack-Driven Defense at the upcoming Nordic Security Conference . This post was written by Zane Lackey (@zanelackey) of the Etsy Security Team Posted by Zane Lackey on July 16, 2013 Category: engineering , security", "date": "2013-07-16,"},
{"website": "Etsy", "title": "June 2013 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2013/07/11/june-2013-site-performance-report/", "abstract": "June 2013 Site Performance Report Posted by Lara Hogan on July 11, 2013 The second quarter of 2013 has come to a close, and it’s time for another site performance report. In our last update we had modest improvements across the board from code changes and removing a module from the homepage. The story this time is the huge win we saw from upgrading to PHP 5.4. Server Side Performance: Here are the median and 95th percentile load times for signed in users on our core pages on Wednesday, June 26th: As you can see, we had a significant drop in load time across all pages, at both the median and the 95th percentile.  As I mentioned above, this was primarily due to upgrading all of our webservers to PHP 5.4.  One of the core enhancements in PHP 5.4 was improved performance and reduced memory consumption, and this really worked out well for us at Etsy: In addition to improved performance, the reduced memory usage in PHP 5.4 allows each of our servers to handle more traffic, so we gained capacity while serving every request more quickly – not a bad deal for a software upgrade. To be clear, this wasn’t a trivial process; it took us months to make sure that we could safely upgrade without breaking anything, and to roll out the upgrade on a gradual basis, fixing things along the way. Our engineers put in a huge amount of effort to get this done, and it paid off in a big way. The transition went flawlessly, and now we get to reap the benefits. If you are a PHP shop and you aren’t on 5.4, I strongly recommend starting the upgrade process. The other big improvement we had on the server side was due to a change in the way we load translation files. Etsy has buyers and sellers in many countries, and the site is currently available in 9 different languages. Until recently we were loading our translations from JSON files on every request, which could take up to 100ms for the larger languages. By changing these to PHP arrays and caching them with OPcache , this time dropped to ~20ms. This improvement only shows up in the 95th percentile numbers, since the majority of our traffic is in US english which doesn’t have this overhead. Front-end Performance: As usual, we are using our private instance of WebPagetest to get synthetic measurements of front-end load time. We use a DSL connection and test with IE8, IE9, Firefox, and Chrome. Here is the data: Overall, the time to start render is pretty stable, with a decent decrease on the Shop page. This is largely due to a change we made to move all of our JavaScript to the bottom of the page. Even though this is one of the core rules for fast websites, like many websites we had some bootstrap files in the <head>, and some additional JavaScript sprinkled throughout the document. We moved everything to the bottom of the page as a test, and we saw an impressive improvement in start render time. We are now in the process of rolling this out to all of our other pages. You will also notice that the document complete times jumped on the listing page and the shop page. For the shop page, we think this is also because we moved the JavaScript. In IE8 specifically we saw a large jump in document complete time after this change. We are thinking about how best to accomodate those users given the diminishing market share of that browser version, but luckily this problem does not appear in later versions of IE. For the listing page, we are rolling out a new version of it that is currently undergoing testing, and we are actively working on tuning its performance. Hopefully that number will come down significantly for the next report. We are also introducing Real User Monitoring (RUM) data to this report on the front-end.  This data is collected with LogNormal/mPulse , and looks at the latest version of Chrome only, since it implements the Navigation Timing spec and is our most popular browser.  Median page load time in seconds is below (baseline is excluded because real users don’t hit that page): The numbers are pretty consistent from six months ago, except for a slight improvement on the homepage and degradation on the search results page. The improvement is expected is part because of the server side improvements we saw above, and the slowdown on search is likely due to front-end changes on that page (new JS files, another row of search ads, etc.). We haven’t really had front-end performance in our sights over the past quarter, so we’re happy to see that things have remained fairly stable. Conclusion: Once again the news in this report is predominantly good. Server side times continue to get better without much work from the performance team – we simply sit back and let hardware and software upgrades do our work for us :-). Front-end performance is pretty stable across our major pages, even though we haven’t spent a ton of time working on them. Our JS deferral strategies are paying off in the time to start render metrics, which is a huge benefit to all of our users. Over the next quarter we are going to continue to focus on improving our slowest pages (typically not pages that appear on this report) and look into bringing down the document complete time for the Listing page. We’re also kicking off a project to investigate how the use of progressive JPEGs impacts customer behavior on Etsy.com. We’ll post the results of that study on CodeAsCraft once we have them! You can follow Jonathan on Twitter at @jonathanklein Posted by Lara Hogan on July 11, 2013 Category: performance", "date": "2013-07-11,"},
{"website": "Etsy", "title": "Atomic deploys at Etsy", "author": ["Rasmus Lerdorf"], "link": "https://codeascraft.com/2013/07/01/atomic-deploys-at-etsy/", "abstract": "Atomic deploys at Etsy Posted by Rasmus Lerdorf on July 1, 2013 A key part of Continuous Integration is being able to deploy quickly, safely and with minimal impact to production traffic. Sites use various deploy automation tools like Capistrano, Fabric and a large number of homegrown rsync-based ones. At Etsy we use a tool we built and open-sourced called Deployinator . What all these tools have in common is that they get files onto multiple servers and are able to run commands on those servers. What ends up varying a lot is what those commands are. Do you clear your caches, graceful your web server, prime your caches, or even stagger your deploys to groups of servers at a time and remove them from your load balancer while they are being updated? There are good reasons to do all of those things, but the less you have to do, the better. It should be possible to atomically update a running server without restarting it and without clearing any caches. The problem with deploying new code to a running server is quite simple to understand. A request that starts on one version of the code might access other files during the request and if those files are updated to a new version during the request you end up with strange side effects. For example, in PHP you might autoload files on demand when instantiating objects and if the code has changed mid-request the caller and the implementation could easily get out of synch. At Etsy, it was quite normal to have to split significant code changes over 3 deploys before implementing atomic deploys. One deploy to push the new files. A second deploy to push the changes to make use of the new code and a final deploy to clean up any outdated code that isn’t needed anymore. The problem is simple enough, and the solution is actually quite simple as well. We just need to make sure that we can run concurrent requests on two versions of our code. While the problem statement is simple, actually making it possible to run different versions of the code concurrently isn’t necessarily easy. When trying to address this problem in the past, I’ve made use of PHP’s realpath cache and APC (a PHP opcode cache, which uses inodes as keys). During a deploy the realpath cache retains the inodes from the previous version, and the opcode cache would retain the actual code from the previous version. This means that requests that are currently in progress during a deploy can continue to use the previous version’s code as they finish. WePloy is an implementation of this approach which works quite well. With PHP 5.5 there is a new opcode cache called Opcache (which is also available for PHP 5.2-5.4). This cache is not inode-based. It uses realpaths as the cache keys, so the inode trick isn’t going to work anymore. Relying on getting the inodes from a cache also isn’t a terribly robust way of handling the problem because there are still a couple of tiny race windows related to new processes with empty caches starting up at exactly the wrong time. It is also too PHP-oriented in that it relies on very specific PHP behaviour. Instead of relying on PHP-specific caching, we took a new look at this problem and decided to push the main responsibility to the web server itself. The base characteristic of any atomic deploy mechanism is that existing requests need to continue executing as if nothing has changed. The new code should only be visible to new requests. In order to accomplish this in a generic manner we need two document roots that we toggle between and a new request needs to know which docroot it should use. We wrote a simple Apache module that calls realpath() on the configured document root. This allows us to make the document root a symlink which we can toggle between two directories. The Apache module sets the document root to this resolved path for the request so even if a deploy happens in the middle of the request and the symlink is changed to point at another directory the current request will not be affected. This avoids any tricky signaling or IPC mechanisms someone might otherwise use to inform the web server that it should switch its document root. Such mechanisms are also not request-aware so a server with multiple virtual hosts would really complicate such methods. By simply communicating the docroot change to Apache via a symlink swap we simplify this and also fit right into how existing deploy tools tend to work. We called this new Apache module mod_realdoc . If you look at the code closely you will see that we are hooking into Apache first thing in the post_read_request hook. This is run as soon as Apache is finished reading the request from the client. So, from this point on in the request, the document root will be set to the target of the symlink and not the symlink itself. Another thing you will notice is that the result of the realpath() is cached. You can control the stat frequency with the RealpathEvery Apache configuration directive. We have it set to 2s here. Note that since we have two separate docroots and our opcode cache is realpath-based, we have to have enough space for two complete copies of our site in the cache. By having two docroots and alternating between them on successive deploys we reuse entries that haven’t changed across two deploys and avoid “ thundering herd ” cache issues on normal deploys. If you understand things so far and have managed to compile and install mod_realdoc you should be able to simply deploy to a second directory and when the directory is fully populated just flip the docroot symlink to point to it. Don’t forget to flip the symlink atomically by creating a temporary one and renaming it with “ mv -T “. Your deploys will now be atomic for simple PHP, CGI, static files and any other technology that makes use of the docroot as provided by Apache. However, you will likely have a bit more work to do for more complex scenarios. You need to make sure that nothing during your request uses the absolute path to the document_root symlink. For example, if you configure Apache’s DOCUMENT_ROOT for your site to be /var/www/site/htdocs and then you have /var/www/site be a symlink to alternatingly /var/www/A and /var/www/B you need to check your code for any hardcoded instances of /var/www/site/htdocs. This includes your PHP include_path setting. One way of doing this is to set your include_path as the very first thing you do if you have a front controller in your application. You can use something like this: ini_set('include_path', $_SERVER['DOCUMENT_ROOT'].'/../include'); That means once mod_realdoc has resolved /var/www/site/htdocs to /var/www/A/htdocs your include_path will be /var/www/A/htdocs/../include for the remainder of this PHP request and even if the symlink is switched to /var/www/B halfway through the request it won’t be visible to this request. At Etsy we don’t actually have a front controller where we could easily make this app-level ini_set() call, so we wrote a little PHP extension to do it for us. It is called incpath . This extension is quite simple. It has three ini settings. incpath.docroot_sapi_list specifies which SAPIs should get the docroot from the SAPI itself. incpath.realpath_sapi_list lists the SAPIs which should do the realpath() call natively. When the extension does the realpath() itself it is essentially a PHP version of the mod_realpath module resolving the symlink in the extension itself. And finally, incpath.search_replace_pattern specifies the string to replace in the existing include_path. It is easier to understand with an example. At Etsy we have it configured something like this: incpath.docroot_sapi_list = apache2handler\nincpath.realpath_sapi_list = cli\nincpath.search_replace_pattern = /var/www/site/htdocs This means that when running PHP under Apache we will get the document root from Apache (apache2handler) and we will look for “/var/www/site/htdocs” in the include_path and replace it with the document root we got from Apache. For cli we will do the realpath() in the extension and use that to substitute into the include_path. Our PHP configuration then has the include_path set to: /var/www/site/htdocs/../include:. which the incpath extension will modify to be either /var/www/A/htdocs/../include or /var/www/B/htdocs/../include. This include_path substitution is done in the RINIT PHP request hook which runs at the beginning of every request before any PHP code has run. The original include_path is restored at the end of the request in the RSHUTDOWN PHP hook. You can, of course, specify different search_replace_pattern values for different virtual hosts and everything should work fine. You can also skip this extension entirely and do it at the app-level or even through PHP’s auto_prepend functionality. Some caveats. First and foremost this is about getting atomicity for a single web request. This will not address multi-request atomicity issues. For example, if you have a request that triggers AJAX requests back to the server, the initial request and the AJAX request may be handled by different versions of the code. It also doesn’t address changes to shared resources. If you change your database schema in some incompatible way such that the current and the new version of the code cannot run concurrently then this won’t help you. Any shared resources need to stay compatible across your deploy versions. For static assets this means you need proper asset versioning to guarantee that you aren’t accessing incompatible js/css/images. If you are currently using Apache and a symlink-swapping deploy tool like Capistrano, then mod_realdoc should come in handy for you, and it is likely to let you remove a graceful restart from your deploy procedure. For non-Apache, like nginx, it shouldn’t be all that tricky to write a similar plugin which does the realpath() call and fixates the document root at the top of a request insulating that request from a mid-request symlink change. If you use this Apache module or write your own for another server, please let us know in the comments. Posted by Rasmus Lerdorf on July 1, 2013 Category: infrastructure Tags: atomic deploy , continuous deployment , continuous integration , deployinator , opcache , PHP", "date": "2013-07-1,"},
{"website": "Etsy", "title": "Introducing Kale", "author": ["Abe Stanway"], "link": "https://codeascraft.com/2013/06/11/introducing-kale/", "abstract": "Introducing Kale Posted by Abe Stanway on June 11, 2013 In the world of Ops, monitoring is a tough problem. It gets harder when you have lots and lots of critical moving parts, each requiring constant monitoring. At Etsy, we’ve got a bunch of tools that we use to help us monitor our systems. You might be familiar with some of them: Nagios, StatsD, Graphite, and Ganglia. Today, we’d like to introduce you to a new tool that we’ve been working on for the past few months. This tool is designed to solve the problem of metrics overload. What kind of overload are we talking about? Well, at Etsy, we really love to make graphs. We graph everything! Anywhere we can slap a StatsD call, we do. As a result, we’ve found ourselves with over a quarter million distinct metrics. That’s far too many graphs for a team of 150 engineers to watch all day long! And even if you group metrics into dashboards, that’s still an awful lot of dashboards if you want complete coverage. Of course, if a graph isn’t being watched, it might misbehave and no one would know about it. And even if someone caught it, lots of other graphs might be misbehaving in similar ways, and chances are low that folks would make the connection. We’d like to introduce you to the Kale stack, which is our attempt to fix both of these problems. It consists of two parts: Skyline and Oculus . We first use Skyline to detect anomalous metrics. Then, we search for that metric in Oculus, to see if any other metrics look similar. At that point, we can make an informed diagnosis and hopefully fix the problem. Skyline Skyline is an anomaly detection system. It shows all the current metrics that have been determined to be anomalous: You can hover over all the metric names and view the graphs directly. Our algorithms do a good job filtering most of the non-anomalous metrics, but for now, they certainly aren’t as good at humans at pattern matching. So, our philosophy is to err on the side of noise – it’s very easy to scroll through a few more false positives if it means you get to catch all the real anomalies. Once you’ve found a metric that looks suspect, you can click through to Oculus and analyze it for correlations with other metrics! Oculus Oculus is the anomaly correlation component of the Kale system. Once you’ve identified an interesting or anomalous metric, Oculus will find all of the other metrics in your systems which look similar. It lets you search for metrics, using your choice of two comparison algorithms… and shows you other metrics which are similar for the same time period. You can even save interesting metrics into a collection (complete with your own notes), for example if a particular set of related graphs occurred during a complex outage: Oculus will then search through all of your saved metric collections along with your other metrics, and will show you any matches alongside your regular search results. What’s that? You’re seeing the same pattern of graphs as you saved during that last site outage you had? Gee, thanks Oculus! Going further into the juicy technical depths of each system is a tad beyond the scope of this introductory post, but we’ll be sure to post more about it in the coming weeks. We will also be speaking about it at Velocity next week (see the abstract here ), so if you’re around, come and say hi! In the meantime, we are open sourcing everything today, and we are looking forward to feedback from the community. Go ahead and try it out! You can find Skyline and Oculus at http://github.com/etsy/skyline and http://github.com/etsy/oculus. monitoring <3, Abe and Jon Abe tweets at @abestanway , and Jon tweets at @jonlives Update: It came to our attention that the “Loupe” name is also used by a commercial product, and so we have changed the name to Kale. Posted by Abe Stanway on June 11, 2013 Category: data , monitoring , operations", "date": "2013-06-11,"},
{"website": "Etsy", "title": "Leveraging Big Data To Create More Secure Web Applications", "author": ["Mike Arpaia"], "link": "https://codeascraft.com/2013/06/04/leveraging-big-data-to-create-more-secure-web-applications/", "abstract": "Leveraging Big Data To Create More Secure Web Applications Posted by Mike Arpaia on June 4, 2013 Here at Etsy, we take measuring things very seriously. We have previously discussed how we harness data to make decisions about software, operations and products, but we have said little about just how useful data can be for the information security practices of a modern web application. Our Data Team has written several blog articles about how we build, maintain and take advantage of our data infrastructure. Over the years, our data stack has grown to consist of several technologies, including Hadoop and Cascading (which we’ve written about in the past .) At a high level, Cascading leverages Hadoop’s infrastructure while abstracting standard data processing operations, such as splits and joins, away from the underlying mapper and reducer tasks. Cascading allows us to write analytics jobs quickly and easily in a familiar languages (we use both the JRuby and Scala DSL ‘s for Cascading). With a mature Hadoop stack and a multitude of helpful data engineers, the Security Team at Etsy has been taking advantage of the data stack in increasing amounts over the past few months to strengthen our security posture. Broadly speaking, there are three main types of security practices in which we utilize big data: reactive security mechanisms, proactive security mechanisms, and incident response security practices. Reactive Security Mechanisms Reactive security mechanisms usually consist of real-time event monitoring and alerting. These security mechanisms focus on events that trigger immediate responses based on regular information querying: they query the same data and they query it often. Some examples of reactive security mechanisms at Etsy are automated Splunk searches and anomaly detection based on StatsD/Graphite logs. We use saved Splunk searches to identify anomalous patterns in access logs and error logs, such as cross-site scripting and increasing failed log-in rates. These searches typically run once a minute in order to give us a real-time monitoring mechanism. We also built an anomaly detection system based on logs of potentially suspicious events such as failed logins. These two mechanisms are similar with respect to the frequency of data aggregated (constantly) and the frequency of analysis (almost constantly). As you may guess, reactive security mechanisms cannot be easily implemented in Hadoop. Although Hadoop is fast (isn’t that the point!), the main benefit of using big data infrastructure is to churn through huge quantities of data. For reactive security mechanisms, we want to get as close as possible to real-time results. It wouldn’t be efficient or rational to run this type of analysis on our Hadoop cluster. Although reactive security mechanisms aren’t performed on our cluster, the initial data gathering step is perfectly suited for Hadoop. Figuring out where the thresholds lie for certain metrics by performing predictive analytics and forecasting on past data is a fantastic way to save time that would previously have been spent over several weeks fine-tuning Splunk queries. Proactive Security Mechanisms Proactive security mechanisms seek to reduce attack surface or eliminate entire vulnerability classes. This category includes mechanisms such as content security policy, output encoding libraries or full-site SSL. These mechanisms are intended to improve the long-term security posture of the application, rather than collect data about an on-going attack. Similarly to reactive security mechanisms, we can use predictive analytics and forecasting in Hadoop to weigh the value of our proactive security mechanisms. For example, when determining if our load balancers could handle activating full-site SSL for all Etsy sellers, we ran Hadoop jobs that analyzed past traffic of our sellers to figure out how many requests were made by sellers that were not HTTPS. Armed with this data, as well as metrics from our load balancers, we were able to push out full-site SSL for sellers without encountering unexpected capacity issues. require 'helpers/analytics'\n\nanalytics_cascade do\n  analytics_flow do\n    analytics_source 'event_logs'\n    tap_db_snapshot 'users_index'\n\n    assembly 'event_logs' do\n      group_by 'user_id', 'scheme' do\n        count 'value'\n      end\n    end\n\n    assembly 'users_index' do\n      project 'user_id', 'is_seller'\n    end\n\n    assembly 'ssl_traffic' do\n      project 'user_id', 'is_seller', 'scheme', 'value'\n      group_by 'is_seller', 'scheme' do\n        count 'value'\n      end\n    end\n\n    analytics_sink 'ssl_traffic'\n  end\nend The Cascading.jruby source code for this task is relatively straightforward. The output of this Hadoop job allowed us to visualize Seller HTTP traffic vs Seller HTTPS traffic. However, unlike reactive security mechanisms, we can use Hadoop to create proactive security mechanisms. The key difference is that these security mechanisms cannot be used to monitor critical metrics that require immediate response or attention. Fortunately, the results of proactive security mechanisms typically don’t require immediate attention. In the SSL example, before we were able to push out full-site SSL for sellers, we had a daily Hadoop job that would break down request patterns to show us which URLs/patterns were requested most often over HTTP. We used this data to iteratively change the scheme of these high value URLs from HTTP to HTTPS while still having data to support the fact that this wouldn’t surpass our restriction on load-balancer terminated SSL connections. Incident Response Web application incident response is something that is done often in practice, as it is not limited to responding to full blown compromise. We often need to investigate a threat that has recently targeted organizations that are similar to our own or investigate a new exploit that may affect our technology stack. Typically we are looking for identifying patterns such as URL patterns or IP addresses that have repeatedly accessed our application. Since similar incident response actions are performed frequently, our incident response practices need to be repeatable and, since an incident can occur at any time, we must be able to get the results of our analysis quickly. Even though we want our analysis to be generalizable, it is dependent on the particular threat we are responding to and thus the parameters often need to be changed. Given all of these conditions, incident response is a perfect example of when to use big data. Incident response is ad-hoc analysis of a large dataset that is driven by an event or incident. We are not going to do it more than once and it needs to be fast. This is a textbook use-case of Hadoop and we take advantage of it constantly for this purpose. Writing template Hadoop jobs that scan our access logs for visits from target IP address or visits to known malicious URL patterns that are easily pluggable with new incident details has proved invaluable to our incident response practices. Conclusion The security posture of an application is directly proportional to the amount of information that is known about the application. Big data can be a great source of this kind of information and can be used to gather data to create reactive security mechanisms, gather data to create proactive security mechanisms, directly create new proactive security mechanisms, and to perform incident response. Although the advantages of analytics from a data science perspective are well-known and well documented, the advantages of analytics from a security perspective have not been explored in-depth. We have found big data to be extraordinarily useful in both creating reactive and proactive security mechanisms, as well as to aiding in incident response. We hope that this will help other organizations in using their data and analytics capabilities more effectively. Want to know more about how Etsy uses Hadoop to create a more secure web application? The authors of this blog post (Mike Arpaia & Kyle Barry) will be presenting a more in-depth discussion of this topic at the upcoming Black Hat USA conference and Nordic Security Conference . If this has been of interest we hope you’ll get a chance to check out our presentations. You can follow Mike on Twitter at @mikearpaia and you can follow Kyle on Twitter at @allofmywats . Posted by Mike Arpaia on June 4, 2013 Category: data , security Tags: data , security Related Posts Posted by Emily Sommer , Mike Adler , John Perkins , Joshua Thiel , Hilary Young , Chelsea Mozen , Dany Daya and Katie Sundstrom on 23 Apr, 2020 Cloud Jewels: Estimating kWh in the Cloud Posted by Mohit Nayyar on 01 May, 2017 Modeling Spelling Correction for Search at Etsy Posted by Ken Lee and Kai Zhong on 15 Sep, 2016 Introducing 411: A new open source framework for handling alerting", "date": "2013-06-4,"},
{"website": "Etsy", "title": "Culture Hacking With A Staff Database", "author": ["Ian Malpass"], "link": "https://codeascraft.com/2013/05/31/culture-hacking-with-a-staff-database/", "abstract": "Culture Hacking With A Staff Database Posted by Ian Malpass on May 31, 2013 When your startup is just you and the cat, your company communication problems are largely whether or not the cat listens when you tell it to get off your keyboard. Scale to a handful of people and you still know everyone, what they’re working on, where they are, and how to get hold of them when you need them. As you scale further, communication becomes increasingly complex. And then Mother Nature throws a hurricane at you and you really need to know that everyone is safe and well, and do it quickly and efficiently. We’ve tried to solve “who” problems with an open, accessible staff database. Free your staff data Most companies will have some form of HR system with all sorts of useful information in it, but much of that is private and confidential, and locked away (and rightly so). What’s needed is something more basic: Who is this person? What do they look like? Where are they? What do they do? How to I get in touch with them? But, because we took our HR data and re-published the “safe” bits to a database, the staff directory data is completely at our mercy. We’re not subject to what the designers of our HR information system think people should look at. And if what we need changes, we can adapt to suit. Even better, because the data is in the same ecosystem as our regular web site data, anyone who knows how to build something for etsy.com can also build something with our staff data with no particular extra knowledge. Making it easy to hack (unsurprisingly) makes for more hacks, and more hacks allows for useful and unexpected tools to crop up to help you scale better, without any particular central planning. Who are you? The obvious thing to build on top of this database is a “staff directory”. We have one, it’s great. It lives with our other internal web-based support tools, easily available to all. For an organisation merrily blazing past Dunbar’s number , it’s critical in helping us maintain a sort of “outboard brain” for who’s who in the company. (This is an ongoing problem, since we’re still hiring .) Extending that, during a past Hack Week, a group of us built a game where you were given a photo of someone, and multiple choices for who they were. Add to that score tracking and a leader board and we had people enthusiastically learning who everyone in the company was. (It was too easy for some people, so we quickly developed a mode where you had to actually type in the right name – the game equivalent of meeting them in a hallway.) Smooth small bumps as well as large ones Hacks don’t have to be complex to be useful. Adding a “You’ve Got Mail” button to the directory simplified the process of sorting the incoming physical mail to the office and meant that people didn’t have to go and check the mail room regularly. Being able to smooth out even minor niggles like this contributes to the feeling of an office that just works. They don’t even have to be permanent features to be useful – one-off queries can be valuable too. At one point, I needed to work out which of my colleagues sold vintage items on Etsy in order to do some research on a new feature. I could have emailed everyone and hoped to get replies, but instead I could extract the list of staff shops from the directory, and then extract their listings from the listings table, and work out exactly who sold vintage. Are you OK? A rather more acute and impromptu use for the data came during Hurricane Sandy, when we wanted to be able to do a roll call to make sure our colleagues were safe and sound. It was trivial to take the data, export name, location, phone numbers, and team to CSV and import it into Google Docs, and then we had a shared document where we could track how everyone was. At the same time, we wanted everyone to be able to get everyone’s contact details into their phones easily in case we needed to get hold of people quickly. Some quick checking of Google and Wikipedia produced the vCard spec and, while not cutting edge, the 3.0 version is very amenable to quick hacking. (More recent versions are fine too, of course, but 3.0 hit the spot for the amount of time and energy I had available for hacking.) There are many, many fields in the full spec, but it’s the work of moments to identify the main ones we care about: name, email, phone numbers, and organization.We also happened to have a field for Skype nicks, so I went ahead and added that. (Staff members have voluntarily entered their phone numbers in our staff database, and the vCards are only accessible for the staff directory, so we felt safe that these vCards would be acceptable. You should always consider your company’s data sources and culture, as well as information security issues when hacking on company data.) In addition, we have photos of many of our staff members on the about us page, so I did some quick work to include a photo where we had one. (The vCard spec allows you to embed photos as base64-encoded strings.) Nothing complicated, and only a few minutes to implement, but it adds a nice polish to the cards. We can generate sets of vCards for departments and teams, the whole organisation, just new hires, etc., to make managing your contacts easier. Call me, maybe The last hurricane-inspired hack we did was an emergency broadcast system so that we could quickly update staff members by SMS about rapidly-changing events or urgent alerts (such as unplanned office closures). We already have a Twilio account, along with PHP libraries to use it so all that was required was to validate the phone numbers to make sure they were valid US numbers, and provide a little bit of code to filter the directory by location (we only want to SMS Brooklynites if the Brooklyn office is closed, for example) and the actual sending is simply sending a suitable request to the Twilio REST API. The implementation also included plenty of error logging and reporting and the like because it’s important to know who you didn’t alert too. Future work may include extending the system to accept replies, so that we can get the bulk of roll calls done quickly and be able to concentrate on finding and helping those who need it. When HR systems go to the dogs One of the notable features of our staff directory’s database schema is the “human” column. The office dog pack is a sufficiently important and intrinsic part of the office culture that it was considered necessary to include them in the database. The past Hack Week saw an expansion of this with a full Doggie DB hack that lets you get to know the pack better. And it might seem trivial on the surface, but that’s because you haven’t had to use the “You’ve Got Poop” button…. All these hacks were (a) easy, (b) unforeseen when we started the staff database project, and (c) helped us in growing the business and keeping our colleagues safe and informed. Grab your company’s data and see what you can do. You can follow Ian on Twitter at @indec . Posted by Ian Malpass on May 31, 2013 Category: engineering , people", "date": "2013-05-31,"},
{"website": "Etsy", "title": "At Percona MySQL Conference This Week", "author": ["John Goulah"], "link": "https://codeascraft.com/2013/04/23/at-percona-mysql-conference-this-week/", "abstract": "At Percona MySQL Conference This Week Posted by John Goulah on April 23, 2013 A few of us are at the Percona MySQL Conference this week in Santa Clara. Please come say hello, and if you have a chance today stop by the talk about development at scale at 1:20PM today Tuesday the 23rd in Ballroom B. Posted by John Goulah on April 23, 2013 Category: databases Tags: mysql , santa clara , technology", "date": "2013-04-23,"},
{"website": "Etsy", "title": "March 2013 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2013/04/09/march-2013-site-performance-report/", "abstract": "March 2013 Site Performance Report Posted by Lara Hogan on April 9, 2013 Four more months have gone by since our last update , and it’s time for another performance report!  We made it through the holiday season with flying colors , and our engineers have been hard at work launching new features since the new year started.  Let’s see what impact this had on performance (spoiler: we keep getting faster). Server Side Performance: Here are the median and 95th percentile load times for signed in users on our core pages on Thursday, 3/14/13: As you can see we had small decreases in load time across the board (the one exception being a tiny uptick in median search performance).  We expected this, since we made a couple of small changes to our application code in the last couple of months that had a positive impact across every page.  The larger decrease in the 95th percentile load time for the homepage was primarily due to the removal of the Taste Test module. As a quick reminder, the “Baseline” page is an extremely simple page that just includes our header and footer, and uses our normal controller architecture.  Improvements in the load time of this page mean improvements across every page on Etsy. We also thought it would be fun to look back and compare our performance today to our performance from the very first performance report , back in August of 2011.  Since we don’t look at averages anymore, we can only compare the 95th percentile: Looking good!  We have made huge strides on the homepage, shop page, and profile page, and a small improvement on the search page.  The listing page hasn’t seen much optimization given its position as the fastest of these pages, so we’re happy with its modest gains.  It’s extremely gratifying to see how much progress we have made over the last ~18 months, and we hope to continue this trend in the months to come. Front-end Performance: We changed our strategy for monitoring front-end performance in our last update, and promised more stable metrics going forward.  That proved to be the case: Performance looks good here as well – reductions in load time for almost every page type again.  The one outlier here is our baseline page, which saw a significant increase in document complete time since our last update.  We’ve started digging into this, and it looks like a couple of extra HTTP requests have snuck into this page – a JS file and a sprite or two.  We’ll be consolidating these requests in the coming days and we expect to see the document complete time on this page drop back down. As far as the rest of the improvements go, we believe that they are due to a combination of changes that we’ve deliberately made to improve performance, and a better hit rate at our CDNs.  We are currently load balancing static content across multiple CDNs, and we’ve worked to improve cachability so that fewer requests have to hit our origin. Conclusion: Overall we are happy with the way performance has been trending, especially because we’ve been focusing our efforts over the last 4 months on projects that don’t involve these core pages.  For the next couple of quarters our plan is to establish clear performance SLAs and bring all of our major pages into compliance with them.  Beyond that we are always looking for big wins and evaluating new standards like SPDY , WebP , and pre-fetching to see if they make sense for Etsy.  It’s an exciting time to be working on web performance! Does this sound like something you would like to work on?  Our team is hiring ! You can follow Jonathan on Twitter at @jonathanklein Posted by Lara Hogan on April 9, 2013 Category: performance", "date": "2013-04-9,"},
{"website": "Etsy", "title": "Re-Exploring New Technologies: The Turbo Encabulator", "author": ["John Allspaw"], "link": "https://codeascraft.com/2013/04/01/re-exploring-new-technologies-the-turbo-encabulator/", "abstract": "Re-Exploring New Technologies: The Turbo Encabulator Posted by John Allspaw on April 1, 2013 One of the things that we like to do here at Etsy is to push the envelope. Pushing the envelope means reaching deep into unknown technologies and approaches, and to experiment in areas that we are not familiar with. It’s in this vein that we’d like to share some early and unfiltered work: The Turbo Encabulator. Each member of the TE team has stretched their cognitive abilities into new realms, and we’re pretty happy with the results thus far. We know that in many ways, engineering is about solidifying a solution well before the problem statement can be fully described within a socio-technical environment. Of course, we also acknowledge that the notion of socio-emotional technocracy needs to fit within this paradigm. This brings us to some questions: What heterogenius surface areas can we explore as prototypes for availability matching? Under what conditions will our biases influence positively the results of anarcho-syndacalist architectures? Where can we look for indications that our Mean-Time-To-Innocence (MTTI) is improving? Understandably, these challenges are unilateral in their nature and we’ve made a large investment in creating an erinaceous atmosphere in order to produce results. One of these results is the Turbo Encabulator. As you can imagine, sinusoidal repleneration threw us for a loop. 🙂 An internal tech talk can be seen below: Posted by John Allspaw on April 1, 2013 Category: engineering , infrastructure , operations , philosophy , video", "date": "2013-04-1,"},
{"website": "Etsy", "title": "Data Corruption To Go: The Perils Of sql_mode = NULL", "author": ["Keyur Govande"], "link": "https://codeascraft.com/2013/03/19/the-perils-of-sql_mode/", "abstract": "Data Corruption To Go: The Perils Of sql_mode = NULL Posted by Keyur Govande on March 19, 2013 A little while back, we hit an extremely embarrassing issue: 32-bit signed integer overflow on some primary keys. In our architecture, an unsigned 64-bit key is generated by a global ticket server , but the sharded tables’ schemas did not use BIGINT UNSIGNED as the column data-type. Inserts into the affected tables started failing en masse with “duplicate key” errors on key 2147483647. We quickly realized the issue and ALTER ed the impacted tables, but the MySQL behavior of silently truncating all values larger than 2147483647 with the only fail-safe being the primary key constraint was worrying. Any columns that lacked a similar constraint would be experiencing silent data corruption. Some digging led us to the MySQL variable sql_mode . What is sql_mode anyway? By default MySQL is very accepting of bad data and bad queries. At best, it may emit a WARNING , but these are almost always ignored by client libraries such as PHP’s PDO. sql_mode is a run-time setting for MySQL and enabling it makes the database much more vocal by changing most WARNINGs to ERRORs . This helps prevent silent data corruption on write, and unexpected results on read. Here are a few examples of issues it helped fix within our stack and the settings we used: PROBLEM: Missing default values, data-type and data-length mismatches CREATE TABLE `sql_test` `id` TINYINT UNSIGNED NOT NULL PRIMARY KEY, `value` VARCHAR(3) NOT NULL, `column_not_null` VARCHAR(1) NOT NULL ) Engine = InnoDB; Consider the following inserts: INSERT INTO sql_test (id, value, column_not_null) VALUES ( 10000 , 'z', 'y'); INSERT INTO sql_test (id, value, column_not_null) VALUES (1, 'abcde' , 'x'); INSERT INTO sql_test ( id, value ) VALUES (2, 'qwerty'); Here’s what the table would look like when sql_mode is disabled: +-----+-------+-----------------+ | id  | value | column_not_null | +-----+-------+-----------------+ |   1 | abc   | x               | |   2 | qwe   |                 | | 255 | z     | y               | +-----+-------+-----------------+ Massively different from the expected values! When sql_mode='STRICT_ALL_TABLES' is set though, each of those inserts will fail hard so that you can recover and handle the problem in your application. ERROR 1264 (22003): Out of range value for column 'id' at row 1 ERROR 1406 (22001): Data too long for column 'value' at row 1 ERROR 1364 (HY000): Field 'column_not_null' doesn't have a default value PROBLEM: Character-set issues CREATE TABLE `charset_test` ( `message` VARCHAR(3000) NOT NULL -- NB: this could also be `message` TEXT NOT NULL ) Engine = InnoDB DEFAULT CHARSET=UTF8 ; Inserting a supplementary UTF-8 character string like would seem to work, but on a subsequent SELECT, the returned string would be quite different: This is because MySQL’s UTF8 implementation can only accommodate characters up to 3 bytes in length, but Unicode characters could be up to 4 bytes long. The data stored in the message column is subtly truncated and corrupted. With the stricter sql_mode='STRICT_ALL_TABLES' , inserts containing 4-byte characters will fail hard with: ERROR 1366 (HY000): Incorrect string value Note that sql_mode setting here is only an alarm-bell, it is not fixing the underlying issue. The real solution is twofold: Change the character-set of the connection to MySQL to UTF8MB4 . If using PHP’s PDO, pass it in via the connection DSN . Or when running queries by hand, make sure to execute SET NAMES utf8mb4 before executing anything. Modify the column and the table definitions to UTF8MB4 : ALTER TABLE `charset_test` DEFAULT CHARSET utf8mb4, MODIFY `message` VARCHAR(3000) CHARACTER SET utf8mb4 NOT NULL; One caveat to note when using the UTF8MB4 charset: the rule-of-thumb for an indexed VARCHAR column being at most 255 characters is no longer valid. MySQL has a length limit of 767 bytes for the index prefix and it has to accommodate the worst case. Thus when using the 3-byte UTF8 character set, 255 characters fit in, but when using 4-byte UTF8MB4 only 191 characters fit. If we absolutely need 255 characters indexed (for example if the column holds email addresses), a workaround is to SHA-1 hash the value, and store and index that column instead. PROBLEM: Unsigned int decrements CREATE TABLE `decrement_test` ( `id` int(11) NOT NULL, `value` int(11) unsigned NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 INSERT INTO decrement_test(id, value) VALUES(1, 20) ON DUPLICATE KEY UPDATE value = value + VALUES(value); SELECT * FROM decrement_test; +----+-------+ | id | value | +----+-------+ | 1  | 20    | +----+-------+ INSERT INTO decrement_test(id, value) VALUES(1, -1 ) ON DUPLICATE KEY UPDATE value = value + VALUES(value); If you were expecting the value to now be 19, you would be wrong: SELECT * FROM decrement_test; +----+-------+ | id | value | +----+-------+ | 1  | 20    | +----+-------+ When sql_mode='STRICT_ALL_TABLES' is enabled, the decrement INSERT statement will fail with: ERROR 1264 (22003): Out of range value for column 'value' at row 1 Note that in order to do atomic decrements as shown above, the column data type would need to be signed-integer rather than unsigned-integer. PROBLEM: Incorrect group-by CREATE TABLE `group_by_test` `id` INT NOT NULL PRIMARY KEY, `name` VARCHAR(30) NOT NULL, `status` INT NOT NULL, `region` INT NOT NULL ) Engine = InnoDB DEFAULT CHARSET = UTF8MB4; INSERT INTO group_by_test(id, name, status, region) VALUES (1, 'etsy', 10, 100); INSERT INTO group_by_test(id, name, status, region) VALUES (2, 'ebay', 12, 100); INSERT INTO group_by_test(id, name, status, region) VALUES (3, 'amazon', 13, 150); INSERT INTO group_by_test(id, name, status, region) VALUES (4, 'fab', 10, 100); SELECT COUNT(1), status, name FROM group_by_test GROUP BY status ; +----------+--------+--------+ | COUNT(1) | status | name   | +----------+--------+--------+ |   2      | 10     | etsy   | |   1      | 12     | ebay   | |   1      | 13     | amazon | +----------+--------+--------+ The `name` column isn’t in the GROUP BY clause, but is in the SELECT and MySQL pulls the first matching value and puts it into the result set. The output in the `name` column depends on relative order of rows in the table and may not be repeatable and this is almost never desirable. By setting sql_mode='ONLY_FULL_GROUP_BY' , we can prevent such unreliable SQL from executing. The query would instead fail with: ERROR 1055 (42000): 'etsy.group_by_test.name' isn't in GROUP BY Just throw the switch and test in Production? Unfortunately our application was continually executing some variations of the bad queries shown above, especially the bad GROUP BY and bad DEFAULT/NOT NULL , and we couldn’t just enable the mode in Production without causing a lot of disruption. We had to find the bad queries and table definitions first. In order to do that, we set up a test database with the Production schema on it and enabled the strict sql_mode='ONLY_FULL_GROUP_BY,STRICT_ALL_TABLES' on it. We slurped all queries from our Production DBs using a local tcpdump and netcat ’ed them over to the test database: prodb-shard> sudo /usr/sbin/tcpdump -i eth0 \"port 3306 and tcp[1] & 7 == 2 and tcp[3] & 7 == 2\" -s 65535 -x -n -q -tttt | nc testdb.etsy.com 20000 On the test box, we used the combination of Percona Toolkit’s pt-query-digest and a custom Go script to execute these queries (NB: Percona Playback did not exist at the time). Our Production servers execute ~4000 queries/sec (~30000 packets/sec in and out) and while pt-query-digest has a built-in execute mode, it cannot keep up with the volume and the tcpdump capture would drop packets. The Go script helped multiplex this query volume and keep packet loss low (~1%). testdb> nc -l 20000 | pt-query-digest --type tcpdump --no-report --print | grep -v \"SET NAMES utf8\" | go run shard_executor.go -db-user=username -db-password=password -db-host=localhost -db-charset=utf8mb4 -log \"/var/tmp/shard.log\" -db-name=etsy_shard -threads=8 Note that pt-query-digest leaks a small amount of memory continually and will need to be restarted every so often. All errors were logged to disk and we now had visibility into exactly what needed fixing: testdb> cut -c 21- /var/tmp/shard.log | grep \"^Error\" | sort | uniq Along with fixing bad queries, we also needed to modify a large number of tables to change the character-set for VARCHAR and TEXT columns from UTF8 to UTF8MB4 in order to accommodate 4-byte UTF-8 characters. We generated the ALTERs with this one-off script and Schemanator allowed us to execute all of these without any downtime! Note that the script does not generate ALTER statements for indexed VARCHAR/TEXT columns longer than 191 characters for reasons explained above. In our case, we worked around it by reducing the column lengths where feasible, and using the hashing method otherwise. A few weeks of bug squashing and schema changes later, our databases and applications were finally ready to accept sql_mode= 'ONLY_FULL_GROUP_BY,STRICT_ALL_TABLES' . CHECK YOUR FLAGS! We strongly encourage everyone running MySQL to review the various sql_mode flags and choose the ones that make the most sense for their set up. 'ONLY_FULL_GROUP_BY,STRICT_ALL_TABLES' is the absolute minimum we’d recommend. We don’t use any of MySQL’s DATE/TIME data-types nor any Engine other than InnoDB in our schemas, but if you do, we strongly suggest also using 'NO_ZERO_IN_DATE,NO_ZERO_DATE,NO_ENGINE_SUBSTITUTION' References and resources Etsy’s sharded database architecture MySQL sql_mode Percona toolkit’s pt-query-digest pt-query-digest ‘s memory leak bug . Although it is marked as Fixed, but it still reproducible with version 2.1.9 The Go query multiplexer UTF-8 to UTF8MB4 VARCHAR and TEXT ALTER generator You can follow Keyur on Twitter at @keyurdg Posted by Keyur Govande on March 19, 2013 Category: databases , engineering , operations", "date": "2013-03-19,"},
{"website": "Etsy", "title": "Java: Not Even Once", "author": ["Avleen Vig"], "link": "https://codeascraft.com/2013/03/18/java-not-even-once/", "abstract": "Java: Not Even Once Posted by Avleen Vig on March 18, 2013 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “allowlist/blocklist” in our Code as Craft entries. Note: This post was co-written by Avleen Vig ( @avleen ) and Zane Lackey ( @zanelackey ) . In mid-January of this year we started an initiative to remove Java browser plugins from all employee systems at Etsy, as we feel this is a best practice to be striving towards. To that end, we wanted to discuss the challenges we encountered when removing Java browser plugins in the hope that it will help other organizations with the removal process. The first question we needed to answer before removing Java was “Who actually needs Java in their browser?” Most organizations face the unfortunate reality that some number of internal systems use Java applets, and that there are groups which use these systems on a daily or weekly basis. In our case, our operations team needed Java to be able to access a group of internal network appliances, such as IPMI console interfaces. Once we identified these requirements, we disabled Java browser plugins across the rest of the organization and set about trying to engineer a safer way for the team to access these network appliances. Initially, we looked at three approaches: Installing the Java plugin in a dedicated browser/browser profile and asking the team to use it to only access the network appliances. Writing a wrapper script to copy the Java plugin into the plugins directory, launch a dedicated browser, then remove the Java plugin when the browser closes. Using nssecurity to allowlist the hosts that could instantiate Java in the browser. However, all of these approaches didn’t fulfill our design goals that the approach be safe by default, be easy to maintain, and ideally wouldn’t require the Java browser plugin on the teams laptops at all. We realized the only way to approach the situation that met our requirements would be to have Java installed in a dedicated, controlled, and isolated environment. This model is similar to a bastion host, or “jump box”. We opted to use NoMachine as the remote desktop protocol because of the increased performance and usability over low latency links. We have operations engineers located in multiple countries and we also occasionally need to use 3G/4G mobile services to diagnose problems, so this was critically important. The installation method we followed for CentOS 6 and FreeNX was: Install the required packages. All of these come from the standard CentOS repos: yum install -y jre freenx gnome-desktop gnome-session \n    gnome-panel nautilus firefox fre (After installation, be sure to replace the provided SSH keys: https://help.ubuntu.com/community/FreeNX#Using_custom_SSH_keys ) Create a symlink from the Java browser plugin, to the plugins directory used by Firefox: ln -s /usr/java/latest/lib/amd64/libnpjp2.so /usr/lib64/mozilla/plugins/libnpjp2.so Now create /etc/nxserver/node.conf with these contents: ENABLE_PASSDB_AUTHENTICATION=\"1\" NX_LOG_LEVEL=7 NX_LOGFILE=/var/log/nx/nxserver.log SESSION_LOG_CLEAN=0 COMMAND_MD5SUM=\"md5sum\" FreeNX is now configured! Each of your users who wants to use the new system needs to do the following steps once to add their user to the NX authentication database: (NOTE: These steps below follow the Ubuntu FreeNX instructions of using a shared account/SSH key for access to the jump system. In this circumstance the risk was accepted as the network appliances that are the end target also use a shared account, so no additional risk was introduced. Obviously different circumstances will have different security requirements.) Copy /etc/nxserver/client.id_dsa.key to your local machine and save it as ~/.ssh/nx_dsa On the jump host, run sudo nxserver --adduser <username> This adds your account to the nxserver database.  Note: at this step it will also add a key to your .ssh/authorized_keys2 file, if you manage this in your configuration management system it will get overwritten, so you should add the key there. On the server again, run sudo nxserver --passwd <username> This sets your password in the nxserver database Download the “NoMachine Player v4” from: http://www.nomachine.com/preview/download-package.php Start the player, click “New connection”. Name: jumpbox Host: jump.your.domain Select: Use the NoMachine login, then click the ... button Check Use an alternate key and point it to ~/<username>/.ssh/nx_dsa Press the X twice and then click the jumpbox connection you see. Enter your login details. Click Create a new session Click Create a new GNOME virtual desktop You should get a gnome desktop, with a firefox icon. Using the jump system approach, we’re now able to firewall off these hosts from Internet browsing, we can re-image them on a frequent basis, and we have a very small number of hosts that need to receive Java updates. This approach also allows us to configure our endpoint management software to nuke Java from orbit if the browser plugin ever shows up on employee systems. While we generally abhor single points of failure in our infrastructure, we felt comfortable in this case because the use of configuration management meant that the environment could be quickly rebuilt if needed. Finally, this approach could also be used to isolate other inherently risky technologies when they are still required by certain groups. In closing, we genuinely hope this helps other organizations facing similar challenges on the road to removing Java browser plugins from the enterprise. Posted by Avleen Vig on March 18, 2013 Category: infrastructure , operations , security", "date": "2013-03-18,"},
{"website": "Etsy", "title": "There and Back Again: Migrating Geolocation Data to GeoNames", "author": ["John Marc Imbrescia"], "link": "https://codeascraft.com/2013/03/26/there-and-back-again-migrating-geolocation-data-to-geonames/", "abstract": "There and Back Again: Migrating Geolocation Data to GeoNames Posted by John Marc Imbrescia on March 26, 2013 People are passionate about where they live. At Etsy we need to keep track of lots of different locations, such as buyers’ billing and shipping addresses and sellers’ shop locations. As Etsy continues to expand internationally we wanted to provide better localization and translations for our location place names. We determined that the best way to effect this change was to move from using a closed location API provider to internal services backed by the open GeoNames data set. Before we could start using GeoNames as our data source we had to map all our existing user locations onto the GeoNames data. There is no established method for creating this mapping so we looked at the data we had and figured out a method. For our existing user locations we had city, region, country, country code, latitude and longitude in a format like this: tokyo             tokyo prefecture   japan            jp   35.670 139.740\nluxembourg city   luxemburg          luxembourg       lu   49.609 6.129\nnewport           england            united kingdom   gb   50.700 -1.295\nearth             texas              united states    us   34.277 -102.508 To execute our mapping we created a PHP script that was able to leverage the much broader amount of data GeoNames provides us and quickly determine if old and new locations matched up. This script is now available on Etsy’s github page . The script relies on a MySQL DB backend that contains the standard data tables distributed by GeoNames with indexes on place names, country codes, and the latitude and longitude columns. For each row in our source data we attempt a few different strategies to try to find a match to the GeoNames data. Our first step is to see if there are any exact text matches to the place name we have in the GeoNames data set. If there is one or more exact matches we sort them by distance from the source latitude and longitude and if the nearest place is less than 20km away we call it a match. 20km is a rather large radius and we could easily run through with much lower limits, but we found that since we sorted by distance already a large radius gave us more positive matches when our data sets disagreed on where the center of large cities should be. If we don’t find an exact text match we look again twice more, once with wildcards around the place name in our source data and once searching the “alternatenames” column in the GeoNames data set instead of the “name” column. This helps us find locations whose names may have changed or have alternate spellings. Results are sorted by distance and again we keep the closest if it falls within our threshold. If we still have not found a match we take a substring of the first several letters of the name and do a wildcard search for that. (The length of the substring depends on the length of the source name and is generally about 35%.) This helps eliminate problems resulting from inconsistent inclusion of things like ‘City’, ‘Township’ and ‘Borough’ from place names in our source data. For this project it was important that we found a match for every location and so we added a final “match at all costs” step.  If none of the previous steps have succeeded we use the source location’s latitude and longitude and get a list of all locations in the source country that are within a few tenths of a degree.  We then sort the results and pick the closest.  This was necessary for less than 1% of our data and allowed us to continue providing local search services even when we were unable to match a city exactly. This was a very exploratory, iterative process as we discovered what worked and what did not, and where our existing data was incomplete or inaccurate. From tuning the radius of the search areas, to dropping all the region data outside of the US (we found that it was contributing a lot of false positive matches, and the results were better without it), we revised and refined the logic until it gave us satisfactory results. After we established a mapping we added a new field to store each seller’s “GeoNameID” in our DB. Where previously we stored all the fields listed above now we need only the GeoNameID.  We do still store the old data format as well which has allowed us to make an easy and transparent transition from using the old data to the new data.  With access to the full breadth of data provided by GeoNames we have been able to speed our Local Search feature.  We now also localize spellings of place names for our international users, so someone searching in German can search near Köln while someone searching in English can look for Cologne. I will be giving a talk on how we handle search and localized place name auto-suggest at Lucene/Solr revolution in San Diego in May and we’ll be publishing more Code as Craft blog posts on geolocation services in the coming months here as well. You can follow John Marc on Twitter @thejohnmarc Posted by John Marc Imbrescia on March 26, 2013 Category: data , databases , engineering , internationalization Tags: etsy , GeoNames , localization , translation Related Posts Posted by Duncan Gillespie and Ben Russell on 22 Mar, 2016 Building a Translation Memory to Improve Machine Translation Coverage and Quality Posted by Dan Miller on 22 Dec, 2014 We Invite Everyone at Etsy to Do an Engineering Rotation: Here’s why Posted by Laurie Denness on 19 Jun, 2014 Opsweekly: Measuring on-call experience with alert classification", "date": "2013-03-26,"},
{"website": "Etsy", "title": "Schemanator: Love Child of Deployinator and Schema Changes", "author": ["Jeff Kolber"], "link": "https://codeascraft.com/2013/01/11/schemanator-love-child-of-deployinator-and-schema-changes/", "abstract": "Schemanator: Love Child of Deployinator and Schema Changes Posted by Jeff Kolber on January 11, 2013 We’ve previously written about our sharded master-master pair database architecture and how that and the Etsy ORM allows us to perform schema changes while keeping the site up . That we can do this at all is really awesome but to actually do this is still hard, risky and time-consuming. We run the site on half of our database servers while we make schema changes to the other half. Then we switch sides and do it again. It’s many steps on many machines and it’s happening while 100+ developers are pushing code. Let’s take a closer look at how we pull this off and the automation we’ve developed around this process. I want to pause to acknowledge that being able to run the site on half of our database servers is in itself the result not only of good design in the ORM but of good ongoing capacity planning in making sure we can carry a full load on half the servers. To understand Schemanator let’s first take a look at how we did schema changes “by hand”. To complete a schema change we need to change the ORM’s configuration a total of 4 times – first to take one half of the databases out of production so we can apply change to them and then to put them back. Then we take the other half out and apply changes and finally put them back. At Etsy we can deploy relatively quickly – generally less than 20 minutes from commit to live — and we have an even more streamlined process for updating just the application configuration — a “config push” as we call it. The challenge with this comes from the fact that at any point during a day, people are waiting their turn to deploy code in our push queue . To do our schema change, we’ll need to wait through the push queue 4 times. Not hard, but not amenable to getting things done quickly. Once we’ve done the first config push to pull out half the DBs, we’re ready to apply the schema changes to the set of dormant servers. For this we’d use some shell-fu in a screen or tmux session to run commands on all the databases at once. When those changes were done, we’d get back in the push queue to put the updated DBs back in production, watch that it’s stable and then take the other side out. Then go back to the terminal, connect to all the other databases and run the changes on them. When that’s done, it’s back to the push queue to return the ORM to its full configuration. Along the way we’d have to deal with Nagios, run checksums on the new schemas on each host, and monitor DB status and site errors. And at the end we should end up with something we can call a log of the changes we made. We did versions of that process for a while. It worked but no one is perfect and we did have an outage stemming from errors made while doing schema changes. This incident helped crystalize the “ quantum of deployment concept as applied to schema changes” for us. We asked ourselves “What’s the smallest number of steps, with the smallest number of people and the smallest amount of ceremony required to get new schemas running on our servers?” With an answer to that, we knew we could, and had to, put a button on it. Thus was born Schemanator. From Continuous Deployment to Concurrent Deployment From the description of the schema change process above it’s clear that one of the pain points is having to wait in the push queue to deploy configuration changes. In order to be truly awesome, Schemanator would have to provide an avenue for the operator to bypass the push queue altogether. In practical terms, not having to wait in the queue shaves at least an hour off the whole process. This part of Schemanator is the first part we wanted to address since it delivered the most overall time savings in the shortest time. To help understand how we made this part work here’s a bit more about our ORM setup. On each request, the configuration file is read in. Part of that configuration is the set of DSNs for all available database servers. We store these in a hash keyed by the shard number and side like this: $server_config[\"database\"] = array(\n    'etsy_shard_001_A' => 'mysql:host=dbshard01.etsy.com;port=3306;\ndbname=etsy_shard;user=etsy',\n    'etsy_shard_001_B' => 'mysql:host=dbshard02.etsy.com;port=3306;\ndbname=etsy_shard;user=etsy',\n    'etsy_shard_002_A' => 'mysql:host=dbshard03.etsy.com;port=3306;\ndbname=etsy_shard;user=etsy',\n    'etsy_shard_002_B' => 'mysql:host=dbshard04.etsy.com;port=3306;\ndbname=etsy_shard;user=etsy',\n    ...\n); Before Schemanator, we would literally just comment out the lines with the DSNs we didn’t want the ORM to use. We didn’t want to automate commenting out lines of code in our config file, and even if we did, doing so would have still left us having to wait in the push queue. So we made a slight change to the ORM setup routine. When the ORM sets up, it now also checks if a special file — a “disabled connections” file exists. If it does, it’s read in as a list of DSNs to ignore. Those servers are skipped when the ORM sets up and the application ends up not using them. Since the config is read at the begining of the web request and PHP is shared nothing, once the disabled connections file is in place, all subsequent requests on that server will respect it. By carefully deploying just that one special disabled connections file into the live docroot we get the changes we need to the ORM…and we can do this while people are pushing code changes to the site. Specifically, we updated our general deploy to exclude the disabled connections file to eliminate possible race conditions on deploy and we set up Deployinator to allow us to deploy the disabled connections file on its own. But there was a problem. Like many php sites we use APC for opcode caching. Part of each deploy included gracefully restarting Apache to clear the opcode cache. Dropping the one file in is all well and good but we’d still have to clear it from APC. After considering some options, we chose to turn on apc.stat . This tells APC to stat each file before returning the cached opcodes. If the file is newer than the cached opcodes then re-read the file and update the cache. We run our docroot from a RAM disk so the extra stats aren’t a problem. With apc.stat on we could drop in our disabled connections file and the next request will start using it. No restarts required. We did need to increase the size of our APC cache to allow this. We were able to stop doing restarts on each deploy and since most deploys only change a small subset of our codebase, we saw an improvement in our cache-hit ratio. With this process in hand, and after much testing, we were able to allow the disabled connections file to be deployed concurrently with our regular deploys. We call this part of Schemanator “Side Splitter”. It provides a Web GUI for selecting which DB servers to disable and a button to deploy that configuration. Pushing the button writes and deploys the disabled connections file. There are also a number of sanity checks to make sure we don’t do things like pull out both sides of the same shard.  We use this not just as part of schema changes, but also when we need to pull DBs out for maintenance. UI For selecting which sides of which shards to disable. Ch-ch-changes Next we needed a process for applying the schema changes to the databases. A number of the choices we had to make here were strongly influenced by our environment. Etsy is in PHP and we use Gearman for asynchronous jobs so using anything but PHP/Gearman for Schemanator would have meant re-inventing many wheels. There are many things that could possibly go wrong while running schema changes. We tried to anticipate as many of these as we could: the operator’s internet connection could die mid-process, the application could keep connecting to disabled databases, gearman workers could die mid-job, the DDL statements might fail on a subset of servers, etc., etc. We knew that we couldn’t anticipate everything and that we certainly couldn’t write code to recover from any possible error. With that awareness – that something unexpected would eventually happen during schema changes – we designed Schemanator to allow the operator to pause or stop the process along the way. Each sub-task pauses and shows the operator an appropriate set of graphs and monitors to allow them to evaluate if it’s safe to proceed. This increases our liklihood of detecting trouble and gives us a way to bail out of the process, if that is the best course of action. Central to Schemanator is the concept of the “changeset” – a data structure where we store everything about the schema change: the SQL, who created it, test results, checksums, and more. The changeset acts as both a recipe for Schemanator to apply the changes and a record of the work done. The early parts of the Schemanator workflow center around defining and testing the changeset. To test a changeset, a gearman job loads up the current schema with no data and applies the changes there. If there are errors the changeset will be marked as failing and the errors reported back. We also generate the checksums we’ll look for later when we apply the changes in production. We recently added a test that inspects the post-change schema to make sure that we’re keeping to our SQL standards. For example, we make sure all tables are InnoDB with UTF-8 as the default character set, that we we don’t add any AUTO INCREMENT fields, which would be trouble in our Master-Master setup. We recently had an issue where some tables had a foreign key field as INT(11) but the related table had the field as BIGINT. This caused errors when trying to store the BIGINT in the INT(11) field. To catch this going forward, Schemanator now checks that our keys are all BIGINTs. Once the changeset passes all the tests, it can be run. When Schemanator runs a changeset, the first thing it does is to tell Nagios not alert for the servers we’re about to update. We use xb95’s nagios-api which allows Schemanator to set and cancel downtime. Schemanator’s “Preflight Check” screen shows the current status of the DB cluster and also checks Nagios to see if any of the DB servers have alerts or warnings. This gives an at-a-glance view to know if it’s OK to proceed. Schemanator: Preflight Checks Once things look OK, the operator can click the button to “Do it: Pull those sides out”. Schemanator will deploy the first of the 4 disabled connections configs. When that config is live, Schemanator drops you back on a page full of graphs where you can confirm that traffic has moved off of the pulled servers and is being adequately handled by the remaining servers. Once that is stable the operator clicks a button to apply the SQL to dormant servers. This is handled by a set of Gearman jobs – one job per database – that connect to the DBs and apply the updates. Schemanator monitors each job and polls the MySQL Process List on the DBs so the operator has good view of what’s happening on each server. Any errors or checksum mismatches bubble up in the UI so the operator can decide how to deal with them. Schemanator: Showing the progress of an alter running on a remote DB and the process list from that DB with our alter highlighted in yellow When all the workers are done, Schemanator prompts the user that it’s ready to move on. From here it’s a matter of repeating these steps with the right variations until the changes have been applied to all the databases. Schemanator handles setting and canceling Nagios downtime, checksumming any tables that were updated and logging everything. We’ve been using Schemanator to help with schema changes for a few months now and it’s achieved most of the goals we’d hope for: increased the speed and confidence we can do schema change, increased reliability that we don’t forget any of the many steps involved, and freed up people from having to devote often a full day to schema changes. We generally make schema changes only once a week, but with Schemanator, if we have to do them spur of the moment, that’s no longer out of the question. While Schemanator doesn’t do everything, we feel it’s the appropriate level of automation for a sensitive operation like schema changes. Posted by Jeff Kolber on January 11, 2013 Category: databases , operations Tags: automation , etsy , mysql operations , schema changes", "date": "2013-01-11,"},
{"website": "Etsy", "title": "mctop – a tool for analyzing memcache get traffic", "author": ["Marcus Barczak"], "link": "https://codeascraft.com/2012/12/13/mctop-a-tool-for-analyzing-memcache-get-traffic/", "abstract": "mctop – a tool for analyzing memcache get traffic Posted by Marcus Barczak on December 13, 2012 Here at Etsy we (ab)use our memcache infrastructure pretty heavily as a caching layer between our applications and our database tiers. We functionally partition our memcache instances into small pools and overall it works fabulously well. We have however suffered occasionally from what we call “hot keys”. What is a “Hot” key? A “hot key” is a single key hashed to an individual memcache instance with a very high get rate, often being called once for every page view. For the most part network bandwidth across all memcache instances within a pool is relatively balanced. These hot keys, however, contribute a significant additional amount of egress network traffic and have the potential to saturate the available network bandwidth of the interface. The graph above is an example of a recent hot key issue. The graph y-axis represents bytes per second inbound and outbound of memcached01’s network interface. As we hit peak traffic, memcached01’s network interface was completely saturated at approximately 960Mbps (it’s a 1Gbps NIC). This has a particularly nasty impact to get latency: As we began to push past 800Mbps outbound, 90th percentile get request latency jumped from 5ms to 35ms. Once the NIC was saturated latency spiked to over 200ms. Diagnosing the Issue This wasn’t the first time a hot key had been responsible for unsually high network bandwidth utilization so this was our first line of investigation. Comparatively memcached01’s bandwidth utilization was significantly higher than the other servers in the pool. Diagnosing which key was causing problems was a slow process, our troubleshooting process took the following steps: Take a brief 60 second packet capture of the egress network traffic from memcached01 Using the tshark (wireshark’s awesome command line cousin) extract the key and response size from the memcache VALUE responses in captured packet data. Post process the tshark output to aggregate counts, estimate requests per second and calculate the estimated bandwidth per key. Sort that list by bandwidth then further investigate that key. Once the potentially offending key is found we’d repeat this process from a couple of client machines to validate this as the offending key. Once the key was confirmed engineers would look at alternate approaches to handling the data contained in the key. In this particular case, we were able to disable some backend code that was utilizing that key with no user facing impact and relieve the network pressure. Overal this diagnostic process is quite manual and time intensive. 60 seconds of packet capture at 900Mbps generates close to 6GB of packet data for tshark to process, and if this process needs to be repeated on multiple machines the pain is also multiplied. Welcome mctop! Given this wasn’t a new issue for us I decided to have a crack at building a small tool to allow us to interactively inspect in-real time, the request rate and estimated bandwidth use by key. The end result is the tool “mctop” we’re open sourcing today. Inspired by “top”, mctop passively sniffs the network traffic passing in and out of a server’s network interface and tracks the responses to memcache get commands. The output is presented on the terminal and allows sorting by total calls, requests/sec and bandwidth. This gives us an instantaneous view of our memcache get traffic. mctop is now available as a gem on rubygems.org and the source is available over at github . Patches welcome, we hope you find it useful! Posted by Marcus Barczak on December 13, 2012 Category: infrastructure , operations Tags: memcache open-source engineering operations", "date": "2012-12-13,"},
{"website": "Etsy", "title": "Measuring Front-end Performance With Real Users", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2012/11/29/measuring-front-end-performance-with-real-users/", "abstract": "Measuring Front-end Performance With Real Users Posted by Lara Hogan on November 29, 2012 When we published our last performance update , we got a comment about the lack of RUM data for front-end performance from Steve Souders.  Steve followed up his comment with a blog post , stating that real users typically experience load times that are twice as slow as your synthetic measurements.  We wanted to test this theory, and share some of our full page load time data from real users as well. To gather our real user data we turned to two sources: LogNormal and the Google Analytics Site Speed Report .  Before we put up the data, there are a few caveats to make: For the day in question (11/14/12) we are providing data for the top three browsers that our customers use, all of which support the Navigation Timing API .  This gives us the most accurate RUM data we can get, but introduces a small bias.  This sample encompassed 43% of our customers on this day. This isn’t completely apples to apples, since Google Analytics (GA) uses average load time by default and WebPagetest/LogNormal use median load time.  The problem with averages has been well documented , so it’s a shame that GA still gives us averages only.  To get rough median numbers from GA we used the technique described in this post .  This results in the range that you will see on the chart below. The WebPagetest numbers are for logged out users, and we don’t have signed in vs signed out data from LogNormal or Google Analytics on that day, so those numbers cover all users (both logged-in and logged-out).  We expect numbers for logged-out users to be slightly faster, since there is less logic to do on the backend and there are some missing UI elements on the front-end in some cases. The WebPagetest 50/50 numbers are calculated by taking the average of the empty cache and full cache WebPagetest measurements (more on that below). With those points out of the way, here is the data: So what’s going on here?  Our RUM data is faster than our synthetic data in all cases, and in all cases except for one (Shop pages in Chrome 23) our two RUM sources agree. Let’s see if we can explain the difference in our findings from Steve’s.  According to Google Analytics, 72% of our visitors are repeat visitors, which probably means that their cache is at least partly full.  Since cache is king when it comes to performance, this gives real users a huge advantage performance wise over a synthetic test with an empty cache.  In addition, around 60% of our visits are from signed-in users, who likely visit a lot of the same URLs (their shop page, profile page, their listings) which means that their cache hit rate will be even higher.  We tried to account for this with the WebPagetest 50/50 numbers, but it’s possible that the hit rate of our customers is higher than that (this is on our list of things to test).  Also, the WebPagetest requests were using a DSL connection (1.5 Mbps/384 Kbps, with 50ms round trip latency), and our users tend to have significantly more bandwidth than that: It’s encouraging to see that LogNormal and Google Analytics agree so closely, although GA provides a wide range of possible medians, so we can’t be 100% confident about assessment.  The one anomaly there is Shop pages in Chrome 23, and we don’t have a great explanation for this discrepancy.  Sample size is fairly similar (GA has 38K samples to LogNormal’s 60K), and the numbers for logged-in vs. logged-out numbers are the same in LogNormal, so it’s not related to that.  The histogram in LogNormal looks pretty clean, and the margin of error is only 56ms.  GA and LogNormal do use separate sampling mechanisms, so there could be a bias in one of them that causes this difference.  Luckily it isn’t large enough to worry too much about. It’s worth pointing out that when we start looking at higher percentiles in our real user monitoring things start to degrade pretty quickly.  The 95th percentile load time as reported in LogNormal for Chrome 23 is 8.9 seconds – not exactly fast (in Google Analytics the 95th percentile falls into the 7-9 seconds bucket).  Once you get out this far you are essentially monitoring the performance of last mile internet connectivity, which is typically well beyond your control (unless you can build fiber to your customers’ doorsteps ). Overall we are showing different results than what Steve predicted, but we think this can be largely explained by our huge percentage of repeat visitors, and by the fact that we are using a DSL connection for our synthetic tests.  The takeaway message here is that having more data is always a good thing, and it’s important to look at both synthetic and RUM data when monitoring performance.  We will be sure to post both sets of data in our next update. Posted by Lara Hogan on November 29, 2012 Category: performance", "date": "2012-11-29,"},
{"website": "Etsy", "title": "October 2012 Site Performance Report", "author": ["Lara Hogan"], "link": "https://codeascraft.com/2012/11/09/october-2012-site-performance-report/", "abstract": "October 2012 Site Performance Report Posted by Lara Hogan on November 9, 2012 It’s been about four months since our last performance report , and we wanted to provide an update on where things stand as we go into the holiday season and our busiest time of the year.  Overall the news is very good! Server Side Performance Here are the median and 95th percentile load times for core pages, Wednesday 10/24/12: As you can see, load times declined significantly across all pages.  A portion of this improvement is due to ongoing efforts we are making in Engineering to improve performance in our application code.  The majority of this dip, however, resulted from upgrading all of our webservers to new machines using Sandy Bridge processors.  With Sandy Bridge we saw not only a significant drop in load time across the board, but also a dramatic increase in the amount of traffic that a given server can handle before performance degrades.  You can clearly see when the cutover happened in the graph below: This improvement is a great example of how operational changes can have a dramatic impact on performance.  We tend to focus heavily on making software changes to reduce load time, but it is important to remember that sometimes vertically scaling your infrastructure and buying faster hardware is the quickest and most effective way to speed up your site.  It’s a good reminder that when working on performance projects you should be willing to make changes in any layer of the stack. Front-end Performance Since our last update we have a more scientific way of measuring front-end performance, using a hosted version of WebPagetest .  This enables us to run many synthetic tests a day, and slice the data however we want.  Here are the latest numbers, gathered with IE8 from Virginia over a DSL connection as a signed in Etsy user: These are median numbers across all of the runs on 10/24/12, and we run tests every 30 minutes.  Most of the pages are slower as compared to the last update, and we believe that this is due to using our hosted version of WebPagetest and aggregating many tests instead of looking at single tests on the public instance.  By design, our new method of measurement should be more stable over the long term, so our next update should give a more realistic view of trends over time. You might be surprised that we are using synthetic tests for this front-end report instead of Real User Monitoring (RUM) data.  RUM is a big part of performance monitoring at Etsy, but when we are looking at trends in front-end performance over time, synthetic testing allows us to eliminate much of the network variability that is inherent in real user data.  This helps us tie performance regressions to specific code changes, and get a more stable view of performance overall.  We believe that this approach highlights elements of page load time that developers can impact, instead of things like CDN performance and last mile connectivity which are beyond our control. New Baseline Performance Measurements Another new thing we created is an extremely basic page that allows us to track the lower limit on load time for Etsy.com.  This page just includes our standard header and footer, with no additional code or assets.  We generate some artificial load on this page and monitor its performance.  This page represents the overhead of our application framework, which includes things like our application configuration ( config flag system ), translation architecture, security filtering and input sanitization, ORM, and our templating layer.  Having visibility into these numbers is important, since improving them impacts every page on the site.  Here is the current data on that page: Over the next few months we hope to bring these numbers down while at the same time bringing the performance of the rest of the site closer to our baseline. Posted by Lara Hogan on November 9, 2012 Category: performance", "date": "2012-11-9,"},
{"website": "Etsy", "title": "Scaling User Security", "author": ["Zane Lackey"], "link": "https://codeascraft.com/2012/10/09/scaling-user-security/", "abstract": "Scaling User Security Posted by Zane Lackey on October 9, 2012 Summer is ending New security features Sweeping in like Fall The Etsy Security Team is extremely happy to announce the simultaneous release of three important security features: Two factor authentication, full site SSL support, and viewable login history data. We believe that these protections are industry best practice, and we’re excited to offer them proactively to our members on an opt-in basis as a further commitment to account safety. A high level overview of the features is available here , while on Code as Craft we wanted to talk a bit more about the engineering that went into the SSL and two factor authentication features. Rolling out Full Site SSL When we initially discussed making the site fully accessible over SSL, we thought it might be a simple change given our architecture at the time. During this time period we relied on our load balancers to both terminate SSL and maintain the logic as to which pages were forced to HTTPS, which were forced to HTTP, and which could be either. To test out our “simple change” hypothesis we set up a test where we attempted to make the site fully SSL by disabling our load balancer rules that forced some pages down to HTTP. After this triggered a thrilling explosion in the error logs, we realized things weren’t going to be quite that easy. The first step was to make our codebase HTTPS friendly. This meant cleaning up a significant number of hard coded “http://” links, and making all our URL generating functions HTTPS aware. In some cases this meant taking advantage of scheme relative URLs . We also needed to verify that all of our image storage locations, and our various CDNs could all play nicely with SSL. Next up was moving the logic for enforcing whether a URL could be HTTP, HTTPS, or both from the load balancer to the application itself. With the logic on the load balancer, adding or changing a rule went like this: Engineer makes a ticket for ops Ops logs into the load balancer admin interface Ops needs to update the rule in three places (our development, preprod, and prod environments) Ops lets engineer know rule was updated, hopes all is well In case of rollback, ops has to go back into admin interface, find the rule (the admin interface did not sort them in any meaningful way), change the rule back, and hope for the best In this flow, changes have to be tracked through a change ticket system not source control and there is no way for other engineers to see what has been updated. Why is application logic in our load balancers anyway? Wat? To address these issues, we moved all HTTPS vs HTTP logic into the web server via a combination of .htaccess rules and hooks in our controller code. This new approach provided us with far greater granularity on how to apply rules to specific URLs. Now we can specify how URLs are handled for groups of users (sellers, admins, etc) or even individual users instead of using load balancer rules in an all-or-nothing global fashion. Finally, the move meant all of this code now lives in git which enables transparency across the organization. HSTS HSTS is a new header the instructs browsers to only connect to a specific domain over HTTPS in order to provide a defense against certain man-in-the-middle attacks. As part of our rollout we are making use of this header when a user opts in to full site SSL. Initially we are setting a low timeout value for HSTS during rollout to ensure things operate smoothly, and we’ll be increasing this value via a config push over time as we’re confident there will be no issues. Why not use full site SSL for all members and visitors? First and foremost, rolling out SSL as the default for all site traffic is something we’re actively working on and feel is the best practice to be striving towards. As with any large scale change in site-wide traffic, capacity and performance are significant concerns. Our goal with making this functionality available on an opt-in basis at first is to provide it to those members who use riskier shared network mediums such as public WiFi. Going forward, we’re analyzing metrics around CDN performance (especially for our international members), page performance times of SSL vs non-SSL, and overall load balancer SSL capacity. When we’re confident in the performance and capacity figures, we’re excited to continue moving towards defaulting to full site SSL for all members and visitors. Two factor authentication Our main focus during the course of our two factor authentication project (aside from security) was how to develop and apply metrics to create the best user experience possible over the long term. Specifically, the questions we wanted to be able to answer about the voice call/SMS delivery providers we selected were: “Does provider A deliver codes faster than provider B?” “Does provider A deliver codes more reliably than provider B?” “Can we easily swap out provider A for provider C? or D? or F?” Provider abstraction From the beginning we decided that we did not want to be tied to a single provider, so abstraction was critical. We went about achieving this in two ways: Only relying on the provider for transmission of the code to a member. All code generation and verification is performed in our application, and the providers are simply used as “dumb” delivery mechanisms. Abstracting our code to keep it as generic and provider-agnostic as possible. This makes it easy to swap providers around and plug in new ones whenever we wish. Metrics and performance testing There are two main provider metrics we analyze when it comes to signins with 2FA: Time from code generation to signin (aka: How long did the provider take to deliver the code?) Number of times a code is requested to be resent (aka: Was the provider reliable in delivering the code?) These metrics allow us to analyze a providers’ quality over time, and allow us to make informed choices such as which provider we should use for members in specific geographical locations. In order to collect this data from multiple providers, we make heavy use of A/B testing . This approach lets us easily balance the usage of providers A and B one week, and B and C the next. Finally, from a SPOF and resiliency point of view this architecture also makes it painless to fail over to another provider if one goes down. In closing, we hope you’ll give these new features a shot by visiting your Security Settings page, and we’re excited to continue building proactive security mechanisms for our members. This post was co-written by Kyle Barry and Zane Lackey on behalf of the Etsy Security Team. Posted by Zane Lackey on October 9, 2012 Category: engineering , security", "date": "2012-10-9,"},
{"website": "Etsy", "title": "Announcing the Etsy Security Bug Bounty Program", "author": ["Zane Lackey"], "link": "https://codeascraft.com/2012/09/11/announcing-the-etsy-security-bug-bounty-program/", "abstract": "Announcing the Etsy Security Bug Bounty Program Posted by Zane Lackey on September 11, 2012 On April 17 of this year we launched our responsible disclosure page ( http://www.etsy.com/help/article/2463 ). At the time, our goal was to provide security researchers with a direct point of contact if they had identified a vulnerability in our site, API, or mobile application. Thus far we’ve received excellent reports from researchers, as well as some exciting offers from Nigerian princes. Today, we’d like to take this a step further and announce the launch our security bug bounty program. Our goal is to reward security researchers who follow responsible disclosure principles and proactively reach out to us if they’ve identified a vulnerability which would impact the safety of our marketplace or members. We believe that this is industry best practice. Our bounty program will pay a minimum of $500 for qualifying vulnerabilities, subject to a few conditions and with qualification determined by the Etsy Security Team. This bounty will be increased at our discretion for distinctly creative or severe security bugs. To give it the proper Etsy feel, we’ll also be throwing in some handmade thank-you’s such as an Etsy Security Team T-shirt. Additionally, we’ll be retroactively applying the bounty to vulnerabilities that have been reported to us since the launch of our responsible disclosure page earlier this year. You can find the full information on the new program here: http://www.etsy.com/help/article/2463 Posted by Zane Lackey on September 11, 2012 Category: security", "date": "2012-09-11,"},
{"website": "Etsy", "title": "The Engineer Exchange Program", "author": ["Marc Hedlund"], "link": "https://codeascraft.com/2012/09/10/the-engineer-exchange-program/", "abstract": "The Engineer Exchange Program Posted by Marc Hedlund on September 10, 2012 Co-authors: Marc Hedlund, SVP, Product Development at Etsy Raffi Krikorian, Director, Platform Services at Twitter Your first week at any new job is (at least if you chose a good job!) filled with tons to learn, new ways of doing things, and working models that you might have considered unattainable in the job you just left. How great would it be to have that experience more than once per new job you take? Twitter and Etsy are working together on a new project to help our engineers learn from each others’ practices, with the idea of making both of our engineering teams better as a result.  We hope to learn what makes each other tick, how we celebrate our successes and learn from our failures, and how we can each be better in the end. This week, one of Etsy’s Staff Engineers is traveling to San Francisco to spend a week at Twitter, observing and helping out, learning what Twitter does particularly well, and seeing differences that may reinforce or refute beliefs we’ve held as core. Likewise, a Twitter Platform Engineer is traveling to Brooklyn for the week, and watching what Etsy does well and poorly, all while helping out (and, of course, deploying on her first day ). New engineers at Etsy go through a several-week bootcamp, working with different teams to learn the codebase, meet people across the group, and take on small tasks. Likewise, new engineers at Twitter go through a “new hire orientation” process where they learn about the Twitter architecture, see first hand Twitter’s raw scale, and play with the back-end technologies.  These engineers will go through these same steps for the week (albiet, a bit accelerated), contributing code and pushing to production, not just observing from a distance. It takes a level of trust to let an unknown engineer into the fold, let them sit in on meetings and make changes to code. Of course some people would be uncomfortable with letting this happen; companies we’ve both worked for would have fits before allowing it. But we believe the value of cross-pollination of ideas and practices is far too high to be blocked by these concerns. While this is an experiment, we’re hopeful it makes both teams stronger, and we’ll be looking for other exchanges to do soon. Posted by Marc Hedlund on September 10, 2012 Category: engineering , people , philosophy", "date": "2012-09-10,"},
{"website": "Etsy", "title": "What Hardware Powers Etsy.com?", "author": ["Laurie Denness"], "link": "https://codeascraft.com/2012/08/31/what-hardware-powers-etsy-com/", "abstract": "What Hardware Powers Etsy.com? Posted by Laurie Denness on August 31, 2012 Traditionally, discussing hardware configurations when running a large website is something done inside private circles; and normally to discuss how vendor X did something very poorly, and vendor Y’s support sucks. With the advent of the “cloud”, this has changed slightly. Suddenly people are talking about how big their instances are, and how many of them. And I think this is a great practice to get in to with physical servers in datacenters too. After all, none of this is intended to be some sort of competition; it’s about helping out people in similar situations as us, and broadcasting solutions that others may not know about… pretty much like everything else we post on this blog. The great folk at 37signals started this trend recently by posting about their hardware configurations after attending Velocity conference … one of the aforementioned places where hardware gossiping will have taken place. So, in the interest of continuing this trend here’s the classes of machine we use to power over $69.5 million of sales for our sellers in July Database Class As you may already know , we have quite a few pairs of MySQL machines to store our data, and as such we’re relying on them heavily for performance and (a certain amount of) reliability. For any job that requires an all round performant box, with good storage, good processing power, and a good level of redundancy we utilise HP DL380 servers. These clock in at 2U of rack space, 2x 8 core Intel E5630 CPUs (@ 2.53ghz), 96GB of RAM (for that all important MySQL buffer cache) and 16x 15,000 RPM 146GB hard disks. This gives us the right balance of disk space to store user data, and spindles/RAM to retrieve it quickly enough. The machines have 4x 1gbit ethernet ports, but we only use one. Why not SSDs? We’re just starting to test our first round of database class machines with SSDs. Traditionally we’ve had other issues to solve first, such as getting the right balance of amount of user data (e.g. the amount of disk space used on a machine) vs the CPU and memory. However, as you’ll see in our other configs, we have plenty of SSDs throughout the infrastructure, so we certainly are going to give them a good testing for databases too. A picture of our various types of hardware, with the HP to the left/middle and web/utility boxes on the right Web/Gearman Worker/Memcache/Utility/Job Class This is a pretty wide catch all, but in general we try and favour as few machine classes as possible, so a lot of our tasks from handling web traffic (Apache/PHP) to any box that performs a task where there are many of them/redundancy is solved at the app level we generally use one type of machine. This way hardware re-use is promoted and machines can change roles quickly and easily. Having said that, there are some slightly different configurations in this category for components that are easy to change, e.g. amount of memory and disks. We’re pretty much in love with this 2U Supermicro chassis which allows for 4x nodes that share two power supplies and 12 3.5″ disks on the front of the chassis Supermicro Chassis with 4 easily serviceable nodes A general configuration for these would be 2x 8 core Intel E5620 CPUs (@ 2.40ghz), 12GB-96GB of RAM, and either a 600GB 7200pm hard disk or an Intel 160GB SSD. Note the lack of RAID on these configurations; We’re pretty heavily reliant on Cobbler and Chef , which means rebuilding a system from scratch takes just 10 minutes. In our view, why power two drives when our datacenter staff can replace the drive and rebuild the machine and have it back in production in under 20 minutes? Obviously this only works where it is appropriate; clusters of machines where the data on each individual machine is not important. Web servers, for example, have no important data since logs are sent constantly to our centralised logging host , and the web code is easily deployed back on to the machine. We have Nagios checks to let us know when the filesystem becomes un-writeable (and SMART checks also), so we know when a machine needs a new disk. Each machine has 2x 1gbit ethernet ports, in this case we’re only using one. Hadoop In the last 12 months we’ve been working on building up our Hadoop cluster, and after evaluating a few hardware configurations ended up with a very similar chassis design to the one used above. However, we’re using a chassis with 24x 2.5″ disk slots on the front , instead of the 12x 3.5″ design used above. Hadoop nodes… and a lot of disk lights Each node (with 4 in a 2U chassis) has 2x 12 core Intel E5646 CPUs (@ 2.40ghz), 96GB of RAM, and 6x 1Tb 2.5″ 7200rpm disks. That’s 96 cores, 384GB of RAM and 24TB per 2U of rack space. Our Hadoop jobs are very CPU heavy, and storage and disk throughput is less of an issue hence the small amount of disk space per node. If we had more I/O and storage requirements, we had also considered 2U Supermicro servers with 12x 3.5″ disks per node instead. As with the above chassis, each node as 2x 1gbit ethernet ports, but we’re only utilising one at the minute. This graph illustrates the power usage on one set of machines showing the difference between Hadoop jobs running and not Search/Solr Just a month ago, this would’ve been grouped into the general utility boxes above, but we’ve got something new and exciting for our search stack. Using the same chassis as in our general example, but this time using the awesome new Sandy Bridge line of Intel CPUs . We’ve got 2x 16 core Intel E5-2690 CPUs in these nodes, clocked at 2.90ghz, which gives us machines that can handle over 4 times the workload of the generic nodes above, whilst using the same density configuration and not that much more power. That’s 128x 2.9ghz CPU cores per 2U (granted, that includes HyperThreading). This works so well because search is really CPU bound; we’ve been using SSDs to get around I/O issues in these machines for a few years now. The nodes have 96GB of RAM and a single 800GB SSD for the indexes. This follows the same pattern of not bothering with RAID; The SSD is perfectly fast enough on it’s own, and we have BitTorrent index distribution which means getting the indexes to the machine is super fast. Less machines = less to manage, less power, and less space. Output of the “top” command with 32 cores on Sandy Bridge architecture Backups Supermicro wins this game too. We’re using the catchily named 6047R-E1R36N . The 36 in this model number is the important part… t his is a 4u chassis, with 36x 3.5″ disks . We load up these chassis with 2TB 7200rpm drives, which when coupled with an LSI RAID controller with 1gb of battery backed write back cache gives a blistering 1.2 gigabytes/second sequential write throughput and a total of 60TB of usable disk space across two RAID6 volumes. 36 disk Supermicro chassis. Note the disks crammed into the back of the chassis as well as the front! Why two RAID6 volumes? Well, it means a little more waste (4 drives for parity instead of 2) but as a result of that you do get a bit more resiliency against losing a number of drives, and rebuild times are halved if you just lose a single drive. Obviously RAID monitoring is pretty important, and we have checks for either SMART (single disk machines) or the various RAID utilities on all our other machines in Nagios. In this case we’re taking advantage of the 2x 1gbit ethernet connections, bonded together to the switch to give us redundancy and the extra bandwidth we need. In the future we may even run fiber to these machines, to get the full potential out of the disks, but right now we don’t get above a gigabit/second for all our backups. Special cases Of course there are always exception to the rules. The only other hardware profile we have is HP DL360 servers (1u, 4x 2.5″ 15,000rpm 146GB SAS disks) which is for roles that don’t need much horsepower, but we deem important enough to have RAID. For example, DNS servers, LDAP servers, and our Hadoop Namenodes are all machines that don’t require much disk space, but need RAID for extra data safety than our regular single disk configurations. Networking I didn’t go into too much detail on the networking side of things in this post. Consider this part 1, and watch this space for our networking gurus to take you through our packet shuffling infrastructure at a later date. Continue the trend If you’re anything like us, we love a good spot of hardware porn. What cool stuff do you have? This post was Laurie Denness ( @lozzd ), who would love it if you came and helped us make Etsy.com even better using this hardware. Why not come and help us? Posted by Laurie Denness on August 31, 2012 Category: engineering , infrastructure , operations", "date": "2012-08-31,"},
{"website": "Etsy", "title": "Posting PostMortems for a (generally) Non-Technical Audience", "author": ["John Allspaw"], "link": "https://codeascraft.com/2012/08/23/posting-postmortems-for-a-generally-non-technical-audience/", "abstract": "Posting PostMortems for a (generally) Non-Technical Audience Posted by John Allspaw on August 23, 2012 The other day I posted to the Etsy News blog about some recent outages we’ve had. We haven’t given this much information about site outages in the past, and this particular post was written for the non-technical-minded members of the community. The process of writing it was a challenge for me. It underscored the lesson that you can’t fully appreciate how complicated some of the failure scenarios we see in our field of web operations until you actually want to explain it to someone who isn’t familiar with software and infrastructure fundamentals. 🙂 In the end, I got a lot of positive feedback on it from a number of Etsy members. This includes sellers who spend a large deal of their time making the things they sell , which is our goal. Generally, we want to make the technical bits of Etsy to disappear for them. But when things go wrong, I think it’s worth giving them some details about what happened and what we’re doing to help avoid similar issues in the future. If you’re interested, here is the post. Posted by John Allspaw on August 23, 2012 Category: engineering , infrastructure , operations , outages", "date": "2012-08-23,"},
{"website": "Etsy", "title": "No more locks with Xtrabackup", "author": ["Arie Kachler"], "link": "https://codeascraft.com/2012/08/07/no-more-locks-with-xtrabackup/", "abstract": "No more locks with Xtrabackup Posted by Arie Kachler on August 7, 2012 Percona’s Xtrabackup is a great product. It performs binary backups of heavily loaded Mysql servers amazingly fast. We’ve used it here at Etsy for years and works very well for us. Mysqldump is another way of doing backups, but for servers with hundreds of gigabytes of data, it’s too slow to be useful, especially for restoring a backup. It can literally take days to restore a couple of hundred of gigabytes generated with Mysqldump. Usually when you need to restore from a backup, you’re in some sort of emergency, and waiting days is not an option. But Xtrabackup has a significant shortcoming: it needs a global lock at the end of its procedure. Not good for a server doing hundreds or thousands of queries per second. Percona touts Xtrabackup as “non-blocking”, which is for the most part true, but not entirely. Restoring a Mysql server, most of the time, involves installing an OS, Mysql, and any additional packages you want. Then restoring the latest Xtrabackup data and finally give the new server replication coordinates where it left off before the crash so it can get to the point where its data is consistent with its master. Here’s where the lock comes in: to get a reliable reading of replication coordinates, Xtrabackup issues a “FLUSH TABLES WITH READ LOCK” in the final stage of its process. When the lock is granted, Xtrabackup reads the “MASTER STATUS” and releases the lock. FTWRL is very disruptive to a busy server. It tells Mysql to start the process of read/write locking all tables, which in turn causes all new connections to wait for the lock to be released. Mysql then waits for all outstanding queries to finish, and then grants the lock. If there’s a long running query when FTWRL is requested, you will undoubtedly get a query pile-up which can quickly overwhelm the maximum number of connections your server is configured to accept and your application will stall. Percona’s documentation states that there is a –no-lock option for Xtrabackup. It also states “Use this option to disable table lock with FLUSH TABLES WITH READ LOCK. Use it only if ALL your tables are InnoDB and you DO NOT CARE about the binary log position of the backup”. We don’t want any locks, but we do want the “binary log position of the backup”, aka replication coordinates. It turns out that replication coordinates are hidden in the backup files when you run it with the –no-lock option. You just have to know how to get to them. Xtrabackup’s backup procedure involves copying Mysql’s data files to another location, knowing the exact point-in-time when the copying started, and creating an extra file, named xtrabackup_logfile, which contains all writes that occurred during the copying time. Restoring a backup with Xtrabackup requires a “prepare” phase, which is basically applying all writes from the xtrabackup_log file onto data files. When you do the prepare phase, a new file named xtrabackup_binlog_pos_innodb will appear in the restore directory. This file contains the replication coordinates that we need to reestablish replication. With or without the –no-lock option, xtrabackup_binlog_pos_innodb is created in your restore directory! Even a 1-second stall can be disruptive for a busy server, but our locks used to last around 30 seconds. That was before we adopted the –no-lock option combined with getting replication coordinates from the xtrabackup_binlog_pos_innodb for restores. An important thing to note is that this works if you only use Innodb tables. You shouldn’t be using MyISAM tables anyway. Use them only for the unavoidable: the `mysql` db which contains grants and other internal metadata. Posted by Arie Kachler on August 7, 2012 Category: databases , operations Tags: mysql operations Related Posts Posted by Jeff Kolber on 11 Jan, 2013 Schemanator: Love Child of Deployinator and Schema Changes", "date": "2012-08-7,"},
{"website": "Etsy", "title": "Performance tuning syslog-ng", "author": ["Avleen Vig"], "link": "https://codeascraft.com/2012/08/13/performance-tuning-syslog-ng/", "abstract": "Performance tuning syslog-ng Posted by Avleen Vig on August 13, 2012 You may have noticed a theme in many of our blog posts. While we do push the technology envelope in many ways, we also like to stick to tried , true and data driven methods to help us keep us sane. One of the critical components of our infrastructure is centralised logging. Our systems generate a vast amount of logging, and we like to keep it all. Everything. The whole bag. When faced with similar situations, other companies have embraced a lot of new technology: Flume Scribe Logstash At Etsy, we’ve stayed old-school – practically prehistoric by technology standards. Our logging infrastructure still uses syslog as the preferred transport because it Just Works™. …Mostly. Syslog-ng is a powerful tool, and has worked well, as long as we’ve paid a little attention to performance tuning it. This a collection of our favourite syslog-ng tuning tips. Our central syslog server, an 8-core server with 12Gb RAM, currently handles around 60,000 events per second at peak at ~25% CPU load. Rule ordering If, like us, you have a central syslog server with all of your hosts sending their vital information (whether it be system logs, Apache logs, Squid logs, or anything else) you will probably have a lot of filters set up to match certain hostnames, and certain log lines so you can get the logs into their correct places. For example, you may be trying to find all Apache access log lines across all of your webservers, so you end up with all the logs in a single file destination (something like /log/web/access.log perhaps) This feature is pretty widely used, and extremely powerful in the sorting of your logs but it’s also very power consuming in terms of CPU time. Regex matching especially has to check every log line event that comes in, and when you’re pushing tens of thousands of them, it can begin to hurt. But, in the case of our Apache access log example, you would use the “flags(final)” attribute in order to tell syslog-ng to stop processing that line, so it doesn’t even have to check the other regexes. That is all well and good, but have you considered internally what order syslog-ng is checking those match statements? For example, we use Chef pretty extensively at Etsy to automate a lot of things; syslog-ng is one of them. Each machine role has it’s own syslog-ng definitions, and our Chef recipes automatically generate both the client and server config for use with that role. To do this, we template out the configuration, and drop the files into /etc/syslog-ng.d/, which is included from the main syslog-ng.conf. Now, if your biggest traffic (log wise) happens to be for your webservers, and your config file ends up being called “web.conf”, syslog-ng will quite happily parse all your configs and when it compiles it’s rules, the config that checks to see if the line is from Apache will end up at the end of the list. You are potentially running tens if not hundreds of other match statements and regexes for no reason what so ever for the bulk of your traffic. Luckily the fix is extremely simple: If using one config file, keep the most used rules at the top. If you use syslog-ng.d, keep your most used rules in a file that begins with a “0-” to force it to the top of the list. This tiny change alone halved the CPU we were using on our syslog-ng server. tl;dr: Make sure syslog-ng is parsing the most frequently logged lines first, not last. Log windows and buffer sizing Math time! Syslog-ng has a great feature called “flow control” – when your servers and applications send more traffic than syslog-ng can handle, it will buffer the lines in memory until it can process them. The alternative would be to drop the log lines, resulting in data loss. Four variables associated with flow control that we will look at, are: log_fifo_size – The size of the output buffer. Each destination has its own one of these. If you set it globally, one buffer is created for each destination, of the size you specify. log_iw_size – The initial size of the input control window. Syslog-ng uses this as a control to see if the destination buffer has enough space before accepting more data. Applies once to each source (not per-connection!) log_fetch_limit – The number of messages to pull at a time from each source. Applies to every connection of the source individually. max_connections – The maximum number of TCP connections to accept. You do use TCP, don’t you?! Each of your servers shouldn’t open more than one TCP connection to your central syslog server, so make sure you set max_connections to more than that number. After that, it’s time to break out your calculator and commit these equations to memory: log_iw_size = max_connections * log_fetch_limit\nlog_fifo_size = log_iw_size * (10~20) There is some variance on how you calculate log_fifo_size which you will have to experiment with. The principle is this: Syslog-ng will fetch at most log_fetch_limit * max_connections messages each time it polls the sources. Your log_fifo_size should be able to hold many polls before it fills up. When your destination (file on disk? another syslog server?) is not able to accept messages quickly enough, they will accumulate in the log_fifo buffer, so make this BIG. log_fifo_size = (log_iw_size = max_connections * log_fetch_limit) * 20 Experiment with disabling power saving features At Etsy, we take pride in trying to do our bit for the planet. It’s what our business is built on, trying to encourage good behaviours around recycling, and re-using precious resources. It’s why we announced earlier this year that we are now a member of the B-Corp movement so we can measure our success in giving back to the world in various ways. One of the criteria for this involves how much energy we use as an organisation; a tough subject when you have a datacenter which are well known for using huge amounts of power. Because of this, we pride ourselves in working with vendors that also care about their power footprint, and being able to squeeze the most savings without effecting performance. However, this isn’t to say that, in particular, power saving modes provided with servers are perfect. We have hundreds of servers that can scale back their CPUs and other devices, saving power with absolutely 0 measured effect to performance or any other metric (and, believe us, we tested). There are two recorded cases where this hasn’t worked out well for us. Briefly, the principle behind power saving in servers is to “scale back” (that is, to decrease the speed and thus power usage) of CPU performance when the server doesn’t need it, and with many modern machines having multiple CPUs they can often be the biggest power draw, so the savings involved here are huge. Power usage on a typical server. This server would use 472 watts of power if the CPUs were constantly at full speed, the maximum they hit in the last 24 hours was 280 watts. What if your server demands a lot of CPU power? Well that’s no problem, the CPU can scale back up to full speed instantly, with basically no effect to the response time. But what if your server does a lot of context switching, or has very bursty CPU usage? Two prime examples of this are Nagios, and syslog-ng. Nagios, for example, has to spin up processes to execute checks in a very spiky manner, and to add to this there are sometimes hundreds of them at once, so the cost of switching between doing tasks in all those processes (even if the time of each individual one on it’s own is tiny) is huge (this is known as context switching). A similar thing happens with syslog-ng, wher the incoming log events are very bursty, so the CPU is actually spending more time scaling back and fourth than doing our processing. In these two instances, we switched from power saving mode to static full power mode, and the amount of CPU consumed halved . More importantly, that CPU shows up as system time; time wasted context switching and waiting for other events is suddenly reduced dramatically, and all CPU cores can operate on whatever events are needed as soon as possible. There are some great tools that allow you to watch your CPU scaling in action, for example i7z which is a great command line tool (and UI, if you fancy) that is easy to get running and gives you a great view into the inner workings of Intel CPUs. The important point here is that we would’ve actually purchased a more power hungry machine to scale with our log traffic if we hadn’t have found this, somewhat defeating the purpose of the power saving feature in the first place. tl;dr: Experiment with power settings on your server, unlock their full potential, if it makes no difference then put it back. Summary In total these changes took about 5 days of research and testing, and 1/2 day of effort to implement. We were regularly hitting the limits of what this servers was capable of processing before making these changes and had considered buying much larger servers and moving to other methods of log aggregation. A small dose of profiling and planning has reduced the load on the server to 1/5th, reduced our power consumption and improved the efficiency of the whole system: This graph illustrates the decrease in CPU usage when we performed two of the steps above. 70% CPU consumed to 15% with two very simple steps. This post was co-written by Avleen Vig ( @avleen ) and Laurie Denness ( @lozzd ) battling to solve hard (and sometimes easy) problems to make operations better for everyone. Why not come and help us? Posted by Avleen Vig on August 13, 2012 Category: infrastructure , operations", "date": "2012-08-13,"},
{"website": "Etsy", "title": "Static Analysis for PHP", "author": ["Nick Galbreath"], "link": "https://codeascraft.com/2012/08/10/static-analysis-for-php/", "abstract": "Static Analysis for PHP Posted by Nick Galbreath on August 10, 2012 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “allowlist/blocklist” in our Code as Craft entries. At Etsy we have three tiers of static analysis on our PHP code that run on every commit or runs periodically every hour. They form an important part of our continuous deployment pipeline along with one-button deploys, fast unit and functional tests, copious amounts of graphing, and a fantastic development environment to make sure code flows safely and securely to production. Sanity and Style Checking These checks eliminate basic problems and careless errors. A obvious rule is that syntax errors never make it make into the source repository, let alone production. So “ php -l ” is run as pre-commit check on each changed file. We don’t use PHP’s native templating where code and HTML is mixed in one file. Our PHP files are purely code, and output is rendered via another templating system. To make sure this works correctly, we check that there is no text before the initial <?php tag. Otherwise that text is sent to the client and prevents us from setting HTTP headers. Likewise we make sure that PHP tags are balanced and there is no trailing text as well. There are also a number of basic coding style checks. Nothing particularly exotic but they help make the code readable and consistent across an ever growing engineering department. Most of these are implemented using CodeSniffer . Formal Checks using HpHp Facebook’s HipHop for PHP is a full reimplementation of the PHP/Apache stack including a compiler, a new PHP runtime, and a web server. To make your application run under it will require some serious surgery since it’s missing many modules you might depend on. However, it has a fantastic static analyzer which can be used independently. It does global analysis of your entire code base for: Too many or too few arguments in a function/method call Undeclared global or local variables Use of return value of a function that actually returns nothing Functions that have a required argument after an optional one Unknown functions, methods, or base classes Constants declared twice and a few others. It’s doesn’t always track exactly the latest PHP versions, so you’ll have to allowlist some of the errors, but overall it’s been wildly successful with almost no false positives. Why not do all this in using PHP’s built-in tokenizer token_get_all or CodeSniffer ? HpHp can analyze 10,000 files in a few seconds, while the built-in function is a few orders of magnitude slower. Since it’s so fast, we can put the static analysis as a pre-commit hook that prevents bugs from even being checked in. Which it does almost every day. Security Checks We run ClamAV and antivirus check our source repository. Has it found anything yet? No (phew). But at 1GB/sec scan rate and the low low cost of free , there is no reason not do it. We aren’t worried so much about PHP code, but occasionally Word and PDF documents are put into the source repo. ClamAV also scans for URLs and matches them against the Google Safe Browsing database for malware and phish sites. The other security checks are alerts and triggers for code reviews. Files are scanned for commonly misused or abused functions involving cryptography, random numbers, and process management. If new instances are found, a alert for a code-review is done. Many times the code is just fine , but sometimes adjustments are needed or the goal can be achieved without using crytopgrahy. Likewise we alert on functions that “take a password” such as ftp_login . We want to avoid passwords being checked into source control. Some files are sensitive enough that any change triggers an alert for full review. A lot more detail can be found in the presentation Static Analysis for PHP first given at PHPDay, Verona, Italy on May 19, 2012 . They put on a great conference and highly recommend it for next year. Posted by Nick Galbreath on August 10, 2012 Category: engineering , infrastructure , security", "date": "2012-08-10,"},
{"website": "Etsy", "title": "Better Random Numbers in PHP using /dev/urandom", "author": ["Nick Galbreath"], "link": "https://codeascraft.com/2012/07/19/better-random-numbers-in-php-using-devurandom/", "abstract": "Better Random Numbers in PHP using /dev/urandom Posted by Nick Galbreath on July 19, 2012 The design of PHP’s basic random number generators rand and it’s newer variant mt_rand is based off the C Standard Library . For better or worse, both use a single global state and this can be reset using stand (or mt_srand ). This means anyone (a developer, a third party module, a library) could set the state to a fixed value and every random number following will be the same for every request. Sometimes this is the desired behavior but this can also have disastrous consequences.  For instance, everyone’s password reset code could end up being the same. Recently, Argyros and Kiayias in I Forgot Your Password: Randomness Attacks Against PHP Applications suggests there might be more fundamental problems in how PHP constructs the state of the random number generator. Just by seeing the output of a few calls to rand or mt_rand , one can predict the next output. With this, and certain password reset implementations, an attacker could perform account takeover. (This paper is also going to be presented on July 25 at Black Hat USA ). Quite some time ago, Etsy switched over to a different way of generating random numbers by using /dev/urandom that prevents both issues. /dev/urandom is a special psuedo-file on unix-like operating systems that generates “mostly random” bytes and is non-blocking. /dev/random (with no “ u “) is for truly cryptographic applications such as key generation and is blocking.  Once you exhaust it’s supply of randomness it blocks until it distills new randomness from the environment.  Therefore, you don’t want to use /dev/random in your web application. To see why, connect to a (non-production!) remote machine and type in “ cat /dev/random > /dev/null “, and the in another window try to log in. You won’t be able to, since SSH can’t read from /dev/random and therefore can’t complete the connection. A pedagogical replacement of rand , mt_rand with /dev/urandom using the mcrypt module might be: // equiv to rand, mt_rand\n// returns int in *closed* interval [$min,$max]\nfunction devurandom_rand($min = 0, $max = 0x7FFFFFFF) {\n    $diff = $max - $min;\n    if ($diff < 0 || $diff > 0x7FFFFFFF) {\n\tthrow new RuntimeException(\"Bad range\");\n    }\n    $bytes = mcrypt_create_iv(4, MCRYPT_DEV_URANDOM);\n    if ($bytes === false || strlen($bytes) != 4) {\n        throw new RuntimeException(\"Unable to get 4 bytes\");\n    }\n    $ary = unpack(\"Nint\", $bytes);\n    $val = $ary['int'] & 0x7FFFFFFF;   // 32-bit safe\n    $fp = (float) $val / 2147483647.0; // convert to [0,1]\n    return round($fp * $diff) + $min;\n} A long time ago, Etsy didn’t even have mcrypt installed and so we read directly from /dev/urandom using open and fread (see also stream_set_read_buffer ). Using /dev/urandom is quite a bit slower than using the native php functions. However, as a percentage of total page load time, it’s effectively free and unnoticeable.  Do we graph this? Yes we do! Here’s a days worth of random: Note that the above code converting bytes to an integer will demonstrate a slight bias with very large ranges, so we can’t use for it with Etsy’s monte-carlo long-range simulation forecasting hand-made supercomputer but for all the other (non-cryptographic) web applications likely to be.  For other algorithms and details on this topic, the main reference is Knuth’s Art of Computer Programming: Seminumerical Algorithms .  A more modern treatment can be found in any of the Numerical Recipes books.  The Java source code for java.util.Random is also a good reference.  Enjoy! Posted by Nick Galbreath on July 19, 2012 Category: security", "date": "2012-07-19,"},
{"website": "Etsy", "title": "June 2012 Site Performance Report", "author": ["Seth Walker"], "link": "https://codeascraft.com/2012/07/12/june-2012-site-performance-report/", "abstract": "June 2012 Site Performance Report Posted by Seth Walker on July 12, 2012 Hello! We’re relocating our series of site performance reports from the Etsy News Blog to its new home here on Code as Craft.  We’ll continue to regularly share our performance numbers here while allowing ourselves to go into a bit more depth. You can find the previous report here . Performance from Your Perspective In the last performance report I introduced front-end performance and described two new metrics that we would be looking at when measuring overall site performance – the time for a visitor to see any content at all (“time to start render”) and the time until a page had downloaded and processed all the components required to be fully displayed and interactive (“time to document complete”). We use a publicly available tool called WebPagetest to measure how a visit to Etsy.com looks in a real web browser from various locations around the world. Let’s take a look at the results of some of these tests. All of these tests were run at the same time on the same day (Thursday, June 28), using Internet Explorer 8 from Dulles, Virginia over a DSL connection as a signed-in Etsy member. It should be noted that because these are single measurements they are more prone to variation than aggregate measurements like our server-side times.  I chose these results to share because they had load times closest to the median of all the tests, but it’s not unusual to see significantly different results from test to test. The most surprising result here is how long it took for listing pages to show any content. This finding wasn’t in line with previous measurements we had taken (including the example shown in the previous post), so we did a bit of investigation and found a recent change that was delaying page rendering for Internet Explorer users, with some visitors seeing a blank white page for nearly three full seconds before any content appeared in the browser.  We were able to optimize the page and cut the time to show content to Internet Explorer users in half, measuring at around 1.5 seconds in a recent test . This is an example of how we’ll be using WebPagetest and other front-end performance measurement tools here at Etsy to keep pages feeling snappy for all of our visitors. We consider all of these results a starting point, and will be working hard to shrink them just as we’ve been doing with the server side times. Update on Server-Side Times The median and 95th percentile times for core pages, Tuesday June 26, 2012: The big news here is the improvement of search performance where the median times dropped by 20% since the last update. The search team dug deep to make these gains, and they’re not done yet. And of course we’ll continue to look for opportunities to improve in all areas of the site. Another change from last update is in what we measure – from now on we’ll be looking at the median (50% experienced a faster response, 50% slower) instead of the average. We believe this more accurately describes the experience of a typical Etsy visitor. Links to WebPagetest results: homepage listing profile search shop Posted by Seth Walker on July 12, 2012 Category: performance", "date": "2012-07-12,"},
{"website": "Etsy", "title": "Is It A Bird? Is It A Plane? No, It’s Supergrep!", "author": ["John Goulah"], "link": "https://codeascraft.com/2012/06/28/is-it-a-bird-is-it-a-plane-no-its-supergrep/", "abstract": "Is It A Bird? Is It A Plane? No, It’s Supergrep! Posted by John Goulah on June 28, 2012 Etsy parties have come to earn quite the boisterous reputation — there may or may not have been kegs dragged up to the roof on a particularly balmy occasion, not to mention cadaver eating contests and mariachi bands. Thus, one of the greatest survival skills I have come to hone here at HQ is the art of selective hearing. Being able to focus in on the subtle wit of your coworker in the midst of a rowdy congo line is the only way you will leave at the end of the evening with your sanity (and self-respect) intact. Luckily for us, eliminating the noise in our data using technology is a little bit easier. After a long week two years ago with a number of site incidents, our then-CTO Chad Dickerson sent an email to our all-engineering mailing list about a new tool: For a few of the incidents we’ve had in the past week, we’ve found obvious trails in the error logs leading directly to the source of the problem. I talked to a bunch of folks and what I generally heard was that the logs were noisy, which is technically true, but. . . . . Through the magic of grep, you can get past the noise. I did some digging through the error logs today to better understand what was in there and wrote a simple ugly grep that kills all the stuff that repeats a lot (not that we don’t need to fix it). And thus supergrep was born. The original was a simple grep -v command line bash script to make our logs easier to parse, but the tool has evolved into a communication mechanism for the entire team to view our logs in real time over a web browser. Since then, we have cleaned up many of the noise problems that obscured our logs, while iterating the tool into something easily accessible by everyone on team. It’s a simple node app that everyone on the team can access as they are pushing code, to see if new errors appear. In addition we’ve linked the errors to the exact line of code that the error is happening on for easier analysis. Its yet another change-awareness tool in our kit that allows us to push code continuously and safely to production. We’ve open sourced the code on our github repository, which you can find here: https://github.com/etsy/supergrep Posted by John Goulah on June 28, 2012 Category: engineering Tags: grep , logs , node , node.js , open source , supergrep", "date": "2012-06-28,"},
{"website": "Etsy", "title": "Building websites with science", "author": ["Peter Seibel"], "link": "https://codeascraft.com/2012/06/21/building-websites-with-science/", "abstract": "Building websites with science Posted by Peter Seibel on June 21, 2012 At Etsy, we want to use data to inform what we do. We have an awesome culture of obsessing over graphs and building dashboards to track our progress. But beyond tracking the effects of our changes from minute to minute via graphs on big screens—which is a great way to see if a change has had some dramatic effect on, say, performance or error rates—there’s another, slightly slower paced way to use data: we can gather data over days or even weeks and then analyze it to determine whether the changes we have made to the web site have had the effects we expected them to. There are different levels of awareness of data that different companies and teams within one company can have. Here at Etsy our search team, perhaps because of the inherently data-centric nature of their product, has been one of the leading users of data-driven development but the techniques they use can be applied to almost any kind of product development and we’re working to spread them throughout the company. In this post I’ll describe some of the different levels and finish up with some caveats about using science in product development. Data? What’s that? If you are building websites in the least scientific way possible your workflow probably goes like this: Think of a change we might make to the website (“I think I’ll change the background color on the Valentine’s Day page from white to pink”). This may be based on a theory about why that change makes sense (“It’ll put people in a more festive mood and they’ll buy more stuff”) or it may just “feel right”. In either case, you make the change and move on to your next idea. Unfortunately, people often disagree about whether something’s a good idea—everybody has an opinion and without any data there’s no way to escape the realm of pure opinion. The mark of a group operating at this level of no data use is the large amount of time people spend arguing about whether changes are worthwhile or not, both before and after the changes are made. You know, we’ve got these web logs Of course nobody really builds websites with quite this level of obliviousness since no one can resist the temptation to look at web logs and make up a story about what’s going on. A team operating at the next level of data awareness would think up an idea (pink background), make the change, and then look at the web logs to see how many people are buying things from the new, pink Valentine’s Day page. This is an improvement over the first level—if things change, at least you’ll know—but not much of one. The problem is that even though the data may tell you whether things are getting better or worse there’s almost no way to know whether the change is due to the pink background or some other factor. Maybe it’s getting closer to Valentine’s Day so people are more likely to buy Valentine’s Day related products. Or maybe over time, the user base has shifted to include people who are more likely to actually buy things. In other words there are any of a number of “confounding factors” that may be causing sales to go up or down, independent of the changed background. The mark of a team at this level is that the same amount of time is spent arguing but now the arguments are about how to interpret the data. Folks who think the change was good will argue that improving sales are the result of the change and declining sales due to something else while folks who don’t like the change will argue the opposite. But the arguments take just as long and are just as inconclusive. Hey kids, let’s do “science”! The obvious next step, for many people, is to try to control for the many confounding factors that result from the passage of time. Unfortunately there are a lot of wrong ways to do that. For instance, you might say, “Let’s use the pink background only on Thursdays and then compare the purchase rate on Thursday to that of rest of the week.” While this is a step in the right direction, and is probably easy to implement, it is not as useful as you might think. While you have controlled for some confounding factors, there are still many others that could still be intruding. Any difference that could possibly exist between the folks who visit on Thursday and those who come during the rest of the week could provide an alternate explanation of any difference in purchase rates. Just because there’s no obvious reason why Thursday visitors would be richer or more profligate or just more into Valentine’s Day than the average user, they might be and if they are it will confound the results. At this stage there are likely to be fewer useless arguments but there is a quite high chance of making bad decisions because the data don’t really say what folks think they say. That’s so random Luckily there’s one simple technique that neatly solves this problem. Instead of dividing visitors by what day they visit the site or some other seemingly arbitrary criteria, divide them by the one criteria which, by definition, is not linked to anything else: pure randomness. If every user is assigned to either the pink or white background at random, both groups should be representative of your total user base in all the respects you can think of and all the ones you can’t. If your total user base is 80% women, both groups will be approximately 80% women. If your user base has 1% of people who hate the color pink, so will both your experimental groups. And if your Thursday users, or any other subset, are for whatever reason more likely to buy than the average user, they will now be distributed evenly between both the pink and white background groups. Another way to look at it is that if you observe a difference in purchasing rate between two randomly assigned groups then you only have to consider two possibilities: that the difference is actually due to the background color or that it’s due to random chance. That is, just as it’s possible to flip a coin ten times and get eight heads and two tails, it’s possible that the random assignment put a disproportionate number of big spenders in one group and that’s why the purchasing rates differ. However statistics provides tools that can tell you how likely it is that an observed difference is due to chance. For example, statistics can tell you something like: “If the background color in fact had no effect on purchasing rate, there’s a 0.002% chance that you would have seen the difference you did due to the random assignment of users to the two groups.” If the chance is small enough, then you can be quite confident that the only other possible explanation, that the background color had an effect, is true. This is what data geeks mean when we say something is statistically significant : results are statistically significant when the chance of having achieved them by pure chance is sufficiently small. How small is sufficiently small is not fixed. One common threshold is 95% confidence which means that the chance of getting the results by chance has to be less than 5%. But note that at that threshold, one out of every twenty experiments where the change actually has no effect, will get a result that passes the threshold for statistical significance leading you to think the change did have an effect. This is called a false positive. You can control how many false positives you get by how you choose your threshold. With a higher confidence level, say 99%, you will have fewer false positives but will increase your rate of false negatives—times when the change did have an effect but the measured difference isn’t great enough to be considered statistically significant. In addition to the confidence level, the likelihood of a false negative depends on the size of the actual effect (the larger the effect, the less likely it is to be mistaken for random fluctuation) and the number of data points you have (more is better). There are, however, two things to note about statistical significance. One is that there is no law that says what confidence level you have to use. For many changes 95% might be a good default. But imagine a change that is going to have a significant operational cost if rolled out to all users, maybe a change that will require expensive new database hardware. In such a case you might want to be 99% or even 99.9% confident that any observed improvement is real before deciding to turn on the feature for everyone. The second thing to keep in mind is that statistical significance is not the same as practical significance. With enough data you can find statistically significant evidence for tiny improvements. But that doesn’t mean the tiny improvements are practically useful. Again, consider a feature that will cost real money to deploy. You may have highly statistically significant evidence that a change gives somewhere between a .0001% and .00015% improvement in conversion rate, but if even the top end of such a small change won’t produce enough new revenue to outweigh the cost of the feature you probably shouldn’t roll it out. Any group that understands these issues and is running randomized tests is truly using data to understand the effects of their changes and to guide their decisions. The marks of a team operating at this level are that at least some people spend their time arguing about statistical arcana—what is the proper alpha level, Bayesian vs frequentist models, and exactly what statistical techniques to use for hypothesis testing—as well as coming up with more sophisticated statistical analyses to glean more knowledge from the data gathered. Real Science™ Note, however, that no matter how fancy the analysis, the only knowledge built is about the effect of each particular change. Each new change requires starting all over—coming up with an idea, testing it with randomly assigned groups of users, and analyzing the data to see if it made things better or worse. Which may not be too bad. If you have reasonably good intuition about what kind of changes are likely to be improvements, and you aren’t running out of ideas for things to do, you may be able to get pretty far with that level of data-driven development. But just as using data to decide what changes are actually successful is often better than relying on people’s intuition, you can also use data to help improving people’s intuition by by providing them with actual knowledge about what kinds of things generally do and do not work. To do this you need to follow the classical cycle of the scientific method: Ask a question Formulate a hypothesis Use the hypothesis to make a prediction Run experiments to test the prediction Analyze the results, refining or rejecting the hypothesis Communicate the results Repeat For instance, you might ask a question like, what kind of images make people more likely to purchase things on my site? One hypothesis might be that pictures of people, as opposed to pictures of inanimate objects, make visitors more likely to purchase. An obvious prediction to make would be, adding a picture of a person to a page will increase the conversion rate for users who visit that page. And the prediction is easy to test: pick a page where it would make sense to add a picture of a person, do a random A/B test with some users seeing the person and others some other kind of picture, and measure the conversion rate for that page. If the results of the experiment are in line with your prediction that’s good but you’re not done—next you want to replicate the result, maybe on a different kind of page. And you may also want to test other predictions that follow from the same hypothesis—does removing pictures of people from a page depress the conversion rate? You can also refine your hypothesis—perhaps the picture must include a clearly visible face. Or maybe you’ll discover that pictures of people increase sales only in certain contexts. Whatever you discover you then need to communicate your results—write them down and make it part of your institutional knowledge about how to build your web site. It’s unlikely that you’ll end up with any equations as simple as E = mc 2 or F = ma that can guide you unerringly to profitability or happier users or whatever it is you care most about. But you can still build useful knowledge. Even if you just know things like, “in these contexts pictures of people’s faces tend to increase click through” you can use that knowledge when designing new features instead of being doomed to randomly wander the landscape of possible features with only local A/B tests to tell you whether you’re going up or down. The limits of science Science, however, cannot answer all questions and it’s worth recognizing some of its limitations. Indeed science is neither necessary nor sufficient: Jonathan Ive at Apple, working with his hand-picked team of designers behind the tinted windows of a lab from which most Apple employees were barred, didn’t—so far as we know—use A/B testing to design the iPhone. And Google, for all their prowess at marshaling data, has not been able A/B test their way to a successful social networking product. Here are a few of the practical problems with trying to design products purely with science: A/B testing is a hill climbing technique If you imagine all the variants of all the features you could possibly build arranged on a landscape, with better variants at higher altitude, then A/B testing is a great way to climb toward better versions of an existing feature. But once we’re at a local maximum, every direction leads downhill. The only way off a local maximum is to leap into space and try to land (or hope that you do) somewhere on a gradient that leads to a higher peak. If you do leap off toward a new hill, you can’t use A/B testing to compare where you land to where you were because most likely you’ll land somewhere below the peak of the new hill and quite possibly lower than where you were on the old hill. But if the new peak is actually higher than the old one, you’d do well to stay on the new hill and use A/B testing to climb to the higher local maximum. Once we’ve made the leap, science and experiments definitely come back into play but they’re of limited use in making the leap itself. A/B testing is only as good as our A and B ideas Another way to look at A/B testing is as a technique for comparing ideas. If neither the A nor B idea is very good, then A/B testing will just tell you which sucks less. Knowledge built with experiments, will—we hope—help designers navigate the space of possible ideas but science doesn’t say a lot about where those good ideas ultimately come from. Even in “real” science exactly how scientists come up with interesting questions and good hypothesis is a bit mysterious. You can’t easily learn about people who aren’t using your product The basis for statistical inference is the sample. We learn about a larger population by measuring the behavior of a random sample from that population. If we want to learn about the behavior of current Etsy users we’re all set—run an experiment with a random sample of users and draw our inferences. But what if we want to know whether a new feature will expand our user base? It’s harder (though not necessarily impossible) to get a random sample of people who aren’t currently using Etsy but who might be interested in some as-yet-undeveloped Etsy feature. Consequently, there’s a real risk of looking under the streetlamp (running experiments on our current users) because the light is better, rather than looking somewhere where we might actually find the keys to a hot new car. Finally there’s one larger structural problem with the scientific method captured by a famous paraphrase of something Max Planck, founder of quantum theory and the 1918 physics Nobel laureate said: “Science progresses one funeral at a time.” In other words, it can be difficult to let go of hard-won scientific “truths”. After you’ve gone to all the trouble to ask an interesting question, devise a clever hypothesis, and design and run the painstaking experiments that convince everyone that your hypothesis is correct, you don’t really want to hear about how your hypothesis doesn’t fit with some new bit of data and is bound to be replaced by some new theoretical hotness. Or as Arthur C. Clarke put it in his First Law: When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong. In product development, instead of distinguished but elderly scientists we have experienced product managers. But we have the same risk of holding on too long to hard-won “scientific” truths. If we do everything right, running experiments to test all our hypothesis about how the product works and even making occasional bold leaps into new territory to escape the trap of A/B incrementalism, eventually accumulated “scientific” knowledge may become a liability. And the danger is even worse in product development than it is in “real” science since we’re not dealing with universal and timeless truths. Indeed, each product you launch (to say nothing of what the rest of the world is doing) changes the landscape into which new products will be launched. What you learned yesterday may no longer apply tomorrow, in large part because of what you do today. Properly applied, science can lead us out of such difficulties but it’s worth noting that it was science, by allowing us to know things at all, that can mislead us into thinking we know a bit more than we really do. So what to do? Science is powerful and it’d be silly to abandon it entirely just because there are ways it can go wrong. And it’d be equally silly to think that science can somehow replace strong, talented product designers. The goal should be to combine the two. The strength of the lone genius designer approach (c.f. Apple) is that when you actually have a genius you get a lot of good ideas and can take giant leaps to products that would never have been arrived at by incremental revisions of anything already in the world. Furthermore, with a single designer imposing their vision on a product, it is more likely to have a coherent, holistic design. On the other hand, if you give your lone genius completely free rein then there is no protection from their occasional bad ideas. The data-driven approach, (c.f. Google), on the other hand, is much stronger on detecting the actual quality of ideas—bad ideas are abandoned no matter who originally thought they were good and the occasional big wins are clearly identified. But, as noted above, it doesn’t have much to say about where those ideas come from. And it can fall into the trap of focusing on things that are easy to measure and test—don’t know what shade of blue to use? A/B test 41 different shades. (An actual A/B test run at Google.) Ultimately the goal is to make great products. Great ideas from designers are a necessary ingredient. And A/B testing can definitely improve products. But best is to use both: establish a loop between good design ideas leading to good experiments leading to knowledge about the product leading to even better design ideas. And then allow designers the latitude to occasionally try things that can’t yet be justified by science or even things that my go against current “scientific” dogma. Posted by Peter Seibel on June 21, 2012 Category: data , philosophy", "date": "2012-06-21,"},
{"website": "Etsy", "title": "Robots, Graphs, and Binary Search", "author": ["Nick Galbreath"], "link": "https://codeascraft.com/2012/05/24/robots-graphs-and-binary-search/", "abstract": "Robots, Graphs, and Binary Search Posted by Nick Galbreath on May 24, 2012 We love our human customers. That said we get a lot of traffic from robots too. This is great, especially when they use the Etsy API .   However, they sometimes misbehave .  And they misbehave frequently in the late hours, not unlike a legendary East Village night club .  This time Craig Ferguson isn’t at the door to keep an eye on things.  Instead we monitor the robots attending our night club with graphs: To do this we first compiled a list of where the robots live . This includes datacenters, places that make search engines and crawlers, and companies that lease out CPUs by the slice.  The full list has over 1800 records, but here’s a sample: So given this list, and an incoming IP address, how can one quickly tell who it belongs too, if anyone?  Linear scan?  No. Binary Search? Yes, and we like binary search . To do this, the raw file is converted into a static PHP file .  It converts all the dotted IPv4 addresses into the equivalent plain integers and turns all the data into an PHP array.  As part of the file, there is a find method that uses a binary search algorithm modified to work with disjoint intervals.  Can you spot the difference? <?php\n/* Autogenerated.  Do not edit */\nclass IpDb {\n    public static function find($ipstr) {\n        $ip = ip2long($ipstr);\n        $haystack = self::$db;\n        $high = count($haystack) - 1;\n        $low = 0;\n        while ($high >= $low) {\n            $probe = floor(($high + $low) / 2);\n            $row = $haystack[$probe];\n            if ($row['_ip0'] > $ip) {\n                $high = $probe - 1;\n            } else if ($row['_ip1'] < $ip) {\n                $low = $probe + 1;\n            } else {\n                return $row;\n            }\n        }\n        return null;\n    }\n    static public $db = array (\n  0 =>\n  array (\n    '_ip0' => 34603008,\n    '_ip1' => 35127295,\n    'owner' => 'http://akamai.com/',\n  ),\n  1 =>\n  array (\n    '_ip0' => 134488576,\n    '_ip1' => 134489087,\n    'owner' => 'http://www.peakwebhosting.com/',\n  ),\n  2 => ... etc... The resulting file is pushed out in the same way all our other code is. Using this mini WHOIS database is just a PHP function call without any database or network access. The same technique can easily be done in other languages or used for other applications involving date-time intervals. This is just one of many ways we monitor the site for bad behavior (robots or otherwise). Seeing how much traffic comes from where “hands aren’t on keyboard” has given us many insights into how the site is used. Some more details on how this data is used can be found in presentation on Fraud Engineering given at the Merchant Risk Council conference in March. Finally, let’s not forget that not all robots are bad. Update: as soon as I posted this, my colleague Jerry Soung informed he implemented the same thing for a different IP database.  Binary search FTW. Posted by Nick Galbreath on May 24, 2012 Category: engineering , operations , security", "date": "2012-05-24,"},
{"website": "Etsy", "title": "Navigating Etsy Support", "author": ["jcarrafa"], "link": "https://codeascraft.com/2012/06/01/navigating-etsy-support/", "abstract": "Navigating Etsy Support Posted by jcarrafa on June 1, 2012 At Etsy we get about 10,000 emails a week from our members.  Since Etsy depends on the community, it’s important for us to respond quickly with detailed answers.  Additionally, we have to address the problem of spammers.  Previously we used a third party system for managing our support.  Quickly we found that in order to provide service of the highest possible quality, we had to build our own.  So we did! The Compass Inbox (names were randomly generated for this example) Introducing Compass, our tool for navigating the wide range of emails we get.  This tool manages the inboxes for many different departments.  Generally, it’s designed for communications that resemble a “Task.”  Tasks encapsulate email threads, and have a state of “open” or “closed.”  If we respond to an email asking for more information, we can leave the Task open, indicating that member still needs attention.  Tasks also support assignment, so that as emails come into a general mailbox, any Etsy employee may assign it to themselves indicating they will answer it.  Any future communication with that member will then automatically go to the same employee. Since Tasks are the cornerstone to the operation of Compass, it’s very easy to use the system for many internal needs.  Recently, we just added a mailbox for the security team to receive feedback from the community on suspected security weaknesses. The Knowledge Base ensures that we have consistent, detailed answers to email.  This was developed as a separate tool, which Compass calls using REST.  The Knowledge Base has over 1,500 answers to common problems.  These answers, or “articles,” are translated into several languages, and also have different versions for different mediums such as email, twitter, forums, and chat. Compass leverages the content of the Knowledge Base by allowing a support agent to fill an email with the most appropriate article.  They usually do this using a shortcut, allowing them to give detailed, comprehensive answers very quickly.  Agents can then customize the email further to make it personal.  The benefits of this system go far beyond high quality responses: we track which articles are used, giving us easy access to metrics showing what people are asking about.  This allows us to track current trending issues by the hour and identify longer term product issues our members continually struggle with. Currently, our most used article (by a somewhat large margin,) is “ Contact seller .”  This article informs the member that they should direct their question to the seller of the item they’re asking about.  It shows us that it is difficult for many users of the site to distinguish between Etsy and the independent shop owner.  We’re always improving the site to provide a better experience in response to this collective feedback. As with all things, the devil is in the details.  How should we handle spam?  Also, members often use attachments, so how should we handle malware?  We’ve used the same solution for both problems: Google.  All of our email accounts are actually GMail accounts in Google Apps.  Google handles all of the spam filtering for us, then we retrieve the legitimate emails over IMAP.  After we’ve parsed and stored the message, we upload any non-image attachments to Google Docs, which provides two great functions:  it checks for malware, and provides an in-browser view of the attachment. It took a serious engineering effort to build this in-house support system, but the benefits have justified the costs.  Most days, we have a turnaround time of 12 hours or less, depending on volume.  We’re able to segment support into specialized teams, maximizing quality and efficiency.  We have detailed information about our users when viewing an email thread.  Notes left on an email thread are available across all tools.  We can create Tasks in Compass beyond email, tying in the “Contact Us” page and other system-generated messages.  Personally, I think the most important part about owning our own tool is control over the interface.  Many Etsy employees use Compass for 8 hours a day.  Even the smallest UI improvement makes a huge impact for over 50 users on 5 teams in 4 countries. This was just a brief overview of what we’re doing with our internal tools.  Stay tuned for more specific posts about decisions we made, and things we learned.  Next up we’ll talk about the evolution of our javascript architecture. If you’re interested in developing internal tools at Etsy, where your customer sits next to you and will come give you a hug when you release a new feature, the Marketplace Operations Engineering team is hiring.  Check out www.etsy.com/careers . Posted by jcarrafa on June 1, 2012 Category: people , support", "date": "2012-06-1,"},
{"website": "Etsy", "title": "Blameless PostMortems and a Just Culture", "author": ["John Allspaw"], "link": "https://codeascraft.com/2012/05/22/blameless-postmortems/", "abstract": "Blameless PostMortems and a Just Culture Posted by John Allspaw on May 22, 2012 Last week, Owen Thomas wrote a flattering article over at Business Insider on how we handle errors and mistakes at Etsy. I thought I might give some detail on how that actually happens, and why. Anyone who’s worked with technology at any scale is familiar with failure. Failure cares not about the architecture designs you labor over, the code you write and review, or the alerts and metrics you meticulously pore through. So: failure happens. This is a foregone conclusion when working with complex systems. But what about those failures that have resulted due to the actions (or lack of action, in some cases) of individuals? What do you do with those careless humans who caused everyone to have a bad day? Maybe they should be fired. Or maybe they need to be prevented from touching the dangerous bits again. Or maybe they need more training. This is the traditional view of “human error”, which focuses on the characteristics of the individuals involved. It’s what Sidney Dekker calls the “Bad Apple Theory” – get rid of the bad apples, and you’ll get rid of the human error. Seems simple, right? We don’t take this traditional view at Etsy. We instead want to view mistakes, errors, slips, lapses, etc. with a perspective of learning . Having blameless Post-Mortems on outages and accidents are part of that. A Blameless Post-Mortem What does it mean to have a ‘blameless’ Post-Mortem? Does it mean everyone gets off the hook for making mistakes? No. Well, maybe. It depends on what “gets off the hook” means. Let me explain. Having a Just Culture means that you’re making effort to balance safety and accountability. It means that by investigating mistakes in a way that focuses on the situational aspects of a failure’s mechanism and the decision-making process of individuals proximate to the failure, an organization can come out safer than it would normally be if it had simply punished the actors involved as a remediation. Having a “blameless” Post-Mortem process means that engineers whose actions have contributed to an accident can give a detailed account of: what actions they took at what time, what effects they observed, expectations they had, assumptions they had made, and their understanding of timeline of events as they occurred. …and that they can give this detailed account without fear of punishment or retribution. Why shouldn’t they be punished or reprimanded? Because an engineer who thinks they’re going to be reprimanded are disincentivized to give the details necessary to get an understanding of the mechanism, pathology, and operation of the failure. This lack of understanding of how the accident occurred all but guarantees that it will repeat. If not with the original engineer, another one in the future. We believe that this detail is paramount to improving safety at Etsy. If we go with “blame” as the predominant approach, then we’re implicitly accepting that deterrence is how organizations become safer. This is founded in the belief that individuals, not situations, cause errors. It’s also aligned with the idea there has to be some fear that not doing one’s job correctly could lead to punishment. Because the fear of punishment will motivate people to act correctly in the future. Right? This cycle of name/blame/shame can be looked at like this: Engineer takes action and contributes to a failure or incident. Engineer is punished, shamed, blamed, or retrained. Reduced trust between engineers on the ground (the “sharp end”) and management (the “blunt end”) looking for someone to scapegoat Engineers become silent on details about actions/situations/observations, resulting in “Cover-Your-Ass” engineering (from fear of punishment) Management becomes less aware and informed on how work is being performed day to day, and engineers become less educated on lurking or latent conditions for failure due to silence mentioned in #4, above Errors more likely, latent conditions can’t be identified due to #5, above Repeat from step 1 We need to avoid this cycle. We want the engineer who has made an error give details about why (either explicitly or implicitly) he or she did what they did; why the action made sense to them at the time. This is paramount to understanding the pathology of the failure. The action made sense to the person at the time they took it, because if it hadn’t made sense to them at the time, they wouldn’t have taken the action in the first place. The base fundamental here is something Erik Hollnagel has said: We must strive to understand that accidents don’t happen because people gamble and lose. Accidents happen because the person believes that: …what is about to happen is not possible, …or what is about to happen has no connection to what they are doing, …or that the possibility of getting the intended outcome is well worth whatever risk there is. A Second Story This idea of digging deeper into the circumstance and environment that an engineer found themselves in is called looking for the “Second Story”. In Post-Mortem meetings, we want to find Second Stories to help understand what went wrong. From Behind Human Error here’s the difference between “first” and “second” stories of human error: First Stories Second Stories Human error is seen as cause of failure Human error is seen as the effect of systemic vulnerabilities deeper inside the organization Saying what people should have done is a satisfying way to describe failure Saying what people should have done doesn’t explain why it made sense for them to do what they did Telling people to be more careful will make the problem go away Only by constantly seeking out its vulnerabilities can organizations enhance safety Allowing Engineers to Own Their Own Stories A funny thing happens when engineers make mistakes and feel safe when giving details about it: they are not only willing to be held accountable, they are also enthusiastic in helping the rest of the company avoid the same error in the future. They are, after all, the most expert in their own error. They ought to be heavily involved in coming up with remediation items. So technically, engineers are not at all “off the hook” with a blameless PostMortem process. They are very much on the hook for helping Etsy become safer and more resilient, in the end. And lo and behold: most engineers I know find this idea of making things better for others a worthwhile exercise. So what do we do to enable a “Just Culture” at Etsy? We encourage learning by having these blameless Post-Mortems on outages and accidents. The goal is to understand how an accident could have happened, in order to better equip ourselves from it happening in the future We seek out Second Stories, gather details from multiple perspectives on failures, and we don’t punish people for making mistakes. Instead of punishing engineers, we instead give them the requisite authority to improve safety by allowing them to give detailed accounts of their contributions to failures. We enable and encourage people who do make mistakes to be the experts on educating the rest of the organization how not to make them in the future. We accept that there is always a discretionary space where humans can decide to make actions or not, and that the judgement of those decisions lie in hindsight. We accept that the Hindsight Bias will continue to cloud our assessment of past events, and work hard to eliminate it. We accept that the Fundamental Attribution Error is also difficult to escape, so we focus on the environment and circumstances people are working in when investigating accidents. We strive to make sure that the blunt end of the organization understands how work is actually getting done (as opposed to how they imagine it’s getting done, via Gantt charts and procedures) on the sharp end. The sharp end is relied upon to inform the organization where the line is between appropriate and inappropriate behavior. This isn’t something that the blunt end can come up with on its own. Failure happens. In order to understand how failures happen, we first have to understand our reactions to failure. One option is to assume the single cause is incompetence and scream at engineers to make them “pay attention!” or “be more careful!” Another option is to take a hard look at how the accident actually happened, treat the engineers involved with respect, and learn from the event. That’s why we have blameless Post-Mortems at Etsy, and why we’re looking to create a Just Culture here. Posted by John Allspaw on May 22, 2012 Category: engineering , people", "date": "2012-05-22,"},
{"website": "Etsy", "title": "Two Sides For Salvation", "author": ["Arie Kachler"], "link": "https://codeascraft.com/2012/04/20/two-sides-for-salvation/", "abstract": "Two Sides For Salvation Posted by Arie Kachler on April 20, 2012 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “primary/replica” in our Code as Craft entries. How do you make changes to your database’s structure that’s getting hammered 24×7 without any disruption? If you use Oracle and paid millions for it, it’s built in. If you use Mysql, it’s one of the holy grails of database operations, and one we’ve learned to do here at Etsy. We have a sharded architecture, which means data is scattered across several “shards”. Each shard has different data than all others. Each shard is a primary-primary pair. MM pairs are primaries and replicas at the same time. They not only give you fault tolerance, they divide the read and write load between them that’s impossible to do in the common primary-replica(s) setup. MM pairs have their own set of challenges. Don’t Let Your Database Generate Anything The main problem with MM pairs is caused by non-deterministic values generated by the database engine itself, such as autoincrement fields, random numbers and timestamps. The solution to that is that we don’t let the DB generate anything . Every value inserted/modified always comes from the application. This allows us to write to either of the two sides of a MM pair knowing it will get replicated to the other side correctly. I’ve heard that MM pairs don’t make sense since you’re executing everything twice. It’s true that you are executing everything twice, but you’re doing it already if you’re using a primary-replica(s) setup, and the benefits that come from MM pairs are huge. In addition to giving you fault tolerance and load balancing, they are the key to being able to do non-disruptive, live schema changes. The other part of the puzzle is our shard-aware software layer: our in-house built ORM. It does many different things, but for our current topic, it finds where in our shards a particular object’s data lives. Whenever we need to access the data for an object, the ORM first goes to one of two “index” servers we have, then go to the shard that has the needed data. These index servers are also a MM pair. Index servers get a very large amount of queries, but they are all extremely fast, all in the order of 10-100 microseconds. It’s common for sharded architectures not to have an index server. You simply decide on a sharding scheme when you start, say by user id, then divide the data among your shards knowing where ranges of users live. Everything works great until the number of users on a shard grows beyond what one shard can handle, and by then you’re already in trouble. By having an index server, we can move data between shards and simply update the index to point to the new location. Our ORM reads a configuration file when it starts, that among other things, contains the list of shard servers available to it. We can add shards as needed with time and add them to the configuration file to start writing data to them, also migrating users so new shards are not idle at first and to balance the load among all shards. The kicker: when we do schema changes, we take out one server from each of the MM pairs from the configuration file and gracefully restart the application. The ORM re-reads its configuration and knows only about the active shard sides. This leaves the application running on half of our database servers. Nobody notices . We immediately see in our many graphs that one side’s traffic plummets and the other side is taking all the load. Note that half of the servers does not mean half of the data. All data lives on both sides of a MM pair. Replication is still going both ways, we never break it. The active side simply stops getting inserts/updates/deletes from the inactive side because nothing is connecting to it. But the inactive side still gets inserts/updates/deletes from the active side since it’s still a replica. We could break replication for the ALTERs, but there’s no benefit in doing so and adds an unnecessary step (with the one exception of the session we’re actively doing ALTERs in. We don’t want those to replicate.) At this point we are ready to make as many changes as we need on the inactive side. In Mysql terms, ALTERs. These ALTERs can take anywhere from minutes to hours to complete and lock the tables they are modifying, but we’re operating on the inactive side and definitely don’t want any of our work to replicate to the other side, so we prepend ALTERs with SET SQL_LOG_BIN=0. When these alters are done, they have been applied to the inactive side only. Another change in the config file places these servers back into active mode. We wait for load to stabilize between both sides, replication to catch up if it has lagged behind, then we’re ready to repeat for the side that hasn’t been ALTER’ed. Taking sides out of production is not only useful for schema changes, but for upgrades, configuration changes, and any other necessary downtime. So this is all great, works well for us. We routinely do schema changes with no user impact. But what if you don’t have an ORM? Mysql Proxy may be your answer. It’s very simple to have web servers connect to a pool of available backend database servers with Mysql Proxy. You can read the documentation for it at Mysql’s website. An important feature of Mysql Proxy is that it allows you to change configuration on-the-fly, so you can take servers in and out without even having to stop or reload your application. MM pairs have had a bad reputation of being quirky. They can be, but as long as you don’t let your database generate anything, they work. When you need to do frequent schema changes in a 24×7 environment, they are key to no-downtime schema changes. If you want more details on our database architecture, you can also check here . Posted by Arie Kachler on April 20, 2012 Category: databases , infrastructure , operations", "date": "2012-04-20,"},
{"website": "Etsy", "title": "Etsy Hacker Grants: Supporting Women in Technology", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2012/04/05/etsy-hacker-grants/", "abstract": "Etsy Hacker Grants: Supporting Women in Technology Posted by Kellan Elliott-McCrea on April 5, 2012 “Today, in conjunction with Hacker School, Etsy is announcing a new scholarship and sponsorship program for women in technology: we’ll be hosting the summer 2012 session of Hacker School in the Etsy headquarters, and we’re providing ten Etsy Hacker Grants of $5,000 each — a total of $50,000 — to women who want to join but need financial support to do so. Our goal is to bring 20 women to New York to participate, and we hope this will be the first of many steps to encourage more women into engineering at Etsy and across the industry.” – find out more , and then checkout Etsy Hacker Grants Posted by Kellan Elliott-McCrea on April 5, 2012 Category: Uncategorized Tags: grants , hackerschool", "date": "2012-04-5,"},
{"website": "Etsy", "title": "Kernel Debugging 101", "author": ["Avleen Vig"], "link": "https://codeascraft.com/2012/03/30/kernel-debugging-101/", "abstract": "Kernel Debugging 101 Posted by Avleen Vig on March 30, 2012 A dark fog had been rolling in that night, and we had been setting up a new cluster of servers for our CI system . CentOS 6.2, LXC and random kernel panics were all there to lend a hand. The kernel panics were new to our party, having been absent at the previous cluster setup. The first set of servers we installed had been running happily, however this new set were not. They would always kernel panic under the slightest load, and sometimes without much load at all. A nice feature, which is enabled by default in CentOS 6, allows your kernel to dump a core file when it panics. When your system comes back, you can retrieve this core file, examine it and work out what happened. This is a short story on how we did it. The work we did is on CentOS but it can easily be applied to other Linux distributions too. Basics and theory When a kernel panics, it dumps a core file into /var/crash/, which can then be examined. In your tool belt, you need: /usr/bin/crash (installed through the “crash” package) A debuginfo kernel (downloaded and installed from http://debuginfo.centos.org/6/x86_64/ ) The vmcore file from /var/crash/.... crash is gdb-like. It uses gdb and lets you examine vmcore files from kernels. Basic debugging Start crash by pointing it at the vmlinux file installed by the debuginfo kernel, and the vmcore file: sudo crash /usr/lib/debug/lib/modules/`uname -r`/vmlinux \n/var/crash/<time>/vmcore You will see output like this: KERNEL: /usr/lib/debug/lib/modules/<kernel>/vmlinux\n    DUMPFILE: /var/crash/127.0.0.1-2012-03-28-23:51:01/vmcore\n  [PARTIAL DUMP]\n        CPUS: 16\n        DATE: Wed Mar 28 23:50:56 2012\n      UPTIME: 00:23:26\nLOAD AVERAGE: 0.95, 1.45, 1.01\n       TASKS: 986\n    NODENAME: buildtest07\n     RELEASE: 2.6.32-220.7.1.el6.x86_64\n     VERSION: #1 SMP Wed Mar 7 00:52:02 GMT 2012\n     MACHINE: x86_64  (2400 Mhz)\n      MEMORY: 24 GB\n       PANIC: \"Oops: 0000 [#1] SMP \" (check log for details)\n         PID: 0\n     COMMAND: \"swapper\"\n        TASK: ffff880337eb9580  (1 of 16)\n[THREAD_INFO: ffff880637d18000]\n         CPU: 12\n       STATE: TASK_RUNNING (PANIC) This tells us some important bits of information: The command being run was swapper . swapper is a kernel process responsible for scheduling time on the CPU. When a panic happens here, there’s a likelihood that we’re looking at something in the kernel space that broke, rather than something in user space. The panic was an Oops . There are dates, times, number of running processes and other handy information too. If you now run: crash> log and jump to the end, you will see much longer output like this: BUG: unable to handle kernel NULL pointer dereference at 0000000000000060\nIP: [<ffffffff8142bb40>] __netif_receive_skb+0x60/0x6e0\nPGD 10e0fe067 PUD 10e0b0067 PMD 0\nOops: 0000 [#1] SMP\nlast sysfs file: /sys/devices/virtual/block/dm-6/removable\nCPU 12\nModules linked in: veth bridge stp llc e1000e serio_raw\n                   i2c_i801 i2c_core sg iTCO_wdt\n                   iTCO_vendor_support ioatdma dca i7core_edac\n                   edac_core shpchp ext3 jbd mbcache sd_mod\n                   crc_t10dif ahci dm_mirror dm_region_hash\n                   dm_log dm_mod [last unloaded: scsi_wait_scan] Pid: 0, comm: swapper Not tainted <kernel> #1 Supermicro X8DTT-H/X8DTT-H\nRIP: 0010:[<ffffffff8142bb40>]  [<ffffffff8142bb40>] __netif_receive_skb+0x60/0x6e0\nRSP: 0018:ffff88034ac83dc0  EFLAGS: 00010246\nRAX: 0000000000000060 RBX: ffff8805353896c0 RCX: 0000000000000000\nRDX: ffff88053e8c3380 RSI: 0000000000000286 RDI: ffff8805353896c0\nRBP: ffff88034ac83e10 R08: 00000000000000c3 R09: 0000000000000000\nR10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000\nR13: 0000000000000015 R14: ffff88034ac93770 R15: ffff88034ac93784\nFS:  0000000000000000(0000) GS:ffff88034ac80000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0018 ES: 0018 CR0: 000000008005003b\nCR2: 0000000000000060 CR3: 000000010e130000 CR4: 00000000000006e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400\nProcess swapper (pid: 0, threadinfo ffff880637d18000,\ntask ffff880337eb9580)\nStack:\n ffffc90013e37000 ffff880334bdc868 ffff88034ac83df0 0000000000000000\n<0> ffff880334bdc868 ffff88034ac93788 ffff88034ac93700 0000000000000015\n<0> ffff88034ac93770 ffff88034ac93784 ffff88034ac83e60 ffffffff8142c25a\nCall Trace:\n <IRQ>\n [<ffffffff8142c25a>] process_backlog+0x9a/0x100\n [<ffffffff814308d3>] net_rx_action+0x103/0x2f0\n [<ffffffff81072001>] __do_softirq+0xc1/0x1d0\n [<ffffffff810d94a0>] ? handle_IRQ_event+0x60/0x170\n [<ffffffff8100c24c>] call_softirq+0x1c/0x30\n [<ffffffff8100de85>] do_softirq+0x65/0xa0\n [<ffffffff81071de5>] irq_exit+0x85/0x90\n [<ffffffff814f4dc5>] do_IRQ+0x75/0xf0\n [<ffffffff8100ba53>] ret_from_intr+0x0/0x11\n <EOI>\n [<ffffffff812c4b0e>] ? intel_idle+0xde/0x170\n [<ffffffff812c4af1>] ? intel_idle+0xc1/0x170\n [<ffffffff813fa027>] cpuidle_idle_call+0xa7/0x140\n [<ffffffff81009e06>] cpu_idle+0xb6/0x110\n [<ffffffff814e5ffc>] start_secondary+0x202/0x245\nCode: 00 44 8b 1d cb be 79 00 45 85 db 0f 85 61 06 00 00 f6 83 b9\n      00 00 00 10 0f 85 5d 04 00 00 4c 8b 63 20 4c 89 65 c8 49 8d\n      44 24 60 <49> 39 44 24 60 74 44 4d 8b ac 24 00 04 00 00 4d\n      85 ed 74 37 49\nRIP  [<ffffffff8142bb40>] __netif_receive_skb+0x60/0x6e0\n RSP <ffff88034ac83dc0>\nCR2: 0000000000000060 At this level the most interesting thing to note is the line starting with BUG : BUG: unable to handle kernel NULL pointer dereference at 0000000000000058 This tells us why the panic happened: A NULL pointer dereference. Intermediate debugging Now some more gnarly stuff. If you’re still here, you’re pretty damn brave. There some cool things we can do in crash which might help us. For example, we can look at the list of running processes: PID    PPID  CPU       TASK        ST  %MEM     VSZ    RSS  COMM\n      0      0   0  ffffffff81a8d020  RU   0.0       0      0  [swapper]\n>     0      0   1  ffff880638628a80  RU   0.0       0      0  [swapper]\n>     0      0   2  ffff880337d934c0  RU   0.0       0      0  [swapper]\n>     0      0   3  ffff8806386294c0  RU   0.0       0      0  [swapper]\n>     0      0   4  ffff880337dd3580  RU   0.0       0      0  [swapper]\n>     0      0   5  ffff880637c84080  RU   0.0       0      0  [swapper]\n>     0      0   6  ffff880337df1540  RU   0.0       0      0  [swapper]\n>     0      0   7  ffff880637c84ac0  RU   0.0       0      0  [swapper]\n>     0      0   8  ffff880337e33500  RU   0.0       0      0  [swapper]\n      0      0   9  ffff880637c85500  RU   0.0       0      0  [swapper]\n>     0      0  10  ffff880337e774c0  RU   0.0       0      0  [swapper]\n>     0      0  11  ffff880637cf40c0  RU   0.0       0      0  [swapper]\n>     0      0  12  ffff880337eb9580  RU   0.0       0      0  [swapper]\n>     0      0  13  ffff880637cf4b00  RU   0.0       0      0  [swapper]\n>     0      0  14  ffff880337ed7540  RU   0.0       0      0  [swapper]\n>     0      0  15  ffff880637cf5540  RU   0.0       0      0  [swapper]\n      1      0   8  ffff880638628040  IN   0.0   21364   1568  init\n      2      0   8  ffff880337c714c0  IN   0.0       0      0  [kthreadd]\n      3      2   0  ffff880337c70a80  IN   0.0       0      0  [migration/0]\n      4      2   0  ffff880337c70040  IN   0.0       0      0  [ksoftirqd/0]\n      5      2   0  ffff880337c99500  IN   0.0       0      0  [migration/0]\n.... The list continues on for another 950 lines. The lines starting with a > indicate the processes currently active on a CPU. We are working on a 16 core system, and 14 of them are currently in kernel mode tending to CPU scheduling duties. (The other 2 further down were gmond and php .) You can also change the context you’re currently in. For example, currently we’re looking at the CPU where the kernel panic happened. However, we can switch to another CPU if we wanted to using set -c <CPU> and examine things there. (We don’t want to do this yet.) Speaking of our kernel panic! Let’s get back to that.. The code at the end of the previous section tells us a null pointer dereference was the cause of the crash, and it gives us some clues as to what happened: IP: [<ffffffff8142bb40>] __netif_receive_skb+0x60/0x6e0 Here’s the code that was being executed! __netif_receive_skb , this sounds like it might have something to do with receiving things on the network interface, if I had to guess! (and if I did, I’d be right – many things in the kernel are named to be obvious). We could even grep through the kernel source to find this! shell> ack -a __netif_receive_skb\nnet/core/dev.c\n    2705:int __netif_receive_skb(struct sk_buff *skb) Looking at line 2705 of net/core/dev.c shows us the method that was running when things broke down. Advanced debugging Are you really still reading this? You’ve just gone from brave, to foolish! There is no turning back! Here’s the rabbit hole: There’s a lot more information here! Just because we know the method that was invoked doesn’t help us that much. The method could be 2 lines and the solution could be clear, but more likely the method will be 100 lines long and make many references to other things. So let’s start poking at the memory to see what specifically happened: IP: [<ffffffff8142bb40>] __netif_receive_skb+0x60/0x6e0 There are two each bits of information in this line: ffffffff8142bb40 is the memory address at which the problem occurred. 0x60 is the hex offset for the line in the memory that caused the problem. 0x60 is 96 in decimal – remember this! We can inspect the memory by doing this: crash> dis -rl ffffffff8142bb40\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 2706\n0xffffffff8142bae0 <__netif_receive_skb>:       push   %rbp\n0xffffffff8142bae1 <__netif_receive_skb+1>:     mov    %rsp,%rbp\n0xffffffff8142bae4 <__netif_receive_skb+4>:     push   %r15\n0xffffffff8142bae6 <__netif_receive_skb+6>:     push   %r14\n0xffffffff8142bae8 <__netif_receive_skb+8>:     push   %r13\n0xffffffff8142baea <__netif_receive_skb+10>:    push   %r12\n0xffffffff8142baec <__netif_receive_skb+12>:    push   %rbx\n0xffffffff8142baed <__netif_receive_skb+13>:    sub    $0x28,%rsp\n0xffffffff8142baf1 <__netif_receive_skb+17>:    nopl   0x0(%rax,%rax,1)\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 2715\n0xffffffff8142baf6 <__netif_receive_skb+22>:    cmpq   $0x0,0x18(%rdi)\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 2706\n0xffffffff8142bafb <__netif_receive_skb+27>:    mov    %rdi,%rbx\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 2715\n0xffffffff8142bafe <__netif_receive_skb+30>:    jne    0xffffffff8142bb16 <__netif_receive_skb+54>\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../arch/x86/include/asm/atomic_64.h: 23\n0xffffffff8142bb00 <__netif_receive_skb+32>:    mov    0xbdcbae(%rip),%eax        # 0xffffffff820086b4\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 1364\n0xffffffff8142bb06 <__netif_receive_skb+38>:    test   %eax,%eax\n0xffffffff8142bb08 <__netif_receive_skb+40>:    jne    0xffffffff8142c008 <__netif_receive_skb+1320>\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 1367\n0xffffffff8142bb0e <__netif_receive_skb+46>:    movq   $0x0,0x18(%rdi)\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../include/trace/events/net.h: 68\n0xffffffff8142bb16 <__netif_receive_skb+54>:    mov    0x79becb(%rip),%r11d        # 0xffffffff81bc79e8\n0xffffffff8142bb1d <__netif_receive_skb+61>:    test   %r11d,%r11d\n0xffffffff8142bb20 <__netif_receive_skb+64>:    jne    0xffffffff8142c187 <__netif_receive_skb+1703>\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../net/core/dev.c: 2719\n0xffffffff8142bb26 <__netif_receive_skb+70>:    testb  $0x10,0xb9(%rbx)\n0xffffffff8142bb2d <__netif_receive_skb+77>:    jne    0xffffffff8142bf90 <__netif_receive_skb+1200>\n/usr/src/debug/kernel-2.6.32-220.7.1.el6/.../include/linux/netpoll.h: 86\n0xffffffff8142bb33 <__netif_receive_skb+83>:    mov    0x20(%rbx),%r12\n0xffffffff8142bb37 <__netif_receive_skb+87>:    mov    %r12,-0x38(%rbp)\n0xffffffff8142bb3b <__netif_receive_skb+91>:    lea    0x60(%r12),%rax\n0xffffffff8142bb40 <__netif_receive_skb+96>:    cmp    %rax,0x60(%r12) WOW! Lots of output! What you’re looking at is the full list of steps that took place, starting with the method that was invoked and ending with the fault. If you look at the last line of the output, you should recognise two key things: 0xffffffff8142bb40 <__netif_receive_skb+96>:    cmp    %rax,0x60(%r12) The memory address is the one we peeked The number 96 is the decimal offset in the memory where the fault occurred. A few lines above this, we see a new filename and line number mentioned, include/linux/netpoll.h:86 . Congratulations! This is the actual line which caused the NULL pointer dereference! But what is it actually DOING? cmp    %rax,0x60(%r12) Let’s examine this bit. In assembly, cmp is called to compare two registers. %rax and %r12 are two CPU registers which we want to compare. If we got back to our log output, we see the registers: RAX: 0000000000000060 RBX: ffff8805353896c0 RCX: 0000000000000000\nRDX: ffff88053e8c3380 RSI: 0000000000000286 RDI: ffff8805353896c0\nRBP: ffff88034ac83e10 R08: 00000000000000c3 R09: 0000000000000000\nR10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000 RAX has some data in it, but R12 is NULL! Now take a look at code that was mentioned earlier: 84 static inline int netpoll_receive_skb(struct sk_buff *skb)\n 85 {\n 86         if (!list_empty(&skb->dev->napi_list))\n 87                 return netpoll_rx(skb);\n 88         return 0;\n 89 } If you know linux networking internals, a few things can be understood from this: We’re dealing with New API (NAPI) code, based on the variable name napi_list NAPI is invoked when the system thinks there are a large number of interrupts being handled, and changing to RX POLLING would be more efficient sk_buff is the struct that holds data about linux sockets We’re checking if the list, &skb->dev->napi_list is empty, so we’re expecting it to be defined, and be of the right type. What is probably going on here, is that expecting &skb->dev->napi_list to hold a list of incoming packets that need to be processed. Unfortunately, it turns out that something, most likely napi_list , is not set, causing the NULL pointed dereference. Congratulations! You’ve found the problem. Finding what the cause of this is, however, will be done in a future tutorial. In the meantime we have some options for workarounds: Try and test if napi_list is NULL just before the !list_empty check, and define it. This might have other unforeseen issues though – what if it causes a memory leak? It’s certainly not fixing the cause of the problem. Disable the use of NAPI by recompiling the network driver. This is a pretty simple operation and probably worth trying, especially if the system isn’t massively CPU bound and dropping packets. We ended up going with option 2, as it was the fastest and most likely fix. But we’ll still be sleeping with one eye open… Posted by Avleen Vig on March 30, 2012 Category: engineering , operations", "date": "2012-03-30,"},
{"website": "Etsy", "title": "Come have drinks with Etsy and Basho", "author": ["John Goulah"], "link": "https://codeascraft.com/2012/03/16/come-have-drinks-with-etsy-and-basho/", "abstract": "Come have drinks with Etsy and Basho Posted by John Goulah on March 16, 2012 We’ll be doing a drinkup next week with our friends at Basho , the folks that invented Riak . We’ll be hanging out a ReBar in downtown Dumbo on Tuesday, March 20th. Come have a drink with us! Some more details here: http://basho.com/blog/technical/2012/03/16/Drinkup-with-Etsy-on-March-20/ Posted by John Goulah on March 16, 2012 Category: events", "date": "2012-03-16,"},
{"website": "Etsy", "title": "Making it Virtually Easy to Deploy on Day One", "author": ["John Goulah"], "link": "https://codeascraft.com/2012/03/13/making-it-virtually-easy-to-deploy-on-day-one/", "abstract": "Making it Virtually Easy to Deploy on Day One Posted by John Goulah on March 13, 2012 At Etsy we have one hard and fast rule for new Engineers on their first day: deploy to production. We’ve talked a lot in the past about our deployment , metrics , and testing processes . But how does the development environment facilitate someone coming in on day one and contributing something that takes them through the steps of committing code, running it through our tests, and deploying it with deployinator ? A new engineer’s first task is to snap a photo using our in house photo booth ( handmade of course ) and upload it to the about page. Everyone gets a shiny new virtual machine with a working development version of the site, along with their LDAP credentials, github write access, and laptop. We use an internal cloud system for the VM’s, mostly because it was the most fun thing to build, but also gives us the advantage of our fast internal network and dedicated hardware. The goal is a consistent environment that mirrors production as closely as possible. So what is the simplest way to build something like this in house? We went with a KVM/QEMU based solution which allows for native virtualization. As an example of how you may go about building an internal cloud, here’s a little bit about our hardware setup. The hypervisor runs on HP DL380 G7 servers that provide us with a total 72G RAM and 24 cores per machine. We provision 11 guests per server, which allows each VM 2 CPU cores, 5G RAM, and a 40G hard drive. Libvirt supports live migrations across non-shared storage ( in QEMU 0.12.2+ ) with zero downtime which makes it easy to allocate and balance VM’s across hosts if adjustments need to be made throughout the pool. We create CentOS based VM’s from a disk template that is maintained via Openstack Glance , which is a tool that provides services for discovering, registering, and retrieving virtual images. The most recent version of the disk images are kept in sync via glance, and exist locally on each server for use in the creation of a new VM. This is faster than trying to pull the image over the network on creation or building it from scratch using Kickstart like we do in production. The image itself may have been kickstarted to match our production baseline, and we template a few key files such as the network and hosts information which is substituted on creation, but in the end the template is just a disk image file that we copy and reuse. The VM creation process involves pushing a button on an internal web page that executes a series of steps. Similar to our one button deployment system, this allows us to iterate on the underlying system without disruption to the overall process. The web form only requires a username which must be valid in LDAP so that the user can later login. From there the process is logged such that it that provides realtime feedback to the browser via websockets . The first thing that happens is we find a valid IP in the subnet range, and we use nsupdate to add the DNS information about the VM. We then make a copy of the disk template which serves as the new VM image and use virt-install to provision the new machine. Knife bootstrap is then kicked off which does the rest of the VM initialization using chef. Chef is responsible for getting the machine in a working state, configuring it so that it is running the same version of libraries and services as the other VM’s, and getting a checkout of the running website. Chef is a really important part of managing all of the systems at Etsy, and we use chef environments to maintain similar cookbooks between development and production. It is extremely important that development does not drift from production in its configuration. It also makes it much easier to roll out new module dependencies or software version updates. The environment automatically stays in sync with the code and is a prime way to avoid strange bugs when moving changes from development to production. It allows for a good balance between us keeping things centralized, controlled, and in a known-state in addition to giving the developers flexibility over what they need to do. At this point the virtual machine is functional, and the website on it can be loaded using the DNS hostname we just created. Our various tools can immediately be run from the new VM, such as the try server , which is a cluster of around 60 LXC based instances that spawn tests in parallel on your upcoming patch. Given this ability to modify and test the code easily, the only thing left is to overcome any fear of deployment by hopping in line and releasing those changes to the world. Engineers can be productive from day one due to our ability to quickly create a consistent environment to write code in. Posted by John Goulah on March 13, 2012 Category: engineering , infrastructure Tags: chef , deployment , first day , KVM , libvirt , QEMU , virtualization Related Posts Posted by Ryan Frantz on 16 Oct, 2013 Migrating to Chef 11", "date": "2012-03-13,"},
{"website": "Etsy", "title": "Going to SxSW? Checkout All Girl* Dev Brunch 2012", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2012/03/07/going-to-sxsw-checkout-all-girl-dev-brunch-2012/", "abstract": "Going to SxSW? Checkout All Girl* Dev Brunch 2012 Posted by Kellan Elliott-McCrea on March 7, 2012 Next Monday, Garann Means, author of Node for Front-End Developers: Writing Server-Side JavaScript Applications , and Etsy’s newest engineer, is hosting All Girl* Dev Brunch 2012 . If you’re a female developer or someone who wants to get more female developers involved in your organization, Austin All-Girl Hack Night and Girl Develop It invite you to come have brunch with us, talk about dev, and hopefully make some new friends! We’ll be serving a full breakfast,  including coffee and breakfast cocktails, thanks to our gracious sponsors: Etsy, Bocoup, spire.io, Headspring, and TEKsystems. Also, the RSVP is a shell , and invitation is executable, so save yourself the pain of trying to cat it like me. Badge optional, RSVP mandatory, Unix skills strongly suggested to get through the RSVP, a willingness to drink breakfast cocktails recommended. Posted by Kellan Elliott-McCrea on March 7, 2012 Category: Uncategorized", "date": "2012-03-7,"},
{"website": "Etsy", "title": "Scaling CI at Etsy: Divide and Concur, Revisited", "author": ["Laura Beth Denker"], "link": "https://codeascraft.com/2012/03/12/scaling-ci-at-etsy-divide-and-concur-revisited/", "abstract": "Scaling CI at Etsy: Divide and Concur, Revisited Posted by Laura Beth Denker on March 12, 2012 In a past post, Divide and Concur , we told you how we approached dividing our large test suite into smaller test suites by keeping similar tests together rather than arbitrarily dividing. Dividing tests by common points of error made triaging failures systemic failures quick, and enticed everyone to write faster, more deterministic tests, but not all was perfect. Our Jenkins dashboard was quite verbose. The numerous jobs on our dashboard were great for pinpointing where the failures were, but it was difficult to determine at which stage of the deploy pipeline the failures existed. Some tests were executed on every commit. Some tests were executed when the QA button was pushed. Some tests were executed against a freshly pushed Princess or Production build. We were using the Jenkins IRC plugin, and the number of messages per hour was drowning out necessary communication in the #push channel. We needed some way to communicate the test status at each stage of the deployment pipeline. We considered using Downstream Jobs , but fingerprinting was awkward and difficult to set up, and all-in-all it wasn’t quite what we were looking for. We also considered Matrix Jobs , but a Matrix Job is designed to execute several jobs with parameter(s) varied along configuration vector(s), i.e. build node, operating system, browser, arbitrary parameter, etc. This was not a fit for the purpose because our jobs had wildly different configurations that could not be coerced into mere parameter differences. What we needed was a way to create a Jenkins job type that would execute a selection of arbitrary Jenkins jobs, wait for the jobs to finish, and report a single result while still making it possible to drill down to sub-jobs to determine the sources of failures. So we wrote a Jenkins plugin to achieve this, the Jenkins Master Project Plugin . Now our Jenkins dashboard represents the deployment pipeline: When a stage turns red (or yellow), you can click through to that particular Master Build, see what tests failed and drill through the results (or even rebuild). We also wrote a Triggering User Plugin for determining the user who triggered the build and a Deployinator Plugin to link key Deployinator information to particular Jenkins build. The Triggering User and Master Project plugins are both integral to our latest version of Try . We have also made our Nagios plugin for Jenkins readily available on the Etsy GitHub account. We used this for experimenting with alerting on the health of Jenkins. All of these plugins are freely available on GitHub under the Etsy organization. Enjoy! Posted by Laura Beth Denker on March 12, 2012 Category: engineering , infrastructure Tags: continuous deployment , continuous integration , github , jenkins , open source , testing Related Posts Posted by Sasha Friedenberg on 15 May, 2017 How Etsy Ships Apps Posted by Jayson Paul on 20 Feb, 2015 Re-Introducing Deployinator, now as a gem! Posted by Rasmus Lerdorf on 01 Jul, 2013 Atomic deploys at Etsy", "date": "2012-03-12,"},
{"website": "Etsy", "title": "The Etsy Way", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2012/02/13/the-etsy-way/", "abstract": "The Etsy Way Posted by Chad Dickerson on February 13, 2012 As you might imagine, we at Etsy get a lot of “can I pick your brain?” requests about how we do things at Etsy, or what we’ll call here The Etsy Way.  While we take these requests as huge compliments to the work we do, we have to be somewhat protective of the team’s time.   We’re proud of what we’ve been doing and believe in sharing it, so we’ve invested hundreds (if not thousands) of hours into providing public information on this blog and elsewhere.  This is the best way to scale our sharing as broadly as possible. (And we’ll still meet with some people — we’ll just ask that you read everything below first since we’ve worked so hard on it!)  Consider this post that first friendly conversation over coffee. The most important component of The Etsy Way is culture and that is as difficult to teach as it is important.  To get a sense of how we think about culture, take a look at Optimizing for Developer Happiness , which includes a 24-minute video of a talk I did and a link to the accompanying slides . Here are a few more links about culture: Scaling startups How does Etsy manage development and operations? Code as Craft: Building a Strong Engineering Culture at Etsy (slides) With the culture bits explained, below are a few other key posts in the Etsy canon.  All of these are inter-related with the culture, of course, and help reinforce it (remember it’s all about culture .  Did someone say “culture”?): Quantum of Deployment (Erik Kastner).  We deployed code to production more than 10,000 times in 2011.  If you wonder “how did they do that?” this post will tell you all you need to know. Track every release (Mike Brittain). Here, we write about the methods we use to track the success of every code deploy with application metrics.  This is part of the not-so-secret sauce. Measure Anything, Measure Everything (Ian Malpass).  We introduce you to StatsD, the open source software we built at Etsy to enable obsessive tracking of application metrics and just about anything else in your environment.  The best part is you can download StatsD yourself and try it out. Divide and Concur (Noah Sussman and Laura Beth Denker).  By reading this post, you’ll learn about all the inner workings of our automated testing setup: what software we use (with plenty of links), how we set it up, and the philosophy behind it all. We also have tons of slides from talks we have done, all available in the Code as Craft group on Slideshare . And last but not least, we have an Etsy Github repository with lots of goodies. Pretty much everything we write about above is open source (even the culture) so the motivated reader will find links to tips and actual software along the way to actually set things up on his/her own.  If there’s anything you’d like to know more about The Etsy Way, just let us know in the comments.  We’ll add it if we have it, and probably write it if we don’t. As you can tell, a really important part of The Etsy Way is encouraging people on the team to contribute to open source , write informative and entertaining blog posts , and put together killer presentations .  If you want to join the fun, we’re always hiring . Posted by Chad Dickerson on February 13, 2012 Category: philosophy", "date": "2012-02-13,"},
{"website": "Etsy", "title": "Did you ‘try’ it before you committed?", "author": ["Laura Beth Denker"], "link": "https://codeascraft.com/2011/10/11/did-you-try-it-before-you-committed/", "abstract": "Did you ‘try’ it before you committed? Posted by Laura Beth Denker on October 11, 2011 At Etsy, we deploy often, from head, and everything that is committed is to trunk and must be ready for production immediately.  This makes it very important to test your code before committing; otherwise, you will be holding up everyone else from committing and deploying since trunk always needs to be clean. Over a year ago, everyone had at least one common gripe:  You could not run the entire test suite on a developer virtual machine (VM), and if you thought you could run the entire suite on a developer VM, then you thought it would take at least 3 hours. Fortunately, on the Continuous Integration (CI) cluster the tests would take roughly 30 minutes.  But really, the chances that the tests would pass were so low that ‘rebuilding reds’ (aka rebuilding a test failure to make sure it was not just a flaky test) and re-running the test once there was a ‘fix’ would actually turn that 30 minutes into an hour, hour and a half, two hours, or more. Why were the chances of the tests passing so low on CI? because almost no one would ever run a full test suite before the integration step. The problem was: developers could not run tests in a reasonable amount of time with the resources they were given, so they would not run the tests until they had access to the shared resources that had the ‘Oomph’ to run the tests in a more reasonable time. So one day, someone told one of the developers, that he could test on one of the behemoths in the CI cluster.  That developer would SSH into the machine.  Edit some code.  Run the tests on the much more robust hardware, and PROFIT!  But then came along another developer.  He began to do the same on the machine.  Then another came, and most likely a couple more.  There were maybe a handful of developers, and the machine was spec-ed high enough to handle the load, but that was not the issue. The problem was: developers were testing, but they had no way to orchestrate who was using the machine at the time.  Also, the manner in which the test suite was architected involved a lot of shared fixtures which would inevitably cause collusions during concurrent test runs.  Sigh… While this situation was brewing, I was working with one developer on how to get his changes onto the machine in the CI cluster.  We figured out what options we needed for svn diff to make the patch file that we wanted.  Then we figured out which options we needed for patch to patch in the patch file we wanted to the svn working copy on the CI machine.  Eureka! Here’s the simple solution to allowing the developers to utilize the awesome resources in the CI cluster without stepping on each other’s toes: Create a new Jenkins Freestyle Project (or copy an existing) and Select Parameterized Build File Parameter: patch.diff String Parameter: username Set up the SCM as usual Add an Execute Shell build step: Apply the patch.diff Use $username in the recipient list of the e-mail publisher Write a short bash script that Creates a patch Sends a cURL request with the patch and $USERNAME to start a build of the Jenkins job The original script looked something like this: #!/bin/bash\n\n    HUDSON=\"ci.example.com\"\n    LOCATION=\"/home/$USER/working_copy\"\n    PATCH='patch.diff'\n    cd $LOCATION\n    svn diff > $PATCH\n\n    file_param=\"{'name': 'patch.diff', 'file': 'file0'}\"\n    user_param=\"{'name': 'executor', 'value': '$USER'}\"\n    args=(\"$@\")\n    for ((i=0;i<${#args[@]};i++)); do\n      curl -F file0=@$LOCATION/$PATCH \n           -F json=\"{'parameter': [$file_param, $user_param]}\" \n           -F Submit=Build http://$HUDSON/job/try-${args[i]}/build\n    done We called this new service try .  The day we introduced try to the team, the number of deploys went from maybe a handful a day to more than 20 deploys a day, and we have not really looked back. Every new deployer at Etsy, deploys code on his or her first day, and each one of them is told by someone, “Make sure you use try before you commit.” try has evolved with the rest the rest of our CI infrastructure.  If you read Divide and Concur , you can probably imagine how we handle so many test jobs. try has also been our guinea pig for integrating Jenkins and Deployinator .  We will save the details of all of this for another post. In the meantime, please take a gander at other ‘try’ implementations: https://wiki.mozilla.org/ReleaseEngineering/TryServer http://buildbot.net/buildbot/docs/0.8.4/try.html#try Posted by Laura Beth Denker on October 11, 2011 Category: engineering , infrastructure", "date": "2011-10-11,"},
{"website": "Etsy", "title": "Etsy at Photo Hack Day!", "author": ["stunji"], "link": "https://codeascraft.com/2011/08/15/etsy-at-photo-hack-day/", "abstract": "Etsy at Photo Hack Day! Posted by stunji on August 15, 2011 Our friends at Aviary are organizing a ginormous Photo Hack Day this Saturday the 20th, and we’re psyched to be there! What makes this hack day so amazing? The winning hack will be featured on the NASDAQ billboard in Times Square.  Seriously! This is a photo-themed hack day, so we’re looking for hacks that do interesting things with Etsy listing images. Things like: Snap a picture of your outfit with a mobile phone, and use Etsy’s API to find accessories in matching colors. Use the Face.com API to identify models in Etsy clothing listings, then replace with your own face to try on the clothing. Import your Instagram photos for use in your own Etsy listings. The hacking goes down at General Assembly in midtown Manhattan. We’ll be there, and we’ll be holding office hours to help hackers with our API, PHP, jQuery and OAuth. We’re also offering this robot sculpture from HeavyMetalMilkman as a prize for the best Etsy API hack! If you need help hacking, we’ll be hanging out in #etsyapi on freenode, on Twitter @etsyapi , and available via email at developer@etsy.com . Space is limited, so sign up now ! Posted by stunji on August 15, 2011 Category: api , events", "date": "2011-08-15,"},
{"website": "Etsy", "title": "Passing the engineering torch", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/07/31/passing-the-engineering-torch/", "abstract": "Passing the engineering torch Posted by Chad Dickerson on July 31, 2011 Kellan Elliott-McCrea, Chad Dickerson, and John Allspaw (left to right).  Photo by Pascal Perich. Last week, I stepped into the CEO role here at Etsy .  This has been keeping me really busy, but I wanted to post a final note here.  Back when I first introduced Code as Craft , I had no idea where it would take us.  I just wanted to give other engineers a glimpse into the personalities behind what we do in engineering at Etsy, and share our experiences with the world.  One of the great pleasures of any leader is watching individuals grow and learn, and watching Etsy engineers express themselves on this blog has been a lot of fun.  I hope it has been as fun for them (and you, the readers) as it has been for me. As I’m stepping up to other things, I’m happy to say that we’ve got things covered — more than covered — on the engineering team.  Kellan Elliott-McCrea is now Etsy’s CTO.  When Kellan joined, I welcomed him on this blog and noted that he is “a person who is abnormally focused on getting things done.”  Nothing could be more true.  In barely a year at Etsy, Kellan has driven the “moving fast at scale” ethos and has been the catalyst in building our “just ship” culture.  He even runs an Etsy shop with “just ship” t-shirts (sorry, they are nearly always sold out!  Kellan, please “just ship” some more of these t-shirts!) John Allspaw is moving up, too, to the role of SVP of Technical Operations.  When John joined, I wrote about how awesome he is , and anyone who has worked with John is a better engineer and person for it.  I think that John’s work around human factors and resilience engineering as they relate to web operations will be referred to for years to come in our industry.  John will continue to lead the operations team, but he’ll also be helping make Etsy a better company overall by helping me drive collaboration across all teams at Etsy.  John is an incredible mentor, and he’ll be working with Kellan, our HR team, and engineering managers on providing clear goals and career paths for engineers in the company. As we move to the next chapter at Etsy, Kellan and John will function as partners (as they have been up to this point), and each will report to me.  As a CTO-turned-CEO, I fully appreciate the value of their ongoing counsel as my duties expand.  Continuing to build and nurture an awesome engineering culture will continue to be a top priority for me as CEO (and, of course, we are always hiring !) I couldn’t be more confident and excited to have Kellan and John leading the engineering team at Etsy.  I wish them the best of luck in their new roles and look forward to creating a new future at Etsy with them alongside me. Posted by Chad Dickerson on July 31, 2011 Category: engineering , people", "date": "2011-07-31,"},
{"website": "Etsy", "title": "We are at OSCON!", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2011/07/25/we-are-at-oscon/", "abstract": "We are at OSCON! Posted by Kellan Elliott-McCrea on July 25, 2011 Generosity of spirit is a core Etsy Engineering principle — as a team we expect each of us to speak in public, release open source software , lead classes, write posts here on Code As Craft and/or publish research papers. In that spirit, we’re at OSCON in force this year: Ephemeral Hadoop Clusters in the Cloud Today, in just a few hours, Greg Fodor is covering the infrastructure our Data Wranglers team has built to support building data driven products on Elastic Map Reduce. Put a Button on It: Removing Barriers to Going Fast John Goulah, and Erik Kastner will be talking about the process, people, and tools that we use to power our continuous deployment culture, and open sourcing our deployment tool, Deployinator. Building an A/B Testing Framework for Web Applications Zhi-Da Zhong will be talking about our A/B framework we use to make data driven decisions, and power the ramp ups that support the “branching in code” aspect of how we work. Come talk to us! In addition to Greg, John, Erik, and Zhi-Da, we’ll have a few more engineers on the floor. And please make sure to find and talk to Tatiana, who is THE person to find out more about what it’s like working at Etsy. (and also the best person to get you in the door) Posted by Kellan Elliott-McCrea on July 25, 2011 Category: Uncategorized", "date": "2011-07-25,"},
{"website": "Etsy", "title": "Hacking Hardware with Etsy", "author": ["stunji"], "link": "https://codeascraft.com/2011/07/26/hacking-hardware-with-etsy/", "abstract": "Hacking Hardware with Etsy Posted by stunji on July 26, 2011 UPDATE: We were blown away by the response at Maker Faire Detroit, with over 800 people playing Whack-A-Treasury!  We also received three Editor’s Choice ribbons from the editors of Make: magazine! Next stop: World Maker Faire in Queens September 17 and 18. In the meantime, we’ve put our source code on GitHub .  Check it out! Maker Faire, a festival devoted to supporting the DIY movement in tools, toys, and art, is traveling to three North American cities this year— and Etsy is along for the ride! Celebrating the joy of making and supporting those who make is what Etsy’s all about. Partnering with Maker Faire allowed us a great opportunity to connect with this community in person. We wanted to build something interactive to attract visitors and allow them to interact with Etsy in a way they never had before, and our API gave us the opportunity to work with data from our site and build something physical. Our API gives access to Etsy listings and Etsy Treasuries— the member-curated item collections that appear on the Etsy homepage. Voilà! Visitors to our booth could create Treasuries remotely using our API and a handmade old-time carnival game. Enter Whack-A-Treasury ! If you’ve played Whack-a-Mole, you know the drill— a mechanical mole pops up from a board, and you have only a second to whack it on the head. With this as our basic inspiration, we built a game that lets each player choose a theme, pulls in Etsy items whose keywords match that theme, and displays the item photo on an oversized monitor while the player hits a game pad in the right spot to grab the listing. If the player racks up enough items to fill an Etsy treasury, they win! Gameplay The player stands before a 2-by-3-foot game pad with a flatscreen monitor at eye level. In her hands, she holds a giant sparkly foam mallet, handmade in our Brooklyn office. The player gets to chose her theme and skill level by whacking one of the game pad’s six squares. Then, the game begins! Listings flash by on the screen as quickly as once every half-second. At the same time, one of the game pad’s six colored squares lights up, and the player must hit it before the listings disappears. If the player whacks 12 listings before the game’s 30-second time limit is up, she’s a winner! Gongs sound. Lights flash. The player gets a custom-printed sticker with a URL to the treasury she just created. Some players check it out right then and there. The winners also get their choice of a lollipop or a DIY plush robot kit (supplied by our friends at Spoonflower .) Good times! How We Did It We’ll be presenting this project on the Make: Live stage at Maker Faire Detroit on Sunday, July 31 from 12:30 PM – 1:00. If you want in-depth technical details, including how to use the Etsy API, how to wire components up to your Arduino, and how talk to your Arduino with Processing, come by and see us! The game runs on a single laptop and has four major elements: a local MySQL database, a Processing sketch that handles the display, a physical game pad, powered by an Arduino microcontroller, that registers hits, and a PHP script that prints out stickers for the lucky winners. We chose six themes for players to select: owls, cowls, kittens, steam punk, mustaches, and robots. We chose these specific themes because they all provided fun, eye-catching search results. Then, we wrote a short script in PHP that searches for hundreds of listings that match themes themes, downloads their photos, and stores them in a MySQL database. The images are resized so that they fit nicely on the game screen. The game pad is basically a giant pillow, sewn from vinyl fabric and stuffed with squishy foam. The pad itself is pressure-sensitive— it knows where you hit it. There are six force sensors embedded under its sparkly surface, plugged into an Arduino microcontroller. The Arduino sends and receives serial data, which the Processing app uses to make sure that hitting the pad lets the player “win” the round. (And, just for sheer carnival goodness, the Arduino also drives a lighting controller that lights up the pad in just the right spot each time.) Then, we built the game using Processing, an open-source programming language for graphics and animation. The Processing sketch draws the listings on the screen, chooses which of the game pad’s six squares to light up, and waits for the Arduino to tell it which squares on the game pad have been hit. Processing also keeps track of the player’s score, and saves the winning game results. Finally, a short PHP script waits for winning games, and prints out stickers for the winners to take. The sticker contains a QR code that can be scanned with a mobile phone. This displays the winning game results, and allows the player to log into Etsy and save the Treasury to her member account. The Results All told, nearly 500 people stopped by to play at Maker Faire San Mateo . Detroit, it’s your turn now! We’ll be at Maker Faire Detroit on July 30 and 31st with our fantastic local street team, Metro Detroit Etsy . Can you top 500 players in a weekend, Detroit? Come out, whack your very own Treasury, and see what else you can build at Maker Faire! (New Yorkers get their shot during World Maker Faire September 17 and 18th in Flushing, Queens.) Credits We made all of this in our Brooklyn office and in the workspace at NYC Resistor . Ari and Justin wrote the Processing script. Ari also worked with Eric to build the pressure-sensitive pad, wire the lights and build the Arduino circuit. The game pad and mallet were sewn by Claire , Julie , and Kimm . Hanna built the table, and Jay rocked the graphic design. Handmade all the way by many hands! Posted by stunji on July 26, 2011 Category: api , events", "date": "2011-07-26,"},
{"website": "Etsy", "title": "Moving from SVN to Git in 1,000 easy steps!", "author": ["John Goulah"], "link": "https://codeascraft.com/2011/12/02/moving-from-svn-to-git-in-1000-easy-steps/", "abstract": "Moving from SVN to Git in 1,000 easy steps! Posted by John Goulah on December 2, 2011 This past summer we completed a project that spanned several months of planning and preparation – moving our source control from Subversion to Git. The code that runs our search engine, front-end web stack, support/admin tools, API, configuration management, and more are now stored in and deployed from Git. We thought some of you might find our approach migrating an 80-100 person engineering team interesting and possibly instructive. We went through three phases: Preparation Execution Follow through Preparation was the longest and most difficult phase. We dealt with figuring out when and how to move, how to educate our team, and making it a smooth transition for everyone. The execution phase had to be done quickly, because at the rate we are committing and releasing changes it would be counter-productive for the cutover to take more than a few hours. We spent a lot of the time in the preparation phase making sure that was possible. The follow through phase refers to supporting our team from the point we cutover and into the future. This post isn’t prescriptive, but before getting into details here is our only piece of advice: If you can deal with your current source code system, do not go through this pain. Seriously. This was a long, painful process for us. Over the years, many tools, systems, and processes had become deeply intertwined with our subversion installation. That said, if your team is small, or your source control system isn’t tied into anything, go for it! Just do it as soon as possible – the only time better than today was yesterday. Preparation Moving to Git is something we’ve been talking about for at least 2 years. It’s also something we put off for a lot of very good reasons. Around that time we had been introducing the culture of continuous deployment, which included the mind shift of moving away from long lived branches, and instead branching in code with feature flags, making small frequent deploys, and using percentage rampups to slowly roll out features. At the time we didn’t want to introduce any other road blocks to instilling this into our engineering culture. What we found happening more and more was that new engineers were coming in already familiar with Git, using things like git-svn and writing tools to make SVN act more like Git. While evaluating the options, it was clear that for our team, Git was a better fit than SVN (and a better fit than any other distributed version control system as well). One of the biggest reasons is github.com and its popularity for open source collaboration. Not only do we put our open source contributions on there, but so do Twitter, Facebook and many others. Though we did not move to Git for its branching capability, our tools weren’t capturing some of the work we were already doing with patches and pushing changes directly between team members for review and testing. We also felt that re-examining and adding new tools to the mix seemed like a healthy trait to have in our culture, and felt the switch to Git would increase engineer happiness . After we committed to the decision, we handled the move to Git slowly and delicately for a few reasons. One is that we deploy around 30 times a day across an engineering organization that was about 80 people at the time. We didn’t want to lose any of that velocity (we knew we might lose some in the beginning, but wanted it to be as seamless as possible). Another was that we had a varying range of Git familiarity across the team. From Git experts to people who had never touched it. Education played a huge part in our successful transition. It was also important for us to continue the use of flagging code on/off and having a continuously deployed trunk mentality even after the switch to git. Prep The first few months of prep consisted of slowly reorganizing our SVN repositories to be more in line with how we were working. Our code had become spread across many different repositories, and we wanted to make sure that when you were ready to work on the main website, that you only needed to clone one repository. This was a good thing for us even if we hadn’t moved to Git, because it introduced a logical organization that was more in line with how our site was laid out. We were also starting to decide which tools we could use around Git. We wanted a front end tool with a good UI, so naturally we contacted GitHub for a trial version of github enterprise . At the same time we tried out some tools such as gitweb with something like gitosis or gitolite underneath. We also took a look at gitorious which is probably the next closest thing to GitHub if you are looking for a free solution. Education We started to look around for training, and it became obvious that github’s training program is the best out there. We wanted to make sure everyone was well prepared, and they offer a training that you can go through online, or have the instructors come to you for a more hands on experience. We also wanted to examine our workflow and integration issues, and found that bringing an instructor on site was the best use of our time. Since we were training people with various skill levels, we decided to split the training into separate sessions based on experience. We surveyed our team, and grouped people into beginner and intermediate buckets. We broke about 3/4 of our team across two beginner days, and the rest in a more advanced session on the last day. We also spent some time after the training each day to discuss our workflow and the integration with our current tooling. Our instructor, Matthew McCullough , couldn’t have been better in explaining git in a sensible way to the team. As a bonus, since we already had github enterprise installed, we were able to use that for the hands on training to get people acclimated to using it, and by the end of the sessions people were creating and hacking on their own repositories in our private installation. It quickly became apparent that this was going to be a great tool for collaboration, with a fantastic UI and all the benefits of the public GitHub, while maintaining the privacy of our codebase that we required. Pre Migration After the training, we had to attempt to move fairly quickly so that all that was learned in training was not forgotten. The best way to learn a tool is to use it, and we had to plan how to carry out the actual migration. There were a few key things we did at this stage. First, we created an Engineering organization within our GitHub, and created a repository that held our web code. We then created a cron that mirrored our SVN commits into the GitHub repo. We were able to use that to update our deploy and testing tools in the background without affecting our current flow. We also created documents in our wiki that described our workflow, including an explanation of how to do similar tasks that one would do in SVN, with Git. We made it clear with a few weeks notice of our plans for the move so that everyone was mentally prepared for the switch, and even did a few in house training sessions specifically on our new workflow. In the end, we kept our workflow similar to SVN to ease our transition. We still don’t use branches (most of the time), we still deploy from trunk (…well, master). Execution The next step was actually flipping the switch. At this point we’d done so much preparation that we were just ready to make this happen and get it over with. We had a code freeze (no commits) one evening in late June, and migrated our deployment and testing tools to use the new Git repo. Our commits were already mirrored into Git, so the new repository was up to date. But we also had to be sure the Git repository was getting chef ‘d out to each developer’s VM, with the web configuration in place to have engineers sit down the next day and be ready to code and deploy. We had to make sure that our hooks were working, and that our commit emails and IRC notifications were uninterrupted. All in all the code freeze lasted about 12 hours, and we were ready to go for the next day. Follow Through As part of our preparation, we made sure to identify some of the members across the team who were key in helping assist others with the transition to this new tool. It certainly helps to have a few people on hand that are familiar with Git and its distributed model to help people get acclimated. We set up a #git IRC channel (we use IRC across the entire company for communication) and we also had our documentation to point to, which people were able to add to if they encountered any new problems or needed clarification with the new workflow. The first day on Git our velocity was above average – we wanted to make sure everyone was comfortable and able to work in this new system, and the migration didn’t end up slowing us down at all. We stated that everyone had to clone-pull-add-commit-push on that first day so that there was no getting lost for weeks. In our opinion this was one of the things that was the most successful aspects of the move. Just like we have people deploy on their first day here, overcoming the fear is a big part of adapting to a new process. Summary Overall we can say the Git migration was a success. It turned out to be an immense task with a maze of dependencies, but in the end we’re on a current version control system that should last for years to come. It opens up many new workflow possibilites and solves some of our existing problems, not to mention it’s blazing fast. If you’re more interested in the technical instead of the social migration of SVN to Git, I wrote a blog post a few years ago on my personal blog that you may be interested in, and there’s also a couple of pointers over on github on how to make the conversion. Posted by John Goulah on December 2, 2011 Category: infrastructure", "date": "2011-12-2,"},
{"website": "Etsy", "title": "Shop Centricity – A Case Study", "author": ["Steve Mardenfeld"], "link": "https://codeascraft.com/2011/07/08/shop-centricity-a-case-study/", "abstract": "Shop Centricity – A Case Study Posted by Steve Mardenfeld on July 8, 2011 Here at Etsy, we’re constantly asking questions about our data: questions about our customers, questions about the items in our marketplace, questions about what works and what doesn’t.  As data analysts, we seek to answer these questions that help guide intelligent and informed decisions to make our marketplace better.  Today’s marketplace at Etsy contains over 9 million items represented by around 800 thousand sellers. Twenty five million visitors from over 150 countries come to Etsy each month, totaling multi-terabytes of data per month, and we use Hadoop and cascading.jruby to help us analyze this data at scale. In this post, we’ll be investigating the role of shops on Etsy. Like other online marketplaces such as eBay or Amazon’s marketplace, items are sold by individual shops.  If you’ve ever bought from these marketplaces, you should ask yourself if you remember the merchant you purchased from.  Compare your experience with that of buying this altered vintage plate from the shop BeatUpCreations who crafts modern portraits onto antique plates.  Etsy’s marketplace sells unique handmade goods crafted by unique individuals: here we explore the hypothesis that shops play a central role in the buying experience on Etsy. Let’s start with analyzing purchase behavior on Etsy.  Etsy’s checkout funnel should look very familiar: view an item, add it to your cart, buy it. To learn how visitors discover the items they ultimately purchase, let’s look at referrals to the listing page of the purchased item. Unsurprisingly, we see that some people find these items via browsing and bookmarking tools like search and favorites.  Some shoppers engage in conversations with a seller before finally buying an item: buyers can easily send sellers a private message from their shop page (something we call a “convo” on Etsy).  At 47.6%, the biggest faction of purchasers funnel into an item directly from a shop page. On Etsy, every seller has his or her own shop page (like BeatUpCreations’ shop page ). Within shop pages, merchandise is organized into different sections and visitors can browse available and sold items. Buyers who visit a shop see an average of almost three different items from that shop before going on to purchase an item. Just as every listing in Etsy’s marketplace is unique, every seller has a personal style. When buyers see an item they like, they click through to the item’s shop page where they can explore variations of the item (different sizes, etc.), or other items with similar styles, themes, or taste. Interestingly, we can see from the graph below that the leading referer to shop pages are in fact listing pages. Listings and shop interact in a mutual and symmetric manner: items are a discovery tool for shops, and shops are a discovery tool for items. By examining the flow of traffic that leads to a purchase on the site, we have learned how shops do in fact play a major role in item discovery at Etsy. In addition to seeking out that perfect item, visitors also tend to focus on finding people and shops which speak to their tastes. We believe that this is due to several factors — the unique nature of all of the products on our site, the emphasis on people and individuals, and a distinctness of style within shops. Analytics is more than just building dashboards and graphs — it’s asking thoughtful, insightful questions and harnessing a wide-array of tools to answer them. Whether it’s large-scale distributed tools like Cascading and Hadoop or more traditional ones like R and Matlab, it doesn’t matter, as long the job gets done. At Etsy, we not only believe in keeping our data around, but we believe in harnessing it — to gather insight, to learn from our mistakes, and to grow. Interested in learning more about analytics at Etsy — check out our jobs page as we’re looking for data engineering analysts and hadoop engineers . We’re also looking for a director of analytics to lead and build out the team! Posted by Steve Mardenfeld on July 8, 2011 Category: engineering", "date": "2011-07-8,"},
{"website": "Etsy", "title": "Object Oriented CSS with Nicole Sullivan: Thursday June 23", "author": ["Marcus Barczak"], "link": "https://codeascraft.com/2011/06/21/object-oriented-css-with-nicole-sullivan/", "abstract": "Object Oriented CSS with Nicole Sullivan: Thursday June 23 Posted by Marcus Barczak on June 21, 2011 In conjunction with the NY Web Performance Group we’re pleased to announce we’ll be playing host to Nicole Sullivan this Thursday, June 23rd in the Etsy Labs. Nicole will be presenting on CSS and its impact on front-end performance, as well as talking about her Open Source project Object-Oriented CSS , a new approach to scaling CSS. Nicole is a highly sought after front-end performance consultant and has worked closely with the W3C, Yahoo! and Facebook to help them improve their CSS code. If you’d like to attend sign up at the NY Web Performance Group’s meetup page . Posted by Marcus Barczak on June 21, 2011 Category: engineering , events , people", "date": "2011-06-21,"},
{"website": "Etsy", "title": "Optimizing for developer happiness", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/06/06/optimizing-for-developer-happiness/", "abstract": "Optimizing for developer happiness Posted by Chad Dickerson on June 6, 2011 A few weeks ago, I gave a talk at Railsconf in Baltimore about how we optimize for developer happiness at Etsy.  In the talk, I go into the philosophical reasons why continuous deployment makes engineers happy, how radically decentralizing authority and thinking of your team as a community optimize for happiness, and the how our approach to tooling makes everything work. Here’s the video: . . .and here are the slides . A big thanks to Ben Scofield and Chad Fowler for inviting me!  It was loads of fun putting the talk together and chatting with folks at the conference afterwards. A big tip of the hat to Charlie Chaplin , Peter Drucker , and Jane Jacobs for the inspiration of their work . Posted by Chad Dickerson on June 6, 2011 Category: engineering , philosophy , video", "date": "2011-06-6,"},
{"website": "Etsy", "title": "Pushing: Facebook, Flickr, Etsy", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2011/06/01/pushing-facebook-flickr-etsy/", "abstract": "Pushing: Facebook, Flickr, Etsy Posted by Kellan Elliott-McCrea on June 1, 2011 Around Etsy Engineering we enjoyed watching chuckr’s tech talk on how Facebook pushes , not the least because we love to see other folks doing no branch, feature flagged, frequently deployed coding. [vodpod id=Video.10007116&w=425&h=350&fv=] We also enjoyed it, because it was inspired by a talk given by John Allspaw, Etsy’s Head of Ops, and Paul Hammond of TypeKit when they were both still at Flickr, 10+ Deploys Per Day: Dev and Ops Cooperation at Flickr” . [vodpod id=Video.10007219&w=425&h=350&fv=] Also, that awesome grumpy old man, the original photo, copyright Akbar Simonse, is here: http://www.flickr.com/photos/simeon_barkas/2713588912/ And if you haven’t see it, we’ve got the slides , and video from our “Moving Fast at Scale” microconference up, talking about how Etsy thinks about pushing: [vodpod id=Video.10007338&w=425&h=350&fv=] (sorry about the audio kind of sucking on that last one) Posted by Kellan Elliott-McCrea on June 1, 2011 Category: video", "date": "2011-06-1,"},
{"website": "Etsy", "title": "Upcoming Etsy Engineering Talks", "author": ["Kellan Elliott-McCrea"], "link": "https://codeascraft.com/2011/04/01/upcoming-etsy-engineering-talks/", "abstract": "Upcoming Etsy Engineering Talks Posted by Kellan Elliott-McCrea on April 1, 2011 Laura Beth Denker will be speaking at PHPCon in Nashville, April 22nd where she asks the question “Is It Handmade Code If You Use Power Tools?”. Learn how Etsy is using, abusing, and extending PHPUnit, and Hudson/Jenkins to allow 40 deploys in a single day, passing a battery of 5000+ tests. Learn more . Gregg Donovan will be speaking at Lucene Revolution in San Francisco, May 26th about building out Solr to power Etsy’s vertical search product, including a number of custom extensions to Solr trunk (contributed back). Learn more. (early bird registration ends Monday) John Allspaw will be speaking at O’Reilly Velocity in Santa Clara, June 14th on “Advanced Postmortem Fu and Human Error 101” using real world examples to illustrate how to run (and possibly not to run) blameless postmortems a key practice of resilient, high performance organizations. Learn more . Posted by Kellan Elliott-McCrea on April 1, 2011 Category: engineering , events", "date": "2011-04-1,"},
{"website": "Etsy", "title": "Live stream of “Moving Fast at Scale”", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/03/31/live-stream-of-moving-fast-at-scale/", "abstract": "Live stream of “Moving Fast at Scale” Posted by Chad Dickerson on March 31, 2011 Hi folks. Here’s the recording of the live stream of our talks.  Jump to the one-minute mark for the actual beginning of the talk. Thanks to everyone for coming! Posted by Chad Dickerson on March 31, 2011 Category: events", "date": "2011-03-31,"},
{"website": "Etsy", "title": "Douglas Crockford at Etsy Speaker Series, April 27", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/03/23/douglas-crockford-at-etsy/", "abstract": "Douglas Crockford at Etsy Speaker Series, April 27 Posted by Chad Dickerson on March 23, 2011 April 25, 2:30pm EST: We just opened up 50 more slots for Wednesday night.  Sign up here ! Hidden under a huge steaming pile of good intentions and blunders is an elegant, expressive programming language.  JavaScript has good parts. – Douglas Crockford As we leap into the spring, we’re cranking up the Etsy Speaker Series at Etsy HQ in Brooklyn with a visit from Douglas Crockford, JavaScript guru and the person who “discovered” JSON .   His talk will be “JavaScript: The Good Parts,” and we’ll have copies of his book of the same name on hand for purchase (Chuck Norris will not be attending, but will be there in Crockford’s spirit .) For more details on Douglas’ talk and to sign up to attend look here . We have more awesome speakers coming up (see the schedule ), so sign up for our new mailing list if you want us to let you know! Posted by Chad Dickerson on March 23, 2011 Category: people", "date": "2011-03-23,"},
{"website": "Etsy", "title": "Moving Fast at Scale: the slides, and a reprise in NYC", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/03/19/moving-fast-at-scale-slides-and-reprise/", "abstract": "Moving Fast at Scale: the slides, and a reprise in NYC Posted by Chad Dickerson on March 19, 2011 Wow.  We had SO MUCH FUN doing the “Moving Fast at Scale” microconference in Austin for SXSW.  We got some really awesome feedback on the talks.  A huge thanks to the 200+ folks who showed up for the talks and to hang out with us on a beautiful spring afternoon in Austin.  Here’s a panorama of the scene (click for a larger version): A lot of folks asked us to record the talks in Austin and we weren’t quite able to get the logistics right in time to make that happen, but take heart — we’re going to do the microconference again on our home turf in NYC on Thursday, March 31 from 7-9pm , and we’ll be live streaming it and recording it for those of you who missed it.  If you’re local, sign up here to see the talks in person at Etsy HQ in Brooklyn.  The live stream will be at http://www.livestream.com/etsy and the recording will be appear there when we’re done (we’ll link to it here — no need to sign up in advance for that). Links to our slides from Austin are below.  These talks were very brisk by design — each was 20 minutes or less. Scaling Startups (Chad Dickerson, CTO) Continuous Deployment: The Why and the How (Kellan Elliott-McCrea, VP of Engineering) Deployinator: Turning THE FEAR into ‘the fear’ (Erik Kastner, Software Engineer, Developer Tools) Metrics-driven Development (Mike Brittain, Lead, Core Platform) We’re looking forward to doing it again.  See you at the event in Brooklyn! (or over the interwebs!) Posted by Chad Dickerson on March 19, 2011 Category: events , operations , people , philosophy", "date": "2011-03-19,"},
{"website": "Etsy", "title": "Divide and Concur", "author": ["Noah Sussman"], "link": "https://codeascraft.com/2011/04/20/divide-and-concur/", "abstract": "Divide and Concur Posted by Noah Sussman on April 20, 2011 By Noah Sussman and Laura Beth Denker. Splitting Up A Large Test Suite Lately we’ve been deploying code 25 times a day or more, and running automated tests every time we deploy.  Conservatively, we run our tests about 25 times on a normal business day. At Etsy, deployments are managed by engineers, not by ops or a release management team.  The idea is: you write code, and then you deploy it to production.  Even dogs deploy code. By the time 8am rolls around on a normal business day, 15 or so people and dogs are starting to queue up, all of them expecting to collectively deploy up to 25 changesets before the day is done. If 15 Engineers Deploy 25 Changesets In 24 Hours… Deploys generally take about 20 minutes.  Any longer than that and the people at the back of the queue can wind up waiting a really long time before they get to deploy.  In our world, “a really long time” means you waited two hours or more before you could make a production deployment. We call the tests that get run before deployment “trunk tests” because they test Etsy’s production functionality.  There are 7,000 trunk tests, and we’re adding more all the time.  In truth, we have more tests than that, but we don’t run all of them when we deploy (more on that in just a moment). If the trunk tests fail, deployment pauses while the engineers look for the source of the problem.  Usually this takes under 5 minutes and ends with someone sheepishly making a fixing commit.  The tests are then re-run, and if they pass, deployment continues. Through trial-and-error, we’ve settled on about 11 minutes as the longest that the automated tests can run during a push.  That leaves time to re-run the tests once during a deployment, without going too far past the 20 minute time limit. Run end-to-end, the 7,000 trunk tests would take about half an hour to execute.  We split these tests up into subsets, and distribute those onto the 10 machines in our Jenkins cluster, where all the subsets can run concurrently.  Splitting up our test suite and running many tests in parallel,  gives us the desired 11 minute runtime. Keep Tests That Are Similar, Together We decided to use PHPUnit’s @group annotations (really just a special form of comments) to logically divide the test suite into different subsets. Jenkins and PHPUnit group annotations turned out to be a simple and powerful combination. Each PHPUnit job we set up in Jenkins has its own XML configuration file that looks something like this: <groups>\n      <include>\n        <group>dbunit</group>\n      </include>\n      <exclude>\n        <group>database</group>\n        <group>network</group>\n        <group>flaky</group>\n        <group>sleep</group>\n        <group>slow</group>\n      </exclude>\n    </groups> This particular config block means that this PHPUnit job will run tests tagged as dbunit . And this job will not run tests tagged database, network, flaky, sleep, or slow. Test Classification: the Good, the Fast and the Intermittent If you feel like 11 minutes should be enough to run 7,000 unit tests, you’re right.  But not all of our automated tests are unit tests.  Or at least, they’re not what we call unit tests.  Every shop seems to have its own terminology for describing kinds of tests, and… so do we. Here’s how we classify the different kinds of tests that run in our CI system. Unit Tests We define a unit test as a test for one-and-only one class, and that has no database interaction at all, not even fixtures. You may have noticed above that we didn’t define a PHPUnit annotation for unit tests.  Unit tests are the default. We run the unit tests on a server where MySQL and Postgres aren’t even available.  That way we find out right away if a database dependency was added accidentally. As of today, we have about 4,500 unit tests, which run in about a minute. Integration Tests For the most part when we say a test is an integration test, this implies that the test uses fixtures.  Usually  our fixtures-backed tests are built with the PHPUnit port of DBUnit.  We’ve even provided some PHPUnit extensions of our own to make testing with DBUnit easier. We also apply the term “integration tests” to test cases that depend on any external service (eg Memcache or Gearman). The integration tests are the slowest part of our suite.  If we ran them all sequentially, the integration tests alone would take about 20 minutes for every deployment.  Instead, we spend about 8 minutes per deploy running them concurrently. Network Tests Some integration tests may also access network resources.  For instance, a test might assert that our libraries can properly send an email using a third-party service. For the most part, we try to avoid tests like this.  When a test depends on a network request, it can fail just because the request was unsuccessful.  That can occur for a number of reasons, one of which may be that the service being tested against is actually down.  But you never really know. Smoke Tests Smoke tests are system level tests that use Curl. For the most part, our smoke tests are PHPUnit test cases that execute Curl commands against a running server.  As the response from each request comes back, we assert that the proper headers and other data were returned from the server. Functional Tests Like smoke tests, end-to-end GUI-driven functional tests are also run against a live server, usually our QA environment (for more on that see the comments ).  For these tests we use Cucumber and Selenium, driving a Firefox instance that runs in an Xvfb virtual desktop environment. Since they are very labor-intensive to develop and maintain, end-to-end functional tests are reserved for testing only the most mission-critical parts of Etsy.  We get a lot of confidence from running our unit, integration and smoke tests.  But at the end of the day, it’s good to know that the site is so easy to use, even a robot user agent can do it. Zero Tolerance for Intermittent Tests Sometimes tests fail for no good reason.  It’s no big deal, it happens. A test can be a bit flaky and still be helpful during development.  And such tests can be useful to keep around for reference during maintenance. But running tests before deployment is different than running tests during maintenance or development.  A test that only fails 2% of the time will fail about every other day if run before each of our 25 deployments.  That’s 2-3 failures a week.  And in practice each failure of the trunk tests translates to around 10 minutes of lost developer time — for every developer currently waiting to make a deployment! So a single test that fails only 2% of the time can easily incur a cost of about 5 wasted work-hours per week. We therefore provide a few other PHPUnit group annotations which anyone is free to use when they encounter a test that isn’t quite robust enough to block deployment. Intermittent Tests We use the annotation @group flaky to denote a test that has been observed to fail intermittently.  Tests so annotated are automatically excluded from running during deployment. This has worked out better for us than skipping or commenting out intermittent tests.  Tests annotated as flaky can still be run (and are still useful) in some contexts. Slow Tests Observation has repeatedly shown that our slowest tests roughly exhibit a power law distribution: a very few tests account for a great deal of the overall runtime of the test suite. Periodically we ask that engineers identify very long-running tests and tag them as @group slow .  Identifying and “retiring” our top 20 slowest tests (out of 7,000) usually results in a noticeable speedup of the test suite overall. Again, tests so annotated may still be run (and are still useful) in some contexts, just not before deployment. Sleep Tests and Time Tests Good tests don’t sleep() nor do they depend on the system clock. Sometimes getting test coverage at all means writing less-than-good tests (this is especially true when writing new tests for legacy code).  We accept this reality — but we still don’t run such tests before deployment. Fast, Reliable Tests Our CI system is still relatively new (most tests are under two years old and the Jenkins instance only dates back to July) so we still have a lot of work to do in terms of building awesome dashboards .  And in the future we’d like to harvest more realtime data from both the tests, and from Jenkins itself. But so far we’ve been very happy with the CI system that has resulted from using PHPUnit annotations to identify subsets of functionally similar tests and running the subsets concurrently on a cluster of servers.  This strategy has enabled us to quadruple the speed of our deployment pipeline and given us fine-grained control over which tests are run before each of our 25 or more daily deploys.  It’s a lightweight process that empowers anyone who knows how to write a comment to have input into how tests are run on Etsy’s deployment pipeline. If this article was interesting to you, catch up with us at the PHP Community Conference in Nashville later this week! Posted by Noah Sussman on April 20, 2011 Category: engineering , infrastructure", "date": "2011-04-20,"},
{"website": "Etsy", "title": "Moving Fast at Scale: a microconference at SXSW", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/03/01/moving-fast-at-scale-sxsw/", "abstract": "Moving Fast at Scale: a microconference at SXSW Posted by Chad Dickerson on March 1, 2011 We know some folks wanted a recorded version of the talk.  Given the complexity of our venue setup and the short timeline, we weren’t able to pull it off in Austin, but take heart, we will do a reprise of the talk when we’re safely back in Brooklyn and let you know! In January, Etsy served over a billion page views and sellers collectively sold over $33 million of goods on the site .  We also deployed the site to production 517 times with changes from over 70 unique individuals. Small, rapid changes are the key to faster release cycles, flexibility, better morale, and better uptime. We’ve written about how we do it here on Etsy’s engineering blog (look here , here , and here for a sampling) but we’ve never explained it all in one place, in person, with free beer.  That’s about to change!   We’re coming to Austin for SXSW Interactive to make it happen. From 4-6pm on Saturday, March 12 , key members of the Etsy engineering team, along with special surprise guests, will offer a series of small rapid talks providing a fast-paced and intensive overview of tools, processes, and cultural practices that make moving fast easy. Come catch a conference worth of tech talks, parties, and hallway conversations in two hours. All details about sign up and location (just 3 blocks from the convention center) are here . Talks include: “Scaling Startups” (Chad Dickerson, CTO) “Continuous Deployment:  the Why and How” (Kellan Elliott-McCrea, VP of Engineering) “Deployinator: Etsy’s Deployment Tool” (Erik Kastner, Software Engineer, Developer Tools) “Metrics-Driven Development” (Mike Brittain, Lead, Core Platform) After the talks end at 6pm, stay for drinks and bands for the rest of the night!  Members of the Etsy engineering team will be hanging out. For other events and to get the low-down on all the DIY goodness in the Etsy space throughout the weekend, check out our special Code as Craft SXSW 2011 page . See you in Austin! Posted by Chad Dickerson on March 1, 2011 Category: engineering , operations , philosophy", "date": "2011-03-1,"},
{"website": "Etsy", "title": "Measure Anything, Measure Everything", "author": ["Ian Malpass"], "link": "https://codeascraft.com/2011/02/15/measure-anything-measure-everything/", "abstract": "Measure Anything, Measure Everything Posted by Ian Malpass on February 15, 2011 If Engineering at Etsy has a religion, it’s the Church of Graphs. If it moves, we track it. Sometimes we’ll draw a graph of something that isn’t moving yet, just in case it decides to make a run for it. In general, we tend to measure at three levels: network, machine, and application. (You can read more about our graphs in Mike’s Tracking Every Release post.) Application metrics are usually the hardest, yet most important, of the three. They’re very specific to your business, and they change as your applications change (and Etsy changes a lot). Instead of trying to plan out everything we wanted to measure and putting it in a classical configuration management system , we decided to make it ridiculously simple for any engineer to get anything they can count or time into a graph with almost no effort. (And, because we can push code anytime, anywhere , it’s easy to deploy the code too, so we can go from “how often does X happen?” to a graph of X happening in about half an hour, if we want to.) Meet StatsD StatsD is a simple NodeJS daemon (and by “simple” I really mean simple — NodeJS makes event-based systems like this ridiculously easy to write) that listens for messages on a UDP port. (See Flickr’s “ Counting & Timing ” for a previous description and implementation of this idea, and check out the open-sourced code on github to see our version.) It parses the messages, extracts metrics data, and periodically flushes the data to graphite . We like graphite for a number of reasons: it’s very easy to use, and has very powerful graphing and data manipulation capabilities. We can combine data from StatsD with data from our other metrics-gathering systems. Most importantly for StatsD, you can create new metrics in graphite just by sending it data for that metric. That means there’s no management overhead for engineers to start tracking something new: simply tell StatsD you want to track “grue.dinners” and it’ll automagically appear in graphite. (By the way, because we flush data to graphite every 10 seconds, our StatsD metrics are near-realtime.) Not only is it super easy to start capturing the rate or speed of something, but it’s very easy to view, share, and brag about them. Why UDP? So, why do we use UDP to send data to StatsD? Well, it’s fast — you don’t want to slow your application down in order to track its performance — but also sending a UDP packet is fire-and-forget. Either StatsD gets the data, or it doesn’t. The application doesn’t care if StatsD is up, down, or on fire; it simply trusts that things will work. If they don’t, our stats go a bit wonky, but the site stays up . Because we also worship at the Church of Uptime, this is quite alright. (The Church of Graphs makes sure we graph UDP packet receipt failures though, which the kernel usefully provides.) Measure Anything Here’s how we do it using our PHP StatsD library: StatsD::increment(\"grue.dinners\"); That’s it. That line of code will create a new counter on the fly and increment it every time it’s executed. You can then go look at your graph and bask in the awesomeness, or for that matter, spot someone up to no good in the middle of the night: We can use graphite’s data-processing tools to take the the data above and make a graph that highlights deviations from the norm: (We sometimes use the “rawData=true” option in graphite to get a stream of numbers that can feed into automatic monitoring systems. Graphs like this are very “monitorable.”) We don’t just track trivial things like how many people are signing into the site — we also track really important stuff, like how much coffee is left in the kitchen: Time Anything Too In addition to plain counters, we can track times too: $start = microtime(true);\r\neat_adventurer();\r\nStatsD::timing(\"grue.dinners\", (microtime(true) - $start) * 1000); StatsD automatically tracks the count, mean, maximum, minimum, and 90th percentile times (which is a good measure of “normal” maximum values, ignoring outliers). Here, we’re measuring the execution times of part of our search infrastructure: Sampling Your Data One thing we found early on is that if we want to track something that happens really, really frequently, we can start to overwhelm StatsD with UDP packets. To cope with that, we added the option to sample data, i.e. to only send packets a certain percentage of the time. For very frequent events, this still gives you a statistically accurate view of activity. To record only one in ten events: StatsD::increment(“adventurer.heartbeat”, 0.1); What’s important here is that the packet sent to StatsD includes the sample rate, and so StatsD then multiplies the numbers to give an estimate of a 100% sample rate before it sends the data on to graphite. This means we can adjust the sample rate at will without having to deal with rescaling the y-axis of the resulting graph. Measure Everything We’ve found that tracking everything is key to moving fast, but the only way to do it is to make tracking anything easy. Using StatsD, we enable engineers to track what they need to track, at the drop of a hat, without requiring time-sucking configuration changes or complicated processes. Try StatsD for yourself: grab the open-sourced code from github and start measuring. We’d love to hear what you think of it. Posted by Ian Malpass on February 15, 2011 Category: data , engineering , infrastructure", "date": "2011-02-15,"},
{"website": "Etsy", "title": "How does Etsy manage development and operations?", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2011/02/04/how-does-etsy-manage-development-and-operations/", "abstract": "How does Etsy manage development and operations? Posted by Chad Dickerson on February 4, 2011 I’ve been loving using Quora these past few months, and have been amazed at the level of behind-the-scenes detail people are providing about really complex and specific things (like how Facebook does automated testing ). Recently, someone asked, “ How does Etsy manage development and operations? ” with these comments: Etsy seems to have scaled far and fast, whilst continuing to add new features; how is all this managed – is there a strictly-defined process within which engineers operate, or is it a case of hiring clever people and letting them get on with it (Facebook-style)? First of all, I love the team and am proud of the work that they do.  It’s an amazing group and none of this would work or be as fun as it is without them. So, here’s the answer I just posted : In 2010, we did grow the engineering team pretty fast, going from 20 to about 70, and the rest of the company grew quickly, too.  As we grew, overall speed has been really important to us, and we’ve continually tuned our processes, tools, and culture to support that.  I wrote about some of these principles behind all of it in my blog over the summer: http://bit.ly/foHPc1 Right now, developers are divided up into a number of small teams, usually 3-7 engineers.  These teams are paired with a product manager and a designer, and there is some movement across teams as needed.  All designers at Etsy code and product managers code at various levels, too. Ops and dev work really closely together, and we have one development team that is very ops-like and straddles both domains. Everyone in the company uses IRC.  Lots of ideas are worked out on a wiki, and people around the company comment on those ideas and plans (we use Confluence).  Some projects form organically, and others are more top-down. We generally plan in 60-day chunks and divide the deliverables up into 2-week periods (though we’re not officially using capital-A Agile).  The 60-day cycle has no special significance — we just felt like it was a reasonable timeframe for planning near-term deliverables.  The 60-day plans go through a review, we set goals, and we publish the plans on the wiki.  Our founder, CEO, and head of product (Rob Kalin) participates in these reviews and stays in close contact with the product and engineering teams throughout.  In general, the teams have a lot of autonomy in how they get their work done within a set of architectural principles we’ve established (a subject for another post) and our overall design approach.  Specs are typically very light, and the focus is on building working features. We onboard engineers quickly and their first goal is simple: deploy on your first day.  The goal here is to constantly emphasize shipping, and get over any deployment fears early.  Engineers get productive very quickly.  The level of cooperation between developers and ops is also really high (see our engineering blog for more: http://etsy.me/hMtu1A ) We practice continuous deployment and make small changes frequently to the site.  We use what we call “config flags,” which are more or less an exact copy of what Flickr does (see the Flickr engineering blog: http://bit.ly/dZZzfY ) and a lot of the code for features runs “dark” for days or weeks, and feature launches mean flipping a switch in the code.  We have a lot of Flickr DNA in the company (John Allspaw, our VP of Ops, ran ops at Flickr, and Kellan Elliott-McCrea was architect at Flickr).  In January (a month in which we did over a billion page views), code committed by 76 unique individuals was deployed to production by 63 different folks a total of 517 times.   Product managers make changes and do deploys (here’s Jenn Vargas, one of our newest product managers, tweeting about it ) and we have trained aspiring developers on our support team to make small changes with our help and guidance, too.  Our deployment environment requires a lot of trust, transparency, communication, coordination, and discipline across the team.  We’ve invested a lot in our automated unit and functional testing (we have a team devoted just to this), tooling for deployment (see our blog post about Deployinator: http://etsy.me/c6RJD7 ), and metrics and monitoring (see “Tracking Every Release”: http://etsy.me/e1ULhO ).   Key system-level and business level metrics (like checkout/listing/registration/sign-in rates)  are projected on screens in the office and we have a number of internal dashboards that the team uses (we mainly use Ganglia and Graphite).  We also have lots of switches and knobs to help us roll features out to percentages of users and ramp them up slowly, or quickly.  Features are used and tested by us here at Etsy for some period of time before they are rolled out publicly. When we make mistakes, we conduct blameless post-mortems and assign remediation items to the appropriate team members.  Engineers frequently post in our community forums when we have any issues and we have a status blog that we maintain ( http://www.etsystatus.com/ ).  I think that interacting with Etsy members gives everyone a deeper sense of responsibility for the code we’re writing.  We also write about the mistakes we make pretty openly ( http://etsy.me/hgZ4qh ). Overall, engineers are treated as creative collaborators in the overall process with design and product, and products are worked out and iterated on with engineers instead of simply being handed to them for implementation.   Rob (our founder and head of product) likes working with engineers and the engineers spend a lot of time interacting with Rob.  Our ability to work this way has as much to do with the personalities of the people involved and the culture as the technologies involved. We’re always learning and adjusting and we’ll continue to evolve as time goes on. Posted by Chad Dickerson on February 4, 2011 Category: operations , people , philosophy", "date": "2011-02-4,"},
{"website": "Etsy", "title": "Mining Facebook for Gifts on Etsy", "author": ["Jason Davis"], "link": "https://codeascraft.com/2011/01/25/mining-facebook-for-gifts-on-etsy/", "abstract": "Mining Facebook for Gifts on Etsy Posted by Jason Davis on January 25, 2011 Buying gifts is hard.  We created the Facebook gift recommender on Etsy to help you overcome the feeling of gift-giving writer’s block.  To do so, it surfaces your friends’ interests from Facebook’s social graph and compares them across millions of items from Etsy’s marketplace. The product works by (1) connecting with your Facebook account and pulling entities for each of your Facebook friends: interests, activities, favorite movies, music, and more, (2) matching these entities to relevant items from Etsy’s marketplace, and then (3) making recommendations on per-friend basis.  Each individual recommendation consists of a context (‘Michael Jackson’), along with a sample set of 4 items from the marketplace.  At the core of the gift ideas finder is the matching algorithm that we train from mining billions of Etsy searches, purchases, and listing favorites over months and months of data. Critical to the matching algorithm is an understanding of Facebook entities and how they relate to items on Etsy.  For each Facebook keyword, our algorithm first measures the quality of the keyword in Etsy’s marketplace, and then analyzes the semantic meaning of the keyword on Facebook compared to its meaning on Etsy.  As an example, the musician ‘Pink’ has over 4 million followers on Facebook, and a quick search for ‘pink’ on Etsy reveals over 500,000 unique items.  Although there are many relevant listings to the keyword ‘pink’, the entity has a very different meaning in the context of Etsy as compared to Facebook.  On the other hand, ‘Michael Jackson’ has a large following on Facebook and also has lots of relevant items on Etsy: Michael Jackson dolls, Michael Jackson 1980’s thriller jackets, etc. To understand the semantic context of keywords on Etsy, we start with an analysis of billions of searches on Etsy.com.  We mine these searches on a per-visit level: the key assumption here is that when people search within a visit to the site, they’re generally searching for semantically similar items.  For example, someone may search for the term ‘tutu’, then ‘pink tutu’, and then ‘pink skirt’.  And in a separate visit, someone else may search for ‘michael jackson’, ‘thriller’, and ‘thriller jacket’.  By mining hundreds of millions of visits in aggregate over many months of search data, we’re able to form semantically similar sets for many of the queries that people have searched for on Etsy.  As an example, the semantic set for ‘pink’ includes ‘hot pink’, ‘pink jewelry’, ‘pale pink’, and ‘fuchsia’.  Since none of these keywords are bands, we then infer that ‘pink’ in the context of Etsy has nothing to do with the musician. The second component to the recommendation algorithm requires understanding quality: a keyword like ‘BMW’ may be semantically similar on Facebook and Etsy yet Etsy isn’t the best place to buy a BMW.  To understand quality, we analyze item page view, purchase, and favoriting behavior that originates from searches on Etsy.  High quality and popular items tend to have lots of searches, page views, purchases, and favorites, and mapping these events back to specific search terms is an important data quality measure.  Our logging infrastructure enables us to precisely attribute such views, purchases, and favorites to originating searches.  Our tracking infrastructure logs all listing ids shown for every search that appears on Etsy.  From this, we’re able to join listing ids of purchase, favorite, and item view events back to their originating searches, and then precisely attribute the sale of an item to a specific search, i.e. either ‘pink tutu’ or ‘tutu’.  We implement this funnel analysis using a custom event sequence analyzer that runs on our Cascading data flow framework (you can read more about how we use Hadoop and Cascading here ). The final step in the process involves combining these two components to decide which recommendations to show and in what order.  The process is similar to search and information retrieval algorithms that must order based on analogous quality and relevance metrics.  We also leverage other data sources that we won’t get into here (for example, popular Facebook likes per-category), and of course we apply lots of heuristics as well – bands in particular often have ambiguous names (Cream, Queen, Tool, Traffic). Want to see all of this in action? Try it out at www.etsy.com/gifts . Posted by Jason Davis on January 25, 2011 Category: data", "date": "2011-01-25,"},
{"website": "Etsy", "title": "Introducing Getsy, A Secure Command Line Interface to the Etsy API", "author": ["John Goulah"], "link": "https://codeascraft.com/2011/01/10/introducing-getsy-a-secure-command-line-interface-to-the-etsy-api/", "abstract": "Introducing Getsy, A Secure Command Line Interface to the Etsy API Posted by John Goulah on January 10, 2011 Some of the most productive tools out there come from the command line and so when you’re testing REST-based requests it makes a lot of sense to use the command line tool curl . But since the OAuth protocol uses 3-legged authentication, and there are several tokens and other intricacies to handle, it makes curl very difficult to use directly. As I was working on the Etsy API last year I found it would be useful to have something similar to curl that I could quickly make API requests with using OAuth in both the sandbox and production environments. WWW::Getsy is a command line tool that does just that. This is an introduction to the tool that shows how to get it setup and start making a few calls. The tool is written in modern Perl and I’ll show how to get everything setup quickly and easily without needing to use root privileges. Perlbrew is a great module that will let you easily switch between multiple versions of Perl. Using it has a couple of advantages over using your system version of Perl. The first is that you can easily install a current version of Perl locally in your home directory. Most distros have a pretty outdated version installed and this will give you a decent version to work with. The second advantage is it will allow you to install your modules locally, without the need for superuser privileges. This way you won’t have to interfere with whats already installed on your system. So lets grab it and make it executable: $ curl -LO http://xrl.us/perlbrew $ chmod +x perlbrew Run the basic initialization: $ ./perlbrew install $ ./perl5/perlbrew/bin/perlbrew init Get the environment setup by adding this line to your ~/.bashrc file: $ echo \"source ./perl5/perlbrew/etc/bashrc\" >> ~/.bashrc $ source ~/.bashrc And then run the installation of a current version, in this case 5.12.2. This step can take a bit depending on your machine. $ perlbrew install perl-5.12.2 Now that its installed switch to that version: $ perlbrew switch perl-5.12.2 So now you’ll see if you look at the perl version that you’re running our newly installed version: $ perl --version This is perl 5, version 12, subversion 2 (v5.12.2) built for i686-linux Now you’re ready to install the WWW::Getsy module, which you can do in one step with this command (note it will take a bit to get all of the dependencies, but should install cleanly) $ PERL_MM_USE_DEFAULT=1 cpan WWW::Getsy While thats installing, if you haven’t already, go ahead and register for a developer account . The confirmation email you receive from that will guide you to get an API key, which should direct you to the app registration form . It will require at a minimum an application name, which you can always change later. After submitting the form you should end up with a key and shared secret. Getsy requires you to assign these keys to some environment variables, so you may want to add this to your ~/.bashrc file and source it like we did above. Just fill in the key and secret that you just received. export OAUTH_CONSUMER_KEY='yourkey' export OAUTH_CONSUMER_SECRET='yoursecret' Now assuming WWW::Getsy finished installing we’re finally ready to make some calls. The simplest call we can make is a GET for the API method list. Since we’re using sandbox keys, we specify the sandbox argument, and the root path: $ getsy --sandbox --path '/' Since this is your first time making a call you will receive some instructions to do the OAuth dance and get your access tokens. This is where you are directed to an Etsy page that you can sign in (or accept if you’re already signed in). When you do you will get a verifier that you can paste in. So if the verifier I got after signing into the link it tells me to go to was “7cb1d239” I would paste that and press enter: Go to http://www.etsy.com/oauth/signin?oauth_token=sometoken&callback=oob Then enter the verifier token 7cb1d239 <enter> If you did everything correctly, you should see some JSON output with all of the method calls that you can make with the Etsy API. You’ll only have to do that once, as the access token and secret are stored in the ~/.getsy_sandbox file. If you want to use another user later just remove that file, and you’ll be prompted to get a different access token. Otherwise the access token and secret can be reused on subsequent calls. Note that we can take a look at what arguments the getsy command will accept with the –help parameter: $ getsy --help\r\nRequired option missing: path\r\nusage: getsy [-?] [long options...]\r\n        --sandbox\r\n        --debug\r\n        --path\r\n        --params\r\n        --method\r\n        -? --usage --help  Prints this usage information. The main argument that you’ll always need (in addition to –sandbox until you get a production key) is the –path argument. The –path argument is how you specify the API command, which you can find from the output of the method list we requested above or in the excellent Etsy API Documentation . The –method argument is used to specify the HTTP method, which defaults to GET. And –params is used to specify parameters in JSON format for commands that accept additional parameters, such as POST commands. So now lets try to make a POST request. A good one is to try to favorite a listing, and we can do that like so: $ getsy --sandbox --path '/users/__SELF__/favorites/listings/58171578' --method=post And the response will look something like this: {\r\n   \"params\" : {\r\n      \"user_id\" : \"__SELF__\",\r\n      \"listing_id\" : \"58171578\"\r\n   },\r\n   \"count\" : 1,\r\n   \"type\" : \"FavoriteListing\",\r\n   \"results\" : [\r\n      {\r\n         \"listing_state\" : \"active\",\r\n         \"creation_tsz\" : 1294628005,\r\n         \"create_date\" : 1294628005,\r\n         \"user_id\" : 6591529,\r\n         \"state\" : \"active\",\r\n         \"listing_id\" : 58171578,\r\n         \"favorite_listing_id\" : 163590951\r\n      }\r\n   ]\r\n} The path we used above corresponds to the createUserFavoriteListings command. It says the URI should follow this format: /users/:user_id/favorites/listings/:listing_id Notice how we used __SELF__ in place of the :user_id parameter. The standard token __SELF__ maps to the user ID of the user using your application under OAuth, which is what we signed in with on our first call. For the :listing_id, we just used a standard numeric listing id, which can be found in Etsy URLs, or more programmatically with the findAllListingActive command We can then use the deleteUserFavoriteListings call to make a DELETE request to remove the listing we just favorited: $ getsy --sandbox --path '/users/__SELF__/favorites/listings/58171578' --method=delete And the response will look something like: {\r\n   \"params\" : {\r\n      \"user_id\" : \"__SELF__\",\r\n      \"listing_id\" : \"58171578\"\r\n   },\r\n   \"count\" : 0,\r\n   \"type\" : \"FavoriteListing\",\r\n   \"results\" : []\r\n} Now lets try a command where we need to specify some parameters with the –params argument. Creating a payment template is a good example. A payment template is basically a template that is setup to signify to a buyer which kinds of payments you accept as a seller. Note you’ll want to be OAuth’d in as a seller or it won’t work correctly. So for the createPaymentTemplate command we can do something like this: $ getsy --sandbox --path '/payments/templates' --method=post --params '{\"allow_paypal\" : \"true\", \"paypal_email\" : \"test@test.com\"}' Note if you already have a payment template setup for that user, it won’t let you create another one, so you could check what the current payment template id is for that user with the findAllUserPaymentTemplates command: $ getsy --sandbox --path '/users/__SELF__/payments/templates' Notice I didn’t specify the –method parameter this time since GET is the default. Now find the payment_template_id in the JSON response and use that to update the payment template. Lets assume that id was 5694812 (it will be different for you), I could then use that to make a PUT request to update the template: $ getsy --sandbox --path '/payments/templates/5694812' --method=put --params '{\"allow_paypal\" : \"true\", \"paypal_email\" : \"test@test.com\"}' So now we’ve been able to make some OAuth requests to the Etsy API using a very simple command line tool. All the difficulties of OAuth are wrapped up behind the scenes, and we are able to make GET, PUT, POST, and DELETE requests for our authenticated user. Posted by John Goulah on January 10, 2011 Category: api", "date": "2011-01-10,"},
{"website": "Etsy", "title": "Etsy Handmade Code Contest Winners", "author": ["stunji"], "link": "https://codeascraft.com/2010/11/16/etsy-handmade-code-contest-winners/", "abstract": "Etsy Handmade Code Contest Winners Posted by stunji on November 16, 2010 Wow!  Judging took a lot longer, and was a lot harder than we expected.  In all, there were 34 eligible entries.  We were especially happy to have 8 mobile entries, spanning the gamut from iPhone/iPad to Android, Blackberry, and Windows mobile.  This is great news for everyone who’s been wanting to browse Etsy on their mobile phones. We also recently launched our Etsy App Gallery .  This will become a comprehensive list of all the Etsy Apps that exist out there in the wild.  So far we have over 30 apps listed, including the contest winners, and developers are listing new apps every day.  We’ve also created the Etsy Apps Team where developers can get in touch with Etsy community members, get feedback on their apps, and talk about what apps the community wants to see.  Check it out! And without further ado… here are the winners: Category Winners These apps won the Grand Prize of $5,000 each: Buyer Tools Etsy Vintage Timeline “Very simple product idea with great execution. Delightful to use.” Seller Tools Etsy On Sale “Awesome idea and very popular with the community.” Mobile App Etsy Lovers “Impressive use of various Etsy APIs.” Honorable Hacks These apps won $500 each.  We had so many great Seller Tools entries, we had to choose three! Buyer Tools Favoritizer “Great feature around one of the key activities on Etsy.” Seller Tools Etsy Catalog “Big scope…very cool idea.” Seller Tools Etsy Text “Nice support for a wide variety of mobile devices.” Seller Tools ShipRush for Etsy “Powerful ideas for order fulfillment optimization.” Mobile App Craftsy “Great design.” Winners were selected by our final round judges: Chad Dickerson, CTO of Etsy, Jared Tarbell, co-founder of Etsy, and Tarikh Korula, founder of Uncommon Projects . Major props to all our contestants from both the Etsy Admins and the whole Etsy Community—you guys are the best!  We’re excited to list all of the contest entries below, so make sure to check them out.  (Some of the entries don’t have public URLs yet; check back for updates.) Autofy for Etsy and QuickBooks Integration • Berry Etsy Browser • Colormatch Craftopolis – Edit Express • Craftopolis – Shop Lovers • Craftopolis – Tag Report Et Cetera • Etsy Addict • Etsy Colors • Etsy Espy • Etsy Stars • Etsy2go etsy4kids • EtsyTube Instant • Etsywishlist • FotoFuze • Glancely HelperOfTheBride • Insomnia • LondonsGate • InstantEtsy • simplebooklet Smo.bo • The Etsy Bazaar • Treasury Junior • Zetsy Posted by stunji on November 16, 2010 Category: api", "date": "2010-11-16,"},
{"website": "Etsy", "title": "Handmade Code Contest Update–Finalists have been selected!", "author": ["Liz Wald"], "link": "https://codeascraft.com/2010/10/29/502/", "abstract": "Handmade Code Contest Update–Finalists have been selected! Posted by Liz Wald on October 29, 2010 Note: This is the third update to the original post from September 15th . Four finalists in each category (buyer tools, seller tools, mobile) have been notified and the final round of judging is underway to determine who will win the three $5,000 grand prizes and the five $500 “honorable hacks.”  We appreciate your patience and will announce the winners soon! Note: This is the second update to the original post from September 15th . We are thrilled to report that we got dozens of entries covering all three categories of Buyer Tools, Seller Tools and Mobile.  We’ve needed a bit more time than we thought to go through them all and give them the attention they deserve so we won’t be announcing our winners today.   Have a great Halloween and we’ll post an update next week! Posted by Liz Wald on October 29, 2010 Category: api , events", "date": "2010-10-29,"},
{"website": "Etsy", "title": "You Can’t Fake Real", "author": ["Randy Hunt"], "link": "https://codeascraft.com/2010/10/21/you-cant-fake-real/", "abstract": "You Can’t Fake Real Posted by Randy Hunt on October 21, 2010 I’m Randy , the Designer Director at Etsy. We think of design like all crafts. We do it best when we have a deep understanding with our tools, so our designers write code alongside our engineers. Together we shape what we build every day. Most days I step into our office , and I see crochet-covered duct work, a wall covered in illustrations crowd-sourced from Twitter, at least a dozen people standing, walking, laughing, and some adorable office dogs ( Dottie Matrix! ). Image: Ty Cole for Hangar Design Group You can’t pull this stuff out of a management handbook. It’s a naturally occurring and organically evolving part of who we are. Somehow, in ways I can’t fully describe, Etsy (the company) truly is a part of and reflection of the Etsy community. It’s quite diverse and extensive community, and the energy is palpable. There are millions of items for sale, half a million shops, and over five years of history. Most of all there are the tens of millions of people from 150 countries who make up Etsy. Sure, you can innovate in products and features, but more importantly you can innovate in culture. At Etsy, that culture emerges organically and is nurtured and encouraged in a simple way: it’s all about being who we really are.  Take for example this blog, Code as Craft, where our Engineering team writes about what, how, and why we do what we do. We’re transparent about tools we use. We’re open about mistakes we make and how we resolve them. Our origins are in the spirit of craft, the passing down of techniques and the exchange of experiences . So that’s what we do.  The idea of being real sounds so straight-forward, but most organizations really mess it up. Our CTO, Chad Dickerson, was at a conference recently and someone asked him why Etsy has so many Twitter followers . His response? We have real people talking to other real people in a real voice.  Quite simply, you can’t fake authenticity. When we were looking to hire engineers, sure, we posted job listings, but we also made a pretty incredible video that I’m proud to show friends and respected colleagues both. http://player.vimeo.com/video/13214706 Is it unconventional? Of course! That’s who we are. (Side note, we’re still hiring .)  You know what’s more awesome than software products or engineering processes? People. Every time. As part of Honda’s “Kick Out the Ladder” thought leadership series, Etsy was asked to provide this unique perspective on how an organization’s philosophy allows for innovation and leads to success. Five organizations are offering their thoughts on the subject, and we’re one of them. (It’s worth noting that there is some amazing history of creativity and craft with some of the other participants, namely Herman Miller and Kodak.) To be honest, I had reservations at first about what this would mean to take part in a conversation like this that involves large brands that are quite different from Etsy, but I’m proud of who we are and what we stand for. We’re excited to spread the love. See what others are saying about “Kick Out the Ladder” at www.facebook.com/honda . Posted by Randy Hunt on October 21, 2010 Category: philosophy", "date": "2010-10-21,"},
{"website": "Etsy", "title": "Tracking Every Release", "author": ["Mike Brittain"], "link": "https://codeascraft.com/2010/12/08/track-every-release/", "abstract": "Tracking Every Release Posted by Mike Brittain on December 8, 2010 We spend a lot of time gathering metrics for our network, servers, and many things going on within the code that drives Etsy.  It’s no secret that this is one of our keys to moving fast.  We use a variety of monitoring tools to help us correlate issues across our architecture.  But what most monitoring tools achieve is correlating the effects of change, rather than the causes. Change to application code (deploys) are opportunities for failure.  Tweaking pages and features on your web site cause ripples throughout the metrics you monitor, including database load, cache requests, web server requests, and outgoing bandwidth.  When you break something on your site, those metrics will typically start to skew up or down. Something obviously happened here… but what was it?  We might correlate this sudden spike in PHP warnings with a drop in member logins or a drop in traffic on our web servers, but these point to effects and not to a root cause. We need to track changes that we make to the system. Different companies track change in ways that are reflective of their release cycle.  A company that only releases new software or services once or twice a year might literally do this by distributing of a press release.  Companies that move more quickly and release new products every few weeks might rely on company-wide emails to track changes.  The faster the iteration schedule, the smaller and less formal the announcement becomes. When you reach the point of releasing changes a couple of times a day, this needs to be automated and needs to be distributed to places where it is quickly accessible, such as your monitoring tools and IRC channels .  At Etsy, we are releasing changes to code and application configs over 25 times a day.  When the system metrics we monitor start to skew we need to be able to immediately identify whether this is a human-induced change (application code) or not (hardware failure, third-party APIs, etc.).  We do this by tracking the time of every single change we ship to our production servers. We’ve been using Graphite for monitoring application-level metrics for nearly a year now. These include things like numbers of new registrations, shopping carts, items sold, image uploaded, forum posts, and application errors. Getting metrics into Graphite is simple , you send a metric name, a value, and the current Unix timestamp.  To track time-based events, the value sent for the metric can simply be “1”. Erik Kastner added this right into our code deployment tool so that every single deploy is automatically tracked. You didn’t think we did this by hand, did you? events.deploy.website 1 1287106599 The trick to displaying events in Graphite is to apply the drawAsInfinite() function. This displays events as a vertical line at the time of the event. (Hat tip: Mark Lin , since this is not well documented.)  The result looks like this: http://graphite.example.com/render/ ?target=drawAsInfinite%28events.deploy.website%29 &from=-24hours Graphite has a wonderfully flexible URL API that allows for mixing together multiple data sets in a single graph. We can mix our code deployments right into the graph of PHP warnings we saw above. Ah-ha! A code change occurred right after 4 PM that set off the warnings.  And you can see that a second deploy was made about 10 minutes later that fixed most of the warnings, and a third deploy that squashed anything remaining. We maintain a battery of thousands of tests that run against our application code before every single deploy, and we’re adding more every day. Combined with engineers pairing up for code reviews, we catch most issues before they get deployed.  Tracking every deploy allows us to quickly detect any bugs that we missed. Equally useful is the reassurance we have that we can deploy many times a day without disrupting core functionality on the site.  Across the 16 code deploys shown below, not a single one caused an unexpected blip in our member logins. These tools highlight the good events along with the bad.  Ian Malpass, who works on our customer support tools, uses Graphite to monitor the number of new posts written in our forums , where Etsy members discuss selling, share tips, report bugs, and ask for help. When we correlate these with deploys, you can see the flurry of excitement in our forums after one of our recent product launches . Automated tracking of code deploys is essential for teams who practice Continuous Deployment.  Monitoring every aspect of your server and network architecture helps detect when something has gone awry.  Correlating the times of each and every code deploy helps to quickly identify human-triggered problems and greatly cut down on your time to resolve them. Posted by Mike Brittain on December 8, 2010 Category: data , engineering , operations Tags: continuous deployment , deploy , graphite , metrics , monitoring Related Posts Posted by Sasha Friedenberg on 15 May, 2017 How Etsy Ships Apps Posted by Jayson Paul on 20 Feb, 2015 Re-Introducing Deployinator, now as a gem! Posted by Rasmus Lerdorf on 01 Jul, 2013 Atomic deploys at Etsy", "date": "2010-12-8,"},
{"website": "Etsy", "title": "Sebastian Bergmann (author of PHPUnit) at Etsy on October 7", "author": ["Noah Sussman"], "link": "https://codeascraft.com/2010/10/05/sebastian-bergmann-author-of-phpunit-at-etsy-on-october-7/", "abstract": "Sebastian Bergmann (author of PHPUnit) at Etsy on October 7 Posted by Noah Sussman on October 5, 2010 Sebastian Bergmann, author of PHPUnit, will be visiting Etsy this Thursday, October 7th. He’ll be giving a talk on the State of PHP Quality Assurance, including an overview of PHP code quality and static analysis tools. PHPUnit is the test harness of choice for PHP developers all over the world.  It has great support for mocks, fixtures, and the other features you’d expect in an XUnit framework, plus awesome extras like tools for mocking database connections, and baked-in Selenium integration. Every engineer at Etsy writes automated tests on a daily basis, and our Hudson continuous integration server runs about 2500 tests with every code push. Since we push code around 20 times a day, that means running around 250,000 PHPUnit tests in a typical work week! So we were thrilled when Sebastian accepted our invitation to visit Etsy.  It’s been fantastic to sit in code reviews with him, and to hear his thoughts on writing and automating tests, on code coverage and on software QA in general.  Some of these ideas will be covered in his forthcoming book, but why wait?  Come to Etsy, have a beer, meet some TDD hackers, and get schooled. Posted by Noah Sussman on October 5, 2010 Category: engineering , events , people", "date": "2010-10-5,"},
{"website": "Etsy", "title": "Frank Talk About Site Outages", "author": ["John Allspaw"], "link": "https://codeascraft.com/2010/09/17/frank-talk-about-site-outages/", "abstract": "Frank Talk About Site Outages Posted by John Allspaw on September 17, 2010 Walter's Mystery Control Panel, by Telstar Logistics, on Flickr (Note: this post is a largely non-technical discussion about technical issues. It’s mainly written for the shoppers and sellers on Etsy, but I’m posting it on our engineering blog because I think engineers might be interested.) The other night we had an outage . Put frankly: outages suck. They suck for shoppers, and sellers, and they suck for us who work here at Etsy. When our members come to the site — to manage their business, favorite an item they like, post in the forums, create a treasury, or send a convo — and the site is down, it’s hugely frustrating. One look at Twitter during an outage will give you an idea of exactly how frustrating it is for them. This is their business we’re talking about here. It’s also our business . The availability and performance of Etsy.com (and Etsy’s API) is of paramount importance to us. That’s the reason for this long-overdue post. We need to be better about communicating what’s going on during an outage, after an outage, and what we’re doing to prevent a repeat in the future. When we communicate well, we’ll further the confidence that Etsy members have in our ability to grow and be a stable place for them to buy and sell the things that they’re passionate about. You may or may not want to get some coffee here, because I’m going to be a little verbose. At the risk of boring you, I hope to gain your confidence that we take these topics very seriously. Making the site fast and resilient to failure isn’t a side project, it’s core to what we’re doing. When we build new servers and site features, or make changes to existing features, we think about worst-case scenarios . We’re not just trying to avoid them, we’re also figuring out how we’ll respond when something goes wrong that we didn’t imagine. How We Use Metrics and Alerts We gather ongoing metrics about all of our servers, networks, and the usage of the site so we won’t be blind to problems. Some of our metrics are set up to trip alarms and alerts so that problems can be fixed right away. Today, we gather a little over 30,000 metrics, on everything from CPU usage, to network bandwidth, to the rate of listings and re-listings done by Etsy sellers. Some of those metrics are gathered every 20 seconds , 24 hours a day, 365 days a year. About 2,000 metrics will alert someone on our operations staff (we have an on-call rotation) to wake up in the middle of the night to fix a problem. Let’s say all our servers are running fine, but people in a certain region of the world suddenly aren’t able to get to Etsy.com. An issue like that might be out of our immediate control (say, a fiber cable was cut in the Middle East ) but it’s still something we want to be aware of so we can alert our members. So, we use a service called Gomez and run tests to Etsy.com from their many thousands of machines around the globe, every minute, to make sure we’ll detect any patterns of slowness. As we build out new infrastructure and features, we’re always adding to and adjusting these metrics and alerts; it’s a work in progress. We also use metrics to predict the technical capacity we’re going to need as we grow as a site. Right now we’re evaluating where and when we’ll likely need capacity in the next 8 months, as well as further streamlining the process of provisioning new capacity as we need it. Outage Coordination and Communication When we have an outage or issue that affects a measurable portion of the site’s functionality, we quickly group together to coordinate our response. We follow the same basic approach as most incident response teams . We assign some people to address the problem and others to update the rest of the staff and post to http://etsystatus.com to alert the community. Changes that are made to mitigate the outage are largely done in a one-at-a-time fashion, and we track both our time-to-detect as well as our time-to-resolve, for use in a follow-up meeting after the outage, called a “post-mortem” meeting. Thankfully, our average time-to-detect is on the order of 2 minutes for any outages or major site issues in the past year. This is mostly due to continually tuning our alerting system. Post-Mortems and Historical Context After any outage, we meet to gather information about the incident. We reconstruct the time-line of events; when we knew of the outage, what we did to fix it, when we declared the site to be stable again. We do a root cause analysis to characterize why the outage happened in the first place. We make a list of remediation tasks to be done shortly thereafter, focused on preventing the root cause from happening again. These tasks can be as simple as fixing a bug, or as complex as putting in new infrastructure to increase the fault-tolerance of the site. We document this process, for use as a reference point in measuring our progress. Single Point of Failure (SPOF) Reduction As Etsy has grown from a tiny little start-up to the mission-critical service it is today, we’ve had to outgrow some of our infrastructure. One reason we have for this evolution is to avoid depending on single pieces of hardware to be up and running all of the time. Servers can fail at any time, and Etsy.com should be able to keep working if a single server dies. To do that, we have to put our data in multiple places, keep them in sync, and make sure our code can route around any individual failures. So we’ve been working a lot this year to reduce those “single points of failure,” and to put in redundancy as fast as we safely can. Some of this means being very careful (paranoid) as we migrate data from the single instances to multiple or replicated instances. As you can imagine, it’s a bit of a feat to move that volume of data around while still seeing a peak of 15 new listings per second , all the while not interrupting the site’s functionality. Change Management and Risk We’re constantly making improvements to Etsy.com. Changing an existing working website can be scary. How do you know that the change you’re going to make won’t crash the site? The fact is, not all changes are created equal; some are riskier than others. But in 2010, the majority of site issues have not been caused by changes we’ve made on the site. Instead, they’ve been hardware-related. We put a lot of work into making it safe to make changes of all kinds to Etsy.com, and treat every change as a possible breaking point.  For every type of technical change, we have answers to questions like: What problem does the change      solve? Has this kind of change      happened before? Is there a successful history? When is the change going to start?      When is it expected to end? What is the expected effect of      this change on the Etsy community? Is a downtime required for the change? What is the rollback plan, if      something goes wrong? What test is needed to make      sure that the change succeeded? As with all change, the risk involved and the answers to these questions are largely dependent on the judgment of the person at the helm. At Etsy, we believe that if we understand the likely failures, and if there’s a plan in place to fix any unexpected issues, we’ll make progress. Just as important, we also track the results of changes. We have an excellent history with respect to the number of successful changes. This is a good record that we plan on keeping. If you’ve actually read this far – congratulations. Like I said, this post was meant to be detailed about how we take site availability and outages seriously. Etsy is growing insanely fast, and keeping up with growth can sometimes be like changing the tires on a moving car. Downtime is something that happens on every web service, but that’s not an excuse to accept it. It’s a privilege to build and grow this site for such an involved and passionate community. In the same way that it’s our job as engineers to build, grow, and evolve our code and infrastructure, it’s also our job to communicate with you, our members, about that work. And in times of distress (i.e. site outages) it’s even more important. We can always be better at that. Posted by John Allspaw on September 17, 2010 Category: infrastructure , operations , people , philosophy", "date": "2010-09-17,"},
{"website": "Etsy", "title": "Etsy API Handmade Code Contest: Update #2 – Judging is happening!", "author": ["Liz Wald"], "link": "https://codeascraft.com/2010/09/21/etsy-api-handmade-code-contest/", "abstract": "Etsy API Handmade Code Contest: Update #2 – Judging is happening! Posted by Liz Wald on September 21, 2010 Note: This is an update to the original post from September 15th which is listed below. We are very happy to announce that Tarikh Korula, founder of Uncommon Projects is  joining our final judging panel for our API Handmade Code Contest.   Uncommon Projects, our neighbor here in Brooklyn, is a hardware and  software design studio that specializes in developing the technology  behind innovative, tangible and whimsical ideas.  And speaking of tangible and whimsical ideas ,  there are loads of them for the contest in the 300+ comments on our  Etsy.com blog post.  Be sure to check them out if you’re looking for  inspiration. What if you could help independent entrepreneurs make a  living doing  what they love? Or provide a passionate community of shoppers with an  iPad application or gift registry tool they’ll love? This is your chance to build an application for the passionate Etsy community using the Etsy API , and be handsomely rewarded for your creativity and effort! Etsy is sponsoring its first-ever Etsy API Handmade Code Contest, from Wednesday, September 15, 2010 at 9 a.m. ET through Friday, October 15, 2010 at 11:59:59 p.m. ET. Your ideas have the potential to reach the more than 5 million Etsy  members who, in August alone, listed over 2 million items and purchased  more than 1.4 million of those, generating more than $25 million in  sales for the community.  We’ve added lots of new features to our API,  such as  OAuth and read/write methods for listings and shops, and we  want you to give the shiny new bits a whirl. For the contest, we’re looking for applications in three broad categories: Buyer Tools: These are applications that leverage the Etsy  API’s search and taxonomy features to help connect buyers with items.   They can provide functionality based around finding Etsy listings, such  as gift registries or wish lists. Seller Tools: These are applications that use the new  OAuth-protected methods in Etsy’s API to help members manage their  shops.  These applications might help sellers upload new listings, renew  about-to-expire listings, chart their sales and more. Mobile: These are applications that bring Etsy to mobile phones and hand-held devices. You may enter more than once, however, each individual application  can only be entered in one category. (For instance, an iPhone  application that uploads listings can be entered in Mobile or Seller  Tools, but not both.)  Try to choose the category where your application  shows the most creativity and original thinking.  The official contest  rules and entry form are available from the links below: Contest Rules * Contest Entry Form To enter you must first obtain an Etsy API  key , and then you may want to join the Google group to get help and advice from the Etsy API team and your fellow  developers.  You can also get ideas from Etsy’s current members about their dream apps (check out the comments below the post).  When you’re ready, simply  create a new and fully-functional application using the exciting new   features in Etsy’s API v2 , and submit it via the Contest Entry Form referenced above. Entries will be screened for compliance with the Etsy API Terms of Use and all Etsy company policies . Applicable contest entries will then be evaluated on the following criteria: Innovation and creativity – 25%, Ease of use – 25%, Usefulness for existing and new members – 25%, Applicability to the category – 15%, and Handmade ethos – 10% On or about October 29, 2010, grand prizes of $5,000 will be  announced in each category.  Five additional $500 “honorable hack”  awards will also be awarded at the judges’ discretion. The contest is open to residents of the United States, Australia,  Canada (except Québec), Germany and the United Kingdom who are  at least  18 years of age as of September 15, 2010. Remember Etsy API Handmade Code Contest only runs through October 15, 2010 so don’t miss your chance at fame and fortune! For those about to code, we salute you! Posted by Liz Wald on September 21, 2010 Category: api , events Tags: API , contest , judging , update Related Posts Posted by Stefanie Schirmer on 26 Sep, 2016 API First Transformation at Etsy – Operations Posted by Stefanie Schirmer on 06 Sep, 2016 API First Transformation at Etsy – Concurrency", "date": "2010-09-21,"},
{"website": "Etsy", "title": "Kellan Elliott-McCrea joins the Etsy team", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2010/07/16/kellan/", "abstract": "Kellan Elliott-McCrea joins the Etsy team Posted by Chad Dickerson on July 16, 2010 Kellan Elliott-McCrea ( blog , Twitter ) will be joining us at Etsy on Monday as VP of Engineering, here at Etsy HQ in beautiful Brooklyn (where Kellan currently lives).  Kellan is awesome, and I couldn’t be happier to have him on the team at Etsy! Kellan comes to us from Flickr , where he was architect and the first developer hired after Flickr was acquired by Yahoo.  At the time, Flickr was really just ramping up to where it is today.  Flickr now has over 5 billion photos, over 50 million registered users, 45,000 photos per second served across 10 data centers,  and 220,000 database queries per second at peak.  Kellan was instrumental in building Flickr up to the huge scale it is at today, and built many of the key systems there himself.  Kellan also co-authored the OAuth spec, one of the key standards on the web today. Kellan’s work on OAuth demonstrates another one of his strengths:  he thinks deeply about the best ways to do things for the larger web and collaborates with others to design systems that solve big, important problems in simple and powerful ways. Most importantly, Kellan is a person who is abnormally focused on getting things done.  There are very few people in the world who have been on the front lines building lasting infrastructure and shipping features at the scale of a site like Flickr (Flickr experienced over 100x growth while Kellan was in the trenches there!)  Kellan is one of those rare people, and a kind, thoughtful person to boot.  Aside from his technical chops, Kellan also really gets what we are trying to do at Etsy, both in terms of our engineering culture and the larger Etsy mission.   You can see this in Kellan’s own words (part of his blog post about joining us ): I’m excited about working on another site built, sustained and funded by a passionate community (not by selling users as grist to a third party), the ridiculously huge potential to build a platform for person to person, and community based commerce, the locally sourced company lunches, a kick-ass engineering team that is also expected to work on creative side projects (and silk screen their own shirts), visiting the San Francisco and Berlin offices, and supporting the continued transformation of Etsy’s technical infrastructure which is in the process of being completely re-thought, revamped, and reinvented. We’re excited, too, for those reasons and more.  Welcome, Kellan! Posted by Chad Dickerson on July 16, 2010 Category: engineering , people", "date": "2010-07-16,"},
{"website": "Etsy", "title": "Batch Processing Millions and Millions of Images", "author": ["Mike Brittain"], "link": "https://codeascraft.com/2010/07/09/batch-processing-millions-of-images/", "abstract": "Batch Processing Millions and Millions of Images Posted by Mike Brittain on July 9, 2010 I joined Etsy back in February and knew immediately that there would be no shortage of technical challenges.  Many of our job postings for Engineering positions describe the company as a place “where the word ‘millions’ is used frequently and in many contexts”.  I got a taste of that within my first weeks on the job. We are in the process of redesigning a few of the major sections around etsy.com .  Every item being sold on the site can have up to five photos posted with it.  When a seller uploads a new photo, it’s resized automatically into six different sizes that are displayed throughout the site.  As we redesigned some pages we realized we would need to replace a few of the existing image sizes. When I started this project, there were 135 million images for items being sold on Etsy, and that number increases every minute as sellers list new items for sale.  To provide a sense of scale, let’s consider how long it would take me to resize these images by hand in Photoshop.  It takes about 40 seconds for me to find and open a file, scale it to a smaller size, and save it to a new location.  With a bit of practice, I could probably shave a couple of seconds off of that.  But at this rate it would take 170 years to resize all of those images. But here’s the spoiler… We did it in nine days.  Every single image. Images resized over a seven day period using four machines. We spent a number of days laying out this project.  We planned for how to move these images to the cloud and resize the whole batch using EC2.  We investigated resizing each photo on-demand as it was displayed on the site.  Both of these options had drawbacks for us.  Instead, we proceeded with what seemed like the simplest approach: batch process the images on our own servers. Our Weapons of Choice There are three tools that made up the majority of this project: GraphicsMagick If you’re not familiar with it, GraphicsMagick is a fork of ImageMagick that has somewhat better performance especially due to its multiprocessor support.  Its flexible command-line parameters (almost the same as ImageMagick’s) provided good opportunities for performance tuning which I’ll talk about shortly. Perl It is the “Swiss army knife”, right?  I didn’t use the GraphicsMagick-Perl library, though.  All of the resizing tasks were executed as shell commands from the Perl script I wrote.  So this really could have been written in any scripting language. Ganglia Nearly all of our projects at Etsy include metrics of one sort or another.  When testing the resizing process, I saw a decent amount of variability in speeds.  By measuring each part of the image conversion process (inspecting original images, copying images to a local ram disk, resizing, comparing to the original, and copying back to the filer), we were able to determine what processes were impacting the overall processing speed.  Another benefit to graphing all of this stuff is that with a simple dashboard you can keep everyone in the project up to date with your day-to-day progress. Tuning GraphicsMagick GraphicsMagick has a lot of command line options.  There is no shortage of dials to turn, and plenty of opportunities for balancing between file size, speed, and quality.  I started testing locally on a MacBook Pro, which was ample for working out the quality and file size settings we would use. Two hundred images from our data set were copied to local disk and used for visual comparisons as we tweaked the settings in GraphicsMagick.  Image quality is a very high priority for Etsy and our sellers, so it was difficult to trade off even a small amount of quality for faster rendering or smaller file sizes.  I provided test pages of results throughout our benchmarking for internal review — imagine an HTML page with 200 sample images displayed in rows, with seven columns of varying quality of each image.  (Here’s a tip for when you do this yourself: don’t label the quality settings, the file size, the processing time, or any other data about the images being compared.  In fact, don’t order the images according to these values.  Don’t even name the files according to any of these values.  Force your judges to make non-biased decisions.) One option I didn’t expect to be fiddling with until we got deep into testing was resampling filters.  There are a number of these in GraphicsMagick and you should test and choose one that best suits your needs – speed, file size, and quality.  We found seven filters that provided acceptable quality for our images: Blackman, Catrom, Hamming, Hanning, Hermite, Mitchell, and triangle.  I tested each of these against our sample set to determine the optimal speed and file size resulting from each filter.  Even a few seconds difference in speed when testing 200 images can equate to days, or weeks, when you’re processing millions of images. Filter File size (KB) Time (Sec) blackman 969 24 catrom 978 29 hamming 915 24 hanning 939 24 hermite 937 23 mitchell 922 29 triangle 909 23 Start with the right image file when you’re down-sizing images.  We keep full-size original images that are uploaded to the site by our sellers.  We cut these into the various image sizes we display on the site.  Let’s say these sizes are “large,” “small,” and “extra-small.”  In this project, we needed to create a “medium” size image and it seemed to make sense that we would want to cut this image directly from the original (full-size) image.  We found out, almost by accident, that using the previously down-sized “large” images resulted in better quality and faster processing than starting with the original full-size images. Compare what you find when tuning performance with a professional.  In my case, I went to the source, Bob Freisenhahn, who writes and maintains GraphicsMagick.  Bob was kind enough to provide some additional advice that improved performance for this project even more. Tuning Everything Else Armed with preliminary testing results, I moved to our production network to test some more under “real world” conditions.  There were fairly dramatic differences in the environment, specifically the machine specs and the NFS mounted storage we use for images. I was expecting CPU to be the bottleneck, but at this point my problem was NFS.  With a primed NFS cache, performance can be snappy.  But touching un-cached inodes is downright sluggish.  I checked out top while running a batch of 10,000 resize operations and saw that the CPU was barely working.  I wanted it to be pegged around 95%, but it was chilling out around one percent.  When I looked through some Ganglia metrics, it was clear we were bound by NFS seek time.  The fastest I was able to process images was five images per second. Fork to the rescue!  I rewrote the portion of the script that handled the read/resize/write operations so that it would be forked from the parent script, which spent its time looping through file names, spawning children, and reaping them when they exited.  (When you do this, make it a command-line option so you can tune it easily, e.g. “–max-children=20.”)  This made a big difference.  Lots of NFS seeks could be made in parallel.  There were enough processes running that a steady queue built up waiting for NFS to return files from disk, and another queue built up waiting for processor time.  Neither spent any time twiddling their thumbs.  The resizing speed improved to about 15 images per second.  At this rate the total working set would take 2500 hours, or 104 days, to resize.  Still not good enough. Now that we could feed enough images from NFS, we reached for more CPU — a 16 core (hyperthreaded) Nehalem server.  Problem solved, right?  Wrong.  The resizing speed on this box was actually worse, around 10 images per second.  Here’s why… Given the opportunity to use additional processors, GraphicsMagick used all of them.  To resize an original image (approx. 1.5 MB) to a thumbnail (approx. 3 KB), GraphicsMagick split up the work across 16 processors, executed each job, and reassembled the results.  This was simply too much overhead for the relatively small amount of work actually being done.  This can be fixed by tuning the OpenMP threads environment variable when running GraphicsMagick, for example: env OMP_NUM_THREADS=2 /usr/local/bin/gm ... This showed an immediate improvement, but I needed to find the sweet spot.  I had knobs for both maximum number of children (Perl script) and number of threads (GraphicsMagick) used for resize operations.  I ran a number of tests tuning each of these parameters. Using two processors per resize operation and running a maximum of 15 children yielded the best results. Note that while tuning these parameters, I tested with local files to exclude variability introduced by NFS.  We’re now closer to 262 hours (11 days) for the entire working set.  This starts to look sufficiently optimized and we can start to simply add some more iron.  Four 16-core Nehalems were used for resizing the production working set.  This may be the point where you are asking, “who has four of these boxes just lying around?”  But if you actually have 135 million images to resize, you probably have some spare hardware around, too . In production, we didn’t see the amazing rates of 140 images per second for each machine.  We still had to contend with cold seeks across NFS.  By applying all of this learning, we managed to get a fairly consistent resize rate for each running machine. Resizing images at 180 per second across four machines Summary We needed to resize about 135 million images right on our production servers.  We accomplished this using GraphicsMagick, Ganglia, Perl, and a very healthy dose of research.  In fact, the research phase of this project took longer than the batch processing itself.  That was clearly time well spent.  This well-tuned resizing process (if you missed the spoiler at the beginning of the article) took only nine days to complete.  And since first running the process, we have re-used it two more times on the same data set. By the way, I can’t end this post without also acknowledging our operations team who worked with me and help out on this project.  They are amazing.  But don’t just take my word for it, come find out for yourself . Posted by Mike Brittain on July 9, 2010 Category: engineering", "date": "2010-07-9,"},
{"website": "Etsy", "title": "Etsy is Hiring", "author": ["Dan McKinley"], "link": "https://codeascraft.com/2010/07/09/etsy-is-hiring/", "abstract": "Etsy is Hiring Posted by Dan McKinley on July 9, 2010 Ever wonder what goes on inside Etsy? Any questions you may have should be answered by this video. News Flash: Etsy is hiring! from Etsy on Vimeo . Interested? Apply here ! Posted by Dan McKinley on July 9, 2010 Category: engineering", "date": "2010-07-9,"},
{"website": "Etsy", "title": "MongoDB at Etsy, Part 2", "author": ["John Allspaw"], "link": "https://codeascraft.com/2010/07/03/mongodb-at-etsy-part-2/", "abstract": "MongoDB at Etsy, Part 2 Posted by John Allspaw on July 3, 2010 Note: In 2020 we updated this post to adopt more inclusive language. Going forward, we’ll use “primary/replica” in our Code as Craft entries. Dan and Wil posted a while ago about how we’re using MongoDB at Etsy . We’ve been using MongoDB as the backing store for Treasury , and over the course of two and a half months , we’ve had over 50,000 excellent Treasuries created by the Etsy community, all stored and served in MongoDB. Not so bad. I thought I’d post a bit about what I like about MongoDB, operationally. The things that were most important to me on that front were: Stability Sane behavior under increasing load Metrics availability Familiar replication behavior Of course the first thing that was of concern was stability . Who wants to implement something crashy?  We got enough honest reports of stability from other folks with MongoDB in production and our experience is no different. This is in large part due to the activity on the mongodb-user list , as well as talking to folks who have already put it into production . Performance One of the things that Dan touched on was that MongoDB behaves well when the working set of data exceeds available physical RAM. The folks over at 10gen obviously have put a good deal of thought into this, because otherwise you wouldn’t be able to call it “humongous”. 🙂 While the mechanics of how data is buffered to RAM and persists to disk appear to be different, the behavior appears to be along the same lines as InnoDB’s buffer pooling and persistence. Which is to say, query performance is excellent when the database is small enough to keep entirely in RAM, but when the data blows past RAM, the performance then plateaus, limited only by your disk I/O subsystem. This is preferable and familiar; there’s no massive drop (or crash) in performance, there’s only a nice plateau of response at the bound-by-disk condition. When we first evaluated MongoDB, we loaded up all of the ‘ favorites ‘ data into a MongoDB instance (it’s currently kept in Postgres in production) which was well over 40G of data. We then hammered it with 5000 random queries at a time, constantly. The machine has 8 15K SAS spindles, RAID10, and only 16G of RAM.  We watched the query performance as the filesystem cache filled to 16G, and once it hit 16G, the performance leveled out, consistent with a purely disk-bound workload. We saw the same results when we loaded up 70G worth of Etsy’s listing data and performed the same test. This is what you’d normally expect with any sane datastore that is expected to scale with large (and growing) working sets of persisted data. Why is this important? Because any datastore can be fast when its working set is in RAM, but we need to prepare for exceeding that, and not relying on vertically scaling our machines to handle increasing traffic. Metrics The other piece that I appreciate in MongoDB is the metrics it exposes about its operations, and how it’s easy to get at them. There’s currently an http interface at http://127.0.0.1:28017/_status that dumps a good deal of parse-able information. We use these metrics not just for alerting thresholds (on current connections, replica replication lag) but also for gathering statistics for all of the Mongo operations being done. Wil has a ganglia gmetric script that parses this data to put into Ganglia.  He’s open-sourced the gmetric and it’s on github . This way, we can correlate what CPU, memory, disk, etc. values look like with increasing usage across all of the various mongo metrics.  Context is everything. 🙂 Familiarity Having come from a LAMP background, I’m used to having database replication be a given. I realized the other day that the last time I worked somewhere replication of data wasn’t an absolute requirement, it was 2003. So when we first evaluated MongoDB, replication was one of the first things I was happy to see. Replication is super simple, and for anyone familiar with MySQL’s built-in replication, all of the fundamentals apply: MongoDB writes to an oplog collection for transactions which is then used as the replication stream between primaries and replicas. The oplog is actually a database, unlike binlogs in MySQL-land. This means you can query it and inspect it in the same way you would any database in Mongo. Dan’s got a project up on github that allows you to inspect and manipulate the oplog easily, and he’ll be adding more abilities to it in the future. This will be familiar to those who have worked with mysqlbinlog in that it provides an easy way to juggle replication events in recovery or troubleshooting scenarios. Dan didn’t write that tool out of curiosity. It turns out that we were going to need something like that soon enough.  Stay tuned for Part 3 of this Mongodb Etsy story…. Posted by John Allspaw on July 3, 2010 Category: databases , operations", "date": "2010-07-3,"},
{"website": "Etsy", "title": "Fred Brooks (author of Mythical Man-Month) at Etsy on June 14", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2010/06/08/fred-brooks-at-etsy/", "abstract": "Fred Brooks (author of Mythical Man-Month) at Etsy on June 14 Posted by Chad Dickerson on June 8, 2010 Fred Brooks was awesome!  Here are a couple of links to blog posts by folks who were there: Fred Brooks at Etsy on Software Development and Collaboration Geek Rockstar Fred Brooks on Design Original post announcing the talk below: When I talk with technologists (and we are hiring , so I do a lot these days!), I like to know who their personal technology heroes are.  Frederick Brooks is one of mine.  Many describe his Mythical Man-Month as the most important book about software engineering ever written.  I think his essay “ No Silver Bullet ” is quite possibly the most insightful writing about our craft that I have come across. Mythical Man-Month was first published in 1975, and “No Silver Bullet” was first published in 1986, so it is nothing short of remarkable that Brooks’ work holds up so incredibly well in such a rapidly-changing discipline. I’m honored and excited that Dr. Brooks will be visiting Etsy in Brooklyn on Monday, June 14 at 6pm to talk about Design of Design: Essays of a Computer Scientist ( available now on Amazon ), his first book since the seminal Mythical Man-Month .   It’s a great read!  We will have copies of the book available at the talk, and Dr. Brooks will be signing them as time allows.  If you’re in the NYC area and care about software engineering (and you do if you’re reading this!) you won’t want to miss this rare chance. This event is open to the public, but space is limited.  Visit our Eventbrite page to sign up.  We would love to see you there.  We’ll be bringing more great technology minds like Dr. Brooks to speak at Etsy — stay tuned! Posted by Chad Dickerson on June 8, 2010 Category: events , philosophy", "date": "2010-06-8,"},
{"website": "Etsy", "title": "Join us for Etsy SF cocktails and crafting (June 10)", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2010/06/01/etsy-sf-cocktails-and-crafting-june10/", "abstract": "Join us for Etsy SF cocktails and crafting (June 10) Posted by Chad Dickerson on June 1, 2010 Etsy is opening a new engineering office in San Francisco ( interested? ), and we want to get to know our neighbors.  Having spent most of my ten years in the Bay Area working South of Market before joining Etsy in Brooklyn, I’m looking forward to being back in the neighborhood more frequently ( Mexico au Parc , anyone?)  We don’t have office space locked down just yet, but thanks to our friends at Covered Communications and Pier 38 we’re not letting that slow us down! The women from Women 2.0 will also be on hand, raising money for Women 2.0 Labs , a summer entrepreneurial incubation program. Come on by June 10th between 7 and 10 pm PT to meet me and make some sweet, geeky crafts (did someone say DIY screen-printed mousepad?) and have a cocktail or two on us. Details: When : June 10th 7-10 p.m. PT Where : Pier 38 Who : You and anyone you want to bring along! See you there! Chad Dickerson CTO, Etsy Posted by Chad Dickerson on June 1, 2010 Category: events", "date": "2010-06-1,"},
{"website": "Etsy", "title": "Quantum of Deployment", "author": ["Erik Kastner"], "link": "https://codeascraft.com/2010/05/20/quantum-of-deployment/", "abstract": "Quantum of Deployment Posted by Erik Kastner on May 20, 2010 aka. Deployinating the Country Side UPDATE 2011-07-29: Deployinator is now Open Source! Grab it on github: https://github.com/etsy/deployinator We deploy a lot of code. Deployinator is our creation to make that as easy and painless as possible. Deployinator is a one button web-based deployment app. Hit that button and code goes to our webservers and is serving requests in almost no time. Using Deployinator we’ve brought a typical web push from 3 developers, 1 operations engineer, everyone else on standby and over an hour (when things went smoothly) down to 1 person and under 2 minutes. At Etsy, we’re doing what’s come to be called Continuous Deployment . However, what we’ve learned is that having a tool like Deployinator is useful for more than just enabling that. This post is about those benefits – for anyone deploying web code. Why Our job as engineers (and ops, dev-ops, QA, support, everyone in the company actually) is to enable the business goals. We strongly feel that in order to do that you must have the ability to deploy code quickly and safely. Even if the business goals are to deploy strongly QA’d code once a month at 3am (it’s not for us, we push all the time), having a reliable and easy deployment should be non-negotiable. It’s a metric I’ve come to call the “quantum of deployment”: what’s the smallest number of steps, with the smallest number of people and the smallest amount of ceremony required to get new code running on your servers? This isn’t a trivial question. Even if you’re on a slow release cycle, and have a push engineer, what happens if there’s an emergency push needed? Does it go through your normal process, or is there a fast-lane? Do your fast-lane deployments get logged? Are they measured for speed? Is everyone aware that it happened the way they would for a normal deployment, or is it your dirty little secret? It’s not hard to get started. If you currently have a bunch of shell scripts that move everything in place, wrap those up with a single shell script. The most important thing is that it’s ONE easy step. This might require changing your process. Try to remove or replace special cases. The less thought it takes to deploy, the more you can focus on getting stuff done. Once deploying is No Big Deal, a lot of things can change. Features can go out a piece at a time instead of one all-or-nothing push. Your app configuration options can be in code – and changed quickly. Your hair can grow back. Puppies will lick your face! What Custom software? There’s a lot of choice out there, and we generally try to not reinvent the wheel whenever possible (and this wheel has been invented again and again ). After comparing our requirements with the available software, we decided to roll our own. Here’s what we were looking for: Web based Logged (When, What and Who) Adaptable to our network Run from a central location Announced in our IRC and email Transparent in regards to its actions Integrated with our graphing/monitoring tools A lot of our requirements are inspired by the way Flickr deploys, as documented in Building Scalable Websites (Written by Friend of Etsy, Cal Henderson ). For anyone deploying code (engineers, designers… anyone really), it’s an easy process. Once your code is ready to go, you go to Deployinator and push the button to get it on QA. From there it visits Princess (see the sidebar). Then, when it’s ready to go live, you hit the “Prod” button and soon your code is live, and everyone in IRC knows who pushed what code, complete with a link to the diff. For anyone not on IRC, there’s the email that everyone gets with the same information. What it looks like This is the main Deployinator screen. Here is how we deploy the “web stack”. Sidebar: What the heck is a “Princess”? Princess is our staging environment. So why didn’t we just call it “staging”? Partly, that’s just how we roll . We used to have an environment called “Staging” that was the last stop before code went live to everyone. However, it was not ideally set up; the first time your code would interact with actual production hardware was when you deployed. Princess uses production data stores, our production network and production hardware. In order to make a clean mental break from the old way, one of our rad engineers, Eric Fixler, came up with “Princess”. Here’s our IRC bot telling everyone that something went out. It also includes a link to the commits that went live. How When we first brought Deployinator online, it was just a web frontend to the shell scripts that moved everything in the right place. What we gained by putting a screen in front of it was the ability to iterate the backend without changing the experience for people deploying. Deployinator currently uses svn to update the code, then rsync to move it between environments. Another important part of Deployinator is that the environments are a one way street. Code going from Princess to production is unaffected by any commits that have happened since getting on princess. This creates something of a “ mantrap “, so that we know exactly what we’re deploying. No surprises! Deployinator isn’t used just for our web stack either. With the simple architecture we’ve built, we can add all kinds of stuff to it easily. It’s currently used for many different things such as the API , Lists service , internal admin-only tools and others. Having a single deployment process has removed a lot of complexity. When This isn’t a post about continuous deployment. Having a very simple deployment procedure is something you should do even if the thought of deploying your code 20 times a day scares you. Deployment can be a contentious subject with many stakeholders. Getting it simple and repeatable allows everyone to share a common vocabulary. For the nerds… Transporting the bits and bytes Here’s a rundown of some of the interesting parts of how Deployinator actually moves bits around. As mentioned above, this has changed and will change again. We analyze our entire process and have some low-hanging performance fruit to pick. As of today, an API push takes about 18 seconds, a Princess push takes about the same, and a production web push is 70-150 seconds. Here are the steps that a web push goes through: From the repo of truth We’re deploying directly from trunk (that’s a whole other post!). So the first step of deploying is to update the code on the deploy host. Builda what now? After the code is updated, we run “builda”, an app we wrote to take our assets and bundle them up with lots of magic. We use a combination of javascript async loading, Google’s closure and versioned directories to make things like css, javascript and sprites as fast as possible. Rsync At this point, we have a bunch of directories on our deploy host that represent our web tree. We then rsync to a staging area on each web box. Fanout At some number of boxes, rsyncing back to a single push host stops working. We’re employing a strategy called fan out. We rsync in chunks of 10 hosts at a time. This is one area where a lot of speed ups will be happening soon. First they came for our assets… Pop quiz, hotshot : Someone visits the site during a deployment and box 1 (the one they randomly get) has the new code. The html they’re returned refers to a new image. When they request that image, they end up on box 451.. which doesn’t have that asset yet. What do you do? WHAT DO YOU DO? We’ve solved this with two steps. The first (mentioned above) is versioned asset directories. The next is to get those assets on ALL hosts before ANY host has the new code referring to it. Graceful We’re using APC user caching, and expect it to have fresh data each deployment. Things like some application settings, database fields and routes are all cached for super fast lookups. However, if someone changes something in some code going out, we need to make sure that it’s fresh. We’re currently issuing a graceful restart to our apaches on each deployment. Deployinator itself One of the design goals of Deployinator has been to be as simple as possible. It’s a sinatra web app, but could really be written in anything. The script that does the svn updating (and checking out for new stacks) is in PHP and some of the straight-up simplest code possible. The commands that Deployinator runs through for each different stack are listed in ruby methods, and are mostly strings (with servers and such interpolated). It’s easy for anyone to come in and change how something works. Simple, understandable software that gets the job done. The one fancy bit of Deployinator is the streaming rack middleware that powers the live updating code window: Database DDLs aren’t code An awesome feature of Capistrano is the ability to run schema migrations as part of your deployment. At a certain scale, however, database changes become more time consuming and dangerous. All of our schema changes go through a stringent process with several checks in place. However, not all schema is defined in the database. Whenever we have schema that’s defined in code, or inside the data itself, it’s just a normal code push. Conclusion Our deployment process is a very important part of how we work at Etsy. We treat it just like our web code, databases or other “serious” things. Deployinator has helped us to get more features out faster with less defects and drama. As we triple our engineering team in 2010 (we’re hiring !), tools like this are what make it possible for us to change the world. Posted by Erik Kastner on May 20, 2010 Category: infrastructure", "date": "2010-05-20,"},
{"website": "Etsy", "title": "MongoDB at Etsy", "author": ["Dan McKinley"], "link": "https://codeascraft.com/2010/05/19/mongodb-at-etsy/", "abstract": "MongoDB at Etsy Posted by Dan McKinley on May 19, 2010 Hi! Dan McKinley and Wil Stuckey from the Etsy Curation team here. We’ll be your hosts for a three-part series about the use of MongoDB here at Etsy. The Curation Team. Well, half of it. (Photo credit: Elizabeth Weinberg .) In this, the first entry, we’ll give some background on how and why we use MongoDB, and explain our initial impressions as developers. In the second post, John Allspaw will talk about how well MongoDB is working out operationally. And not to build this up excessively or give too much away, but the third post will be a harrowing look inside web operations gone horribly awry, chock full of fear, loathing, and remediation checklists. The Application The first project using MongoDB is the new Treasury . For the unfamiliar, the Treasury is a member-curated browsing tool, originally built as a flash application. Our team has rewritten it as a modern ajax application in order to solve the scaling issues inherent in the original FMS design. Etsy homepages are most frequently chosen from the treasuries, and there are quite a few other existing or proposed features that are at least vaguely similar. So we wanted our backend to be flexible and to make something like a polymorphic, generalized “list” object as easy as possible. It was thinking along those lines that first made us consider using a schemaless database, although we would not currently consider that to be the primary benefit. Why not use a relational database? For us, building a read-heavy social application, this question should really be rephrased as “why not use MySQL?” This was actually a pretty difficult decision. MySQL is very well-understood operationally (especially by the people on our team). Replication in MySQL is pretty easy, and we love replication. There is absolutely no part of this project that is technically impossible with MySQL or, for that matter, any relational database. For us, this just came down to development speed. Ignoring everything else, the following two solutions are roughly equivalent in terms of performance. Relational Solution Document Store Solution Use a relational database, with a normalized or semi-normalized schema. When rendering a response, run a handful of queries and then aggregate the data for the object. Cache the resultant aggregate object either on a TTL or do invalidation. Return the cached copy of the aggregate object. Use a document datastore, and embed sub-objects or child lists within their parents. When rendering a response, retrieve the document by key and return it. In our case, the development time saved using a document database is worth the risks. Caching at many levels is of course still a part of our application, but so far we’ve not found any reason to cache a single MongoDB document retrieved by primary key in an external cache like memcached, a practice that is currently common for us when we use relational databases. Why MongoDB? The number of databases that could be used for this kind of project have, um, proliferated somewhat recently. Why would we choose MongoDB over all of the others? Well, there were a few characteristics that we knew that we wanted: The database should be safe to use as the system of record. In other words, it will not be storing data that is essentially replicated from other locations. We need the data on disk, backed up, and to have reasonable operational guarantees when there are hardware failures or the process is killed. This requirement also imposes certain constraints on the database’s maturity–we had to rule out CouchDB because of the possibility that the storage format would change before it came out of alpha. The database performance should degrade gracefully when the data volume exceeds available RAM (this rules out some contenders, such as Tokyo Cabinet ). Our tests found MongoDB to be a sweet spot between reliability, speed, and maturity. But to be clear, this was the picture six months ago when we started prototyping. The world of document datastores since then has changed significantly even in that time frame. Today, the choice would probably be more difficult. And to be perfectly honest, the proximity of 10gen to Etsy’s Brooklyn headquarters as well as the responsiveness of Eliot and his team to questions was also a factor in our decision. (In the interest of full disclosure: 10gen shares investors with Etsy.) Stay Tuned In upcoming posts, we’ll dive deeper into our production experience with MongoDB. In short, things are going well, but we have learned some lessons the hard way. Hopefully, we can help you avoid making the same mistakes. Next time, John Allspaw will talk about MongoDB from the perspective of an operations professional. See you then! Posted by Dan McKinley on May 19, 2010 Category: data , databases , infrastructure , operations Tags: Curation , MongoDB , NoSQL", "date": "2010-05-19,"},
{"website": "Etsy", "title": "Announcing Etsy’s New API", "author": ["stunji"], "link": "https://codeascraft.com/2010/04/22/announcing-etsys-new-api/", "abstract": "Announcing Etsy’s New API Posted by stunji on April 22, 2010 Today I’m pleased to announce the pilot program of Etsy’s new read-write API, version 2. For a few months now, we’ve made veiled references to “the next version of the API” in response to bugs and feature requests for our current API, but hadn’t made any official announcements or given a timeline.  Behind the scenes, we’ve been working on a totally-new, revamped API that will eventually replace the current one. Here’s a brief overview of some of the new features of this API: Read-Write Support via OAuth The new API supports OAuth , a way of letting 3rd-party applications log into your Etsy account without requiring you to hand out your username and password (If you’ve ever used a Twitter client like Tweetie, you’ve used OAuth.)  This means that applications written on the new API will be able to interact with your Etsy account in totally new ways. Built on Etsy’s New Platform You may have noticed that the current API seems to lag behind the rest of Etsy.com by five or ten minutes–this is because we’re caching the API results fairly heavily.  For the past year, we’ve been quietly upgrading the infrastructure behind Etsy piece-by-piece, and this process has finally caught up with the API.  The new API will not have a delay compared to the rest of the site, and will receive bugfixes and updates on a regular schedule. New, Cleaner Interface We’re doing away with the “detail levels” of the old API in favor of a new interface that lets you specify which fields and child objects to include with every request.  For instance, you’ll be able to include images within a listing, listings within a shop, or the user who owns a shop within it.  You’ll also be able to filter out fields that you don’t care about, which will keep the size of your requests at a minimum.  This is much more flexible than the old “low, medium and high” system. We’re calling this a “pilot” because not everything is finished yet.  To start off with, we’ll be opening the beta up to a small group of testers.  The top users of the current API will be included, as will everyone who’s already written in asking to become a beta tester.  Then, we’ll get to work fixing the bugs that are found and adding new features.  Every time we add new features to the API, we’ll open it up to a few more beta testers. Here’s what you’ll find in the pilot API: OAuth support Basic view of users, listings, shops and favorites Ability to add and remove favorite users and listings Inclusions (specifying which linked objects to return in results) No delay compared to Etsy site Basic keyword search support Here’s what you won’t find in the pilot (but that we’re working on:) Advanced Search support Filtering of fields (mentioned above) Resources other than those named above Add to cart / upload listings / other “write” methods In addition, the new API won’t be backwards-compatible with the current API.  The current API will remain online for the duration of the beta phase.  Once the new API launches, the current API will remain online for six additional months.  This will give you plenty of time to upgrade your applications to the new API. If you’d like to become a beta tester, send an email to < developer@etsy.com >.  We’ll hand out API keys on a first-come, first-served basis. We hope you’re as excited as we are! Posted by stunji on April 22, 2010 Category: api", "date": "2010-04-22,"},
{"website": "Etsy", "title": "Analyzing Etsy’s data with Hadoop and Cascading", "author": ["Jason Davis"], "link": "https://codeascraft.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/", "abstract": "Analyzing Etsy’s data with Hadoop and Cascading Posted by Jason Davis on February 24, 2010 Browsing Etsy’s marketplace, you can find over five million active item listings posted by 158,000 different sellers. Digging deeper, you may heart an item you really like (we have over 40MM total hearts), or maybe even buy a thing or two (2009 marketplace sales of $180.6MM). On a monthly basis, Etsy generates over 750MM page views – over 60GB of raw web logs per day. On top of this, we doubled in size last year and are continually growing. As our community has grown larger and larger, so have our data processing needs. In the process of looking for a long-term solution, we sought something that can scale not only with the magnitude of our site, but also something that enables us to ask deep questions about our data. Instead of measuring bottom line numbers – sales per day, traffic volume per url, or site-wide conversion rate – we wanted a means to perform much more specific analyses. What is the hearting rate via users who issued two consecutive searches? What fraction of users who visit Etsy through regretsy.com have never visited us before? What fraction are existing members? Hadoop’s MapReduce implementation is pervasive and seemed like the obvious choice to us. It’s used by Yahoo!, Facebook, and lots of other web shops operating at massive scale. The framework provides many fundamentals required for effective parallel processing: (1) job coordination, (2) redundant data storage & management (via HDFS), and (3) a flexible parallel programming framework to facilitate a wide array of analysis tasks. I won’t discuss Hadoop too much here — there are plenty of great resources on the web you can check out elsewhere ( Hadoop’s project page is a great starting point). MapReduce jobs consist of two operations: a map, and a reduce. However, most of the data analysis tasks that we want to solve here at Etsy are more naturally expressed using operations such as filters for discarding records, joins for merging records, splits for separating records, or grouping for aggregating and counting. The problem is that standard map and reduce operations are at a different level of abstraction from filters, joins, etc. For example, the canonical word count example used in MapReduce tutorials can be viewed in terms of two operations: (1) a split to separate records into individual terms, and (2) an aggregation on terms to count frequencies. Here, the split operation corresponds to a mapping operation, and the aggregation operation is handled by the reducer. Counting words is fairly straightforward, but Hadoop’s abstraction mismatch results in additional complexity. Enter Cascading. Cascading provides a data flow framework that leverages Hadoop’s infrastructure while abstracting standard data processing operations (split, joins, etc.) away from underlying mapper and reducer tasks. In terms of our data analysis goals at Etsy, Cascading is a great fit as it combines the scalability of Hadoop with the right level of abstraction to perform deep data diving. The project is relatively young but is currently used in several large-scale settings (check out Rapleaf’s Casading overview ), and is similar to Pig in abstracting away low-level map and reduce operations.  Unlike Pig, Cascading can be extended via its API to create a Domain Specific Languages (DSL) to streamline operations based on our particular use case or to provide more code structure and cleaner syntax (for all you Java haters). Finally, the data warehousing team at Etsy has a contingent of programming language junkies with lots of experience using JRuby, a Java implementation of Ruby. Welcome cascading.jruby ! Instead of diving into benefits and idiosyncrasies of cascading.jruby (which we’ve adopted based on Grégoire Marabout’s implementation), we’ll dive right into an example: affiliate sales tracking. Affiliate programs are standard among many online retailers today. Affiliates sign up for the program and can then set up special links to the retailer’s site with their affiliate code embedded. Affiliates are then paid on a per-sale basis, usually a fixed percentage of final sales price.  Etsy does not currently have an affiliate program, so this is purely a hypothetical example. In this example we’ll assume that affiliate tracking codes are embedded in the retailer’s url via the format affiliate_code=XYZ , and that a user is directed to a sales confirmation page immediately after their sale is completed. We’ll also assume that this confirmation page has the sales id parameter embeded in the url ( sale_confirmation_id=1234 ). Affiliates are credited only for sales occurring within ninety days from the referral. This example uses cascading.jruby to implement the entire affiliate tracking infrastructure based on existing event logs (i.e. standard apache logs). No code pushes to production required! Cascading jobs consist of three components: the data source, data sink, and one or more assemblies. Data flows from source to sink and assemblies are used to modify the flow or branch it. In this example, we’ll create two data assemblies. The first assembly consists of two branches that filter (1) URLs containing an affiliate_code parameter, and (2) sales confirmation pages with a sales_id parameter. source 'raw_log_data'\nassembly 'raw_log_data' do\n  # Helper method to parse fields from raw web logs: session_id, url parameters, etc.\n  parse_log :query_fields => ['affiliate_code', 'sale_confirmation_id']\n\n  branch 'affiliate_events' do where 'affiliate_code:string != null'\n    rename 'created_at' => 'affiliate_timestamp'\n  end\n\n  branch 'sales_events' do\n    where 'sale_confirmation_id:string != null'\n    rename 'created_at' => 'sale_timestamp'\n  end\nend In the next assembly, we’ll join the output of these two branches. assembly 'affiliate_sales' do\n  join 'affiliate_events', 'sales_events', :on => 'session_id'\n  insert 'time_diff' => expr('(sales_timestamp:double-affiliate_timestamp:double)/(24.0*60.0*60.0*1000.0)')\n  where '0.0 <= time_diff:double && time_diff:double <= 90.0' group_by 'session_id' do\n    min 'time_diff'\n  end\n  project 'affilite_code', 'sale_confirmation_id'\nend\nsink 'affiliate_sales' Here we join affiliate-linked inbound events to sales events. Valid affiliate sales credit requires that a user purchased after he clicked through an affiliate link (i.e. positive time_diff ) and within a ninety-day period. The final group_by is used to dedup sales that were linked to multiple affiliate events. In this example, we credit the affiliate who most recently linked the user through to the sale. Finally, we emit the affiliate code & sales confirmation id. In a production system, this output could be exported into a relational database to key off appropriate tables and complete the final sales commission assignment steps (i.e. determine sale price for sales events, compute commissions, etc.). As you can see from this example, our cascading.jruby framework provides a clean interface to compute complex workflows with minimal coding complexity. Our full stack runs on Amazon’s Elastic MapReduce web service. We store exported web logs on Amazon S3, and Amazon’s EMR takes care of behind-the-scenes cluster management. We’ve had no problem processing a month’s worth of data using 25 machines in a few hours time. We’re hoping to be able to release our cascading.jruby distribution sometime in the near future. In the meantime, you can use our example as pseudo-code for constructing a Cascading job using their standard API. Stay tuned! Posted by Jason Davis on February 24, 2010 Category: data , infrastructure Tags: analytics , hadoop Related Posts Posted by David Schott on 07 Nov, 2018 Double-bucketing in A/B Testing Posted by Callie McRee and Kelly Shen on 03 Oct, 2018 How Etsy Handles Peeking in A/B Testing Posted by Daniel Marks , Bill Ulammandakh and Sungwan Jo on 25 Jan, 2017 Optimizing Meta Descriptions, H1s and Title Tags: Lessons from Multivariate SEO Testing at Etsy", "date": "2010-02-24,"},
{"website": "Etsy", "title": "Dev / ops cooperation", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2010/02/17/dev-ops-cooperation/", "abstract": "Dev / ops cooperation Posted by Chad Dickerson on February 17, 2010 The authors of Peopleware (a must-read) wrote: “The major problems of our work are not so much technological as sociological in nature.” At Etsy, cooperation between the dev and ops teams is important to me as CTO for one simple reason: things work better that way. When a productive fluidity exists between those two teams, things get done faster and better. The simple matters that trip up engineering teams are less likely when the developers and ops team not only get along well, but see themselves as partners with the same goals. Yes, this sounds obvious, but it is surprising how often this is not the case, and this “sociological” problem creates all sorts of technology problems. It doesn’t have to be that way, and shouldn’t be that way. Earlier this year, John Allspaw joined Etsy after almost five years leading the operations team at Flickr. John has spoken extensively on the subject of dev/ops cooperation, including in the Agile Executive Podcast just last week. John and Paul Hammond of Flickr also delivered one of the most useful talks at Velocity last year: 10+ Deploys Per Day: Dev and Ops Cooperation at Flickr ( slides , video ). Read and watch the whole thing if you haven’t already. We’re applying many of the same principles here at Etsy, and I’m looking forward to having John and other members of the team write about our experiences here. Posted by Chad Dickerson on February 17, 2010 Category: operations , philosophy", "date": "2010-02-17,"},
{"website": "Etsy", "title": "Code as craft", "author": ["Chad Dickerson"], "link": "https://codeascraft.com/2010/02/10/code-as-craft/", "abstract": "Code as craft Posted by Chad Dickerson on February 10, 2010 At Etsy , our mission is to enable people to make a living making things.  The engineers who make Etsy make our living making something we love: software.  We think of our code as craft — hence the name of the blog.  Here we’ll write about our craft and our collective experience building and running Etsy, the world’s most vibrant handmade marketplace. In The Pragmatic Programmer (the one programming book I think every engineer should own), the authors beautifully explain the notion of code as craft and the intersection of craftsmanship and engineering: The construction of software should be an engineering discipline. However, this doesn’t preclude individual craftsmanship. Think about the large cathedrals built in Europe during the Middle Ages. Each took thousands of person-years of effort, spread over many decades. Lessons learned were passed down to the next set of builders, who advanced the state of structural engineering with their accomplishments. But the carpenters, stonecutters, carvers, and glass workers were all craftspeople, interpreting the engineering requirements to produce a whole that transcended the purely mechanical side of the construction.  It was their belief in their individual contributions that sustained the projects:  We who cut mere stones must always be envisioning cathedrals. (Quarry worker’s creed). At Etsy, we approach what we do with the same care and inspiration as the makers in our marketplace.  We talk about the tools we use, celebrate the experiments that worked, and learn from the ones that went awry, always learning new things to improve our craft.  With this blog, we plan to make some of those discussions public. Most importantly, craftsmen sign their work.  Think of this blog as us signing ours. Posted by Chad Dickerson on February 10, 2010 Category: philosophy", "date": "2010-02-10,"}
]