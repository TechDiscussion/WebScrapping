[
{"website": "Indeed-Engineering", "title": "Indeed University: Building Data-Driven Products", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2015/12/indeed-university-building-data-driven-products/", "abstract": "July 2015 marked the kickoff of Indeed University (IU), our inaugural 12-week summer program to teach Indeed’s development culture to new Indeedians. Over 50 new Indeed software engineers from our Tokyo, San Francisco, Seattle, and Austin offices took part in the program, held at our Austin headquarters. Indeed University onboarded new hires by acquainting them with Indeed’s culture, technology, and software development philosophy. Our three main goals were: Accelerate the onboarding of new college grads Provide leadership opportunities for current employees Prototype new ideas for Indeed’s business With these goals in mind, we set out to test a new means of onboarding Indeed’s software engineers. Onboarding We like to pair new hires with mentors, who help with anything from dev environment setup to version control to A/B test definition . If a new hire has a question about something beyond their mentor’s expertise, the mentor helps them find the right expert. We encourage every new engineer to get a change into production in their first week, and the mentor supports them as they navigate that learning experience. From Fall 2014 through Spring 2015, Indeed hired 51 new graduates from Computer Science and Engineering programs in the United States and Japan. With this growth, we could not sustain an onboarding process based on 1:1 mentorship. But Indeed University did not lose the emphasis on taking ownership and getting code into production. Within the first two weeks of the program, every participant had completed their “gong project,” in which they defined and implemented an A/B test on Indeed. Then they dove into a hands-on curriculum, attended informative talks, and formed teams to build new products. IUers collaborating in their main work space Gong project We take pride in our ability to test everything from simple changes to new experiences. For IU, we embraced this philosophy and offered a blank canvas for our new hires to implement an A/B test in their first week. The A/B tests aimed to help job seekers find jobs faster by emphasizing helpful filters, such as salary. Screenshot of an Indeed University A/B test that encouraged job seekers to filter by salary It’s an Indeed tradition for an engineer to ring the gong after their first line of code goes live on Indeed. At IU, everyone gathered around when a test was live. The IU participant (IUer) explained what A/B test they implemented and then rang the gong. Usually with great gusto. Ringing the gong Designing and implementing these tests provided an opportunity for these new Indeedians to bring fresh perspectives to Indeed’s core products. What better way for a new hire to learn about data-driven development than to form a hypothesis, design a test, and analyze the results? IUers got a first-hand experience with our culture of data-driven development. It was exciting to see new ideas come out of these A/B tests. Curriculum After completing the gong project, IUers launched into the curriculum – a self-paced course designed by seasoned Indeed engineers. Exercises followed each section, making the curriculum more hands-on. The curriculum began with engineering basics and covered the tools and technologies we use at Indeed to build webapps and services. Then it covered logging (using logrepo ) and data analysis (using Imhotep ). Talks IU talks introduced our product development philosophy and some interesting market opportunities. We wanted participants to think about new products Indeed could build. Paul D’Arcy presents on How People Look for Jobs (Photo: Hannah Spellman) Tech talks and product deep dives provided more specific details about infrastructure and technology. Unlike typical startups, the IU teams had immediate access to Indeed data and users, as well as tools for deployment, testing, and authentication. Social events Social activities helped IUers get to know one another. They developed lasting connections before many of them headed off to Seattle, San Francisco, and Tokyo. We planned Friday happy hours that were complete with freshly baked cookies from Tiff’s Treats, local brews, a slew of board games, and Wii U. We also took our new hires to our favorite local restaurants, including Franklin Barbecue, Torchy’s Tacos, and Michi Ramen. We went to a Round Rock Express baseball game. We also did standup paddleboarding , go-karting, and paintball. Last, but not least, we visited the Austin Panic Room . Standup Paddle Board on Town Lake in Austin Enjoying Franklin Barbecue Developing new leaders Twelve emerging leaders from the engineering, product, and online marketing organizations joined Indeed University to teach best practices and our style of iterative, data-driven development. These leads gained experience by directly managing 4-5 new hires. They also each advised up to 3 product teams. The experience gave the leads their first taste of engineering management, including weekly 1-on-1s and quarterly evaluations. The 1:4 ratio allowed leads to develop relationships with the new hires. IUers talked with their leads about product and technology challenges as well as more mundane concerns like finding an apartment. As product team advisors, leads challenged themselves to teach rather than tell. They encouraged teams to plan product iterations, prioritize issues, design tests, and analyze results. In other words, leads taught participants how to think like Indeed engineers. They encouraged teams to be independent but unafraid to ask questions, to take risks and use data to measure outcomes, and to take ownership of their products. New products We encourage all engineers to imagine product changes that focus on our mission: helping people get jobs. IUers brainstormed new product ideas for Indeed. We held three brainstorming sessions. After each session, groups pitched solutions to problems they had identified. Brainstorming wrapped up with a final round of pitches to Indeed’s senior leadership team, Indeed University leads, and other interested Indeedians. Brainstorming in the IU lounge Following brainstorming, IUers formed teams based on their interests and spent the remaining 9 weeks building working products. They served as their own product managers, team leads, designers, marketers, and testers. At Indeed, we believe we must explore as many product ideas as we can, as quickly as we can. IU immersed participants in this culture of engineering velocity. Before product development began, every team researched their market. Teams created Google Surveys, called Indeed employers, and ventured to local shopping malls to speak with retail managers and employees directly. These conversations challenged some assumptions, helped drive initial product direction, and caused one early product pivot . We challenged each team to build the minimal viable product (MVP) that would allow them to validate their idea. How would they demonstrate value? One word: data. Teams needed to collect data in order to confirm their product’s value. Indeed University teams had a few “unfair advantages.” For one, they had Indeed’s data at their fingertips. They could use this data to do preliminary research or to tailor their design to a specific audience. Second, each team had a generous online marketing budget for traffic acquisition. Teams tested showing ads on Google, Facebook, and Indeed job search, to see where they could best connect with potential users. Teams presented their work at weekly product reviews with Indeed executives and IU leads. In these weekly reviews, teams answered the following questions: What did you do this week? What data did you collect? What did you learn from this data? What are you doing based on the data? In these meetings, we encouraged teams to start small, validate with data, and iterate. They learned that building successful software is about more than software architecture. The product reviews encouraged discussion of A/B test results and opportunities for testing assumptions. Many teams experienced high bounce rates on their landing pages, and they ran A/B tests to test their hypotheses about these bouncing users. One team tested a new call to action on their landing page. Another tested delaying the sign-in requirement, allowing job seekers to experience the product before creating an account. In the course of three months of Indeed University, the new hires built eleven new products: A search engine for college students to find jobs A data trends exploration tool for HR professionals A site that helps high school students choose a college major based on jobs that interest them A product that uses social sentiment analysis to give job seekers unfiltered comments about company reputation A hiring tool that helps employers manage active candidate statuses A product that asks job seekers a series of questions in order to match them to jobs based on their interests An application that allows employees to track progress toward their goals A mobile app for finding local retail and food service jobs An automated phone screening solution to help employers efficiently evaluate candidates A gig marketplace for job seekers to find small jobs in their area A site that lets people visually explore career paths based on a current or desired position Graduation At the Indeed University Graduation Party, each team introduced the product they built and presented their learnings. Five products continued beyond IU for product validation through further development, testing, and iteration. An IU team talks about what they learned at the Graduation Party After IU, the participants joined their new teams in Tokyo, Seattle, San Francisco, and Austin. They take with them the connections, skills, and knowledge they gained during the program. They are on their way to having a real impact at Indeed. Whether you are a student or an industry veteran, we are looking for talented engineers to help with our mission of helping people get jobs. To learn more, check out our open positions .", "date": "2015-12-31"},
{"website": "Indeed-Engineering", "title": "A Funny Thing Happened on the Way to Java 8", "author": [" by Bill Smith"], "link": "https://engineering.indeedblog.com/blog/2016/09/job-search-web-app-java-8-migration/", "abstract": "Indeed is migrating from Java 7 to Java 8. We’re migrating in phases, starting with running our Java 7 compiled binaries on JRE 8, the Java 8 runtime. Rather than switch everything at once, we started with a few canary apps to get a sense for the kinds of problems we might find. We selected our job search web app as our first canary. Last May, we switched a single job search server in production to JRE 8. We’d been running with JRE 8 in a QA environment for several weeks without any trouble. But when we switched to JRE 8 in production, the system load rose from 5 (a normal level) to 20 (an abnormal level): high enough to require a fix before proceeding. Fixing the problem required unusual measures. Production metrics in Datadog I started by looking at Datadog , a third-party, cloud-based metric and event correlation service that we use for real-time monitoring. The following Datadog graph shows the system load by host for every job search server in one of our data centers. The outlier (blue line) is the server running JRE 8. Figure 1. System load for job search servers in a data center Here is the median response time by host. Again, the server running JRE 8 is the outlier. Figure 2. Median response time by host The job search servers that were still on JRE 7 had normal system loads and normal median response times, so the problem was specific to the JRE 8 server. It's not unusual for a Java process to slow down because of garbage collection (GC) activity. We publish our GC metrics to Datadog, and the GC graphs for the JRE 8 instances looked normal. Production log files Next, I checked the web application log files. I retrieved every log message from the four hours when we used JRE 8, plus the log messages for the previous 24 hours. There were too many messages to read by hand. Instead, I focused on changes in the distribution of errors. I extracted the name of the associated Java class for each error and then generated histograms for different time periods. The histograms showed a slight increase in memcached client timeouts during the JRE 8 period. They also showed we used an open-source memcached client library that is no longer maintained. A Google search didn’t turn up any JRE 8 specific problems with the library. I glanced through its source code but didn’t see any problems. I decided that the increase in timeouts was most likely just a side effect of the higher system load. Reproducing the problem in QA Since the evidence from our production environment didn’t identify the problem, the next step was to try to reproduce it in our QA environment. As I mentioned earlier, the web app had been running normally on JRE 8 in QA. That wasn’t surprising, because performance problems might not manifest themselves until the system is under enough load, and our QA servers don’t get as much traffic as our production servers. I hoped that if I increased traffic beyond normal QA levels, we would see the same symptoms. I queried our Imhotep system to find the relative frequencies of our most common incoming requests. Then, using JMeter , I simulated that mixture of traffic and ran it twice while monitoring system load, once each with instances on JRE 7 and JRE 8. In both cases, the web app processed about 50 requests per second. The system load was about the same in both cases, too. Fifty requests per second is a higher traffic load than we normally simulate in QA, but it is still not at the level we receive in production. When we started this effort, our production job search instances each served approximately 350 requests per second. To reproduce the problem in QA, I would need to get closer to that traffic volume. Ruling out JMeter as a bottleneck Why was our application pegged at 50 requests/sec? Before blaming the QA environment, I needed to rule out JMeter as the bottleneck. I installed JMeter on a second server, configured it like the first JMeter instance, and then ran both at the same time. If the two instances combined had run at more than 50 requests/sec, I would know JMeter was the bottleneck. In that case, the next step would be to add more JMeter instances until we either saturated the web app or caused the system load to rise to 20. It turned out JMeter wasn’t the bottleneck. When I ran both instances at the same time, the aggregate throughput was still 50 requests/sec. Eliminating slow services The job search web app depends on 22 services. If some of them are slow enough, the web app will slow down, too. A service in QA may be slower than its production counterpart. We sometimes replace slow services with fake but fast versions when isolating the dependent application for performance analysis. I decided to create fake versions of the five services that our application uses the most. Using the new fake services, JMeter drove the webapp at 100 requests/sec. JRE 8 was about 15% slower than JRE 7, but the system load still wasn’t very high -- and the difference between JRE 7 and JRE 8 varied a lot from one run to the next. Still, since the numbers were different, I profiled the performance of both configurations using YourKit. YourKit YourKit is a Java profiler you attach to a Java process to collect and analyze runtime statistics. Once attached, you can create a performance snapshot: an aggregation of stack traces from every Java thread from multiple points in time. YourKit provides multiple ways to visualize a performance snapshot, including a hot spot view that shows which methods consumed the most time. Since our web app on JRE 8 consumed a lot more CPU than on JRE 7, I expected the respective hot spot views to differ. The views were about the same, presumably because I still hadn’t put the systems under enough load in QA. Taking performance profiles in production At this point, I could have invested more time trying to reproduce the problem in QA, but instead I decided to use YourKit in production. We took performance snapshots in production with both JRE 7 and JRE 8. The only significant difference between the two was that the JRE 8 configuration seemed to use a lot more CPU. In fact, it used much more CPU than the performance snapshot could account for. It was as if the application was busy doing something in the background that YourKit couldn’t measure. We then decided to try profiling with the perf command. Using the perf command A Java process is more than a collection of Java code; it’s also a JVM composed of multi-threaded C++. A Java profiler can’t measure what that C++ code does; for that, you need a different kind of profiler, like the Linux perf command . The perf command can collect call stacks from a Linux process. It uses the process’s debug symbol table to map a call stack’s addresses from virtual addresses to symbolic names of functions and methods. The JVM binary doesn’t contain a debug symbol table for your Java code, but there’s a way to query a Java process for its Java debug symbol table, too. In production, we configured perf to capture the call stack for every web app thread every ~10ms for 30 seconds. That yielded about 11,000 call stacks. I aggregated the data into a flame graph (Figure 3). In the flame graph, the y axis is stack depth. Any box at the top represents code that was on CPU when a sample was taken. The box below it is its caller, and so on. The flame graph in Figure 3 is really tall because our call stacks are deep. The x axis is just an alphabetical sort. The wider the box, the more call stacks it appears in. Green boxes are Java code, yellow is C++ code in the JVM, and red is C code in the JVM. Lightness and saturation are chosen for contrast and have no special meaning. Figure 3. Flame graph showing job search call stacks Figure 4 shows the bottom portion of the flame graph: Figure 4. Flame graph zoomed view The CompileBroker::compiler_thread_loop (left, in yellow area) appears in 39% of call stacks. That is the bytecode compiler, the part of the JVM that converts frequently used bytecode into machine instructions. The value 39% seemed like a lot, and it represented something in the background that we couldn’t measure with YourKit. Why would it consume so much time? The bytecode compiler writes its machine code to a dedicated area of memory called the codecache, which has a configurable size. There are JVM settings to configure what happens when the codecache fills up, but by default, the JVM is supposed to make room by flushing older entries. YourKit showed that our codecache was close to full, so it seemed worthwhile to try increasing the maximum size. Doubling the codecache We doubled the codecache size in production from 64MB to 128MB, restarted the web app, and let it run over the weekend. The application behaved normally and the system load stayed at a normal level. We ran the perf command and flame-graphed the results (Figure 5). Figure 5. Flame graph zoomed view after doubling the codecache The width of the CompileBroker::compiler_thread_loop box (right) shows that the HotSpot compiler was less busy than before: about 20% of call stacks vs. about 40% before. Codecache fluctuation Even with a 128MB codecache, we eventually ran into trouble. Figure 6 shows what happened. The flat blue line is the maximum codecache size, and the purple curve below it is the amount of codecache used. At about 8am, the usage curve began to fluctuate and didn’t stabilize again until about 3am the next day. Figure 6. Codecache graph from Datadog showing fluctuation The codecache churn slowed response times. Figure 7 shows the median response times for every job search server in that data center. The outlier in orange is the JRE 8 server. It’s about 50% worse than the rest. Figure 7. Job search web app median response times The right size for our web app Our new codecache size of 128MB was still only half of the Java 8 default. To try to address the performance problem, we decided to double the size again to 256MB. We hoped our codecache usage would level off below this size. If 256MB still wasn’t enough, we could try doubling once more. If that didn’t work, we would experiment with JVM optimizer settings to reduce the codecache usage. One factor in our favor was our deployment schedule. We usually redeploy this web app at least once a week and often multiple times per week. Aside from the Christmas holidays, we never go two weeks without redeploying. We decided to configure a total of 8 instances with a 256MB codecache and let them run for at least a week. We didn’t detect any problems, so we declared it a success. Why more codecache? The default codecache size for JRE 8 is about 250MB, about five times larger than the 48MB default for JRE 7. Our experience is that JRE 8 needs that extra codecache. We have switched about ten services to JRE 8 so far, and all of them use about four times more codecache than before. One big contributor to the 4x multiplier is tiered compilation. The JVM has two bytecode compilers: C1, optimized for fast startup, and C2, optimized for throughput of long-running processes. In earlier Java versions, C1 and C2 were mutually exclusive; you chose C1 with the ‑client switch or C2 with the ‑server switch. Java 7 introduced tiered compilation, which runs C1 at startup and then switches to C2 for the remainder of the process. Tiered compilation is intended to boost startup times for long-running server processes and, according to Oracle, can yield better long-term throughput than C2 alone. In Java 7 you have to explicitly enable it, but in Java 8 it is on by default. In QA, when I disabled tiered compilation on JRE 8, the codecache size dropped by 50%. Java 8 also uses more codecache because it optimizes more aggressively than Java 7 does. For example, the InlineSmallCode option controls how short a previous compiled method needs to be in order to be inlined. Java 8 raised the default from 1000 to 2000, which means the Java 8 optimizer allows longer methods to be inlined than before. If you inline more methods, and the inlined methods are bigger, you need more codecache. Do developers migrating to Java 8 need to worry about this? If you don’t override the default codecache size and have 100MB per JVM to spare, you can probably ignore the codecache increase. Most Indeed services do not override the default codecache size. They will use more codecache on Java 8 than on Java 7, but the codecache is still a lot smaller than our average heap size. If you do override the default codecache size but are not memory-constrained, it will probably suffice to remove the override. I recommend monitoring your codecache size just to be safe. This Github project includes an example that shows how to upload JMX codecache statistics to Datadog. If you are already memory-constrained, you may want to measure how disabling tiered compilation affects your startup time, long-term performance, and memory consumption. Acknowledgments Connor Kelly and Mike Salsone in our Operations team supported this project, patiently putting up with lots of unusual requests. Indeed software engineers David Wahler and Chen Yang helped me interpret the YourKit profiles. David also suggested using the perf command and helped me interpret the flame graphs. References Brendan Gregg is a Netflix engineer who specializes in performance measurement. His website is full of performance-related articles, and his talks on YouTube are worth watching. Brendan popularized using flame graphs for visualizing performance data. He also contributed JVM changes that made it possible to view Java symbols in perf data . When I first learned about codecache, I checked the Oracle documentation for conceptual material and relevant JVM settings: Summary of the JVM settings relating to codecache (but for the embedded version of Java, and some of the defaults are wrong for the server JVM) Server JVM defaults for codecache settings (less detailed than above) Codecache bugs fixed for Java 8", "date": "2016-09-14"},
{"website": "Indeed-Engineering", "title": "IndeedEng San Francisco: 600% Growth in 2015", "author": [" by Dan Heller"], "link": "https://engineering.indeedblog.com/blog/2015/12/indeed-engineering-san-francisco-2015/", "abstract": "Indeed San Francisco is growing! It's been a great year and we've enjoyed a lot of growth -- starting the year with 5 people and ending with over 30! A mixer with the San Francisco and San Mateo offices The Indeed San Francisco office is primarily focused on product development in these areas: My Jobs provides job seekers with tools to more effectively organize and manage their search for a new job. The My Jobs team uses modern web technologies like ReactJS and ES6 to create an engaging user experience across desktop and mobile. The Candidate Quality team builds models to predict how good of a fit a job seeker is for an opening. These models help employers quickly assess candidates who apply to their jobs. We also use these models to ensure that our job search products are helping people find jobs that they're qualified for. The Notifications team works on Indeed's Job Alerts as well as our internal systems and infrastructure for high-volume email and mobile push notifications. These systems generate and deliver almost a billion emails to job seekers every week. New product teams use rapid prototyping and iteration to investigate product ideas. The teams identify opportunities for new products, build and launch working implementations, and collect and analyze data to validate their ideas while iterating on new features. We're always looking for new ways to connect job seekers with their ideal jobs. Just like all teams at Indeed, the teams in San Francisco are small, work full-stack, deploy multiple times per week , and use our powerful experimentation and analytics tools -- Proctor and Imhotep -- to continuously test ideas and improve our products. This year we moved to a larger space on Market St , had two hackathons (one at a cabin in the Sierra Nevadas), took a trip to the beach in Santa Cruz, and had a holiday party on the Embarcadero. View from our office The site of our recent hackathon in the Sierra Nevadas As we grow, Indeed San Francisco will take on more projects to help people get jobs. If you like that mission and want to help us tackle some really interesting problems, check out our open positions !", "date": "2015-12-18"},
{"website": "Indeed-Engineering", "title": "Luck, Latitude, or Lemons? How Indeed Locates for Low Latency", "author": [" by Andrew Noonan"], "link": "https://engineering.indeedblog.com/blog/2015/09/luck-latitude-or-lemons-how-indeed-locates-for-low-latency/", "abstract": "Indeed likes being fast. Similar to published studies ( Speed Matters and Velocity and the Bottom Line ), our internal numbers validate the benefits of speed. It makes sense: a snappy site allows job seekers to achieve their goals with less frustration and wasted time. Application processing time, however, is only part of the story, and in many cases it is not even the most significant delay. The network time – getting the request from the browser and then the data back again – is often the biggest time sink. How do you minimize network time? Engineers use all sorts of tricks and libraries to compress content and load things asynchronously. At some point, however, the laws of physics sneak in, and you just need to get your data center and your users communicating faster. Sometimes, your product runs in a single data center, and the physical proximity of that data center is valuable. In this case, moving is not an option. Perhaps you can do some caching or use a CDN for static resources. For those who are less tied to a physical location, or, like Indeed, run their site out of multiple data centers, a different data center location may be the key. But how do you choose where to go? The straightforward methods are: Word of Mouth . The price is good and you’ve talked to other customers from the data center. They seem satisfied. The list of Internet carriers the data center provide seems comprehensive. It’s probably a good fit for your users … if you’re lucky. Location . You have a lot of American users on the East Coast. Getting a data center close to them, say in the New York area, should help make things faster for the East Coast. Prepare to be disappointed. These aren’t bad reasons to pick a data center, but the Internet isn’t based on geography – it’s based on peering points, politics, and price. If it’s cheaper for your customer’s ISP to route New York through New Jersey because they have dedicated fiber to a facility they own, they’re probably going to do that, regardless of how physically close your data center is to the person accessing your site. The Internet’s “series of tubes” don’t always connect where you’d think. What we did In October of 2012, Indeed faced a similar quandary. We had a few data centers spread out across the U.S., but the West Coast facility was almost full, and the provider warned that they were going to have a hard time with our predicted growth. The Operations team was eager to look at alternate data centers, but we also didn’t want to make things slower for the West Coast users. So we set up test servers in a few data centers. We pinged the test servers from as many places as we could, comparing the results to the ping times of the original data center. This wasn’t a terrible approach, but it also didn’t mimic the job seeker’s experience. Meanwhile, other departments were thinking about the problem too. A casual hallway conversation with an engineering manager snowballed into the method we use today. It was important to use real user requests to test possible new locations. After all, what better measure would there be to how users perceive a data center than those same users? After a few rounds of discussion, and some Dev and Ops time, we came up with the Fruits Test, named for the fruit-based hostnames of our test servers. Utilizing this technique, we estimated that the proposed new data center would shave an average of 30 milliseconds off of the response time for most of our West Coast job seekers. We validated this number once we migrated our entire footprint to the new facilities. How it works First, we assess a potential data center for eligibility. It doesn’t make sense to run a test against an environment that’s unsuitable because of space or cost. After clearing that hurdle, we set up a lightweight Linux system with a web server. This web server has a single virtual host named after a fruit, such as lemon.indeed.com . We set up the virtual host to serve ten static JavaScript files, named 0.js , 1.js , etc., up to 9.js . Once the server is ready, we set up a test matrix in Proctor , our open-sourced A/B testing framework. We assign a fruit and a percentage to each test bucket. Then, each request to the site is randomly assigned to one of the test buckets based on the percentages. Each fruit corresponds to a data center being tested (whether new or existing). We publish the test matrix to Production, and then the fun begins! Figure 1: Fruits test requests, responses, and logging Legend The site instructs the client to perform the fruits test. The 0.js request and response call dcDNSCallback . dcDNSCallback sends the latency of the 0.js request to the site. The [1-9].js request and response call dcPingCallback . dcPingCallback sends the latency of the [1-9].js request to the site. Requests in the test bucket receive JavaScript instructing their browser to start a timer and load the 0.js file from their selected fruit site. This file includes a blank comment and an instruction to call the dcDNSCallback function. On lemon.indeed.com , it passes in \"l\" to indicate the test fruit: /*\r\n\r\n*/\r\ndcDnsCallback(\"l\"); dcDnsCallback then stops the previous timer, and sends a request to indeed.com , which triggers a log event with the recorded request latency. The dcDnsCallback function serves two purposes. Since the user’s system may not have the fruit hostname’s IP address in its DNS cache, we can get an idea of how long it takes to do a DNS lookup and a single request round trip. Then, subsequent requests to that fruit host within this session won’t have DNS lookup time as a significant variable, making those timing results more precise. After the dcDnsCallback invocation, the test selects one of the 9 static JavaScript files at random and repeats the same process: start timer, get the file, run function in the file. These files look a little bit like: /*\r\n3firaei1udgihufif5ly7zbsqyz59ghisb13u1j26tkffr7h67ppywg12lfkg7ortt5t3xoq5\r\n*/\r\ndcPingCallback(\"l\"); These 9 files ( 1.js through 9.js ) are basically the same as 0.js , but call a dcPingCallback function instead, and contain a comment whose length makes the overall response bulk up to a predefined size. The smallest, 1.js is just 26 bytes, and 9.js comes in at a hefty 50 kilobytes. Having different sized files helps us suss out areas where latency may be low, but available bandwidth is limited enough that getting larger files takes a disproportionately long time. It also can identify areas where bandwidth is plentiful enough that the initial TCP connection setup is the most time-consuming aspect of the transaction. Once the dcPingCallback function is executed, the timer is stopped and the information about which fruit, which JavaScript file, and how long the operation took is sent to Indeed to be logged. These requests are all placed at the end of the browser’s page rendering and executed asynchronously to minimize the impact of the test on the user’s experience. On indeed.com , the logging endpoint receives this data and records it, along with the source IP address and the site the user is on. We then write the information to a specially formatted logstore that Indeed calls the LogRepo – mysterious name, I know. After collecting the LogRepo logs, we build indexes from them using Imhotep , which allows for easy querying and graphing. Depending on the nature of the test, we usually let the fruits test run for a couple of weeks, collecting hundreds of thousands or even millions of samples from real job seekers that we can use to make a more informed decision. When the test has run its course, we just turn off the Proctor test and shut down the fruit test server. That’s it! No additional infrastructure changes needed. One of the nice things about this approach is that it is flexible for other types of tests. Sure, we mainly use it for testing new data center locations, but when you boil it down to its essentials (fruit jam!), all the test does is download a set amount of data from a random sampling of users and tell you how long it took. Interpreting the results is up to the test designer. Rather than testing data centers, you could test two different caching technologies, or the performance difference between different versions of web or app servers, or the geographic distribution of an Anycast/BGP IP (we’ve done that last one before). As long as the sample size is large enough to be statistically diverse, it makes for a valid comparison, and from the perspective of the best people to ask: your users. That’s nice, but why “Fruits Test”? When we were discussing unique names to represent potential and current data center locations, we wanted names that were: easily identifiable to Operations a little bit obscure to users, but not too mysterious not meaningful for the business As a placeholder while designing things, we used fruits since it was fairly easy to come up with different fruits for letters of the alphabet. Over the course of the design the names became endearing and they stuck. Now I relish opening up tickets to enable tests for jujube, quince (my favorite), and elderberry! Now what? Now that we have a pile of data, we graph the heck out of it! But more about that in Part 2 of the Fruits Test series.", "date": "2015-09-16"},
{"website": "Indeed-Engineering", "title": "Status: A Java Library For Robust System Status Health Checks", "author": [" by Mya"], "link": "https://engineering.indeedblog.com/blog/2015/07/status-java-library-for-system-status-health-checks/", "abstract": "We are excited to highlight the open source availability of Status , a Java library that can report a system’s status in a readable format. The Status library enables dynamic health checks and monitoring of system dependencies. In this post, we will show how to add health checks to your applications. Why use system status health checks? Health checks play an important role at Indeed. We set up and run large-scale services and applications every day. Health checks allow us to see the problematic components at an endpoint, rather than combing through logs. In production, a health check can let us know when a service is unreachable, a file is missing, or the system cannot talk with the database. Additionally, these health checks provide a controlled way for developers to communicate issues to system administrators. In any of these situations, the application can evaluate its own health check and gracefully degrade behavior, rather than taking the entire system offline. The Status library will capture stack traces from dependencies and return the results in a single location. This feature makes it easy to resolve issues as they arise in any environment. Typical dependencies include MySQL tables, MongoDB collections, RabbitMQ message queues, and API statuses. System states When dependencies fail, they affect the condition of the system. System states include: OUTAGE - the system is unable to process requests; MAJOR - the system can service some requests, but may fail for the majority; MINOR - t he system can service the majority of requests, but not all; OK - t he system should be able to process all requests. Get started with Status Follow these instructions to start using the Status library: Extend the AbstractDependencyManager . The dependency manager will keep track of all your dependencies. public class MyDependencyManager extends AbstractDependencyManager {\r\n  public MyDependencyManager() {\r\n    super(\"MyApplication\");\r\n  }\r\n} Extend PingableDependency for each component that your application requires to run. public class MyDependency extends PingableDependency {\r\n  @Override\r\n  public void ping() throws Exception {\r\n    // Throw exception if considered unhealthy or unavailable\r\n  }\r\n} Extending the PingableDependency class is the simplest way to incorporate a dependency into your application. Alternatively, you can extend AbstractDependency or ComparableDependency to get more control over the state of a dependency. You can control how your dependency affects the system’s condition by providing an Urgency level. Add your new dependencies to your dependency manager. dependencyManager.addDependency(myDependency);\r\n... For web-based applications and services, create an AbstractDaemonCheckReportServlet that will report the status of your application. public class StatusServlet extends AbstractDaemonCheckReportServlet {\r\n  private final AbstractDependencyManager manager;\r\n\r\n  public StatusServlet(AbstractDependencyManager manager) {\r\n    this.manager = manager;\r\n  }\r\n\r\n  @Override\r\n  protected AbstractDependencyManager newManager(ServletConfig config) {\r\n    return manager;\r\n  }\r\n} Once this process is complete and your application is running, you should be able to access the servlet to read a JSON representation of your application status. Below is a sample response returned by the servlet. If the application is in an OUTAGE condition, the servlet returns a 500 status code. Associating the health check outcome with an HTTP status code enables integration with systems (like Consul) that make routing decisions based on application health. Otherwise, the servlet returns a 200 since it can still process requests. In this case, the application may gracefully degrade less-critical functionality that depends on unhealthy code paths. {\r\n  \"hostname\": \"pitz.local\",\r\n  \"duration\": 19,\r\n  \"condition\": \"OUTAGE\",\r\n  \"dcStatus\": \"FAILOVER\",\r\n  \"appname\": \"crm.api\",\r\n  \"catalinaBase\": \"/var/local/tomcat\",\r\n  \"leastRecentlyExecutedDate\": \"2015-02-24T22:48:37.782-0600\",\r\n  \"leastRecentlyExecutedTimestamp\": 1424839717782,\r\n  \"results\": {\r\n    \"OUTAGE\": [{\r\n      \"status\": \"OUTAGE\",\r\n      \"description\": \"mysql\",\r\n      \"errorMessage\": \"Exception thrown during ping\",\r\n      \"timestamp\": 1424839717782,\r\n      \"duration\": 18,\r\n      \"lastKnownGoodTimestamp\": 0,\r\n      \"period\": 0,\r\n      \"id\": \"mysql\",\r\n      \"urgency\": \"Required: Failure of this dependency would result in complete system outage\",\r\n      \"documentationUrl\": \"http://www.mysql.com/\",\r\n      \"thrown\": {\r\n        \"exception\": \"RuntimeException\",\r\n        \"message\": \"Failed to communicate with the following tables:\r\n          user_authorities, oauth_code, oauth_approvals, oauth_client_token,\r\n          oauth_refresh_token, oauth_client_details, oauth_access_token\",\r\n        \"stack\": [\r\n          \"io.github.jpitz.example.MySQLDependency.ping(MySQLDependency.java:68)\",\r\n          \"com.indeed.status.core.PingableDependency.call(PingableDependency.java:59)\",\r\n          \"com.indeed.status.core.PingableDependency.call(PingableDependency.java:15)\",\r\n          \"java.util.concurrent.FutureTask.run(FutureTask.java:262)\",\r\n          \"java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\",\r\n          \"java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\",\r\n          \"java.lang.Thread.run(Thread.java:745)\"\r\n        ]\r\n      },\r\n      \"date\": \"2015-02-24T22:48:37.782-0600\"\r\n    }],\r\n    \"OK\": [{\r\n      \"status\": \"OK\",\r\n      \"description\": \"mongo\",\r\n      \"errorMessage\": \"ok\",\r\n      \"timestamp\": 1424839717782,\r\n      \"duration\": 0,\r\n      \"lastKnownGoodTimestamp\": 0,\r\n      \"period\": 0,\r\n      \"id\": \"mongo\",\r\n      \"urgency\": \"Required: Failure of this dependency would result in complete system outage\",\r\n      \"documentationUrl\": \"http://www.mongodb.org/\",\r\n      \"date\": \"2015-02-24T22:48:37.782-0600\"\r\n    }]\r\n  }\r\n} This report includes these key fields to help you evaluate the health of a system and the health of a dependency: condition Identifies the current health of the system as a whole. leastRecentlyExecutedDate The last date and time that the report was updated. Use these fields to inspect individual dependencies: status Identifies the health of the current dependency. thrown The exception that caused the dependency to fail. duration The length of time it took to evaluate the dependency’s health. Because the system caches the result of a dependency’s evaluation, this value can be 0. urgency The urgency of the dependency. Dependencies with a WEAK urgency may not need to be fixed immediately. Dependencies with a REQUIRED urgency must be fixed as soon as possible. Learn more about Status Stay tuned for a future post about using the Status library, in which we’ll show how to gracefully degrade unhealthy applications. To get started, read our quick start guide and take a look at the samples . If you need help, you can reach out to us on GitHub or Twitter .", "date": "2015-07-10"},
{"website": "Indeed-Engineering", "title": "Building a Large-Scale Machine Learning Pipeline for Job Recommendations", "author": [" by Preetha Appan"], "link": "https://engineering.indeedblog.com/blog/2016/04/building-a-large-scale-machine-learning-pipeline-for-job-recommendations/", "abstract": "(Editor's note: This post was originally published on oreilly.com . ) With 200 million unique visitors every month, Indeed relies on a recommendation engine that processes billions of input signals every day -- resulting in more external online hires than any other source. To create this recommendation engine, we started with a minimum viable product (MVP) built with Apache Mahout and evolved to a hybrid, offline + online pipeline. Along the way, our changes affected product metrics, and we addressed challenges with incremental modifications to algorithms, system architecture, and model format. The lessons we learned in designing this system can apply to any high-traffic machine learning application. From search engine to recommendation Indeed’s production applications run in many data centers around the world. Clickstream data -- and other application events from every data center -- are replicated into a central HDFS repository , based in our Austin data center. We compute analytics and build our machine learning models from this repository. Our job search engine is simple and intuitive, with two inputs: keywords and location. The search results page displays a list of matching jobs, ranked by relevance. The search engine is the primary way our users find jobs on Indeed. Our decision to go beyond search and add job recommendations -- as a new mode of interaction -- was based on several reasons: 25% of all searches on Indeed specify only a location and no search keywords. Many job seekers aren’t sure what keywords to use in their search. When we deliver targeted recommendations, the job seeker's experience is personalized. Recommendations can help even the most sophisticated user discover additional jobs that their searches would not have uncovered. With recommendations driving 35% of Amazon sales and 75% of Netflix content views , it’s clear they provide added value. Recommending jobs is significantly different than recommending products or movies. Here are just a few of the things we took into careful consideration as we built our engine: Rapid Inventory Growth . We aggregate millions of new jobs on Indeed every day. The set of recommendable jobs is changing constantly. New Users. Millions of new job seekers visit Indeed every day and begin their job search. We want to be able to generate recommendations with very limited user data. Churn . The average lifespan of a job on Indeed is around 30 days. Content freshness matters a lot, because the older the job, the more likely it is to have been filled. Limited Supply . One job posting is usually meant to hire one individual. This is different from books or movies, where as long as there is inventory it can be recommended to many users at the same time. If we over-recommend a job, we could bombard an employer with thousands of applications. How to approach recommendation algorithms Recommendations are a matching problem . Given a set of users and a set of items, we want to match users to their preferred items. There are two high-level approaches to this: c ontent-based and b ehavior-based . They each have pros and cons, and there are also ways to combine these approaches to take advantage of both techniques. Content-based approaches use data, such as user preferences and features of the items being recommended, to determine the best matches. For recommending jobs, using keywords of the job description to match keywords in a user’s resume is one content-based approach (note that users can upload their resume to the Indeed site). Using keywords in a job to find other similar jobs is another way to implement content-based recommendations. Behavior-based approaches leverage user behavior to generate recommendations. These approaches are domain-agnostic, meaning the same algorithms that work on music or movies can be applied to the jobs domain. Behavior-based approaches do suffer from a cold start problem. If you have little user activity, it is much harder to generate good quality recommendations. Mahout collaborative filtering We started by building a behavior-based recommendation engine because we wanted to leverage our existing job seeker traffic and click activity. Collaborative filtering algorithms are well understood in this space. Our first attempt at personalized recommendations was based on Apache Mahout’s user-to-user collaborative filtering implementation. We fed clickstream data into a Mahout builder that ran in our Hadoop cluster, and produced a map of users to recommended jobs. We built a new service to provide access to this model at runtime, and multiple client applications accessed this service to recommend jobs. MVP results and roadblocks As an MVP, the behavior-based recommendation engine showed us that it is important to start small and iterate . Building this system quickly and getting it in front of users demonstrated that these recommendations were useful to job seekers. However, we ran into several immediate problems using Mahout on our traffic: The builder took around 18 hours on Indeed’s 2013 clickstream, which is about 3X smaller than present day. We could only run the builder once a day, which meant that millions of new users joining Indeed every day wouldn’t see recommendations until the next day. Millions of new jobs aggregated on Indeed were not visible as recommendations until the builder ran again. The model we produced was a large map, which was around 50GB, and took several hours to copy over a WAN from the data center where it was built to our data centers around the globe. Mahout’s implementation exposed a few tweakable parameters, like similarity thresholds. We could tweak the parameters of the algorithm, but we wanted the flexibility to test entirely different algorithms. Implementing MinHash for recommendations We addressed the most important problem first: the builder was too slow. We found that user-to-user similarity in Mahout is implemented by comparing every user to every other user in n 2 time. For only U.S. traffic in Indeed (50 million UVs), this amounts to 15 * 10 15 comparisons, which is intractable. This calculation is also batch in nature. Adding a new user or a new click event requires recalculating all similarities. We realized that recommendations were an inexact problem. We were looking for ways to find the closest users to a given user, but we didn’t need 100% accuracy. We looked for ways to approximate similarity without having to calculate it exactly. Principal contributor Dave Griffith came across MinHash from a Google News academic paper . MinHash, or minwise independent permutations, allows approximating Jaccard similarity . Applying this measure to jobs that two users clicked on at Indeed, we see that the more jobs these two users have in common, the higher their Jaccard similarity. Calculating Jaccard similarity for all pairs of users is O(n 2 ) , and with MinHash, we can reduce this down to O(n) . The MinHash of a set of items, given a hash function h1 , is defined as taking the hash of all items in that set using that hash function, and then taking the minimum of those values. A single hash function is not sufficient to approximate the Jaccard similarity because the variance is too high . We have to use a family of hash functions to reasonably approximate Jaccard distance. With a family of hash functions, MinHash can be used to implement personalized recommendations with tweakable Jaccard similarity thresholds. Mining Massive Datasets , a recent Coursera course from Stanford professors Leskovec, Rajaraman, and Ullman, explains MinHash in great detail. Chapter 3 (PDF) of their book, “Mining Massive Datasets,” explains the mathematical proof behind MinHash. Our implementation of MinHash for recommendations involved the following three phases: Phase 1: Signature calculation/cluster assignment Every job seeker is mapped to a set of h clusters, corresponding to a family of hash functions H. The following pseudocode shows this: H = {H 1 , H 2 , ..H 20 }\r\nfor user in Users\r\n     for hash in H\r\n          minhash[hash] = min{x∈Items i | hash(x)} Where H is a family of h hash functions. At the end of this step, each user is represented by a signature set: a cluster consisting of h values. Phase 2: Cluster expansion Users that share the same signature are considered similar, and their jobs are cross recommended to each other. We expand each cluster with all the jobs from each user in that cluster. Phase 3: Recommendation generation To generate recommendations for a given user, we union all jobs from the h clusters that the user is in. We remove any jobs that this user has already visited to get the final set of recommended jobs. Recommending jobs to new users MinHash’s mathematical properties allow us to address recommending jobs to new users and recommending new jobs to all users. We update the MinHash signature for users incrementally as new clicks come in. We also maintain a map in memory of new jobs and their MinHash clusters. By keeping these two pieces of data in memory, we are able to recommend jobs to new users after they click on a few jobs. As soon as any new jobs posted throughout the day receive clicks, they are recommended to users. After transitioning to MinHash, we had a hybrid recommendation model -- consisting of an offline component that builds daily in Hadoop -- and an online component implemented in memcache -- consisting of the current day’s click activity. Both models are combined to compute the final set of recommendations per user. The recommendations for each user became more dynamic after we made this change, because they would update as users clicked on jobs that interested them. With these changes, we learned that we could trade off a little bit of accuracy for a lot of performance. We also learned to complement a slower offline model with an online model for fresher results . Engineering infrastructure improvements The recommendation model that contained a map from each user to their recommendations was a large monolithic file. Because jobs are local to each country, we first attempted to shard our data into zones based on approximate geographic boundaries. Instead of running one builder for the entire world, we ran one builder per zone. Each zone consisted of multiple countries. As an example, the East Asian zone contained recommendations for China, Japan, Korea, Hong Kong, Taiwan, and India. Even after sharding, some of our zones produced data files that were too big and took hours to copy from our Austin Hadoop cluster over a WAN to a remote data center in Europe. To address this, we decided to ship recommendation data incrementally rather than once per day. We reused sequential write ahead logs and log structured merge trees to implement this. This was already validated in other large production applications at Indeed, like our document service. Instead of producing a large model file, we modified our builder to write small segments of recommendation data. Each segment file is written using sequential I/O and optimized for fast replication. These segments get reassembled into a log structured merge tree in recommendation services running in remote data centers. This infrastructure change caused users to see their new recommendations hours faster in remote data centers. From our A/B testing of this change, we saw a 30% increase in clicks due to the fact that users received newer recommendations faster. This improvement demonstrated that engineering infrastructure improvements can make as much of an impact on metrics as algorithm improvements. A/B testing velocity Building out the pipeline to compute and update recommendations was only the beginning. To improve the coverage and quality of recommendations, we needed to increase our A/B testing velocity. We were making many decisions in the builder to tune the final set of recommendations. These decisions included similarity thresholds, the number of jobs to include in an individual’s recommendations, and different ways to filter out poor quality recommendations. We wanted to tweak and optimize every aspect of computing recommendations, but to do so would require building and shipping a new model per algorithm tweak. Testing multiple improvement ideas meant many times more disk and memory usage on the servers that handled requests from users. We began to improve our A/B testing velocity by shipping the individual components of the recommendation calculation, rather than the final results. We changed the recommendation service to perform the final calculation by combining the pieces, instead of simply reading the model and returning results. Critical subcomponents of recommendations are cluster assignments per user, the mapping from each cluster to jobs in that cluster, and a blacklist for each user that contained jobs that should not be recommended for them. We modified our builder to produce these components and modified our service to put them together at request time to return the final list of recommendations. By implementing this architectural change, we only shipped subcomponents that changed per A/B test. For example, if the test only tweaked what jobs got removed from a user’s recommendations, we would only ship the blacklist for the test group. This change improved A/B testing velocity by orders of magnitude. We were able to test and validate several ideas that improved the quality and coverage of recommendations in a short period of time. Previously, we averaged testing one improvement in the model every quarter because of the overhead in setting up our experiments. Our experience shows that A/B testing velocity should be considered when designing large machine learning systems . The faster you can get your ideas in front of users, the faster you can iterate on them. This post summarizes a series of algorithmic and architectural changes we made as we built our recommendation engine. We build software iteratively at Indeed -- we start with an MVP, learn from it, and improve it over time. As a result of these changes, job recommendations grew from a small MVP to contributing 14% of all clicks on Indeed, which is up from 3% in early 2014. Architecture diagram for Indeed's recommendation engine Conclusion Moving forward, we continue to refine our recommendation engine. We are prototyping a model using Apache Spark . We are building an ensemble of models, and we are refining our optimization criteria to combat popularity bias .", "date": "2016-04-26"},
{"website": "Indeed-Engineering", "title": "A Bounty of Security", "author": [" by Gregory Caswell"], "link": "https://engineering.indeedblog.com/blog/2016/03/a-bounty-of-security/", "abstract": "“Do what's best for the job seeker.” This has been Indeed's guiding principle since the beginning. One way we put the job seeker first is by keeping their information safe and secure. We always consider the security of our systems as we develop the services that millions of people use every day. But someone will outsmart us. Hackers are always trying out new ways of bypassing security and gaining access to systems and information. Our challenge: to bring these security experts over to our side and benefit from their findings. Image by stockarch - stockarch.com (Licensed by Creative Commons ) Our answer to this challenge is, well, money. Actually, money and fame. Indeed offers security testers a legitimate route to reporting their findings, and we award them for their time with cold, hard cash and recognition. Through our bug bounty program we have awarded over 300 submissions in the past year and a half , with payouts as high as $5,000 for the most severe bugs. Our most successful participants (looking at you, Angrylogic , Avlidienbrunn , and Mongo ) have earned cash while building their reputations as highly regarded testers for Indeed. Reward amount per submissions in the last 18 months Criticality Reward Amount Relative Submission Counts CRITICAL Up to $5000 0.7% HIGH Up to $1800 4% MEDIUM Up to $600 31% LOW Up to $100 64% Why create this program? Prior to our bug bounty program , we occasionally received messages that sounded like blackmail. An anonymous person would contact us, insisting that we pay them, or they would publicly release the details of an unspecified, but totally serious , security bug. These individuals expected payment up front, with no guarantee that they even had a bug to expose. While we’re happy to compensate researchers for helping us improve our services, we didn’t want to encourage this coercive behavior. It felt wrong. To solve the mutual distrust, we started using Bugcrowd.com as an impartial arbiter. On Bugcrowd, security researchers are more willing to provide evidence up front, giving us the chance to fairly assess the bug’s severity. Indeed can now provide rewards without abuse, and everyone lives happily ever after... Theory vs practice “Happily ever after...” is more difficult in practice. Since the program started, we have received almost 2,500 submissions, each issue potentially taking hours to validate. Every time we advertise our bounty program or raise our payouts, we see a large spike in submissions. To an outsider, it might look like we’re dragging our feet, but in reality, it’s all hands on deck to reply to these submissions. This blog post alone will generate several more hours worth of bug validation thanks to the increased visibility of the program. We initially struggled to quickly respond to testers’ submissions, creating a backlog. This backlog grew because we received more submissions than we had time to process. We ended up doubling down on our efforts over a painful couple of weeks and then implementing a new standard for response time. Since then, response times have been under control. Sum of open Ticket Days over Time Note: The value of Ticket Days is the sum of days that every ticket is open on a particular date. For example, on a given date, one ticket open for 3 days + one ticket open for 2 days = 5 Ticket Days. Communicating clearly with the researchers is also important, so that they don’t think we are trying to take advantage of them. We keep in mind that they don’t have as much visibility into the process as we do. One common issue is handling duplicates. Paying for an issue the first time you hear about it makes sense, but how should we handle a duplicate submission from another researcher? The second submission doesn’t add any additional value, but from the tester’s point of view, they found a real bug. Clearly communicating why you are marking a ticket a duplicate and quickly fixing identified issues helps minimize this concern. In some cases, we decide to pay for the duplicate if it has great reproduction steps and a proof of concept. Finally, we’re working on balancing the time we spend finding new bugs and fixing known bugs. Building and managing a popular bounty program leads to lots of good submissions, but that all falls to pieces if we don’t also spend the time fixing the bugs. At Indeed, the benefits of investing time improving our bug bounty program can’t be overstated. Our successes so far It seems we’re doing something right. Bugcrowd recently asked their security researchers which company’s program was their favorite, and you’ll never guess who won! ...Tesla won (we blame those fabulous Teslas). But we took runner up, with 8% of all votes, racing against over 35 other programs. Many of the specific responses for our program referenced our fair payout practices, great communication, and permissive scope. While we know that we can still rev up the experience, we are happy for the validation that we are headed down the right road.", "date": "2016-03-17"},
{"website": "Indeed-Engineering", "title": "Gracefully Degrading Functionality Using Status", "author": [" by Mya"], "link": "https://engineering.indeedblog.com/blog/2017/01/degrade-functionality/", "abstract": "In a previous blog post , we described how to use our Status library to create a robust health check for your applications. In this follow-up, we show how you can check and degrade your application during an outage by: short-circuiting code paths of your application removing a single application instance from a data center load balancer removing an entire data center from rotation at the DNS level Evaluating application health The Status library allows you to perform two different types of checks on a system -- a single dependency check and a system-wide evaluation. A dependency is a system or service that your system requires in order to function. During a single dependency check, the DependencyManager uses an evaluate method that takes the dependency’s ID and returns a CheckResult . A CheckResult includes: the health of the dependency some basic information about the dependency the time it took to evaluate the health of the dependency A CheckResult is a Java enum that is one of OK , MINOR , MAJOR , or OUTAGE . The OUTAGE status indicates that the dependency is not usable. final CheckResult checkResult = dependencyManager.evaluate(\"dependencyId\");\r\nfinal CheckStatus status = checkResult.getStatus(); The second approach to evaluating an application’s health is to look at the system as a whole. This gives you a high-level overview of how the entire system is performing. When a system is in OUTAGE , this indicates that the instance of an application is not usable. final CheckResultSet checkResultSet = dependencyManager.evaluate();\r\nfinal CheckStatus systemStatus = checkResultSet.getSystemStatus(); If a system is unhealthy, it’s often best to short circuit requests made to the system and return an HTTP status code 500 (“Internal Server Error”). In the example below, we use an interceptor in Spring to capture the request, evaluate the system’s health, and respond with an error in the event that the application is in an outage. public class SystemHealthInterceptor extends HandlerInterceptorAdapter {\r\n    private final DependencyManager dependencyManager;\r\n\r\n    @Override\r\n    public boolean preHandle(\r\n            final HttpServletRequest request,\r\n            final HttpServletResponse response,\r\n            final Object handler\r\n    ) throws Exception {\r\n        final CheckResultSet checkResultSet = dependencyManager.evaluate();\r\n        final CheckStatus systemStatus = checkResultSet.getSystemStatus();\r\n        \r\n        switch (systemStatus) {\r\n            case OUTAGE:\r\n                response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR.value());\r\n                return false;\r\n            default:\r\n                break;\r\n        }\r\n\r\n        return true;\r\n    }\r\n} Comparing the health of dependencies CheckResultSet and CheckResult have methods for returning the current status of the system or the dependency, respectively. Once you have CheckStatus, there are a couple of methods that allow you to compare the results. isBetterThan() determines if the current status is better than the provided status. This is an exclusive comparison. CheckStatus.OK.isBetterThan(CheckStatus.OK)              // evaluates to false\r\nCheckStatus.OK.isBetterThan(/* any other CheckStatus */) // evaluates to true isWorseThan() determines if the current status is worse than the provided status. Again, this operation is exclusive. CheckStatus.OUTAGE.isWorseThan(CheckStatus.OUTAGE)          // evaluates to false\r\nCheckStatus.OUTAGE.isWorseThan(/* any other CheckStatus */) // evaluates to true The isBetterThan() and isWorseThan() methods are great tools to check for a desired state of an evaluated dependency. Unfortunately, these methods do not offer enough control to produce a graceful degradation. Either the system was healthy, or it was in an outage. To better control the graceful degradation of our system, two additional methods were needed. noBetterThan() returns the unhealthier of the two statuses. CheckStatus.MINOR.noBetterThan(CheckStatus.MAJOR) // returns CheckStatus.MAJOR\r\nCheckStatus.MINOR.noBetterThan(CheckStatus.OK)    // returns CheckStatus.MINOR noWorseThan() returns the healthier of the two statuses. CheckStatus.MINOR.noWorseThan(CheckStatus.MAJOR) // returns CheckStatus.MINOR\r\nCheckStatus.MINOR.noWorseThan(CheckStatus.OK)    // returns CheckStatus.OK During the complete system evaluation, we use a combination of these methods and the Urgency#downgradeWith() methods to gracefully degrade our application’s health. By having the ability to inspect the outage state, engineers can dynamically toggle feature visibility based on the health of its corresponding dependency. Suppose that our service that provides company information was unable to reach its database. The service’s health check would change its state to MAJOR or OUTAGE . Our job search product would then omit the company widget from the right rail on the search results page. The core functionality that helps people find jobs would be unaffected. In a healthy state, the company widget appears on the right rail. In an unhealthy state, the company widget does not appear but the results are otherwise unaffected. Instance level failovers Generally, running multiple instances of your application in production is highly recommended. This helps keep your system resilient by allowing it to continue to handle requests even if a single instance of your application crashes. These instances of your application can live on a single machine, multiple machines, and even in multiple data centers. The Status library lets you configure your load balancer to remove an instance if it becomes unhealthy. Consider the following basic example within a single data center. When all of the applications within a single data center are healthy, the load balancer distributes requests among them evenly. To determine if an application is healthy, the load balancer sends a request to the health check endpoint and evaluates the response code. When an instance becomes unhealthy, the health check endpoint returns a non-200 status code, indicating that it should no longer receive traffic. The load balancer then removes the unhealthy instance from rotation, preventing it from receiving requests. When instance 1 is removed from rotation, the other instances within a data center start to receive instance 1’s traffic. Within each data center, we provision enough instances so that we can handle traffic even if some of the instances go down. Data center level failovers Before a request is even sent to a data center, our domain (e.g. www.indeed.com ) is resolved to an IP address using DNS. We use Global Server Load Balancer (GSLB) that allows us to geographically distribute traffic across our data centers. After the GSLB resolves the domain to the IP address of the nearest available data center, the data center load balancer then routes and fails over traffic as described above. What if an entire data center can no longer service requests? Similar to the single instance approach, GSLB constantly checks each of our data centers for their health (using the same health check endpoint). When GSLB detects that a single data center can no longer service requests, it fails requests over to another data center and removes the unhealthy data center from rotation. Again, this helps keep the site available by ensuring that requests can be processed, even during an outage. As long as a single data center remains healthy, the site can continue to service requests. For users that hit unhealthy data centers, this just looks like a slower web page load. While not ideal, the experience is better than an unprocessed request. The last scenario is a complete system outage. This occurs when every data center becomes unhealthy and can no longer service requests. Engineers try to avoid this situation like the plague. When Indeed encounters complete system outages, we reroute traffic to every data center and every instance. This policy, known as “failing open,” allows for graceful degradation of our system. While every instance may report an unhealthy state, it is possible that an application can perform some work. And being able to perform some work is better than performing no work. Status works for Indeed and can work for you The Status library is an integral part of the systems that we develop and run at Indeed. We use Status to: quickly fail over application instances and data centers detect when a deploy is going to fail before the code reaches a high traffic data center keep our applications fast by failing requests quickly, rather than doing work we know will fail keep our sites available by ensuring that only healthy instances of our applications service requests To get started with Status, read our quick start guide and take a look at the samples . If you need help, you can reach out to us on GitHub or Twitter .", "date": "2017-01-19"},
{"website": "Indeed-Engineering", "title": "Indeed Reads: October 2017", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2017/10/indeed-reads-oct-2017/", "abstract": "The Indeed Engineering blog lets us write about the technologies we get to work with every day. We talk about what works, what sometimes doesn’t work, and how we’re scaling our technologies as a fast growing company. We participate in industry-wide conversations through this blog. We also read a lot of other tech blogs. Indeedians at the Austin Tech Campus Here are a few interesting pieces recommended by Indeed engineers: Engineering Personas by Casey Rosenthal We know that sales and marketing research often includes user personas for building software. This article looks at five engineering personas that also can help you with hiring decisions. These personas aren’t intended to define people, but they provide insight into the kinds of behaviors and characteristics that would most benefit your team. Read post Gray Failure by Adrian Colyer Working from the paper “Gray failure: the Achilles’ heel of cloud-scale systems” by Huang et al., Colyer explores the idea that breakdowns and performance anomalies in cloud-scale systems are often caused by gray failures, subtle failures perceived differently by various components of a system. Huang et al., describe these failures: “One entity is negatively affected by the failure and another entity does not perceive the failure.” Colyer takes this observation of gray failures and asserts that when we fail, we should fail properly. Read post How To: S3 Multi-Region Replication (And Why You Should Care) by Jessica Lucci Indeed’s Jessica Lucci provides step-by-step instructions to create a replica set by combining Amazon’s S3, SNS, SQS and Lambda technologies. As Amazon only provides two-region bucket replication, deploying your own replica set allows you to replicate data across as many regions as you need. A custom replica set also allows you to customize replication behavior via your Lambda functions. Read post The Pitfalls of A/B Testing in Social Networks by Brenton McMenamin A/B testing can be the perfect tool for most testing situations. This method helps determine whether a feature positively affects user behavior in a given test group. But if the purpose of your site is to get users to connect with as many other users as possible, like on the dating site OkCupid, keeping the feature away from your control group can lead to unreliable results. Tests for user-to-user features, such as chat or video, need interaction with as many people as possible to verify effectiveness. When A/B testing involves social interactions, it’s highly likely you’ll need to rethink and restructure your experiment methodology. Read post How to Do Code Reviews Like a Human (Part One) by Michael Lynch Code reviews are more than a technical process used to help identify bugs; they’re also social interactions that, when done mindfully, can improve the process itself. Delivering critical feedback is rarely easy, and when the feedback is focused on a technical subject, our instinct can often be to provide technical responses without considering the human interaction involved. But a human wrote the code that’s being reviewed, and the best way to iterate with that human is to follow guidelines that help you to be constructive and avoid miscommunications. Read post Floating Point Visually Explained by Fabien Sanglard Floating-point arithmetic is one of the trickier concepts in computer programming. This datatype has been called both esoteric and essential. To better understand floating points, Sanglard proposes visualizing the sections of the mathematical notation of a floating-point number (sign, exponent, and mantissa). Then, instead of exponent, Sanglard proposes thinking of a window between two consecutive power of two integers, and instead of a mantissa, thinking of an offset within that window. Redefining the components of a floating-point number in a more visual way provides a clearer path to explaining how this datatype works. Read post", "date": "2017-10-20"},
{"website": "Indeed-Engineering", "title": "New Eng Manager at Indeed? First: Write Some Code", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2016/12/new-eng-manager-onboarding/", "abstract": "I joined Indeed in March 2016 as an “industry hire” manager for software engineers. At Indeed, engineering managers act as individual contributors (ICs) before taking on more responsibilities. Working with my team as an IC prepared me to be a more effective manager. Before my first day, I talked with a few engineering managers about what to expect. They advised that I would spend about 3-6 months contributing as an individual developer. I would write unit tests and code, commit changes, do code reviews, fix bugs, write documentation, and more. I was excited to hear about this approach, because in my recent years as an engineering manager, I had grudgingly stopped contributing at the code level. Instead, I lived vicariously through others by doing code reviews, participating in technical design reviews, and creating utilities and tools that boosted team productivity. When new managers start in the Indeed engineering organization as an IC, they can rotate through several different teams or stay with a single team for about a quarter. I was in the latter camp and joined a team that works on revenue management. Onboarding as an individual contributor My manager helped to onboard me and directed me to self-guided coursework on our wiki. I was impressed by the amount of content provided to familiarize new hires with the tools and technologies we use at Indeed. In my experience, most companies don’t invest enough in creating and maintaining useful documentation. Equally as valuable, fellow Indeedians gladly answered my questions and helped me to get unblocked when I encountered technical hurdles. I really appreciated that support as a new employee. During my time as an IC, I had no management responsibilities. That was a change for me….and it was wonderful! I focused on code. I built technical competence and knocked the rust off mental processes that I hadn’t needed to use for awhile. I observed practices and processes used by the team to learn how I could become equally productive. I had a chance to dive deeper into Git usage. I wrote unit and DAO tests to increase code coverage. I learned how to deploy code into the production environment. For the first time in a long while, I wrote production code for new features in a product. To accelerate my exposure to the 20 different projects owned by my team, I asked to be included on every code review. I knew I wouldn’t be able to contribute to all of the projects, but I wanted to be exposed to as many as possible. Prior to my request, the developer typically selected a few people to do a code review and nominated one to be the “primary” reviewer. Because I was included in every review, I saw code changes and the comments left by team members on how to improve the code. I won’t claim I understood everything I read in every code review, but I did gain an appreciation for the types of changes. I recommend this approach to every new member of a team, not just managers. Other activities helped me integrate with people outside of my team. For example, I scheduled lunch meetings with everyone who had interviewed me. This was mostly other engineering managers, but I also met with folks in program management and technical writing. Everyone I contacted was open to meeting me. These lunch meetings allowed me to get a feel for different roles; how they planned and prioritized work; their thoughts on going from IC to manager; and challenges that they had faced during their tenure at Indeed. On-site lunches (with great food, by the way) allowed me to meet both engineering veterans as well as people in other departments. Transitioning into a managerial role By the time I was close to the end of my first full quarter, I had contributed to several projects. I had been exposed to some of the important systems owned by my team. Around this time, my manager and I discussed my transition into a managerial role. We agreed that I had established enough of a foundation to build on. I took over 1-on-1 meetings, quarterly reviews, team meetings, and career growth discussions. Maintaining a technical focus Many software engineers who take on management roles struggle with the idea of giving up writing code. But in a leadership position, what matters more is engaging the team on a technical level. This engagement can take a variety of forms. Engineering managers at Indeed coach their teams on abstract skills and technical decisions. When managers have a deeper understanding of the technology, they can be more effective in their role. I am glad that I had a chance to start as an IC so that I could earn my team’s trust and respect. A special shout out to the members of the Money team: Akbar, Ben, Cheng, Erica, Kevin, Li, and Richard.", "date": "2016-12-07"},
{"website": "Indeed-Engineering", "title": "Forget Methodology — Focus on What Matters", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2016/03/forget-methodology-focus-on-what-matters/", "abstract": "At Indeed, we tackle interesting and challenging problems, at scale. We move from idea to implementation as fast as possible. We ship incremental changes, pushing code to production frequently. Our small releases reduce risk and increase quality. But before we work on any solution, we ask: how will we measure success? This question keeps our solutions focused on what matters -- measurable results. Our approach to software development might be called \"measure-learn-evolve.\" Our teams employ techniques from various software development methodologies, but no single published methodology rules. We collaborate, we iterate, and we measure. Sometimes we succeed, sometimes we fail, but we are always learning. We don’t view process implementation and improvement as success. Process is a means to an end. Process doesn’t deliver a successful product. (People do.) Process doesn’t provide talent and passion. (People do.) But the right process and tools can help people do those things and provide predictable mechanisms for: planning what we need to do and setting relative priorities communicating what we are doing or might do remembering what we’ve done managing our risk We use Atlassian’s Jira to implement these mechanisms. In Jira, we propose ideas, define requirements, and plan projects. We document dependencies, track work, and manage releases. We describe experiments and record results. Customizing Jira to our needs has helped us collaborate on success metrics and maintain our engineering velocity. It wasn’t always this way. We started simple. We were a startup and we focused on getting stuff done, quickly. As we grew, we didn’t want to lose this focus on getting things done quickly and with quality. But our ad hoc process was neither repeatable nor predictable. Inconsistencies abounded and we were not creating a memory for the future. So we began to model our development process in Jira. Customizing Jira We have our own Jira issue types, workflows, fields, and roles. These customizations allow us to plan, communicate, and deliver our software in the way we want. Linking custom project types We use two types of Jira projects for product development: a “planning project” that corresponds to the product, and an “engineering project” that corresponds to a deployable application or service. Our planning projects contain Initiative and Experiment issues. We use the Initiative type to capture goals, plans, and success metrics for a product change. We plan product initiatives each quarter, and we iterate on them throughout the quarter. As part of that iteration, we use the Experiment type to describe specific ideas we want to test to optimize our products. The engineering projects include issues that detail the implementation necessary for the initiatives and experiments. Each deployable application or service has a corresponding engineering project. Issue links connect related issues to one another. Jira provides multiple types of bi-directional links. The following table gives examples of how we use them. Issue link Description incorporates / incorporated by Product initiatives incorporate engineering project issues. depends upon / depended on by Issues can depend upon on other issues. This can model feature development dependencies or deploy order dependencies, for example. references / referenced by An issue for a functional regression references the project issue that introduced the bug. Issue types and workflows We use Jira’s standard issue types: Bug, Improvement, New Feature. The workflow for these standard issue types is a slight modification of a typical Jira workflow: We create an issue and assign it to a project lead. The issue transitions to a Pending Triage state. If we can target work to a near-term release, we triage the issue, setting its Fix Version and assigning it to a developer. The issue then moves to Pending Acceptance. We move other issues to On Backlog. The developer accepts the issue, moving it to Accepted when they make a plan to start work. When the code is complete, the developer resolves the issue, moving it to Pending Review. After code review, we transition the issue to Pending Merge. When we’re ready to create a release candidate, we merge changes into the release branch and deploy to the QA environment, transitioning the issue to Pending Verification. The QA analyst verifies the work and either reopens the issue or verifies it, transitioning it to Pending Closure. After we verify all issues in a targeted release, we can release the build to production and move all the issues to Closed. We also use custom issue types to model our process. In a previous post, we described the ProTest issue type (short for Pro ctor Test ). We use this custom issue type to request new Proctor A/B tests or to change test allocations. We have another custom issue type and associated workflow for localization. As we continue to grow internationally, we need a localization process that doesn’t slow us down. Coordinating with many translators can be a challenge, so we model our translation process in Jira. Our Explosion issue type incorporates an issue for each target translation language. The workflow follows: We create an issue with English strings that require translation. We triage the issue and submit it for review. When the strings are ready to be translated, an automated step creates one Translation issue for each target language and links them all to the Explosion issue. Each “exploded” issue follows its own workflow: Accept, Resolve, Verify and Close. When all Translation issues are closed, we verify and close the Explosion issue. The Explosion and Translation custom issue types and workflows help streamline a process with many participants. Because we triage by language and feature, translation issues do not block the release of an entire feature. Using Jira also allows us to integrate machine translation and outside translation services. Team triage Many of our development teams use dashboards and agile boards in Jira for easy access to issues associated with a product. During routine triage meetings, product development teams use these tools to prioritize and distribute development work. Closing the memory loop Each code commit in Git is traceable to a corresponding issue in Jira. Further, if the referenced Jira links to the initiative, the trail leads all the way to the initiative. This means that an engineer can review any code commit and follow the trail in Jira to understand all related implementation details, requirements, and business motivation. Production deploys Deploying code to production requires clear communication and coordination, and our Deploy issue type helps us track this process. Using Jira to track deploys results in smooth handoffs and transparency for all stakeholders. A deploy ticket is associated with each Fix Version and has a unique workflow that facilitates communication for moving artifacts through the build and release process. We use issue links to document all sysadmin tasks necessary for a successful deployment. The deploy ticket has the same fix version as the other issues in the release. Most teams plan their work weekly but deliver to production as they complete the work. On some regular cadence - semi-weekly, daily, or more often - the release manager creates a release candidate from all open merge requests. We developed an internal webapp that coordinates across Git (branch management), Jira (code changes and deploys), Crucible (code review), and Jenkins (build). Status changes to the deploy ticket trigger issue reassignments, promoting smooth handoffs. This approach provides our teams with the information they need to assess and manage risk for their production releases. The QA analyst can better understand potential regressions that a change may cause. The release manager can have a holistic view of what’s changing and quickly react when issues arise. And small releases make bug investigation more straightforward. Working in the open Jira enables effective, efficient collaboration for our software development and deployment process. We use it to clarify requirements, discuss implementation choices, verify changes, and deploy to production. Across teams and up and down the organization, our use of Jira provides transparency into the work that is getting done. By working in the open, we can achieve a shared understanding of plans, progress, and challenges for hundreds of active projects and initiatives. Do what makes sense for you Methodology and process only help when they provide repeatable and predictable mechanisms for planning, communication, and delivery. Jira has helped us establish these mechanisms. Try to avoid taking a methodology “off the shelf” and implementing it. And don’t depend on tools to solve your problems. Instead, think about how your team needs to plan, communicate, and deliver. Then, define the best process and tools that serve your needs. Iterate on your process as needed. And stay focused on what really matters: success. Adapted from Jack Humphrey's presentation at Keep Austin Agile 2014 .", "date": "2016-03-09"},
{"website": "Indeed-Engineering", "title": "Indeed at PyData Seattle 2017", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2017/08/indeed-at-pydata-seattle-2017/", "abstract": "Indeed was a proud sponsor of PyData Seattle 2017 , an international conference promoting the use of open source data analysis tools for the Python community, such as Pandas, Matplotlib, IPython and Project Jupyter. Indeed Data Scientists presented two tutorials at the conference: Using Pandas for analyzing structured time series data Using open source natural language processing (NLP) libraries for analyzing unstructured text This post introduces their presentations and includes links to videos and tutorials for you to try the exercises yourself. Joe McCarthy illustrated how to use tools in the Pandas data analysis library to investigate unevenly spaced time series data in The Simpsons . This type of data analysis tends to focus more on the intervals between events rather than the frequency of events occurring within regularly spaced intervals. At Indeed, one example of such a task is estimating how long it takes a recruiter to review a resume (or profile), based on the gaps in timestamps of initial profile disposition events. Joe’s tutorial focused on a collection of data about episodes, characters, locations and scripts from The Simpsons . This collection is one of many data sets available at data.world . Video 1. D’oh! Unevenly spaced time series analysis of The Simpsons in Pandas Alex Thomas demonstrated how to use open source NLP tools such as the Natural Language Toolkit (NLTK) and word_cloud for vocabulary analysis of job descriptions . His tutorial covered basic NLP techniques such as tokenization, stemming and lemmatization in the context of analyzing job descriptions posted on Indeed. Other techniques include the use of stop words, multi-word phrases (n-grams) and the TF-IDF statistic for estimating the relevance of documents. Alex highlighted challenges in processing text and some interesting and often-unanticipated problems in interpreting the results of applying each of these techniques. Video 2. Vocabulary Analysis of Job Descriptions Exercises and Jupyter Notebooks for both Indeed tutorials are on GitHub at pydata-simpsons and pydata-vocab-analysis . For more PyData conference presentations, check out their YouTube channel .", "date": "2017-08-31"},
{"website": "Indeed-Engineering", "title": "Indeed at Litmus Live 2017: How to Run a Successful Email Workshop", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2017/07/indeed-litmus-live-2017/", "abstract": "Indeed is proud to announce that Lindsay Brothers will be speaking at Litmus Live in Boston on August 3, 2017. Lindsay is a Product Manager at Indeed. Her team sends billions of job alert emails every month to job seekers around the world. As the world’s #1 job site, Indeed communicates with job seekers around the globe. A unified email strategy allows us to effectively understand how, when, and why we should email job seekers. To develop this strategy, we built an email planning workshop to share ideas and come to consensus quickly. During this workshop, we created a job seeker’s Bill of Rights and brought users onsite for feedback and validation. Lindsay’s session will cover workshop details and offer takeaways for anyone developing a similar email strategy. Litmus Live brings together email marketers for two days of real-world advice, best practices, and key takeaways. Free from product pitches and hype, Litmus Live is all about content: Teaching designers, developers, marketers, and strategists how to create emails that look great, perform well, and engage audiences. If you're at Litmus Live Boston this year, join Lindsay to learn more about Indeed! Litmus Live: The Email Design Conference Indeed is hiring talented Sales, Product , Marketing and Engineering minds from Toronto to Tokyo and beyond. Find out more about opportunities to work at one of our 24 global offices.", "date": "2017-07-28"},
{"website": "Indeed-Engineering", "title": "Finding Anomalies in User Behavior with Python", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2016/10/finding-anomalies-with-python/", "abstract": "In the course of helping over 200 million unique visitors every month find jobs, we end up with a lot of data. The data we collect can tell us a lot about the behavior of our users, and for the most part we observe predictable patterns in that behavior. But unexpected changes could be evidence of failures in our system or an actual shift in user behavior. When the data shows something strange, we want to understand. Identifying and acting on anomalies in user behavior is a complex problem. To help detect these anomalies, we take advantage of several open-source libraries, chief among them Twitter’s AnomalyDetection library. Observing anomalies in user behavior Detecting anomalies in a large set of data can be easy if that data follows a regular, predictable pattern over time. If we saw the same range of pageviews for our job listings every day, as simulated in Figure 1, it would be easy to identify outliers. Figure 1. Single outlier But most of the data we collect is determined by user behavior, and such data does not follow patterns that we can easily observe. A variety of factors influence user behavior. For example, each of the following factors might affect what we consider a “normal” range of pageviews, depending on the geographic location of the user: What day of the week is it? What time of day is it? Is it a holiday? We might understand some factors in advance. We might understand others after analyzing the data. We might never fully understand some. Our anomaly detection should account for as many variations as possible, but still be precise enough to provide significant statistical outliers. Simply saying “traffic is normally higher on Monday morning” is too fuzzy: How much higher? For how long? Figure 2 shows a variable range of expected data, while Figure 3 shows a range of actual data. Anomalies within the actual data are not immediately visible. Figure 2. Expected data Figure 3. Actual data Figure 4 shows the actual and expected data overlaid. Figure 5 shows the difference between the two at any point in time. Viewing the data in this way highlights the significant anomaly in the final data point of the sequence. Figure 4. Expected and actual overlaid Figure 5. Difference between actual and expected data We needed a sophisticated method to quickly identify and report on anomalies such as these. This method had to be able to analyze existing data to predict expected future data. And it had to be accurate enough so that we wouldn’t miss anomalies requiring action. Step one: Solving the statistical problem The hard mathematical part was actually easy, because Twitter already solved the problem and open sourced their AnomalyDetection library . From the project description: AnomalyDetection is an open-source R package to detect anomalies which is robust, from a statistical standpoint, in the presence of seasonality and an underlying trend. The AnomalyDetection package can be used in wide variety of contexts. For example, detecting anomalies in system metrics after a new software release, user engagement post an A/B test, or for problems in econometrics, financial engineering, political and social sciences. To create this library, Twitter started with an extreme studentized deviate (ESD) test, also known as Grubb’s test , and improved it to handle user behavior data. Originally, the test used the mean value for a set of data to identify outliers. Twitter’s developers realized that using the median value was more precise for web use , as user behavior can be volatile over time. The result is a resource that makes it easy to quickly identify anomalous results in time-based datasets with variable trends. Twitter’s data scientists use this data to perform their own internal analysis and reporting. For example, they report on tweets per second and the CPU utilization of their internal servers. Twitter’s library allowed us to use historical data to estimate a highly complicated set of user behavior and quickly identify anomalous behavior. There was only one problem: Twitter wrote the library using R , while our internal alerting systems are implemented in Python. We decided to port Twitter’s library to Python so that it would work directly with our code. Step two: Porting the library Of course, porting code from one language to another always involves some refactoring and problem solving. Most of our work in porting the AnomalyDetection library dealt with differences between how math is supported in R and in Python. Twitter’s code relies on several math functions that are not natively supported in Python. The most important is a seasonal decomposition algorithm called seasonal and trend decomposition using loess ( STL ). We were able to incorporate STL from the open-source pyloess library . We found many of the other math functions that Twitter used in the numpy and scipy libraries. This left us with only a few unsupported math functions, which we ported to our library directly by reading the R code and replicating the functionality in Python. Taking advantage of the excellent work done by our neighbors in the open-source community allowed us to greatly reduce the effort required in porting the code. By using the pyloess, numpy, and scipy libraries to replicate the R math functions Twitter used, one developer completed most of the work in about a week. Open-source Python AnomalyDetection We participate in the open source community because, as engineers, we recognize the value in adapting and learning from the work of others. We are happy to make our Python port of AnomalyDetection available as open source. Download it, try it out, and reach out to us on GitHub or Twitter if you need any help.", "date": "2016-10-19"},
{"website": "Indeed-Engineering", "title": "Friendly Machine Learning", "author": [" by Artem Onuchin"], "link": "https://engineering.indeedblog.com/blog/2017/06/friendly-machine-learning/", "abstract": "At Indeed, machine learning is key to our mission of helping people get jobs. Machine learning lets us collect, sort, and analyze millions of job postings a day. In this post, we’ll describe our open-source Java wrapper for a particularly useful machine learning library, and we’ll explain how you can benefit from our work. Challenges of machine learning It’s not easy to build a machine learning system. A good system needs to do several things right: Feature engineering. For example, converting text to a feature vector requires you to precalculate statistics about words. This process can be challenging. Model quality. Most algorithms require hyper parameters tuning, which is usually done through grid search. This process can take hours, making it hard to iterate quickly on ideas. Model training for large datasets. The implementations for most algorithms assume that the entire dataset fits in memory in a single process. Extremely large datasets, like those we work with at Indeed, are harder to train. Wabbit to the rescue Fortunately for us, an excellent machine learning system that meets those needs already exists. John Langford, a computer science researcher from Microsoft, possesses a rare combination of excellence in machine learning theory and programming. His command line tool, Vowpal Wabbit (VW), implements state-of-the-art techniques for building generalized linear models, including feature hashing , adaptive bound optimization , normalized online learning , and online importance weight aware updates . It also includes useful features such as a flexible input data format. VW has garnered a lot of attention in the machine learning community and enjoys success in the industry. Benefits of Vowpal Wabbit At Indeed, we use VW to build models that help discover new job sites, improve quality of search results, and accurately measure performance of our products. VW is convenient for a number of reasons. Benefit 1: An input format that makes your life easier To feed VW with data, you need to convert that data to a special format first. While this format might seem strange, it has many benefits. It allows you to split features into namespaces, put weight on a whole namespace, name features, pass categorical features as-is, and even pass text as a feature. With VW, you can pass raw text with almost zero prep and train a decent model on it! The data format is also less error prone. During the prediction phase, you only need to convert prediction features into this format and not into numerical vectors. Benefit 2: Powerful feature engineering techniques out-of-the-box Another strength of Vowpal Wabbit is implemented feature engineering techniques, described in this wiki page . These techniques range from less complex, such as quadratic interactions and n-grams, to more complex, such as low rank quadratic approximation (also known as factorization machines ). You can access all of these feature engineering techniques just by changing program options. Benefit 3: Excellent speed Vowpal Wabbit is written in optimized C++ and it can take advantage of multiple processor cores. VW is 2-3 times faster than R if you count only train time, and ten times faster than R if you count preparation time, such as computing tf-idf. Benefit 4: No bottleneck on data size Most machine learning algorithms require you to read an entire dataset in the memory of one process. VW uses a different approach called online learning: it reads a training set, example by example, and updates the model with each example. It doesn't need to keep the mapping from a word to an index for weight in memory, because it uses a hashing trick . All it needs to store in a memory is a weight vector . This means you can train a model on a dataset of any size on a single machine -- tens of gigabytes of data is not an issue. Improving the VW API Vowpal Wabbit is inspired by good old UNIX command line tools, such as find . At Indeed, however, most of our infrastructure is in Java. We wanted to invoke VW from Java code, but we encountered two issues with its default Java wrapper: The wrapper requires boost to be installed on every server where it is used. Its API is very low-level, requiring you to operate with strings instead of providing a more convenient domain abstraction. To address these issues, we built our own open source JNI wrapper for VW. Adding vw-wrapper to your project Add a dependency on vw-wrapper using Maven as follows. No additional software is necessary. com.indeed vw-wrapper 1.0.0 Deploying the model to production You can deploy the model to production in three ways: Train the model via command line and deploy it to production by replicating the file with the model or putting it in Git with the sources Build one Java component that trains the model, stores it in a file, and replicates it to make predictions in a different component Train and make predictions in the same Java process: this can be useful if you want to make an online learning system (a system that continuously updates the model as new data becomes available) We’ve tested the library in the three main environments we use: CentOS, Ubuntu and macOS. We include shared libraries that are statically linked to VW in the distributed jar file. Examples of usage We reproduced each deployment model in integration tests, which also demonstrate using the Java API. The \"Movies lens dataset\" test illustrates how to use VW for user rating prediction. This test uses the lrqfa option to get a signal from latent (user, movie) interactions, as described in this factorization machines paper. See the test here . The \"Twitter sentiment analysis\" test illustrates how to use VW for NLP. This test demonstrates using raw text as features, the n-grams and skip-n-grams feature engineering techniques, and how to perform feature selection using the featureMask option. See the test here . What about the name: Vowpal Wabbit? Vowpal Wabbit is Elmer Fudd's version of \"Vorpal Rabbit,\" as seen in this video . Vorpal: a nonsense word from Lewis Carrol’s poem Jabberwocky and in this context, quick. One, two! One, two! And through and through The vorpal blade went snicker-snack! He left it dead, and with its head He went galumphing back. A Vorpal Rabbit is very quick. Get started with Vowpal Wabbit and Vowpal Wabbit Java Learn more about VW with Langford’s VW documentation . It explains VW features and includes tutorials and links to research describing how VW works under the hood. Check out our Vowpal Wabbit Java wrapper on Github. To learn how to use the wrapper, refer to our integration tests and Java API documentation , including information about useful parameters .", "date": "2017-06-14"},
{"website": "Indeed-Engineering", "title": "Delaying Asynchronous Message Processing", "author": [" by Mya"], "link": "https://engineering.indeedblog.com/blog/2017/06/delaying-messages/", "abstract": "At Indeed, we always consider what’s best for the job seeker. When a job seeker applies for a job, we want them to have every opportunity to be hired. It is unacceptable for a job seeker to miss an employment opportunity because their application was waiting to be processed while the employer makes a hire. The team responsible for handling applies to jobs posted on Indeed maintains service level objectives (SLOs) for application processing time. We constantly consider better solutions for processing applications and scaling this system. Indeed first adopted RabbitMQ within our aggregation engine to handle the volume of jobs we process daily. With this success, we integrated RabbitMQ into other systems, such as our job seeker application processing pipeline. Today, this pipeline is responsible for processing more than 1.5 million applications a day . Over time, the team needed to implement several resilience patterns around this integration including: Tracing messages from production to consumption Delaying message processing when expected errors occur Sending messages that cannot be processed completely to a dead letter queue Implementing a delay queue A delay queue prolongs message processing by setting a message aside for a set amount of time. To understand why we implemented this pattern, consider several key behaviors of most messaging systems. RabbitMQ: Guarantees at least once delivery (some messages can be delivered multiple times) Allows acknowledgement (ack), negative acknowledgement (nack), or requeue of messages Requeues messages to the head of the queue, not the end The team implemented a delay queue primarily to deal with the third item. Since RabbitMQ requeues messages to the head of the queue, the next message your consumer will likely process is the one that just failed. Although this is a non-issue for a small volume of messages, critical problems occur as the number of unprocessable messages exceeds the number of consumer threads. Since consumers can’t get past the group of unprocessable messages at the beginning of the queue, messages back up within the cluster. Between time indexes 18:38 and 18:39 on a 24 hour clock, the number of messages requeued due to error rapidly began to increase from 0 to over 90 thousand by time index 18:41. How it works While mechanisms such as a dead letter queue allowed us to delay message processing, they often required manual intervention to return a system to a healthy state. The delay queue pattern allows our systems to continue processing. Additionally, it requires less work from our first responders (engineers who are “on call” during business hours to handle production issues), Site Reliability Engineers (SREs), and our operations team. The following diagram shows the options for a consumer process that encounters an error: When a consumer encounters an error and cannot process a message, engineers must choose to have the consumer requeue, place into the delayed queue (which requeues after ttl expiration, or deliver to the dead letter queue, which requires a manual requeue. Engineers can make decide which of the three queues to choose by considering the following questions: Was the error unexpected? If your system encounters an unexpected error that is unlikely to happen again, requeue the message. This gives your system a second chance to process the message. Requeuing the message is useful when you encounter: Network blips in service communication A database operation failure caused by a transaction rollback or the inability to obtain a lock Does the dependent system need time to catch up? If your system encounters an expected error that may require a little time before reprocessing, delay the message. This allows downstream systems to catch up so the next time you try to process the message, it’s more likely to succeed. Delaying the message is useful for handling: Database replication lag issues Consistency issues when working with eventually consistent systems Would you consider the message unprocessable? If a message is unprocessable, send it to your dead letter queue. An engineer can then inspect the message and investigate before dropping or manually requeueing it. A dead letter queue is useful when your system: Expects a message to contain information that is missing Requires manual inspection of dependent resources before trying to reprocess the message Escalation policy To further increase your system’s resilience, you might establish an escalation policy among the three options. If a system requests a message to be requeued n times, start to delay the message. If the message is delayed another m times, send it to your dead letter queue. That’s what we have done. This type of policy has reduced the work for our first responders, SREs, and operations team. We have been able to scale our application processing system as we process more and more candidate applications every day.", "date": "2017-06-14"},
{"website": "Indeed-Engineering", "title": "Shifting Modes: Creating a Program to Support Sustained Resilience", "author": [" by Alex Elman"], "link": "https://engineering.indeedblog.com/blog/2021/03/sustained-resilience/", "abstract": "Originally published on InfoQ . Imagine for a moment that you work at a company that continuously ships customer-facing software. Say that your organization has managed to do the impossible and stopped having any serious incidents — you’ve achieved 100% reliability. Your product is successful. It’s fast, useful, usable, and reliable. Adoption increases, users desire new features, and they become accustomed to this success. As this happens, various pressures are continuously exerted on your organization — such as the pressure to ship features more quickly, the pressure to increase revenue, and the pressure to do more with less. Concurrently, there are additional constraints. Employees cannot be asked to work longer hours because work-life balance is a stated corporate priority. Given both this short-term success coupled with the constraints, what would happen over time? Photo by Mitchell Luo on Unsplash Since employees are not spending time responding to incidents, engaging with retrospective activities, and delivering on action items in this highly reliable future, they’ll have more time to respond to those business pressures for greater efficiency. The tradeoff with having no incidents is that existing employees will fall out of practice on how to collaboratively work to respond to and understand their products in Production (also known as operational underload ). Work will continue to increase in tempo, pace, and complexity. New employees will be hired and trained to account for the increase in workload. Unforeseen threats will act upon the system. Inevitably, there will be more incidents. Incidents are a signal from the system that change is happening too quickly and that there are mismatches between people’s models of the system versus the actual system. Incidents are a buffer that stabilizes the pace of change. Success is the reason that you will never be able to truly prevent incidents according to the Law of Stretched Systems . Embracing this inevitability will be the key to continued success in a climate of increasing complexity and interconnectedness. What I'm witnessing in the software industry is that we're getting stuck in a local maxima. We've plateaued in our approach to safety. I predict that if we don't level up how we cope with increases in complexity and scale soon, we'll be in big trouble. At Indeed, we’ve recognized that we need to drive organizational change to maintain the success we’ve had and keep pace with changing complexity and greater scales. Over the last 16 years, Indeed has grown quickly and the pace of change has accelerated. Because we recognize the importance of getting this right, we are implementing a shift to a Learn & Adapt safety mode within our resilience engineering department. In this article I will advocate that this mode shift is necessary in order to contend with the direction that the software industry is being pushed. I’ll describe the work necessary to enact this shift. Finally, I’ll compare the traits of an organization that is well poised for successfully persisting this mode shift. This shift won’t just make your organization safer, but also as Allspaw (2020) notes, \"changing the primary focus from fixing to learning will result in a significant competitive advantage.\" Different approaches to safety Facing down this increase in complexity and scale requires escaping the local maxima. A change in how an organization works is necessary. The shift is away from the traditional “prevent and fix” mode that's popular in software today. A prevent and fix safety mode is defined by a preoccupation with accident avoidance, strict controls, and a focus on what breaks. Prevent & Fix cycle An organization preoccupied with this type of safety mode is not spending time focusing on how to adapt to surprise. The organization might also be spending a lot of time fixing things that don't need the most attention. Sometimes preventative work can actually hinder opportunities for adaptations. For example, turning on MySQL safe mode in production to prevent UPDATE statements without a WHERE clause might prevent a recurrence of this type of mistake. Safe mode can also stymie a DBA jumping onto the MySQL command line to make a critical repair during an incident. By contrast, practicing a “learn and adapt” (Learn & Adapt) approach to safety means that encounters with incidents lead to an enhanced understanding of how normal, everyday work creates safety. Organizations that prioritize learning and adapting over preventing and fixing will also improve their ability to prevent and fix. I describe in more detail how that can lead to safer operations in a talk I gave at SREcon20 Americas . Learn & Adapt reinforcing loop There appears to be a broad consensus from the Resilience Engineering research literature that the Learn & Adapt approach is superior to approaches aimed at accident avoidance and local fixes. A set of traits make some organizations more successful at this than others. As article 1 in the InfoQ series mentioned , it’s unreasonable to expect anyone in an organization to have predicted the coronavirus pandemic, but it’s perfectly reasonable to anticipate and prepare for future encounters with surprise. It’s something that an organization can get better at over time with continuous focus and investment. One example of achieving this mode shift is in how an organization approaches its incidents. In the prevent and fix safety mode, incidents are seen as evidence of poor team performance, poor product quality, or avoidable losses. One primary cause is uncovered through causal analysis techniques like The Five Whys. The analysis typically ends there. By contrast, Learn & Adapt promotes using incidents as a lens through which an organization casts a light on processes, decision making, collaboration, and how work gets done. This is accomplished using an incident analysis loop that focuses on at least 50% of the human factors. This mode shift isn’t achieved by creating a new team, changing people’s titles, hiring the “right” person, or buying the “right” vendor product. It’s also not something that happens overnight. This mode shift requires the organization to change from within. It begins by sowing the seed of organizational change. Once the seed becomes a sapling, the organization can begin to achieve a continuous reinforcing loop of learning and adapting. This reinforcing loop requires constant nurturing and attention, much like caring for a delicate plant. The caveat is that the sapling can only emerge from the soil and thrive with the right mix of nutrients and the right environmental conditions. Many of those nutrients and conditions are related to organizational culture. Driving organizational change My intense focus in this area was inspired by an experience I had years ago when I participated in a string of hour-long retrospective meetings. I was invited to these meetings because I was an SRE and a recognized subject matter expert in RabbitMQ — a factor in several of those incidents. What I noticed struck me as a missed opportunity. In each of those meetings, over a dozen people were present in the conference room. In some cases, it was standing room only. It was a very expensive meeting. The facilitator went through the agenda, going over the timeline, the action items, and the contributing factors. It was a rote presentation rehashing what had happened, driven by the template document produced a priori. There was a call for questions, and the meeting ran to the end of the agenda within 25 to 30 minutes. We wrapped early. This was an opportunity where we had a lot of eager people in a room to discuss the incident, but I left the meeting without an improved or enhanced understanding about what happened. The facilitator followed the process faithfully, so I identified a problem with the process itself. I wanted to learn how to make this process more effective. And in pursuing this research, I found that there was so much more to learning from incidents than what I originally assumed. Once I recognized that process change was necessary, I solicited viewpoints from co-workers on why we conduct retrospectives at Indeed. Reasons I heard are likely familiar to most software organizations: Find out what caused the outage Measure the impact Ensure that the outage never happens again Create remediation items and assign owners While these answers reflect Indeed’s strong sense of ownership, it’s important to use these opportunities to direct efforts toward a deeper analysis into our systems (both people and technical) and the assumptions that we’ve made about them. When someone’s service is involved in an incident, there’s a concern that we were closer to the edge of failure than we thought we were. Priorities temporarily change and people are more willing to critically examine process and design choices. These approaches to a different organizational culture at Indeed are still relatively new and are evolving toward widespread adoption, but early indications are promising. After a recent learning review where we discussed an incident write-up, I received this piece of feedback: The write-up had interesting and varied content, effectively summarized crucial Indeed context, and demonstrably served as the basis for a rich dialogue. Participants revealed thoughtful self-reflection, openly shared new information about their perspective, refined their mental models, became closer as colleagues, and just plain learned cool things. I have made headway, but there is still a lot to do. While my efforts have benefitted from my tenure in the company, experience participating in hundreds of incidents, and connection to the research literature, I can also attribute some of my progress so far to three key organizational elements: Finding other advocates in the company Communicating broadly, and Normalizing certain critical behaviors Find advocates Advocates are colleagues who align closely with the goals, acknowledge where we could be doing better, and share a vision of what could be. They are instrumental to drive organizational change. Having multiple colleagues model new behaviors can help spur social change and create a movement. It’s very difficult to engage in this work alone. I’ve found these advocates and I wager they exist within your company as well. They are colleagues who keep an open mind and have the curiosity to consider multiple perspectives. I found one such advocate during an incident in 2020 that I analyzed. In a 1:1 debrief interview with a responder who had only peripherally been involved, I asked why they had participated in a group remediation session. Their answer demonstrates that advocates aren’t created; they’re discovered: I like to join just about every event [Slack] channel I can even when I'm not directly related. I find that these kinds of things are one of the best ways to learn our infrastructure, how things work, who to go to when things are on fire. Who [are] the people that will be fixing stuff? I learn a lot from these things. Like I said, even when it's not my stuff that's broken. Incident debrief interviewing is not the only place to locate advocates. I hold numerous 1:1s with leaders and stakeholders across the organization. I find opportunities to bring these topics up during meetings. I give internal tech talks and reach out to potential advocates whenever I visit one of our global engineering offices. Internal tech talks have the effect of drawing people out who have aligned interests or stories to share. They will make themselves known, perhaps by approaching you after the talk. You may find them to be advocates who can help socialize the movement within your organization. Indeed has offices all over the world, across different time zones. Advocates in each of those offices bring uniformity to the campaign. Communicate broadly The second key component of driving organizational change is ensuring the messages are heard across the entire organization — not just within a single team or function. Organization size is an important influence when engaging in broad communication. A 10,000 person org poses different challenges than a 1,000 or 100 person org. As much as I might think that I am sufficiently communicating the details of a new program, it’s rarely enough. I find that I have to constantly over-communicate. As I over-communicate and leverage multiple channels, I may sound repetitive to anyone in close proximity to my message. This is the only way to reach the far edges of the organization that might not otherwise hear me. The same communication challenges present themselves in the aftermath of an incident when a team discovers and applies corrective actions. These are often “local-only” fixes, interventions, and lessons that only benefit the part of the organization that experiences the incident. The global organization fails to learn this (sometimes costly) lesson. Ron Westrum, a researcher in organizational behavior, notes in A typology of organisational cultures : One of the most important features of a culture of conscious inquiry is that what is known in one part of the system is communicated to the rest. This communication, necessary for a global fix, aids learning from experience, very important in systems safety. The communication occurs because those in the system consider it their duty to inform the others of the potential danger or the potential improvement. It's not enough for a team to capture and address important technical fixes and lessons learned in their retrospective materials. Allspaw (2020) spent two years observing how software organizations engage with incidents and found that \"hands-on practitioners do not typically capture the post-incident write-up for readers beyond their local team\" and \"do not read post-incident review write-ups from other teams.” The organization doesn’t truly benefit until those lessons are scaled far and wide. For the write-ups to be useful, they have to teach the reader something new and help draw out the complexities of the incident. Normalize new behaviors Organizational change involves new modes and behaviors. Some of those modes and behaviors might be at odds with how things used to be done. Or they are just non-intuitive. This places a barrier on reaching a critical mass in these desired behaviors. A good place to get started is by modeling the changes yourself. Normalizing these modes and behaviors will help them spread to early adopters and then spawn a social movement. I’ve found there are four main areas to focus on to successfully promote a Learn & Adapt mode to safety. 1. Normalize stating your assumptions as much as possible Assumptions are beliefs you hold that are sometimes so obvious or (seemingly) self-evident that stating them explicitly doesn’t seem necessary. It’s very likely that what you think is obvious might be surprising to others. For example, you might believe that the fact that the MySQL primary can’t safely fail over to another datacenter automatically is so obvious as to not be worth explicitly stating often. In reality, your colleague might believe the exact opposite. Stating your assumptions gives others an opportunity to recalibrate their model if there’s a mismatch or vice-versa. The conversations between a group of people recalibrating their models of the system are some of the most insightful conversations I’ve experienced. Great places to state your assumptions are in design review materials and in merge requests. What do you assume will happen in the presence of 10% packet loss? What about 50% packet loss? Do you assume that system clocks are always monotonically increasing? Do you assume that your consumer services will never encounter duplicate messages? What do you assume might happen if they do encounter duplicates? Stating these assumptions explicitly will elicit important conversations because participants in these reviews will come with their own assumptions about what you assumed about your design. There’s no impetus for participants to challenge your assumptions if they assume yours matches theirs. 2. Normalize asking a lot of questions This is another approach that can help surface mismatched models of the system. Curiosity is an important cultural trait that nurtures Learn & Adapt. You might worry that asking questions betrays a surprising gap in your knowledge, but if everybody asks a lot of questions, it takes the sting out of asking them. Asking questions can also help promote a more psychologically safe workplace. Discussing technical topics in front of an audience of peers can be stressful. Everybody has errors somewhere in their mental models and you’re bound to surface those through discussions. The way that those errors are revealed to you are reflected by the cultural norms of your organization. Telling a colleague, “Well, actually there are several problems with what you just said…” has a chilling effect on their willingness to state their assumptions in the future. Even if you’re certain that somebody is wrong, be curious instead of corrective. Ask follow-up questions to reveal more of their mental model: “Did you notice any deprecation warnings at compile time?” Posing the mismatch as a question instead of a correction will lead to a more productive and psychologically safe exploration of the problem space. It also makes room for you, the corrector, to be incorrect, which also promotes an aspect of psychological safety. 3. Normalize increased cooperation between roles that traditionally don’t directly work together A great example of this is product/engineering and client-facing roles like customer support or client success. Invite members of those teams to design reviews. Invite them to retrospective meetings or group learning reviews. Sometimes the client-facing support teams are the very first people in an organization to learn about a serious problem. The time between client-facing teams discovering the issue and the product teams learning about them is critical. The work needed to shorten that delay has to happen before the incident occurs, not during. There was an incident in 2019 that was first detected by the client success team. During the interview phase of the incident analysis, I asked a product manager about how their team directly engages with the client success team. Their response was dismissive of the idea at first: “I don’t think that a sufficient solution for [this incident] should be relying on [customer] feedback to let us know of an issue. It’s too slow of a mechanism to help us identify a high impact issue.” The corrective action for this incident was to add automated detection. While that corrective action will help detect a recurrence of the same impact, it misses an opportunity to work on better engagement and cooperation with the customer-facing teams. Incidents with impact that evade the existing detection in the future will take longer to resolve. 4. Normalize sharing incident analysis deliverables with everyone in the company Sharing and discussing incident write-ups is arguably the most important aftermath activity. The STELLA report delivered by the first cycle of the SNAFUcatchers Workshop on coping with complexity highlights this value: Postmortems can point out unrecognized dependencies, mismatches between capacity and demand, mis-calibrations about how components will work together, and the brittleness of technical and organizational processes. They can also lead to deeper insights into the technical, organizational, economic, and even political factors that promote those conditions. Postmortems bring together and focus significant expertise on a specific problem for a short period. People attending them learn about the way that their systems work and don't work. Postmortems do not, in and of themselves, make change happen; instead, they direct a group’s attention to areas of concern that they might not otherwise pay attention to. Cultural traits Moving from a prevent and fix safety mode to Learn & Adapt involves changing the very nature of how organizations get work done. If your organization is already relatively successful at delivering products to customers, then making changes to the organization can be risky or even ill advised. Change must be deliberate, incremental, and continuously monitored if it is to result in a net benefit. While the idea of a “safety culture” is problematic, there exists a connection between an organization’s culture and its ability to successfully prepare for surprise, navigate complexity, and learn from incidents. Culture is, as defined by Westrum (2004), “... the organisation’s pattern of response to the problems and opportunities it encounters .” These patterns are informed by the shared set of behaviors, beliefs, and actions promoted and encouraged in an organization. A cultural norm might be obligating people to “own” their mistakes by expecting a set of follow-up behaviors in the aftermath of an incident. In reflecting on the cultural norms within my own organization, I’ve identified some tradeoffs we’ve made that have helped cultivate and promote this shift toward Learn & Adapt. Opportunity over obligation How an organization handles accountability and responsibility is one aspect of the cultural norms. After a costly incident, a lot of attention is cast upon the parts of the system seen as broken or faulty. If there are considerable losses involved, a common reaction is to isolate a person or team to take responsibility and show accountability for the recovery and prevention activities. People engage with a task differently when they feel it’s an obligation versus an opportunity. Opportunity is taken whereas obligation is assigned (whether explicitly or implicitly). It is leadership’s role to highlight opportunities by making them attractive, clearly defined, and actionable. One way to make opportunities more attractive is to change the incentive structures. Ryn Daniels, a software infrastructure engineer, describes a leverage point for crafting a resilient culture : While there is a lot that goes into psychological safety in the workplace, one way to design for a culture of learning and blamelessness is to look at the incentive structures within your organization. Instead of expecting people to own their post-incident activities, strive to make the opportunity attractive enough for anyone to select. Ryn suggests a strategy: If your skills matrices for promotions include things like community contributions, post-mortem facilitation, or incident writeups, that can also provide incentive for people to take part in learning-focused activities. The behaviors that get rewarded and promoted within your organization will have a great deal of impact on its culture. Creating opportunities instead of assigning ownership not only helps ensure more thorough results, but fosters psychological safety. Flexibility over rigidity Placing rigid constraints on decision-making, new technologies, access, and what people are allowed to do in delivering their work can hinder opportunities for adaptation by undermining sources of resilience. These constraints accumulate over time as scar tissue from previous encounters with costly outages. Rigid constraints can help an organization navigate legal risk, security risk, and financial risk, but they can limit flexibility. More flexibility can prove useful for adaptation because it gives people space to be curious and exercise their interests in other roles. How does the organization respond to a database administrator giving unsolicited help to the security team? What about a data scientist participating in a code review when it’s unrelated to their work or product? Being told to “stay in your lane” can be a manifestation of cultural norms that bias toward rigidity and could be a reflection of people’s insecurities, previous encounters with failure, or fear there is more work to do than available bandwidth. Fostering this flexibility can pay immense dividends when expertise emerges during an incident in an unexpected way. Agility over speed One of the most important engineering priorities at Indeed is velocity, which is the shortening of the development cycle from idea to delivery. While speed is important in the delivery of software, speed isn’t sufficient to adapt to unanticipated challenges. “Turning the ship” is a common metaphor to highlight the challenges of quickly changing direction as a function of organization size and speed. Agility is a trait that is useful in helping recognize when to change course and accept the sunk costs. In an incident, agility could mean recognizing and escaping cognitive fixation during diagnosis. After an incident, agility could result in the local participants sharing what they’ve learned so that the global organization can take heed and quickly recruit resources by pulling them from less important projects. Agility is a necessary (but not sufficient) aspect of promoting a Learn & Adapt approach to safety. Trust over suspicion Trust is fundamental to an organizational culture that learns and adapts. Trust colors our interpretations when we witness the actions of others when we don’t have the benefit of context. Trust means that we can assume that others are acting in good faith. Sometimes it can be easy to jump to anger or disgust with our colleagues when we are armed with hindsight in the aftermath of an incident. Trust means that we allow that they may have encountered substantial challenges. In a low-trust environment, fear, judgment, sanction, rigidity, and blame are common coping mechanisms. Making the shift In the course of introducing these new approaches in my own organization, I sometimes encounter pushback about how engaging in incident analysis distracts from getting \"real\" work done. I remind them that this is the real work. Engineering is knowledge work and requires continual learning. Not only does engaging in incident analysis help people get better at their job as they learn more, but incident analysis is a form of knowledge creation. Ralph D. Stacey, an organizational theorist, helped me make the profound observation that simply filing away an incident report is not new knowledge : From mainstream perspectives, knowledge is thought to be stored in individual heads, largely in tacit form, and it can only become the asset of an organization when it is extracted from those individual heads and stored in some artifact as explicit knowledge. Incident write-ups do not become organizational knowledge until they are actually used: Knowledge is the act of conversing and new knowledge is created when ways of talking, and therefore patterns of relationship, change. Knowledge, in this sense, cannot be stored. Knowledge is created when a group of people meet to discuss a well-crafted incident write-up. Knowledge is created when it is communicated broadly and reinforced through normalized behaviors. Incidents cannot be prevented, because incidents are the inevitable result of success. Organizations that have the cultural elements to foster a Learn & Adapt mode to safety will embrace the desirable aspects of incidents. Incidents can lead to bridging new connections, engaging with fresh perspectives, surfacing risks, and creating new training material. If you’re part of an organization that considers incidents avoidable, detestable, or disruptive, it’s likely that you’ll need to change more than just the retrospective process. Start small, mirror the behaviors that cultivate Learn & Adapt, and be patient. Before long, a sapling will emerge. About the author Alex Elman has been helping Indeed cope with ever-increasing complexity and scale for the past nine years. He is a founding member of Indeed's site reliability engineering team. Alex leads the resilience engineering team that focuses on learning from incidents, chaos engineering, and fault-tolerant design patterns. References Allspaw, J. (2020). How learning is different than fixing . (Adaptive Capacity Labs blog.) (video). Accessed Oct. 20, 2020. Daniels, R. (2019). Crafting a Resilient Culture: Or, How to Survive an Accidental Mid-Day Production Incident . (InfoQ: DevOps.). Accessed Dec 28, 2020. Elman, A. (2019). Improving Incident Retrospectives at Indeed . (Learning from Incidents in Software blog.). Accessed Oct. 20, 2020. Elman, A. (2020). Are We Getting Better Yet? Progress Toward Safer Operations. In USENIX Association SREcon Conference, USA. Schemmel, M. (2019). Obligation vs Opportunity. (Internal Indeed Engineering blog.) Unavailable. Accessed Oct. 20, 2020. Spadaccini, A. (2016). Being On-Call. In Site Reliability Engineering: How Google Runs Production Systems. Sebastopol, CA, USA: O'Reilly Media, Inc. First Edition, ch. 11, pp. 125-132. Stacey, R. D. (2001). Mainstream Thinking about Knowledge Creation in Organizations. In Complex Responsive Processes in Organizations: Learning and knowledge creation. London, England, UK: Routledge. Westrum, R. (2004). A typology of organisational cultures. In Quality & Safety in Health Care, 13, ii22-ii27. 10.1136/qshc.2003.009522 Woods, D. D. (2002). Steering the Reverberations of Technology Change on Fields of Practice: Laws that Govern Cognitive Work. In Proceedings of the Twenty-Fourth Annual Conference of the Cognitive Science Society. pp.14-16 10.4324/9781315782379-10. Woods, D. D. (2017). STELLA: Report from the SNAFUcatchers Workshop on Coping With Complexity . Columbus, OH: The Ohio State University.", "date": "2021-03-04"},
{"website": "Indeed-Engineering", "title": "Indeed’s FOSS Contributor Fund: 2021 Updates", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2021/02/indeeds-foss-contributor-fund-2021-updates/", "abstract": "Indeed’s FOSS Contributor Fund is now live for 2021. The program enables Indeed employees who make open source contributions to nominate and vote for projects to receive a donation from the fund. We’re proud that the fund has introduced more Indeedians to the open source community. This post shares updates for those interested in adopting a fund of their own. What’s new with Indeed’s FOSS Fund As we enter year three of running the program, we’re excited to announce a few updates for how it will operate in 2021: Previously, we selected one project per month. For 2021, we’ll select three projects each quarter, each receiving $10,000 USD. We are making this change as a way to streamline the operations of running the fund and to improve the overall experience for our open source contributors. We believe a quarterly voting cadence will open up some thoughtful conversations about nominations, eligible projects, and impact. To be eligible to vote on which projects will be selected, Indeed employees will need to participate in open source initiatives during the previous quarter. For our first quarter in 2021, nominations for the fund will be open through March 31. Our first round of voting will begin on April 15. Anyone at Indeed can nominate a project. Updates to our GitHub repo Indeed Open Source also launched a new FOSS Fund GitHub Pages site . Here, you can find information on how we organize our FOSS Fund and suggestions on how your company can integrate the program. We will continue to update this resource as our fund evolves. Contact us If you are interested in joining the community of FOSS Fund adopters, want more information, or would like to join a session, please email us at opensource@indeed.com . Learn more about Indeed’s open source program . Indeed’s FOSS Contributor Fund: 2021 Updates—cross-posted on Medium.", "date": "2021-02-08"},
{"website": "Indeed-Engineering", "title": "Engineering at Indeed", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2012/11/engineering-at-indeed/", "abstract": "Welcome to Indeed’s engineering blog. For the last 8 years, our team has been building the software that has made Indeed the #1 job site worldwide: over 85 million unique visitors search for jobs on Indeed each month. We offer this service in more than 50 countries and 26 languages. Update 12/4/2014: At our 10-year mark, 150 million unique visitors conduct job searches on Indeed each month and we offer the service in 28 languages. We like our mission: we help people get jobs. “What’s best for the job seeker?” is a question that we ask ourselves every day. That’s at the core of what makes Indeed special, but there are a few other reasons we think Indeed is a fantastic place to be a software developer. Delivering the best job seeker experience at scale means we get to work on interesting technical challenges. We make data-based decisions using tools that allow us to test and measure ideas. And we have a high bar for hiring, so we get to work with people we respect and want to learn from. We appreciate how some of the great engineering blogs out there (including Twitter , Netflix , and Etsy ) have contributed to the community by sharing what works (and sometimes what doesn’t). We want to contribute to that discussion by sharing what we’ve learned at Indeed. Our first few posts will include an overview of our event logging architecture and an introduction to our unique service framework. We’ll occasionally share tidbits about other fun stuff like hackathons and pinball machines , but mostly we want to talk about the technology that we get to work on every day. Follow us on Twitter @IndeedEng and we’ll keep you in the loop! Working @ Indeed Austin", "date": "2012-11-28"},
{"website": "Indeed-Engineering", "title": "Boxcar: A self-balancing distributed services protocol", "author": [" by ", " and Courtney Jeffries"], "link": "https://engineering.indeedblog.com/blog/2012/12/boxcar-self-balancing-distributed-services-protocol/", "abstract": "At Indeed, we have an unrelenting obsession with speed. We believe site performance is critical to providing the best experience for job seekers. Since Indeed launched in 2004, traffic growth has been swift and steady. Today, we have over 85 million unique visitors and over 2 billion searches per month. Traffic to Indeed continues to rapidly increase, so ensuring our systems scale with this traffic really matters. In order for our site to be fast and reliable, we developed Boxcar, an RPC framework and load distribution strategy for interacting with remote services using a generic layer to transport raw bytes. Considerations Before we developed Boxcar in 2010, Indeed’s main search site was powered by a single web application. In 2009, we split this application into two separate but highly-coupled server processes to address some bottlenecks, but this was only a stop-gap solution. The two processes were always deployed on the same machine to simplify configuration and minimize potential protocol compatibility issues. This solution unfortunately prevented us from maximizing hardware utilization, increased vulnerability to certain kinds of failures, and required a carefully managed deploy process during our weekly releases. We decided that we needed to implement a more general service-oriented architecture to improve system scalability. We considered various architectures and network topologies that were in common use. Some required expensive, custom hardware or larger numbers of commodity machines. Others introduced latency or additional points of failure. Dissatisfied with readily available options, we decided to develop something new. The resulting solution includes an elegant algorithm that reliably routes requests from front-end applications to back-end services without a single point of failure or intermediate hops. This layer is itself protocol-agnostic, but we standardized on Google Protocol Buffers to provide structured remote service invocation with excellent forward and backward compatibility. When a service instance becomes unavailable, there is no disruption to our site as a result of the core algorithm adapting nearly instantaneously and routing requests only to the remaining available instances. It also transparently diverts traffic towards servers with faster response time. Implementation The basic Boxcar algorithm is simple. Each server maintains an ordered list of connection slots. Each client sends a given request to a server based on the lowest slot number available across the whole pool. This provides a lightweight mechanism for clients to balance requests without the need to directly measure server load. Figure 1: each client’s pool contains the lowest server slot number available Servers give out their lowest available slot when a connection is established. The same slot number is never simultaneously associated with multiple active connections. The slot associated with a connection does not affect how the server treats requests on that connection; it is only on the client that this affects request distribution. Each Boxcar client maintains a persistent, fixed-size connection pool that it constantly adjusts to maintain balance. When making a request, a client uses the connection from its pool with the lowest slot number. In the background, the client optimizes this connection pool. It establishes new connections to available servers even while servicing requests using already established connections. If a new connection has a lower slot number than any connection already in the pool, the client kicks out the existing connection and replaces it with the new one. Otherwise, it discards the new connection and sleeps briefly before trying again. The pool is managed by a specialized data structure that handles the non-I/O operations in a matter of microseconds. When a client goes away, the server reclaims the slot numbers associated with its connections. If these are low slot numbers, they are quickly allocated to the remaining clients. These clients then discard the connections they had associated with higher slot numbers in order to make room in their pools. (see Figures 2-3). Figure 2: client with a low slot disappears Figure 3: low slots are reclaimed and allocated The net effect is that each client has pre-computed which connection will be used for the N th concurrent request. Our clients are typically web applications that service simultaneous requests themselves. If a client only services one request at a time, every request will use the same connection — the one in the pool that has the lowest slot number. All other connections will sit idle. If the client services two concurrent requests, each request will use one of the two connections in the pool with the lowest slot numbers. The other connections will remain idle. This generalizes to as many concurrent requests as there are connections in the pool. If there are more concurrent requests than there are connections, additional requests are rejected. We reject requests in this extreme condition in order to fail fast and prevent excessive load on the services. We size our connection pools based on expected peak traffic levels, so this rejection should only happen during extraordinary conditions. If a remote service fails to serve a request, the client disposes of the associated connection and retries the request on the next best connection. The number of retries that can happen is bounded in configuration. Disposing of the connection frees up an additional space in the pool for the greedy connection acquisition threads to fill, which quickly restores the pool to capacity. Given the random distribution of connections, the connection selected for retry could be to the same failing server. Failures due to unavailable servers tend to be fast, usually sub-millisecond in our networks, so the client will incrementally purge its pool of these connections. This continues until the client finds a connection to a live server, reaches its retry limit, or purges all connections in its pool. The latter two conditions both result in the request failing outright. Figure 4: three clients seeking the best slots on two servers Figure 5: server loss Figure 6: following server loss, clients continue to find the best slots Figure 7: a new server is added Figure 8: clients connect to the best slots available If any single server is slow, connections to that server are more likely to be busy. Additional requests from that client will need to be serviced using different connections. The client preference for lower slot numbers means that those are likely to be connected to different servers, thus avoiding the bottleneck. The overall effect is that slower servers will service fewer requests, meaning more load is distributed to the remaining servers. Servers that are running at half speed will receive half as many requests, on average. Insights Load balancing implies a strictly equal distribution of load across all instances. That degree of consistency is a difficult problem to solve, and we do not claim to have solved it. The key realization we had when designing Boxcar was that could achieve our goals by solving a much simpler problem. Boxcar balances requests using an elegant strategy for allocating persistent connections to incoming requests. Our approach does not guarantee a uniform distribution of requests or load. Instead, we try to avoid sending too many requests to a single server. This is a considerably simpler goal to achieve and was based on two insights. The first insight was that we don’t need to directly balance individual requests. Instead, we balance connections probabilistically and impose a stable connection selection heuristic. This indirectly results in good balance for individual requests. The balance is not perfect in every instance at every moment, but in aggregate, the results are well-balanced, and the outliers are brief and rare. In our use cases, the number of requests is a reasonable proxy for server load, so a relatively uniform distribution of requests results in a relatively uniform distribution of load. The second insight was that we do not need to have uniform distribution of load across instances. Suppose there are 10 servers and 3 servers worth of load. The implied goal with load balancing is to have each server at 30% utilization. However, for our goals, it’s acceptable if at some moment we have 3 servers at 100% utilization and 7 servers at 0% utilization, or 5 servers at 60% utilization and 5 servers at 0%. Our goal is to stably and efficiently serve our site traffic. As long as no server exceeds its capacity to serve requests stably and in a timely fashion, we really don’t care how uniform the load is. Results We’ve now been using Boxcar for nearly three years. We use it in production, testing, and development environments. Today, well over a dozen different services across nearly all of our products use this framework, carrying many hundreds of millions of requests and many gigabytes of data each day. With Boxcar, Indeed is able to scale well with low cost commodity hardware while maintaining high availability. Our networks are set up with a simple topology without any added latency or network hops. Failures of individual systems have almost no impact on our ability to serve our users. Our operational costs are lower because our systems are less fragile. Over the years, Boxcar has delivered all of the benefits we hoped for and has proven durable in numerous different applications. It has proven versatile, bringing valuable scaling capabilities to a broad range of products that collectively generate many hundreds of millions of requests per day. The core of Boxcar combines a few simple concepts into a sophisticated, resilient system for load distribution. We will cover other interesting aspects of Boxcar in future posts.", "date": "2012-12-17"},
{"website": "Indeed-Engineering", "title": "Indeed Coding Duel – UIUC vs UT Austin", "author": [" by Doug Gray"], "link": "https://engineering.indeedblog.com/blog/2013/03/indeed-coding-duel-uiuc-vs-ut-austin/", "abstract": "On February 19, we hosted the annual Indeed Coding duel between UT Austin and UIUC . Sixty-six contestants, mostly Computer Science and Computer Engineering undergrads, battled it out online for four hours.  As in previous contests, the challenges for the contest were developed by Indeed software engineers, including recent UT and UIUC graduates as well as previous contestants from the renowned global programming contest ACM-ICPC World . We presented the contestants with seven logic and mathematical problems, and challenged them to solve them with code. The winning programmers solved the most problems in the least amount of time. Three of the top five contestants were from UT, while UIUC was the overall winner with five of the top nine entrants. On top of the bragging rights, the highest-scoring contestants from each school took home cool prizes including Apple iPads, Amazon Kindle Fires, and gift cards.", "date": "2013-03-12"},
{"website": "Indeed-Engineering", "title": "From 1 to 1 Billion: Evolution of a Document Serving System", "author": [" by ", ", ", " and Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2013/03/from-1-to-1-billion-part-1/", "abstract": "[Editor’s note: This post is part 1 of a two-part companion piece to our first @IndeedEng talk . Slides and video are available .] Indeed.com launched in November of 2004 in the US. Today, Indeed is in more than 50 countries and has 100 million unique visitors performing over 3 billion job searches each month. We have more traffic than any other job site in the world. If we had tried to support this scale on day one, we might never have launched, since building a job search system to support our eventual level of traffic would have required time and knowledge we didn’t have. Likewise, if we hadn’t evolved our systems as growth demanded more of us, we wouldn’t be where we are today. The journey to 100 million unique visitors took us 8 years. We believe in building products that are fast , simple , comprehensive , and relevant . We keep fast and simple in mind when building solutions, too -- we put new features in front of our users and iterate quickly to continuously improve our products. This post provides a brief history of the iterations we made as we scaled our system to billions of searches per month. 2004: The simplest solution that works To get the first version of job search implemented back in 2004, we turned to Lucene , a Java-based open source project that provides indexing and search. Once you’ve built an index of your documents -- jobs, in our case -- Lucene can efficiently and quickly turn a search query into a set of matching document IDs. Our search webapp used the Lucene index to get the document IDs that matched a user’s search, but we also needed the corresponding job data to display. Turning those IDs into displayable job data is what we call “document serving.” The watch words for document serving are comprehensive and fast . We are constantly aggregating job postings from all over the world, and we need to be able to serve data for all jobs for all time. We need to make new jobs and job updates available on the site as quickly as possible, and we need job document serving to be extremely fast. We show 10 jobs on a search results page. For each of these results, we display attributes like title, company, location, job age, and a snippet based on the job description. Figure 1: sample search result highlighting document attributes For our first iteration, we kept the job document data in the Lucene index using stored fields . A commonly-used feature of Lucene, stored fields allow you to persist the document data with the index while it is being built. The data flow of this first iteration is shown in Figure 2. The index builder ran every few minutes, read jobs from the database, and built the Lucene index. This index was then copied to the web servers with rsync and used by the search webapp to both search and serve the documents. Figure 2: serving documents from stored fields In our first month we aggregated 2 million jobs, and on our busiest day we served 160,000 job documents, with peaks of 7 documents/second. Our system worked; search was fast, and job results were fresh. 2005: Size matters One year later, in November 2005, we were aggregating almost 4 million new jobs per month. We were serving up to 5 million documents per day with peaks of 100 documents/second. We had nearly doubled the number of searchable jobs, causing the index to grow to between 6GB and 7GB. The stored fields accounted for most of that file size. Lucene is extremely fast when the entire index can fit in the disk cache, but our production servers only had 4GB of RAM. To execute a search, the server had to page in data from disk, resulting in slower job search performance. The index determines which jobs we show on a search result page, so we don’t want to keep jobs in it that are old, or have been removed from the original posting source. Even if we no longer showed a particular job on a search result page, a user might come back to a job details page at any time from a bookmark or email. Therefore, even when a job is not in our index, we still need to be able to serve it. Since it would have not been feasible to keep all jobs in our Lucene index forever, we had to use another approach. The evolution begins We needed to revise our architecture in order to optimize search performance, which is a key part of the job seeker experience. We decided to serve job data directly from a MySQL database that was already the primary storage location of job data. MySQL provides options for aggressive caching, and the main job data table already had a primary key index on the job ID, making lookup fast. The database was hosted on a separate, more powerful machine. Querying it directly, instead of requiring all that data to be available on the machine hosting the search webapp, would allow us to reduce resource utilization on those web servers. Retrieving jobs directly from the database allowed us to quit using stored fields, drastically reducing the size of the Lucene index. We still indexed the same jobs, and the search webapp still used that index for searching. The difference was that now Lucene would only return the matching job IDs, and the search webapp would retrieve the data for those jobs from the database (see Figure 3). This change was a success. The index was smaller, document lookups were faster, and job search performance improved. Figure 3: serving documents from MySQL database 2006: Too much contention We used the direct database access approach for over a year. In November of 2006, we were aggregating about 5.2 million new jobs each month. We were serving up to 21 million documents per day, with peaks of 500 documents/second, a 5x increase from the previous year. Though our average job search time measurements were still quite fast, we noticed slower performance at certain times of day. The cause of this slowdown was write contention on the main jobs table, which used the MyISAM storage engine. Writes from our aggregation processes were locking out reads from search, because MyISAM uses table-level locking. Conversely, reads from the search side were blocking those writes. Executing user searches and getting new job data into our database were getting in each other’s way. One option was to switch to MySQL’s InnoDB storage engine, which does row-level locking, but that migration required some tricky changes in terms of increased hardware cost and far-reaching code changes. We eventually did migrate, but we needed a solution we could deploy sooner. Another solution we considered was to replicate the primary database and serve job documents from a replica. This approach works for systems with a read-heavy workload. We had too many writes happening for that to help us. Since MySQL replication is single-threaded , it would fall behind the master as the large number of reads from the search webapp locked out the replication stream writes from that single writer thread. If our search webapp was reading from a replica and replication fell behind, the index might return job IDs to the webapp that corresponded to jobs not yet in the replica. When the database can’t keep up, cache it! To address these problems, we chose a classic approach: application-level caching. We used Memcached , an open source, high performance, in-memory object caching system. At this same time, we were evolving other aspects of our search webapp. We started transitioning from a single webapp that did everything to a service-oriented architecture. One of the first services we carved out of the search webapp was docservice, which handled the work of document serving. The search webapp made a request to the docservice for job document data. Multiple search webapp instances could be supported by a single docservice, as shown in Figure 4. For the first iteration, the search webapp communicated with the docservice over HTTP. We later changed the implementation of that communication to use our own efficient service infrastructure, called boxcar . Figure 4: serving documents from memcache To serve a document request, the docservice first looked in the memcached and only queried the database if the job was not cached. A memcached lookup took between 1 and 5 ms, and a database read could take 10ms or longer, depending on lock contention. To maximize the cache hit rate and avoid database access, each docservice had a background thread that would watch for new job data and load those jobs into the cache. To avoid a new docservice with an empty memcache sending too many requests to the database, we “primed” the memcache. Before putting a new memcached instance into production, we would load data for as many recent jobs as possible into the cache. This technique allowed us to ensure that most job documents would be served out of the cache, even for a brand new docservice instance. The introduction of memcached made job document serving reliably fast throughout the day. It resulted in far less database load and contention with aggregation processes, and it enabled continued rapid growth. 2009: Worldwide This approach to document serving served us well for more than 2 years, through our early international rollout. By November 2008, Indeed was in 6 countries, and we were aggregating 7.1 million jobs per month. We were serving up to 150 million documents each day with peaks of 3,000 documents/second. To support this scale, we added more production servers and brought new docservice instances online. We could not aggressively prime the memcache using the database, since that would lock out writes on the MySQL master for minutes at a time. Instead, we had to limit the rate at which the priming process queried the database. We projected that it would soon take 36 hours of rate-limited priming to warm up a new memcache daemon before it could serve live traffic. At the same time, we faced a limitation with our data pipeline. We were extracting more data from jobs after our initial aggregation, and these attributes were stored in other tables and even other databases. Some example attributes were job language, job type (full-time, part-time, etc) and salary. The only options for making this information available for job display in our current architecture were table joins or multiple database queries per search. Both were unacceptable because they would slow down job document serving and increase database load. We could have also gone back to using stored fields but with our index sizes steadily increasing, that was a non-starter. We also needed to host our site in data centers around the world to deliver the best experience for users outside the US. Even with memcached, we still needed the database sometimes. Having data centers around the world depend on a single database would add an unacceptable amount of latency, and we had already learned that replication of the database wasn’t workable for our use case. It was time to implement a single data store that would contain all job attributes, replicate easily to remote data centers, and provide fast access. A new data store Our new solution, called docstore, was a denormalized, read-only, file-based store for job data. A docstore builder process wrote a job’s data, serialized using Thrift , and stored it in a compressed segment file of up to 100 jobs (in job ID order). A simple naming convention made it easy to look up a job given its ID, as shown below. When a request came in for a document, we would locate the segment file by a simple directory lookup, read in the segment file, and retrieve the job data by its offset in the file. The docservice still used memcache as the preferred source of data, but it would fall back to the docstore when a job was not in the cache, and only make a database request if a job was absent in both. The docstore never deleted data, so a job would only be absent if the docstore building or replication had fallen behind the latest production index. We later updated our search service interface to exclude results that were not yet in the docstore, so we could remove that database dependency entirely. This allowed us to launch in remote data centers, where it would have been too expensive to query a distant database, even infrequently. It worked -- serving jobs and priming the cache to bring a new docservice online were fast worldwide. With this new approach, the docstore builder became the only direct consumer of the job databases, dramatically reducing database contention. We again used rsync to replicate the resulting docstore to remote docservice servers (see Figure 5), and the docstore was the source of all job data for serving search results. Launching the docstore enabled us to expand our international presence in 2009 to 23 countries. By November 2009, we were aggregating 22.5 million jobs per month worldwide. We were serving up to 312 million documents each day with peaks of 6,000 documents/second. Figure 5: serving from docstore 2011: Great performance worldwide, until... The docstore solution worked well for us into 2011. Breaking the runtime database dependency in document serving enabled rolling out to worldwide data centers while maintaining our high standards for performance and availability. By the end of 2011, we were aggregating 41 million new jobs per month and serving up to 850 million job documents each day, at a peak rate of 14,000 documents/second. Basing our docstore directory structure on job ID made it easy to find jobs. However, it also meant that when our aggregation system discovered an update to a job, the docstore builder needed to change the contents of an existing segment file to stay up to date. We know from our data that the average number of updates per job is 1.5 -- close to half of the jobs in our index will be updated at least once, and many are updated more than once. For each update, the docstore had to first find the segment file containing the job, decompress and load the data for all 100 jobs in the segment, update the job, then recompress and write the whole segment file back to disk. The job update stream processed by the docstore builder was based on time and not jobID. Therefore, the writes to the docstore did not benefit from the jobID locality we had in our directory scheme. This lead to the first bottleneck we encountered with the docstore: write volume. There were so many updates happening essentially randomly across the disk that a single drive had trouble keeping up. This also slowed down replication for the same reasons. Updating the remote docstores required the exact same pattern of random updates that were slowing the primary docstore builder. Another side effect of updates being written in place is that a single run of the docstore builder could affect disparate segment files across the docstore. This made it very hard to detect corruption of segment data due to unexpected issues. Though corruptions were rare, they required a lot of manual work to determine what jobs were being updated at the time of the corruption, locate all of the segment files affected, and restore them. Fundamentally, the docstore was based on having a single copy of every job, which was kept current as the job was updated, and had a strict requirement for where the file lived on disk. But the resulting write patterns of this paradigm were hitting the physical limits of our drives, and there was no clear way to improve that with the existing docstore. By late 2011, it was time for something new. We built a completely new, highly scalable docstore, and it has been in production for over a year now with no scalability limits in sight. In our next blog post, we will describe the key implementation details of docstore v2.", "date": "2013-03-11"},
{"website": "Indeed-Engineering", "title": "Logrepo: Enabling Data-Driven Decisions", "author": [" by Jeff Plaisance and Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2012/11/logrepo-enabling-data-driven-decisions/", "abstract": "Why Event Logging Matters Data-driven decision making is a core part of our culture at Indeed. When we test new changes to our applications and services, whether we’re changing our user interface or our backend algorithms, we want to measure how those changes affect job seekers. For example, when we test a ranking algorithm change, we look at search result click-through rate as a measure of relevance. To compute click-through rate, we need to know which jobs were shown as search results and which of those were clicked. To that end, we have built a system called logrepo (short for “log repository”) that allows us to track all potentially interesting events that occur on our site. Logrepo is a distributed event logging system that aggregates events from our data centers around the world into one place so we can analyze them. Once logrepo aggregates events into its central repository, purpose-built analysis tools can process events and deliver important insights that enable us to make decisions based on real usage data. For example, we built a powerful analysis tool called Ramses that provides a way to quickly visualize the results of querying multi-field, multi-dimensional indexes built on time series data from logrepo. This allows us to easily answer questions about how job seekers are interacting with our site and compare metrics across a variety of data segments. The Ramses graph in Figure 1 shows how an A/B test performed outside the US in terms of search result clicks by new versus returning visitors. This is just a taste of the power of Ramses, a tool that could not exist without a reliable, uniform approach to event log data collection. Figure 1: A sample graph from Ramses, our dynamic event data query tool As powerful as it is, Ramses is far from the only consumer of logrepo data. We have built numerous business metric dashboards that process logrepo event data, allowing us to choose arbitrary dimensions for the organization of the data presented. We also process logrepo data in map-reduce jobs for analysis and creation of machine learning models that power important features like search result ranking and job recommendations. What We Log We log over 500 event types, each with its own particular set of fields. A log event might be anything from a user action to a system performance metric. Figure 2 contains examples of some of the event data that we capture in logrepo. Each application is free to log whatever data it chooses, but there is a set of common fields we collect with most events. Event Type Description Example Log Data all events data common to almost all log events uid, type, user tracking cookie, active A/B test groups, user agent, referer, and many more jobsearch logged for every search result page search terms (what and where), country, # results, page number, # jobs on page, time spent in different backend services, + 60 more orgClk logged when a user clicks on a job in search results uid of source jobsearch event, job id resContactEmail logged when an employer contacts a jobseeker via their Indeed Resume uid of source resume search event, employer id, email message id, amount charged, country of resume, whether message was sent or blocked (and why), + 30 more Figure 2: example log event data Goals We had several goals in mind when designing logrepo. We focused on how the system could provide the best possible support for analysis tools that empower our decision-making process. We want to prevent loss or duplication of log events. While our log data would still be useful even with minor loss or duplication, any degradation in consistency limits the questions that can be answered with the accuracy required to make informed decisions. We require exactly-once delivery semantics for our log events to avoid these limitations. To complicate matters, we also require logrepo to be able to tolerate failure of a single logrepo server at any time without any loss of consistency or availability. We are willing to give up real-time processing to achieve this consistency. We also want logrepo to be able to deliver streams of data for a particular event type, in a parseable format, for consumption by log analysis tools like Ramses. We want for it to be very simple to implement such tools in any programming language. Why not use Flume? Building our own log collection system might seem unnecessary given the number of open source options available today, of which Flume is probably the most popular. Logrepo has been in production at Indeed since 2007, so it predates Flume and several other options. However, even today Flume does not provide the delivery guarantees and consistency characteristics of logrepo. We discuss those differences when explaining our transport mechanism below. Flume has a few notable advantages. Its strength lies in its modular architecture, which makes it easy to add new sources, collectors, and sinks -- such as a sink for HDFS. Replicating logrepo data into HDFS was not an original goal of the system, and maintaining consistency during HDFS replication is a problem we have not yet solved completely. Another advantage of Flume is that it stores its logs in arrival order, so once you’ve processed the log stream up to a point, you can be sure that all events in the stream before that point have been processed. With logrepo, everything is accessed in timestamp order. Due to the asynchronous nature of the system, you must wait a certain amount of time to ensure that all log events from a time range have arrived. As mentioned above, we are willing to trade off real-time processing for consistency. Our primary use case is analysis rather than monitoring, so several minutes of latency is acceptable. System Architecture To understand logrepo, we need to consider five parts of the the system: Entry Format: how event log entries are structured Archival Structure: how log entries are organized into files on disk Builder: process that collects entries and converts them into the archival structure Transport: process that moves log entries between servers and data centers Reader Daemon: process that streams event log entry data for processing Figure 3: how event log data is transported and stored Entry Format We wanted a standardized entry format to make it easy to build general purpose analysis tools. We chose to encode each entry as key-value pairs using the familiar URL query string format. This format is relatively human readable, handles escaping properly, and offers good tool support. All log entries have two common fields: uid and type. The uid field is a unique identifier for the log entry that also contains a timestamp, and the type field indicates the specific action and source application that provide the data contained in the entry. uid =14us0t7sm064g2uc& type =orgClk&v=0&tk=14us0soeo064g2m3&jobId=8005d47e09b124f4&onclick=1&url=http%3A%2F%2Faq.indeed.com%2Frc%2Fclk&href=http%3A%2F%2Faq.indeed.com%2FPlant-Manager-jobs&agent=... Figure 4: a partial log file entry Archival Structure The structure used for storing the data on disk is critical for being able to efficiently deduplicate, store, and later read the log entries. Log entries are stored in such a way that all disk IO is sequential for both writes and reads. This gives logrepo a big scalability advantage for sequential access (our primary use case) over a general purpose datastore that is implemented with B+ trees, with the disadvantage of not having good random access performance for single entries (not our primary use case). In the central repository, log entries are stored one per line in text files, sorted by uid (and therefore timestamp). The path at which each file is stored is determined by the data center, event type, and timestamp of its log entries. The timestamp used to determine the path is the top 25 bits of a 45 bit millisecond timestamp. This leaves 20 bits of the timestamp remaining within each file, so each file corresponds to a timespan of about 17 minutes. There can be multiple files corresponding to each prefix which can be easily merged when read since the entries are sorted. Figure 5: example log file path with path components explained dc1/orgClk/15mt/0.log4181.seg.gz: uid= 15mt0 00000k1j548&type= orgClk & ... uid= 15mt0 00010k2j72n&type= orgClk & ... uid= 15mt0 00050k3j7bd&type= orgClk & ... uid= 15mt0 00060k2j6lu&type= orgClk & ... uid= 15mt0 000b0k430co&type= orgClk & ... uid= 15mt0 000d0k2i1ed&type= orgClk & ... Figure 6: example of partial log entries showing uid and type Builder The builder converts the raw log entry data into the archival structure. The builder takes potentially new log entries, determines if they are new, and if so adds them. As the logs are accessed in time sorted order, the builder must also keep its output sorted. To maintain performance it is critical that all disk IO done by the builder is sequential. The logrepo builder buffers incoming log entries and writes them to files. It first sorts the log entries into the appropriate buckets based on the first 25 bits of their timestamps. The builder sorts its entries and flushes them to a file when one of the buckets reaches the limit of the buffer size or a maximum wait time between flushes is exceeded. If the builder finds existing files for the bucket, it compares and merges entries in the buffer and the files, skipping any duplicates. If there are too many files in the bucket, they may be merged into a single larger file to reduce future merge overhead. This buffering introduces a delay in log entry availability at the reader. To avoid too much copying and merging, which limits throughput, we need to make sure that we buffer a substantial amount of data before each flush. Currently we flush at 17 minute intervals (to match the timespan determined by the archival structure above), which means that to be reasonably sure that all the logs have arrived for a 17 minute period we wait 34 minutes. We can easily tolerate an hour or two delay in data availability for our analysis purposes. For real-time monitoring, we consume the same log entries from a different stream that bypasses the repository archive -- sacrificing consistency for much lower latency. Transport The goal of the transport layer is to reliably move log messages from the application servers through the logrepo servers in each datacenter and finally to the central log repository. Each data center contains two logrepo servers, each of which receives all log entries for that data center in order to improve fault tolerance. We use syslog-ng to send log entries from our application servers to both logrepo servers. A third copy of every log entry is stored on the application server’s local drive for additional fault tolerance. One of the logrepo servers is designated as the primary and runs the logrepo builder, which converts the raw logs from syslog-ng into the archival format. The primary then uses rsync to push the files in the archive to the central log repository. After a delay, the primary also checks for missing entries by copying the raw logs from the backup logrepo server and adding any log entries not found in the archive. The uniqueness of the uid field guarantees that this process introduces no duplicates. The redundant delivery of log messages to two servers gives us the ability to fail over from the primary to the secondary at any time. By always duplicating and then deduplicating we can suffer a total machine failure with no loss in availability or consistency. In contrast, Flume handles consistency through delivery acknowledgements. Once a message has been committed (acknowledged) downstream, its delivery is acknowledged to the upstream server. In cases where a downstream server fails, messages will buffer indefinitely until it recovers. If the failure is irrecoverable, consistency issues arise around whether the messages that were unacknowledged at the time of the failure have been committed further downstream. The same issues occur if Flume fails over to a spare collector that has no knowledge of what has and has not been committed. In either case, the upstream Flume server must decide whether to send the messages again, in which case they may be duplicated, or not to re-send, in which case they may be lost. Reader Daemon We access the events stored in the logrepo through a reader daemon running on the central logrepo server or one of its replicas. The server supports querying for log entries by type and time range. This is where we see the benefits of our archival structure. When serving results for a query, the server finds all files for the type and 25 bit prefix of the start timestamp and streams the log entries back to the client. It skips all events with timestamps less than the start and merges events from the files as it streams. It repeats this process for each subsequent time-based bucket until it reaches the bucket containing the end timestamp. Once it has seen a log entry beyond the end timestamp, it knows it is done and terminates the stream. The reader is optimized for low latency for retrieving events of a single type in a time range. We did not optimize for interleaving events of different types. And since a typical analysis request includes millions or billions of events, we did not need to optimize for looking up a single event by uid. Optimizing for this simple retrieval case has served us well in the creation of our analysis tools, which process the log entry data streams into purpose-built data structures for ad-hoc analysis. The interface to the reader daemon is extremely simple. To ask for event log data, we connect to the daemon on a TCP socket and send a one-line string, consisting of a start-time/end-time pair (in epoch milliseconds) and an event type. The matching data is streamed back instantly. For interactive peeking at raw log data we often simply use the Unix command nc ( netcat ): ~$ date --date \"2012-04-01 05:24:00\" +%s\r\n1333275840\r\n~$ date --date \"2012-04-01 05:25:00\" +%s\r\n1333275900\r\n~$ echo \"1333275840000 1333275900000 orgClk\" | nc logrepo 9999 | head -2\r\nuid=16pmmtjgh14632ij&type=orgClk&v=0&tk=16pmmsulc146325g&jobId=11eaf231341a048f&onclick=1&url=http%3A%2F%2Fwww.indeed.co.uk%2Frc%2Fclk&href= ... uid=16pmmtjh2183j353&type=orgClk&v=0&tk=16pmmntjr183j1ti&jobId=029b8ec3ddeea5ae&onclick=1&url=http%3A%2F%2Fwww.indeed.com%2Frc%2Fclk&href= ... Figure 7: using netcat to query the logrepo reader daemon This simple interface to the reader daemon makes it easy to process logrepo data in any programming language. The core of a Python implementation is 13 lines of code. Deciding With Data We now capture 1 to 2 billion logrepo events every day across all Indeed properties. We feed those events into a diverse set of powerful analytics tools, like Ramses (about which we plan to share more in a future blog post). We use those tools to determine how to improve our services to better serve job seekers and employers. Over the past 5 years, logrepo has provided significant value to Indeed. It serves as the foundation of our data-driven approach to decision making. Indeed's products are vastly better thanks to its reliable, flexible, and scalable architecture. To keep this up for another 5 years, we will need to make it even more reliable, flexible, and scalable. If this sounds like the kind of problem you'd be interested in solving, take a look at some of our career opportunities .", "date": "2012-11-28"},
{"website": "Indeed-Engineering", "title": "Indeed ♥♥♥ Interns", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2013/02/indeed-%e2%99%a5%e2%99%a5%e2%99%a5-interns/", "abstract": "We wanted to take a few minutes on this Valentine’s Day to talk about something very special to us -- our interns. Every May, 20+ college students descend on our Austin office to spend their summer learning about how engineering works in the “real world.” They are enthusiastic; they are excited to learn; they come from all over the US and Canada; they wear funny clothes, and we love them. Interns at Indeed have a unique chance to work on real problems across many dimensions of software development. Not only are these projects challenging, they are critical to our business and impact millions of job seekers around the world. Recent intern projects include: instant search functionality for our resume search application, improving ranking algorithms based on analysis of very large data sets, and improving the performance of a distributed message delivery system. We asked one of our Interns from last summer why she loved Indeed. Here’s what she had to say. From Caroline Horn, University of Illinois Computer Science, class of 2013: During my summer at Indeed, I worked on the resume team doing front-end development. My projects included creating a drag-and-drop UI for resume uploads, implementing an autocomplete feature in the resume builder, and designing a splash page to highlight Indeed’s edge over its competitors. Through these projects, I was given the chance to work with the codebase of a large-scale application, an opportunity that a college student does not come across every day. I worked heavily with Google Closure Tools, including the Closure JavaScript Library and the Closure Compiler. It was fascinating to learn more about all of the behind-the-scenes processes that go into keeping a site as huge as Indeed running regularly. A main focus in front end development today is centered around the mindset of “how can I make this product even easier to use?” My projects attempted to answer that question while maintaining Indeed’s simple design aesthetic. Both the drag-and-drop and autocomplete functionalities contribute to smoothing out the job seeker’s experience when interacting with the site, and the splash page provides a nice segue for employers when searching for resumes. The engineering culture at Indeed is a living, breathing example of the “work hard, play hard” mentality. The engineers expressed a passion for their work through their willingness and eagerness to answer questions, as well as in weekly tech talks showcasing various components of Indeed’s engineering foundation. As an intern, I was treated as a full-fledged team member and was encouraged to contribute my own ideas and opinions regarding the challenges that the team was trying to solve. One thing that especially stood out to me was the sincerely strong effort made by every single person at the company towards keeping the best interests of the job seeker in mind at all times. It’s easy to stay motivated about your work when you know that the end result will help thousands of people all over the world. I love Indeed engineering because I love Indeed and I love engineering. See more from our interns in Summer 2012 We're updating the accompanying video. Please check back later.", "date": "2013-02-14"},
{"website": "Indeed-Engineering", "title": "@IndeedEng: A Technical Speaker Series", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2013/02/indeedeng-technical-speaker-series-austin/", "abstract": "We’re excited to host @IndeedEng, a technical speaker series. Building successful large-scale consumer applications takes smart, passionate people with a variety of backgrounds and expertise. Our goal is to bring together the tech community to discuss the challenges of developing great products. In this ongoing series, Indeed will share our real-world experience building a site that supports 250 million job seekers in 60+ countries a month and host compelling tech talks by outside speakers.", "date": "2013-02-08"},
{"website": "Indeed-Engineering", "title": "Building Resume Instant Search", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2013/08/building-resume-instant-search/", "abstract": "[Editor's Note: This post is a companion piece to a recent @IndeedEng talk .] Our resume instant search feature allows employers to see search results as they type, helping them efficiently connect with job seekers. Google found that Instant saved users 2-5 seconds per search . To prevent undesired search results for partially-typed queries, we also had to implement an auto-complete feature that predicts a user’s query before they finish typing it. Both of these features must be lightning-fast in order to be effective. The combination of the instant and auto-complete features allows us to deliver a streamlined search to our users. We know from testing that the faster we deliver results to users, the more time they spend doing what matters -- reading resumes and contacting applicants. Building Instant Search One of our goals for the architecture of Instant Search was to build a clean, modular JavaScript application. Our use of the Google Closure framework helped us achieve this goal. The Google Closure tools include a dependency manager, a feature-rich JavaScript library, a JavaScript compiler, and templates that compile to both Java and JavaScript. We used these tools and implemented an event-driven architecture, resulting in a modular, easy-to-maintain application. In event-driven models, program flow is based on events such as a mouse click or a key press. A common example in web development is the onclick handler, which allows developers to capture the click event on a DOM element. The element being clicked has no knowledge of the code that runs as a result of the click. It fires an event to signal it has been clicked, and the browser dispatches that event to all components that have registered to receive it. Instant Search has four components: AutoComplete - offers search query completions based on what the user has typed. Instant - retrieves and renders resume search results. Preview - retrieves and renders a preview of the resume on mouseover. Result Page - the main dispatcher; the only component aware of the other components. Each component fires off events as they occur without knowing anything about the other components on the page. For example, AutoComplete will fire an event when the search completion changes, and that event is handled by the Instant component, which retrieves results. The Result Page component listens for all events and dispatches them to the registered components. Here is example code that shows the Result Page component listening for a set of events from the AutoComplete component: // Attach events for Query AutoComplete\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.QUERY_TYPE,\r\n  this.handleQueryChange_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.QUERY_CHANGE,\r\n  this.handleQueryChange_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.ENTER,\r\n  this.handleQueryEnter_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.TAB,\r\n  this.handleQueryTab_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  [indeed.AutoComplete.EventType.SELECT,\r\n    indeed.AutoComplete.EventType.NEW_SUGGESTIONS],\r\n  this.handleQuerySelect_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.FOCUSIN,\r\n  this.handleQueryFocusIn_, false, this);\r\ngoog.events.listen(this.queryAutocomplete_,\r\n  indeed.AutoComplete.EventType.FOCUSOUT,\r\n  this.handleQueryFocusOut_, false, this); The function handleQueryChange_ passes the text of the query to the Instant component: /**\r\n * Handles changes to the query by notifying Instant\r\n * @param {indeed.events.TextEvent} e Change event.\r\n * @private\r\n */\r\nindeed.Search.prototype.handleQueryChange_ = function(e) {\r\n  if (this.instant_) {\r\n    this.instant_.newQuery(e.text);\r\n  }\r\n}; All other components on the page act in a similar manner -- Instant fires an event when it has new results, and Preview fires an event when it displays a resume. The architecture of the application ends up looking like this: Figure 1: Result Page is composed of high-level components, including AutoComplete and Instant, which in turn are composed of lower-level handlers and renderers. Long Description of Figure 1 The flow chart follows the following structure: Result Page: Controls the whole page AutoComplete: A single autocompleter InputHandler: Handles the autocomplete text box Renderer: Handles the AutoComplete rendering RPCHandler Instant: Handles all instant functionality InputHandler: Handles the instant text box FormHandler: Handles form actions Renderer: Handles instant rendering RPCHandler Figure 2: How AutoComplete handles events from its sub-components Long Description of Figure 2 The AutoComplete process is as follows, using \"jav\" as an example input: User types \"jav\" and the InputHandler sends the input to AutoComplete AutoComplete asks RPCHandler for suggestions for \"jav\" RPCHandler sends AutoComplete new suggestions for \"jav\" AutoComplete renders suggestions for \"jav\" using Renderer The user clicks on the option \"java developer,\" displayed by AutoComplete The InputHandler sets input text to \"java developer\" This event-driven approach allows us to add new components to the search page without having to modify the components that were already on the page. Since the components have no knowledge of the Result Page, we have been able to package them into a library for use in other projects. Navigation and Search Engines If we just used JavaScript-triggered requests to populate search results and did not change the URL, our users would never be able to save or share specific search result pages, and search engines like Google could not crawl these pages for indexing. In our initial implementation of Instant Search, we updated the URL fragment on every new search. If a user came to http://www.indeed.com/resumes and did a search for “java” in Austin, we updated the URL to http://www.indeed.com/resumes#!q=java&l=Austin . For comparison, before we implemented Instant Search, our search URLs used a more traditional query string approach (still supported): http://www.indeed.com/resumes?q=java&l=Austin . We used #! instead of simply # in order to work with Google’s Making AJAX Applications Crawlable guidelines, which specify a mapping from #! URL fragments to query string parameters so that crawlers can make requests with information stored in the URL fragment. A drawback of this approach is initial page load performance. Since the data in URL fragments is not sent to the server, the browser has to do a full round-trip (request/response) to the server before processing the fragment, then an additional round-trip to get the search results. In contrast, using the traditional query string approach allows results to come back in a single round-trip. To improve performance for users coming directly to a search results page from an external site, we used the HTML5 history API supported by most modern browsers. This API enables modifying the whole URL via JavaScript rather than just the URL fragment. We now use this API to update the browser URL to include the search terms in the path (e.g. http://www.indeed.com/resumes/java/in-Austin ). These URLs can then be shared and published widely, and users clicking on the resulting links receive search results in a single round-trip. Templates and Rendering Dynamically updating search results in the browser with JavaScript provides a superior user experience, but we also need to be able to render identical results on the server. We could have just developed two versions of our templates, one for server-side and one for client-side, but this approach leads to maintainability challenges. It would be annoying and error-prone to keep two versions, in two different template languages, in perfect sync. We could have generated HTML on the server-side only, and send the HTML back in response to the Instant request, but this approach results in a larger response and adds server load. Instead, we opted to use the same template on both the client and server. Technologies like Node.js , mustache , and Google Closure Templates are making this technique viable and more common. We chose Google Closure Templates to enable a single template that works both on the server (Java-based) and the client (JavaScript-based). Here is an example template: /**\r\n * Renders a single block of refinements on the result page\r\n * @param title Title of the refinement\r\n * @param refinements List of refinements\r\n *\r\n * @private\r\n */\r\n{template .refinementBlock}\r\n  {if length($refinements) > 0} {$title} {foreach $refinement in $refinements} {$refinement['text']} {$refinement['count']} {/foreach} {/if}\r\n{/template} The Closure template compiler generates JavaScript that integrates well into the client-side rendering of our search result pages. We also use the templates in our server-side Java code, delivering results quickly for initial requests and enabling subsequent searches for users and crawlers that are not using JavaScript. No Compromises Our event-driven architecture has allowed us to build a maintainable library of components that can be used in multiple projects. We have used URL fragments and the HTML5 history API to keep navigation, link sharing, and search engine discovery simple. Closure Templates have helped us avoid unnecessary complexity and duplication. We have built a great resume search experience for our users without compromising on maintainability, reuse, and performance. At Indeed, we focus on making the job seeker and employer experiences as fast and simple as possible. If these kind of challenges sound interesting to you, check out our open positions .", "date": "2013-08-08"},
{"website": "Indeed-Engineering", "title": "Indeed College Tour: On the Road in Search of Indeed’s Next Hires", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2013/09/indeed-college-tour-on-the-road-in-search-of-indeeds-next-hires/", "abstract": "Hi, I’m Jobby. You might know me from Indeed’s Antarctica site . A team of our engineers created Indeed’s Antarctica job search site and me as an April Fool’s Day prank in 2010. That little prank turned out to be useful to job seekers, so Indeed Antarctica and I got to stick around! In fact, I made an encore appearance this April 1, 2013, on Indeed’s search results page as a Clippy-inspired job search assistant. In the Fall 2013 and Spring 2014, I will hit the road with Indeed’s University Relations team and don my collegiate gear as we visit 11 colleges in the U.S. and Canada in search of top engineering talent for full-time and summer internship opportunities. During our university visits, we attend career fairs and hold on-campus interviews with computer science, engineering, and technical operations candidates. \"We have a high bar for hiring, so we go to the top programs to identify the top computer science and technical operations candidates,\" says Jolynn Cunningham, Director of Talent at Indeed. Want to know which schools I’m visiting in Fall 2013 and Spring 2014? See if you recognize me masquerading as your school’s mascot in our t-shirt design. Now hiring:  Summer interns & new grads University recruiting has been a priority at Indeed since its inception. In fact, one of Indeed’s first interns, Andrew Hudson, has been with Indeed for 8 years and is now our CTO. A key goal of our internship program is to get an intern’s code into production within their first week. This code often becomes a part of our systems and is used every day. Examples of past intern projects include: Resume instant search Job search country launches Android file upload Company Pages photo uploading, viewing, & data management All of our teams at Indeed have recent college grads on them. Many of our key technologies and products were built by recent college grads and all of them are building great software to help people find jobs. Indeed’s college hires are paired with a mentor on their team who provides guidance on process and development during a new hire’s first few months. From 24-hour hack-a-thons to ping pong tournaments to an end-of-summer boat party on Lake Travis, all Indeedians enjoy the fun, active culture of our Austin office. Our University Relations team puts together events to help interns and new hires quickly acclimate to the Austin, Texas, lifestyle. Past summer intern events include segway tours of downtown Austin, swimming at Barton Springs Pool, and horseback riding at a dude ranch. How to apply Check with your campus career services office to find out if Indeed will be at your career fair, and be sure to drop by the Indeed table, meet us, and pick up a t-shirt. If Indeed is not visiting your campus, don’t worry. Indeed looks for talent everywhere, and all jobs for summer internship and full-time, new graduate positions are available here . Connect with Indeed Take a picture in your Indeed t-shirt and tweet it to @IndeedEng with #jobbytour Or, check out our @IndeedEng Tech Talk series on our YouTube channel .", "date": "2013-09-16"},
{"website": "Indeed-Engineering", "title": "@IndeedEng 2013 Year In Review", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2013/12/indeedeng-2013-year-in-review/", "abstract": "Indeed engineers help people get jobs . We take that mission very seriously, and we love that we get to work on interesting problems in an exciting, dynamic environment that encourages end-to-end ownership, engineering velocity, and data-driven decision making. Every member of our engineering organization contributes to this unique culture. We're not complacent about it, either. We treat our culture like our products, as an evolving set of values and processes that require constant iterative improvement. In the past, we didn’t share very much about what we’d done at Indeed with the software development community at large. We were focused on our mission and constantly improving how we help people get jobs. We began to emerge from that cocoon last year, when we launched this blog . Then, in February 2013, we began presenting @IndeedEng, a series of monthly talks in our Austin headquarters. The slides and video for these talks have been posted on our blog and are now available on our new site, engineering.indeedblog.com/talks . We presented nine @IndeedEng talks in 2013. We poured lots of effort into these talks, making sure that they clearly presented interesting technology ideas and best practices for developing great software. In our early October talk , we announced the open source release of Proctor , our A/B testing framework. The response to these talks has been encouraging and rewarding, with over 600 people attending the series in 2013. We have enjoyed testing the capacity of our \"auditorium\" (it's actually our cafeteria). We'd like to take this opportunity to thank every great software developer out there who has inspired us to give back to the community, to thank those at Indeed who have helped make these efforts a reality, and finally to thank all of you who have read our blog, watched an @IndeedEng talk, or used our open source projects. We look forward to continuing our conversation with you in 2014!", "date": "2013-12-19"},
{"website": "Indeed-Engineering", "title": "Announcing Proctor:  Indeed’s Open Source A/B Testing Framework", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2013/10/announcing-proctor-open-source/", "abstract": "At this very moment, Indeed is running more than one hundred A/B experiments. These experiments are controlled by a system we built called Proctor. We're excited to announce that Proctor is now open source. We built Proctor here at Indeed, applying the lessons learned from many years of running experiments. The design and features of Proctor were focused on these key goals: Easy, rapid, and safe adjustment of experiments Finely-targeted segmentation of traffic Separation of experiment management from behavior Supporting experiments in all of our different types of applications, services, and tools Synchronizing experiments across multiple applications We've integrated Proctor with a large number of our products, giving us powerful capabilities throughout Indeed’s systems. In addition, Proctor has grown beyond its initial focus on experimentation to allow us to carefully manage overall system behavior through dynamic \"feature toggle\" functionality. This allows us to safely and gradually apply changes to our production systems. Proctor is written in the Java language and can be used from any language hosted on the Java Virtual Machine. The Proctor source code is now hosted on GitHub ( github.com/indeedeng/proctor ). We also have documentation and a reference implementation that illustrate how you can use Proctor on your own applications, and we have a Google Group where you can discuss Proctor and ask questions. As a part of our ongoing @IndeedEng series, we gave a tech talk on \" Managing Experiments and Behavior Dynamically with Proctor .\" We hope that Proctor will be as useful to you as it has been to us.", "date": "2013-10-03"},
{"website": "Indeed-Engineering", "title": "Serving over 1 billion documents per day with Docstore v2", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2013/10/serving-over-1-billion-documents-per-day-with-docstore-v2/", "abstract": "[Editor’s note: This post is the second installment of a two-part piece accompanying our first @IndeedEng talk .] The number of job searches on Indeed grew at an extremely rapid rate during our first 6 years. We made multiple improvements to our document serving architecture to keep pace with that growing load. A core focus at Indeed is to keep jobs as “fresh” as possible. We measure job freshness as the time from when our systems find a job posting on the web to when that data is searchable on our site. Providing comprehensive job data in as close to real-time as possible is one way we create the best job search experience for our users. In 6 years, the amount of data that we needed to process to reach that goal had drastically changed. By late 2011, we were aggregating 41 million new jobs from around the world each month. We were serving up to 850 million job documents each day, at a peak rate of 14,000 documents/second. Our document serving system consisted of a replicated, file system-based document store (“docstore”), accessed via a remote Boxcar service (“docservice”). This system worked well for over two years but was straining under the continued growth. Updating the docstore with fresh job data took about 12 hours of every day. Delays or interruptions in replication made it difficult to catch up with the stream of job updates. To continue to provide the most recent jobs to jobseekers, we needed to make it faster. In our first docstore implementation, we wrote raw job data to disk in segment files, each containing the data for some set of jobs. We wrote these files in job ID order to easily be able to find data for a single job based on its ID. When a job was updated, we wrote the new version back into the same segment file. Figure 1: docstore v1 segment file organization Indeed sites serve thousands of searches per second for jobseekers from many countries. The selection of jobs returned by those searches is uncorrelated to the job ID. From the perspective of the docstore, they are effectively random accesses. Even for a single search, the results are hardly ever ordered by job ID and are not even likely to have jobs with close IDs. Similarly, writes to the docstore do not happen in strict job ID order, because updates to existing jobs are mixed in with additions of new jobs. The job ID order structure of the docstore led to essentially random disk writes and reads. Scaling random reads is a well-understood problem. To keep reads fast under load, you can add servers and/or do more aggressive caching. Scaling writes is a much more challenging problem, but one that we had to solve in order to achieve better performance than docstore v1. Consider the physical capabilities of a single magnetic hard disk. A basic commodity disk spinning at 7200 rpm has an average 30MB/second sequential write throughput. An lzo-compressed job entry contains about 1KB of data. Therefore, in theory, we should be able to write about 30,000 jobs/second. Disk Seek 10ms Sequential Disk Bandwidth 30 MB/second Average Job Size, LZO-compressed 1 KB Job Throughput, Sequential 30,000 jobs/second Job Throughput, Random 100 jobs/second Disk Bandwidth, Jobs Random 100 KB/second Figure 2: disk I/O numbers However, we were nowhere close to this ideal rate. To do a job update, we had to first locate the existing file on disk, read the whole file, decompress it, update it, re-compress it, and write the whole file back to disk. This resulted in at least two disk seeks, one to read and one to write. Our drives averaged about 10ms per seek. By writing randomly, we limited our maximum throughput to be about 50 jobs/second, and we averaged about 37.5 updates/second. Docstore v1 was only achieving about 0.125% of optimal write throughput. When our throughput was even lower, our docstore builders fell behind. When throughput fell to 20 job updates/second, the approximate rate at which job updates were coming in, we saw replication delays, and we no longer had the freshest jobs available on our site. We realized that because our job data accesses were essentially random, there was no benefit to writing jobs in any particular order. Since our bottleneck was write throughput, we decided to write job data in the most efficient way possible for the disk: sequentially as the updates came in. By writing the jobs to an append-only structure, the disk would avoid seeks before writes, which had the potential to allow us to write 300 times more jobs per second. New Storage Scheme To write our job data to disk, we created a new filesystem scheme we called the docstore queue. The new docstore updater process works similarly to the docstore v1 updater in that it writes segment files to disk, each containing the raw data for some set of jobs. The difference is, instead of putting the job data in a specific location on disk, it writes each job sequentially. The resulting segment files are copied using rsync to the docservice machines, where job data is then loaded into Memcached. In the case of a Memcached miss, job lookups fall back directly to the segment files on disk. Figure 3: docstore queue Each segment file is immutable once written. An update to an existing job writes the entire updated job contents to the current segment file. Once the current segment file reaches a certain size, it is closed, made available for slaving by downstream consumers and a new current segment file is created. Readers ignore older copies in the docstore queue. Since we average 1.5 updates per job, the extra storage is only an additional 50%. We don’t bother to garbage collect the obsolete data. A segment file consists of a series of blocks. At the start of each block is a table of offsets that tells you where in the file to find a specific job. Given a file number, address, and index within the block, any job can be loaded in constant time. Figure 4: segment file structure Ordering updates sequentially helps us scale writes, but we have introduced a new problem. We no longer have an easy way to look up data for an arbitrary job in this unordered docstore queue. To provide this mapping from job ID to docstore queue address, we must construct an index. This index is much smaller than the underlying data store, but we run into the same problem keeping an ordered index on disk -- the disk seeks required to make updates to existing entries would be too slow. To scale index updates at the same rate as docstore updates, we must also write the index sequentially. Alternatives for indexing the docstore queue The obvious solution for storing this index using only sequential I/O is to not store the index on disk at all, but to write a transaction log to disk as each job update is processed. This log would be written sequentially so that all transactions could be replayed on a restart in order to rebuild the in-memory index. The docservice would keep the mapping of job ID to docstore address in memory for quick lookup. This solution does not scale because it would require storing a map of tens of millions of entries and exceed the memory constraints on our machines. A simple optimization to the in-memory solution would be to keep a limited-size index in memory. When the in-memory index reaches its size limit, the docservice can flush it to disk. When the docservice needs to retrieve the contents of a job, it first checks the in-memory index. If that index doesn’t have the job data, then the docservice uses the on-disk index. This prevents the docservice from consuming all available system memory. However, at some point, the in-memory index will fill up again and will need to be flushed to disk a second time. We already have an index on disk so we need a way to integrate this new data with the existing data. One strategy is to merge the in-memory index with the on-disk index and then write out a new on-disk index. Using a sorted B-tree, we could accomplish this with a simple merge algorithm. Each time the in-memory index reaches its size limit, the entire contents of the on-disk index would be rewritten. That means that every time a batch of n jobs gets written, it rewrites all existing jobs too, so the number of writes is O(n 2 ) . If each batch contains 100,000 jobs, we will have performed 1 million job writes for an index of 400,000 jobs. By the 100th flush, the numbers would be 505 million job writes for an index of 10 million jobs. Merging the indexes every time a new batch is flushed to disk would not scale because we would eventually spend all our time merging. Another strategy for subsequent flushes of the in-memory index is to write more indexes instead of merging. Each time the in-memory index fills up, we could just write it to disk independently of any other index. When the docservice performs a lookup, it first looks in the in-memory index. If the job data is not there, it looks in the first on-disk index, then the second, and so on, until it finds the job or has searched all on-disk indexes. Each index is a B-tree, so the lookup to see if an element is present is O(log(n)) in the size of the index. We can consider this a constant factor due the capped index size. The number of indexes will grow over time, so reading a specific job is O(m) where m is the number of indexes on disk. While this approach scales writes, it does so at the expense of reads. But here’s what really happened We implemented an index using a log-structured merge tree ( LSM-tree ). This is a hierarchical key/value store that minimizes reads and writes. LSM-tree indexes have an in-memory index and a series of on-disk indexes, searched newest-first as described in the second strategy above. The primary advantage of the LSM-tree approach is its strategy for deciding when to merge on-disk indexes. LSM-tree only merges indexes of similar size, which means that it does not merge indexes on every flush of the in-memory index. This significantly reduces the number of writes to disk while also constraining the number of reads. When writing a full in-memory index to disk, we must determine which on-disk indexes will be merged with the in-memory index to produce a new on-disk index. This set of indexes is called the compaction set . The compaction set is empty when the most recent on-disk index is larger than the in-memory index. The compaction set can contain multiple indexes, and it occasionally will contain all indexes on disk. newIndex = new DiskIndex()\r\nnewIndex.add(memoryIndex)\r\nwhile newIndex.size >= diskIndexes.first.size\r\n    newIndex.merge(diskIndexes.removeFirst())\r\n\r\nnewIndex.write()\r\ndiskIndexes.addFirst(newIndex) Figure 5: compaction heuristic Let’s walk through this algorithm in action. We start with an empty in-memory index and no indexes on disk. As updates stream in, the initial in-memory index fills up, and we write it to disk. When the in-memory index fills up again, the in-memory index is the same size as the sole on-disk index. Figure 6: in-memory and on-disk index are equal size, triggering merge We merge the in-memory index and the on-disk index into a single on-disk index containing the jobs from both. Figure 7: single on-disk index When the in-memory index fills a third time, it is smaller than the on-disk index, which contains two batches of jobs, so we write it out as a new index. Figure 8: on-disk index is larger than in memory, so no merge That leaves two on-disk disk indexes, the new one half the size of the older one. Figure 9: two on-disk indexes The next time the in memory index fills up, it will be the same size as the first on disk index, so they will be merged. Figure 10: first on-disk index and in-memory index are same size, triggering merge The result of that merge would be of size two. The second on disk index is size two, so we will also add that index to the compaction set. Figure 11: resulting index would be same size as 2nd on-disk index, triggering merge The result of this compaction is now one, large, on-disk index. The first batch of jobs locations in the record log will have been written 3 times, the second batch 2 times, the third batch 2 times, and the fourth batch once. Figure 12: one complete on-disk index Let’s look at the results of the LSM Tree approach compared to the “always merge” and “always write” approaches we discussed above for when the in-memory index fills up. If each batch contains 100,000 jobs, the LSM Tree will have done 800,000 job writes for an index of 400,000 jobs. The “always merge” approach would have done 1 million. But by the 100th flush, we will have only done 15.2 million job writes for an index of 10 million jobs and have only 3 on-disk indexes. The “always write” approach would have 100 on-disk indexes at this point, and the “always merge” would have done 505 million job writes! As the number of jobs grows, the total number of job writes required approaches O(n*log(n)) where n is the number of jobs. The maximum number of on-disk indexes present at any time grows very slowly, being reduced to as few as one index when the compaction merge occurs. This method of index building uses only sequential I/O, because the entire on-disk index is always written at once. Merging the indexes is simple, because they are already sorted. This method minimizes write volume, while keeping a bound on the lookup time. There is still one problem with this solution. There are O(log(i)) indexes (where i is the number of job batches processed), and it takes log(j) time to do a lookup in each one (where j is the number of jobs in the index). The number of reads required to find a job in this data structure is O(log(i) * log(j)) , which could be worse in practice than our previous docstore. We needed lookups to be as fast as the existing docstore implementation before we could use it as a replacement in production. Optimizing Reads with Bloom Filters If we knew which index contained the job entry we are looking for, we could eliminate the log(i) factor and read an O(log(j)) lookup time. To achieve this desired lookup time, we added a Bloom filter on top of each index. A Bloom filter is a type of set. Its contains function is slightly different than other Set implementations in that it is probabilistic rather than definite. It may return a false positive, but it never returns a false negative. When a \"no\" is returned for one of our LSM tree's indexes, it is guaranteed that the key does not exist in that index, and thus we can skip the O(log(j)) lookup. In a Bloom filter, a “yes” is actually a “maybe”, so we must check the corresponding index, but the key is not guaranteed to be present. Our particular Bloom filter implementation is tuned for a false positive rate of about 2%. Bloom filters are relatively small compared to the index -- our implementation is about 5% of the total index size. They also provide constant time lookup for a key. When the Bloom filter returns false for a jobid, we skip the O(log(j)) lookup into the underlying index. A maybe means we still have to check the index, which may not have the actual data. Given our false positive rate of 2%, the use of Bloom filters ensures that we look in a single index 98% of the time. One limitation of the Bloom filter in our usage is that they must be kept in memory to be useful. Paging in the filter from disk to do a lookup ends up being more expensive than checking the index directly. Even though our Bloom filters are small compared to the index, they are too big to be held entirely in memory. To address this limitation, we optimize how we load the Bloom filters to keep the memory utilization constrained. We want to keep the Bloom filter pages in memory that are most likely to say “no” to a given contains request, since “no” is the only useful answer. If a page is not in memory, instead of loading it, we assume that it would return “maybe” for all keys. We determine the “usefulness” of a given page based on the number of requests for the page in the last t seconds multiplied by the probability of a “no” returned by that page. Figure 13: probability Bloom filter says no and whether the job is in index In this example, the in-memory index contains 12.5% of the jobs, the newest on-disk index contains 12.5%, the next on-disk index contains 25%, and the oldest index contains 50%. We assume that the requested job ID exists in one of the indexes. If the in-memory index doesn’t contain the job, then there’s a 14.3% chance that the newest on-disk index contains it. If not, a 33% chance the 2nd on-disk index contains it. If the job is not contained in the first three indexes, then there is a 100% chance it is in the oldest index. Having a Bloom filter on the oldest index is not useful in this case, because we know the key exists. Since job data access is not evenly distributed across the entire index, we also keep a count of actual accesses per page because there tend to be hotspots. For example, newer jobs tend to be accessed more than older ones. We can multiply the access counts by the usefulness to determine which pages to keep in memory at all times. Figure 14: usefulness of each page of Bloom filter Index Durability Our new solution meets our write throughput requirement, while keeping reads fast without exploding memory utilization. We have one more requirement: the index must be durable. We have to be able to recover from a system failure at any time without losing any data. To meet that requirement, we added a write-ahead log for the in-memory index. By writing sequentially to an on-disk log file prior to adding data to the in-memory index, we can easily restore its state at any time. When a docservice instance comes back online, it can replay the write-ahead log to rebuild the in-memory index and continue processing new updates to the docstore queue, with no data loss. Figure 15: complete document serving system Benchmarks Around the same time that we were deploying docstore v2, Google announced the release of LevelDB , which is their own LSM tree implementation. Since we had already written ours, there were no plans to take advantage of their work. We did, however, want to see how our code stacked up against Google’s. We also compared our LSM tree to Kyoto Cabinet , a commonly used B-tree implementation, to show the benefit of sequential disk writes and efficient I/O benefits. For our benchmark, we performed 10,000,000 writes with an 8-byte random key and a 96-byte value. We first wrote all keys in random order. Then, we read all keys in random order. The results were as follows. Implementation Random Writes (seconds) Random Reads (seconds) Indeed LSM-tree 272 46 Google LevelDB 375 80 Kyoto Cabinet B-tree 375 183 Our implementation was the fastest, but the B-tree implementation wasn’t as far behind as we expected. 10 million entries of 100 bytes each is still less than 1 GB, meaning that the entire index fit in memory. We ran the benchmark again, but this time limited the available memory to 512 MB. The results make the benefits of our techniques more apparent. Implementation Random Writes Indeed LSM-tree 454 seconds Google LevelDB 464 seconds Kyoto Cabinet B-tree 50 hours The B-tree implementation took 400x longer than the LSM-tree implementations, primarily because it is constantly re-writing the data on disk as new keys come in. It scales Docstore v2 has been in production now for over one year. Docstore v1 took 12 hours to build one day of job updates in 2011. Docstore v2 takes only 5 minutes to process a whole day’s data, even though the number of job updates we process in a day has grown significantly since 2011. It takes an average of 9 minutes from the time we discover a new job somewhere on the internet to having it live and retrievable in docservice. In July of 2013, we aggregated over 79 million job updates and served over 1 billion documents every day, with peaks of 22,000 documents/second. At that scale, our document serving is still extremely fast. Figure 16: growth of document serving over time Our docstore queue is currently 60GB in size and contains every job that indeed has ever known about. The LSM tree that tells us exactly where to find any job in the queue is 23GB. The Bloom filter that tells us whether a job is likely present in a given index is 1.6GB. Job reads from docservice take on average 5ms and the set of jobs is kept current throughout the day by the constant stream of updates coming through the docstore queue with no performance impact on the reads. We have also been able to use the same infrastructure that builds the docstore queue for some of our other systems. We now use a similar queue and LSM tree index in resume, company pages, job recommendations, and several other backend data artifacts that we use to determine the most relevant jobs to show for a search. Each use case is slightly different, but the underlying benefits of scalable reads and writes over very large data sets is common to all. If you like to solve these kinds of interesting problems at scale, take a look at some of our career opportunities .", "date": "2013-10-24"},
{"website": "Indeed-Engineering", "title": "Efficient Query String Parsing with util-urlparsing", "author": [" by Preetha Appan"], "link": "https://engineering.indeedblog.com/blog/2014/02/efficient-query-string-parsing-util-urlparsing/", "abstract": "We’re excited to announce the open source release of util-urlparsing , a Java library we created to parse URL query strings without unnecessary intermediate object creation. It also includes number parsing methods in ParseUtils that are faster than Java’s equivalent methods like Integer.parseInt and Float.parseFloat . Java versions 1.6 and lower have a significant flaw that leads to inefficient memory usage when using the String.substring method. When processing data from our log repository, we need to extract small substrings from much larger strings containing event data key/value pairs. The primary class in util-urlparsing, QueryStringParser , was written to efficiently parse this data without generating any intermediate string objects. It does this via a callback mechanism that lets you only parse the keys you are interested in from the larger query string. Our query parsing benchmark shows nearly 4X speedup over a naive Java implementation using String.split under significant heap space constraints. It can parse a million key-value pairs in under 3 seconds given a max heap of only 64MB. Our number parsing benchmark shows over 2X speedup compared to equivalent methods like Integer.parseInt and Float.parseFloat . util-urlparsing is available for download on GitHub ( github.com/indeedeng/util/tree/master/urlparsing ) or maven.org ( search.maven.org/#browse%7C-525259937 ), and we have documentation which includes usage examples to help you get started. If you have any questions, check out the Q&A forum for our open-source Java utilities. If you’re interested in learning more about Indeed’s log repository, check out the video and slides of our January 26th talk “Logrepo: Enabling Data-Driven Decisions.”", "date": "2014-02-21"},
{"website": "Indeed-Engineering", "title": "5 Tips For a Best. Hackathon. Ever.", "author": [" by Brendan Sterne"], "link": "https://engineering.indeedblog.com/blog/2014/02/5-tips-best-hackathon-ever/", "abstract": "It’s been a week since the 5th Semi-Annual Indeed Music, Food and Art Festival Hackathon and I’ve fully caught up on my sleep. How did it go? Screenshot from IndeedFeed: \"Best. Hackathon. Ever.\" Here are some of the lessons we've learned for putting on hackathons. Tip 1: Pick a theme, but have room for interpretation. Although it may seem liberating to have a no-theme hackathon, a few boundaries actually help people to come up with ideas - creativity loves constraints . Our approach is to have a theme, but allow people to interpret it as they like - or even ignore it if they have some idea they’re passionate about. Past Indeed hackathon themes have included the usual suspects for a consumer-focused Internet company: mobile, social, data, etc. For this hackathon, we did something a little different. At Indeed, our mission is to help people get jobs. Our theme this time was \"Help People Who Help People Get Jobs,\" which encouraged the hackers at our engineering offices around the world to think of their colleagues in other departments (sales, marketing, client services, etc) and how they could be helped. To: All Sites From: Indeed Hackathon Committee Subject: Hackathon Theme Who helps people get jobs? WE DO! All of us - from Ops to Dev to HR to Marketing to International to Sales and Client Support. We’re pretty good at it. Jobseekers love Indeed, and we love them. But for the first Hackathon of 2014 lets give some attention to another special group of people: Indeeders. That’s right. It’s time to help people who help people get jobs! Does your buddy in International look stressed out? Write some code to generate his powerpoints! Think our software should sell itself?  Make it so with Twilio! Know a CS rep who needlessly suffers because he or she can’t automate away some daily task?  Prevent hardship with Python. Feeling particularly subversive? Build a web app to replace your (or someone else’s) manager. To solicit ideas from the whole company we had an idea wall on IndeedFeed (our in-house social network) where everyone could vote. Then we had an idea happy hour and pitch-fest where folks from all parts of the business could practice their elevator pitch and entice hackers to work on their ideas. Tip 2: Provide plentiful good food. Great hackers deserve great food (better than pizza and soda). And healthy food! That’s why we kicked off mid-day Thursday with a chocolate fountain at lunch. Oops: s/healthy/delicious/g For dinner, Korean-Mexican fusion food truck Chi’Lantro set up a taco buffet. Foodies in Austin love Chi’Lantro and so did the participants. But it wouldn’t be Austin without some high quality barbecue! FYI: in Texas, barbecue (\"BBQ\") does not mean meat cooked over an open flame on a grill, it means smoked for many hours at low temperature in a smoker. Fortunately for us, we have a couple of amateur pitmasters on staff. They brought a smoker and treated us to a whole side of beef brisket at midnight and another mid-day Friday. Delicious! Photos: The chocolate fountain and brisket (not at the same time). Yum! Tip 3: Get quality T-Shirts. Make t-shirts that people want to wear. Besides great artwork, this means comfortable good-looking t-shirts in a range of women’s and men’s sizes. We’ve tried a number of suppliers, and our current favorite is American Apparel tri-blend . Many thanks to Hans Krebs, one of our visual designers, who made the artwork for this hackathon's t-shirt. The design depicts Indeed employees, wearing our trademark blue \" I Help People Get Jobs \" t-shirts, arranged to spell hack . But it’s also a game: find the 10 unique representations of people (hint: one has a mustache, one is wearing a skirt, etc). Tip 4: Involve everyone, not just coders. Not everyone can code, but everyone is creative and can make stuff. Our hackathons are more than just code-a-thons. They’re a 24-hour creative-expression-fest. For example, at previous hackathons we've painted artwork, including a giant system diagram, on the walls. At this hackathon we had non-coders learning to code (thanks codecademy!), we had a product manager direct a movie, and folks made some great oil-on-canvas paintings. Next time we’re thinking of expanding the art-side of the hackathon even further. As for music, if you were saddened by the shutdown of Turntable.fm rejoice! PlugDJ is a great replacement - a real-time shared music-playing experience. Set up a PlugDJ community room for your event and send your hackathon participants the link. For our hackathon everyone moves into one big workspace and we set up loudspeakers hooked up to our PlugDJ room. This way everyone gets to take turns choosing the music, and people can vote it up or down. Tip 5: Finish strong. After a hard 24 hours hacking, everyone deserves the opportunity to show off their work. Teams need to be encouraged to practice their presentation ahead of time. We limited our presentations to 2 minutes, which is very short when you’re trying to demo something and explain it. To keep the 30+ presentations moving, we set up two presentation stations, each with its own laptop and projector. While one team was presenting, the next team was setting up their demo. This made for fast transitions, which everyone appreciated after being up all night. The best part of this hackathon? 15 projects are going into production to help people who help people get jobs! These include a new internal Question & Answer platform, a tool for managing translations, several new reports for various departments, improvements to IndeedFeed, and much more. If you have other suggestions for a great hackathon, share your thoughts with us, we’re @IndeedEng .", "date": "2014-02-03"},
{"website": "Indeed-Engineering", "title": "Using Proctor for A/B Testing from a Non-Java Platform", "author": [" by Parker Seidel"], "link": "https://engineering.indeedblog.com/blog/2014/09/proctor-pipet-ab-testing-service/", "abstract": "We’re excited to announce the open sourcing of proctor-pipet , a tool we created that allows you to deploy Proctor as a remote service. proctor-pipet is a Java web application that exposes Proctor as a simple REST API accessible over HTTP. This means that you can do A/B testing in applications written in non-JVM languages like Python. In addition to proctor-pipet, we have made available a Python package called django-proctor that makes it easy for your Django web app to use Proctor groups. We look forward to others implementing similar packages for their favorite web frameworks, such as Ruby on Rails or .NET MVC. These packages are the result of some great work by one of our fantastic summer 2014 interns . How it works Your web application makes HTTP requests to Proctor through proctor-pipet. Proctor returns the group assignments, which your web app can then use to make decisions on the content it returns to the user’s browser. Deploying Proctor remotely through proctor-pipet lets you take advantage of all the features of  the Proctor library: Assign users to test groups Use identifiers to map to different test types Toggle features or implement gradual rollouts of new features Make changes to test allocations independently of your code Determine group membership based on rules that use arbitrary context variables (for example, to target mobile devices) Click for Proctor documentation . Download both tools on GitHub: proctor-pipet ( https://github.com/indeedeng/proctor-pipet ) and django-proctor ( https://github.com/indeedeng/django-proctor ). Both pages include documentation with examples to help you get started. If you have any questions, ask them in our Proctor Q&A forum .", "date": "2014-09-03"},
{"website": "Indeed-Engineering", "title": "Bug Bounty Program: Cash Rewards for Reported Vulnerabilities", "author": [" by Gregory Caswell"], "link": "https://engineering.indeedblog.com/blog/2014/08/bug-bounty-program-cash-rewards-for-reported-vulnerabilities/", "abstract": "As part of Indeed’s focus on constantly improving how we help people get jobs, we are proud to announce the rollout of our bug bounty program. Through Bugcrowd , interested security professionals will now be able to disclose vulnerabilities and be rewarded for their efforts. For every unique submission that leads to a code change, we will be paying between $50 and $1,500. The range is dependent on the type and severity of the vulnerability reported. To view everyone who has helped us so far, or just to see how you stack up against the competition, head on over to the Hall of Fame . Full details on how you can help us improve our services (and get paid!) can be found on our Bugcrowd site . Please keep in mind that attacks against the current user base are strictly prohibited, as are automated vulnerability scanners. Responsible pen testers should always minimize system degradation and impact against users. Ready to get started? Sign up at Bugcrowd and join over 10,000 security researchers on the largest and most diverse security testing team in the world. +", "date": "2014-08-03"},
{"website": "Indeed-Engineering", "title": "Proctor: Indeed’s A/B Testing Framework", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2014/06/proctor-a-b-testing-framework/", "abstract": "(Editor’s Note: This post is the first of a series about Proctor, Indeed’s open source A/B testing framework.) A/B Testing at Indeed Indeed’s mission is to help people get jobs. We are always asking ourselves the question \"What’s best for the jobseeker?\" We answer that question by testing and measuring everything. We strive to test every new feature and every improvement in every product at Indeed, and we measure the impact of those changes to ensure they are helping us achieve our mission. In October 2013, Tom Bergman and Matt Schemmel presented an @IndeedEng talk on Proctor, Indeed’s A/B testing framework. In that talk, we announced that we had made Proctor available open source . Since then, we have also open sourced Proctor Webapp , the web application that we use to manage Proctor test definitions. In the October talk, Tom gave the example of a simple A/B test to determine if changing the background color of a button would improve user experience. Figure 1 shows control group A, in which we haven’t changed our Find Jobs button, and test group B, in which the button has a blue background. Figure 1: Testing an existing black text on gray background find jobs button treatment (A) against a version with a white text and blue background (B) As discussed in our logrepo talk and blog post , we log everything at Indeed so that we can analyze, learn, and improve our products. For this simple test, we logged the group (A or B) of each user visiting Indeed and the subsequent clicks. Then we used our analysis tools to determine that the test group led to more searches and greater overall user engagement. The example above has one test behavior, but we typically try out multiple alternate behaviors in a given test. In this test, we would be likely to try more than one different background color. We can also test multiple ideas at the same time, as in the example in Figure 2, in which one test is for the button text and the other is for the background color. Testing multiple variables (like text and color) for a particular area of functionality is known as \"multivariate testing.\" Figure 2: Running two tests on the same button simultaneously—where we test changes to text color, background, and text content (\"find jobs\" against \"search\"). We’ve been doing A/B testing at Indeed for years, and many of the lessons we learned informed the development of Proctor. The October talk covers in more detail Proctor’s design decisions and ways to use for it for more than just A/B testing. In this blog post, we focus on some of Proctor’s key features and concepts, and we explain the nuts and bolts of how we use Proctor at Indeed. Proctor Features and Concepts Standard representation Proctor provides a standard JSON representation of test definitions and allows adjustments to those definitions to be deployed independently of code. We refer to the full set of test definitions as the test matrix . A test matrix can be distributed to multiple applications as a single file, allowing for greater agility when managing tests and for sharing of consistent test definitions across multiple applications. Figure 3 shows a very simple version of our button test, with 50% of users allocated to the control group A (bucket 0) and 50% to the test group B (bucket 1). \"buttontst\": {\r\n    \"description\": \"backgroundcolortest\",\r\n    \"salt\": \"buttontst\",\r\n    \"buckets\": [\r\n    {\r\n        \"name\": \"control\",\r\n        \"value\": 0,\r\n        \"description\": \"current button treatment (A)\"\r\n    },\r\n    {\r\n        \"name\": \"altcolor\",\r\n        \"value\": 1,\r\n        \"description\": \"test background color (B)\"\r\n    }\r\n    ],\r\n    \"allocations\": [\r\n    {\r\n        \"ranges\": [\r\n        {\r\n            \"length\": 0.5,\r\n            \"bucketValue\": 0\r\n        },\r\n        {\r\n            \"length\": 0.5,\r\n            \"bucketValue\": 1\r\n        }\r\n        ]\r\n    }\r\n    ],\r\n    \"testType\": \"USER\"\r\n} Figure 3: a simple Proctor test definition To understand this example, here is a quick overview of some Proctor terminology: Every test has a testType . The most common type is USER, meaning that we use a user identifier to map to a test variation. More on test types later. Each test is made of an array of buckets and an array of allocations . A bucket is a variation, or group, within a Proctor test definition. Each bucket has a short name , an integer value , and a human-friendly description . An allocation specifies the size of the buckets as an array of ranges . Each range has a length between 0 and 1 and a reference to the bucketValue for the bucket. Ranges in an allocation must sum to 1. You can have more than one allocation if you use rules (more about that later). Proctor Webapp Using the Proctor Webapp, you can manage and deploy test definitions from a web browser. You can customize the application in a number of ways, allowing integration with: revision control systems for maintaining history of test changes, issue tracking systems for managing test modification workflow, and other external tools, such as build and deployment systems. Figure 4: Screenshot of a test definition in the Proctor Webapp Java code generation from JSON test specifications Test specifications in Proctor are JSON files that are independent of the test definitions and allow applications to declare the tests and buckets of which they are aware. They can be used in the build process for Java code generation and at runtime to load the relevant subset of the test matrix. Code generation is optional but provides compile-time type-safety, so you don’t have to litter your code with string literals containing test and bucket names. The generated classes also make it easier to work with tests in Java code and in template languages (figure 5 shows a JSP example). Furthermore, the generated Java objects enable serialization of test group membership into formats like JSON or XML. <c:if test=&quot;${groups.buttontstAltColor}&quot;>\r\n  .searchBtn { background-color: #2164f3; color: #ffffff; }\r\n</c:if> Figure 5: Conditional CSS based on test group membership in a JSP template Rule-based contextual allocation Using Proctor’s rule definition language, your system can apply tests and test allocations by evaluating rules against runtime context. For example, you can define your entire test to only be available for a certain segment of users, or you can adjust the allocation of test groups depending on the segment. Your test could be 50% A and 50% B for users in one country, and 25% each A/B/C/D for users in all other countries. Rule-based group assignment allows for great flexibility in how you roll out and evaluate your tests. \"allocations\" : [\r\n{\r\n    \"rule\" : \"'US' == country && 'en' == userLanguage\",\r\n    \"ranges\": [\r\n    {\r\n        \"length\": 0.5,\r\n        \"bucketValue\": 0\r\n    },\r\n    {\r\n        \"length\": 0.5,\r\n        \"bucketValue\": 1\r\n    }\r\n    ]\r\n},\r\n{\r\n    \"rule\" : null,\r\n    \"ranges\" : [\r\n    {\r\n        \"length\" : 1.0,\r\n        \"bucketValue\" : -1\r\n    }\r\n    ]\r\n}\r\n] Figure 6: 50/50 test for US English, test inactive (bucket -1) for everyone else Payloads The ability to attach data payloads to test groups in test definitions allows you to simplify your code. In figures 7 and 8, we demonstrate how the color being tested for the button can be specified as a payload in the test definition and accessed in the template. Although in this example the total amount of template code is not reduced, if you had multiple test variations, each with its own color, the use of payloads would result in fewer lines of code. \"buckets\": [\r\n{\r\n    \"name\": \"control\",\r\n    \"value\": 0,\r\n    \"description\": \"current button treatment (A)\",\r\n    \"payload\": {\r\n        \"stringValue\": \"#dddddd\"\r\n    }\r\n},\r\n{\r\n    \"name\": \"altcolor\",\r\n    \"value\": 1,\r\n    \"description\": \"test background color (B)\",\r\n    \"payload\": {\r\n        \"stringValue\": \"#2164f3″\r\n    }\r\n}\r\n] Figure 7: Attaching a data payload containing a color value to the test group B <style>\r\n  .searchBtn { background-color: ${groups.buttontstPayload}; }\r\n</style> Figure 8: Using the data payload in CSS in a JSP template Flexible test types Proctor has a flexible concept of test types, allowing bucket determination to be based on user (typically defined by a tracking cookie value), account ID (which can be fixed across devices), email address, or completely random across requests. You can also extend Proctor with your own test types. Custom test types are useful, for example, when you want test group determination to be based on a context- or content-based attribute such as page URL or content category. Unbiased, Independent Tests To assign a bucket for a test, Proctor maps the input identifiers (e.g. user ID) to an integer value using a uniformly distributed hash function. The range assignments for a bucket determine the range of integers that define each bucket. Figure 9 shows a simple example with a 50/50 control/test distribution. Since the hash function is uniform, the distribution of bucket assignments should be unbiased. Figure 9: 50/50 control/test buckets mapped onto an integer range for use with hash function Furthermore, Proctor tests are independent, meaning that group membership in one test has no correlation with membership in another. This independence is accomplished by assigning a different salt to each test. The salt is used along with the identifiers as input to the hash function. Including the salt in the test definition allows for two advanced features: You can intentionally align buckets in different tests (make them dependent) by sharing a salt (shared salts must start with \"&\"). In practice, we have very rarely seen the need to align two tests in this way. You can \"shuffle\" the distribution of a test by changing its salt, resulting in completely different output from the hash function. This shuffling can be used to reassure yourself that there is no accidental bias in a test. Proctor at Indeed Proctor has become a crucial part of Indeed’s data-driven approach to product development, with over 100 tests and 300 test variations currently in production. In our next post, we will provide more details on how we use Proctor at Indeed. UPDATE : Our second post in this series, How Indeed Uses Proctor for A/B Testing is now available.", "date": "2014-06-13"},
{"website": "Indeed-Engineering", "title": "I Challenge you to a Duel: Indeed’s 2013-2014 coding competitions", "author": [" by Doug Gray, Nick Whelan and Courtney Jeffries"], "link": "https://engineering.indeedblog.com/blog/2014/04/challenge-duel-indeeds-2013-2014-coding-competitions/", "abstract": "This past February, 136 collegiate coders from UT Austin , UIUC , MIT , UTokyo and 10 other Japanese universities gathered in classrooms across the nation to compete in Indeed’s Coding Duel. This “final showdown” coding competition resulted from the culmination of two duels held in Fall 2013, in the United States and Japan. Where the Fall 2013 Indeed Coding Duels were isolated to each university, this showdown pitted the top finalists from previous duels against one another. The February showdown also raised the stakes by challenging coders to solve 10 logic and mathematical problems in exchange for a $3,000 cash prize for the top finisher. What events led up to this final competition? What programming languages dominated the competition? Read on to find out how the competition went down, and how your university can get involved next year. University Fall Face-off: U.S. competition The Fall 2013 Indeed Coding Duel attracted about 100 participants from UT, UIUC, and MIT. Participants were given three hours to attempt seven programming exercises similar to the types one might find at an ACM-ICPC competition. With MIT as the new competitor and a friendly rivalry between UIUC and UT, the atmosphere became increasingly lively as the results began rolling in on our live results dashboard. MIT had the first correct answer on the board, but UIUC and UT quickly followed. As the competitors entered into the final hour of the competition, the competition was neck and neck between MIT and UIUC. In the final half-hour, we turned off the results board, increasing the suspense of the event’s finale. Despite their newcomer status to the competition, MIT edged ahead and won the competition. Both UIUC and UT were close behind, with Illinois ultimately coming in second, and UT in third. The winners at each school won Chromebooks. The second and third place finishers at each school won a Nexus 7 and a Das Keyboard, respectively. UT students at Indeed’s Fall 2013 Coding Duel (Photo by Jolynn Cunningham) The casual walk-up nature of Indeed Coding Duels at the U.S. universities attracted engineers representing a variety of programming languages and skills, and underclassmen were eager to hear how they could prepare for future duels (practice makes perfect!). A December Duel in the East: Japan competition On December 8, 2013, Recruit and Indeed hosted a similar contest in Japan, inviting all Japanese computer science students to participate. 159 students (109 in Tokyo and 50 in Kyoto) joined the competition. The excitement of all the participants was palpable in the room and on Twitter at #rprocon . To our surprise, 34 coders submitted a correct answer to the first problem in less than five minutes! Within one hour, one contestant had already solved seven of the eight questions, so we added a ninth question that was not used in the U.S. to keep the winner entertained for the last few hours. An hour later, we had to add a tenth question. The final results were astounding: 19 people solved eight or more problems, including one person who solved all 10. No one from MIT, UIUC or UT Austin had solved more than seven. It was clear to us that the coders from Japan would be a force to be reckoned with in the final competition in the U.S. Indeed/Recruit’s Coding Duel in Japan (Photo by Shigeru Nishikawa) East Meets West: Top finishers from Japan compete stateside In February 2014, Indeed flew the top finishers from the Japan Indeed/Recruit Coding Duel to Boston for a final showdown against the top U.S. contestants from UT, UIUC, and MIT. Of the 136 competitors in the final duel, 117 students submitted at least one correct solution. Breakdown of final showdown participants by university: Japanese universities (22) UIUC (26) MIT (35) UT Austin (53) Doug Gray, SVP Engineering at Indeed, describes the competitive environment at MIT, “We held the competition in a lecture room. Nearly every seat was taken and reporters from Japan and spectators formed a ring around the room. It was quiet except for the sound of clicking keys.” The scene at the final showdown at MIT (Photo by Takeshi Akita) Our live results dashboard showed coder’s scores as they solved each problem and provided a view into the competitions that were occurring on the UT and UIUC campuses at the same time. The composition of the top 10 finalists included 2 students from MIT, 1 from UIUC and 7 from Japan. One of the most exciting parts of this competition was the variety of programming languages used by the participants. The two most common languages used were Java and Python, but we also saw some C++ and Ruby. Interestingly, competitors at UT primarily wrote in Java, MIT students favored Python, and UIUC represented a diverse mix. The programming language breakdown at the final Coding Duel is as follows (by solution, not by contestant): C++ (281) Java (279) Python (234) C (48) Ruby (11) Perl (6) Matlab (3) The overall winner of the final competition walked away with a $3,000 prize after solving 10 correct problems in three hours. He solved all of the problems using C++. Japanese tech media reported the results of Indeed and Indeed/Recruit’s Coding Duel’s in Gizmodo (Japan) and Wired (Japan). Get coding! Want to be a part of Indeed’s 2014-2015 coding competitions? Make sure to keep your coding skills fresh by participating in Top Coder or by trying your hand at past problems like these from the ACM-ICPC World Finals. Would you like your school to participate in an Indeed Coding Duel next year? Please email university-tech-recruiting@indeed.com . UPDATE (2/11/2016): Changed recruiting contact instructions.", "date": "2014-04-09"},
{"website": "Indeed-Engineering", "title": "@IndeedEng 2014 Year In Review", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2015/01/indeedeng-2014-year-in-review/", "abstract": "We help people get jobs. That’s our mission at Indeed. And being the #1 job site worldwide challenges each of our engineers to deliver the best job seeker experience at scale. When we launched this blog in 2012, we set out to contribute to the broader software community by sharing what we’ve learned from these challenges. Response to our @IndeedEng tech talks continues to be strong, with over 700 people attending the series in 2014. And our engineering blog received 59,000 views from 128 countries. Here’s a brief recap of content we shared in 2014: Testing and measuring everything is central to our development process at Indeed. We use Proctor , our A/B testing framework, to accomplish this, and we open sourced the framework in 2013. Building on this in 2014, we described Proctor’s features and tools and then wrote about how we integrate Proctor into our development process at Indeed. We also released proctor-pipet , a Java web application that allows you to deploy Proctor as a remote service. We open sourced util-urlparsing , a Java library we created to parse URL query strings without unnecessary intermediate object creation. In the first half of 2014, we held several tech talks devoted to Imhotep, our interactive data analytics platform. Imhotep powers data-driven decision making at Indeed and we were excited to talk about the technology: scaling decision trees , building large-scale analytics tools , and using Imhotep to focus on key metrics . Then in November, we open sourced part of the platform and held a tech talk and workshop for attendees to explore their own data in Imhotep. Being a global company is a challenge we take seriously. We shared our experience of iteratively expanding to new markets and how international success requires solving a diverse set of technical challenges . Changes coming to the blog and talks pages include translating content into Japanese . Look for more translated posts in the months to come. We’d like to thank everyone who helped make these accomplishments possible. If you follow the blog and watch our @IndeedEng talks, thank you for your support! We look forward to continuing the conversation in 2015.", "date": "2015-01-05"},
{"website": "Indeed-Engineering", "title": "Why I Unit Test", "author": [" by Dave Griffith"], "link": "https://engineering.indeedblog.com/blog/2014/12/why-i-unit-test/", "abstract": "If you’ve done any software development in the last fifteen years, you’ve heard people harping on the importance of unit testing. Your manager might have come to you and said \"Unit tests are great! They document the code, reduce the risk of adding bugs, and reduce the cost and risk of making changes so we don't slow down over time! With good unit tests, we can increase overall delivery velocity!\" Those are all great reasons to unit test, but they are all fundamentally management reasons. I agree with them, but they don't go to the core of why I, as a developer, unit test. The reason I unit test is simple: Unit testing is both an opportunity and a strong incentive to improve new and existing designs, and to improve my skills as a designer of software. The trick is to write as few unit tests as possible and ensure that each test is very simple. How does that work? It works because writing simple unit tests is intrinsically boring, and the worse your code is, the more difficult and boring it will be to test. The only way to get any traction with unit testing is to drastically improve your implementation to the point where it can be covered with hardly any unit tests at all, and then write those. Avoiding unit tests by improving your implementation Here are some approaches for writing fewer unit tests: Refactor out repeated code . Each block of code that you are able to abstract out is one less unit test to write. Delete dead code . You don’t have to write unit tests for code that you can delete instead. If you think this is obvious, then you haven’t seen many large legacy code bases. Externalize framework boilerplate as configuration or annotation . That way, you only have to write unit tests for product logic rather than scaffolding. Every branch of code needs at least one unit test, so every if statement or loop you can remove is one less test to write . Depending on your implementation language, if statements and loops can be removed by subtype polymorphism, code motion, pluggable strategies, aspects, decorators, higher order combinators or a dozen other techniques. Each branch point in your code is both a weakness and a requirement for additional testing. Remove them if at all possible. Identify deeper data-flow patterns and abstract them . Often pieces of code that don’t look similar can be made similar by pulling out some incidental computations. Once you’ve done that, then underlying structures can be merged. That way, more and more of your code becomes trivially testable branch-free computations. In the limit, you end up with a bunch of simple semantic routines (often predicates or simple data transformations) strung together with a double handful of reusable control patterns. Separate out your business logic, persistence, and inter-process communications as much as possible, and you can avoid a bunch of tedious mucking with mock objects. Mock objects are code smells, and overuse of them may indicate that your code has become overly coupled. Figure out how to generalize your logic so that your edge cases are covered by your main flow, and single tests can cover diverse and complex inputs. Too often we write single-purpose code for special cases, when we could instead search for more general solutions that cover those cases without special handling. Note however, that discovering the simpler, more general solutions is often much more difficult than creating a bunch of special cases. You may not have enough time to write small amounts of simple code, and instead have to write large amounts of complex code. Recognize and replace logic that is already implemented as methods in existing libraries , and you can push the trouble of unit testing off onto the library’s author. If you can simplify your data objects so much that they are immutable and their operations follow simple algebraic laws, you can utilize property-based testing , where your unit tests literally write themselves. But yammering is cheap, let’s see some code! Finding deep patterns and abstracting out repeated code A common pattern in data-science code is to look to find the element of some collection for which some function is optimized. The simplest Java code for this might resemble the following: double bestValue = Double.MIN_VALUE;\r\n  Job bestJob = null;\r\n  for (Job job : jobs) {\r\n    if (score(job) > bestValue) {\r\n      bestJob = job;\r\n    }\r\n  }\r\n  return bestJob; This is quick enough to code that you might write it without even thinking about it. Just a loop and an if! What can go wrong? That’s fine the first few times you write it, but you’re building up technical debt every time. Writing unit tests is where the repetition and risk starts to really show up. Every block of code like this will need tests not just for correctness in the common case, but also for a bunch of edge cases: what happens if we passed in an empty collection? a single element collection? null? Even the simple code above has some bugs that unit tests can find, but you have to write a lot of them every time you wish to do an optimization, and I don’t know about you, but frankly I’ve got more useful things to do with my time. A better solution is to realize that even this small amount of code repetition can and should be abstracted out, coded and tested only once. It also gives us a chance to genericize the code and fix some edge cases. public static <J> J argMax(Iterable<J> collection,\r\n                               Function<J, Double> score) {\r\n      double bestValue = Double.MIN_VALUE;\r\n      J bestElement = null;\r\n      if (collection != null) {\r\n        for (J element : collection) {\r\n          if (score.apply(element) > bestValue) {\r\n            bestElement = element;\r\n          }\r\n        }\r\n      }\r\n      return bestElement;\r\n    } This code needs to be unit tested only once. For an even better solution, we can replace all of this logic with a library call (in this case from Google’s Guava library): public static <J> J argMax(Iterable<J> collection,\r\n                             Function<J, Double> score) {\r\n    return Ordering.natural().onResultOf(score).max(collection);\r\n  } After that, you only need unit tests for each different scoring function you use. Everything else has already been handled. Avoiding unit tests: a path to understanding great software design The thing about all of these unit-test avoidance techniques is that they are essential to the process of creating robust and supple designs even if you weren't going to do any unit testing at all! Too often, in our rush to simply get something working, we don't follow these techniques, but continual unit testing gives us a time and a reason to do it right. In this way, you can leverage aggressive laziness in implementing unit tests to drive continuous improvement of your project design and implementation. At least, it can if you let it. If you spend your unit testing time writing unit tests for your code without improving its underlying design, you'll most likely never learn anything, and you'll have little reason to create code with quality better than \"it mostly works.” If you spend your unit testing time looking to minimize the total amount of testing code that you write (by improving your product code), you'll quickly learn just what it means for software to be well-designed. I don’t know about you, but that’s why I love programming in the first place. Dave Griffith has been building software systems for over 20 years.", "date": "2014-12-31"},
{"website": "Indeed-Engineering", "title": "How Indeed Uses Proctor for A/B Testing", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2014/11/how-indeed-uses-proctor-for-a-b-testing-2/", "abstract": "(Editor’s Note: This post is the second in a series about Proctor, Indeed’s open source A/B testing framework.) Proctor at Indeed In a previous blog post , we described the features and tools provided by Proctor , our open-source A/B testing framework. In this follow-up, we share details about how we integrate Proctor into Indeed’s development process. Customized Proctor Webapp Our internal deployment of the Proctor Webapp integrates with Atlassian JIRA , Subversion , Git , and Jenkins . We use JIRA for issue linking, various sanity checks, and automating issue workflow. For tracking changes over time, we use Subversion (for historical reasons — Git is also an option). We use Jenkins to launch test matrix builds, and the webapp integrates with our internal operational data store to display which versions of a test are in use in which applications. Figure 1: Screenshot of a test definition’s change history in the Proctor Webapp Issue tracking with JIRA At Indeed, we track everything with JIRA issues, including changes to test definitions. Requests for new tests or changes to existing tests are represented by a custom issue type in JIRA that we called “ProTest” (short for “ Pro ctor Test ”). We track ProTest issues in the JIRA project for the application to which the test belongs. The ProTest issues also use a custom workflow that is tied into our deployment of the Proctor Webapp. After accepting an assigned ProTest issue, the issue owner modifies the test definition using Proctor Webapp. When saving the changes, she must provide a ProTest issue key. Before committing to our Proctor test definition repository, the webapp first verifies that the ProTest issue exists and is in a valid state (for example, is not closed). The webapp then commits the change (on behalf of the logged-in user), referencing the issue key in the commit message. After the issue owner has made all changes for a ProTest issue, the JIRA workflow is usually as follows: The issue owner resolves the issue, which moves to state QA Ready . A release manager uses Proctor Webapp to promote the new definition to QA. The webapp moves the issue state to In QA . A QA analyst verifies the expected test behavior in our QA environment and verifies the issue, which moves to state Production Ready . A release manager uses Proctor Webapp to promote the new definition to production, triggering worldwide distribution and activation of the test change within one or two minutes. The webapp moves the issue state to In Production . A QA analyst verifies the expected test behavior in production and moves the issue state to Pending Closure . The issue owner closes the issue to reflect that all work is complete and in production. In cases where we are simply adjusting the size of an active test group, Proctor Webapp skips this process and automatically pushes the change to production. Our QA team verifies test modifications because those modifications can result in unintended behavior or interact poorly with other tests. Rules in test definitions are a form of deployable code and need to be exercised to ensure correctness. The verification step gives our QA analysts one last chance to catch any unintended consequences before the modifications go live. Consider the case of this rule, intended to make a test available only to English-language users in the US and Canada: (lang=='en' && country=='US') || country=='CA' The parentheses are in the wrong place, allowing French-language Canadians to see behavior that may not be ready for them. A developer forcing himself into the desired group might have missed this bug. When we catch bugs right away during QA, we avoid wasting the time it would take to notice that the desired behavior never made it to production. Test definition files We store test definitions in a single shared project repository called proctor-data . The project contains one file per test definition: test-definitions/ /definition.json Modifications to tests most often are done via the Proctor Webapp, which makes changes to the JSON in the definition file and commits those changes (on behalf of the logged-in user) to the version control repository. The definition files are duplicated to two branches in proctor-data : qa and production .  When a test definition revision is promoted to QA, the entire test definition file is copied to the qa branch and committed (as opposed to applying or “cherry-picking” the diff associated with a single revision). Similarly, when a test definition revision is promoted to production, the entire file is copied to the production branch and committed. Since we have one file per test definition, this simple approach maintains the integrity of the JSON definition while avoiding merge conflicts and not requiring us to determine which trunk revision deltas to cherry pick. Building and deploying the test matrix Proctor includes a builder that can combine a set of test definition files into a single text matrix file, while also ensuring that the definitions are internally consistent, do not refer to undefined bucket values, and have allocations that sum to 1.0. This builder can be invoked directly from Java or via an Ant task or a Maven plugin . We build a single matrix file using a Jenkins job that invokes Ant in the proctor-data project. An example of building with Maven is available on GitHub . A continuous integration (CI) Jenkins job builds the test matrix every time a test change is committed to trunk. That matrix file is made available to applications and services in our CI environment. When a release manager promotes a test change to QA, a QA-specific Jenkins job builds the test matrix using the qa branch. That generated matrix file is then published to all QA servers. The services and applications that consume the matrix periodically reload it. An equivalent production-specific Jenkins job handles new changes on the production branch. Proctor in the application Each project’s Proctor specification JSON file is stored with each project’s source code in a standard path (for example, src/main/resources/proctor ). At build time, we invoke the code generator (via a Maven plugin or Ant task ) to generate code that is then built with the project’s source code. When launching a new test, we typically deploy the test matrix before the application code that depends on it. However, if the application code goes out first, Proctor will “fall back” and treat the test as inactive – if you follow our convention of mapping your inactive bucket to value -1. You can change the fallback behavior by setting fallbackValue to the desired bucket value in the test specification. We follow the convention of falling back on the unlogged inactive group to help ensure that test and control groups do not change size unexpectedly. Suppose that you have groups 0 (control) and 1 (test) for a test that runs Monday-Thursday with fallback to group 0. If your test matrix is broken as a result of a change from Tuesday 2pm to Tuesday 5pm, summing your metrics across the whole period from Monday to Thursday will skew the results for the control group. If your fallback was -1 (inactive), there would be no skew for your control and test groups. When adding a new bucket to a test, we typically take this sequence of actions: Deploy the test matrix with no allocation for the new bucket. Deploy the application code that is aware of the new bucket. Redeploy the matrix with an allocation for that bucket. If the matrix is deployed with an allocation for a new bucket of which the application is unaware, Proctor errs on the side of safety by using the fallback value for all cases. We made Proctor work that way to avoid telling the application to apply an unknown bucket in some cases for some period of time, which could skew analysis. We take similar precautions when deleting an entire test from the matrix. Testing group membership, not non-membership Proctor’s code generation provides easy-to-use methods for testing group membership. We have found it best to always use these methods to test for membership rather than non-membership. If you’ve made your code conditional on non-membership, you run the risk of getting that conditional behavior in unintended circumstances. As an example, suppose you have a [50% control, 50% test] split, and in your code you use the conditional expression !groups.isControl() , which is equivalent to groups.isTest() . Then, to reduce the footprint of your test while keeping an equal-sized control group for comparison, you change your test split to [25% control, 50% inactive, 25% test] . Now your conditional expression is equivalent to groups.isTest() || groups.IsInactive() . That logic is probably not what you intended, which is to keep the same behavior for control and inactive. In this example, using groups.isTest() in the first place would have prevented you from introducing unintended behavior. Evolving bucket allocations We recognize that assigning users to test buckets may affect how the site behaves for them. Proctor on its own cannot ensure consistency of experience across successive page views or visits as a test evolves. When growing or shrinking allocations, we consider carefully how users will be affected. Usually, once a user is assigned to a bucket, we’d like for that user to continue to see the behavior associated with that bucket as long as that behavior is being tested. If your allocations started as [10% control, 10% test, 80% inactive] , you would not want to grow to [50% control, 50% test] , because users initially in the test bucket would be moved to the control bucket. There are two strategies for stable growth of buckets. In the “split bucket” strategy (Figure 2), you add new ranges for the existing buckets, moving from 10/10 to 50/50 by taking two additional 40% chunks from the inactive range. The resulting JSON is shown in Figure 3. Figure 2: Growing control and test by splitting buckets into multiple ranges \"allocations\": [\r\n  {\r\n      \"ranges\": [\r\n      {\r\n          \"length\": 0.1,\r\n          \"bucketValue\": 0\r\n      },\r\n      {\r\n          \"length\": 0.1,\r\n          \"bucketValue\": 1\r\n      },\r\n      {\r\n          \"length\": 0.4,\r\n          \"bucketValue\": 0\r\n      },\r\n      {\r\n          \"length\": 0.4,\r\n          \"bucketValue\": 1\r\n      }\r\n      ]\r\n  }\r\n  ] Figure 3: JSON for “split bucket” strategy; 0 is control and 1 is test In the “room-to-grow” strategy, you leave enough inactive space between buckets so that you can adjust the size of the existing ranges, as in Figure 4. Figure 4: Growing control and test by updating range lengths to grow into the inactive middle We use the “room-to-grow” strategy whenever possible, as it results in more readable test definitions, both in JSON and the Proctor Webapp. Useful helpers Proctor includes some utilities that make it easier to work with Proctor in web application deployments: a Spring controller that provides three views: the groups for the current request, a condensed version of the current test matrix, and the JSON test matrix containing only those tests in the application's specification; a Java servlet that provides a view of the application’s specification; and support for a URL parameter that allows you to force yourself into a test bucket (persistent via a browser cookie) We grant access to these utilities in our production environment only to privileged IP addresses, and we recommend you do the same. It works for Indeed, it can work for you Proctor has become a crucial part of Indeed’s data-driven approach to product development, with over 100 tests and 300 test variations currently in production. To get started with Proctor, dive into our Quick Start guide . To peruse the source code or contribute your own enhancements, visit our GitHub page .", "date": "2014-11-13"},
{"website": "Indeed-Engineering", "title": "Open Source Interactive Data Analytics with Imhotep", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2014/10/open-source-interactive-data-analytics-with-imhotep/", "abstract": "We are excited to announce the open source availability of Imhotep, the interactive data analytics platform that powers data-driven decision making at Indeed. When we test changes to our applications and services, whether to our user interface or our backend algorithms, we measure how those changes affect job seekers. We built Imhotep to allow our engineering and product organizations to focus on key metrics at scale. Key features The Imhotep platform and tools allow you to: Perform fast, interactive, ad hoc queries and aggregate results for large datasets Combine results from multiple time-series datasets Build your own data tools for analysis, monitoring, reporting, and automated data processing on top of the Imhotep platform At its core, Imhotep is a distributed inverted index on time-series data that runs across a cluster of servers. We’ve made it easy to set up an Imhotep cluster on Amazon Web Services (AWS). Once you’ve set up your cluster, you can upload your data and then interactively query that data using IQL, the Imhotep Query Language. The IQL web client enables you to answer all sorts of questions about your data, and iterate quickly on those questions to get to important insights. For example, at Indeed, we use Imhotep to answer these and many more questions about how people around the world are using our job search engine: How many unique job search queries were performed on a specific day in a specific country? What are the top 50 queries in a specific country? How many times did job seekers click on a search result for each of those queries? Which job titles have the highest click-through rate for the query “Architecture” in the US? Which titles have the lowest click-through rate? Getting started with Imhotep You can use our tools to configure your Imhotep cluster on AWS. These setup tools require that you have an AWS account, two S3 buckets for data storage, and your time-series data in TSV or CSV format for uploading into the system. To learn more, read our Imhotep documentation . If you need help, you can ask questions in our Q&A forum for Imhotep .", "date": "2014-10-23"},
{"website": "Indeed-Engineering", "title": "Finding Great (and Profitable) Ideas in the Computer Science Literature", "author": [" by Dave Griffith"], "link": "https://engineering.indeedblog.com/blog/2015/03/finding-great-and-profitable-ideas-in-the-computer-science-literature/", "abstract": "I spend quite a bit of time trawling through recent computer science papers, looking for anything algorithmic that might improve my team’s product and Help People Get Jobs. It’s been a mixed bag so far, often turning up a bunch of pretty math that won't scale at Indeed. But looking through the computer science literature can pay off big, and more of us should use the research to up our game as software developers. Word cloud generated by WordItOut Why read a computer science paper The first question you might ask is why ? Most working developers, after all, simply never read any computer science papers. Many smart developers look at me blankly when I even suggest that they do a literature search. \"You mean look on StackOverflow?\" The short answer: to get an edge on your problem (and occasionally on your competition or your peers). Some academic is looking into some deep generalization of whatever problem you are facing. They are hungry (sometimes literally, on academic salaries) to solve problems, and they give away the solutions . They are publishing papers at a ferocious pace, because otherwise their tenure committees will invite them to explore exciting opportunities elsewhere. Academics think up good, implementable approaches and give them away for free. And hardly anyone notices or cares, which is madness. But a smart developer can sometimes leverage this madness for big payouts. The key is knowing how to find and read what academics write. Finding computer science papers Thousands of computer science papers are published each year. How do you find a computer science paper worth reading? As with so many questions in this new century, the answer is Google, specifically Google Scholar . As near as I can tell, Google Scholar includes almost all the academic papers ever written, for free. Almost every computer science paper since Alan Turing is accessible there. With Scholar, Google is providing one of the most amazing resources anyone has ever given away. Some links point to papers behind paywalls, but almost all those have extra links to copies that aren’t. I've read hundreds of papers and never paid for one. Google doesn't even attempt to monetize it. Nobody in the general public has heard about scholar.google.com. More surprisingly: according to my Google contacts, not many Googlers have heard about it either. With Google Scholar, you’ve solved the problem of finding interesting papers. Filtering computer science papers Next, the problem is filtering and prioritizing the interesting papers you find. Google Scholar search algorithms are powerful, but they aren’t magic. Even your best search skills will net you too many papers to read and understand. The chance that you are reading the one that will most help your work is small. Here’s my basic strategy for quickly finding the best ones. First, figure out the paper’s publication date. This seems like an obvious bit of metadata, but you’ll rarely find the date on the paper itself. Instead, look for clues in Google Scholar. You can also assume that it's two years after the latest paper listed in the citations. This seems sloppy, but it’s effective. Computer science papers older than fifteen years are unlikely to contain anything of value beyond historical interest. Next, read the first paragraph of the paper. This paragraph covers the problem the researchers are trying to solve, and why it's important. If that problem sounds like yours, score! Otherwise, unless the authors have hooked you on the intrinsic interest of their results, dump it and move on to the next paper. If things still seem promising, read the second paragraph. This paragraph covers what the authors did, describes some constraints, and lets you know the results (in broad strokes). If you can replicate what they did in your environment, accept the constraints, and the results are positive, awesome. You’ve determined the paper is worth reading! How to read a computer science paper The biggest trick to reading an academic paper is to know what to read and what not to read. Academic papers follow a structure only slightly more flexible than that of a sonnet . Some portions that look like they would help you understand will likely only confuse. Others that look pointless or opaque can hold the secrets to interpreting the paper’s deeper meanings. Here’s how I like to do it. Don't read the abstract . The abstract conveys the gist of the paper to other researchers in the field. These are folks who've spent the last decade thinking about similar problems. You're not there yet. The abstract will likely confuse you and possibly frighten you, but won't help you understand the topic. Don't read the keywords . Adding keywords to papers was a bad idea that nonetheless seems to have stuck. Keywords tend to mislead and won’t add anything you wouldn’t get otherwise. Skip 'em, they're not worth their feed. Read the body of the paper closely . Do you remember the research techniques your teachers tried to drum into you in eighth grade? You’ll need them all. You’re trying to reverse engineer just what the researchers did and how they did it. This can be tricky. Papers tend to leave out many shared assumptions behind the research, as well as many details and small missteps. Read every word. Look up phrases or words you don’t know -- Wikipedia is usually fine for this. Write down questions. Try to figure out not just what the researchers did, but what they didn’t do, and why. Don't read the code . This is counterintuitive, because the clearest way software developers communicate is through code -- ideally with documentation, revision history, cross-references, test cases, and review comments. It doesn't work that way with academics. To a first approximation, code in academic papers is worthless. The skills necessary to code well are either orthogonal to or actively opposed to the skills necessary for interesting academic research. It's a minor scandal that most code used in academics is unreviewed, not version-controlled, lacks any test cases, and is debugged only to the point of \"it didn't crash, mostly, today.\" That's the good stuff. The bad stuff is simply unavailable, and quite probably long-deleted by the time the paper got published. Yes, that’s atrocious. Yes, even in computer science. Read the equations . Academics get mathematics, so their equations have all the virtues that software developers associate with the best software: precision, correctness, conciseness, evocativeness. Teams of smart people trying to find flaws offer painstaking reviews of the equations. In contrast, a bored grad student writes the code, which nobody reads. Don’t read the conclusions section . It adds nothing. Leveraging a computer science paper for further search Academic papers offers a bounty of contextual data in references to other papers. Google Scholar excels at finding papers, but there’s no substitute for actually following the papers that researchers used to inform their work. Follow the citations in the related work . Authors put evocative descriptions of the work that matters to them in “Related Work.” This provides an interesting contrast for interpreting their work. In some ways, this section memorializes the most important social aspects of academic work. Follow the citations in the references. Long before HTML popularized hypertext, academic papers formed a dense thicket of cross-references, reified as citations. For even the best papers, half of the value is the contents, half is the links. Citations in papers aren't clickable (yet), but following them is not hard with Google Scholar. Repeated citations of older papers? There's a good chance those are important in the field and useful for context. Repeated citations of new papers? Those papers give insight into the trajectory of the subject. Odd sounding papers with unclear connections to the subject? They are great for getting the sort of mental distance that can be useful in hypothesis generation. Once you’ve done all that… It’s just a simple matter of coding. Get to it! Dave Griffith has been building software systems for over 20 years.", "date": "2015-03-12"},
{"website": "Indeed-Engineering", "title": "Vectorized VByte Decoding: High Performance Vector Instructions", "author": [" by Jeff Plaisance"], "link": "https://engineering.indeedblog.com/blog/2015/03/vectorized-vbyte-decoding-high-performance-vector-instructions/", "abstract": "Data-driven organizations like Indeed need great tools. We built Imhotep , our interactive data analytics platform (released last year), to manage the parallel execution of queries. To balance memory efficiency and performance in Imhotep, we developed a technique called vectorized variable-byte (VByte) decoding. VByte with differential decoding Many applications use VByte and differential encoding to compress sorted sequences of integers. The most common compression method for inverted indexes uses this style of encoding. This approach encodes successive differences between integers instead of the integers themselves, using fewer bytes for smaller integers at the cost of using more bytes for larger integers. A conventional VByte decoder examines only one byte at a time, which limits throughput. Also, each input byte requires one branch, leading to mispredicted branches. Vectorized VByte decoding Our masked VByte decoder processes larger chunks of input data -- 12 bytes -- at one time, which is much faster than decoding one byte at a time. This is important for Indeed because Imhotep spends ~40% of its CPU time decoding variable-byte integers. We described this approach in a tech talk last year: Large Scale Analytics and Machine Learning at Indeed . Jeff Plaisance (Indeed), Nathan Kurz (Verse Communications), and Daniel Lemire (LICEF, Université du Québec) discuss the masked VByte decoder in detail in Vectorized VByte Decoding . The paper’s abstract follows: We consider the ubiquitous technique of VByte compression, which represents each integer as a variable length sequence of bytes. The low 7 bits of each byte encode a portion of the integer, and the high bit of each byte is reserved as a continuation flag. This flag is set to 1 for all bytes except the last, and the decoding of each integer is complete when a byte with a high bit of 0 is encountered. VByte decoding can be a performance bottleneck especially when the unpredictable lengths of the encoded integers cause frequent branch mispredictions. Previous attempts to accelerate VByte decoding using SIMD vector instructions have been disappointing, prodding search engines such as Google to use more complicated but faster-to-decode formats for performance-critical code. Our decoder (MASKED VBYTE) is 2 to 4 times faster than a conventional scalar VByte decoder, making the format once again competitive with regard to speed. Vectorized VByte Decoding has been accepted to the International Symposium on Web Algorithms (iSWAG) on June 2-3, 2015. iSWAG promotes academic and industrial research on all topics related to web algorithms. Large-scale interactive tools To learn more about Imhotep, check out these tech talks and slides: Scaling Decision Trees and Large-Scale Analytics with Imhotep . You can find the source and documentation for Imhotep on GitHub.", "date": "2015-03-09"},
{"website": "Indeed-Engineering", "title": "Memory Mapping with util-mmap", "author": [" by Preetha Appan"], "link": "https://engineering.indeedblog.com/blog/2015/02/memory-mapping-with-util-mmap/", "abstract": "We are excited to highlight the open-source availability of util-mmap , a memory mapping library for Java. It provides an efficient mechanism for accessing large files. Our analytics platform Imhotep (released last year) uses it for managing data access. Why use memory mapping? Our backend services handle large data sets, like LSM trees and Lucene indexes. The util-mmap library provides safe memory mapping of these kinds of large files. It also overcomes known limitations of MappedByteBuffer in the JDK. Memory mapping is the process of bringing part of a file into a virtual memory segment. Applications can then treat the mapped part like primary memory. We use memory mapping in latency-sensitive production applications that have particularly large files. By doing so, we prevent expensive I/O operations. Limitations with MappedByteBuffer The JDK provides MappedByteBuffer in the java.nio package for doing memory mapping. This library has three main problems: Unable to safely unmap The only way to request unmapping with MappedByteBuffer is to call System.gc() . This approach doesn’t guarantee unmapping and is a known bug . You must unmap a memory mapped file before you can delete it. This bug will cause disk space problems when mapping large, frequently-updated files. Unable to map files larger than 2GB MappedByteBuffer uses integers for all indexes. That means you must use multiple buffers to manage files that are larger than 2GB. Managing multiple buffers can lead to complicated, error-prone code. Thread safety ByteBuffer maintains internal state to track the position and limit. Reading using relative methods like get() requires a unique buffer per thread via duplicate() . Example: public class ByteBufferThreadLocal extends ThreadLocal<ByteBuffer>\r\n{\r\n    private ByteBuffer src;\r\n    public ByteBufferThreadLocal(ByteBuffer src)\r\n    {\r\n        src = src;\r\n    }\r\n\r\n    @Override\r\n    protected synchronized ByteBuffer initialValue()\r\n    {\r\n        return src.duplicate();\r\n    }\r\n} Memory mapping with util-mmap util-mmap addresses all of these issues: implements unmapping so that you can delete unused files immediately; uses long pointers, so it is capable of memory mapping files larger than 2GB; works well with our AtomicSharedReference for safe, simple access from multiple threads. Example: memory mapping a large long[] array Use Guava's LittleEndianDataOutputStream to write out a binary file: try (LittleEndianDataOutputStream out =\r\n        new LittleEndianDataOutputStream(new FileOutputStream(filePath))) {\r\n    for (long value : contents) {\r\n        out.writeLong(value);\r\n    }\r\n} Use MMapBuffer to memory map this file: final MMapBuffer buffer = new MMapBuffer(\r\n       filePath,\r\n       FileChannel.MapMode.READ_ONLY,\r\n       ByteOrder.LITTLE_ENDIAN);\r\nfinal LongArray longArray =\r\n    buffer.memory().longArray(0, buffer.memory().length() / 8); Why not use Java serialization? Java manages data in big-endian form . Indeed’s production systems run on Intel processors that are little endian. Also, the actual data for a long array starts at 17 bytes into the file, after the object header. To properly memory map a native Java serialized array, you would have to write code to manage the above mentioned offset correctly. You would also have to flip the bytes around, which is expensive. Writing data in little endian results in more straightforward memory mapping code. Thread Safety For safe access from multiple threads, use AtomicSharedReference . This class wraps the Java object that’s using the memory mapped file. For example: final AtomicSharedReference<LongArray> objRef =\r\n    AtomicSharedReference.create(longArray); The objRef variable is a mutable reference to the underlying SharedReference , a ref-counted object. When using the array, you must call getCopy() and then close the reference. try(final SharedReference<LongArray> myData = objRef.getCopy())  {\r\n    LongArray obj = myData.get();\r\n    // … do something …\r\n} SharedReference keeps track of references and unmaps the file when none are still open. Reloads Use the setQuietly method to replace newer copies of the file. final MyObject newMyObj = reloadMyObjectFromDisk();\r\nobjRef.setQuietly(newMyObj); Close Use closeQuietly upon application shutdown to unmap the file. objRef.closeQuietly(); Get started with util-mmap At Indeed, we use util-mmap in several production services. We are using it to access files that are up to 15 GB and updated every few minutes. If you need to memory map your large files, visit us on GitHub and give util-mmap a try.", "date": "2015-02-25"},
{"website": "Indeed-Engineering", "title": "Making History with Code Festival 2014", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2015/01/making-history-with-code-festival-2014/", "abstract": "What do you get when you combine a coding contest, a music and arts festival, and games? Do those things even belong together? For two days in November 2014, two hundred collegiate coders came together in Tokyo to participate in Code Festival 2014, put on by Recruit and Indeed Tokyo. The event included five distinct coding challenges, as well as fun non-coding activities. After the initial coding challenge - the main round - participants could brush up on their skills with tutoring tailored to that challenge. Not your average coding contest. So, what does a room full of 200 coders look like? Why make history? Organizers wanted to capitalize on love of coding and competition and bring lots of talented coders together to work hard, have fun, make friends, and learn something new. Traditionally, programming contests are limited to a few top competitors, which can discourage those who don't make the cut. Ayaka Matsuo, the event's project lead, decided to break free from that tradition. The structure of the festival allowed many more participants to take advantage of the events. Another history-making facet of the event was Matsuo's event team: 16 new college hires who will join Indeed Tokyo after graduation in April 2015. They provided ideas, helped run the event, and generated a lot of enthusiasm. Expanding on a tradition Indeed and Recruit held coding duels in Fall 2013, December 2013, and February 2014. ( Read about them here .) They were a warm-up to the November 2014 festival in Tokyo, providing a lot of valuable insights into how to expand the types of coding challenges. Code Festival 2014 was so successful that plans are already forming for the next event. Could we host even more competitors? The coding challenges The event included 5 separate coding challenges. Two of the challenges - the main round and the morning programming contest on the second day - were standard programming contests, but the remaining three were not at all traditional. Main round All 200 participants worked through 10 questions (including debugging) in 3 hours. Participants during the main round portion of the contest The participant who solved the most problems in the least amount of time won the round. The top five participants from the main round advanced to the exhibition challenge. Top five winners with Indeed’s Senior VP of Engineering, Doug Gray The winner took home 294,409 yen, an amount that resembles a prime number. Last year's Tokyo coding duel awarded prize amounts that were prime numbers. This year, to mix it up, the organizers chose amounts that are strong pseudo-primes. In Japanese, these numbers are called Kyogi sosu (強擬素数) -- a clever choice, since “Kyogi” can also mean strongly doubted or competition . Check out the prize amounts for the top 20 contestants here . Exhibition In the evening of the first day, the top five finalists from the main round moved to a separate room for the exhibition challenge. Participants were filmed during the exhibition challenge. This room was far from private, however, as live video from each of the five computers was streamed into the main hall, allowing everyone to follow along with the competitors' progress in solving the problem. Audio commentary added to the excitement. Onlookers during the exhibition challenge Were the challengers aware that their every move was being evaluated in the next room? Yes! And being watched only made the competition more lively. Morning programming contest All 200 participants were invited to return the next morning for another programming contest. To change it up, participants joined one of three groups, determined by skill level, and competed individually against others in the same group. AI challenge The AI programming contest required participants to write code that manipulates virtual players in a computer game. Fifty participants who had registered in advance of the festival participated in a preliminary challenge, with the top 16 progressing to the final. Those 16 were divided into four groups of 4, competing tournament style. An exhibition match followed with Naohiro Takahashi (President of AtCoder and a competition programmer) and Colun (a competition programmer) and the first- and second-place winners of the AI challenge. Team relay During the last challenge on day 2, the 200 participants were divided into 20 teams of 10 members each. Each team needed to solve 10 questions, one at a time, within 1 hour 30 minutes. Live video aired, along with commentator play-by-play. While the participant solved the problem, the rest of the team huddled apart from the contest area. If the teammate with the \"baton\" had a question, s/he stepped away from the computer to collaborate with the other teammates. Team huddle during the relay Other festival activities Event organizers sought to ensure that all participants had a chance to learn, play, and connect with their peers. Non-coding activities included calligraphy with code-related content, board games, Taiko-no Tatsujin (drum masters), and DDR (Dance Dance Revolution). Calligraphy coding Participants also had the opportunity to take private lessons with coding competition experts and attend panels with industry professionals covering these topics: The future of programming contests A question: Is the coding competition effective for learning programming? How to create redcoder How to handle increasing speed in coding competitions Want to know more? Gizmodo Japan wrote about the Code Festival . To review the participants' submissions, navigate to the AtCoder standings page and click the magnifying glass beside each user's name. To brush up on your own skills, participate in Top Coder and challenge yourself with past problems from the ACM-ICPC World Finals .", "date": "2015-01-21"},
{"website": "Indeed-Engineering", "title": "Interns Help People Get Jobs", "author": [" by Heather Wood"], "link": "https://engineering.indeedblog.com/blog/2016/02/interns-help-people-get-jobs/", "abstract": "At Indeed, an internship isn’t just a summer job, it’s a full-time experience. We immerse interns into our world of software engineering. Interns experience an exciting company culture and explore all that Austin has to offer. More importantly, we provide challenging projects that allow interns to make their mark at the company (in fact, one of our first interns is now our CTO). Indeed interns on a food truck tour at Micklethwait Craft Meats Our goal is for interns to help people get jobs, have fun, and promote Indeed. To achieve these goals, we start with an engaging, challenging, and well-defined intern project. Defining an intern project The choice of project is crucial to a successful internship. We put a lot of effort into defining projects, because we want all our interns to have a successful summer. An ideal intern project is: Self-contained . If the project depends too much on other projects, those dependencies could slow down the intern's progress. Well-defined . The intern doesn’t have to wait on requirements and can get to work right away. Appropriately scoped . The intern should be able to complete the project in three months. Fun and flexible . The project allows room for the intern’s creativity. Before interns start on their projects, mentors introduce them to Indeed tools and processes. Each intern deploys a small, contained code change by the end of their first week. When the code goes live, per Indeed tradition, the intern rings a gong and everyone applauds. After this training, interns dig into the technical challenges of their projects. Mentors provide continuous advice, introductions, and general support. The art of mentorship Our software engineers who volunteer to be mentors are passionate about helping others and growing their own skills. Mentoring develops skills that engineers can leverage in their everyday work. Mentors learn the value of investing time to support others. They learn when to help out and when to take a step back. And they learn how to inspire and motivate. For example, a mentor customizes the phases of a project to fit an intern's skills and interests. Mentors listen to interns, assess their work, and give them room to be creative. For some engineers, mentorship provides a leadership opportunity that informs their future career decisions. Location, location, location Sometimes, the most enlightening part of the internship isn’t the job or the technology. The Indeed internship program provides a realistic view into the work life of a software engineer. For Indeed interns, this life is set in Austin, a city renowned for live music and food trucks. In Austin, you can find roller derby, chamber music, and everything in between. Indeed's University Recruiting team organizes events so interns can get to know the city and each other. Past events included a party barge on Lake Travis, a Segway tour downtown, and go-kart racing. Interns work hard on their projects, and Indeed works hard to make their summer a blast. Intern Segway tour of downtown Austin Case study: One intern’s success story Tom Werner (University of Iowa) interned at Indeed during the summer of 2015. His project involved building new features that employers could use during the hiring process -- features that let employers view interviewer metrics and check interviewer availability. This project was challenging in several ways. First, Tom was new to the world of front-end web development, but his project required creating an intuitive and simple webapp. Additionally, Tom collaborated with another team to gain access to interviewer data. Finally, he needed to ensure each employer could only access their own data. Implementing this access control required testing and fixing the existing data segmentation infrastructure. Tom was quick to accept these challenges. As a self-directed and fast learner, he completed the core goals of this project in half of the scheduled time. Now what? Tom then had over a month to work on other useful features, based on his interests. He addressed security concerns, and he created admin features for the webapp. Specifically, Tom developed an access control list interface for user permissions. This interface allows designated employer admins to manage their own user permissions. Without this feature, employers would have to contact Indeed to make permission changes. Tom also improved the interface employers use to manage information about their interview funnel. He added more filtering and configuration options, including creating, editing, and copying funnels. Tom's work improved usability for employers as well as internal Indeed users. Demo time Because of Tom's excellent work, his mentor asked him to demo his project to a large internal group that included the broader product team, recruiters, and executives. Everyone was impressed by Tom’s contributions to the product. No limits on success Tom’s talent and dedication made his internship a great success. Indeed gave him opportunities to showcase his abilities, and he went beyond our expectations. Tom delivered on business needs while bringing his own creativity to the project, a balance to which we always aspire. Everybody wins Tom was just one of 27 Austin interns in 2015 who helped Indeed achieve our goals. Another Indeed intern developed a webapp to display international marketing metrics. Xingtong Zhou (University of Michigan), who is now a full-time employee, created the webapp to help marketing analysts identify countries where we need to adjust our marketing investment. With better international data, we are able to make smarter investments in international growth. Interns bring a fresh perspective that inspires innovation in our products and technologies. They contribute to the vibrancy and energy of our work environment. They return to school and share their experiences, helping us build a great campus reputation and attract the best full-time talent. The ultimate goal of our internships, however, is to gain full-time Indeed employees. An internship is a lot like a three-month-long job interview, giving interns the unique opportunity to showcase all their skills. And we have the opportunity to give interns a glimpse into just how exciting it can be to work at Indeed. When interns return to Indeed as full-time employees, they have the added benefit of starting a job they already know is the right fit. The bottom line: when interns have a great experience, Indeed thrives. If you’re interested in an internship at Indeed, please email university-tech-recruiting@indeed.com .", "date": "2016-02-12"},
{"website": "Indeed-Engineering", "title": "Automating Indeed’s Release Process", "author": [" by "], "link": "https://engineering.indeedblog.com/blog/2017/03/automating-release-process/", "abstract": "Indeed’s rapid growth has presented us with many challenges, especially to our release process. Our largely manual process did not scale and became a bottleneck. We decided to develop a custom solution. The lessons we learned in automating our process can be applied to any rapidly growing organization that wants to maintain software quality and developer goodwill. How did we end up here? Our software release process has four main goals: Understand which features are being released Understand cross-product and cross-team dependencies Quickly fix bugs in release candidates Record release details for tracking, analysis, and repeatability Our process ended up looking like this: This process was comprehensive but required a lot of work, containing as many as 40 possible workflow states. To put it in perspective, a software release with 4 new features required over 100 clicks and Git actions . Each new feature added about 13 actions to the process. We identified four primary problems: Release management took a lot of time. It was hard to understand what exactly was in a release. There was a lot of potential for error through so many manual steps. Only senior engineers knew enough to handle a release. We came to a realization: we needed more automation. But wait -- why not just simplify? Of course, rather than automating our process, we could just simplify it. However, our process provided secondary benefits that we did not want to lose: Data . Our process provided us with a lot of data and metrics, which allowed us to make continual improvements. History . Our process allowed us to keep track of what was released and when it was released. Transparency . Our process, while complicated, allowed us to examine each step. Automating our way out We realized that we could automate much of our process and reduce our overhead. To do so, we would need to integrate better with the solutions we already had in place — and be smart about it. Our process uses multiple systems: Atlassian JIRA: issue management and tracking Atlassian Crucible: code reviews Jenkins: release candidate builds and deploys Gitlab: source control Various build and dependency management tools Rather than replace these tools, we decided to create a unified release system that could communicate with each of them. We called this unified release system Control Tower . Main Control Tower screen JIRA issues in release candidate Code review status Git branches and features that depend on code from other features Actual commits in a branch Open JIRA dependencies Commits in upgraded libraries Create a release Unresolved JIRA issue highlighting Commits missing from release Close release button Results of close release Slideshow of Control Tower features Integration with dependency management tools allows release managers (RMs) to track new code coming in through library updates. RMs can quickly assess code interdependencies and see the progress of changes in a release. Finally, when an RM has checked everything, they can trigger a build through Jenkins. The Control Tower main view allows RMs to see details from all the relevant systems. Changes are organized by JIRA issue key, and each change item includes links to Crucible code review information and Git repo locations. By automating, we significantly reduced the amount of human interaction necessary in our release process. In the following image, every grey box represents a manual step that was eliminated. After automating, we reduced the number of required clicks and Git actions from over 100 to fewer than 15 . And new features now add no extra work , instead of requiring 13 extra actions. To learn even more about Control Tower, see our Indeed Engineering tech talk . We talk about Control Tower starting at 32:45. Lessons learned In the process of creating our unified release system, we learned some valuable lessons. Lesson 1: Automate the process you have, not the one you want When we first set out to automate our release process, we did what engineers naturally do in such a situation -- we studied the process to understand it as best as we could before starting. Then, we did what engineers also naturally do -- we tried to improve it. While it seemed obvious to “fix” the process while we were automating it, we learned that a tested, working process -- even one with problems -- is preferable to an untested one, no matter how slick. Our initial attempts at automation met with resistance because developers were unfamiliar with the new way. Lesson 2: Automation can mean more than you think When most people think of “automating” a process, they assume it means removing decisions from human actors -- “set it and forget it.” But sometimes you can’t remove human interaction from a process. It might be too difficult technically, or you might want a human eye on a process to assure a correct outcome. Even in these situations, automation can come into play. Sometimes automation means collecting and displaying data to help humans make decisions faster. We found that, even when we needed a human to make a choice, we were able to provide better data to help them make a more informed choice. Deciding on the proper balance between human and machine action is key to automating. We see future opportunities for improvement by applying machine learning techniques to help humans make decisions even faster. Lesson 3: Transparency, transparency, transparency Engineers might not like inefficiency, but they also don’t like mystery. We wanted to avoid a “black box” process that does everything without giving insight as to how and why. We provide abundant transparency through logging and messaging whenever we can. Allowing developers to examine what the process had done -- and why -- helped them to trust and adopt the automation solution. Logging also helps should anything go wrong. Where do we go from here? Even with our new system in place, we know that we can improve it. We are already working behind-the-scenes on the next steps. We are developing algorithms that can monitor issue statuses, completed code reviews, build/test statuses, and other external factors. We can develop systems capable of programmatically understanding when a feature is ready for release. We can then automatically make the proper merge requests and set the release process in motion. This further reduces the time between creating and shipping a feature. We can use machine learning techniques to take in vast amounts of data for use in our decision-making process. This can point out risky deploys and let us know if we need to spend extra effort testing or if we can deploy with minimal oversight. Our release management system is an important step toward increasing our software output while maintaining the quality our customers expect. This system is a step, not the final goal. By continually improving our process, by learning as we go, we work toward our ultimate goal -- helping even more people get jobs.", "date": "2017-03-07"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Be Ambitious, Stay Humble", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/12/indeedeng-culture-be-ambitious-stay-humble/", "abstract": "In my previous posts on Indeed’s engineering culture, I wrote about keeping teams independent , making better mistakes , dealing with consolidation debt , encouraging autonomy and initiative , and the importance of not stack ranking individuals . These are my closing thoughts on how we work here at Indeed. Perhaps one of Indeed’s traits that has struck me the most is how individuals, including top engineers and managers, have stayed humble, although they have built this company from the ground up. Bragging and boasting are not behaviors I have seen at Indeed. As I met my colleagues, it seems like being right or wrong is overrated. Instead, we are all focused on doing the right thing. I was surprised how veterans had time and patience. At first I wondered what I did to deserve it. They were curious to hear my perspectives although I had not demonstrated anything yet. It took me some time to understand why. Then I figured it out. Being data-driven and expecting initiative means there is always a practical and immediate way to settle a burning argument. If you really believe that feature X is what the business needs, you don’t need to convince an army of managers and directors. You don't need to be resentful for failing to do so, or not even trying. Your credentials (almost) do not matter. The only thing you need to do is to design the experiment that will prove or disprove your theory, and ship it. And if you are not doing so, why are you talking? Editor’s note: This post concludes our 5-year anniversary series highlighting some of the most important aspects of Indeed’s engineering culture. We want to thank James Dingle for sharing his perspective and reminding us why we love coming to work here every day. Want to join us? Learn more at http://indeed.jobs . IndeedEng Culture: Be Ambitious, Stay Humble cross-posted on Medium .", "date": "2017-12-01"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Don’t Stack Rank", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/11/indeedeng-culture-dont-stack-rank/", "abstract": "In this post, I stand unabashedly against stack ranking. Evaluate how employees perform, not how they compare. Is this really controversial? If you assign to teams or divisions a fixed budget for bonuses, promotions or stocks, you create de facto a zero-sum game. You need losers to have winners. From that point, you can make lengthy speeches about the importance of being on a team, you will see people nod at you silently, but immediately after your all-hands meeting, they will go on competing with each other. \"But competition is good, right?\" Not when it turns into sabotaging each other’s work, by action or inaction, consciously or not. Teams accomplish much more when they combine their efforts. It won’t happen if peers do not have a genuine interest in helping each other, but have more interest in watching each other fail. \"Everything in the company is budgeted, why not compensation?\" Every budget item has unknowns, including compensation. In a small company, the absolute numbers do not really matter. As the company gets bigger, statistics kick in, and compensation becomes predictable enough without requiring comparing individuals. \"If there is no fixed budget, managers will be too generous.\" After all, it is not their money, so why not give it away. But that would quickly lower the productivity and incentive. At Indeed, we don’t stack rank or fit to an expected distribution. We have a process that aligns performance reviews across teams and offices, so the company keeps the same definition of what “good”, “better”, “best”, and “not enough” means. No manager decides alone an individual’s evaluation, and there is no competition between teams. This review process is probably the fairest I have ever seen. What’s next? In the next post , I conclude my series on Indeed’s culture with thoughts on the most striking trait of individuals at Indeed. IndeedEng Culture: Don't Stack Rank cross-posted on Medium .", "date": "2017-11-30"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Encourage Autonomy, Measure Impact", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/11/indeedeng-culture-encourage-autonomy-measure-impact/", "abstract": "In the previous post , I wrote about innovation and consolidation and the importance of balancing the two. In this post, I write about how Indeed fosters initiative through autonomy. Most compensation systems reward for business impact Engineers get better bonuses when their impact on the business is greater. Fair, right? No, it is not. Engineers are not salespeople. There are probably not two of them in your company who are executing the same tasks (and when it happens, they try to write a library or something, because they just can’t help it...). Rewarding by impact cannot be a fair system, can it? Rewarding by impact cannot be a fair system if, in reality, individuals are assigned tasks. If you still reward for business impact, you will create and fuel a system where the well-intended will do their work and hope for the best, while the less-so-well-intended can train their skills into political back-stabbing games, trying to grab the juiciest projects, as they already have figured out that is how you have defined they will move up the ladder. Guess who you’ll find at the top in a few years, and what it does to your enterprise culture. The other option, which Indeed fosters, is to let individuals pick their own tasks. They can’t complain about their project, the product or their boss, because they chose what they wanted to work on. You would think that this does not make sense at all. A company is not a democracy or an entertainment park. If we let people decide their tasks, they’ll pick the easiest or safest, not the most important or most urgent, and most importantly it’s going to be completely disorganized and the product won’t make sense. Employees in a company, as it turns out, are people with common sense. They can understand that there are times for high-business impact work, and times for washing dishes. They need complex tasks and great challenges, but they also need to rest from them and execute less challenging, yet still productive work. I observed that giving more autonomy does not change much what eventually gets done; but it greatly changes the energy and passion put to the task. At Indeed, we strive to measure business impact and recognize initiative as key components of the quarterly performance review process. By doing so, we give our engineers the right incentives to balance their work in ways that are best for Indeed’s mission: to help people get jobs. What’s next? In the next post in the series, I’ll describe why I think stack ranking is a bad idea. IndeedEng Culture: Encourage Autonomy, Measure Impact cross-posted on Medium .", "date": "2017-11-29"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Balance Innovation and Consolidation", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/11/indeedeng-culture-balance-innovation-consolidation/", "abstract": "In a previous post, I described the importance of keeping teams independent . Is there a downside to this approach? One drawback of fostering independent teams is that the product can look inconsistent or even somewhat messy to the end-user. It does not necessarily mean the product or feature will be of poor quality, but it might not be perfect. The fonts will not be the same on all web pages. A credit card that we saved in one section of the website cannot be reused in another section. Keeping teams independent so they can innovate more will generate a consolidation debt. Consistency is a key component of engineering and user experience quality; but requiring consistency at the earliest stages might be just what kills a good idea. Allowing some consolidation debt can be a necessary evil. Managing consolidation debt Consolidation debt is a type of technical debt. Consolidation debt expresses itself as redundancies and inconsistencies across your products. It is why you have three logging libraries and two payment systems. It is also a source of misunderstanding between what the business thinks the company can do, and what the company is really prepared for and can leverage. Similar to technical debt, business rarely develops without accruing consolidation debt; so there is nothing to worry about. But let it grow too much, and it will attach itself to and hide your product and business, much like the ivy will mask the trunk. \"We have too many of X! Let’s build a single Super-X so all our services can use it!\" You probably have seen this many times. Sometimes it succeeds brilliantly, sometimes it fails miserably, some other times the end result is moderate and not much better than the original situation. Undoubtedly, your teams have gained a little in delegation, but they lost a lot of their autonomy because now, any new feature they want to ship has to involve another team. The team that has resolved its consolidation debt is now worse off. After some time, it will have surrendered all of its innovative power, and you can’t expect it to move the business forward, but only to support it. A consolidation project has to be a brilliant success, and nothing short of that. The benefit in consolidation has to vastly overcome the sacrifice in autonomy. Succeeding in a consolidation debt project Consolidation can fail or be insufficient when in turn, it gets embedded into too many objectives, and the team created to solve the problem is entangled with commitments from day one. Because the company wants to solve this problem, the company forces other teams to adopt the product of the infrastructure team, whether this makes sense or not. An infrastructure team has to be created in the same spirit of independence as a product team: it has to get its own success metric, conquer its own customers, face honest criticism and competition with the ad-hoc solutions it is trying to replace. At Indeed, our Engineering Capabilities group focuses on building internal tools and services that reduce redundancy, increase velocity, and maintain quality. We address these problems with the same data- and metrics-driven approaches that we apply when developing our public-facing services. What’s next? In the next post , I ask the question: What happens if you let engineers decide what tasks they’ll work on? IndeedEng Culture: Balance Innovation and Consolidation cross-posted on Medium .", "date": "2017-11-28"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Make Better Mistakes", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/11/indeedeng-culture-make-better-mistakes/", "abstract": "In the previous post , I described the importance of keeping teams independent. In this post, I talk about how Indeed encourages teams to turn mistakes into learning. In most organizations, mistakes are never part of the plan. Plans are always designed to succeed. Plans never contain a chapter for “what if we are totally wrong about this or that.” Most of the time, by sheer force of habit; and sometimes, out of someone’s ego. This idea is so brilliant that it cannot fail. Let’s rally everybody around it and celebrate how awesome it is. If it fails, let’s blame the project owner; it is certainly their fault. Let’s blame the engineers; they were not smart enough to understand the requirements or solve their technical problems. Let’s blame marketing or sales; these people had the product that the competition wanted and they just blew it. Look back at how many mistakes NASA made before sending someone on the moon. Look at how many rockets SpaceX lost before its first successful return. These organizations succeed not because they don’t make mistakes, but because they have defined a process on how to make them and learn from them. At Indeed, we turn making mistakes into a process, so that they become learnings. Identify the assumptions. Instead of residing in an echo chamber, discuss your idea with those who think it’s doomed to failure. Do not try to convince each other; it rarely happens. Instead, try to agree on the smallest experiment possible, which will scientifically demonstrate where the truth lies. Use a test framework. Comparing before and after is usually what people do, but it has its flaws: what about external factors? How can we run several experiments at the same time? We use A/B testing for everything we ship (except high priority bugs). Take single steps as much as possible. Change one thing at a time. Avoid having multiple assumptions or variables in the same experiment. Do not blame. The only true mistake is to not have learned anything. When something does not work, draw the conclusions and move on to the next experiment. If you fail fast enough, the mistakes are not personal setbacks for careers. What’s next? In the next post , I describe how to manage consolidation debt, a type of technical debt that expresses itself as redundancies and inconsistencies across your products. IndeedEng Culture: Make Better Mistakes cross-posted on Medium .", "date": "2017-11-27"},
{"website": "Indeed-Engineering", "title": "IndeedEng Culture: Keep Teams Independent", "author": [" by James Dingle"], "link": "https://engineering.indeedblog.com/blog/2017/11/indeedeng-culture-independent-teams/", "abstract": "Editor’s note: Five years ago ( see our first post ) we launched this blog to talk about the technology we work on every day. Since then, we’ve grown more than ten times and continue to build the software that makes Indeed the #1 job site worldwide. To celebrate, we’re proud to host a week-long series of posts by Indeed Engineering Manager James Dingle. These posts highlight some core aspects of what makes Indeed Engineering a great place to work. Over 5 years of IndeedEng blog content, many key words frequently appear – such as \"job,\" \"data,\" \"index,\" \"new,\" and \"test.\" During my year-and-a-half tenure at Indeed, I have discovered a new way to accomplish business goals with software. These principles and elements of Indeed’s culture are quite surprising; if you didn’t see them in action, you might believe any company trying them would be doomed to failure. Quite the opposite is true. While some of these principles are deliberate, some of them happened organically. This series describes my perspective on the principles of our engineering culture at Indeed. Step One: Keep teams independent Usually, in large organizations, the company defines a grand vision and then breaks it down into products, stories, features, and tasks. These tasks are transverse, and teams commit to other teams on deliverable dates. When too many teams depend on each other, hitting dates becomes more important than the actual value delivered. As the deadline approaches, teams start to cut out pieces of their planned work, until the product attractiveness becomes lukewarm, its features unbalanced, and its user experience confusing. The enterprise culture suffers, too. When teams slip on their due dates, the org can drift to finger-pointing and defensive management. In most software products, these dates are not bound to any external event, but some managers still behave as if the project is date-driven. Engineering new products means a great deal of uncertainty about the difficulties ahead and how much time it will take to overcome them. Beyond a certain size, adding a buffer to estimates does not guarantee success. This is why smaller, independent teams are so important. Independent teams (and uncertain dates) may frighten executives who like big milestones on a calendar. They might believe milestones are a source of clarity, motivation, and accountability, internally and externally. They might think that if the product turns into a flow of sand of features, they will not control it. They might think that if there are no dates, people will become lazy, wander eternally and never ship anything. But in reality, employees have multiple reasons to stay realistic. Independent teams are beneficial for the team and for managers. For the team, independence ensures: less friction more identification with the product by contributors, which in turn triggers more motivation and initiative most decisions by the team, where context is better understood For executive management, an independent team: is accountable for the true business impact it delivers diminishes the risk of a global impact on the company’s strategy if the team fails or is delayed can take more risks What you need to keep teams independent: An efficient deployment pipeline so teams can push or roll back their bits as they please. Goals and metrics only they can move. A test framework to evaluate the impact of new features and experiments. An efficient data collection and analysis pipeline. Indeed has focused on developing the four abilities above while growing rapidly. Goals come top-down, and innovation rises bottom-up. What’s next? In the next post , I’ll explore the importance of making better mistakes. IndeedEng Culture: Keep Teams Independent cross-posted on Medium .", "date": "2017-11-26"},
{"website": "Indeed-Engineering", "title": "Transitioning from Academia to Industry", "author": [" by Robyn Rap"], "link": "https://engineering.indeedblog.com/blog/2017/12/data-scientist-academia-industry-transition/", "abstract": "Perspectives from Indeed’s Data Scientists I still remember the moment I told my advisor that I was considering leaving academia. The stress. The fear. Saying the words, “I don’t think I want this for myself” out loud. And afterwards, the relief. At the time, I was juggling work on my dissertation proposal, multiple publications, teaching responsibilities, and a research assistantship. Meanwhile, I had begun researching data science, a field where I could use the skills I had learned in my doctoral program, but in a setting that better suited how I wanted to live and work. So, I made the decision to leave academia, pivot my skillset, and look for data science work in the private sector. Of course, my experience was not unique. Tenure is often touted as the ultimate form of job security, but the number of tenure track jobs available has declined while the number of graduating Ph.D.’s has increased . Furthermore, pay has not risen with inflation . Many of the data scientists I interviewed who had left academia said that “the road to professorship was long and uncertain” and often full of “soul-crushing” levels of anxiety. By contrast, as companies look to leverage their vast stores of data, job posts on Indeed for data scientist positions have exploded since 2013 . Data science jobs on Indeed.com have quadrupled over the last four years. We looked at a 7-day rolling mean of all Indeed job posts that featured “data science” or “data scientist” in the title across the world from January 1, 2014 to November 16, 2017. In that time frame, those posts went from slightly more than .02% of all job posts to over .08%. The data was pulled using Indeed's open source analytics platform Imhotep . Due to the sequestered nature of the academy, those who might want to leave academia often don’t know what things are like in “the real world.” Since I left the academy two years ago, some of the more common questions I receive are: “Why did you leave academia?” “What should I do to make myself more hireable?” And the most existential question of all: “Will everything be OK if I leave?” To answer these questions, I enlisted the help of eleven other Data Scientists and Product Scientists here at Indeed who left academia. For simplicity, I refer to them as “data scientists” throughout this post. The data scientists I interviewed come from a variety of backgrounds, some from Liberal Arts and others from STEM. For some, Indeed was their first job after leaving academia, while others have been in industry for nearly 10 years. They left at various points in their academic careers, some while still ABD (“all but dissertation”), others while serving as assistant professors. Some went on the academic job market first, others “didn’t even bother,” still others never intended to. Some of our data scientists discussed that a major life event, like getting married or having a child, spurred them to rethink staying in academia. It’s my hope that this blog post offers some guidance and reassurance for academics considering making the leap to industry. Why move to industry? Simply put, moving to industry allows for more freedom for how you spend your time, energy, and money; where you’re able to live; and what kinds of statistical methods you can use. Suddenly, you have more autonomy to decide where to live or work. For one data scientist, moving to industry after a series of post-doctoral fellowships allowed for “a more predictable career path where there were plenty of job opportunities.” One of the biggest adjustments former academics made when moving to industry was how much more free time they had and how much more relaxed they felt. Our data scientists described “getting weekends back” and being able to “go home at 5pm and not needing to do any more work.” Some of the former academics discovered new hobbies with their newfound time and money, like cooking, cycling, photography, and sports analytics. Working in the private sector also provides another form of security: a better salary. One data scientist commented that her “pay doubled and the amount of work required halved” and another about “how good it feels to not be struggling financially!” The average data scientist salary on Indeed is well above the United States median income and the median salary for academics in postdocs or professor positions . I vividly remember the first time I bought a new pair of shoes on my new salary and realized that I did not have to worry about the effects on my budget. Apart from the positive effects on their personal lives, data scientists who left academia also noted benefits for their work. Some noted that they were “afforded more methodological freedom outside of the academy,” due to the amount of data readily available and fewer concerns about whether their work would be publishable. Nearly all of the data scientists I spoke with mentioned how much they enjoyed “how much impact you can have, and how quickly.” Speed and collaboration in the private sector Non-academic jobs do have some notable differences that can surprise newcomers, particularly around speed and collaboration. Nearly all of those I interviewed commented on the shorter project cycles and faster pace in the private sector, adding that “there is a constant focus on moving fast and iterating.” The tendency for academics to “spend forever on a project until it is exactly right is not a habit that will transfer well.” Another data scientist noted that “in academia you stay with one topic or research area for many years. In industry you need to be OK with working on something new every quarter or so.” Perhaps the biggest difference is that you no longer work in isolation. “Academia was very isolating and highly values individual contributions you could claim as entirely your own,” discussed one data scientist. “However, in industry the most successful folks are the ones that collaborate well.” The opportunity to collaborate can be especially meaningful for academics transitioning to industry. “If you’re struggling on something,” one data scientist mentioned, “you don’t have to do it alone.” Teams also expect you to check in with them periodically and not “disappear to work on a project for weeks or months at a time.” While all of the data scientists I spoke with had experience coding for their academic research, it was usually done “in an environment where the only person I would affect was myself. Having to share a codebase with 5 other people and not trip all over each other was a skill I had to learn quickly.” Transferable skills between research and industry It’s sometimes hard to remember in the hustle of research, but academics have a multitude of transferable skills beyond expertise in their field. In a way, “academics are trained to be startups with a single employee. They need to gather information, find funding and support, allocate scarce resources, and acquire the technical and non-technical skills needed to get off the ground,” one data scientist noted. For some, these transferable skills include coding, statistics, and model building. For others, skills can include a variety of non-technical abilities that are so familiar to academics that they overlook their value to the private sector. As one example, because of all their independent work, academics usually pick up valuable time and project management skills. The importance of being able to do independent research and teach oneself cannot be overstated. “Lots of data science questions are complicated and no one has all the required skills,” said one data scientist. “It’s therefore super important to teach yourself.” Academics also learn valuable communication skills. One data scientist said, “I had so much practice giving talks to an audience of experts trying to pick holes in my argument, that presenting results to clients was a breeze.” Context switching between teaching an intro class and presenting to fellow experts also “provides plenty of practice in pitching your presentation to the appropriate audience.” After writing theses, dissertations, books, and publications, it’s a lot less daunting to write documentation, project updates, and blog posts. Finally, the value of critical thinking is immense. Academics are trained to be skeptical and to take on “big, complex, and messy problems.” They’re well-equipped to “build a project in an iron-clad way and understand all of the caveats of [their] work.” How to get hired in industry The data scientists I spoke with took several different approaches to redefining themselves and their skillset. These included reading books, attending onsite and online courses, working on internships over the summer, and meeting with an on-campus career counselor. Many universities offer these resources for a small fee or even for free. Several former academics suggested starting a personal blog to write about side projects while learning data science skills. Organizations such as DataKind and Data for Democracy offer volunteer opportunities where budding data scientists can hone their skills. A couple of them “asked around for data projects to do pro bono so I could practice my skills and have some projects to share in my portfolio on my website.” At least two of the eleven data scientists I talked to attributed getting their first industry job in part to these blogs. Some data scientists who went into industry “deliberately switched” from languages like Matlab or Stata to languages like Python and R for their research projects while still in academia. Others tried to “guide [their] academic work toward slightly more practical technological aims.” One of the most common data skills not usually learned in academia is SQL. Luckily, SQL is relatively easy to learn with abundant resources in print and online. Our data scientists recommend SQL in 10 Minutes by Ben Forta and Learning SQL by Alan Beaulieu . Finding other academics interested in leaving academia can also be helpful. Some data scientists I spoke with formed study groups, and even practiced mock whiteboarding interview questions together. Others attended local meetups or conducted informational interviews to learn more about the field of data science. Indeed is looking for data scientists! Indeed’s Data and Product Science teams look for people who know statistics and how to program. Our Data Science team looks for “full-stack” engineers with strong machine learning backgrounds. Our Product Science team emphasizes prioritizing and solving business problems. Equally important to both teams are soft skills: curiosity; a desire to grow, learn, and improve; being humble and self-aware of one’s limitations; and “a true passion for helping people get jobs.” Interested in applying? Indeed is hiring Data Scientists and Product Scientists at all levels of seniority and experience, and we love former academics! Find all of our open job listings at this link . Methodology This was a qualitative and self-reported study with an admittedly small sample size. We’d love to hear if this was representative (or not) of your experiences. Did you leave academia to pursue a data science role? How has your experience been similar or different? Leave us a comment below! Special thanks to Christo du Plessis, John Jardel, Evan Koh, Eric Lawrence, Chris Lindner, Donal McMahon, Elias Ponvert, Ke Sang, Annette Taberner-Miller and Wenzhao Xu for taking the time to share their thoughts about transitioning from academia to industry. Trey Causey gave fabulous feedback on an early draft of this post, and James Beach and Leah Pasch helped edit the final draft. Thanks to all academic advisors out there who are understanding and supportive when their students leave academia (thank you, Pam!). Transitioning from Academia to Industry cross-posted on Medium .", "date": "2017-12-19"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Sponsoring the Python Software Foundation", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2018/05/indeed-sponsors-python-software-foundation/", "abstract": "At Indeed, we’re committed to taking a more active role in the open source community. Earlier this year, we joined the Cloud Native Computing Foundation . This week, we are pleased to announce that Indeed is sponsoring the Python Software Foundation . We write lots of Python code at Indeed — it's one of our major languages — so we benefit from a thriving Python ecosystem. Indeed is excited to join other industry leaders who support the Python Foundation . We believe Indeed has a lot to bring to the Python community, including participation, promotion, and sponsorships. Supporting the Python Software Foundation is a great place for us to start. We recognize that great open source software relies on engagement at all levels, and we are looking forward to becoming a steadfast supporter of the Python community. As we continue to take a more active role in the open source community, Indeed will seek out additional partnerships, sponsorships, and memberships. For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page . Open Source at Indeed: Sponsoring the Python Software Foundation cross-posted on Medium .", "date": "2018-05-09"},
{"website": "Indeed-Engineering", "title": "Improving Security with OAudit Toolbox", "author": [" by Ashley Graves"], "link": "https://engineering.indeedblog.com/blog/2018/04/oaudit-toolbox/", "abstract": "Trust is a big part of why Indeed continues to be the world’s number one job site. Users trust us to keep their information safe. We’ve always taken our responsibilities seriously, and as abuse of personal data continues to dominate the news, we cannot afford to lose diligence. That’s why we need to keep our own data safe -- not just to protect Indeed employees and corporate data, but also to protect the data that our users entrust us with. I work on Indeed’s Information Security team and in early 2017 started to address shortcomings in our Google GSuite implementation. Our solution -- the OAudit Toolbox -- is now available as an open source tool that you can use as well. The problem: Third party apps can be risky A major area of risk for Indeed’s GSuite implementation is integrating third party apps. GSuite users can grant applications access to their account -- anything from basic account info to full read/write access to Gmail. The OAuth Scopes presented in the authorization prompt control access to Google resources. For businesses running GSuite, this presents a number of issues: User education. Users may unknowingly grant access, or they might not understand the privacy or security implications of their choices. Data sharing. Authorizing apps for certain scopes allows third parties access to sensitive data. Your business might not have the proper data sharing contracts in place for this kind of access. Data exfiltration. Malicious applications can use this OAuth flow to effectively phish and exfiltrate data from Google accounts. Limited tooling. At the time of my investigation, Google’s options for restricting apps were limited: the API only allowed for reactive blacklisting. The relatively new feature allowing for whitelisting of connected apps requires that you actually have a whitelist of apps. Policy culture. Many companies allow overly permissive scope access. Implementing Google’s new whitelist functionality could require your security team to review, approve, and whitelist a giant backlog of apps -- not to mention fielding requests from employees who feel their productivity depends on Sketchy Mail Tracker Pro™ . It wasn’t long before we had a real-world attack that highlighted these issues and helped us define our own solution. The proof: A massive phishing attack In May 2017, a massive phishing attack masquerading as a Google Docs invitation hit Google Apps users all over the world. Users received an email, apparently sent from one of their contacts, requesting that they view a shared document. If they worked for a business using GSuite, this wasn’t an unusual request. What was unusual was this fake “Google Docs” requesting permission to access the recipient’s email and contacts. If users granted this access, the fake “Google Docs” app then sent the same phishing email onward to the victim’s contacts, using the victim’s account and masquerading as them. As part of Indeed’s Security team, I was on the front lines of our company’s response to this attack. After several hours of panic and revocation of OAuth tokens, I felt oddly inspired. I wanted to fill in the gaps in GSuite’s available tooling, detect attacks like this one sooner, and find a way to educate users about the dangers of authorizing third party apps. Along with my coworker Dustin Decker, I started work on a set of tools that might get us closer to a perfect solution. Our solution: OAudit Toolbox OAudit Toolbox is a set of tools that detects third party app integrations and notifies users of their danger and scope. How OAudit Toolbox works OAudit Toolbox contains two major components: Oaudit-collector indexes authorization events from the Google Admin API into Elasticsearch Oaudit-notifier sends notifications with educational information about Oauth scopes and contains whitelisting/blacklisting logic Blacklisting allows us to revoke access to a list of known bad apps in near-real time, including malicious apps and apps in violation of corporate policy. Access is only revoked to apps authorized after the app is defined in the blacklist. Whitelisting allows us to stop notifications for trusted apps so that users don’t suffer alert fatigue. Oaudit-collector fetches authorization token event data using the Google Admin SDK API. Oaudit-collector indexes fetched data in Elasticsearch. Oaudit-notifier checks to see whether the authorized application is whitelisted, blacklisted, or unknown. If the app is whitelisted , a notification is not sent. This is typically used for apps that have passed security review, and in the case of third party apps, have the appropriate data sharing agreements in place (if applicable). If the app is blacklisted , a notification is sent to the user that the app is blacklisted and access has been revoked. This is used for malicious apps (such as the ones seen in the 2017 phishing attack) and apps that go against corporate policy. Apps that are neither blacklisted nor whitelisted trigger a notification to the user informing them of the potential risk of authorizing untrusted applications and how to revoke unwanted access. How OAudit Toolbox helps OAuth Toolbox solves, or at least mitigates, each of the issues I identified with using GSuite. Solution: User education There are a million reasons why users might authorize third party apps: A manager told them to They don’t understand the authorization prompt They haven’t heard of all the fun acronyms like DPA and MNDA that help us more securely share data with third parties And many more! With OAudit enabled, users receive an easy-to-understand visualization of the risks associated with each third party app, including user-friendly descriptions of the risks. Each scope is assigned a score based on the risk of sharing sensitive data, and highlighted with an associated color. After enabling this feature, we saw a significant increase in questions from our users about whether apps are safe to use. We received more requests for our application security team to review third party apps. We were also contacted by teams outside of our engineering organization. Non technical users had previously felt uneasy about using some third party tools but lacked the technical or security context to explain why. Solution: Data sharing Users add tools to Google Apps to do things like improve spreadsheet visualizations, enable mail marketing campaigns, or send themselves Google Form results. To the typical user, it’s not obvious that these integrations exist on a third party’s server rather than within GSuite itself. Some users assume that Google thoroughly vets each app. Other users don’t realize that once this data resides on third party equipment, they have no control over it beyond contractual agreements regarding further usage and sharing. For companies that must comply with GDPR , this issue goes beyond security and becomes regulatory. Using the OAudit Toolbox has helped us socialize the concept of high-risk data sharing. At the same time, we have been able to work with our privacy and contracts teams to get the appropriate agreements in place where needed and revoke access to those apps that don’t pass our assessments. Retroactively revoking access to unapproved (but non-malicious) apps using the blacklist has been reasonably effective, as the expected functionality of these apps does not involve immediate data exfiltration. Solution: Data exfiltration While there are legitimately useful third party apps, there are also malicious apps masquerading as useful tools, such as the “Google Docs” app involved in the phishing attack of 2017. You can always revoke a malicious app’s access, but doing so is minimally helpful because these apps are likely to exfiltrate data and/or abuse your account as soon as possible. The reliability of this method also depends on the timeliness of token logs -- which, during the Google Docs phishing attack, were as far as 12 hours behind. More recently, token activity lag time is between 1-10 minutes, though Google claims this can be as much as a few hours . Since the OAuth Toolbox also sends data to Elasticsearch, we recommend setting up ElastAlert or Watcher to detect never-before-seen apps being authorized or a spike in a single app being authorized in a short period of time. By proactively warning users about dangers, OAudit Toolbox is more like an early alerting system for malicious apps, or IDS , and less like a proactive blocker, such as an IPS . Solution: Available tooling When development on OAuth Toolkit started, the ability to block app access was scattered across GSuite. A Google Apps administrator could block Marketplace Apps, Drive API, and Chrome extensions. That would have been too heavy-handed for us to successfully implement. Administratively, it was difficult to manage with minimal ROI since the solution was incomplete. Google now allows you to block OAuth access to Google Apps such as Drive, GMail, and Contacts using the Google Admin Security panel. Optionally, you can block only “high risk” scopes, but there is no documentation as to which scopes are considered high risk. There is also a whitelist available to allow use of trusted apps. This is a useful addition if and when your team is ready and able to: Block all apps in use Know what you need to whitelist Have a workflow in place for approving new applications Have application security resources for reviewing those apps We were glad to see Google take these steps, but OAudit Toolkit provided us a solution that was easier to implement and less disruptive to workflow. Solution: Policy culture A major hurdle for implementing this tool and the subsequent review process wasn’t technical, but human. Going from an open, “bring your own app” culture to a more restrictive, seemingly bureaucratic process is a struggle -- especially when transparency is a keystone of company culture. We found a few practices to be helpful: Use tools (such as OAudit Toolbox), training, tech talks, or internal blog posts to introduce users to the risk posed by third party apps. Instead of adding a pile of legalese to your Acceptable Use policy, keep any actions users need to perform simple and available on an easily accessed page: the intranet home page, a wiki page, or an IT Support landing page. Bucket frequently used, high-risk tools into categories such as “Mail Merge” or “Spreadsheet automation.” Doing so makes it easier to rip-and-replace them with a single, approved app rather than evaluate 50 unique apps. Accept that you might not solve the problem overnight. Even if you can only mitigate the problem by allowing continued access to some apps and approving or restricting new apps, that still puts you in a better place than you were before. Get started with OAudit Toolbox We’ve made OAudit Toolbox open source and available so you can benefit from our experience. By integrating it with your company’s Google suite, you can take similar steps to improve organizational education and, hopefully, better resist the next phishing attack. Let’s stay ahead of these malicious actors and keep the web -- or at least our little corners of it -- safer. Improving Security with OAudit Toolbox cross-posted on Medium .", "date": "2018-04-19"},
{"website": "Indeed-Engineering", "title": "Indeed at FOSSASIA 2018", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2018/03/indeed-at-fossasia-2018/", "abstract": "Indeed is a proud sponsor of the FOSSASIA Open Tech Summit, from March 22-25 in Singapore. FOSSASIA is Asia's premier developer event and covers tracks from AI, Cloud, Blockchain Science Tech, Web, and more. On March 24, Indeed speakers will present the following topics. We encourage you to attend. How to Overcome Obstacles and Take Control of Your Career in Tech Speaker: Doug Gray Doug Gray is Senior Vice President of Engineering at Indeed. As a member of Indeed’s senior leadership team, Doug defines the organizational and development processes for the engineering team. His areas of expertise include product revitalization, process change and team development. Previously, Doug was a senior leader at Versata Software. Prior to that, he held a number of senior positions at Trilogy, where he worked for over 12 years. He holds a Bachelor of Science degree in Computer Science from Stanford University. Finding the perfect job can be difficult. Figuring out how to stand out from the competition and then navigating human biases, archaic hiring processes and other challenges can make landing that dream job seem like a long shot. ​ ​ Learn how you can take control of your tech career path. Gain key insights from new research on what drives top tech talent’s career decisions​ while also learning how technology can help you overcome obstacles that may occur during the hiring process so that you can be in control of finding your dream job. Indeed MPH: Fast and Compact Immutable Key-Value Stores Speaker: Alex Shinn Alex Shinn is an engineer working on Indeed's job recommendation system. A former Google search engineer, he is a generalist with experience in natural language processing, building scalable architectures, and various AI technologies from expert systems to modern machine learning. In his free time, he writes Scheme compilers and libraries and contributes to the Scheme standardization process. When you need to scale an application with a lot of data, how do you decide on a storage solution? How can you both safely store and efficiently interact with large data sets? This usually boils down to a choice between SQL or NoSQL -- but what if there was a third option? In this session, developer Alex Shinn will discuss Indeed’s MPH-Table: an open source storage solution that uses minimal perfect hash functions to generate a fast and compact key/value store. Alex will discuss how we use MPH-Table at Indeed, what situations are ideal for it, additional optimizations it allows, and how it compares to alternate solutions. Attendees should enter with a fundamental understanding of existing scalable storage solutions, and will leave with a basic understanding of MPH-Table, when they might want to use it, and how other solutions compare. Find the full schedule for FOSSASIA, and more information about Doug and Alex's presentations, on the FOSSASIA Summit 2018 site . Indeed at FOSSASIA 2018 cross-posted on Medium .", "date": "2018-03-21"},
{"website": "Indeed-Engineering", "title": "Indeed Expands its Commitment to Open Source", "author": [" by Doug Gray"], "link": "https://engineering.indeedblog.com/blog/2018/03/commitment-to-open-source/", "abstract": "At Indeed, we’re committed to an active role in open source communities. We’re proud to announce that we’ve joined the Cloud Native Computing Foundation (CNCF), an open source software foundation dedicated to making cloud-native computing universal and sustainable. The CNCF, part of The Linux Foundation , is a vendor-neutral home for fast-growing projects. It promotes collaboration among the industry’s top developers, vendors, and end users. CNCF members include many leading public cloud providers and private cloud companies. Deciding to join the CNCF was easy for us. We rely on open source technologies like OpenTracing to build and deliver best-of-class products. We believe we can serve job seekers and employers best by ensuring that open source software projects continue to grow. Open source is now the industry model for practical software development. No longer relegated to Linux companies like Red Hat, the commercial success of open source has accelerated thanks to adoption by large technology companies such as IBM and Oracle. More recently, companies like Walmart and Verizon rely on open source programs and host their own open source projects. Microsoft has also become a major open source contributor. Our open source projects include Proctor , an A/B testing framework that enables data-driven product design; LSM Tree , a fast key-value store for high-volume random access reads and writes; and our newest release, MPH , an immutable key-value store with efficient space utilization and fast reads. Cloud strategies that leverage open source projects play an increasingly important role in business success. As a result, the demand for employees with open source development skills is also growing. Indeed research indicates skills in the following areas are in high demand for open source development roles: Java, Python, Git, JavaScript, Node.js, Docker, AngularJS, Jenkins, Amazon Web Services, and Agile. Our strategic open source initiative involves partnerships, sponsorships and memberships … like our new CNCF role. In the next few months, we plan to announce sponsorships that support critical open source projects and help us engage targeted audiences. We’ll also continue to expand our open source team and become more involved in open source communities. For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page . Doug Gray is the Senior VP of Engineering at Indeed.", "date": "2018-03-06"},
{"website": "Indeed-Engineering", "title": "Open Source At Indeed: Sponsoring Webpack", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2018/07/indeed-sponsoring-webpack/", "abstract": "Indeed is proud to announce our sponsorship of webpack. Like many other companies, Indeed uses webpack to help deliver a high-quality user experience. Because webpack is so important to our development process, we’re joining our industry peers in supporting and sponsoring the development of this critical open source technology. Rebecca Murphey, Front-End Engineering Lead at Indeed, notes that “Webpack has been such a key tool in my team’s work to modernize and optimize the front-end development experience at Indeed. I’m so proud that we’re able to give a little bit back to a tool and a community that has provided so much value for us.” Sean Larkin, from the webpack core team, says, “ We are extremely proud and humbled by Indeed’s decision to sponsor webpack. When you sponsor a project, you are not only becoming a stakeholder, but also an investor in your own infrastructure that you rely on every day. It makes a statement that says: ‘We love open source and want to hire people who know and love webpack.’ We look forward to our new partnership with Indeed and are excited to have tight feedback between their team, and help solve problems that not only create lasting value for Indeed, but the open source ecosystem as a whole.” As Indeed continues to take a more active role in the open source community, we will seek out additional partnerships, sponsorships, and memberships. Recently, Indeed joined the Cloud Native Computing Foundation and now sponsors: The Python Software Foundation The Apache Software Foundation For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page .", "date": "2018-07-16"},
{"website": "Indeed-Engineering", "title": "Indeed MPH: Fast and Compact Immutable Key-Value Stores", "author": [" by Alex Shinn"], "link": "https://engineering.indeedblog.com/blog/2018/02/indeed-mph/", "abstract": "When you need to scale an application with a lot of data, how do you decide on a storage solution? How can you both safely store and efficiently interact with large data sets? This often turns into a debate about whether to use SQL or NoSQL. Each comes with strengths and drawbacks. But what if there was a third option that sidestepped database concerns altogether? Consumers might need updates no more often than every few minutes. In this case, being able to load a data set into memory dramatically speeds access and allows for tremendous scale. This is why, for many projects at Indeed, we distribute a full copy of the needed data to each consumer and render the SQL or NoSQL debate unnecessary. To make this possible, we manage data size using a new key-value store based on minimal perfect hash functions. We've implemented this store as a Java library called Indeed MPH . The problems with distributing data Usually, distributing a full copy of your data to consumers is impractical for two reasons. First, the data must be mostly read-only. Second, the data cannot be so large as to prevent distribution or processing. You can overcome the first problem by implementing batch updates. Redistribute the data every day, every hour, or even more frequently, and you keep the data read-only while ensuring that it remains useful. However, how do you keep the size down? As it turns out, the same batch update strategy that addresses the first problem also addresses the second. Minimal perfect hash functions Generating read-only data sets means that we know all the keys for our data set in advance. This lets us apply optimizations to the data set to reduce its size. In the case of hash tables, this knowledge allows us to compute a perfect hash function . A perfect hash function maps every element to a distinct integer with no collisions — in mathematical terms it’s an injective function. A minimal perfect hash (mph) function maps n keys to the n consecutive integers [0, n -1]. With such a structure we can guarantee a single disk seek while maintaining 100% load in the table, but as we’ll demonstrate later, there are even more advantages. Finding such hash functions can be difficult. In fact the Birthday Paradox, described in this Wikipedia article , tells us that zero collisions is unlikely even if m is several times larger than n. However, recent advances have yielded efficient techniques (such as described in this abstract ) and quality libraries for generating minimal perfect hashes. These techniques allow our method to succeed with only a small downside: the resulting hash function itself requires a lookup table of ~2.2 bits per key, so it does not occupy a constant size. However, this lookup table is a fraction of the size of a bloom filter and only requires about 26MB for 100 million entries. In practice, the size of the lookup table is negligible compared to the size of the data being stored. Practicalities An mph function gives us a mapping from the keys to [0, n -1], but we still need to implement the table. In memory, we’d typically just store the data in an array. To translate this to offline storage we need to make something explicit that programming language arrays make implicit: memory offsets. Our representation is a directory with three files: data.bin: the raw serialized key/value pairs offsets.bin: a fixed size array where the i th value is the byte offset in data.bin for the entry whose hash is i meta.bin: the serialized hash function itself, along with any other desired metadata The following figure shows this representation with an index mapping colors to animals: The file containing the serialized hash requires roughly 2 bits per key, while the file containing the offsets requires 4 bytes. The raw serialized key/value pairs require 46 bytes per key. One convenience of this representation is that data.bin is raw data available for iterating, independent of the hash function. In addition, we might have multiple hash functions indexing the same data.bin by different keys as long as those keys are always unique. The disadvantage of this representation is with many small entries. In this case, the size of the offsets could be on par with or even larger than the data itself. For an extreme example, if the data contains 750 million mappings from int to half-precision float, we require 7.5e8 * 6 = ~4.2G for data.bin, but 7.5e8 * 8 = ~5.6G for offsets.bin! In this example, we can take advantage of every entry having a fixed size. We don’t need to store the offsets — we can index directly into data.bin. But we want to optimize the variable-sized case as well. This high cost comes specifically when we’re dealing with small entries. If the entries are large, the size of the offsets are negligible by comparison. Because the total size is small, we can represent it with a rank (bitcount) data-structure , on the order of bits per entry instead of bytes. For a little more overhead, we can compute the inverse of rank, called select, in constant time on this same data structure. Why is that useful? If the bits are set for the start byte of every entry in data.bin, then finding the offset of the i th entry is just computing select( i ). The structure becomes the following. In this case we now have to sort data.bin by hash value. In the optimized structure, the file containing the offsets requires 1.x * 46 bits. A further optimization comes from the often very regular structure in our data. For instance, to map from a long to list of longs (using a single byte for the count), the size of every individual entry i (including key and value) can be represented as 8 xi + 9, assuming the length of the list is xi . The offset of the k th entry becomes ∑(8 xi + 9) = 9 k + ∑8 xi = 9 k + 8 x . In other words, all we have to store is x , which is much smaller than the actual offset, and a big savings with the select approach. In this case, with 10 million entries averaging 2 elements in the list, the size of data.bin is 10000000 * (9 + 16) = ~250M, but the compressed select method uses only ~2.5M. Our implementation computes what the size would be with both an array of offsets and the select approach and chooses accordingly. More optimizations Throw away the keys! If we have no collisions and know that the key we are looking up is in the table, we don’t need to verify it. Alternately, we may be able to validate the key from the value, or from other sources given the value. Finally, if we can accept some small probability of false positive lookups, we can always probabilistically verify whether keys are in the table by using a bloom filter. We have a better alternative. Since we’ve already hashed to a value that has no collisions in the original key set, we can store a k -bit signature (the low bits of any universal hash will do) of the key. Any key not in the original set that hashes to the same value will only match the signature with probability 2- k . This is a much better error rate than with a bloom filter. In all of these cases, we can omit the keys entirely from the table. When the values are small, the keys may be a significant fraction, or even the majority of the table. Link by hashes, not keys It’s common to want to link multiple tables together — the values of one table either are, or contain foreign keys into, another table. Another benefit of using an mph function is that it compresses keys to a compact integer range. We can store the hash value instead of the key, which for most practical purposes will fit in a 4-byte integer, as opposed to 8-byte longs or larger for many other foreign keys. Not only is this more compact, but it’s also faster to read because we don’t need to re-compute the hash for the second lookup. Code and benchmarks We’re excited to announce that we’ve open-sourced our mph code. For benchmarks we compare with several open source alternatives: SQLite3 : a single-file RDBMS suitable for replication LevelDB : Google’s LSM tree key-value store lsm : our LSM tree key-value store discodb : an immutable key-value store also based on minimal perfect hashing TinyCDB : an immutable key-value store using non-perfect hashing Note these options do not have equivalent feature sets. SQLite provides full relational operations, lsm and LevelDB provide range queries, and all three of these are mutable. Here we are concerned only with the common functionality of these options. We look at the results for two conceptually linked stores based on our production data. This allows us to demonstrate the link by hashing optimization described above, and also enables a more natural representation in SQLite. The first store is a mapping from 50 million 64-bit hashes to a small cluster of “items,” which are 80-bit integers represented as 16-digit base-32 strings. The second store is the reverse mapping from each item in the cluster to its associated hash. An important aspect of our own key-value stores is their use of arbitrary serialization via a plugin infrastructure. So for a fair comparison, in the sizes for our LSM trees and MPH tables we include both the sizes for opaque string to string mappings, as well as two optimizations. We encode keys as fixed 8-byte long values, and encode the values as short lists of 80-bit integers. For SQLite we encode the keys as integers. In the general case of string mappings, LevelDB, lsm and mph are all of comparable size, and notably smaller than the alternative solutions. If we apply more efficient serialization lsm and mph become much smaller, and mph is increasingly able to take advantage of the regularity in size to become the smallest option. Here we consider only the best serialization for lsm and mph. For mph we also show the size with the optimizations discussed above, first not storing the keys and then also linking to the above table via the perfect hash value. For SQLite we include the size when indexing on both columns, which allows us to remove the previous table altogether. In this case, the SQLite index is smaller than the combined sizes for discodb and TinyCDB, and comparable to LevelDB. However, mph remains smaller than everything, and in the ideal case with all optimizations applied is in fact an order of magnitude smaller. The hash-based solutions tend to be faster, as one might expect due to their requiring fewer seeks. Mph is fastest overall by a large margin, completing hash to item operations in 2 milliseconds and item to hash operations in 3 milliseconds. Write performance shows lsm and mph have the strongest performance. TinyCDB is excluded at this point due to data corruption — it has a fixed 4GB storage limit by design, but even at 3GB we were unable to retrieve 30% of the keys. Try it yourself We've made Indeed MPH available and open-source on Git . If you’re already shipping a key-value store to your servers, our mph tables may be able to offer smaller size and faster reads. If you’re not already doing so, consider this as an alternative to centralized databases. With a compact representation, you can store more data in the same space, or reduce data size that you previously thought too large. Indeed MPH: Fast and Compact Immutable Key-Value Stores cross-posted on Medium .", "date": "2018-02-02"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Sponsoring the Apache Software Foundation", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2018/06/indeed-sponsors-apache-software-foundation/", "abstract": "As Indeed continues to grow our commitment to the open source community, we are pleased to announce our sponsorship of the Apache Software Foundation . Earlier this year, we joined the Cloud Native Computing Foundation and began sponsoring the Python Software Foundation . For Indeed, this is just the beginning of our work with open source initiatives. \"We are thrilled to have Indeed join us as a Gold sponsor. We really appreciate their support for open source,\" said ASF Chairman Phil Steitz. \"Their sponsorship will help our project communities continue to grow, providing great opportunities for thousands of volunteers, and great software for billions of users.\" The Apache Software Foundation is home to a wide range of open source software — software that runs up and down our organization — from Apache Mesos to Apache Commons and everywhere in between. The Apache Software Foundation provides a neutral home to critical software that runs the internet as we know it. Supporting this organization was an easy decision for Indeed, and we’re pleased to add our name to a roster that includes many of our industry peers. We are looking forward to becoming long-term supporters of this organization, both as financial sponsors and code contributors. As we continue to take a more active role in the open source community, Indeed will seek out additional partnerships, sponsorships, and memberships. For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page . Open Source at Indeed: Sponsoring the Apache Software Foundation cross-posted on Medium .", "date": "2018-06-20"},
{"website": "Indeed-Engineering", "title": "Bringing Job Spotter to iOS: Indeed’s First React Native App", "author": [" by Andrew Roth"], "link": "https://engineering.indeedblog.com/blog/2018/01/bringing-job-spotter-to-ios-writing-indeeds-first-react-native-app/", "abstract": "In 2013, Facebook released ReactJS and changed the way a lot of people (including us) think about web development. In 2015, Facebook released React Native , allowing developers to create powerful native mobile experiences using JavaScript and React. When we planned to bring the Job Spotter app to iOS, we were hopeful that React Native would live up to its promise. Spoiler alert: it did. Intro to React and React Native React is a different way of thinking about web development. Instead of separating code by function, React encourages separating code by components. Each component contains all the layout, styling and business logic it needs – often in the same file. React’s other important feature, described in this conceptual document , is one-way data flow. Compared to the two-way flow implied by other frameworks, it is much easier to figure out how state changes affect the app in React. One-way data flow also enables powerful libraries like Redux . React Native lets you write JavaScript for Android or iOS apps with React that deliver native performance. Web developers can write powerful native apps, without needing to spend time learning unfamiliar native mobile development technologies. Wins React Native helped us speed up our mobile development cycle. Using out-of-the-box tools, we can run our React Native app locally in the iOS simulator. That allows us to refresh the app to test changes without recompilation, just like we would in the web. Even better, application state is maintained across these “hot reloads.” These development features of React Native translate into a big savings in time and effort, allowing us to focus on developing new features. Since we could leverage our previous React knowledge, we didn’t have to learn a new language and could get straight into coding. This was especially great for our team, since none of us had used Objective-C or Swift. Another bonus was that, except for some custom event logging, we hardly had to write any native code. Another interesting aspect of React Native is that it requires using inline-styles, in which CSS “classes” are actually JavaScript objects. This seemed strange at first, but we’ve grown to love it. Since we were dealing with objects, we could create powerful, composable, and extensible CSS classes and avoid a lot of style duplication seen in traditional webapps. We could develop different looks and functions as needed, rather than having to repurpose the same styles for different needs. React Native makes heavy use of Flexbox , a newer CSS module that aims to make styling more consistent and straightforward. This removes the need to know many of the traditional quirks of CSS. After overcoming the learning curve, the more declarative approach of Flexbox allowed us to style new components more rapidly than in traditional CSS. Debugging was also easier than we expected. We just connected to the debugger in the Chrome Developer Tools and debugged the same way we would on the web. React Native has several additional tools – the element inspector determines the styling applied to a component, and the network inspector analyzes the XHR requests your application makes. Being able to reuse familiar tools helped us track down and fix issues as they arose. Challenges React Native enabled us to develop more rapidly, but React Native itself developed just as fast. In the two months that we worked on the Job Spotter iOS app, React Native had six releases. After updating to each new release, we would have to do a full regression test of our app. Another major and related challenge: React Native’s newness. A lot of capabilities that would be better as default features still required third-party libraries, such as accessing the camera and file system and handling permissions. The community works hard to pick up the slack of React Native core. However, these third-party libraries sometimes lacked the quality we expected from the core library. In fact, we had to patch more than one such library. The components available for navigation and screen transitions were also inconsistent. We ran into an issue with the Navigator component in which every transition needed some kind of animation, with no option for an empty transition. This initially made some of our scene transitions look awkward. We tried using the NavigatorIOS component, but its interface was less friendly and straightforward, and lacked many of Navigator’s features. Similarly the TabBarIOS component works quite well, but its transitions were completely independent of whatever Navigator we used. We wanted a single navigator handling everything. We had to use different code paths for supporting navigation when we would have liked one. It’s worth it Despite these challenges, we continue using React Native. It fits well in our toolbox. We can use our web development expertise to build rich mobile experiences without having to become Android or iOS experts. And as React Native matures, we expect many of these current issues to be resolved. Our next step is porting the React Native version of Job Spotter to Android to replace our existing native Android app. Bringing Job Spotter to iOS: Indeed’s First React Native App cross-posted on Medium .", "date": "2018-01-09"},
{"website": "Indeed-Engineering", "title": "Mesos at Indeed: Fostering Independence at Scale", "author": [" by Gordon Murphy"], "link": "https://engineering.indeedblog.com/blog/2018/06/mesos-indeed/", "abstract": "Independent teams are vital to Indeed development. With a rapidly growing organization, we strive to reduce the number of team dependencies. At Indeed, we let teams manage their own deployment infrastructure. This benefits velocity, quality, and architectural scalability. Apache Mesos helps us eliminate operational bottlenecks and empower teams to more fully own their products. The operations bottleneck During Indeed’s early years, we manually provisioned and configured applications. For every new application, we sent a request to the operations team, which would then attempt to find a server with enough capacity to run the new application. If none could be found, operations would spin up new virtual machines or order additional servers. Provisioning a new application could take up to two months. Subsequent deployments were faster and more self-service, but that first provisioning step was a definite problem. This led to developers optimizing for their own velocity at the expense of application design. Applications became bloated monoliths, as it was easier to bolt on new services than to undertake the time-consuming process of creating new applications. This didn’t scale. Something had to change. Enter Mesos Around three years ago, Indeed began using Mesos, which gave teams the freedom to configure, deploy, and monitor their applications themselves. Today, if an application needs more CPU, memory, or disk, the team adds it. The primary benefits of this are: No gatekeepers. This increases velocity and the propensity for scalable architecture. Teams know the performance profile of their application. Because teams must specify their own CPU, memory, and disk numbers, they become familiar with expected performance, troubleshooting, and areas for improvement. Increased reliability. When a server goes down, applications are restarted elsewhere. Engineers no longer have to manage individual instances. Indeed’s Mesos ecosystem Mesos alone is not enough to reliably run applications. Our Mesos ecosystem incorporates many open source projects and in-house applications built to create a seamless experience for teams. Marathon for daemons We use Marathon to run daemons. Most Indeed developers are unaware of that, though. Since Indeed runs in ten data centers and Marathon can only run in one, we wrote a system called Marvin that coordinates deployment across all data centers. Developers independently specify their resource requirements, the version of the application to run, the number of instances, and in which data centers to run. An agent runs in each data center that compares the defined configuration with Marathon’s configuration. If they don’t match, the agent works directly with Marathon to scale up/scale down instances or initiate a new deployment. Our internal tool for batch and one-off jobs We also run a large number of batch or “one-off” jobs. Marathon is not appropriate for this use case, so we built Orc, a Mesos Framework similar to Marathon that handles configuring and scheduling these jobs—tasks that previously fell on the operations team. Because we built the tool, we can make a number of optimizations, such as last-host affinity. This interacts nicely with our Resilient Artifact Distribution system so that jobs run close to the data they require. Developers can also schedule their jobs to run whenever they need to. Orc, Indeed's tool for configuring and scheduling batch and \"one-off\" jobs HAProxy for load balancing With Marathon constantly bringing up new instances on different servers and different ports, we needed an easy way to address these instances. We use HAProxy as a reverse proxy due to its well-known performance characteristics. We wrote a small application that discovers where daemons are running and generates HAProxy configurations to match. When the configuration changes, we try to dynamically update HAProxy using its Runtime API . If that’s not possible, we restart HAProxy using a seamless reload mechanism that ensures that no packets or requests are lost. Vault for configuration Lastly, we required a robust way to configure our applications. Most applications at Indeed are configured via a simple, flat properties file. Prior to Mesos, we used Puppet to disseminate properties files to each data center, but this wasn’t self-service and there was a high degree of lag. We wanted to make it quick and easy for teams to securely configure their applications themselves, so we designed a system built around Vault , HashiCorp’s product for managing secrets. Before an application runs, we generate a short-lived token for retrieving the properties. We built a small Marathon plugin that does this for Marvin daemons, and we modified Orc to do this for batch jobs. Result: Independent teams and scalable applications All of these changes led to a 14% decrease in deployment time. Additionally, it reduced provisioning time from months to minutes and allowed our development teams to take more responsibility for their applications. We’ve seen a sharp fall in configuration and deployment tickets, and we’ve reduced our average configuration ticket resolution time from 15.6 to 3.4 days. As a result, the operations team can focus on more pressing initiatives, like re-allocating resources to create a site reliability engineering practice. We are now working toward Docker containerized deployments on Mesos. Developers will eventually roll their own Docker images, automatically scan them for vulnerabilities, and easily deploy their containerized apps on our cloud infrastructure. With these upcoming advances, we will continue enabling new capabilities on top of Mesos, allowing our engineering teams to independently create scalable applications. Mesos at Indeed: Fostering Independence at Scale cross-posted on  Medium . Also published on Mesosphere .", "date": "2018-06-15"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Sponsoring Outreachy", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2018/11/sponsoring-outreachy/", "abstract": "Indeed is committed to supporting the open source community. That’s why we’re excited to announce our sponsorship of Outreachy ! What is Outreachy? Outreachy supports diversity and inclusion across the whole open source community. By providing paid internships to people from underrepresented groups, Outreachy creates meaningful opportunities for individuals to make real contributions to open source while helping to improve inclusion in the community. Open source benefits from diverse participation, and Outreachy is making a difference. Outreachy accepted 46 interns for the December 2018 to March 2019 round of internships. Find more information about their projects on the Outreachy Alums page . Marina Zhurakhinskaya, Outreachy co-organizer, says: \"Outreachy is excited to welcome Indeed as a sponsor and is grateful for the commitment from Indeed to support diversity in free and open source software. With the help from Indeed, we are able to support more Outreachy applicants making their first contributions to free and open source software and more interns gaining in-depth experience.\" Indeed and the Community As we continue to take a more active role in the open source community, Indeed will seek out additional partnerships, sponsorships, and memberships. In addition to sponsoring Outreachy, this year Indeed joined the Cloud Native Computing Foundation and began sponsoring the Python Software Foundation , the Apache Software Foundation , the Open Source Initiative , and Webpack . For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page . Cross-posted on Medium .", "date": "2018-11-21"},
{"website": "Indeed-Engineering", "title": "The Benefits of Hindsight: A Metrics-Driven Approach to Coaching", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2018/10/the-benefits-of-hindsight-a-metrics-driven-approach-to-coaching/", "abstract": "In a previous post , I described using a measure-question-learn-improve cycle to drive improvements to development processes. In this post, I assert that this cycle can also help people understand their own opportunities for improvement and growth. It can be a powerful coaching tool — when used correctly. At Indeed, we’ve developed an internal web app called Hindsight that rolls up measurements of work done by individuals. This tool makes contributions more transparent for that person and their manager. Each individual has a Hindsight card that shows their activity over time (quarter by quarter). Many of the numbers come from Jira, such as issues resolved, reported, commented on, etc. Others come from other SDLC tools. All numbers are clickable so that you can dive down into the details. When we introduced Hindsight, we worried about the Number Six Principle and Goodhart’s Law (explained in the earlier post ). To protect against these negative effects, we constantly emphasize two guidelines: Hindsight is a starting point for discussion. It can’t tell the whole story, but it can surface trends and phenomena that are worth digging into. There are no targets. There’s no notion of a “reasonable number” for a given role and level, because that would quickly become a target. We even avoid analyzing medians/averages for the metrics included. Hindsight in action: How’s your quality? To see how Hindsight fits into the measure-question-learn-improve cycle, consider this example: Suppose my card shows that for last quarter I resolved 100 issues and had 30 issues reopened during testing. As my manager, you might be tempted to say, “Jack is really productive, but he tries to ship a lot of buggy code and should pay more attention to quality.” But remember — the metrics are only a starting point for discussion. You need to ask questions and dig into the data. When you read through the 30 reopened issues, you discover that only 10 of them were actual bugs, and all of those bugs were relatively minor. Now the story is changing. In fact, your investigation might drive insight into how the team can improve their communication around testing. Measure, question, learn, improve In this five-part series, I’ve explored how metrics help us improve how we work at Indeed. Every engineering organization can and should use data to drive important conversations. Whether you use Imhotep, spreadsheets, or other tools, it’s worth doing. Start by measuring everything you can. Then question your measurements, learn, and repeat. You’ll soon find yourself in a rewarding cycle of continuous improvement. Read the full series of blog posts: Imhotep: Scalable, Efficient, and Fast Using Metrics to Improve the Development Process (and Coach People) Metrics-Driven Process Improvement: A Case Study What’s Up, ASF? Using Imhotep to Understand Project Activity The Benefits of Hindsight: A Metrics-Driven Approach to Coaching The Benefits of Hindsight: A Metrics-Driven Approach to Coaching cross-posted on Medium .", "date": "2018-10-22"},
{"website": "Indeed-Engineering", "title": "What’s Up, ASF? Using Imhotep to Understand Project Activity", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2018/10/whats-up-asf-using-imhotep-to-understand-project-activity/", "abstract": "As I described in an earlier post , we built Imhotep as a data analytics platform for the rapid exploration and analysis of large time-series datasets. In the previous post , I showed how an Imhotep dataset based on Atlassian Jira can drive improvements to the development process. We’re continually searching for new ways to collect metrics. Examining actions in Jira, the tool we use for tracking our development process, seemed like a natural fit for gaining process insights. We decided to find a way to convert Jira issue history for a large set of projects into an Imhotep dataset of actions, organized by time. The open source Jira Actions Imhotep Builder transforms issue activity in a Jira instance into an Imhotep dataset. Each document in the resulting dataset corresponds to a single action on a Jira issue, such as creation, edit, transition, or comment. The builder queries the Jira REST API for each Jira issue in the specified time range, then deconstructs the issue into a series of actions. The actions are written to a series of .TSV (tab-separated values) files, which are uploaded to an Imhotep dataset. Using that builder, we created a dataset of activity on projects in the Apache Software Foundation (from their Jira instance ). We hope Apache projects take advantage of the dataset to gain insights about ways they can improve processes for their developer and user communities. Diving into the ASF Jira data We created an Imhotep dataset of ASF Jira data from January 1, 2016 through the present. As of October 17, 2018, the apachejira dataset: contains nearly 3.4 million Jira actions, including 230,298 issue creations, 1.8 million edits, and 1.3 million comments requires only 274MB on disk, or about 81 bytes per action Using the apachejira dataset, we can answer many questions about what's happening in ASF projects, such as the following examples. Who reported the most bugs in ASF projects from July-September? from apachejira 2018-07-01 2018-10-01\r\n   where action=\"create\" issuetype=\"Bug\"\r\n   group by actor Beam JIRA Bot, with presumably actual person Sebb in the #2 position: Which projects have the most bugs reported from July-September? from apachejira 2018-07-01 2018-10-01\r\n   where action=\"create\" issuetype=\"Bug\" \r\n   group by project Ignite edges out Ambari for the top spot, with 401 bugs reported. The next two questions explore some differences in project workflows. How many distinct status values exist in the most active projects? from apachejira 2018-01-01 today\r\n   group by project[10] \r\n   select count(), distinct(status) Five of the top ten projects have 6 distinct statuses, and the other five have 5 distinct statuses. For example, Apache Beam has 5, and Apache Hive has 6. How do the statuses used by Apache Beam and Apache Hive compare to one another? from apachejira 2018-01-01 today\r\n   where project in (Beam, Hive)\r\n   group by status\r\n   select project='Beam',project='Hive' Hive uses the Patch Available state, Beam doesn't. It turns out that about 11% of the Apache JIRA projects take advantage of this state. Which projects had the most contributors changing issue status to Patch Available in 2018? from apachejira 2018-01-01 2019-01-01\r\n   where fieldschangedtok='status' \r\n      status='Patch Available'\r\n   group by project[10]\r\n   select distinct(actor) Hadoop ecosystem projects (Hive, HDFS, Hadoop Common, YARN, HBase, and Hadoop Distributed Data Store) claim six of the top 10 spots. Who contributed to (set status to Patch Available in) Apache Hive in 2018? from apachejira 2018-01-01 2019-01-01\r\n   where fieldschangedtok='status' \r\n      status='Patch Available' project = 'Hive'\r\n   group by actor[10]\r\n   select count(), distinct(issuekey) The top 10 contributors contributed to 578 issues in 2018. How long does it take to get a patch accepted in the 20 most active projects? from apachejira 2018-01-01 2019-01-01\r\n   where prevstatus=\"Patch Available\" \r\n      status=\"Resolved\" \r\n      fieldschangedtok=\"status\"\r\n   group by project[10]\r\n   select count(), timeinstate\\3600/count() \r\n      /* hours in state */ Hadoop Distributed Data Store is the fastest, with an average of 102 hours between the Patch Available and Resolved states. The average for Kafka is really high, but it turns out that about 28 outliers with resolutions of Not A Problem, Auto Closed, Duplicate, Won’t Fix, and Won’t Do contributed to the high average. from apachejira 2018-01-01 2019-01-01\r\n   where prevstatus=\"Patch Available\" \r\n      status=\"Resolved\" \r\n      fieldschangedtok=\"status\" project = Kafka\r\n   group by resolution\r\n   select count(), timeinstate\\3600/count() \r\n      /* hours in state */ That might be a bad thing or an okay thing for the community. Either way, digging into numbers like these can raise interesting questions. These are a small sample of the questions we could explore in this dataset. Creating and analyzing your own Jira datasets We’ve made the Jira Actions Imhotep Builder available as open source . We hope you will use it to build your own Jira-based Imhotep datasets. This builder is the first one we've published, and we’ve also listed it in a new Imhotep Builder Directory . If you have an idea for a new builder, or need help getting started with Imhotep, open an issue in the GitHub repository or reach out on Twitter . In the next post in this series, I describe Hindsight, an internal tool we use to make internal contributor work visible and drive coaching insights. Read the full series of blog posts: Imhotep: Scalable, Efficient, and Fast Using Metrics to Improve the Development Process (and Coach People) Metrics-Driven Process Improvement: A Case Study What’s Up, ASF? Using Imhotep to Understand Project Activity The Benefits of Hindsight: A Metrics-Driven Approach to Coaching Cross-posted on Medium .", "date": "2018-10-21"},
{"website": "Indeed-Engineering", "title": "Metrics-Driven Process Improvement: A Case Study", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2018/10/metrics-driven-process-improvement-case-study/", "abstract": "In the previous post , I described how we use a measure-question-learn-improve cycle to refine development processes. To reiterate: Measure everything we possibly can. Learn by asking questions and exploring the data we’ve collected. Use our learnings to try to improve. Measure continuously to confirm improvement. At Indeed, we get as much data as we can into Imhotep — everything that happens in our products, but also everything that happens in the development process. Process-oriented Imhotep datasets at Indeed include Git commits, Jira issue updates, production deploys, wiki edits, and more. Let’s take a look at how we applied the measure-question-learn-improve cycle to a problem at Indeed: translation verification. How long are translations in “Pending Verification”? Our international team believed it was taking way too long to verify string translations in our products. We track translation work in Jira, and we track Jira issue updates in an Imhotep dataset. So we started asking questions of our measurements that might help us understand this problem. This Imhotep query gives us a day-by-day grouping of time spent in the Pending Verification state. It includes only Translation issues (in a sample project called LOREM) that moved out of that state: from jiraactions 2017-01-08 2017-04-02\r\nwhere issuetype = 'Translation' AND\r\n      prevstatus = 'Pending Verification' AND\r\n      status != 'Pending Verification' AND\r\n      project = 'LOREM'\r\ngroup by time(1d)\r\nselect timeinstate/86400 /* days pending */ We ask Imhotep to graph that metric cumulatively over time. We see that for the given 3-month period, Translation issues spent a total of ~233 days in Pending Verification. That sounds like a lot of time, but it’s important to ask more questions! Be skeptical of the answers you’re getting, whether they support your hypothesis or not. Can we dig into the data to better understand it? What other information do we need to interpret? What are the sources of noise? Do we need to iterate on the measurement itself before it is generally useful? In this example, what if only a few issues dominated this total? Let’s tweak our query to look at how many issues are contributing to this time. from jiraactions 2017-01-08 2017-04-02\r\nwhere issuetype = 'Translation' AND\r\n      prevstatus = 'Pending Verification' AND\r\n      status != 'Pending Verification' AND\r\n      project = 'LOREM'\r\ngroup by time(1d)\r\nselect distinct(issuekey) /* number of issues */ Our translators shared with our engineers their perspective on the root cause. Translation changes had to wait for a full build before they would show up in a testing environment. Translators would move on to other work and return to verify later. We paid the cost of these context switches in a slower rate of translation. The spikes we see in the graph above show that delay. Each time a set of changes reached a testing environment, a number of verification events closely followed. The visualized data confirms the inefficiency described by the people actually doing the work. When we switch that graph to cumulative, we see that translators verified 278 issues in the time period. That is probably a large enough dataset to validate the hypothesis. These are just a few examples of questions we can quickly and iteratively ask using Imhotep. When we have good measurements and we ask good questions, we learn. And based on our learnings, we can try to improve. Translation verification: There is a better way If a translation change could go straight to a testing environment as soon as it was submitted, we would eliminate the inefficiency described above. In fact, a couple of engineers at Indeed figured out a way to deploy translations separate from code. They started to try that incrementally on a project-by-project basis. This capability enabled translators to verify issues minutes after completing their changes. After a period of time, we were able to compare two similar projects. The IPSUM project used the new translation deployment mechanism, while the LOREM project used the old method. To illustrate the benefits of the new mechanism, it’s worth comparing the worse case scenarios. This query lets us see the 90th percentile time in Pending Verification for just those two projects. from jiraactions 2016-09-15 2017-02-28\r\nwhere issuetype = 'Translation' AND\r\n      prevstatus = 'Pending Verification' AND\r\n      status != 'Pending Verification'\r\ngroup by project in ('LOREM','IPSUM')\r\nselect percentile(timeinstate, 90) The new process does look faster, with a 90th percentile of 1.8 days, compared to 12 days for the project using the old mechanism. After digging into the data, asking more questions, and further verifying, we decided to move more projects onto the new system and keep measuring the impact. Using Imhotep to understand your process In order to track Jira activity with Imhotep, we wrote a tool that we run daily to extract the data from Jira into an Imhotep dataset. We’ve open sourced this tool, and you can find it in the Imhotep Builder Directory . In the next post , I describe using that builder to analyze Apache Software Foundation projects. Read the full series of blog posts: Imhotep: Scalable, Efficient, and Fast Using Metrics to Improve the Development Process (and Coach People) M etrics-Driven Process Improvement: A Case Study What’s Up, ASF? Using Imhotep to Understand Project Activity The Benefits of Hindsight: A Metrics-Driven Approach to Coaching Cross-posted on Medium .", "date": "2018-10-20"},
{"website": "Indeed-Engineering", "title": "Using Metrics to Improve the Development Process (and Coach People)", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2018/10/using-metrics-to-improve-the-development-process-and-coach-people/", "abstract": "In the previous post , I described Imhotep, our scalable, efficient, and fast open source data analytics platform. Imhotep helps us use metrics at Indeed for fast, iterative experimentation, which drives improvement to our products. Improving process and coaching people We use the same tools and techniques to improve development processes, following a measure-question-learn-improve cycle: Measure everything we possibly can. Learn by asking questions and exploring the data we’ve collected. Use our learnings to try to improve. Measure continuously to confirm improvement. Beyond process improvements, this approach can also work for people. Data can help us understand our own work and coach others. How much am I getting done? How am I engaging with other teams? How has my work changed over time? What are my blind spots? Is measuring processes and people a good idea? You might be skeptical of using this approach for improving process and measuring people. It’s good to be skeptical. To truly benefit from this approach, you must proceed with caution . Gaming the stats (Goodhart’s Law) The first caution is Goodhart’s Law , which states that “When a measure becomes a target, it ceases to be a good measure.” For example, your manager might say: “Our measurements show that our team productivity is declining. Let’s set a target to increase features completed by 20% next quarter. If we hit it, big bonuses all around!” Okay, but your team might then start “gaming the stats” — making changes that improve the metric without improving productivity. Now your measure is meaningless for gauging productivity, and you’ve rewarded your team for counterproductive measures that don’t advance your goals. The Number Six Principle The second caution is something I’ve named the Number Six Principle (inspired by a classic TV character and his famous line ): Don’t reduce people to a set of numbers. No one enjoys being judged entirely by numbers. If you tell people you’re measuring them, you run the risk of seriously damaging morale. Many people will assume you’re not considering qualitative performance elements. It’s how you use them You can avoid these pitfalls if you’re careful. Consider the example above in which your team is concerned about slipping productivity metrics. If you take a close look at the numbers, understand them in context, and diagnose the situation, you can have a productive dialog about how to improve. Perhaps your team tackled more complex features, therefore completing fewer. That might be okay, or you might agree as a team that you could have done a better job of simplifying the feature work. Or maybe you look at a different metric and see that your overall support load went up 50% due to growth in your customer base. You can then decide to live with that balance or try to augment your team’s capacity to handle support while developing new features. Starting with the measurements, a considered discussion can lead to tangible process improvement. In the next post , I describe a process improvement we validated and measured with Imhotep. Read the full series of blog posts: Imhotep: Scalable, Efficient, and Fast Using Metrics to Improve the Development Process (and Coach People) Metrics-Driven Process Improvement: A Case Study What’s Up, ASF? Using Imhotep to Understand Project Activity The Benefits of Hindsight: A Metrics-Driven Approach to Coaching Using Metrics to Improve the Development Process (and Coach People) cross-posted on Medium .", "date": "2018-10-19"},
{"website": "Indeed-Engineering", "title": "Imhotep: Scalable, Efficient, and Fast", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2018/10/imhotep-scalable-efficient-and-fast/", "abstract": "This post is the first in a five-part series on improving the development process (and coaching developers) with metrics-driven insights. Move fast and try things — that’s how we develop products at Indeed. We don’t believe in betting on a small number of great ideas. Instead, we bet on exploring lots of ideas as quickly as possible. To be successful in this approach, we need innovative team members with diverse perspectives. We hire people who are excited to quickly explore ideas in service of our mission — to help people get jobs. Once they’re on board, we give them ownership and autonomy to do exactly that. And we give them the tools to track and analyze their experiments. The right tools for the job We’ve developed and open sourced some of these tools, including Imhotep , our data analytics platform. Imhotep enables rapid exploration and analysis of large time-series datasets. It includes a query language (IQL), a web-based UI, and a distributed backend. It is scalable , efficient , and fast . Imhotep is scalable Imhotep scales horizontally by adding daemon instances that can run on commodity hardware or in the cloud. Indeed’s internal Imhotep cluster handles up to 5 million queries each week across thousands of datasets. Roughly 90% of those queries come from automated systems. Our most popular dataset includes about 39 billion events just for the last year. That dataset alone receives around 25,000 distinct queries each month. Imhotep is efficient Because the data structure underlying Imhotep is an inverted index , the disk utilization is remarkably low for most time-series datasets. The dataset mentioned above, with 39 billion events and 384 possible fields per event, takes up 5.7 terabytes on disk. That works out to 146 bytes per event. That kind of storage efficiency allows us to keep all the data for analysis and avoid sampling. Sampling is fine when you want to just look at aggregate trends. But if you want to actually dig down into your data and examine the outliers, you can’t reliably find them or see their effects if you sample. Imhotep is fast Imhotep’s speed lets us rapidly iterate and collaborate. Over a recent 90-day period at Indeed, our internal cluster saw around 2 million interactive Imhotep queries (queries done from the webapp). The median response time for those queries was 276 milliseconds . A powerful cache implementation contributes to this blazing speed, with nearly 60% of interactive queries coming from the cache. But even uncached queries are quite fast, with a median response time of around 4 seconds. An uncached query over a long time span takes longer, but not that much longer. For uncached queries with a 365-day time span, the median response time is about 9 seconds. How do we know all these stats about Imhotep performance? Because we have an Imhotep dataset for Imhotep usage. In just a few minutes, I was able to iteratively query that dataset to understand recent cluster performance. Imhotep drives insight and improvement Imhotep empowers us to experiment and quickly improve our products. We’ve also applied this data-driven approach to improving development processes. In the next post in this series , I explain more about how we use metrics to improve process. Read the full series of blog posts: Imhotep: Scalable, Efficient, and Fast Using Metrics to Improve the Development Process (and Coach People) Metrics-Driven Process Improvement: A Case Study What’s Up, ASF? Using Imhotep to Understand Project Activity The Benefits of Hindsight: A Metrics-Driven Approach to Coaching Imhotep: Scalable, Efficient, and Fast cross-posted on Medium .", "date": "2018-10-18"},
{"website": "Indeed-Engineering", "title": "Audit Your Web Applications with AVA", "author": [" by Jonathan Hawes"], "link": "https://engineering.indeedblog.com/blog/2018/09/application-scanning/", "abstract": "Hosting a web application is an excellent way to expose useful services to the public, but it comes at a cost: vulnerabilities in your web apps could allow attackers to access important systems, endangering your customers and your business. The Indeed Security team developed Another Vulnerability Auditor (AVA) to address this problem. By using AVA to automate application scans, we can continuously monitor production and QA systems for potential vulnerabilities. And, because we have released it as an open source tool, you can use it to monitor your applications as well. How does it work? AVA scans a set of application endpoints, defined in HTTP Archive (HAR) format. The HAR file catalogs the URL, headers, cookies, and POST data for HTTP requests. AVA uses this information to model endpoints and scan them using a combination of auditors and checks. Auditors Auditors determine the HTTP elements that AVA audits. These include URLs, headers, cookies, and POST data. Type What does it audit? Cookie Individual cookies in the Cookie request header Header Most request headers JSON JSON data in the request body Multipart Multipart form data in the request body Parameter Parameters in the URL query string and request body Response Aspects of a response (passive audit) Text Plain text data in the request body URL Request URL Checks Checks determine the types of security vulnerabilities AVA checks. These include cross-site scripting , open redirects , SQL injection , and shell injection . Type What does it check for? Code injection Code injection in Python’s pickle and eval statements Header injection Header injection in responses Open redirect Open redirects to arbitrary URLs Path traversal Path traversal on the local filesystem Shell injection Shell injection in Bash statements SQL injection SQL injection in database queries Cross-site scripting HTML and JavaScript injection in responses XML external entity XML external entities in XML documents Personally Identifiable Information (PII) Email addresses, credit cards, and Social Security numbers How can I use it? We designed AVA for use within automated systems. We automate AVA with Docker Swarm and Jenkins. However, you can use AVA anywhere Python can be installed. Use in Docker Swarm Indeed’s Security team uses Docker Swarm to automate AVA and scan public-facing applications daily. This allows us to identify vulnerabilities shortly after they are introduced. The pipeline has three components: Enricher combines data from sources, such as WES , into endpoint definitions Scheduler maintains a schedule and configuration Vulnerability manager stores reports and displays vulnerability information The process is as follows: The scheduler contacts the enricher and requests endpoint definitions for the current application. The enricher returns these definitions in HAR format. The scheduler pushes the HAR data and configuration settings to AVA. AVA runs the configured scan against the application and generates a report. AVA sends the report to the vulnerability manager for storage. Use in Jenkins We also use AVA in Jenkins to examine systems in our QA environment. This allows us to identify vulnerabilities before they reach production. The pipeline has two components: functional tests and AVA. The functional tests are a collection of Selenium-based test cases for verifying release candidates in QA. The process is as follows: Functional tests run against the application. A proxy collects traffic from the tests and exports it as HAR files. AVA scans the application using the exported HAR files. AVA provides a report documenting the results of the scan. How can I get AVA? We’ve made AVA available and open source on Git . Download it, try it out, and reach out to us on GitHub or Twitter if you need any help. You can open an issue on the GitHub repository, or hit us up on Twitter . Audit Your Web Applications with AVA cross-posted on Medium .", "date": "2018-09-13"},
{"website": "Indeed-Engineering", "title": "Where Do Data Scientists Come From?", "author": [" by Chris Lindner"], "link": "https://engineering.indeedblog.com/blog/2018/12/where-do-data-scientists-come-from/", "abstract": "Our previous article in this series on Data Science titles made the case that there’s no such thing as a data scientist — instead, the phrase “data scientist” has come to represent a number of distinct roles. So in addition to their different skills and job duties, we’d like to know who data scientists are and what backgrounds they come from. In this article, we dig into the resume data of practicing data scientists, and discover that data scientists come from a wide variety of fields of study, levels of education, and prior jobs. We also explore what this data can tell us about the similarities and differences in the roles of data scientists, analysts, engineers, and software and machine learning engineers. Who are data scientists? If you ask every data scientist around you what they did before data science, they’re each likely to give you a different answer. Many have master's and PhD degrees in fields ranging from astrophysics to zoology. Others come from the many new data science graduate programs that universities now offer. And still others come from technology roles, such as software engineering or data analysis. At Indeed, we help people get jobs. One way we do this is by letting job seekers submit resumes so employers can find a perfect match. Our datasets contain tens of thousands of resumes from current and former data scientists. We can use this resume data to gain some insight into where data scientists come from. Does educational background matter? Highest degree achieved First, we took a look at the highest degree achieved by those who hold the title of “data scientist” or a related field¹. We’ve chosen the job titles of data engineer, data analyst, software engineer, machine learning engineer, and data scientist², as these reflect some of the distinct roles we found in our previous articles . Data Scientists Data scientists have the highest average education level of any of the job titles we examined. Data scientists have more PhDs than any of the other job titles. However, a PhD is not required for becoming a data scientist; only 20% of data scientists have them. Advanced degrees (master's or PhD) are held by 75% of data scientists. Less than 5% of data scientists have only a high school diploma or associate's degree. Machine Learning, Data, and Software Engineers Software and data engineers have more bachelor’s degrees than advanced degrees, while machine learning engineers are more likely to hold advanced degrees. Machine learning engineers have a similar distribution of education levels to data scientists, but are about 30% less likely to hold a PhD. These results seem roughly in line with a similar study by Stitch Data . Engineering-focused roles tend to favor bachelor’s degrees with some master's degrees, but very few (<5%) PhDs. One in four data engineers has a high school diploma or associate's degree as their highest level of education. Data Analysts Data analysts have a very different distribution of degrees than data scientists, and more closely resemble software engineers in their levels of academic achievement³. Data scientists have PhDs at almost 10 times the rate of data analysts, and are twice as likely to hold a graduate degree. As we’ll see later, this may be due in part to an emerging pattern of software engineers transitioning into data analysis. This could also mean that PhDs are being treated as relevant work experience by employers, who may be seeing data scientists as having more senior roles. Or perhaps the training in a master's or PhD program uniquely prepares individuals for research-oriented data science work. Field of study Looking at the distribution of fields of study between job titles reveals some intriguing results. The “data scientist” job title exhibits the most diversity in field of study of any of the titles we looked at, and no one field seems to dominate. We can quantify the diversity by calculating the gini impurity of each job title. Gini Impurity (Larger means more diverse fields of study) Data Scientist — 85% Machine Learning Engineer — 73% Software Engineer — 53% Data Analyst — 78% Data Engineer — 79% Data Scientists Data scientists clearly have the most diverse fields-of-study in the job titles we’ve looked at, while software engineers have the least diverse educational backgrounds. While the social sciences are somewhat under-represented in the data science population, they still make up about 5% of data scientists. Data science majors make up a slightly larger portion of data scientists (9%), which is somewhat surprising given how new most university data science programs are. Machine Learning Engineers Our data also shows a pronounced distinction between data scientists and machine learning engineers. Over 60% of machine learning engineers come from a computer science or engineering background, and are almost twice as likely to be from these backgrounds than someone holding the title of “data scientist.” There were effectively no social scientists with the title of “machine learning engineer” in our sample. Software Engineers Software engineers are — unsurprisingly — even more heavily focused on computer science and engineering majors. It’s been proposed that machine learning engineers are a merger between software engineers and data scientists . Our data appears to support this assertion. Data Analysts Like data scientists, data analysts seem to come from a diverse educational background. They differ from data scientists in that they are more often business, economics, and social science majors, and less often have mathematics, statistics, and natural science degrees. It’s also interesting to note that those with data science degrees represent more of the data scientist population than the analyst population. Data Engineers Data engineers show a field of study distribution that is somewhere between data scientists and machine learning engineers. However, as noted above, many data engineers don’t have any degree beyond a high school diploma! Which jobs do data scientists hold prior to data science? Unsurprisingly, many individuals (approximately 25% of our sample) held the same title in their previous role as in their current one. This is especially true of software engineers, who are very likely (71%) to have held a software engineering role previously. This is probably due to the relative maturity of the field of software engineering as opposed to data science, which didn’t even have its own title until fairly recently. “Academic” here means actually being employed by a university, or as a researcher in an academic environment. Graduate students in particular are likely to have held such positions, and we see that the most graduate-degree heavy fields (data science, machine learning engineer, data analyst) have the most transitions from academia. Perhaps a more interesting question is, what was the last different job title that data scientists held? Here we see some interesting patterns: data scientists, machine learning engineers, and software engineers are more likely to start straight out of academia. Many of the “other” previous jobs are unrelated, such as catering, tutoring, store clerks, and other positions people can often hold while completing their degrees. Many roles transition into data scientists or machine learning engineers, but rarely do we see data scientists and machine learning engineers transitioning into any of the other roles. This is likely due in part to the relative sizes of the fields, the infancy of the “data scientist” and “machine learning engineer” titles, and the recent growth in popularity of those titles. However, I believe we are also observing an interesting phenomenon that speaks to how individuals are moving between and progressing⁶ through each role. This chord diagram illustrates the main transitions we see between these roles. The color of the chord indicates which role people are transitioning from . Software engineers make up a big slice of the pie. Many transition to analyst roles, while others hop straight to data science. Data science is equally fed by academia, analysts, and software engineers. Software engineers are far more likely to hop into a data analyst role, although this is in part due to the larger number of analyst roles than data scientist roles. Again, we see few individuals leaving data science at this moment. It’s unclear if this pattern will change in the future. The key takeaway here is that the data science field is fed by a wide variety of backgrounds, and it is relatively common to see software engineers become data analysts, and data analysts become data scientists. This may represent a viable path for anyone looking to transition out of a software engineering role. Transitions into data engineering come almost exclusively from software engineering⁴. Conclusion Where do data scientists come from? Everywhere! Although the field is predominantly populated by individuals with master's and PhD degrees, there are still plenty of individuals with bachelor's degrees (26%) in the role. No field of study seems to dominate data science at this time; conversely, we see a great diversity in backgrounds for data scientists, especially compared to fields like software engineering. In addition, we see a large number of individuals moving from other tech roles — such as software engineering and data analytics — into data science. While machine learning engineers reflect data scientists in their levels of academic achievement, they seem to be more heavily focused in engineering backgrounds, and are more likely to have transitioned from a software engineer role. Data engineers also have more of an engineering focus, but tend to have lower levels of degree achievement when compared to the other roles in this study. What does this mean for data science job seekers? Graduate school is still the dominant way data scientists get into the field. Data science degrees have a growing presence, and now appear to be a somewhat common way to get entry into the field. Any field of study seems viable if one has obtained an advanced degree. If you’re in a graduate program now, there’s almost certainly someone in your field of study working in data science. I suggest you reach out to them and find out how they made the leap! Software engineers and data analysts seem to transition into data science roles quite regularly, and represent substantial portions of new data scientists. Future job seekers should consider these routes as well. What does this mean for employers looking for data scientists? If you’re looking for a generalist data scientist, don’t throw out a resume just because the field or degree isn’t what you expect. Data scientists are diverse in their education and background. Although most have an advanced degree in some field, there is no one field that dominates the job market. If you’re having difficulty hiring experienced data scientists or scientists out of academia, consider bringing in individuals from software engineering or data analyst roles, as that is clearly a common pathway to data science. Also — as we’ll discuss in a later article — make sure you know the role you’re actually hiring for. Do you think need a data scientist, but feel your role is more heavy on engineering? Consider introducing a “machine learning engineer” role. Do you think you need a data scientist, but with more focus on a business background? Consider hiring an analyst. Do you need someone with a focus on database and infrastructure skills? Consider a data engineer, and don’t focus as much on their educational background. Finally, if you think you do need some sort of generalist data scientists for your team, consider looking for a variety of educational backgrounds. At Indeed, the members of our data science and product science teams span a wide range of fields, including astronomy, sociology, biology, mathematics, economics, and business. Having a diverse data science team — both in demographics and in field of study — is essential for doing great work ⁶ ⁷. Footnotes ¹Note that there is almost certainly a bias here, in that we’re looking at the resumes of job seekers that have already added “data scientist” to their resume. This means we’re going to be looking at individuals who have likely already been in the field for several years, and may not be entirely representative of more recent trends. ²For each job title, we’ve bucketed related job titles as well, e.g. “Senior Data Scientist” will be in the Data Scientist category, and “C++ Programmer” will be in the Software Engineer category. ³ Paula Leonova has a good, data-driven discussion of the difference between data science and data analyst roles . ⁴To be absolutely clear, I do not mean to imply a hierarchy of roles. Many software engineering roles, for example, are far more senior than many data scientist roles. I am simply referring to the directional pattern that seems to be emerging. ⁵Stitch Science did a nice breakdown of data engineering roles , and also noted the major overlap with software engineering. ⁶For more information on the importance of diversity in the workplace, see also The Difference by Scott E. Page ; “Why diversity matters” by Hunt, Layton, and Prince ; and “Evidence for a Collective Intelligence Factor in the Performance of Human Groups” by Woolley et al. ⁷It is not my intention to conflate “diversity in field of study” with broader diversity topics. I strongly believe diversity in all dimensions is essential for doing great work and creating a better society, and it will take far more than focusing on degree of study to overcome the overwhelming lack of diversity in tech workers in the US right now. As argued by an article from Stitch, Data Science does not appear to be doing any better than engineering roles in many aspects of diversity . Where Do Data Scientists Come From? cross-posted on Medium .", "date": "2018-12-13"},
{"website": "Indeed-Engineering", "title": "Market Your Data Science Like a Product", "author": [" by Erik Oberg"], "link": "https://engineering.indeedblog.com/blog/2018/12/marketing-data-products/", "abstract": "A 7-Step ‘Go-to-Market’ Plan for Your Next Data Product Why do internal tools need marketing? Have you ever developed a great solution that never gets used? Accuracy, statistical significance, model type: none of these matter if your data product is not put into action. Positively impacting your organization as a data scientist means developing high quality data products and successfully launching those data products. As a product scientist at Indeed ( product science is a team in data science ) , I think about launching both business products and internal data products. This has helped me see that marketing techniques for launching goods and services can also be applied to launching data products internally. With this perspective, I’ve helped the tools I developed become among the top 10% most used at Indeed. I have broken down what I do into seven steps: Naming/branding Documentation Champion identification Timing Outreach Demoing Tracking 1. Get an MBA name Your product needs a name that’s MBA: Memorable, Brandable, and Available. Indeed runs over 500 IPython notebook web applications for internal reporting each day. We’ve developed and deployed over 12,000 IPython notebook web applications. In this rich reporting environment, data products need a way to distinguish themselves from one another. It’s hard to summarize the months you have spent exploring data, developing a model, and validating output into just a few words, but it also can shortchange your work to go with “The model” or “The revenue/ job seeker behavior/ sales thing I have been making!” Identify your high-quality data products in ways that signal your past and future investment in the work. Memorable Apple and Starbucks are two of the most valuable brands in the world. Still, only 20% of people in a study by Signs.com could draw the Apple logo perfectly and only 6% for Starbucks. This points to the power of the name. People do not need to remember exactly how a logo or your data product looks and works, but they need to be able to recall it by name. Memorable names are often: Pronounceable . They start with a sharp sound and roll off the tongue. Research on English speakers suggests names with initial plosive consonants (p, t, k) are more memorable, but also see research on word symbolism . Plain. They frequently repurpose common words (e.g., Apple or Indeed), which help you combine rich mental images to your product. Be aware that discoverability through search may be limited when using common words. Slightly modifying the word can help overcome this (Lyft) as long as it’s memorable. Produced . They can even be entirely new. Making up a new word is also a strategy (Google, Intel, Sony, or Garmin), but this requires substantially more initial seeding to establish the name. This may not be in line with the audience and timeframe of an internal data product launch. Brandable You want your name to consistently represent the identity of the data product and reflect an overall positive attitude towards it. This way it can be incorporated seamlessly into the tool and documentation. Available Make sure no one else has called their data product the same thing! Once you have picked the name, you can dress it up with a logo. The logo can simply be your MBA name that’s been stylized following the same MBA principles. A shortcut like Font Meme Text Generator can quickly create a sufficient design. For example, 2. Document the product You know what your code does. But what if you’re not around to answer questions, or give a demo when the CEO or a curious new intern ponder to themselves, “What does this thing do?” Documentation is not only good practice as a data scientist/developer, it is also an opportunity for your work to be found. When one business wants to know if another business has the products and services it needs, 71% start with a simple Google search . Similarly, in addition to being valuable for your user group, wiki documentation and code comments create searchable content that helps your work get discovered. When writing your documentation, identify: the main problem your data product is solving key features and how they solve the problem key definitions key technical aspects that need to be explained Documenting your product's journey can also help build trust in the product. Use consistent messaging by including your MBA name and logo within the documentation to further establish your brand. 3. Identify champions Who else \"gets\" the problem you are trying to solve and how the data product delivers a solution? Seek out people who are affected by that problem, and share your work with them. Also, look to your own team members who have participated in the build or know your work. These champions can recommend your work to others who would also appreciate the solution. Identifying champions is analogous to customer advocacy in consumer business. Word-of-mouth is a leading influencer across continents and generations for ~83% of consumers ( according to a study by Nielsen ) when making a purchase decision. Your data product champions will be your top sales reps, lending credibility to the tool and answering questions when you are not around. 4. Timing is everything Before each launch, consider the current business environment, and time your launch accordingly. The moment you have finished working on your data product is not necessarily the best time to launch it. For example, a product team may be in the middle of fixing a major bug and not ready for a new idea. Conversely, an upcoming related communication activity (e.g., blog post) could be an opportune time for a release with cross promotion. Look at other recent data products: When were they released and how were they received? Stakeholders can feel inundated with too many new dashboards and models and this may even contribute to \"analysis paralysis.\" 5. Know your audience If your champions are not happy, your product can lose its luster in a Snap . Developing positive working relationships with your champions and users is important for the early and long-term success of your data product. Identify and reach your audience — those who will be using what you’ve made and can benefit from it. With this target audience in mind, comment on tickets, post on Slack, chat, send emails to relevant groups, or go directly to talk to your audience. Use your audience’s preferred channels to communicate development progress, releases, and feedback. Establishing this communication will build early confidence in your data product. As iteration requests come in, you will have the opportunity to build this confidence with thoughtful acknowledgement of requests. In 2017, Indeed’s Data Science Platform team — software engineers who built a machine learning deployment framework — went on a roadshow to Indeed’s multiple tech offices to share the data science platform framework. This was a great example of engaging with an audience across offices. 6. Go live! Only you can see the picture in your mind of how something works. Demoing is a powerful way to communicate what your new data product does. A great way to do this is by getting a minimum viable data product, a prototype, out early to your champions. Examples include creating a working application with minimal data, sketching a mockup of a dashboard, or taking screenshots. Forbes has more examples of consumer products . As a demo to explain a sales lead qualification machine learning model to the Sales organization, the product science team built a simple interactive web app that returned the model results when a user changed the value of the model features with sliders. 7. Own the results “ It’s not that I’m so smart, it’s just that I stay with problems longer.” — Albert Einstein You may love the theoretical foundation and implementation of your data product, but ultimately the success of a data product comes down to the user. Long term marketing and retaining users depends on how much you can ensure reliability. Reliability is key to building your data product’s brand, your reputation and your technical credibility. This affects the marketing for your other current and future data products as well. It’s worth noting that this doesn’t mean perfection — it often just means dealing with problems quickly, fully and transparently. Monitor key metrics of your data product to see how it’s working and what its impact is. Actively seek and be responsive to feedback. Evaluate if your data product is achieving its intended objectives and determine if features can be improved to better suit your audience. If you are not achieving impact or the tool is not being used, revisit your initial assumptions about the problem you thought you were solving. Then, talk to your users (and non-users) about what might not be working. Be willing to destroy and start again, and create something even better with a new perspective. The initiative to iterate and improve your data product tools requires persistence but will raise the quality of your data products and enhance the rest of your marketing efforts. Final thoughts Teams outside the analytics community depend on your marketing efforts to learn about your data products that can make them and the company more effective. You don’t have to wait until the product is finished to start letting other teams know about the product. The marketing can start with documentation, champion identification, and outreach as soon as initial requirements are being gathered. That being said, creating a data product of quality is a priority over marketing for data science, so choose what you market. A data scientist’s credibility is essential for people to trust your data-driven recommendations and act on them. Ensure that you’re investing it wisely. If you are passionate about both developing great data products and making sure your data products have impact, check out product science and data science at Indeed! Market Your Data Science Like a Product cross-posted on Medium .", "date": "2018-12-08"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Sponsoring the Open Source Initiative", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2019/02/sponsoring-osi/", "abstract": "At Indeed, we’re committed to taking a more active role in the open source community. In 2018, we joined the Cloud Native Computing Foundation and began sponsoring the Python Software Foundation , the Apache Software Foundation , and Outreachy . We also began sponsoring the Open Source Initiative (OSI) and we are pleased to renew our sponsorship for 2019. Since 1998, the Open Source Initiative was formed to hold the definition of what open source means. The OSI provides the final word when it comes to open source licensing, ensuring a common understanding of which software licenses do and do not adhere to the core open source principles. For more than 20 years, the OSI has done critical work in education and advocacy. We’re excited to be on their list of sponsors as they embark on the next 20 years of their mission. “Indeed’s active engagement with open source communities highlights that open source software is now fundamental, not only for businesses, but developers as well,” says Patrick Masson, General Manager at the OSI. “Like most companies today, Indeed is a user of and contributor to open source software, and interestingly, Indeed's research of resumes shows developers are too, as job seekers highlight open source skills and experience to win today’s most sought after jobs across technology.” As we continue to take a more active role in the open source community, Indeed will seek out additional partnerships, sponsorships, and memberships. For updates on Indeed’s open source projects, visit our open source site . If you’re interested in open source roles at Indeed, visit our hiring page . Open Source at Indeed is Sponsoring the Open Source Initiative---cross-posted on Medium .", "date": "2019-02-12"},
{"website": "Indeed-Engineering", "title": "Build and Learn: Accelerating From New Grad to Experienced Data Scientist", "author": [" by Lucy Guo"], "link": "https://engineering.indeedblog.com/blog/2018/12/new-grad-to-experienced-data-scientist/", "abstract": "A data scientist’s perspective on Indeed’s onboarding program What if -- on your first day at a new job -- you were given three months to build a product that helps people find jobs? (Something Indeed has spent 14 years developing!) This is what I experienced when I attended Indeed University -- Indeed’s onboarding program. At first I felt terrified. How could I contribute as a data scientist? Building a software product, especially during the early stages of development, might not yield enough data for most data science work. Indeed expects full stack data scientists to help with the entire data science process, from collecting data and performing analysis to deploying models in production. But writing React code and participating in discussions about how to build the logging infrastructure -- expectations at Indeed U -- is a whole different story. Fortunately I was able to survive the program. In the process, I gained valuable experience that otherwise I might never have had the chance to acquire. What is Indeed University? Indeed University (IU) is a three-month program for new Indeed employees who are fresh out of college. At the same time that IU gets new hires up to speed, it incubates new and innovative products for the company. Employees come from diverse disciplines, including Engineering, Product, Software Reliability Engineering, Data/Product Science, and Online Marketing. Indeed University Participants and Leads, Seattle 2018 At the start of IU, anyone can pitch their new ideas. All ideas are welcome, as long as they aim to solve real problems for job seekers or employers. People then form teams based on the problems they most want to solve. Diverse teams consist of 3–5 new employees and senior employees as team leads. With a shared vision and real marketing dollars (to access tens of millions of real users), the group builds and tests a new product. Products that prove their value can continue and those who’ve launched them have the chance to create a formal product team. What did I help build? Our team included two software engineers and me, a data scientist. Together, we built a product for job seekers facing career transitions. Our objective was to help these job seekers identify their next potential field. Our product asked users to name their current field, and then recommended new fields that were most relevant. Users were also provided skill requirements, salaries, the percentage of job seekers who have made similar transitions, and related information. What role did I play on the team? While being embedded in Indeed’s product teams means data scientists have opportunities to impact the team’s product decisions, new data scientists don’t usually start leading discussions on designing a product’s framework — or deciding the product’s next big initiatives. We can more typically expect responsibilities like exploratory data analysis, building models and deploying them. In IU, the roles are a lot more flexible. As I anticipated, I designed A/B tests, did test analysis, and helped the team make data-driven decisions. I also acted as product manager, marketing analyst, UX researcher, and part-time front-end engineer. As a product manager, I was responsible for defining and tracking product metrics, and prioritizing work within the team. As a marketing analyst, I owned the marketing campaigns of our product on Google and Facebook as well as Indeed’s internal ads system. I designed the ads, budgeted our spending and made sure that we used our budget on the most effective channels. As a UX researcher, I created and launched surveys to get user feedback on our product. At times I even went out of the office and interviewed people. Why participate in IU as data scientists? Having data scientists at IU brings value to everyone involved. Data scientists have unique experience to offer, and IU gives us valuable first-hand knowledge that can be hard to gain elsewhere. 1. Observing data-driven decision making in action At Indeed, we want all our product decisions to be backed up by data. Through IU, I got a sense of what it means to “A/B test everything.” As my team’s IU product rapidly iterated, we were constantly faced with the question: “What feature should we add to our product next?” The easy answer is “whatever we liked most,” but the correct answer is to “prioritize based on effort and do A/B tests”! We should identify which features give the largest potential impact for the least amount of effort. Only those features that show impact in tests should be kept in production. Rigorous A/B tests require a lot of data science effort, such as defining success metrics, defining A/B test requirements, and doing A/B test analysis. As our product evolved and our user base grew from the data-driven decisions we made, I saw how building a solid product takes engineering effort AND scientific effort. 2. Learning how Indeed.com works Even though our product attracted more than 20K users, we ended up not continuing with it because it was not performing as well as Indeed’s job search. We wanted to keep adding new features and providing as much information as possible to the users, thinking that more is always better. What we found out was that with more features comes a more complicated product. This inevitably means more users lose interest, because they are navigating through an increasingly intricate system. We really learned to appreciate Indeed’s simple yet effective what+where job search interface. It turns out that Indeed really knows how to do its job well! As a more general rule, we found it is often more effective to focus on one feature and make it shine as opposed to building a variety of different features. 3. Learning about the big picture Data scientists’ work often starts with gathering data. Sometimes we might not get to look closely at what is behind all the data. How is it stored? Where does it come from? What architecture is in place for the data to be readily available? Building a product from scratch gives data scientists a chance to view the design and development process from a more holistic level. We are thus able to think about data science questions that derive from this process from a more critical point of view. 4. Building empathy As data scientists, a lot of our work involves effective communication and collaboration with software developers, product managers, and other members of the team. Having been in their shoes, I have a much better understanding of what their work is like, and how a data scientist can make everyone else’s work easier. 5. Having fun! Lastly, we got to have lots of fun! You might spend some late nights in the office — but this hard work is often accompanied by a variety of fun activities. No matter which city hosts IU, you have the opportunity to familiarize yourself with the area. IU schedules all kinds of activities for the teams throughout the entire program. Cruises, room escapes, fancy dinners, Go-Karts, VR challenges… you name it! Just as in formal academic universities, in IU you meet and build close relationships with a group of people from all over the world. You see your ideas transform into real products that benefit real users. You go out of your comfort zone and practice skills that are outside of your expertise. If any of these sound interesting to you, check out our open positions for Data Scientist and Product Scientist at Indeed ! From New Grad to Experienced Data Scientist---cross-posted on Medium .", "date": "2018-12-27"},
{"website": "Indeed-Engineering", "title": "Does Your Job Title Matter?", "author": [" by Zhuying Xu"], "link": "https://engineering.indeedblog.com/blog/2018/12/does-your-job-title-matter/", "abstract": "The Importance of Picking the Right Job Title for Your Job Job titles are often the first interaction between job seekers and employers. As a job seeker searches, they click relevant titles before getting to know the role more deeply through its job description. Calling a job “software engineer” versus “programmer” will likely lead to a different number of applicants and proportion of those meeting the minimum qualifications, but just how different? Surprisingly, after a single word change in nearly identical job titles, we observed more qualified candidates and more total candidates. This post describes our initial research and how we can improve on this in the future. Data and Product Science at Indeed There are two main roles in Indeed’s Data Science organization — data scientists and product scientists. Indeed currently has data/product scientists in five offices: Austin, San Francisco, Seattle, Singapore, and Tokyo, working on a wide variety of product and engineering teams. Both roles employ advanced statistical and machine learning techniques to help people get jobs. Data science has a higher emphasis on machine learning and software engineering, while product science focuses on experiments, analysis, and simpler models that can improve the product. In short, data scientists are closer to software engineering than product management, and vice versa for product scientists. You can view the differences in the job descriptions here: ( Product Scientist / Data Scientist ). Despite their differences, the ultimate requirements for data and product scientists are essentially the same: a deep understanding and experience in mathematics and computer science, and domain expertise. Palmer, Shelly. Data Science for the C-Suite. New York: Digital Living Press, 2015. Print. Conway, Drew. The Data Science Venn Diagram. http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram Sequential test: Changing the job title To find out how job titles affect the hiring process, we conducted an experiment and changed the Product Scientist title to “Data Scientist: Product” in Seattle and “Product Scientist: Data Science” in San Francisco on March 15, while keeping the job title unchanged for Austin. Job descriptions remained the same for all three cities. Engineering work was required for an A/B test, so we chose to look at this sequentially. We conducted a statistical power analysis to determine the sample size ahead of time. We first compared the click-through rate (defined as clicks/impressions) and number of applies for the three cities before and after March 15. From the following two charts, we see both the number of applies and the click-through rate jumped up since March 15 for Seattle and San Francisco (SF). We performed t-tests that show that applies and clickthrough rates are significantly higher for Seattle and San Francisco than for Austin starting from March 15. Click-through Growth Rates in Austin, San Francisco, and Seattle However, changing the job titles might affect the job search ranking, and we know the top and bottom ranked jobs on a page usually have a higher probability of being clicked. In order to account for this position bias, we conducted a logistic regression to predict clicks on page, position on the SERP , city (Austin, Seattle or San Francisco), and whether we changed the job title. We also included the interaction term between city, and if we changed the job title to test the hypothesis that log-odds ratios for various cities are different after changing titles than before changing titles. The regression equation was estimated¹ as follows: The non-parallel lines in the interaction plot below suggest that there are significant interaction effects , which the associated significant p-values for interaction terms confirms. Before changing titles, the equation is simply: Switching from Austin to Seattle yields a change in log odds of -0.18 and to San Francisco yields a change in log odds of -0.09. After changing titles, the equation is: Switching from Austin to Seattle yields a change in log odds of -0.18+0.6 = 0.42 and to San Francisco yields a change in log odds of -0.09+0.71 = 0.62 The graph below also confirms that log-odds ratio for Seattle and San Francisco are much higher after changing titles vs before changing titles. To sum up, we see significantly higher applicants for cities with changed titles. Qualified application model We see more applicants after changing titles, but is this pool of applicants more suitable for the role? A team at Indeed has developed a model that scores the likelihood of a resume containing skills and experiences that meet the requirements in a job description. We applied this model to all candidates who applied for “Product Scientist” (before changing titles) from February 1 to March 14 and got the scores² for each candidate. The mean scores for Austin, Seattle and San Francisco were 0.489, 0.498, and 0.471 respectively. The plot below shows the score Kernel Density Estimation (KDE) for Austin, Seattle, and San Francisco, and the chart shows the p-values (insignificant) for t-tests and Kolmogorov-Smirnov (KS) tests. The KS test tries to determine if two samples are drawn from the same distribution. The test is nonparametric and makes no assumption about the data distribution. Both tests indicate that our applicant qualification rate was at the same level for all three locations before changing titles. When the model was applied to all applicants after changing titles, the mean scores for Austin, Seattle, and San Francisco were 0.466, 0.516, 0.528 respectively. We observed a small decrease in the mean rate for Austin, accompanied by increases in Seattle and San Francisco. The plot below shows the score distributions for Austin, Seattle, and San Francisco. After controlling the False Discovery Rate to adjust for p-values, both tests indicate that applicant qualification rates with changed titles (Seattle and San Francisco) are significantly higher than those with the original title (Austin), while there is no significant difference between different changed titles (Data Scientist: Product and Product Scientist: Data Science). Are you surprised by these findings? Our pilot research shows that simply making small changes to job titles led to more and better qualified candidates for Indeed. Job titles do matter, more than you think — they are great attention catchers and a prime focus as much as the job descriptions. So, you should care about your job titles and pick ones that can be noticed and easily stand out for job seekers. For further reading, more rigorous approaches to establishing causal effect include: Randomized experimental design that you can deliberately change one or more factors in order to observe the effect of factors on one or more response variables. Causal inference models such as structural equation modeling ³ and Rubin causal model ⁴ can be used to analyze causal effect statistically in both observational and experimental studies. If you are interested in using the scientific method to improve or develop products and help people get jobs, check out our open Product Scientist and Data Scientist positions at Indeed ! This is the second article in our ongoing series about Data Science from Indeed. The first article is There's No Such Thing as a Data Scientist from our colleague, Clint Chegin . Footnotes: 1. P-value for the hypothesis test for which the Z value is the test statistic. It tells you the probability of a test statistic being at least as unusual as the one you obtained, if the null hypothesis were true (the coefficient is zero). If this probability is low, it suggests that it would be rare to get a result as unusual as this if the coefficient were really zero. Signif. code is associated to each estimate and is only intended to flag levels of significance. The more asterisks, the more significant p-values are. For example, three asterisks represent a highly significant p-value (if p-value is less than 0.001). 2. These model scores are non-standardized and not probabilities. An application score of 0.8 represents a higher likelihood relative to an application with a score of 0.4 (but doesn’t mean twice as likely). 3. Bollen, K.A.; Pearl, J. (2013). “Eight Myths about Causality and Structural Equation Models”. In Morgan, S.L. Handbook of Causal Analysis for Social Research . Dordrecht: Springer. pp. 301–328. 4. Sekhon, Jasjeet (2007). “The Neyman–Rubin Model of Causal Inference and Estimation via Matching Methods” (PDF). The Oxford Handbook of Political Methodology . Cross-posted on Medium .", "date": "2018-12-11"},
{"website": "Indeed-Engineering", "title": "Qualitative + Quantitative: How Qualitative Methods Support Better Data Science", "author": [" by Robyn Rap and Vicky Zhang"], "link": "https://engineering.indeedblog.com/blog/2018/12/qualitative-methods-for-better-data-science/", "abstract": "Have you ever been embarrassed by the first iteration of one of your machine learning projects, where you didn’t include obvious and important features? In the practical hustle and bustle of trying to build models, we can often forget about the observation step in the scientific method and jump straight to hypothesis testing. Data scientists and their models can benefit greatly from qualitative methods. Without doing qualitative research, data scientists risk making assumptions about how users behave. These assumptions could lead to: neglecting critical parameters, missing a vital opportunity to empathize with those using our products, or misinterpreting data. In this post, we’ll explore how qualitative methods can help all data scientists build better models, using a case study of Indeed’s new lead routing machine learning model, which ultimately generated several million dollars in revenue. What are qualitative methods and how are they different from quantitative methods? Few data scientists are formally trained in qualitative methods. They’re more deeply familiar with quantitative methods like A/B testing, surveys, and regressions. Quantitative methods are great for answering questions like “How much does the average small business spend on a job posting?”, “ What are the skills that make someone a data scientist? ”, or even “ How many licks does it take to get to the center of a Tootsie roll pop? ” (The answer is 3. Three licks.) But there are some questions that quantitative methods can’t answer, such as “Why do account executives reach out to this lead instead of that lead?” or “How do small businesses make the decision to sponsor or not sponsor a job?” Or the truly deep question: “Why do you want to get to the center of the Tootsie roll pop?” To answer these questions, qualitative researchers rely on methods like in-depth interviews, participant observation, content analysis and usability studies. These methods involve more direct contact with who and what you’re studying. They allow you to better understand how and why people do what they do, and what kinds of meaning they ascribe to different behaviors. Put another way, quantitative methods can tell you the what , the how much , or how often ; qualitative methods can tell you the why or the how . Cartoon created by Indeed UX Research Manager Dave Yeats using cmx.io Why should you use qualitative methods? A case study in Lead Generation Our Lead Generation team recently benefited greatly from the use of qualitative methods. When an employer posts a job, it represents a revenue opportunity for Indeed. We route that employer to an account executive, who then reaches out and helps the employer set an advertising budget to sponsor their job. This increases the job’s visibility and therefore the velocity at which they make a successful hire. Employers who have not yet spent with us on Indeed are referred to as “leads.” Some leads are better than others, however. We wanted to be able to give leads a score on a scale from one to five stars that would indicate our best estimate for whether or not they would spend. Our Product Science team decided to build a machine learning model that would score leads and route them more effectively. But where to start? Prior to this project, we had little experience with lead scoring and little intuition about what a good lead would look like. How could we even know what features should be in our model? To answer that question, we turned to people with the most hands-on experience with leads: account executives themselves. Not only are they experts on what makes a good lead, they would also be the beneficiaries of our efforts. We took a three-pronged qualitative approach: Observation. To learn about the day-to-day sales experience, each member of our team shadowed different reps and listened to them on sales calls. We observed how they would select which lead to call, how they would decide what to talk about on the call, and how they actually made deals. Interviews. We sat down with several sales managers and representatives across the company and asked them questions about leads they had previously decided to call or drop, like “How do you pick which leads to call first?” or “Why did you decide to drop this lead?” Content Analysis. We combed through thousands of open-ended responses to a company-wide survey of account executives to better understand their pain points with regards to leads. We learned a lot! Just by doing three simple qualitative studies for a few hours, we collected a long list of potential features. Had we not sat down next to members of the sales team and observed as they worked, we would have never obtained these insights. Our next step was to start digging into the data and validating how generalizable the findings from reps were. With the intuition we gained from our qualitative studies on account executives’ behaviors and thought processes, we ultimately built a machine learning model that generated millions of dollars in annual incremental revenue. And we didn’t stop there: we kept interviewing and shadowing reps to get their feedback on the model. We built a new version that generated a additional annual incremental revenue. And we made sure to market our new model so people knew about it. In short, these qualitative studies kept us grounded and built empathy with our end users. Without qualitative studies, the models we built would have been out of touch with reality and made it harder for us to address our users’ needs. With qualitative methods, we infused our models with intuition and working hypotheses that we could later verify with quantitative data. Where to start learning the basics for qualitative methods In the case study above, our end users were our coworkers here at Indeed. It’s worth noting that it’s not always as simple to conduct qualitative studies with external users. Here at Indeed, we have a fantastic UX Qualitative Research team to turn to for these kinds of studies. We encourage you to reach out to such teams at your own companies, and if they don’t exist yet, create them. Work with them. Shadow them. Buy them a beer. They are wonderful! But don’t just stop there. Below are some of our favorite readings and resources on qualitative methods, recommended by former academics here at Indeed . “When to Use Which User-Experience Research Methods” — A great article by the Nielsen Norman group on identifying the right method for the research question at hand. Learning from Strangers — A classic guide on how to ask questions in an in-depth interview. “How to Conduct User Interviews” — A shorter guide geared toward industry and product development. “5 Steps to Create Good User Interview Questions” — A great Medium post on avoiding biased or leading questions in in-depth interviews. Writing Ethnographic Field Notes — The seminal work on how to collect details during observational studies. Geared toward anthropological ethnographies, but with a lot of great tips for being more aware of details in day-to-day interactions as well. Salsa Dancing in the Social Sciences — While arguably one of the weirdest book titles, this is an enjoyable and approachable overview of the benefits of qualitative methods. Don’t Make Me Think — Steve Krug focuses primarily on usability, but his book offers good tips for observing how people interact with websites. If you are passionate about methods and data science, check out product science and data science jobs at Indeed ! How Qualitative Methods Support Better Data Science---cross-posted on Medium .", "date": "2018-12-12"},
{"website": "Indeed-Engineering", "title": "Time-Tested: 7 Ways to Improve Velocity When A/B Testing a New UX", "author": [" by Robyn Rap"], "link": "https://engineering.indeedblog.com/blog/2019/08/velocity-a-b-testing-ux/", "abstract": "A/B testing holistic redesigns can be tough. Here at Indeed, we learned this firsthand when we tried to take our UX from this to this: The Indeed mobile Search Engine Results Page (SERP) circa mid-2017 (left) and circa mid-2018 (right) Detailed description of image The before and after image includes two screenshots: one of the Search Engine Results Page mid-2017 and one of the Search Engine Results Page mid-2018. SERP circa mid-2017: Each search result includes a job title, an associated employer, a rating, a review count, a location, an \"Apply via phone\" indicator if applicable, and a label indicating whether or not the job is sponsored. The job title is on the first line of the result, in the largest font, in bold, and colored blue for emphasis. If the job title is new, a \"new\" indicator in orange is present. In the second line of the result, from left to right, is the employer in a smaller blue font, a rating on a five-star scale, and the number of reviews in blue. At times, the location of the position is on the second line of the result. Other times, the location of the position is on the third line depending on the size of the screen and how much information is on the second line. The font is black. On the fourth line of the result, there is an estimated salary range if available. The font is the same size as the second line. On the very bottom of the result, there is an indication of whether the job in sponsored (in red) or when the result was first posted. Each result is in a white rectangle separated by a thick grey border. They are stacked vertically. On the top right of each result is a heart icon. Upon tapping it, it saves the result for later. SERP circa mid-2018: Each search result includes a job title, an associated employer, a rating, a review count, a location, an \"Apply via phone\" indicator if applicable, and a label indicating whether or not the job is sponsored. The job title is on the first line of the result, in the largest font, in bold, and colored black for emphasis. If the job title is new, a \"new\" indicator in red is present. In the second line of the result, from left to right, is the employer in a smaller black font, a rating on a five-star scale, and the number of reviews. In comparison to the 2017 SERP, the font is a bit larger. In the third line of the result, the location of the position is present. The font is black. On the fourth line of the result, there is an estimated salary range if available. The font is grey and smaller than the second line. On the very bottom of the result, there is an indication of whether the job in sponsored (in red) or when the result was first posted. Each result is in a white rectangle separated by a thin grey border. They are stacked vertically. On the top right of each result is a heart icon. Upon tapping it, it saves the result for later. Things didn’t go so hot. We’d spent months and months coming up with a beautiful design vision, and then months and months trying to test all of these changes all at once. We had a bunch of metrics move (mostly down) and it was super confusing because we couldn’t figure out what UI changes caused what effects. So, we took a new approach. In the middle of 2018, we founded the Job Search UI Lab, a cross-functional team with one goal: to scientifically test as many individual UI elements as we could to understand the levers on our job search experience. In just the last 12 months, our team ran over 52 tests with over 502 groups. We’ve since used our learnings to successfully overhaul the Job Search UX on both desktop and mobile browsers. In this blog post, we share some of the A/B test accelerating approaches we incorporated in the JSUI Lab — approaches that garnered us the 2018 Indeed Engineering Innovation Award. Whether you’re interested in doing a UX overhaul or just trying out a new feature, we think you can incorporate each of these tips into your A/B testing, too! #1: Have a healthy backlog No one should ever be waiting around to start development on a new test. Having a healthy backlog of prioritized A/B tests for developers helps you roll out A/B tests one after the other. One way that the JSUI Lab creates a backlog is by gathering all of our teammates — regardless of their role — at the beginning of each quarter to brainstorm tests. We pull up the current UX on mobile and desktop and ask questions about how each element or feature works. Each question or idea ends up on its own sticky note . We end up with over 40 test ideas, which we then prioritize based off of how each test might address a job seeker pain point or improve the design system while minimizing effort. And while we may not get to every test in our backlog, we never have to worry about not having tests lined up. #2: Write down hypotheses ahead of time In the hustle and bustle of product development, sometimes experimenters don’t take the time to specify their hypotheses for a given A/B test ahead of time. Sure, not writing hypotheses may save you 10–30 minutes up front. But this can come back to bite you once the test is completed , when your team is looking at dozens of metrics and trying to make a decision about what to do next. Not only is it confusing to see some metrics go up while others go down, chances are you’re probably also seeing some false positives (also known as Type I error) the more metrics you look at. You may even catch yourself looking at metrics that wouldn’t feasibly be affected by your test (e.g., “How does changing this UI element from orange to yellow affect whether or not a job seeker gets a call back from an employer?!”). So do yourself a solid. Pick 3–4 metrics that your test could reasonably be expected to move, conduct a power analysis for each one, and write down your hypotheses for the test ahead of time. #3: Test UI elements one at a time This one’s a little counterintuitive. It might seem like it would increase the amount of time it would take to do a UX holistic redesign by testing each and every UI element separately. But by testing elements one at a time, the conclusions about our tests were more sound. Why? Because we could more clearly establish causality. Consequently, we were able to take all of the learnings from our tests and roll them into one big test that we were fairly confident would perform well. Rather than see metrics tank like the first time we did a holistic design test, we actually saw some of Indeed’s biggest user engagement wins for 2018, in less than half the time of the first attempt. By running tests on UI elements one at a time, we were able to iterate on our design vision in a data-driven way and set up our holistic test for success. Indeed's mobile SERP circa mid-2019 Detailed description of image Each search result includes a job title, an associated employer, a rating, a review count, a location, an \"Apply via phone\" indicator if applicable, and a label indicating whether or not the job is sponsored. The job title is on the first line of the result, in the largest font, in bold, and colored black for emphasis. If the job title is new, a \"new\" indicator is present. Unlike the SERP circa mid-2018, this indicator has a black font with a light red highlight. In the second line of the result, from left to right, is the employer in a smaller black font and a rating. However, instead of showing all five stars like on the SERP circa mid-2018, the rating is conveyed by a number with a yellow star to the right. In the third line of the result, the location of the position is present. The font is black. On the fourth line of the result, there is an estimated salary range if available. The font is black with a pastel green highlight. On the fifth line of the result, the \"Apply via phone\" indicator is present in red. There are also indicators noting whether you can be an early applicant or if the employer is responsive. These types of indicators are new. On the very bottom of the result, there is an indication of when the result was first posted. Each result is in a white rectangle separated by a thin grey border. They are stacked vertically. On the top right of each result is a heart icon. Upon tapping it, it saves the result for later. So, what do these tests look like in practice? Below are a few examples of some of the groups we ran. You’ll notice that the only real difference between the treatments is a minor change, like font size or spacing. #4: Consider multivariate tests Multivariate tests (sometimes referred to as “factorial tests”) test all possible combinations of each of the factors of interest in your A/B test. So, in a way, they’re more like an A/B/C/D/E/… test! What’s cool about multivariate tests is that you end up with winning combinations that you would have missed had you tested each factor one at a time. An example from the JSUI Lab illustrates this benefit. We knew from UX research that our job seekers really cared about salary when making the decision to learn more about a job. In 2018, this was how we displayed salary on each result: We wanted to see if increasing the visual prominence using color, font size, and bolding would increase job seeker engagement with search results. So, we developed four font size variants, four color variants, and two variants that were bolded or unbolded. We ended up with 4x4x2 groups for 32 total groups including control. While multivariate tests can speed up how you draw conclusions about different UI elements, they’re not without their drawbacks. First and foremost, you’ll need to weigh the tradeoffs to statistical power, or the likelihood that you’ll detect a given effect if one actually exists (also known as Type II error). Without sufficient statistical power, you risk not detecting an effect of your test if there is one. Power calculations are a closed-form equation that require your product team to make tradeoffs between your α -level and β -level of choice, your sample size ( n ), and the effect size you care about your treatment having ( p1 ). On Indeed, we have the benefit of having over 220M+ unique users each month. That level of traffic may not be available to you and your team. So, to have sufficient statistical power, you’ll potentially need to run your experiment for longer, run groups at higher allocations, cut some groups, or be willing to introduce more Type I error, depending on how small of an effect you’d like to confidently detect. n =[( z 1-α + z 1-β )/(( P 0 - P 1 )/√ P 1 (1- P 1 ))] The closed form calculation is a power test between two proportions With a typical A/B test, it’s usually relatively straightforward to analyze with a t-test. Multivariate tests, however, will benefit from multivariate regression models, which will allow you to suss out the effects of particular variables and their interaction effects. Here’s a simplified regression equation: ŷ = β 0 +β 1 x 1+β 2 x 2 +β 3 x 3 + e And an example of a regression equation for one of the tests we ran that modified both font size and the spacing on the job card: P(click = 1) = β 0 +β 1 (FontSize)+β 2 (Spacing) +β 3 (FontSize×Spacing)+ e Another caveat of multivariate tests is that they can quickly become infeasible. If we had 10 factors with 2 levels each, we’d have a 2^10 multivariate test, with a whopping 1,024 test groups. In cases like these, running what’s called a fractional factorial experiment might make more sense. Finally, multivariate tests may sometimes yield some zany combinations. In our salary example above, our UX Design team was mildly mortified when we introduced the salary variant with 16pt, green, and bolded font. We lovingly referred to this variant as “the Hulk.” In some cases, it may not be feasible to run a variant due to accessibility concerns. In the JSUI Lab, we determine on a case-by-case basis whether the tradeoff of statistical rigor is worth a temporarily poor user experience. #5: Deploy CSS and JavaScript changes differently Sometimes a typical deploy cycle can get in the way of testing new features quickly. At Indeed, we developed a tool called CrashTest, which allows us to sidestep the deploy cycle. CrashTest relies on a separate code base of CSS and JavaScript files that are injected into “hooks” in our main code base. While installing CrashTest hooks follows the standard deploy, once hooks are set up, we can inject new CSS and JavaScript treatments and see the changes reflected in our product in just a few minutes. In the JSUI Lab, we rely on our design technologist Christina to quickly develop CSS and JavaScript treatments for dozens of groups at a time. With CrashTest, Christina can develop her features and get them QAed by Cory. We can push them into production that same day using our open source experimentation management platform Proctor . Had we relied on the typical deploy cycle, it would have taken Christina’s work several more days to be seen by job seekers, and that much more time until we had results from our A/B tests. #6: Have a democratized experimentation platform Combing through logs and tables to figure out how your tests performed is not the best use of time. Instead, consider building or buying an experimentation platform for your team. As a data-driven company, Indeed has an internal tool for this called TestStats. The tool displays how each test group performed on key metrics and whether the test has enough statistical power to draw meaningful conclusions at the predetermined effect size. This makes it easy to share and discuss results with others. #7: Level up everyone’s skills through cross-training On the JSUI team, we firmly believe that allowing everyone to contribute to team decisions equally helps our team function better . Our teammates include product managers, UX designers, QA engineers, data scientists, program managers, and design technologists. Each of us brings a unique background to the team. Teaching each other the skills we use in our day-to-day jobs helps increase velocity for our A/B tests because we’re able to talk one another’s language more readily. For instance, I’m a product scientist , and led a training on A/B testing. This allowed all of the other members of JSUI Lab to feel more empowered to make test design decisions without my direct guidance every time. Our UX designer Katie shadowed our product managers CJ and Kevin as they turned on tests. Katie now turns on tests herself. Not only does this kind of cross-training reduce the “bus factor” on your team, it can also be a great way of helping your teammates master their subject and improve their confidence in their own expertise. Now it’s time to test! Whether you take only one or two tips or all seven, they can be a great way of improving your velocity when running A/B tests. The Job Search UI Lab has already started sharing these simple steps with other teams at Indeed. We think they’re more broadly applicable to other companies and hope you’ll give them a try, too. And if you're passionate about A/B testing methods, Indeed’s hiring! Improving Velocity When A/B Testing a New UX---cross-posted on Medium .", "date": "2019-08-26"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Proud Sponsor of FOSS Asia", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2019/03/indeed-open-source-foss-asia-sponsor/", "abstract": "The Indeed Open Source Program Office is excited to be at FOSS Asia in Singapore March 15-17, 2019. This is our second consecutive year at FOSS Asia. If you’re attending, visit our booth and check out three talks from Indeed’s Head of Open Source, Duane O’Brien. Sustaining FOSS Projects by Democratizing the Sponsorship Process Friday, March 15 | 2.30pm | Hackstage Speaker: Duane O'Brien Within a given company, typically only a few people are involved in deciding which FOSS projects and initiatives to support financially. This year we decided to change all that and democratize the decision making process. We set up an internal FOSS Sustainability Fund, and invited everyone to participate. This talk will examine how we got executive buy-in for the fund, how the fund was set up, how we encouraged participation, and what the impact has been so far. Panel: FOSS Around the World Sunday, March 17 | 11:05am | Lecture Theatre How is the state of FOSS around the world? Open Techies from across the globe share their insights. Learn from our panel: Duane O'Brien -- Head of Open Source, Indeed Hong Phuc -- Founder, FOSSASIA Michael Cheng -- Licensing & Open Source Legal, Facebook Roland Turner -- Chief Privacy Officer, Labs Director, TrustSphere Gerry Demaret -- Organizer, FOSDEM Don't judge candidates by their GitHub profiles: How open source participation plays into hiring Sunday, March 17 | 12:00pm | Lecture Theatre Speaker: Duane O'Brien You've heard that having a GitHub profile can help when job hunting. Is it true? How can you get started? What if you don’t have the time? You're hiring to fill a position, and you have two great candidates. One has a robust history of working in open source, and the other has no history at all. Is this an easy choice? This session will look at the implications of working in open source---from the perspectives of the candidate and the hiring manager. Individuals will come away with an understanding of how open source participation affects their hireability; how they can get started; and what to do when active participation is challenging. Hiring managers will learn how to evaluate the open source work of potential candidates; how to evaluate candidates with no history; and how requiring an open source footprint creates harmful bias. Speaker profile Duane is passionate about enabling smart and meaningful contributions to the open source ecosystem. He navigates the path between engineering and management, drawing on both his experience as a developer and program manager at companies, as well as his experience supporting Agile transformations. Open Source at Indeed is a Proud Sponsor of FOSS Asia---cross-posted on Medium .", "date": "2019-03-08"},
{"website": "Indeed-Engineering", "title": "The Evolving Language of Data Science ", "author": [" by James Beach"], "link": "https://engineering.indeedblog.com/blog/2019/08/the-evolving-language-of-data-science/", "abstract": "...or Grokking the Bokeh of Scarse Meaning Increasement \"You keep using that word. I do not think it means what you think it means.\" -- Dr. Inigo Montoya I’m a technical writer at Indeed. One of the many great things about my job is that I get to work with smart people every day. A fair amount of that work involves translating between them. They will all be speaking English, but still might not understand each other. This is a natural consequence of how knowledge advances in general, and how English develops in particular. As disciplines evolve, alternate meanings and new words develop to match. That can extend to creating new phrases to name the disciplines themselves (for example, what is a data scientist? ). English’s adoption of such new words and meanings has always been pragmatic. Other Western languages have more formal approval processes , such as French’s Académie française and German’s reliance on a single prestigious dictionary . The closest to formal authorities for correct English are popular dictionaries such as the Oxford English Dictionary, the American Heritage Dictionary, and Merriam-Webster. None of them reign supreme. This informal adoption of new words and meanings can lead to entire conversations in which people don’t realize they’re discussing different things. For example, consider another recently adopted word: “bokeh.” This started as a term in the dialect of professional photography, for the aesthetically pleasing blurred look that strong depth of field can give a picture. “Bokeh” is also the name for a specific python data visualization package . So “bokeh” may already be headed for a new meaning within the realm of data science. As a further example of the fluid nature of English, \"bokeh\" comes from the Japanese word boke (暈け or ボケ) . In its original form it meant “intentional blurring,” as well as sometimes “mental haze,” i.e., confusion. Bokeh of flowers by Sergei Akulich on Unsplash Data science bokeh — https://bokeh.pydata.org/ The clouded meaning of “data” A data scientist told me that when she hears “the data” she tends to think of a large amount of information, a set large enough to be comprehensive. She was surprised to see another team’s presentation of  “the data” turn out to be a small table inside a spreadsheet that listed a few numbers. This term can also cause confusion between technical fields. Data scientists often interpret “data” as quantitative , while UX researchers interpret “data” as qualitative . Exploring evolving language with Ngram Viewer A product science colleague introduced me to the Google Books Ngram Viewer . It’s a search engine that shows how often a word or phrase occurs in the mass of print books Google has scanned. Google’s collection contains most books published in English from AD 1500 to 2008. I entered some new words that I had come across, and screened out occurrences that weren’t relevant, such as place or person names and abbreviations. I also set the search to start from 1800. Medieval data science could be interesting, but I expect it to be “scarse.” (That’s not a typo.) Features When I first came across this newer meaning of “features,\" I wasn’t even aware that it had changed. From previous work with software development and UX, I took “features” to mean “aspects of a product that a user will hopefully find useful.” But in data science, a “feature\" relates to covariates in a model . In less technical English, a measurable property or characteristic of a phenomenon being observed. This dual meaning led me to a fair amount of head-scratching when I was documenting an internal data science application. The application had software features for defining and manipulating data features. The following Ngram graph indicates this emerging meaning for “feature” by tracking the emergence of a related phrase, “ model feature .” The usage of the term \"model feature\" peaks sometime in the 1990s. Diving into Ngram’s specific citations, the earliest mention I can find that’s near this meaning is in 1954. Interestingly, it’s from a book on management science : The next use that seems exact turns up in 1969, in the Digest Record from Association for Computing Machinery, Society for Industrial and Applied Mathematics, Institute of Electrical and Electronics Engineers. Leaving aside the intervening comma, the example is so dead-on that I wonder if we’re looking at near the exact moment this new meaning was fully born: To grok “Grok” is an example of English going so far as to steal words from languages that don’t even exist . Robert A. Heinlein coined the word in his 1961 science fiction classic Stranger in a Strange Land . In the novel, the Martian phrase “grok” literally means “drink” and metaphorically means “understanding something so completely that you and it are one.” Usage of the term \"grok\" increases starting in the 1960s. Like many other aspects of science fiction and fantasy, computer programming culture absorbed the term. The Jargon File from 1983 shares an early defined example: GROK (grahk) verb. To understand, usually in a global sense especially, to understand all the implications and consequences of making a change. Example: \"JONL is the only one who groks the MACLISP compiler.\" Since then, computer jargon has absorbed “grok” and applied it in many different ways. One immediate example is the source code and reference engine OpenGrok . It’s intended to let users “grok (profoundly understand) source code and is developed in the open.” Salt Salt is an example of a common word that has gone through two steps of technical change. First it gained a meaning relating to information security, and then an additional one in data science. As a verb and noun, “salt” originally meant what it sounds like - adding the substance chemically known as NaCl to food for flavoring and preservation. It gained what is perhaps its better-known technical meaning in information security. Adding “salt” to password hashing makes encrypted passwords more difficult to crack. In the word’s further and more recent permutations in data science, “salt” and “resalt” mean to partly randomize the results of an experiment by shuffling them. The following Ngram graph tracks the association of “salt” and “resalt” over time. This was hard to parse out, and required diving deeply into Ngram's options . I ended up graphing the different times “salt” modifies the words “food,” “password,” or “data.” Google stopped scanning in new books in 2008 - you can see the barest beginning of this new usage in 2007. From 2000 to 2008, salt in the context of food is most used, followed by salt in the information security sense. Pickling Traditionally “pickling” refers to another way to treat food, this one almost entirely for preservation. In Python, this refers to the object serialization method made possible by the Pickle module . Data scientists have found increasing use for this term , in ways too recent to find on Ngram. The bleeding edge of language? Here are some words that may just be in the sprouting stage of wider usage. Scarse This came from an accidental jumble of words in a meeting, and has remained in use since. It describes situations where data is both scarce (there’s not a lot of it) and sparse (even when there is some, it’s pretty thin). This meaning for “scarse” doesn’t appear in the Ngram graph . So it appears we’re seeing mutation and evolution in word form in the wild. Will it take root and prosper, continuing to evolve? Only time will tell. Increasement “We should look for the source of that error message increasement.” I’ve observed this word once in the wild–from me. “Increasement” came to me in a meeting, as a word for the amount of an increase over time. I had never used the word before. It just seemed like a word that could exist. It had meaning similar to other words, and fit those other words’ rules of word construction. In the context I used, its meaning isn’t exactly the same as “increment.” Increment refers to a specific numeric increase. One wouldn’t refer, for example, to an increasing amount of users as an increment. You might, however, refer to it as an increasement. Searching for increasement in Ngram revealed that this word previously existed but fell out of common usage, as shown on the following graph. The word \"increasement\" tapers in usage, with its peak in the 1810s, followed by a gradual decline. Previous examples: Book: The Fathers of the English Church Paul was, that he should return again to these Philippians, and abide, and continue amongst them, and that to their profit; both to the increasement of their faith Book: The Harleian miscellany; or, A collection of ... pamphlets and tracts ... in the late earl of Oxford's library ….when she saw the man grown settled and staid, gave him an assistance, and advanced him to the treasurership, where he made amends to his house, for his mis-spent time, both in the increasement of his estate and honour... Perhaps it’s time for “increasement” to be rebooted into common use? Bottom line Language is likely to continue evolving as long as we use language. Words in general, and English words in particular, and words in English technical dialects above all, are in a constant state of flux. Just like the many fields of knowledge they discuss. So if you’re in a technical discussion and others’ responses aren’t quite what you expect, consider re-examining the technical phrases you’re using. The people you’re talking with might grok those words quite differently. The Evolving Language of Data Science---cross-posted on Medium .", "date": "2019-08-22"},
{"website": "Indeed-Engineering", "title": "Recognize Class Imbalance with Baselines and Better Metrics", "author": [" by Samuel Taylor"], "link": "https://engineering.indeedblog.com/blog/2019/08/recognizing-class-imbalance/", "abstract": "In my first machine learning course as an undergrad, I built a recommender system. Using a dataset from a social music website, I created a model to predict whether a given user would like a given artist. I was thrilled when initial experiments showed that for 99% of the points in my dataset, I gave the correct rating – I was wrong only 1% of the time! When I proudly shared the results with my professor, he revealed that I wasn't, in fact, a machine learning prodigy. I'd made a mistake called the base rate fallacy . The dataset I used exhibited a high degree of class imbalance . In other words, for 99% of the pairs between user and artist, the user did not like the artist. This makes sense: there are many, many musicians in the world, and it's unlikely that one person has even heard of half of them (let alone actually enjoys them). When we're unprepared for it, class imbalance introduces problems by producing misleading metrics. The undergrad version of me ran face-first into this problem: accuracy alone tells us almost nothing. A trivial model that predicts that no users like any artists can achieve 99% accuracy, but it's completely worthless. Using accuracy as a metric assumes that all errors are equally costly; this is frequently not the case. Consider a medical example. If we incorrectly classify a tumor as malignant and request further screening, the cost of that error is worry for the patient and time for the hospital workers. By contrast, if we incorrectly state that a tumor is benign when it is in fact malignant, the patient may die. Examine the distribution of classes Moving beyond accuracy, there are a number of metrics to think about in an imbalanced problem. Knowing the distribution of classes is the first line of defense. As a rule of thumb, Prati, Batista, and Silva find that class imbalance doesn't significantly harm performance in cases where the minority class makes up 10% or more of the dataset. If you find that your dataset is imbalanced more than this, pay special attention. I recommend starting with an incredibly simple model: pick the most frequent class. scikit-learn implements this in the DummyClassifier . Had I done this with my music recommendation project, I would quickly have noticed that my fancy model wasn't really learning anything. Evaluate the cost In an ideal world, we could calculate the exact costs of a false negative and a false positive. When evaluating our models, we could multiply those costs by the false negative and false positive rates to come up with a number that describes the cost of our model. Unfortunately, these costs are often unknown in the real world, and improving the false positive rate usually harms the true positive rate. To visualize this tradeoff, we can use an ROC curve . Most classifiers can output probability of membership in a certain class. If we choose a threshold (50%, for example), we can declare that all points with probability over the threshold are members of the positive class. Varying the threshold from a low percentage to a high percentage produces different ways of classifying points that have different true positive and false positive rates. Plotting the false positive rate on the x-axis and the true positive rate on the y-axis, we get an ROC curve. As an example, I trained a classifier on the yeast3 dataset from KEEL and created an ROC curve: After training the model, the curve shows a likely chance for true positives. While we could certainly write the code to draw an ROC curve, the yellowbrick library has this capability built in (and it's compatible with scikit-learn models). These curves can suggest where to set the threshold for our model. Further, we can use the area under them to compare multiple models (though there are times when this isn't a good metric ). The next time you're working on a machine learning problem, consider the distribution of the target variable. A huge first step towards solving class imbalance is recognizing the problem. By using better metrics and visualizations, we can start to talk about imbalanced problems much more clearly. More on class imbalance In my upcoming talk at ODSC West , I’ll dive deeper into the causes of class imbalance. I’ll also explore different ways to address this error. I hope to see you in October! Class Imbalance---cross-posted on Medium .", "date": "2019-08-02"},
{"website": "Indeed-Engineering", "title": "You Probably Have Missing Data", "author": [" by Amy Westmoreland"], "link": "https://engineering.indeedblog.com/blog/2019/08/you-probably-have-missing-data/", "abstract": "Here’s a Guide on When to Care Strategies to address missing data¹ At Indeed, our mission is to help people get jobs. Searching for a job can be stressful , which is one reason why Indeed is always looking for ways to make the process easier and our products better. Surveys provide us with an ongoing measure of people’s feelings about Indeed’s products and the job search experience. We realize that when someone is looking for a job (or has just landed one ), answering a survey is the last thing they want to do. This means that a lot of the survey data that Indeed collects ends up with missing data. To properly analyze user satisfaction and similar surveys, we need to account for potential missing patterns to ensure we draw correct conclusions. I’d like to discuss identifying and handling missing data. I’m inspired by my training in the University of Michigan’s Program in Survey Methods . I’ve also wanted to apply the theories about data sets that I learned in academia to Indeed’s terabyte-sized data. I recently worked on a project that dealt with missing data. I learned a lot from the analysis. Walking through this process can show how Indeed collects survey data, illustrate the difference between non-response rate and non-response bias, and provide examples of why “randomness” in non-response bias is a good thing. One quick note : While the examples in this blog post reference Indeed, all data in this blog post are entirely bogus and made up by the author (aka me). Measuring hires at Indeed If you have ever canceled a job alert from Indeed, you might have seen this survey: The purpose of this survey is to determine whether a job seeker is canceling their job alert because they found a job. This information helps us improve our products and enables us to celebrate the success stories of job seekers. One challenge with this survey is that only a subset of job seekers completes it. From a user perspective this makes sense — people who unsubscribe from an email notification probably don’t want to spend time answering a survey. This means that we end up with a substantial amount of missing data, especially regarding a key question: did they unsubscribe because they got a job? Non-response rate vs non-response bias When discussing missing data, people often conflate response rate with non-response bias . When this misunderstanding of response rate is further conflated with the question of data quality, people might assume that a higher response rate means higher quality survey responses. This is not necessarily the case. For the following job alert cancellation survey results, you’ll note that 13.8% did not respond. Detailed description of chart. This chart shows survey response results, including the total count of responses and percentage of whole. According to this chart: 49,109 survey participants (33.99%) selected \"I found a job on Indeed\" 43,256 survey participants (29.94%) selected \"I found a job elsewhere.\" 32,105 survey participants (22.22%) selected \"Other.\" 20,000 users (13.84%) did not respond to the survey Does a non-response rate of 13.8% say something about the quality of responses in the survey? The short answer is no . While this might initially sound counterintuitive, stay with me! Imagine that Indeed revised the job alert cancellation survey to include a “prefer not to say” option. After collecting data for a few weeks, we would then see that only 5.8% of job seekers didn’t respond to the revised survey. Detailed description of charts. In the original survey response results: 49,109 survey participants (33.99%) selected \"I found a job on Indeed\" 43,256 survey participants (29.94%) selected \"I found a job elsewhere\" 32,105 survey participants (22.22%) selected \"Other\" 20,000 users (13.84%) did not respond to the survey With the addition of the new \"Prefer not to say\" survey option, the results change to the following: 49,109 survey participants (14.31%) selected \"I found a job on Indeed\" 43,256 survey participants (12.60%) selected \"I found a job elsewhere\" 32,105 survey participants (9.35%) selected \"Other\" 20,000 users (5.83%) did not respond to the survey 198,772 survey participants (57.91%) selected \"Prefer not to say\" Does this mean an increase in useful data? Before you start celebrating an 8% decrease in non-response, take a closer look at the response distribution. You’ll notice that a whopping 57% of job seekers selected “prefer not to say”! Typically, we treat the response option of “prefer not to say” as missing data. We don’t know if job seekers selected “prefer not to say” because they are in the process of finalizing an offer, or for some other reason, such as concern that their current employer might find out they have a competing offer. If so, there is a potential for response bias. Response bias refers to bias towards selecting a certain response (e.g., “prefer not to say’’) due to social desirability or another pressure. Non-response bias, also known as participation bias, refers to refusing to respond to the survey because of social desirability or another pressure. The example above shows response bias, because respondents may have selected “prefer not to say” due to the sensitive nature of the question. If the respondents hadn’t completed the survey at all due to the nature of the survey, we would have non-response bias. This illustrates that non-response rate alone (i.e., the percentage of people who responded to your survey) is not the sole indicator of data quality. For example, if your most recent survey has a 61% response rate while past surveys had a response rate of 80–90%, there’s probably enough rationale to look into potential problems associated with non-response rate. However, if your recent survey has a 4% response rate and past surveys had a response rate of 3–5%, it’s unlikely that there’s a non-response issue with your specific survey. Instead, perhaps your team’s strategy in how surveys are sent (e.g., collecting survey data by landlines versus mobile phones) or how participants are identified for your study (e.g., using outdated and/or incorrect contact information) is leading to low response rates overall. Whether you have a 3% or 61% response rate, response rate is not synonymous with low-response bias. As we saw with the revised survey, even when the response rate was high, over half of the respondents still selected “prefer not to say” — a response that isn’t usable for data analysis. In addition to paying attention to the number of people who responded to your survey, you also need to check the distribution of the responses to each question in your survey. Simple and easy frequency statistics are a great way to notice oddities and potential biases in your data. Missing at random vs missing not at random Non-response bias can be summarized as whether the missing data are random or non-random. Each possibility has different implications for analysis. Worst-case scenario: MNAR The worst-case scenario for missing data is if it’s missing not at random (MNAR) . In these cases, the missingness can be correlated with at least one variable and the missingness is likely due to the survey question being sensitive. This indicates potential problems with the survey design. For example, let’s say we ran a chi-square test on the job alert cancellation survey to examine the relationship between survey response (no response vs. responded) and current employment status (employed vs. unemployed). We might see the following findings: Detailed description of charts. In the observed survey response results: 9,623 users who did not respond were unemployed 2,572 users who did not respond were employed 46,731 users who did respond were unemployed 13,474 users who did respond were employed In the expected survey response results 9,856.69 users who did not respond were unemployed 3,118.31 users who did not respond were employed 46,009.31 users who did respond were unemployed 14,55.69 users who did respond were employed The above findings show a statistically significant relationship between responding to the job alert cancellation survey and the job seeker’s current employment status. This gives us a test statistic and p-value of 𝚾²(1) = 9.70 and p = 0.0018, respectively. Thus, significantly more unemployed job seekers responded to the job alert cancellation survey than would be expected at random. This is an example of “missing not at random” because the survey question itself might have influenced how people chose to respond. Job seekers who are currently unemployed might be more inclined to respond to the job alert cancellation survey, because finding a job after a period of joblessness is a huge deal. Best-case scenario: MAR The best-case scenario for missing data is if it’s missing at random (MAR) . In these cases, missingness can be correlated with at least one variable, and the missingness is not due to the survey question itself. You might be thinking that I’m intentionally using jargon to confuse you…and I am! Just kidding, MNAR and MAR are commonly used among survey methodologists when discussing missing data. MNAR and MAR live in the same world of jargon as Type 1 and Type 2 error and mediation and moderation. For example, let’s imagine that we ran a chi-square test on the job alert cancellation survey results to examine the relationship between survey response (no response vs. responded) and the job seeker’s device type (desktop vs. mobile). This gives us a test statistic and p-value of X²(1) = 75.57 and p Detailed description of charts. In the observed survey response results: 6,980 users who did not respond were mobile users 5,995 users who did not respond were desktop users 38,212 users who responded were mobile users 22,353 users who responded were desktop users In the expected survey response results: 5,001.57 users who did not respond were mobile users 7973.43 users who did not respond were desktop users 23,346.43 users who responded were mobile users 37,218.57 users who responded were desktop users The above findings show a statistically significant relationship between job seekers responding to the job alert cancellation survey and those job seekers' devices. Significantly fewer job seekers on desktop computers responded to the job alert cancellation survey than expected. However, we might also know from previous experience that more job seekers search for jobs on mobile devices than desktops. In that case, the missingness is likely attributable to device popularity and not to the survey question itself. Additional scenarios for missing data Additional scenarios for missing data include cases where the data are missing completely at random, and cases where the data are missing by design. Missing completely at random (MCAR) refers to cases where the missingness is uncorrelated with all other variables. This type of missingness is typically impossible to validate for large and complex data sets like those found in web analytics. With large data sets, especially rapidly growing ones, the chance of finding some spurious but significant correlation is almost 100%. Missing by design refers to cases where the missingness is intentional. For example, imagine a product change where job seekers are only presented with the job alert cancellation survey if they applied for a job on Indeed in the past 30 days. In this scenario, job seekers who haven't applied for jobs in the past 30 days will never see the survey. Data will thus be missing by design based on the number of applies. The challenge of addressing missing data A core challenge of missing data is determining if it's missing due to randomness, and if so, then which type of randomness  --- MNAR or MAR. While it's fairly easy to check for significant differences in the distribution of missing data, a p-value and confidence interval will not tell you why the data is missing. Determining whether data are MNAR or MAR is a daunting task and relies heavily on assumptions. In the MAR example above, we assumed that the missingness was because users were more inclined to use the mobile version of Indeed than the desktop version. However, we only know this pattern exists because we've talked with people who noticed similar patterns in users preferring mobile over desktop. Without that knowledge we could very easily have misinterpreted the pattern. Thankfully, there are strategies you can use to diagnose whether your data are MAR or MNAR. To start, you can ask yourself: Does the question ask people to reveal sensitive or socially undesirable behavior?” If it does, be aware that asking people to reveal sensitive information is more likely to cause your data to be MNAR rather than MAR. It might be possible to reduce the impact of such survey design by reassuring confidentiality and using other strategies to gain the trust of respondents. If the question does not ask people to reveal sensitive information but you’re still concerned the missing data might be MNAR (the bad one), you can try other strategies. If you have longitudinal data from the respondents, you can check whether the non-response pattern you observe is consistent with previous responses at other time points. If the pattern replicates, you can at least say that your observations are not unusual. Of course, just because the non-response pattern replicates doesn’t mean you’re in the clear for declaring your data are MNAR and not MAR. If, for example, you’re asking people to report socially undesirable behavior, you’d likely see the same MNAR pattern over time. If you don’t have access to longitudinal data, a second solution is to talk with people in your team/organization or look at papers from related research to see if anyone else has observed similar patterns of non-response. Another Research 2.0 solution might be to crowdsource via reaching out to colleagues on Slack and other social media. There you might discover if the non-response pattern you’re observing is typical or atypical. This relatively simple yes/no logic isn’t perfect, but using the strategies above is still better than a head-in-the-sand “missing data never matters” approach. Missing data isn’t always the end of the world Not all missing data is inherently tied to response bias. It can be missing by design, missing completely at random (MCAR), missing not at random (MNAR), or missing at random (MAR). In the job alert cancellation survey, we saw how the survey design might lead to different scenarios of missingness. Are you a data scientist or data aficionado who is also a critical thinker? If so, remember to take a deep dive into your missing data. Suggested reading De Leeuw, E. D. (2001). Reducing missing data in surveys: An overview of methods. Quality and Quantity , 35 (2), 147–160. ← concise article on missing data and response bias Kish, L. (1997). Survey sampling ← although this book is a bit dense, it’s a go-to resource for learning more about sampling bias Tourangeau, R., & Yan, T. (2007). Sensitive questions in surveys. Psychological bulletin , 133 (5), 859. About the author For a bit of background about myself, I’m a University of Michigan Ph.D. graduate (Go Blue!) who recently transitioned to industry as a Quantitative UX Researcher at Indeed. Feel free to message me if you want to chat about my transition from academia to industry or if you just want to muse about missing data 😉 Interested in joining Indeed? Check out our available opportunities . [1] It’s worth acknowledging that the topic of non-response bias is an enormous field. Several textbooks and many dissertations are available on this topic. For a deeper understanding of the field, check out my suggested reading section above. This is designed to be an easy resource you can reference when you are dealing with missing data. Missing Data---cross-posted on Medium .", "date": "2019-08-01"},
{"website": "Indeed-Engineering", "title": "The FOSS Contributor Fund: Six Months In", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2019/07/foss-fund-six-months-in/", "abstract": "Early this year we began an internal FOSS Contributor Fund and invited everyone at Indeed to participate. Why? We wanted to support the open source community in a meaningful way. So we decided to democratize the decision-making process and encourage individual contributions. FOSS: Free or Open Source Software Our unique approach aligns with the ideals that have helped sustain the open source community. These ideals also drive our own company culture. The FOSS Contributor Fund has now reached the 6-month mark. We want to talk here about how it works and reflect on what we’ve learned. How does the FOSS Contributor Fund work? Each month we select an open source project that will receive a $10,000 USD donation. Anyone in the company can nominate a project, as long as it meets four criteria. The project: must be in use by the company or one of its subsidiaries must use an OSI-approved license must have some mechanism for receiving funds cannot be employee-owned Any Indeed employee who makes an open source contribution during the monthly voting cycle can vote on a nominated project. This means that those who are active with open source projects also make the decision. Once the votes are cast for the cycle, we count them and declare the winner. Then, we contact the receiving project and make sure that it is prepared to receive the funds. Recipients decide how to use the funds to best suit their project’s needs. Results to date Since launching the program, participants have engaged from groups like Engineering, QA, Site Reliability Engineering, and Technical Content Management. We’ve also seen strong employee engagement with open source since announcing the fund: over 3,000 contributions from Indeed employees. Indeed has distributed funds to five open source projects: Django, Git, Homebrew, pandas, and pytest. We’ve also selected ESLint to receive future funds. The Git project relies completely on donations for its funds. Git has a few core contributors and a very long tail of developers who only work part-time on the proj ect. These funds help us keep them involved, as they pay for travel to conferences, including our yearly Contributor Summit. We also u se project funds for the Outreachy program, increasing the diversity of our contributor base. --- Jeff King, Git Funds will be mainly used to pay for future Homebrew maintainers to meet in person and may be used to pay for contractors to do some repeatedly postponed tasks around updating our CI setup. --- Mike McQuaid, Homebrew Takeaways After launching the fund, we learned that all Indeedians wanted to participate. Here are some takeaways for encouraging that wider participation. Open up the nominating process to all employees , not just those who’ve registered their GitHub IDs or self-identified their work in open source. If an employee believes a project is important enough to your company that you should support it---and they nominate it---that is enough. Be aware of the parameters that you set for project nominations. You want to ensure a smooth donation process on the backend. You don’t want to create roadblocks and discourage participation. Essential: the nominated project must have a mechanism for receiving funds. A small number of our nominations required a support contract or a subscription. Because the fund operates within a fixed budget, recurring payments were not possible. Yet, these nominations did provide visibility into projects that we would not have known about otherwise. Continue asking which projects would benefit most from a one-time injection of funds. Our initiative has highlighted dependencies about which people have strong feelings. Yet, the initiative does not necessarily show which projects are in most need of financial support. To date, the projects we support are reasonably well known. They generally have a community of contributors and supporters. These projects need support, but there are many other lesser-known projects that need support too. We continue to dig deeper as we work to identify and highlight projects that will benefit the most from these donations. Use nominated projects as a starting point for getting more involved in open source. Figuring out where to start is often the biggest barrier to making that first open source contribution. Picking from a list of known dependencies helps us by narrowing the list to projects nominated during the initiative. It allows us to send new contributors to projects that someone believes Indeed should be supporting. The pandas community is grateful for the funds donated by Indeed's FOSS Contributor Fund. We will put these to good use. We intend to modernize our documentation, improve our performance metric and benchmarking tools, and help fund annual core developer sprints, where we can really work on pandas in depth. This donation helps unlock more resources to continually improve pandas and make it the best data science library toolkit in any language. ---  Jeff Reback, pandas Pytest will use the donation to fund overall maintenance and future gatherings of pytest developers, possibly including another development sprint sometime in the next year. --- Bruno Oliveira Donations from companies like Indeed allow ESLint, a project that is currently run entirely by volunteers, to pursue improvements that wouldn’t otherwise be possible. In the short term, funds are being allocated for translating our documentation into different languages; in the long term, funds will be used to support individual developers who contribute regularly to ESLint to ensure that development and maintenance can continue. --- Nicholas C. Zakas, ESLint TSC Member We’re committed Indeed’s Open Source Program Office is committed to helping sustain the projects we depend on. We’re committed financially and by encouraging our internal community to contribute to the projects they use. The FOSS Contributor Fund is a great way to marry the two. We gave our open source contributors a voice in the process and are enjoying these benefits: broad contribution activity, increased visibility into a wider range of projects, and a great list of projects we can use to onboard new contributors. Learn more about Indeed’s open source program . The FOSS Contributor Fund: Six Months In---cross-posted on Medium .", "date": "2019-07-17"},
{"website": "Indeed-Engineering", "title": "Open Source at Indeed: Heading to OSCON", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2019/07/open-source-indeed-oscon/", "abstract": "The Indeed Open Source Program Office is excited to be at the O’Reilly Open Source Software Conference ( OSCON ) in Portland July 15-18, 2019. If you’re attending, visit Booth 409 and check out both of Indeed’s speaking sessions. Look for our espresso bar and an opportunity to donate to the Apache Software Foundation , the Open Source Initiative , and the Software Freedom Conservancy . For every cup of coffee we hand out, we’ll donate $5 to ASF, OSI, or SFC---your choice. Ignite OSCON Tuesday, July 16 | 5pm | Location: Portland Ballroom Duane O’Brien, Indeed’s head of open source, is happy to emcee Ignite OSCON , a fun evening that follows the day’s tutorials. “Enlighten us, but make it quick!” is the Ignite motto. Each speaker will try to fulfill that mandate in their talk---backed by 20 slides that automatically advance every 15 seconds. Sustaining FOSS Projects by Democratizing the Sponsorship Process Wednesday, July 17 | 2:35pm | Location: E141/142 Speaker: Duane O’Brien, Indeed's Head of Open Source Indeed is democratizing the decision-making process and inviting all of our employees to participate in the FOSS Contributor Fund. Our unique approach aligns with the ideals that have helped sustain the open source community. These ideals also drive our own company culture. Duane's talk will examine how we got executive buy-in for the fund, how the fund was set up, how we encouraged participation, and what the impact has been so far. Speaker Profile Duane is passionate about enabling smart and meaningful contributions to the open source ecosyste m. He navigates the path between engineering and management, drawing on his background as a developer and program manager. Duane also relies on his experience supporting agile transformations. Open Source at Indeed is Heading to OSCON---cross-posted on Medium .", "date": "2019-07-09"},
{"website": "Indeed-Engineering", "title": "Being Just Reliable Enough", "author": [" by Andrew Ford"], "link": "https://engineering.indeedblog.com/blog/2019/10/being-just-reliable-enough/", "abstract": "One Saturday morning, as I settled in on the couch for a nice do-nothing day of watching college football, my wife reminded me that I had agreed to rake the leaves after putting it off for the last two weekends. Being a good neighbor and not wanting another homeowners’ association (HOA) violation (and it being a bye week for the Longhorns ), I grabbed my rake and went outside to work. There were a lot of leaves. I would say my yard was 100% covered in leaves. I began to rake the leaves and with a modest effort I was able to collect about 90% of the leaves into five piles, which I then transferred into those bags you buy at Home Depot or Costco. The yard looked infinitely better, but there were still plenty of leaves in the yard. I had the rake, I had the bags, I was already outside, and I was already dirty, so I went to work raking the entire yard again to get the remaining 10% I had missed in the first pass. This took about the same amount of time, but wasn’t nearly as fulfilling. My piles weren’t as impressive, and I was only able to get 90% of the remaining leaves into piles and then into bags, but I had cleared 99% of the leaves. Still having plenty of daylight and knowing I could do better, I went to work on that last 1%. Now, I don’t know if you know this about leaves, but single leaves can be slippery and evasive. When you don’t have a lot of leaves to clump together to get stuck in the rake it may take two, three, sometimes four passes over the same area to get any good leaf accumulation into your pile. This third pass over the yard was considerably more time consuming, but I was able to get 90% of that remaining 1%. I had now cleared 99.9% of the leaves in my yard. As I sat back and admired my now mostly leaf-free yard, I could see some individual leaves that had escaped my rake and even some new leaves that had just fallen from the trees. There weren’t too many, but they were there. Wanting to do a good job, I started canvassing the yard on my hands and knees, picking up individual leaves one by one. As you can imagine, this was very tedious and it took much longer to do the whole yard, but I was able to pick up 90% of the remaining 0.1%. I had now cleared 99.99% of the leaves in my yard. The sun was starting to set and all that was left were mostly little leaf fragments that could only really be picked up by tweezers. I went inside and asked my wife, \"Where are the tweezers?\" \"Why do you need tweezers to paint the fence?\" she asked. \"Paint the fence?\" I thought. Oh, yeah. I had also agreed to paint the fence today. I told her I hadn’t started on the fence yet and wouldn’t be able to do that this weekend because it was getting late and the Cowboys were playing the next day. She was not happy. Yes, this story is ridiculous and contrived, but it demonstrates some good points that we apply to how we manage system reliability and new feature velocity at Indeed. Where did I go wrong? It was way before I thought about getting the tweezers. When I started raking, my definition of a successfully raked yard was too vague. I did not have a service level objective (SLO) specifying the percentage of my yard that could be covered in leaves and still be considered well-raked by my clients. Should I have defined the SLO? I could have defined the SLO, but I might have based it on what I was capable of achieving. I was capable of picking up bits and pieces of leaves with tweezers until I had a 99.999% leaf-free yard. I could have also gone in the other direction (if it wasn't a bye week) and determined that raking 90% of the leaves would be sufficient. SLOs should be driven by the clients who care about them The clients in my story are my HOA and my wife. My HOA cites me when my yard is only 50% raked for an extended period of time. My wife says she is happy when I rake 99% of the leaves once a year. For the SLO, we would take the higher of the two. I could have quit raking leaves after the second pass when I reached 99% and had time to paint the fence (depending on the SLO for the number of coats of paint). But, I still did a good job, right? I did, but I far exceeded my undefined SLO of 99% by two 9s, and yet I was not rewarded. Sadly, I was punished, because my wife didn’t care about the work I did on that remaining 1% and was upset that I didn’t have the time to meet my other obligation of painting the fence. This brings us to the moral of the story: We need to have the right SLOs and work to exceed them, but not by much. At Indeed, when our SLOs describe what our users care about, we avoid the effort of adding unnecessary 9s. We then use that saved effort to deploy more features faster, achieving a balance between reliability and velocity. About the author Andrew Ford is a site reliability engineer (SRE) at Indeed, who enjoys solving database reliability and scalability problems. He can be found on the couch from the start of College Gameday to the end of the East Coast game most Saturdays from September to December. Do you enjoy defining SLOs that your clients care about? Check out SRE openings at Indeed ! Being Just Reliable Enough---cross-posted on Medium .", "date": "2019-10-28"},
{"website": "Indeed-Engineering", "title": "Indeed Open Source: All Things Open 2019 Speakers", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2019/10/indeed-open-source-ato-2019/", "abstract": "We’re excited to have three Indeed representatives presenting at All Things Open this year. Join us in Raleigh, NC October 13-15 for engaging discussions. Sustaining FOSS Projects by Democratizing the Sponsorship Process Tuesday, October 15 | 10:45am | Room 201 Speaker: Duane O’Brien, Indeed head of open source Within a given company, there are typically only a few people involved in deciding which FOSS projects and initiatives to support financially. This year we decided to change all that and democratize the decision making process. We set up an internal FOSS Sustainability Fund and invited everyone to participate in the process. Sustaining FOSS Projects by Democratizing the Sponsorship Process examines how we got executive buy-in for the fund, set it up, and encouraged participation. It also explores the fund’s impact on our engineering culture. Using Open Source Tools for Machine Learning Tuesday, October 15 | 10:45am | Room 301A Speaker: Samuel Taylor, Indeed data scientist Machine learning can feel like a magic black box, especially given the wealth of proprietary solutions and vendors. This beginner-friendly talk opens the box. It reveals the math that underlies these services and the open source tools you can use in your own work. It introduces machine learning through the lens of three use cases: Teaching a computer sign language (supervised learning) Predicting energy usage in Texas (time series data) Using machine learning to find your next job (content-based filtering) You’ll walk away prepared to practice machine learning in the real world. Your Company Cares about Open Source Sustainability. But Are You Measuring and Encouraging Upstream Contributions? Tuesday, October 15 | 2:15pm | Room 201 Speaker: Dani Gellis, Indeed software developer You encourage the behavior that you measure. If you want your company to help sustain the open source projects you depend on, start by measuring how your employees participate in those projects. How many of your engineers contribute to projects your company consumes? Do they only open issues, or do they contribute code? Are they part of the conversation? Are your non-engineers also involved in the open source community? Your Company Cares about Open Source Sustainability demonstrates how we use open source tools to measure the velocity of our employees’ open source contributions, as well as how Indeed chose these tools. It covers the evolution of our tooling as our open source program has grown. And it reveals our exciting new initiatives to promote sustainable contributions. You’ll leave with new ideas for measuring and improving your organization’s contributions to open source projects. Indeed Open Source: All Things Open 2019---cross-posted on Medium .", "date": "2019-10-07"},
{"website": "Indeed-Engineering", "title": "Jobs Filter: Improving the Job Seeker Experience", "author": [" by Haiyan Luo"], "link": "https://engineering.indeedblog.com/blog/2019/09/jobs-filter/", "abstract": "As Indeed continues to grow, we’re finding more ways to help people get jobs. We’re also offering more ways job seekers can see those jobs. Job seekers can search directly on Indeed.com, receive recommendations, view sponsored jobs or Indeed Targeted Ads, or receive invitations to apply — to name a few. While each option presents jobs in a slightly different way, our goal for each is the same: showing the right jobs to the right job seekers. If we miss the mark with the jobs we present, you may lose trust in our ability to connect you with your next opportunity. Our mission is to help people get jobs, not waste their time. Some of the ways we’d consider a job to be wrong for a job seeker are if it: Pays less than their expected salary range Requires special licensure they do not have Is located outside their preferred geographic area Is in a related field but mismatched, such as nurses and doctors being offered the same jobs To mitigate this issue, we built a jobs filter to remove jobs that are obviously mismatched to the job seeker. Our solution uses a combination of rules and machine learning technologies, and our analysis shows it to be very effective. System architecture The jobs filter product consists of the following components, as shown in the preceding diagram: Jobs Filter Service. A high throughput, low latency application service that evaluates potential match-ups of jobs to users, identified by ID. If the service determines that the job is appropriate for the user ID, it returns an ALLOW decision; otherwise it returns a VETO. This service is horizontally scalable so it can serve many real-time Indeed applications. Job Profile. A data storage service that provides high throughput, low latency performance. It retrieves job attributes such as estimated salary, job titles, and job locations at serving time. The job profile uses Indeed NLP libraries and machine learning technologies to extract or aggregate user attributes. User Profile. Similar to the job profile, but provides attributes about the job seeker rather than the job. Like the job profile, it is a data storage service that provides high throughput, low latency performance. It retrieves job seeker attributes such as expected salary, current job title, and preferred job locations at serving time. Like the job profile, it uses Indeed NLP libraries and machine learning technologies to extract or aggregate user attributes. Offline Evaluation Platform. Consumes historic data to evaluate rule effectiveness without actually integrating with the upstream applications. It is also heavily used for fine-tuning existing rules, identifying new rules, and validating new models. Offline Model Training. Component that consists of our offline training algorithms, with which we train models that can be used in the jobs filter rules at serving time for evaluation. Filter rules to improve job matches The jobs filter uses a set of rules to improve the quality of jobs displayed to any given job seeker. Rules can be simple: “Do not show a job requiring professional licenses to job seekers who don’t possess such licenses,” or “Do not show jobs to a job seeker if they come with a significant pay cut.” They can also be complex: “Do not show jobs to the job seeker if we are confident the job seeker will not be interested in the job titles,” or “Do not show jobs to the job seeker if our complex predictive models suggest the job seeker will not be interested in them.” All rules are compiled into a decision engine library. We share this library in our online service and offline evaluation platform. Although the underlying data for building jobs filter rules might be complex to acquire, most of the heuristic rules themselves are straightforward to design and implement. For example, in one rule we use a user response prediction model to filter out jobs that the job seeker is less likely to be interested in. An Indeed proprietary metric helps us evaluate our performance by measuring the match quality of the job seeker and the given jobs. Ads ranking and recommender systems commonly rely on user response prediction models, such as click prediction and conversion prediction, to generate a score. They then set a threshold to filter out everything with low scores. This filtering is possible because the models predict positive reactions from users, and low scores indicate poor match quality. We adopted similar technologies in our jobs filter product, but we used negative matching models when designing our machine learning based rules. We build models to predict negative responses from users. We use Tensorflow to build the Wide and Deep model . This facilitates future experimentation with more complex models such as Factorization machine or neural networks. The features we use cover major user attributes and job data. After we train a model that performs well, we export it using the Tensorflow SimpleSave API . We load the exported model into our online systems and serve requests using the Tensorflow Java API . Besides traditional classifier metrics such as AUC, precision, and recall, we also load our model into our offline evaluation platforms to validate the performance. Putting it all to work We apply our jobs filter in several applications within Indeed. One application is Job2Job, which recommends similar jobs to the job seeker based on the jobs they have clicked or applied for. Using the Job2Job service, we saw a greater than 20% increase in job match quality. When we applied the service to other applications, we observed similar, if not greater, improvements. Rule-based engines work well in solving corner cases. However, the number of rules can easily spiral out of control. Our design’s hierarchy of rules and machine learning technologies effectively solve this challenge and keep our system working. In the future, we aim to add more features into the model so that it can become even more effective. Jobs Filter---cross-posted on Medium .", "date": "2019-09-12"},
{"website": "Indeed-Engineering", "title": "IndeedEng: Proud Supporters of the Open Source Community", "author": [" by Indeed Engineering"], "link": "https://engineering.indeedblog.com/blog/2019/09/indeed-supports-open-source/", "abstract": "At Indeed, open source is at the core of everything we do. Our collaboration with the open source community allows us to develop solutions that help people get jobs. As active participants in the community, we believe it is important to give back. This is why we are dedicated to making meaningful contributions to the open source ecosystem. We’re proud to announce our continuing support by renewing our sponsorship for these foundations and organizations. The ASF thanks Indeed for their continued generosity as an Apache Software Foundation Sponsor at the Gold level. In addition, Indeed has expanded on their support by providing our awesome ASF Infrastructure team the opportunity to leverage Indeed.com job listing and advertising resources. This helped us bring on new hires to ensure Apache Infrastructure services continue to run 24x7x365 at near 100% uptime. We are grateful for their involvement, which, in turn, benefits the greater Apache community. --- Daniel Ruggeri, VP Fundraising, Apache Software Foundation CNCF is thrilled to have Indeed as a member of the Foundation. They have been a great addition to our growing end-user community. Indeed's participation in this vibrant ecosystem helps in driving adoption of cloud native computing across industries. We’re looking forward to working with them to help continue to grow our community. --- Dan Kohn, Executive Director, Cloud Native Computing Foundation Indeed’s active engagement with open source communities highlights that open source software is now fundamental, not only for businesses, but developers as well. Like most companies today, Indeed is a user of and contributor to open source software, and interestingly, Indeed’s research of resumes shows developers are too---as job seekers highlight open source skills and experience to win today’s most sought after jobs across technology. --- Patrick Masson, General Manager at the OSI We're so happy that Indeed continues to join our sponsors---making it possible for us to provide critical opportunities to people who are impacted by systemic bias, underrepresentation and discrimination---and helping them get introduced to free and open source software. --- Karen Sandler, Executive Director, Software Freedom Conservancy Participation in the PSF Sponsorship Plan shows Indeed's support of our mission to promote the development of the Python programming language and the growth of its international community. Sponsorships, like Indeed's, fund programs that help provide opportunities for underrepresented groups in technology and shows support for open source and the Python community. --- Betsy Waliszewski, Python Software Foundation We’re committed Our open source initiatives involve partnerships, sponsorships and memberships that support open source projects we rely on. We work to ensure that Indeed’s own open source projects thrive. And we involve all Indeedians. This year we began a FOSS Contributor Fund to support the open source community. Anyone in the company can nominate an open source project to receive funds that we award each month. We’re committed to open source. Learn more about how we do it . IndeedEng Supports the Open Source Community---cross-posted on Medium .", "date": "2019-09-25"},
{"website": "Indeed-Engineering", "title": "Unthrottled: Fixing CPU Limits in the Cloud", "author": [" by Dave Chiluk"], "link": "https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/", "abstract": "This post is the first in a two-part series. This year, my teammates and I solved a CPU throttling issue that affects nearly every container orchestrator with hard limits, including Kubernetes, Docker, and Mesos. In doing so, we lowered worst-case response latency in one of Indeed’s applications from over two seconds to 30 milliseconds. In this two-part series, I’ll explain our journey to find the root cause and how we ultimately arrived at the solution. Photo by twinsfisch on Unsplash The issue began last year, shortly after the v4.18 release of the Linux kernel. We saw an increase in tail response times for our web applications, but when we looked at CPU usage, everything seemed fine. Upon further investigation, it was clear that the incidence of high response times directly correlated to periods of high CPU throttling. Something was off. Normal CPU usage and high throttling shouldn’t have been possible. We eventually found the culprit, but first we had to understand the mechanisms at work. Background: How container CPU constraints work Almost all container orchestrators rely on the kernel control group (cgroup) mechanisms to manage resource constraints. When hard CPU limits are set in a container orchestrator, the kernel uses Completely Fair Scheduler (CFS) Cgroup bandwidth control to enforce those limits. The CFS-Cgroup bandwidth control mechanism manages CPU allocation using two settings: quota and period. When an application has used its allotted CPU quota for a given period, it gets throttled until the next period. All CPU metrics for a cgroup are located in /sys/fs/cgroup/cpu,cpuacct/ . Quota and period settings are in cpu.cfs_quota_us and cpu.cfs_period_us . You can also view throttling metrics in cpu.stat . Inside cpu.stat you’ll find: nr_periods – number of periods that any thread in the cgroup was runnable nr_throttled – number of runnable periods in which the application used its entire quota and was throttled throttled_time – sum total amount of time individual threads within the cgroup were throttled During our investigation into the response time regression, one engineer noticed that applications with slow response times saw excessive amounts of periods throttled ( nr_throttled ). We divided nr_throttled by nr_periods to find a crucial metric for identifying excessively throttled applications. We call this metric \"throttled percentage.\" We didn’t like using throttled_time for this purpose because it can vary widely between applications depending on the extent of thread usage. A conceptual model of CPU constraints To see how CPU constraints work, consider an example. A single-threaded application is running on a CPU with cgroup constraints. This application needs 200 milliseconds of processing time to complete a request. Unconstrained, its response graph would look something like this. Now, say we assign a CPU limit of .4 CPU to the application. This means the application gets 40ms of run time for every 100ms period—even if the CPU has no other work to do. The 200ms request now takes 440ms to complete. If we gather metrics at time 1000ms, statistics for our example are: Metric Value Reasoning nr_periods 5 From 440ms to 1000ms the application had nothing to do and as such was not runnable. nr_throttled 4 The application is not throttled in the fifth period because it is no longer runnable. throttled_time 240ms For every 100ms period, the application can only run for 40ms and is throttled for 60ms. It has been throttled for 4 periods, so 4 multiplied by 60 equals 240ms. throttled percentage 80% 4 nr_throttled divided by 5 nr_periods. But that’s at the high-level, not real life. There are a couple of problems with this conceptual model. First, we live in a world of multi-core, multi-threaded applications. Second, if all this were completely true, our problematic application shouldn’t have hit throttling before exhausting its CPU quota. Reproducing the problem We knew a succinct reproducing test case would help convince the kernel community that a problem actually existed and needed to be fixed. We tried a number of stress tests and Bash scripts, but struggled to reliably reproduce the behavior. Our breakthrough came after we considered that many web applications use asynchronous worker threads . In that threading model, each worker is given a small task to accomplish. For example, these workers might handle IO or some other small amount of work. To reproduce this type of workload, we created a small reproducer in C called Fibtest . Instead of using unpredictable IO, we used a combination of the Fibonacci sequence and sleeps to mimic the behavior of these worker threads. We split these between fast threads and slow worker threads. The fast threads run through as many iterations of the Fibonacci sequence as possible. The slow threads complete 100 iterations and then sleep for 10ms. To the scheduler, these slow threads act much like asynchronous worker threads, in that they do a small amount of work and then block. Remember, our goal was not to produce the most Fibonacci iterations. Instead, we wanted a test case that could reliably reproduce a high amount of throttling with simultaneous low CPU usage. By pinning these fast and slow threads each to their own CPU, we finally had a test case that could reproduce the CPU throttling behavior. The first throttling fix / regression Our next step was to use Fibtest as the condition for running a git bisect on the kernel. Using this technique, we were able to quickly discover the commit that introduced the excessive throttling: 512ac999d275 “sched/fair: Fix bandwidth timer clock drift condition\" . This change was introduced in the 4.18 kernel. Testing a kernel after removing this commit fixed our issue of low CPU usage with high throttling. However, as we analyzed the commit and the related sources, the fix looked perfectly valid. And more confusingly, this commit was also introduced to fix inadvertent throttling. The issue this commit fixed was exemplified by throttling that appeared to have no correlation with actual CPU usage. This was due to clock-skew between the cores that resulted in the kernel prematurely expiring the quota for a period. Fortunately, this problem was much rarer, as most of our nodes were running kernels that already had the fix. One unlucky application ran into this problem, though. That application was mostly idle and allocated 4.1 CPUs. The resulting CPU usage and throttle percentage graphs looked like this. CPU usage graph with 4 CPUs allocated and usage not exceeding .5 CPU Graph of throttled percentage showing excessive throttling Commit 512ac999d275 fixed the issue and was backported onto many of the Linux-stable trees. The commit was applied to most major distribution kernels, including RHEL, CentOS, and Ubuntu. As a result, some users have probably seen throttling improvements. However, many others are likely seeing the problem that initiated this investigation. At this point in our journey, we found a major issue, created a reproducer, and identified the causal commit. This commit appeared completely correct but had some negative side-effects. In part two of this series , I’ll further explain the root cause, update the conceptual model to explain how CFS-Cgroup CPU constraints actually work, and describe the solution we eventually pushed into the kernel. Fixing CPU Limits in the Cloud---cross-posted on Medium .", "date": "2019-12-05"},
{"website": "Indeed-Engineering", "title": "The FOSS Contributor Fund: Forming a Community of Adopters", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2019/11/foss-fund-adopters/", "abstract": "In January 2019, Indeed launched a new program that democratizes the way we provide financial support to open source projects that we use. We call it The FOSS Contributor Fund . The fund enables Indeed employees who make open source contributions to nominate and vote for projects. Each month, the winning project receives funding. This program encourages support of projects we use and more engagement with the open source community. Join our community of FOSS Fund Adopters Now, we want to help other companies start similar funds. Our goal is to collaborate for the benefit of the open source community. Regardless of a company's size or resources, we want to discover what we can accomplish when we work together. Indeed is forming a community of FOSS Fund Adopters—companies that will run their own FOSS Contributor Fund initiatives in 2020. We invite you to join us and other FOSS Funders in sharing knowledge and experiences. We’re looking for adopters who are willing to run the same experiment we ran, or something similar. We will work with the community of Funders to set up regular events, exploring different models of open source support and funding. We've seen great results In our blog post at the six month mark , we described how the program helped encourage Indeed employees to make open source contributions. Since program launch, we’ve seen thousands of such contributions. Indeedians have reported and fixed bugs. They’ve reviewed pull requests and developed features. They’ve improved documentation and designs. Even better, Indeed developers now have an avenue to advocate for projects in need of our support. And the program has inspired some employees to make their first open source contributions. The FOSS Contributor Fund is one of the ways Indeed’s Open Source Program Office honors our commitment to helping sustain the projects we depend on. We gave our open source contributors a voice in the process, and we’ve seen some great benefits from doing so: increased contribution activity, better visibility into our dependencies, and a short list of projects where we can send new contributors. Watching the program evolve and grow is exciting. We’ve learned a lot this year and look forward to more growth in 2020. Now, we’d like you to join us. Use Indeed's blueprint to start your FOSS Fund To find out how we administer the FOSS Fund at Indeed, read our blueprint (released under a Creative Commons license). We’ve also released an open source tool called Starfish that we use to determine voter eligibility. In the coming months, FOSS Funders will publish additional documentation and tools to support these programs. We want to make it easy for anyone to run their own FOSS Fund. If you are interested in joining the community of FOSS Fund Adopters, want more information, or would like to join a Q&A session, please email us at opensource@indeed.com . Learn more about Indeed’s open source program . The FOSS Contributor Fund---cross-posted on Medium .", "date": "2019-11-07"},
{"website": "Indeed-Engineering", "title": "Making Our Code More Inclusive", "author": [" by Jack Humphrey"], "link": "https://engineering.indeedblog.com/blog/2020/07/inclusion-in-code/", "abstract": "At Indeed, inclusion extends beyond employee resource groups and celebrations. Diversity of background, experience, and thought makes for a stronger workforce , more effective decision-making, and powerful innovation. To foster inclusion, we want to build a psychologically safe environment at every level and in every area of the business. That’s why we’re removing terminology that works against such inclusion from our codebase. Diversity and inclusion are ingredients for success. Leaders of Indeed Amsterdam’s Women at Indeed employee resource group (l-r): Edwin Moses, Trudy Danso-Osei, Freek van Schaik, Renske van der Linden, and Valerie Sampimon. What does technical terminology have to do with inclusion? Like all words, technical terms have connotations that give them immense expressive power. Some connotations are well known and generally understood. Others depend on context and are understood differently by people with varying lived experiences. The original etymology of a term often has little to do with the connotations it accrues over time. Computer science and software engineering employ many terms that are convenient, meaningful, and useful. However, some terms ask groups of people to ignore the powerful negative and exclusive connotations they carry. The terms “master” and “slave” exemplify this. Some engineers see these words and are privileged to deduce a benign connotation—a slave is a subordinate process that acts in accordance with the demands of the master. However, for many people, particularly people of color, these terms immediately conjure images of human slavery’s horrors. This connotation doesn’t just exist in the context of one country’s history, such as American slavery. With an estimated 21-45 million people currently enslaved worldwide , the terms master and slave represent both an historic and current global humanitarian crisis. Many other terms have similar negative connotations. Words that associate colors with value judgments, such as “blacklist,” and language around the exploitation or denigration of cultures, such as “tribal knowledge,” represent just a couple. Ableist language such as “lame” and “blind” used in the wrong context can negatively impact people with disabilities. People continually fight bigotry and prejudice based on these characteristics, and these terms invoke and perpetuate those injustices. Some of these terms might surprise those of us who don’t share the lived experiences of marginalized individuals. But when our colleagues tell us we are using terms that exclude or hurt them, we should trust them and find new words to use. Starting the conversation Even before Indeed officially introduced inclusion and belonging as one of our company values in 2019, our engineers began removing problematic terms from our technology. We started by opening up the discussion on our internal wiki, with internal blog posts and a dedicated content hub for identifying and deprecating exclusive terminology. All engineers can contribute to and comment on the Inclusive Terminology wiki page. From contributions made there, we created a non-exhaustive quick reference guide to help each other make responsible terminology decisions. Instead of Use Why master* primary, main, trunk These terms represent an inherently oppressive relationship . *The removal of “slave” from the set in common usage does not remove the implied oppressive relationship. Historically, the usage of the term “master” in relation to a Git branch stems from a master / slave relationship . slave replica, secondary whitelist allowlist, unblocklist These terms imply a culturally specific binary of good versus evil. blacklist denylist, blocklist, banned backlog grooming backlog refinement “Grooming” is a term with specific legal meaning in British English. tribal knowledge institutional knowledge “Tribe” is a loaded term with negative connotations for First Nations and African communities. grandfathered legacy, pre-existing Grandfather clauses originated from Jim Crow era discrimination laws in the United States. Each engineering team chose how to implement the new language in their code. Then, teams shared best practices and processes. We continue these conversations today. Case study: Replacing “master” with “primary” in a Git project Renaming the master branch of a Git project is not a trivial exercise, especially for projects with lengthy histories. Recently, our Design Systems team completed this work for one of their projects. To do this, the team: Cloned the master branch and named the clone “primary.” Updated the default branch in GitLab from master to primary. Locked down the master branch. It still exists for historical purposes, but it can no longer be used. Applied the former settings for the master branch to the new primary branch. A couple of issues could arise in this scenario. For example, a user could create a branch off master before the team created the new primary branch. Because primary and master share a common history, the user could theoretically merge the feature into primary. To mitigate such issues, the team enacted a code freeze while they made the change. They also tested their process on a smaller project before renaming the main project. Tangible results To track this work, Indeed engineers leaned on Atlassian’s Jira , our tool for software development tracking . We added a label to Jira tickets that involve inclusive terminology so we can filter and sort them. This gives us a high-level view of where exclusive language exists, our ongoing efforts to remove that language, and our progress. To date, we’ve closed 97 of 113 issues and counting. Pie graph showing the number of Jira issues labeled \"inclusive-terminology\" by status, with 97 closed, 1 deferred, 9 on backlog, 1 pending review, 2 pending triage, and 3 in wish list status Challenges to making this happen This work sparked lots of discussion among our engineers. The last thing we wanted to do was turn these language changes into a policing and shaming process. So, we decided to make this a grassroots effort instead of a top-down mandate. That way, everyone is empowered to respectfully discuss terminology changes while learning from one another. Leadership provides support and guidance when necessary and actively participates in the conversation. One subject that came up in these discussions was cost and level of effort. Changing terminology throughout all our products is a long-term project that requires many engineer hours. In fact, as of today we still need to remove over 5000 instances of the term “slave” from our codebase. We’re committed, and the psychological safety generated by this work far outweighs the time and effort required to remove exclusive terminology. A way forward Language constantly evolves to meet the needs of those who use it, and words fall out of fashion as we progress. Because of this, we know changing the terms in our codebase is an ongoing practice, not a one-time effort. We continue to document words we want to replace and offer suitable alternatives. We avoid using those terms in any new code and ask our vendors to avoid those terms in their products as well. As we change our codebase, we methodically and carefully locate and replace the existing usages. We still have work to do . We constantly increase our awareness of exclusive terms and their implications, and we engage in respectful conversations about these topics with each other. Together, we want to create a work environment that is psychologically safe, inclusive, and welcoming for all people at Indeed. By sharing these practices, we hope to model inclusivity and improve the tech industry as well. Inclusive Code---cross-posted on Medium .", "date": "2020-07-15"},
{"website": "Indeed-Engineering", "title": "Coming Together to Support the Open Source Community", "author": [" by Duane O'Brien"], "link": "https://engineering.indeedblog.com/blog/2020/03/support-the-oss-community/", "abstract": "Over the last few weeks, conference and event cancellations around the world have heavily impacted the open source community. These events play an important role in the ecosystem that supports free and open source software. If we want that ecosystem to remain healthy, it is important for us to act now. Why support matters now more than ever Conferences and events provide essential opportunities for the open source community to: Coordinate activities Raise funds Grow their user bases Support each other Evangelize their technologies Educate and onboard new contributors Ship new releases Share knowledge Running these events requires a lot of time, effort, and money. When they are cancelled, the event organizers bear the brunt of the losses. If we want these events to continue—and the open source community to sustain itself and grow—all users of free and open source software must respond. The FOSS Responders Open source community leaders from across the industry have come together to form a working group called FOSS Responders . We’re focused on identifying open source events, communities, foundations, and community members who are most in need of support. We also want to support individuals who are unable to absorb conference-related cancellation fees. Our goals: amplify these community needs and mobilize organizational and individual resources to help. This working group of committed industry professionals includes participants from Indeed , GitLab , Open Collective , the Sustain community, the Drupal Association , and several other organizations. You can find more information about this working group—including information on how to join and participate—at fossresponders.com . Virtual funding event Indeed and Open Collective are collaborating to host a virtual funding event on May 22, 2020 . We want to raise funds for conference organizers that have suffered irrecoverable losses due to event cancellations. We are calling on our peers in the industry to join us at this event as fellow FOSS Funders. By coming together to share knowledge, collaborate on decision making, and coordinate our collective response, we can ensure that these events will continue to serve and support the community in the months and years to come. Find more information about the virtual funding event . Taking action Regardless of your need or your capacity to help, the time to act is now. Here are some specific actions you can take. How to help Now is the time to give back to the projects we depend on. This is how we future-proof our open source infrastructure investments and help millions who built the software we benefit from: Donate and buy membership to foundations that support your projects Donate to individuals working on projects you use via GitHub Sponsors and Open Collective Donate to the FOSS Responders Open Collective —your funds will be used to help individuals who might otherwise fall through the cracks How to get help FOSS Responders will amplify your need so others can easily see who and how to help. By sharing your need, we can connect you with helpers. If you are an individual who needs help paying for conference-related cancellation fees, fill out the FOSS Responders Individual Request If you had to cancel an event and your organization needs financial assistance as a result, open an EVENT issue If you need other kinds of help, open an ORGANIZE issue How to get involved The FOSS Responders working group is growing quickly and we could use your help. Spread the word—we need to hear stories from people who have felt the impact Join the FOSS Responders Working Group Slack channel Ask your company to participate in the funding event on May 22, 2020 Supporting the Open Source Community---cross-posted on Medium .", "date": "2020-03-30"},
{"website": "Indeed-Engineering", "title": "Improving Incident Retrospectives", "author": [" by Alex Elman"], "link": "https://engineering.indeedblog.com/blog/2020/01/improving-incident-retrospectives/", "abstract": "This post was originally published on learningfromincidents.io . Photo by Jared Erondu on Unsplash As a Site Reliability Engineer (SRE) at Indeed, I often participate in the retrospective process that follows an incident. Retrospectives---in use at Indeed since late 2015---are a meaningful part of our engineering culture. I have never questioned their importance, but recently I was struck by shortcomings I saw in some retrospectives. For example: A retrospective meeting might use only ~30% of the allotted time. What is discussed might be gleaned from reading the incident ticket and retrospective document instead of attending the meeting. Too much focus is devoted to the conditions that “triggered” the incident. Signals used for deciding to hold a retrospective tend to direct focus toward incidents with high impact or high visibility. Were participants actually learning anything new? It became apparent to me that we were not using every incident to realize our full potential to learn. I decided to explore why so that we could improve our process. The typical retrospective Retrospectives at Indeed are usually a one-hour discussion including up to several dozen participants. The meeting is open to anyone in the company, but usually participants have either been involved in the incident response or have a stake in the outcome. Facilitators follow a prescribed process: Review the timeline. Review the remediation items in the template. Find owners for the remediation items. Open the room for questions. Spotting opportunities for improvement In Summer 2018 I visited one of our tech sites and was invited to several local retrospective meetings to discuss some recent incidents. As an SRE it wasn’t unusual for me (or members of my team) to be invited. I also had subject matter expertise in a technology related to the incidents. The facilitators took about 5 minutes to review the timeline, spent 8-10 minutes reviewing the remediation items, and concluded with questions related to the specific technologies involved in the causal chain. I didn’t learn anything new. I could have gained the same information from reading the incident ticket and retrospective document. This was a rare opportunity when a unique and eager group of people gathered in a conference room ready to collaboratively investigate. Instead, we never achieved the full potential. This result is not uniform across retrospectives. I’ve been present in retrospectives where the participants offered such rich detail that the conversation continued well beyond the one-hour time limit, culminating with a huddle outside of the conference room. The facilitators for these particular retrospective meetings followed the process faithfully but had only utilized ~30% of the time. It was clear to me that the retrospective process itself needed improvement. Nurturing a safety culture To understand potential changes, I first solicited viewpoints on why we conduct retrospectives at Indeed. Reasons I heard are likely familiar to most software organizations: Find out what caused the outage Measure the impact Ensure that the outage never happens again Create remediation items and assign owners These goals also reflect Indeed’s strong sense of ownership. When someone’s service is involved in an incident, there’s a concern that we were closer to the edge of failure than we thought we were. Priorities temporarily change and people are more willing to critically examine process and design choices. It’s important to use these opportunities to direct efforts toward a deeper analysis into our systems (both people and technical) and the assumptions that we’ve made about them. These approaches to a different safety culture at Indeed are still relatively new and are evolving toward widespread adoption. Recommendation: Decouple remediation from the retrospective process One process change I recommend is around the creation of remediation items. The retrospective process is not necessary as a forcing function for driving accountability of finding and owning remediation items. I consistently observe that the creation of remediation items occurs organically after Production is stabilized. Many fixes are obvious to teams in the hindsight following an incident. I see value in decoupling these “after action” activities from the retrospective process for many reasons. The search for remediation items is often a tacit stopping point that halts further or deeper investigation. The accountability around owning remediation items should be tightly coupled to incident ownership. The retrospective process should be an optional activity. By making the retrospective process optional, teams that decide to engage in it are doing so because they see value in it rather than as an obligation or a checklist item. Participants are freed up to conduct a deeper investigation unencumbered by the search for remediation items and shallow explanations. Recommendation: Lighten up the retrospective template Another useful change is with the retrospective template itself. Using retrospective templates can be a lot like filling out forms. The focus is directed toward completion of an activity rather than free exposition. A blank document encourages a different kind of sharing. I have witnessed incidents where responders were so motivated to share their thoughts and descriptions that they produced rich and detailed analysis simply by starting with a blank document. If every incident is shaped like a snowflake, it’s impossible to develop a template that is equipped to capture its unique characteristics. A template constrains detail and triggers explanations through close-ended questions. A blank canvas is open-ended. A template is yet another tacit stopping point that hinders deeper analysis. I recommend that we apply templates to incident analysis, but that we use blank documents for the retrospective process. Driving organizational change I have learned a lot by working to drive change at Indeed as we’ve grown quickly. My efforts have benefitted from my tenure in the company, experience participating in hundreds of incidents, and connection to the literature. I have made headway but there is still a lot to do. I attribute some of my progress so far to finding other advocates in the company and remembering to communicate. Find advocates Advocates are colleagues who align closely with my goals, acknowledge where we could be doing better, and share a vision of what could be. I had no trouble finding these advocates. They are colleagues who are willing to listen, have an open mind and have the patience to consider another perspective. I held numerous 1:1s with leaders and stakeholders across the organization. I found opportunities to bring these topics up during meetings. I gave tech talks and reached out to potential advocates whenever I visited one of our global Engineering offices. Over-communicate As much as I might think that I was communicating what I was working on, it was never enough. I found I had to constantly over-communicate. As I over-communicated and leveraged multiple media, I may have sounded repetitive to anyone in close proximity to my words. But this was the only way to reach the far edges of the organization who might not have otherwise heard me. Not everybody has time to read every email or internal blog post. Looking ahead Response to these changes has been largely positive. The focus during retrospectives is still anchored to the technological factors, when more attention could be paid to the human factors. I’m exploring different avenues for increasing the reach and effectiveness of these efforts. This includes working with our instructional design team to create a debrief facilitator program, communicating more often and more broadly, making more process changes, continuing to help teams produce and share high quality write-ups, and focusing on producing educational opportunities. At this point we’ve only scratched the surface and I’m looking forward to what we will accomplish. About the author Alex Elman is a founding member of the Site Reliability Engineering team at Indeed. He leads two teams: one that focuses on Resilience Engineering and one that supports the flagship Job Search product. For the past eight years Alex has been helping Indeed adopt reliability practices to cope with ever increasing complexity and scale. Follow Alex on Twitter @_pkill . Improving Incident Retrospectives---cross-posted on Medium .", "date": "2020-01-08"},
{"website": "Indeed-Engineering", "title": "Unthrottled: How a Valid Fix Becomes a Regression", "author": [" by Dave Chiluk"], "link": "https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/", "abstract": "This post is the second in a two-part series. In a previous post , I outlined how we recognized a major throttling issue involving CFS-Cgroup bandwidth control . To uncover the problem, we created a reproducer and used git bisect to identify the causal commit. But that commit appeared completely valid, which added even more complications. In this post, I’ll explain how we uncovered the root of the throttling problem and how we solved it. Photo by Jake Givens on Unsplash Scheduling on multiple CPUs with many threads While accurate, the conceptual model in my prior post fails to fully capture the kernel scheduler’s complexity. If you’re not familiar with the scheduling process, reading the kernel documentation might lead you to believe the kernel tracks the amount of time used. Instead, it tracks the amount of time still available. Here’s how that works. The kernel scheduler uses a global quota bucket located in cfs_bandwidth->quota . It allocates slices of this quota to each core ( cfs_rq->runtime_remaining ) on an as-needed basis. This slice amount defaults to 5ms, but you can tune it via the kernel.sched_cfs_bandwidth_slice_us sysctl tunable. If all threads in a cgroup stop being runnable on a particular CPU, such as blocking on IO, the kernel returns all but 1ms of this slack quota to the global bucket. The kernel leaves 1ms behind, because this decreases global bucket lock contention for many high performance computing applications. At the end of the period, the scheduler expires any remaining core-local time slice and refills the global quota bucket. That’s at least how it has worked since commit 512ac999 and v4.18 of the kernel. To clarify, here’s an example of a multi-threaded daemon with two worker threads, each pinned to their own core. The top graph shows the cgroup’s global quota over time. This starts with 20ms of quota, which correlates to .2 CPU. The middle graph shows the quota assigned to per-CPU queues, and the bottom graph shows when the workers were actually running on their CPU. Time Action 10ms A request comes in for worker 1. A slice of quota is transferred from the global quota to the per-CPU queue for CPU 1. Worker 1 takes exactly 5ms to process and respond to the request. 17ms A request comes in for worker 2. A slice of quota is transferred from the global quota to the per-CPU queue for CPU 2. The chance that worker 1 takes precisely 5ms to respond to a request is incredibly unrealistic. What happens if the request requires some other amount of processing time? Time Action 30ms A request comes in for worker 1. Worker 1 needs only 1ms to process the request, leaving 4ms remaining on the per-CPU bucket for CPU 1. Since there is time remaining on the per-CPU run queue, but there are no more runnable threads on CPU 1, a timer is set to return the slack quota back to the global bucket. This timer is set for 7ms after worker 1 stops running. 38ms The slack timer set on CPU 1 triggers and returns all but 1 ms of quota back to the global quota pool. This leaves 1 ms of quota on CPU 1. 41ms Worker 2 receives a long request. All the remaining time is transferred from the global bucket to CPU 2’s per-CPU bucket, and worker 2 uses all of it. 49ms Worker 2 on CPU 2 is now throttled without completing the request. This occurs in spite of the fact that CPU 1 still has 1ms of quota. While 1ms might not have much impact on a two-core machine, those milliseconds add up on high-core count machines. If we hit this behavior on an 88 core (n) machine, we could potentially strand 87 (n-1) milliseconds per period. That’s 87ms or 870 millicores or .87 CPU that could potentially be unusable. That’s how we hit low-quota usage with excessive throttling. Aha! Back when 8- and 10-core machines were considered huge, this issue went largely unnoticed. Now that core count is all the rage, this problem has become much more apparent. This is why we noticed an increase in throttling for the same application when run on higher core count machines. Note: If an application only has 100ms of quota (1 CPU), and the kernel uses 5ms slices, the application can only use 20 cores before running out of quota (100 ms / 5 ms slice = 20 slices). Any threads scheduled on the other 68 cores in an 88-core behemoth are then throttled and must wait for slack time to be returned to the global bucket before running. Resolving a long-standing bug How is it, then, that a patch that fixed a clock-skew throttling problem resulted in all this other throttling? In part one of this series, we identified 512ac999 as the causal commit. When I returned to the patch and picked it apart, I noticed this. -       if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { +       if (cfs_rq->expires_seq == cfs_b->expires_seq) { /* extend local deadline, drift is bounded above by 2 ticks */ cfs_rq->runtime_expires += TICK_NSEC; } else { /* global deadline is ahead, expiration has passed */ cfs_rq->runtime_remaining = 0; } The pre-patch code expired runtime if and only if the per-CPU expire time matched the global expire time ( cfs_rq->runtime_expires != cfs_b->runtime_expires ). By instrumenting the kernel, I proved that this condition was almost never the case on my nodes. Therefore, those 1 milliseconds never expired. The patch changed this logic from being clock time based to a period sequence count, resolving a long-standing bug in the kernel. The original intention of that code was to expire any remaining CPU-local time at the end of the period. Commit 512ac999 actually fixed this so the quota properly expired. This results in quota being strictly limited for each period. When CFS-Cgroup bandwidth control was initially created, time-sharing on supercomputers was one of the key features. This strict enforcement worked well for those CPU-bound applications since they used all their quota in each period anyway, and none of it ever expired. For Java web applications with tons of tiny worker threads, this meant tons of quota expiring each period, 1ms at a time. The solution Once we knew what was going on, we needed to fix the issue. We approached the problem in several different ways. First, we tried implementing “rollover minutes” that banked expiring quota and made it usable in the next period. This created a thundering herd problem on the global bucket lock at the period boundary. Then, we tried to make quota expiration configurable separate from the period. This led to other issues where bursty applications could consume way more quota in some periods. We also tried returning all the slack quota when threads became unable to run, but this led to a ton of lock contention and some performance issues. Ben Segall, the author of the CFS scheduler, suggested tracking the core-local slack and reclaiming it only when needed. This solution had performance issues of its own on high-core count machines. As it turns out, the solution was actually staring us right in the face the whole time. No one had noticed any issues with CFS CPU bandwidth constraints since 2014. Then, the expiration bug was fixed in commit 512ac999, and lots of people started reporting the throttling problem. So, why not remove the expiration logic altogether? That’s the solution we ended up pushing back into the mainline kernel. Now, instead of being strictly limited to a quota amount of time per period, we still strictly enforce average CPU usage over a longer time window. Additionally, the amount that an application can burst is limited to 1ms for each CPU queue. You can read the whole conversation and see the five subsequent patch revisions on the Linux kernel mailing list archives . These changes are now a part of the 5.4+ mainline kernels. They have been backported onto many available kernels: Linux-stable: 4.14.154+, 4.19.84+, 5.3.9+ Ubuntu: 4.15.0-67+, 5.3.0-24+ Redhat Enterprise Linux: RHEL 7: 3.10.0-1062.8.1.el7+ RHEL 8: 4.18.0-147.2.1.el8_1+ CoreOS: v4.19.84+ The results In the best-case scenario, this fix enables a .87 increase in usable CPU for each instance of our affected applications, or a corresponding decrease in required CPU quota. These benefits will unlock increased application density and decreased application response times across our clusters. How to mitigate the issue Here’s what you can do to prevent CFS-Cgroup bandwidth control from creating a throttling issue on your systems: Monitor your throttled percentage Upgrade your kernels If you are using Kubernetes, use whole CPU quotas, as this decreases the number of schedulable CPUs available to the cgroup Increase quota where necessary Ongoing scheduler developments Konstantin Khlebnikov of Yandex proposed patches to the Linux kernel mailing list to create a “burst bank.” These changes are feasible now that we have removed the expiration logic, as described above. These bursting patches could enable even tighter packing of applications with small quota limits. If you find this idea interesting, join us on the Linux kernel mailing list and show your support. To read more about kernel scheduler bugs in Kubernetes, see these interesting GitHub issues: CFS quotas can lead to unnecessary throttling ( GH #67577 ) Setting CFS Period from within Kubernetes ( GH #51135 ) Unset CFS quota with CPU sets ( GH #70585 ) (GH #75682 ) Please also feel free to tweet your questions to me @dchiluk . How a Valid Fix Becomes a Regression---cross-posted on Medium .", "date": "2019-12-18"},
{"website": "Indeed-Engineering", "title": "Want to Code as an Engineering Manager? Time to Find a Unicorn", "author": [" by Robyn Rap"], "link": "https://engineering.indeedblog.com/blog/2020/08/eng-managers-want-to-code/", "abstract": "Coding as an engineering manager is an exercise in cognitive dissonance. If you’ve just become a manager, you’ve likely been measuring success by the quantity and quality of the code you ship as an individual contributor (IC). Suddenly, you have new metrics for success and your day-to-day work looks wildly different. One mentor tells you that coding as a manager is futile. Another tells you to stay in the code or risk dulling your technical skills. Some companies encourage engineering managers to conduct IC work as part of their culture, even codifying this behavior in promotion criteria. Others penalize managers for the exact same behavior. It’s confusing and stressful! Since I started managing, I’ve tried to deliver IC work at least once a quarter (some quarters more successfully than others). In this post, I share some of what I’ve learned: the challenges of coding as an engineering manager 1 , the benefits, and the ways to identify well-scoped unicorn projects to work on. Ruby star rainbow sparkle via SVG SILH While there is no I in Team, there is both a U and an IC in unicorn, just sayin'. Coding as a manager is hard Doing IC work as an engineering manager boils down to two challenges: constantly shifting contexts and priorities. Management requires a lot of context shifting . One meeting you’re coming up with a strategy for headcount, another you’re listening to someone grapple with giving tough feedback, and in yet another you’re leading a kanban meeting. By contrast, coding requires deep work , with an uninterrupted sense of focus for several hours. If you’ve ever tried to code in the half-hour between meetings, you know it looks something like this: import numpy as np\r\nimport pandas as pd\r\n# pull data here\r\nIf n in list(TODO):\r\n# look this up, used to know how to do this As a leader, your job is to prioritize supporting your teammates. That means having career conversations, providing feedback, and helping them deliver innovative and impactful work. While this often takes place in one-on-ones, there are follow-up meetings and check-ins with relevant collaborators, too. As you increase the size of your team, it becomes harder and harder to find any breathing room in a 40-hour work week, let alone the several hours needed to code even a small project from beginning to end. Why you should consider coding anyway 2 First, coding an IC project can help build empathy for your teammates, the tools that they use, and the challenges they encounter. For instance, I worked on a project with a teammate and learned firsthand about several services around Indeed and their challenges. As a result, in later meetings I was able to speak more specifically and with greater confidence about them. I submitted requests to the maintaining teams and, consequently, got updates prioritized that greatly helped my team. Second, coding (sometimes for the first time in weeks or even months) means you’ll need to ask your teammates for help. Recently, I was trying to solve a data wrangling problem that ended up with not one, not two, but three nested for loops. My teammates joked that it put the “OH NO” in Big O Notation. I humbly asked for help and together we figured out a way to solve the problem with much less complexity (and had a good laugh). In my experience, people like feeling helpful, and there’s something special about helping your boss when they’re struggling. We want our teammates to ask others for help . Being vulnerable and asking for help as a leader helps you model that behavior for your teammates, too. Finally, and most important to your mental health as a manager: shipping code can make you feel good. Management rarely lends itself to that feeling of “doneness” and is often riddled with self-doubt. Doing IC work, by contrast, is usually a discrete task. You can build something, point at it, and say, “Look! I built that. Sweet.” How to code as a manager Broadly speaking, doing IC work as a manager looks something like this: Make sure nothing is on fire. Find a narrowly scoped unicorn project. Block off time for deep work. Start with delegation in mind. I’ll walk through each of these in the sections below. Make sure nothing is on fire Coding while someone on your team is struggling with something urgent is like fiddling while Rome burns . It also means that you’re not doing the core functions of your job, i.e., helping your teammates. So, before you even think about scoping an IC project, make sure things on your team feel relatively stable. Find a narrowly scoped unicorn project Let me show you what scoping the right projects doesn’t look like. One time, in my eagerness to help a new product team, I took on running several A/B tests that we wanted to roll out by the end of the quarter. The A/B tests were simple enough to keep an eye on until I needed to spin other managerial plates. Meanwhile, my product manager had to pick up the slack. In the end, we delegated the tests to someone else who ran them to completion. It wasn’t a good feeling knowing I was letting my PM down. By contrast, a well-scoped IC project for managers: is not time-sensitive is fairly small does not have any dependencies is a \"nice-to-have\" or quality-of-life improvement that won’t get prioritized by your teammates and might have some nice impact plays to your strengths That is, a unicorn. Unicorn IC projects are not going to come up all the time. You can’t find them at all if you don’t know what to look for, though. For instance, I was in a design jam a few years back, where some UX teammates said to one another, “Yeah, we don’t always know X about Y queries. It would be nice if we had a tool that could do that.” What they were asking for was fairly small. Before they even knew it, a couple hours later I had built an Ishbook that they still use to help them understand user behaviors on the site. Alas, ye have yeself a unicorn! It’s also important that your IC project plays to your strengths . It’s already going to be easy to fall into the trap of feeling bad about your coding skills because they will likely be rusty and you’ll code more slowly than you used to. Consequently, you probably won’t keep feeling motivated to do more IC work and this blog post and I will have failed you. My IC projects usually are some kind of analysis of survey or measurement bias, helping A/B testing, or building well-designed graphs. Why? Because I like these things, I’m good at them, and they give me energy. When you choose an IC project that feels the same way, you’ll be able to get it done more quickly and at a higher quality. Block off time for deep work Half an hour here and there is usually not enough to get meaningful deep work done. Since becoming a manager, I have blocked off several hours in the morning to do deep work every week, whether that’s coding or reading the latest in my field. This encourages others to message me first before booking over my deep work block. Sometimes I need to join the meeting anyway, but more often than not, I avoid meeting during my peak coding hours. Google Calendar’s new Out of Office feature makes this even more aggressive, by auto-declining meetings booked over your block. Some weeks, I can’t prioritize deep work as much, other weeks I have more time than usual. I’ve seen managers beat themselves up about not having time to do IC work every week. Stop. It’s not a realistic goal. In his discussion of coding as a manager , Ben Edmunds writes, “Redefine what success looks like for yourself […] understand that day-to-day tasks aren’t set in stone. As a manager you need to be fluid.” Amen. Start with delegation in mind Coding as a manager means that you’re going to need to spin your other managerial plates again fairly quickly. You won’t have a ton of time to code projects, so whatever you build will likely need to be a prototype of some kind. When coding an IC project as a manager, figure out what a delegatable minimum viable product (MVP) might look like. Hint: it likely includes well-commented code and pair programming. Keep that MVP as your end goal. One of the tools I built as a manager helped our job search product run multivariate A/B tests more rigorously. I knew it was janky (heck, it pulled in data from a Google spreadsheet), but it could get the job done for the team and was better than nothing. I was then able to delegate it to my teammate. This was great for two reasons. First, it gave my teammate the chance to learn from my expertise. She got to deepen her understanding of measuring statistical significance in multivariate A/B tests . Second, she took what I had originally built and made it way better. While my prototype effectively shipped with Comic Sans as its font, her version had these beautiful, easily digestible graphs and an even more rigorous statistical approach. Her V.1 of the tool is a much better finished project that’s still in use today. To sum up Engineering managers get a lot of conflicting messages about whether they should code. Coding can help you build empathy and trust with your teammates, thereby making you a more effective leader. You set yourself up for failure, though, if you take on the same kinds of projects you had as an IC and try to stuff in coding between meetings. Instead, reframe the kinds of code you ship. Carve out dedicated time for deep work. And keep an eye out for small, non-urgent, delegatable unicorn projects that play to your strengths and can bring value to the team. Notes By the way, when I refer to “engineering,” I don’t just mean software engineers: I also include data scientists, QA, i.e., those whose IC work involves some degree of coding. Probably. The field of engineering management is still in its infancy—up until recently, very few books ( The Manager's Path , Things Every Engineering Manager Should Know , An Elegant Puzzle: Systems of Engineering Management ) were published on the subject. A lot of the evidence presented here is anecdotal, so your mileage may vary. For instance, one hypothesis I’ve heard about coding as a manager is that it helps you build \"street cred.\" I honestly don’t know if I’ve helped my cred or I just made myself look foolish in the Git repository here at work, so I chose not to touch on this point in this article, but I’m curious about others’ experiences with this. The scientist in me wants more rigorously collected qualitative and quantitative data around the benefits and drawbacks of doing IC work in the fields of software engineering and data science. So, reach out to me if you’re interested. I have some ideas. Coding as an Engineering Manager---cross-posted on Medium .", "date": "2020-08-03"},
{"website": "Indeed-Engineering", "title": "Jackson: A Growing User Base Presents New Challenges", "author": [" by Tatu Saloranta"], "link": "https://engineering.indeedblog.com/blog/2020/10/jackson-a-growing-user-base-presents-new-challenges/", "abstract": "Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. In my previous post , I introduced Jackson’s core competency as a JSON library for Java. I went on to describe the additional data formats, data types, and JVM languages Jackson supports. In this post, I will present challenges resulting from Jackson’s expansion, in my view, as Jackson’s creator and primary maintainer. I’ll also share our plans to address these challenges as a community. \"The other side of Cape Flattery\" by Tatu Saloranta Growing pains Over its 13 years, the Jackson project added a lot of new functionality with the help of a large community of contributors. Users report issues and developers contribute fixes, new features, and even new modules. For example, the Java 8 date/time datatype module —which supports Java 8 date/time types, java.time.*, specified in JSR-310 —was developed concurrently with the specification itself by a member of the community. Because of its ever-expanding functionality and user base, Jackson is experiencing growing pains and could use help with: Improving the documentation Enhancing the project’s website and branding Managing and prioritizing large-scale changes and features Refining the details of reported issues Testing the compatibility of new Jackson versions against  downstream dependencies Documentation’s structure and organization The availability, accessibility, and freshness of documentation is always a challenge for widely used libraries and frameworks. Jackson is no exception. It may even suffer from more documentation debt than other libraries with a comparable user base. The problem is not so much a lack of documentation per se. A lot of content exists already: 3rd-party tutorials Baeldung/jackson Jenkov.com/jackson LogicBig Jackson tutorial StudyTrails Jackson tutorial Jackson project repos Main Jackson GitHub portal Jackson-docs GitHub repository Individual component repository READMEs and wiki pages especially about jackson-databind Full javadoc generated API Reference is included in component repos StackOverflow StackOverflow questions and answers with over 13,000 questions tagged with “jackson” Jackson also has a Javadoc reference that documents most classes of interest. However, Jackson is a vast library. The Javadoc reference can seem overwhelming, especially to new users. And the Javadoc reference does not include usage examples. Our first priority is creating How-To guides that walk library consumers through the most typical use cases, such as: How to convert the f.ex JSON structure to match Java objects. How to format other data types such as XML. How to use Jackson within frameworks, such as Spring/Spring Boot, DropWizard, JAX-RS/Jersey, Lombok, and Immutables. We also want to improve the Javadoc reference. Some classes and methods have no descriptions, while descriptions for others are incomplete. In addition to the auto-generated Javadoc reference, we publish manually created wiki pages that detail the most used features and objects. We’d like to find ways to auto-update these wiki pages. To implement these modifications, we need the following documentation structure, tooling, and process changes: A super structure for adding inline content and links to external content Access permissions for contributors to add and correct documentation A documentation feedback mechanism to help focus improvement and maintenance efforts Documentation templates to make it easier to add information Project website and branding Even though the project is 13 years old and widely used, the main project home page still has the stock GitHub project look. This repo contains a lot of helpful information about Jackson, but it doesn’t have a distinct style, brand, or logo. For Hacktoberfest 2020, we created an issue for a Jackson logo design . Examples we like: Gopher mascot of the Go language Branding for the Kotlin language Wheel logo of Kubernetes and the Docker whale Managing large-scale changes We appreciate the healthy stream of code contributions—mostly to fix reported issues, but for new features as well. The most valuable type of contribution for the Jackson project has historically been user bug reports filed as GitHub issues. We get a dozen issues each week, leading to fixes and quality improvements. Without these reports, Jackson would not be half as good as it is. The new feature requests we receive regularly are another valuable source of improvements. New feature requests also help us prioritize new development work. Better management of larger changes and features is an area for improvement. These efforts take a long time to execute and benefit from careful planning and collaboration. While it’s possible to handle bigger changes under a single issue, this can get unwieldy quickly. The Jackson project could use help with setting the priority of new feature requests. Which issues should we focus on improving? Can we gauge the consensus of the whole community instead of relying on a small number of active and vocal participants? This helps to avoid “squeaky wheel problems” getting priority. What’s the best way to obtain early feedback on API and implementation plans? Using existing mailing lists, issue trackers, and chat all have their challenges. To address the need for more community feedback, I introduced the idea of “mini-specifications” called Jackson STrategic Enhancement Plan: JSTEP . Similar to KIP s in Kafka, JSTEPs are intended to foster collaboration. So far this has had limited success, partly due to the limitations of the tool used: GitHub wiki pages do not have the same feature set as GitHub issues or Google documents. I also started keeping a personal but public todo or work-in-progress (WIP) list— Jackson WIP —as a GitHub wiki page. I wanted a low-maintenance mechanism for me to track and share my own near-term plans. Refining reported issues, collaboratively As mentioned above, bug reports have historically been the most useful type of feedback. But while useful, managing reported issues and new feature requests takes a lot of effort. This has become especially challenging now that the data formats, data types, JVM languages, and user base have grown. Basic issue triage has become time consuming and includes the following steps: Validating the reported issues. Determining if the reported issues resulted from documentation that is missing or unclear. Asking for more information. The triage time takes away from the limited time we have to work on functionality and documentation. It can also slow down responses to issue reports, which in turn leads to a poor reporter experience. We would like to find ways to train and include volunteer issue triagers. They can help with refining issues in a timely manner and improving user engagement by: Adding labels Asking questions to get more information Adding findings of their own Getting module and domain experts engaged where applicable Starting with jackson-databind issues , we improved the workflow and refined GitHub issues by creating: Issue templates to help users submit better issue reports The “to-evaluate” label—set automatically for new issues The “most-wanted” label—which indicates issues that have consistently come up on mailing lists or up-voted using the GitHub thumbs-up emoticon If these changes are successful, we’ll propagate them to other project repos. Similarly, we can make other incremental changes along these lines. Compatibility testing of new versions with dependencies The unit test coverage in the Jackson project’s code is reasonable, and coverage for core components is quite good. There’s even a nascent effort to write more inter-component functional tests in the jackson-integration-tests repo. However, there is currently a sizable gap in testing the compatibility between the minor Jackson version that is under development and downstream dependencies, such as frameworks like Spring Boot. One challenge is the difference between the development lifecycles of Jackson and the libraries and frameworks that use Jackson. Existing libraries and frameworks depend on the stable Jackson minor version and test everything only against that version. Version 2.11 is the current stable version. They report any issues they find. We may fix the issues in a patch release, like 2.11.1 . Simultaneously, the Jackson project is developing a new minor version, 2.12 , which will be released and published as version 2.12.0. Working on versions 2.11.1 and 2.12 concurrently should not be an issue. We use the standard semantic versioning scheme for the Jackson public API, which should guard against creating new issues. In practice, standard semantic versioning cannot protect against all actual issues. Semantic versioning is used for Jackson’s public API, but internal APIs across core components do not have the same level of backwards compatibility. While Jackson maintainers consider the compatibility between the published and documented API, users tend to rely on the API’s actual observed behavior rather than its intended behavior. Sometimes what maintainers consider bugs or unexpected behaviors, users consider the behavior an actual feature or working as expected. That said, we can catch and address such issues during the development cycle of the new version if we test downstream systems using the under-development SNAPSHOT version of Jackson. At the time of writing this article, the current version of Spring Boot ships with a dependency on Jackson 2.11.2, the latest patch version. We can create an alternative set of Spring Boot unit and integration tests that instead rely on Jackson 2.12.0-SNAPSHOT. The snapshot version is published regularly, and the Jackson project provides automatic snapshot builds. Such cross-project integration tests would benefit all involved projects. This prevents the release of some (or perhaps most) regression issues, reducing the need for patch versions. Over time it should also significantly improve compatibility across Jackson versions, closing the gap between expected and actual behavior of the API. How to get involved Jackson’s growing user base and features have presented us with documentation, branding, issue triaging, issue prioritization, and code testing challenges. I’ve shared some of the steps we’ve taken to address them. You can help us by implementing some of these solutions or suggesting alternates. Together we can lay a foundation for Jackson’s future growth. For contribution opportunities, visit Jackson Hacktoberfest 2020 . To find out more, join us on Gitter chat or sign up for the jackson-dev mailing list . About the author Tatu Saloranta is a staff software engineer at Indeed, leading the team that is integrating the next-generation continuous deployment system. Outside of Indeed, he is best known for his open source activities. Under the handle @cowtowncoder , he has authored many popular open source Java libraries, such as Jackson, Woodstox, lzf compression codec, and Java ClassMate. For a complete list see FasterXML Org . Jackson's Growing User Base---cross-posted on Medium .", "date": "2020-10-19"},
{"website": "Indeed-Engineering", "title": "Jackson: More than JSON for Java", "author": [" by Tatu Saloranta"], "link": "https://engineering.indeedblog.com/blog/2020/09/jackson-more-than-json-for-java/", "abstract": "Jackson is a mature and feature-rich open source project that we use, support, and contribute to here at Indeed. As Jackson’s creator and primary maintainer, I want to highlight Jackson’s core competencies, extensions, and challenges in this two-part series. Photo of Cape Flattery by Tatu Saloranta Jackson’s core competency If you’re creating a web service in Java that reads or returns JSON, you need a library to convert Java objects into JSON and vice versa. Since this functionality is not available in the Java standard library (JDK), you can use the Jackson library for its core competency: writing Java objects as JSON and reading JSON as Java objects. Later in this post, I introduce Jackson’s additional features. As a Java JSON library, Jackson is: Feature-rich Highly configurable and performant Mature and reliable -- the project started 13 years ago, in 2007 Widely used and trusted Jackson is downloaded 25 million times per month via Maven Central, and 16,000 projects depend on jackson-databind . It’s the second most widely used Java library, after Guava, according to the Core Infrastructure Initiative’s census results . You can use Jackson directly, but nowadays users are more likely to encounter its functionality exposed by libraries and frameworks. These libraries and frameworks use Jackson by default for JSON handling to expose JSON requests and responses; or JSON configuration files as Java objects. Some examples are: Server-side frameworks: Spring/Spring Boot , DropWizard , RESTeasy , Restlet HTTP client libraries: OkHttp , Retrofit Application-specific clients: aws-java-sdk , elastic-search-client Data-processing frameworks: Apache Spark , Apache Flink , Apache Hadoop Users of these frameworks, libraries, and clients may not even be aware that they utilize Jackson. We are more likely to learn this when troubleshooting usage issues or making changes to default handling. Examples of Jackson usage The following example shows how to annotate request and response models in the Spring Boot framework. // Model of updated and accessed content\r\npublic class User {\r\n  private Long id;\r\n  private String name;\r\n  @JsonProperty(“dob”) // customize name used in JSON\r\n  private LocalDate dateOfBirth;\r\n  private LocalDateTime lastLogin;\r\n  // plus public Getters, Setters\r\n}\r\n\r\n// Service endpoint definition\r\n@RestController\r\n@RequestMapping(\"/users\")\r\npublic class UserController {\r\n  // ...\r\n  @GetMapping(\"/{id}\")\r\n  public User findUserById(@PathVariable Long id) {\r\n    return userStorage.findUserById(id);\r\n  }\r\n  @PostMapping(consumes = MediaType.APPLICATION_JSON_VALUE)\r\n  @ResponseStatus(HttpStatus.CREATED)\r\n  public Long createUser(@RequestBody User user) {\r\n    return userStorage.createUser(user);\r\n  }\r\n} In this example, the Spring Boot framework reads JSON as an instance of the User class. In the POST method, it passes a User instance to a storage handler. It also fetches the User instance via a storage handler for a given ID and writes serialized JSON of that instance. For a full explanation, see Jackson JSON Request and Response Mapping in Spring Boot . You can also read and write JSON directly via the Jackson API, without any framework or library, as follows: final ObjectMapper mapper = new ObjectMapper();\r\n// Read JSON as a Java object:\r\nUser user;\r\ntry (final InputStream in = requestContext.getInputStream()) {\r\n  user = mapper.readValue(in, User.class);\r\n}\r\n// Write a Java object as JSON\r\ntry (final OutputStream out = requestContext.getOutputStream()) {\r\n  mapper.writeValue(out, user);\r\n} Jackson is more than JSON for Java While Jackson’s core functionality originally started as JSON for Java, it quickly expanded through modules, which are pluggable extension components you can add to the Jackson core. These modules support features that the core does not handle by default. Jackson also has extensions for other JVM languages. These are some of the types of Jackson modules and extensions that are especially helpful: Data format modules Data type modules Modules for Kotlin, Scala, and Clojure Data format modules The data format modules allow you to read and write content that is encoded in formats other than JSON. The low-level encoding details of JSON differ from those of YAML, CSV, or Protobuf. However, much of the higher-level data-binding functionality is similar or identical. The higher-level data binding deals with the structure of Java Objects and expressing them as token streams (or a similar abstraction). Most of the code in the Jackson core is format-independent and only a small part is truly JSON-format specific. So, these so-called data format modules can easily extend Jackson to read and write content in other data formats. The modules implement low-level streaming from the Jackson API, but they share common data-binding functionality when it comes to converting content to and from Java objects. In the implementation of these modules, you find factories for streaming parsers and generators. They use a specific factory to construct an ObjectMapper that handles format-specific details, while you only interact with a (mostly) format-agnostic mapper abstraction. The very first data format module added to Jackson is jackson-dataformat-xml for XML. Supported data formats now include: Textual formats YAML, CSV, Properties files ( jackson-dataformats-text ) Binary formats Binary JSON ( CBOR , Smile ) ( jackson-dataformats-binary ) Avro , Protobuf , Amazon Ion ( jackson-dataformats-binary ) MessagePack ( jackson-dataformat-msgpack ) BSON ( bson4jackson , MongoJack and more) The usage for data format modules is similar to the Jackson API JSON usage. You only need to change JsonMapper (or the generic ObjectMapper ) to a format-specific alternative like XmlMapper . There are format-specific features that you can enable and disable. Also, some data formats require additional schema information to map content into JSON-like token representation, such as Avro, CSV and Protobuf. But across all formats, the API usage is similar. Examples Compared to the previous example that simply reads and writes JSON, these are alternatives for reading and writing other data formats: // XML usage is almost identical except it uses a different mapper object\r\nObjectMapper xmlMapper = new XmlMapper();\r\nString doc = xmlMapper.writeValueAsString(user);\r\nUser user = xmlMapper.readValue(doc, User.class);\r\n\r\n// YAML usage is almost identical except it uses a different mapper object\r\nObjectMapper yamlMapper = new YAMLMapper();\r\nbyte[] asBytes = yamlMapper.writeValueAsString(user);\r\nUser user2 = yamlMapper.readValue(asBytes, User.class);\r\n\r\n// CSV requires Schema for most operations (to convert between property\r\n// names and column positions)\r\n// You can compose the schema manually or generate it from POJO.\r\nCsvMapper csvMapper = new CsvMapper();\r\nCsvSchema userSchema = csvMapper.schemaFor(User.class);\r\nString csvDoc = csvMapper.writer(schema)\r\n.writeValueAsString(user);\r\nUser user3 = csvMapper.readerFor(User.class)\r\n.with(schema)\r\n.readValue(csvDoc); Data type modules Jackson core allows you to read and write Plain Old Java Objects (POJOs) and most standard JDK types (strings, numbers, booleans, arrays, collections, maps, date, calendar, URLs, and UUIDs). However, many Java projects also use value types defined by third-party libraries, such as Guava , Hibernate and Joda . It doesn’t work well if you handle instances of these value types as simple POJOs, especially collection types. Without explicit support, you would have to implement your own handlers as serializers and deserializers to extend Jackson functionality. It’s a huge undertaking to add such explicit support in Jackson core for even the most common Java libraries and could also lead to problematic dependencies. To solve this challenge, Jackson added a concept called data type modules . These are extensions built and packaged separately from core Jackson and added to the ObjectMapper instance during construction. Data type modules are released by both the Jackson team and external contributors such as authors of third-party libraries. Authors usually add these modules because they want to solve a specific use case and then share the fruits of their labors with others. Due to the pluggability of these modules, it is possible to use data type modules with different formats and to mix different value types. For example, you can read and write Hibernate-backed Guava ImmutableLists that contain Joda-defined Period values as JSON, CSV, or XML. The list of known data type modules is long---see Jackson Portal . Here are some examples: Guava (part of jackson-datatypes-collections ) Hibernate ( jackson-datatype-hibernate ) Joda ( jackson-datatype-joda ) Java 8 date/time ( jackson-datatype-jsr310 ) In addition to the data type module implementations, many frameworks also directly support Jackson data type module usage. In particular, various “immutable values” libraries offer such support, such as: Immutables.org Lombok Modules for Kotlin, Scala, Clojure If you're using Jackson, you're not limited to only using POJOs and JDK types with other JVM languages. Jackson has extensions to handle custom types of many other JVM languages. The following Jackson modules support Kotlin and Scala. Jackson-module-kotlin supports Kotlin Data classes, Ranges, Pair, Triple and Kotlin-style usage in general Jackson-module-scala supports Scala Case classes, Option, Either, Seq and Scala Collection types And for Clojure , there are a few libraries that use Jackson under the hood to implement similar support, such as: Cheshire (also supports Smile format) Jsonista This simplifies interoperability further. It makes it easier to use Java functionality from Kotlin, Scala, Clojure, or vice versa. What’s next In my next post, I will share my observations on the challenges that the Jackson project faces and how you can contribute to help. About the author Tatu Saloranta is a staff software engineer at Indeed. He leads the team that is integrating the next-generation continuous deployment system. Outside of Indeed, he is best known for his open source activities. Under the handle @cowtowncoder , he has authored many popular open source Java libraries, such as Jackson, Woodstox, lzf compression codec, and Java ClassMate. For a complete list see FasterXML Org . Jackson is more than JSON for Java---cross-posted on Medium .", "date": "2020-09-23"},
{"website": "Indeed-Engineering", "title": "Indeed + Hacktoberfest 2020: By The Numbers", "author": [" by Alison Yu"], "link": "https://engineering.indeedblog.com/blog/2020/12/hacktoberfest-2020-by-the-numbers/", "abstract": "Indeed + Hacktoberfest 2020 is in the books! We’re thrilled to share our results. External focus Internal focus As a Hacktoberfest Community Partner, we engaged directly with the external community. 1 external landing page 1 case study 6 supported open source projects tagged with the ‘hacktoberfest’ label 11 virtual office hours 437 commits into our supported repos To build on our strong base of internal contributors, we focused on flexibility. 29 virtual study halls hosted in 4 time zones 11 open source ambassadors with weekly check-ins 65 new open source participants 100 total Hacktoberfest participants 2,229 activities---pull requests opened, issues filed, comments posted, and code reviews conducted 328 submitted pull requests Our focus is on open source sustainability. To help us understand what this means, we use the oceanic ecosystem as a model. The ocean requires clean water, all sizes of fish, reefs for congregating, critters to clean up, and plankton to let bigger animals thrive. Similarly, our open source ecosystem is varied and interrelated. We support all sizes of projects, events for contributors to congregate, and an emphasis on cleaning up to help projects thrive. Our objective with this mindful approach: release open source projects that benefit interested adopters and contributors. September was Prep-tember September was a busy month of preparation. Indeed’s Open Source Program Office (OSPO) identified six projects to work with and promote during Hacktoberfest. Our qualifying criteria for the projects: Indeed actively uses the project and at least one of the project’s maintainers is an Indeed employee. Our program office shared the Hacktoberfest guidelines with project maintainers. We asked them to tag repos and issues within their projects that they wanted help with. We requested a three-day turnaround time for responding to comments or opening pull requests (PRs). We then worked with maintainers to schedule and publicize open office hours. Manager buy-in was crucial, so we worked with maintainers and their managers to dedicate time towards Hacktoberfest during the work week. Engaging with the external community Office hours and timely PR merges helped us make sure that the experience of Hacktoberfest participants was positive. The maintainers scheduled multiple office hours. These were times during which anyone, Indeed employee or not, could join a video call and ask project-specific questions. Our program office coordinated the publicity through the Hacktoberfest Event Board , Indeed’s Hacktoberfest landing page, and on each project’s readme.md page. Expanding our internal reach Virtual study halls---internal office hours that were not project specific---allowed us to help as many Indeed employees as possible. Instead of standing meeting times, the program office and our open source ambassadors hosted these events on an as-needed basis, resulting in more than one study hall every workday in October. We invited mentors and mentees to our new mentorship program. We paired people by timezone and their experience with open source: from brand new to needing help finding issues to needing guidance closing a “reach” issue to expand technical capabilities. The study hall events and mentorship programs were great. It felt like there was an involved community and lots of support and encouragement throughout the month. ---Technical Business Analyst Leveraging Indeed’s open source projects For Hacktoberfest, we leveraged our existing tools to share open issues in projects that Indeed is dependent on. First, we used Mariner (open sourced by our OSPO in 2019) to identify beginner-friendly issues recently opened in open source projects. For 2020, we open sourced Mariner Issue Collector ---a version of Mariner that runs as a GitHub Action. Since August 2019, we’ve been using Mariner output to produce a weekly internal blog post highlighting contribution opportunities for everyone at Indeed. We generated the list of Indeed employees who participated in Hacktoberfest using Starfish (open sourced by our program office in 2019). We used Starfish because it gives us accurate contributions over a period of time, no matter the date in which we receive a GitHub ID. We also use Starfish to compile the list of employees who are eligible to vote in our FOSS Contributor Fund . Encouraging open source sustainability We’re happy with the great results from Hacktoberfest 2020. We can only reach our open source sustainability goals if we create and maintain a habit of using and contributing to open source projects. Events like Hacktoberfest help us motivate and inspire Indeedians to get involved in supporting the open source software they use every day. One way we measure program success is by counting the number of people who contribute to open source on two or more days throughout the quarter. We refer to this metric as active recurring participants (ARPs). Compared to previous months, we saw an increase of over 200% of ARPs in October. I’ve had it as a goal for, let’s be honest, years to commit to OSS [open source]. And I went from 0 to 5 PRs this week. So thanks for the motivation and support to finally get me committing to open source. ---Senior Quality Assurance Automation Engineer To build on our Hacktoberfest 2020 momentum, we’re continuing to post open issues that Indeed has dependencies on. We’ve surveyed our 100 participants so that we can meet Indeed employees where they are. We believe the best time to start to contribute to open source is now. And the best open source ecosystem is sustainable. Indeed + Hacktoberfest 2020---cross-posted on Medium .", "date": "2020-12-03"},
{"website": "Indeed-Engineering", "title": "k8dash: Indeed’s Open Source Kubernetes Dashboard", "author": [" by Eric Herbrandson"], "link": "https://engineering.indeedblog.com/blog/2020/11/k8dash-indeeds-open-source-kubernetes-dashboard/", "abstract": "So you’ve got your first Kubernetes (also known as k8s) cluster up and running. Congratulations! Now, how do you operate the thing? Deployments, replica sets, stateful sets, pods, ingress, oh my! Getting Kubernetes running can feel like a big enough challenge in and of itself, but does day two of operations need to be just as much of a challenge? Kubernetes is an amazing but complex system. The learning curve can be steep. Plus, the standard Kubernetes dashboard has limited features. Another option is kubectl , which is extremely powerful but also a power user tool. Even if you become a kubectl wizard, can you expect everyone in your organization to do the same? And with kubectl it’s difficult to gain visibility into the general health and performance of the entire cluster all at once. Enter k8dash ---pronounced Kate Dash (/kāt,daSH/)---Indeed’s open source Kubernetes dashboard. Since k8dash’s release in March of 2019, it’s received over 625 Github stars and been downloaded from DockerHub over 1 million times. k8dash is a key component of Kubernetes operations for many organizations. In May of 2020, the Indeed Engineering organization adopted the k8dash project. We’re excited about the visibility this brings to the project. Benefits of managing your Kubernetes cluster with k8dash Here are a few of the benefits of k8dash. Quick installation Low operational complexity is a core tenet of the k8dash project. As such, you can install k8dash quickly with a couple dozen lines of YAML. k8dash runs only a single service. No databases or caches are required. This extends to AuthN/AuthZ via OIDC. If you’re already using OIDC to secure your cluster, k8dash makes extending this to your dashboards easy: configure 2-3 environment variables and you’re up and running. No special authenticating proxies or other complicated configurations are required. Cluster visualization and management k8dash helps you understand the current status of all of your cluster’s moving parts: namespaces, nodes, pods, deployments. Real-time charts show poorly performing resources. The intuitive interface removes much of the behind-the-scenes complexity and helps flatten your Kubernetes learning curve. You can manage your cluster components via the dashboard and leverage k8dash’s YAML editor to edit resources. k8dash uses the Kubernetes API and provides context-aware API docs. With k8dash you can view pod logs and even SSH directly into a running pod via a terminal directly in your browser, with a single click. k8dash also integrates with Metrics Server , letting you visualize CPU/RAM usage. This visualization helps you understand how well your services are running. As Kubernetes simplifies the complexity of running hundreds or even thousands of microservices across an abstract compute pool, it brings the promise of improved resource utilization through bin packing. However, for many organizations this promise goes unrealized because it can be difficult to know which services are over- or under-provisioned. k8dash’s intuitive UI takes the guesswork out of understanding how well services are provisioned. Real-time dashboard Because k8dash is a real-time dashboard, you don’t need to refresh pages to see the current state of your cluster. Instead, you can watch charts, graphs, and tables update in real time as you roll out a deployment. You can watch as nodes are added to and removed from your cluster, and know as soon as new nodes are fully available. Or, simply monitor a stream of Kubernetes cluster-wide events as they happen. Because k8dash is mobile optimized, you can monitor---and even modify---your cluster on the go. If you’re getting paged about a troublesome pod just as your movie is about to start, with k8dash you can restart the pod directly from your phone! The k8dash project: How to contribute k8dash is made up of a lightweight server and a client, and we’re always looking for core contributors. The server---built in Node.js and weighing in at ~150 LOC---is predominantly a proxy for the front end to the Kubernetes API server. The k8dash server makes heavy use of the following npm packages: express (web server) http-proxy-middleware (proxies requests to Kubernetes API) @kubernetes/client-node (official Kubernetes npm module. Used to discover Kubernetes API location) openid-client (fantastic npm module for OIDC) The client is a React.js application (using create-react-app) with minimal additional dependencies. If you would like to contribute, see the list of issues in GitHub . About the author Eric Herbrandson is a staff software engineer and member of the site reliability engineering team at Indeed. Eric has used orchestration frameworks, including ECS, Heroku, Docker Swarm, Hashicorp’s Nomad, DCOS (Marathon/Mesos), and Kubernetes. While finding Kubernetes to be the clear winner in the orchestration space, Eric recognized that existing visualization options were lacking compared to other frameworks. In an effort to better understand the Kubernetes API and to create a solution that contained all of the features he needed, Eric developed k8dash over a three-week period. k8dash---cross-posted on Medium .", "date": "2020-11-05"}
]